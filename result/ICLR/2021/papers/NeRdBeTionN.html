<html><head><meta charset='utf-8'><title>On Self-Supervised Image Representations for GAN Evaluation</title></head><body><h1>On Self-Supervised Image Representations for GAN Evaluation</h1><h3>By: ['Stanislav Morozov', 'Andrey Voynov', 'Artem Babenko']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3413</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> The embeddings from CNNs pretrained on Imagenet classification are de-facto standard image representations for assessing GANs via FID, Precision and Recall measures. Despite broad previous criticism of their usage for non-Imagenet domains, these embeddings are still the top choice in most of the GAN literature. In this paper, we advocate the usage of the state-of-the-art self-supervised representations to evaluate GANs on the established non-Imagenet benchmarks. These representations, typically obtained via contrastive learning, are shown to provide better transfer to new tasks and domains, therefore, can serve as more universal embeddings of natural images. With extensive comparison of the recent GANs on the common datasets, we show that self-supervised representations produce a more reasonable ranking of models in terms of FID/Precision/Recall, while the ranking with classification-pretrained embeddings often can be misleading.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 10</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=NeRdBeTionN</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=NeRdBeTionN</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 10</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Outstanding Paper Session 1</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 9</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 9</div></body></html>