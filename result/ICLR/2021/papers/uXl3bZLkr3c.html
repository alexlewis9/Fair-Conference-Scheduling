<html><head><meta charset='utf-8'><title>Tent_ Fully Test-Time Adaptation by Entropy Minimization</title></head><body><h1>Tent_ Fully Test-Time Adaptation by Entropy Minimization</h1><h3>By: ['Dequan Wang', 'Evan Shelhamer', 'Shaoteng Liu', 'Bruno Olshausen', 'trevor darrell']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3479</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=uXl3bZLkr3c</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=uXl3bZLkr3c</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 12</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 2</div></body></html>