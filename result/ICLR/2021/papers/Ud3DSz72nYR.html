<html><head><meta charset='utf-8'><title>Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions</title></head><body><h1>Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions</h1><h3>By: ['Zhengxian Lin', 'Kin-Ho Lam', 'Alan Fern']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/oral/3434</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Ud3DSz72nYR</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Ud3DSz72nYR</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 10</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 5</div></body></html>