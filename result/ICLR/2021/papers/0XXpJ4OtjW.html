<html><head><meta charset='utf-8'><title>Evolving Reinforcement Learning Algorithms</title></head><body><h1>Evolving Reinforcement Learning Algorithms</h1><h3>By: ['John Co-Reyes', 'Yingjie Miao', 'Daiyi Peng', 'Esteban Real', 'Quoc V Le', 'Sergey Levine', 'Honglak Lee', 'Aleksandra Faust']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/oral/3399</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 8</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=0XXpJ4OtjW</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=0XXpJ4OtjW</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 8</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 10</div></body></html>