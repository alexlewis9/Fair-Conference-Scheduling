<html><head><meta charset='utf-8'><title>Meta-GMVAE_ Mixture of Gaussian VAE for Unsupervised Meta-Learning</title></head><body><h1>Meta-GMVAE_ Mixture of Gaussian VAE for Unsupervised Meta-Learning</h1><h3>By: ['Dong Bok Lee', 'Dongchan Min', 'Seanie Lee', 'Sung Ju Hwang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3424</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Unsupervised learning aims to learn meaningful representations from unlabeled data which can captures its intrinsic structure, that can be transferred to downstream tasks. Meta-learning, whose objective is to learn to generalize across tasks such that the learned model can rapidly adapt to a novel task, shares the spirit of unsupervised learning in that the both seek to learn more effective and efficient learning procedure than learning from scratch. The fundamental difference of the two is that the most meta-learning approaches are supervised, assuming full access to the labels. However, acquiring labeled dataset for meta-training not only is costly as it requires human efforts in labeling but also limits its applications to pre-defined task distributions. In this paper, we propose a principled unsupervised meta-learning model, namely Meta-GMVAE, based on Variational Autoencoder (VAE) and set-level variational inference. Moreover, we introduce a mixture of Gaussian (GMM) prior, assuming that each modality represents each class-concept in a randomly sampled episode, which we optimize with Expectation-Maximization (EM). Then, the learned model can be used for downstream few-shot classification tasks, where we obtain task-specific parameters by performing semi-supervised EM on the latent representations of the support and query set, and predict labels of the query set by computing aggregated posteriors. We validate our model on Omniglot and Mini-ImageNet datasets by evaluating its performance on downstream few-shot classification tasks. The results show that our model obtain impressive performance gains over existing unsupervised meta-learning baselines, even outperforming supervised MAML on a certain setting.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=wS0UFjsNYjn</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=wS0UFjsNYjn</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 1</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 1</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1</div></body></html>