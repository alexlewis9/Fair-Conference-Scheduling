<html><head><meta charset='utf-8'><title>Long-tail learning via logit adjustment</title></head><body><h1>Long-tail learning via logit adjustment</h1><h3>By: ['Aditya Krishna Menon', 'Sadeep Jayasumana', 'Ankit Singh Rawat', 'Himanshu Jain', 'Andreas Veit', 'Sanjiv Kumar']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3516</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also  makes naive learning biased towards dominant labels. In this paper,  we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques  for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=37nvvqkCo5</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=37nvvqkCo5</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 12</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 2</div></body></html>