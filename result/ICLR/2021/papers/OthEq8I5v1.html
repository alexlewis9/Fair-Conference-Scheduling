<html><head><meta charset='utf-8'><title>Mutual Information State Intrinsic Control</title></head><body><h1>Mutual Information State Intrinsic Control</h1><h3>By: ['Rui Zhao', 'Yang Gao', 'Pieter Abbeel', 'Volker Tresp', 'Wei Xu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3509</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Reinforcement learning has been shown to be highly successful at many challenging tasks. However, success heavily relies on well-shaped rewards. Intrinsically motivated RL attempts to remove this constraint by defining an intrinsic reward function. Motivated by the self-consciousness concept in psychology, we make a natural assumption that the agent knows what constitutes itself, and propose a new intrinsic objective that encourages the agent to have maximum control on the environment. We mathematically formalize this reward as the mutual information between the agent state and the surrounding state under the current agent policy. With this new intrinsic motivation, we are able to outperform previous methods, including being able to complete the pick-and-place task for the first time without using any task reward. A video showing experimental results is available at https://youtu.be/AUCwc9RThpk.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 4</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=OthEq8I5v1</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=OthEq8I5v1</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 4</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 10</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 10</div></body></html>