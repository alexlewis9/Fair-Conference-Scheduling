<html><head><meta charset='utf-8'><title>Rethinking Attention with Performers</title></head><body><h1>Rethinking Attention with Performers</h1><h3>By: ['Krzysztof Choromanski', 'Valerii Likhosherstov', 'David Dohan', 'Xingyou Song', 'Georgiana-Andreea Gane', 'Tamas Sarlos', 'Peter Hawkins', 'Jared Q Davis', 'Afroz Mohiuddin', 'Lukasz Kaiser', 'David Belanger', 'Lucy J Colwell', 'Adrian Weller']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/oral/3397</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low  estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Ua6zuk0WRH</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Ua6zuk0WRH</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 6</div></body></html>