<html><head><meta charset='utf-8'><title>Generalization bounds via distillation</title></head><body><h1>Generalization bounds via distillation</h1><h3>By: ['Daniel Hsu', 'Ziwei Ji', 'Matus Telgarsky', 'Lan Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3433</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds.  The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation.  This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fully-connected layers, and skip connections, to name a few.  To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar and mnist demonstrating similar generalization performance between the original network and its distillation.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=EGdFhBzmAwB</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=EGdFhBzmAwB</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 8</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 6</div></body></html>