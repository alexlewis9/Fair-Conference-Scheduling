<html><head><meta charset='utf-8'><title>Getting a CLUE_ A  Method for Explaining Uncertainty Estimates</title></head><body><h1>Getting a CLUE_ A  Method for Explaining Uncertainty Estimates</h1><h3>By: ['Javier Antorán', 'Umang Bhatt', 'Tameem Adel', 'Adrian Weller', 'José Miguel Hernández Lobato']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/oral/3505</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=XSLF1XFq5h</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=XSLF1XFq5h</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 4</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1</div></body></html>