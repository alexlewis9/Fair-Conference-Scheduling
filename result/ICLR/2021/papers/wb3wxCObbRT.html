<html><head><meta charset='utf-8'><title>Growing Efficient Deep Networks by Structured Continuous Sparsification</title></head><body><h1>Growing Efficient Deep Networks by Structured Continuous Sparsification</h1><h3>By: ['Xin Yuan', 'Pedro Savarese', 'Michael Maire']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/oral/3451</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=wb3wxCObbRT</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=wb3wxCObbRT</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Outstanding Paper Session 2</div><h4>KMeans</h4><div style='margin-bottom:1em'>Outstanding Paper Session 2</div></body></html>