<html><head><meta charset='utf-8'><title>Information Laundering for Model Privacy</title></head><body><h1>Information Laundering for Model Privacy</h1><h3>By: ['Xinran Wang', 'Yu Xiang', 'Jun Gao', 'Jie Ding']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3510</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=dyaIRud1zXg</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=dyaIRud1zXg</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 10</div></body></html>