<html><head><meta charset='utf-8'><title>Learning a Latent Simplex in Input Sparsity Time</title></head><body><h1>Learning a Latent Simplex in Input Sparsity Time</h1><h3>By: ['Ainesh Bakshi', 'Chiranjib Bhattacharyya', 'Ravi Kannan', 'David Woodruff', 'Samson Zhou']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3530</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We consider the problem of learning a latent $k$-vertex simplex $K\in\mathbb{R}^d$, given $\mathbf{A}\in\mathbb{R}^{d\times n}$, which can be viewed as $n$ data points that are formed by randomly perturbing some latent points in $K$, possibly beyond $K$. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a $k$-vertex latent simplex in time roughly $O(k\cdot\text{nnz}(\mathbf{A}))$, where $\text{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. We show that the dependence on $k$ in the running time is unnecessary given a natural assumption about the mass of the top $k$ singular values of $\mathbf{A}$, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known.

We obtain a spectral low-rank approximation to $\mathbf{A}$ in input-sparsity time and show that the column space thus obtained has small $\sin\Theta$ (angular) distance to the right top-$k$ singular space of $\mathbf{A}$. Our algorithm then selects $k$ points in the low-rank  subspace with the largest inner product (in absolute value) with $k$ carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the $\Theta(k\cdot\text{nnz}(\mathbf{A}))$ running time.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 12</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=04LZCAxMSco</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=04LZCAxMSco</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 12</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 12</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 8</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 10</div></body></html>