<html><head><meta charset='utf-8'><title>Stabilized Medical Image Attacks</title></head><body><h1>Stabilized Medical Image Attacks</h1><h3>By: ['Gege Qi', 'Lijun GONG', 'Yibing Song', 'Kai Ma', 'Yefeng Zheng']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3419</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=QfTXQiGYudJ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=QfTXQiGYudJ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 10</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 3</div></body></html>