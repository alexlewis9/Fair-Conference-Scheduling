<html><head><meta charset='utf-8'><title>Neural Approximate Sufficient Statistics for Implicit Models</title></head><body><h1>Neural Approximate Sufficient Statistics for Implicit Models</h1><h3>By: ['Yanzhi Chen', 'Dinghuai Zhang', 'Michael U Gutmann', 'Aaron Courville', 'Zhanxing Zhu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3432</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=SRDuJssQud</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=SRDuJssQud</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 1</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>KMeans</h4><div style='margin-bottom:1em'>Outstanding Paper Session 1</div></body></html>