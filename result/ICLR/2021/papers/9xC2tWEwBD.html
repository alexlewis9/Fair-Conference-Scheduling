<html><head><meta charset='utf-8'><title>A Panda_ No, It's a Sloth_ Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference</title></head><body><h1>A Panda_ No, It's a Sloth_ Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference</h1><h3>By: ['Sanghyun Hong', 'Yigitcan Kaya', 'Ionut-Vlad Modoranu', 'Tudor Dumitras']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3384</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent increases in the computational demands of deep neural networks (DNNs), combined with the observation that most input samples require only simple models, have sparked interest in input-adaptive multi-exit architectures, such as MSDNets or Shallow-Deep Networks. These architectures enable faster inferences and could bring DNNs to low-power devices, e.g., in the Internet of Things (IoT). However, it is unknown if the computational savings provided by this approach are robust against adversarial pressure. In particular, an adversary may aim to slowdown adaptive DNNs by increasing their average inference time—a threat analogous to the denial-of-service attacks from the Internet. In this paper, we conduct a systematic evaluation of this threat by experimenting with three generic multi-exit DNNs (based on VGG16, MobileNet, and ResNet56) and a custom multi-exit architecture, on two popular image classification benchmarks (CIFAR-10 and Tiny ImageNet). To this end, we show that adversarial example-crafting techniques can be modified to cause slowdown, and we propose a metric for comparing their impact on different architectures. We show that a slowdown attack reduces the efficacy of multi-exit DNNs by 90–100%, and it amplifies the latency by 1.5–5× in a typical IoT deployment. We also show that it is possible to craft universal, reusable perturbations and that the attack can be effective in realistic black-box scenarios, where the attacker has limited knowledge about the victim. Finally, we show that adversarial training provides limited protection against slowdowns. These results suggest that further research is needed for defending multi-exit architectures against this emerging threat. Our code is available at https://github.com/sanghyun-hong/deepsloth.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=9xC2tWEwBD</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=9xC2tWEwBD</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 11</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 3</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 7</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 7</div></body></html>