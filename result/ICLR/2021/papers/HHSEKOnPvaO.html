<html><head><meta charset='utf-8'><title>Graph-Based Continual Learning</title></head><body><h1>Graph-Based Continual Learning</h1><h3>By: ['Binh Tang', 'David S Matteson']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3535</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 9</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=HHSEKOnPvaO</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=HHSEKOnPvaO</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 9</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Outstanding Paper Session 2</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 9</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 9</div></body></html>