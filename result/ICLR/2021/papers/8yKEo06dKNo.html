<html><head><meta charset='utf-8'><title>How Does Mixup Help With Robustness and Generalization_</title></head><body><h1>How Does Mixup Help With Robustness and Generalization_</h1><h3>By: ['Linjun Zhang', 'Zhun Deng', 'Kenji Kawaguchi', 'Amirata Ghorbani', 'James Zou']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2021/spotlight/3506</div><h4>year</h4><div style='margin-bottom:1em'>2021</div><h4>abstract</h4><div style='margin-bottom:1em'> Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However,  it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=8yKEo06dKNo</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=8yKEo06dKNo</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 5</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 8</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 6</div></body></html>