{
  "Baseline": {
    "Oral 1A": [
      "Predictive auxiliary objectives in deep RL mimic learning in the brain",
      "ClimODE_ Climate and Weather Forecasting with Physics-informed Neural ODEs",
      "Protein Discovery with Discrete Walk-Jump Sampling"
    ],
    "Oral 1B": [
      "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
      "ASID_ Active Exploration for System Identification in Robotic Manipulation",
      "Learning Interactive Real-World Simulators"
    ],
    "Oral 1C": [
      "LongLoRA_ Efficient Fine-tuning of Long-Context Large Language Models",
      "MathVista_ Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "Model Tells You What to Discard_ Adaptive KV Cache Compression for LLMs"
    ],
    "Oral 1D": [
      "Gene Regulatory Network Inference in the Presence of Dropouts_ a Causal View",
      "Robust agents learn causal world models"
    ],
    "Oral 2A": [
      "MetaGPT_ Meta Programming for A Multi-Agent Collaborative Framework",
      "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
      "Self-RAG_ Learning to Retrieve, Generate, and Critique through Self-Reflection"
    ],
    "Oral 2B": [
      "Vision Transformers Need Registers",
      "Ghost on the Shell_ An Expressive Representation of General 3D Shapes",
      "LRM_ Large Reconstruction Model for Single Image to 3D"
    ],
    "Oral 2C": [
      "Lipschitz Singularities in Diffusion Models",
      "Improved Techniques for Training Consistency Models",
      "W\u00fcrstchen_ An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models"
    ],
    "Oral 2D": [
      "Beyond Weisfeiler-Lehman_ A Quantitative Framework for GNN Expressiveness",
      "The mechanistic basis of data dependence and abrupt learning in an in-context classification task"
    ],
    "Oral 3A": [
      "LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "ReLU Strikes Back_ Exploiting Activation Sparsity in Large Language Models"
    ],
    "Oral 3B": [
      "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
      "Learning Energy Decompositions for Partial Inference in GFlowNets",
      "Cameras as Rays_ Pose Estimation via Ray Diffusion"
    ],
    "Oral 3C": [
      "Improving Convergence and Generalization Using Parameter Symmetries",
      "Meta Continual Learning Revisited_ Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
      "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization"
    ],
    "Oral 3D": [
      "How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_",
      "ValUES_ A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation"
    ],
    "Oral 4A": [
      "SWE-bench_ Can Language Models Resolve Real-world Github Issues_",
      "Batched Low-Rank Adaptation of Foundation Models"
    ],
    "Oral 4B": [
      "Topological data analysis on noisy quantum computers",
      "Flow Matching on General Geometries",
      "Graph Neural Networks for Learning Equivariant Representations of Neural Networks"
    ],
    "Oral 4C": [
      "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
      "METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction",
      "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning"
    ],
    "Oral 4D": [
      "Amortizing intractable inference in large language models",
      "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems."
    ],
    "Oral 5A": [
      "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
      "Diffusion Model for Dense Matching",
      "Generative Modeling with Phase Stochastic Bridge"
    ],
    "Oral 5B": [
      "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
      "Finetuning Text-to-Image Diffusion Models for Fairness",
      "Unprocessing Seven Years of Algorithmic Fairness"
    ],
    "Oral 5C": [
      "Neural Fine-Tuning Search for Few-Shot Learning",
      "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
      "Quick-Tune_ Quickly Learning Which Pretrained Model to Finetune and How"
    ],
    "Oral 5D": [
      "Interpreting CLIP's Image Representation via Text-Based Decomposition",
      "Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video"
    ],
    "Oral 6A": [
      "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation",
      "ExeDec_ Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
      "How I Warped Your Noise_ a Temporally-Correlated Noise Prior for Diffusion Models"
    ],
    "Oral 6B": [
      "GNNCert_ Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "Proving Test Set Contamination in Black-Box Language Models",
      "LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models"
    ],
    "Oral 6C": [
      "Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning",
      "Towards a statistical theory of data selection under weak supervision",
      "Never Train from Scratch_ Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"
    ],
    "Oral 6D": [
      "Multi-granularity Correspondence Learning from Long-term Noisy Videos",
      "Zipformer_ A faster and better encoder for automatic speech recognition"
    ],
    "Oral 7A": [
      "Small-scale proxies for large-scale Transformer training instabilities",
      "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment",
      "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming"
    ],
    "Oral 7B": [
      "DreamGaussian_ Generative Gaussian Splatting for Efficient 3D Content Creation",
      "_What Data Benefits My Classifier__ Enhancing Model Performance and Interpretability through Influence-Based Data Selection",
      "Knowledge Card_ Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"
    ],
    "Oral 7C": [
      "Less is More_ Fewer Interpretable Region via Submodular Subset Selection",
      "On the Joint Interaction of Models, Data, and Features"
    ],
    "Oral 7D": [
      "One-shot Empirical Privacy Estimation for Federated Learning",
      "On the Humanity of Conversational AI_ Evaluating the Psychological Portrayal of LLMs"
    ],
    "Oral 8A": [
      "Self-Alignment with Instruction Backtranslation",
      "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
      "Detecting, Explaining, and Mitigating Memorization in Diffusion Models"
    ],
    "Oral 8B": [
      "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
      "InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
      "Improved Active Learning via Dependent Leverage Score Sampling"
    ],
    "Oral 8C": [
      "Mastering Memory Tasks with World Models",
      "LEGO-Prover_ Neural Theorem Proving with Growing Libraries"
    ],
    "Oral 8D": [
      "Provable Compositional Generalization for Object-Centric Learning",
      "Multisize Dataset Condensation",
      "BooookScore_ A systematic exploration of book-length summarization in the era of LLMs"
    ]
  },
  "GreedyCohesive": {
    "Oral 1A": [
      "LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models",
      "ClimODE_ Climate and Weather Forecasting with Physics-informed Neural ODEs",
      "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"
    ],
    "Oral 1B": [
      "ASID_ Active Exploration for System Identification in Robotic Manipulation",
      "Learning Interactive Real-World Simulators",
      "METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction"
    ],
    "Oral 1C": [
      "LongLoRA_ Efficient Fine-tuning of Long-Context Large Language Models",
      "LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "Amortizing intractable inference in large language models"
    ],
    "Oral 1D": [
      "Towards a statistical theory of data selection under weak supervision",
      "_What Data Benefits My Classifier__ Enhancing Model Performance and Interpretability through Influence-Based Data Selection",
      "Gene Regulatory Network Inference in the Presence of Dropouts_ a Causal View"
    ],
    "Oral 2A": [
      "Meta Continual Learning Revisited_ Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
      "MetaGPT_ Meta Programming for A Multi-Agent Collaborative Framework",
      "Improved Techniques for Training Consistency Models"
    ],
    "Oral 2B": [
      "LRM_ Large Reconstruction Model for Single Image to 3D",
      "How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_",
      "Cameras as Rays_ Pose Estimation via Ray Diffusion"
    ],
    "Oral 2C": [
      "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
      "Lipschitz Singularities in Diffusion Models",
      "Generative Modeling with Phase Stochastic Bridge"
    ],
    "Oral 2D": [
      "Robust agents learn causal world models",
      "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
      "Predictive auxiliary objectives in deep RL mimic learning in the brain"
    ],
    "Oral 3B": [
      "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
      "DreamGaussian_ Generative Gaussian Splatting for Efficient 3D Content Creation",
      "Protein Discovery with Discrete Walk-Jump Sampling"
    ],
    "Oral 3D": [
      "ValUES_ A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation",
      "Less is More_ Fewer Interpretable Region via Submodular Subset Selection",
      "BooookScore_ A systematic exploration of book-length summarization in the era of LLMs"
    ],
    "Oral 4A": [
      "Batched Low-Rank Adaptation of Foundation Models",
      "Unprocessing Seven Years of Algorithmic Fairness"
    ],
    "Oral 4B": [
      "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming",
      "Topological data analysis on noisy quantum computers",
      "Improving Convergence and Generalization Using Parameter Symmetries"
    ],
    "Oral 4C": [
      "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
      "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning",
      "Mastering Memory Tasks with World Models"
    ],
    "Oral 4D": [
      "How I Warped Your Noise_ a Temporally-Correlated Noise Prior for Diffusion Models",
      "Multi-granularity Correspondence Learning from Long-term Noisy Videos",
      "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems."
    ],
    "Oral 5A": [
      "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
      "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation",
      "Ghost on the Shell_ An Expressive Representation of General 3D Shapes"
    ],
    "Oral 5B": [
      "Diffusion Model for Dense Matching",
      "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment",
      "Finetuning Text-to-Image Diffusion Models for Fairness"
    ],
    "Oral 5C": [
      "Quick-Tune_ Quickly Learning Which Pretrained Model to Finetune and How",
      "Neural Fine-Tuning Search for Few-Shot Learning",
      "Never Train from Scratch_ Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"
    ],
    "Oral 5D": [
      "Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video",
      "Vision Transformers Need Registers",
      "One-shot Empirical Privacy Estimation for Federated Learning"
    ],
    "Oral 6A": [
      "Learning Energy Decompositions for Partial Inference in GFlowNets",
      "ExeDec_ Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
      "Flow Matching on General Geometries"
    ],
    "Oral 6B": [
      "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
      "GNNCert_ Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "Beyond Weisfeiler-Lehman_ A Quantitative Framework for GNN Expressiveness"
    ],
    "Oral 6C": [
      "InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
      "Multisize Dataset Condensation",
      "Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning"
    ],
    "Oral 6D": [
      "Zipformer_ A faster and better encoder for automatic speech recognition",
      "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
      "Interpreting CLIP's Image Representation via Text-Based Decomposition"
    ],
    "Oral 7A": [
      "Small-scale proxies for large-scale Transformer training instabilities",
      "ReLU Strikes Back_ Exploiting Activation Sparsity in Large Language Models",
      "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"
    ],
    "Oral 7B": [
      "Model Tells You What to Discard_ Adaptive KV Cache Compression for LLMs",
      "Knowledge Card_ Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
      "Detecting, Explaining, and Mitigating Memorization in Diffusion Models"
    ],
    "Oral 7C": [
      "Provable Compositional Generalization for Object-Centric Learning",
      "W\u00fcrstchen_ An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
      "On the Joint Interaction of Models, Data, and Features"
    ],
    "Oral 7D": [
      "On the Humanity of Conversational AI_ Evaluating the Psychological Portrayal of LLMs",
      "SWE-bench_ Can Language Models Resolve Real-world Github Issues_",
      "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
    ],
    "Oral 8A": [
      "Self-Alignment with Instruction Backtranslation",
      "Proving Test Set Contamination in Black-Box Language Models",
      "Self-RAG_ Learning to Retrieve, Generate, and Critique through Self-Reflection"
    ],
    "Oral 8B": [
      "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
      "Improved Active Learning via Dependent Leverage Score Sampling",
      "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization"
    ],
    "Oral 8C": [
      "LEGO-Prover_ Neural Theorem Proving with Growing Libraries",
      "Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "MathVista_ Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
    ]
  },
  "KMedoids": {
    "Oral 1A": [
      "Predictive auxiliary objectives in deep RL mimic learning in the brain"
    ],
    "Oral 1B": [
      "ASID_ Active Exploration for System Identification in Robotic Manipulation",
      "Learning Interactive Real-World Simulators"
    ],
    "Oral 1C": [
      "LongLoRA_ Efficient Fine-tuning of Long-Context Large Language Models",
      "LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "Amortizing intractable inference in large language models",
      "Batched Low-Rank Adaptation of Foundation Models",
      "Model Tells You What to Discard_ Adaptive KV Cache Compression for LLMs",
      "Knowledge Card_ Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
      "BooookScore_ A systematic exploration of book-length summarization in the era of LLMs"
    ],
    "Oral 1D": [
      "Robust agents learn causal world models"
    ],
    "Oral 2A": [
      "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
      "MetaGPT_ Meta Programming for A Multi-Agent Collaborative Framework",
      "Meta Continual Learning Revisited_ Implicitly Enhancing Online Hessian Approximation via Variance Reduction"
    ],
    "Oral 2B": [
      "Vision Transformers Need Registers"
    ],
    "Oral 2C": [
      "Lipschitz Singularities in Diffusion Models",
      "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
      "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.",
      "Diffusion Model for Dense Matching",
      "W\u00fcrstchen_ An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
      "How I Warped Your Noise_ a Temporally-Correlated Noise Prior for Diffusion Models",
      "Detecting, Explaining, and Mitigating Memorization in Diffusion Models"
    ],
    "Oral 2D": [
      "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
      "The mechanistic basis of data dependence and abrupt learning in an in-context classification task"
    ],
    "Oral 3A": [
      "MathVista_ Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "LEGO-Prover_ Neural Theorem Proving with Growing Libraries"
    ],
    "Oral 3B": [
      "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
      "DreamGaussian_ Generative Gaussian Splatting for Efficient 3D Content Creation",
      "Learning Energy Decompositions for Partial Inference in GFlowNets",
      "Flow Matching on General Geometries",
      "Protein Discovery with Discrete Walk-Jump Sampling"
    ],
    "Oral 3C": [
      "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization"
    ],
    "Oral 3D": [
      "Provable Compositional Generalization for Object-Centric Learning",
      "ValUES_ A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation"
    ],
    "Oral 4A": [
      "SWE-bench_ Can Language Models Resolve Real-world Github Issues_",
      "On the Humanity of Conversational AI_ Evaluating the Psychological Portrayal of LLMs"
    ],
    "Oral 4B": [
      "Topological data analysis on noisy quantum computers"
    ],
    "Oral 4C": [
      "METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction"
    ],
    "Oral 4D": [
      "ClimODE_ Climate and Weather Forecasting with Physics-informed Neural ODEs"
    ],
    "Oral 5A": [
      "Multi-granularity Correspondence Learning from Long-term Noisy Videos",
      "Finetuning Text-to-Image Diffusion Models for Fairness",
      "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
      "ExeDec_ Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
      "On the Joint Interaction of Models, Data, and Features",
      "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space",
      "Generative Modeling with Phase Stochastic Bridge",
      "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming"
    ],
    "Oral 5B": [
      "Unprocessing Seven Years of Algorithmic Fairness"
    ],
    "Oral 5C": [
      "Neural Fine-Tuning Search for Few-Shot Learning",
      "Quick-Tune_ Quickly Learning Which Pretrained Model to Finetune and How"
    ],
    "Oral 5D": [
      "Interpreting CLIP's Image Representation via Text-Based Decomposition"
    ],
    "Oral 6A": [
      "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation"
    ],
    "Oral 6B": [
      "How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_",
      "Ghost on the Shell_ An Expressive Representation of General 3D Shapes",
      "LRM_ Large Reconstruction Model for Single Image to 3D",
      "Cameras as Rays_ Pose Estimation via Ray Diffusion",
      "LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models"
    ],
    "Oral 6C": [
      "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
      "Small-scale proxies for large-scale Transformer training instabilities",
      "Improved Techniques for Training Consistency Models",
      "ReLU Strikes Back_ Exploiting Activation Sparsity in Large Language Models",
      "Never Train from Scratch_ Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"
    ],
    "Oral 6D": [
      "Zipformer_ A faster and better encoder for automatic speech recognition"
    ],
    "Oral 7A": [
      "Gene Regulatory Network Inference in the Presence of Dropouts_ a Causal View",
      "Beyond Weisfeiler-Lehman_ A Quantitative Framework for GNN Expressiveness",
      "Improving Convergence and Generalization Using Parameter Symmetries",
      "GNNCert_ Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment",
      "Graph Neural Networks for Learning Equivariant Representations of Neural Networks"
    ],
    "Oral 7B": [
      "Towards a statistical theory of data selection under weak supervision",
      "_What Data Benefits My Classifier__ Enhancing Model Performance and Interpretability through Influence-Based Data Selection"
    ],
    "Oral 7C": [
      "Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning",
      "Less is More_ Fewer Interpretable Region via Submodular Subset Selection",
      "InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
      "Multisize Dataset Condensation"
    ],
    "Oral 7D": [
      "One-shot Empirical Privacy Estimation for Federated Learning"
    ],
    "Oral 8A": [
      "Self-Alignment with Instruction Backtranslation",
      "Proving Test Set Contamination in Black-Box Language Models",
      "Self-RAG_ Learning to Retrieve, Generate, and Critique through Self-Reflection"
    ],
    "Oral 8B": [
      "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
      "Improved Active Learning via Dependent Leverage Score Sampling"
    ],
    "Oral 8C": [
      "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
      "Mastering Memory Tasks with World Models",
      "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning"
    ],
    "Oral 8D": [
      "Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video"
    ]
  },
  "KMeans": {
    "Oral 1A": [
      "Predictive auxiliary objectives in deep RL mimic learning in the brain"
    ],
    "Oral 1B": [
      "ASID_ Active Exploration for System Identification in Robotic Manipulation",
      "METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction",
      "Learning Interactive Real-World Simulators"
    ],
    "Oral 1C": [
      "Protein Discovery with Discrete Walk-Jump Sampling"
    ],
    "Oral 1D": [
      "Robust agents learn causal world models"
    ],
    "Oral 2A": [
      "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
      "MetaGPT_ Meta Programming for A Multi-Agent Collaborative Framework",
      "Meta Continual Learning Revisited_ Implicitly Enhancing Online Hessian Approximation via Variance Reduction",
      "On the Humanity of Conversational AI_ Evaluating the Psychological Portrayal of LLMs"
    ],
    "Oral 2B": [
      "How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_",
      "Ghost on the Shell_ An Expressive Representation of General 3D Shapes",
      "LRM_ Large Reconstruction Model for Single Image to 3D",
      "Cameras as Rays_ Pose Estimation via Ray Diffusion",
      "LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models"
    ],
    "Oral 2C": [
      "Lipschitz Singularities in Diffusion Models",
      "Generalization in diffusion models arises from geometry-adaptive harmonic representations",
      "Improved Techniques for Training Consistency Models",
      "Diffusion Model for Dense Matching",
      "W\u00fcrstchen_ An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models",
      "How I Warped Your Noise_ a Temporally-Correlated Noise Prior for Diffusion Models",
      "Detecting, Explaining, and Mitigating Memorization in Diffusion Models"
    ],
    "Oral 2D": [
      "Beyond Weisfeiler-Lehman_ A Quantitative Framework for GNN Expressiveness",
      "GNNCert_ Deterministic Certification of Graph Neural Networks against Adversarial Perturbations",
      "Graph Neural Networks for Learning Equivariant Representations of Neural Networks"
    ],
    "Oral 3A": [
      "Less is More_ Fewer Interpretable Region via Submodular Subset Selection",
      "Model Tells You What to Discard_ Adaptive KV Cache Compression for LLMs",
      "ReLU Strikes Back_ Exploiting Activation Sparsity in Large Language Models",
      "Knowledge Card_ Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models"
    ],
    "Oral 3B": [
      "Gene Regulatory Network Inference in the Presence of Dropouts_ a Causal View",
      "Amortizing intractable inference in large language models",
      "Multi-granularity Correspondence Learning from Long-term Noisy Videos",
      "ClimODE_ Climate and Weather Forecasting with Physics-informed Neural ODEs",
      "Learning Energy Decompositions for Partial Inference in GFlowNets",
      "Finetuning Text-to-Image Diffusion Models for Fairness",
      "Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time",
      "ExeDec_ Execution Decomposition for Compositional Generalization in Neural Program Synthesis",
      "Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space"
    ],
    "Oral 3C": [
      "Improving Convergence and Generalization Using Parameter Symmetries"
    ],
    "Oral 3D": [
      "ValUES_ A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation"
    ],
    "Oral 4A": [
      "LongLoRA_ Efficient Fine-tuning of Long-Context Large Language Models",
      "LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "Batched Low-Rank Adaptation of Foundation Models",
      "BooookScore_ A systematic exploration of book-length summarization in the era of LLMs"
    ],
    "Oral 4B": [
      "Flow Matching on General Geometries"
    ],
    "Oral 4C": [
      "Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning",
      "Mastering Memory Tasks with World Models",
      "Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning"
    ],
    "Oral 4D": [
      "Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems."
    ],
    "Oral 5A": [
      "Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks",
      "DreamGaussian_ Generative Gaussian Splatting for Efficient 3D Content Creation",
      "Generative Modeling with Phase Stochastic Bridge"
    ],
    "Oral 5B": [
      "Unprocessing Seven Years of Algorithmic Fairness"
    ],
    "Oral 5C": [
      "Neural Fine-Tuning Search for Few-Shot Learning",
      "Quick-Tune_ Quickly Learning Which Pretrained Model to Finetune and How"
    ],
    "Oral 5D": [
      "Interpreting CLIP's Image Representation via Text-Based Decomposition"
    ],
    "Oral 6A": [
      "Multi-Source Diffusion Models for Simultaneous Music Generation and Separation"
    ],
    "Oral 6B": [
      "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
      "Self-Alignment with Instruction Backtranslation",
      "Proving Test Set Contamination in Black-Box Language Models",
      "Self-RAG_ Learning to Retrieve, Generate, and Critique through Self-Reflection"
    ],
    "Oral 6C": [
      "Vision Transformers Need Registers",
      "Small-scale proxies for large-scale Transformer training instabilities",
      "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions",
      "The mechanistic basis of data dependence and abrupt learning in an in-context classification task",
      "Never Train from Scratch_ Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"
    ],
    "Oral 6D": [
      "Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning",
      "Zipformer_ A faster and better encoder for automatic speech recognition",
      "InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning",
      "Multisize Dataset Condensation"
    ],
    "Oral 7A": [
      "Topological data analysis on noisy quantum computers",
      "Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming"
    ],
    "Oral 7B": [
      "Towards a statistical theory of data selection under weak supervision",
      "_What Data Benefits My Classifier__ Enhancing Model Performance and Interpretability through Influence-Based Data Selection"
    ],
    "Oral 7C": [
      "Provable Compositional Generalization for Object-Centric Learning",
      "On the Joint Interaction of Models, Data, and Features"
    ],
    "Oral 7D": [
      "One-shot Empirical Privacy Estimation for Federated Learning"
    ],
    "Oral 8A": [
      "Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video",
      "An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment"
    ],
    "Oral 8B": [
      "Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks",
      "Improved Active Learning via Dependent Leverage Score Sampling"
    ],
    "Oral 8C": [
      "SWE-bench_ Can Language Models Resolve Real-world Github Issues_",
      "MathVista_ Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
      "Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "LEGO-Prover_ Neural Theorem Proving with Growing Libraries"
    ],
    "Oral 8D": [
      "Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization"
    ]
  }
}