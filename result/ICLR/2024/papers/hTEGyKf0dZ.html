<html><head><meta charset='utf-8'><title>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</title></head><body><h1>Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</h1><h3>By: ['Xiangyu Qi', 'Yi Zeng', 'Tinghao Xie', 'Pin-Yu Chen', 'Ruoxi Jia', 'Prateek Mittal', 'Peter Henderson']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19735</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5B</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=hTEGyKf0dZ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=hTEGyKf0dZ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5B</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5B</div></body></html>