<html><head><meta charset='utf-8'><title>LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models</title></head><body><h1>LLMCarbon_ Modeling the End-to-End Carbon Footprint of Large Language Models</h1><h3>By: ['Ahmad Faiz', 'Sotaro Kaneda', 'Ruhan Wang', 'Rita Osi', 'Prateek Sharma', 'Fan Chen', 'Lei Jiang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19750</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \url{https://github.com/SotaroKaneda/MLCarbon}.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6B</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=aIok3ZD9to</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=aIok3ZD9to</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6B</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6B</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3A</div></body></html>