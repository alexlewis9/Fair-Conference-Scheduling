<html><head><meta charset='utf-8'><title>Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning</title></head><body><h1>Candidate Label Set Pruning_ A Data-centric Perspective for Deep Partial-label Learning</h1><h3>By: ['Shuo He', 'Chaojie Wang', 'Guowu Yang', 'Lei Feng']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19776</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Partial-label learning (PLL) allows each training example to be equipped with a set of candidate labels. Existing deep PLL research focuses on a \emph{learning-centric} perspective to design various training strategies for label disambiguation i.e., identifying the concealed true label from the candidate label set, for model training. However, when the size of the candidate label set becomes excessively large, these learning-centric strategies would be unable to find the true label for model training, thereby causing performance degradation. This motivates us to think from a \emph{data-centric} perspective and pioneer a new PLL-related task called candidate label set pruning (CLSP) that aims to filter out certain potential false candidate labels in a training-free manner. To this end, we propose the first CLSP method based on the inconsistency between the representation space and the candidate label space. Specifically, for each candidate label of a training instance, if it is not a candidate label of the instance's nearest neighbors in the representation space, then it has a high probability of being a false label. Based on this intuition, we employ a per-example pruning scheme that filters out a specific proportion of high-probability false candidate labels. Theoretically, we prove an upper bound of the pruning error rate and analyze how the quality of representations affects our proposed method. Empirically, extensive experiments on both benchmark-simulated and real-world PLL datasets validate the great value of CLSP to significantly improve many state-of-the-art deep PLL methods.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6C</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Fk5IzauJ7F</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Fk5IzauJ7F</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6C</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6D</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 8B</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6C</div></body></html>