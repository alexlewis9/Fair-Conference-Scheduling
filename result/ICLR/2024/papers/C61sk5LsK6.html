<html><head><meta charset='utf-8'><title>InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning</title></head><body><h1>InfoBatch_ Lossless Training Speed Up by Unbiased Dynamic Data Pruning</h1><h3>By: ['Ziheng Qin', 'Kai Wang', 'Zangwei Zheng', 'Jianyang Gu', 'Xiangyu Peng', 'Zhaopan Xu', 'Zhou Daquan', 'Lei Shang', 'Baigui Sun', 'Xuansong Xie', 'Yang You']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19779</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Data pruning aims to obtain lossless performances with less overall cost. A common approach is to filter out samples that make less contribution to the training. This could lead to gradient expectation bias compared to the original data. To solve this problem, we propose InfoBatch, a novel framework aiming to achieve lossless training acceleration by unbiased dynamic data pruning. Specifically, InfoBatchrandomly prunes a portion of less informative samples based on the loss distribution and rescales the gradients of the remaining samples to approximate the original gradient. As a plug-and-play and architecture-agnostic framework, InfoBatch consistently obtains lossless training results on classification, semantic segmentation, vision pertaining, and instruction fine-tuning tasks. On CIFAR10/100, ImageNet-1K, and ADE20K, InfoBatch losslessly saves 40% overall cost. For pertaining MAE and diffusion model, InfoBatch can respectively save 24.8% and 27% cost. For LLaMA instruction fine-tuning, combining InfoBatch and the recent coreset selection method (DQ) can achieve 10 times acceleration. Our results encourage more exploration on the data efficiency aspect of large model training. Code is publicly available at NUS-HPC-AI-Lab/InfoBatch.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 8B</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=C61sk5LsK6</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=C61sk5LsK6</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 8B</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5C</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3A</div></body></html>