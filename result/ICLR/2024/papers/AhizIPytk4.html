<html><head><meta charset='utf-8'><title>How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_</title></head><body><h1>How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks_</h1><h3>By: ['Wenxuan Li', 'Alan Yuille', 'Zongwei Zhou']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19781</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> The pre-training and fine-tuning paradigm has become prominent in transfer learning. For example, if the model is pre-trained on ImageNet and then fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL from scratch. While ImageNet pre-training has shown enormous success, it is formed in 2D, and the learned features are for classification tasks; when transferring to more diverse tasks, like 3D image segmentation, its performance is inevitably compromised due to the deviation from the original ImageNet context. A significant challenge lies in the lack of large, annotated 3D datasets rivaling the scale of ImageNet for model pre-training. To overcome this challenge, we make two contributions. Firstly, we construct AbdomenAtlas 1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes with high-quality, per-voxel annotations of 25 anatomical structures and pseudo annotations of seven tumor types. Secondly, we develop a suite of models that are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary analyses indicate that the model trained only with 21 CT volumes, 672 masks, and 40 GPU hours has a transfer learning ability similar to the model trained with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the transfer learning ability of supervised models can further scale up with larger annotated datasets, achieving significantly better performance than preexisting pre-trained models, irrespective of their pre-training methodologies or data sources. We hope this study can facilitate collective efforts in constructing larger 3D medical datasets and more releases of supervised pre-trained models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3D</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=AhizIPytk4</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=AhizIPytk4</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3D</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3D</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3D</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5C</div></body></html>