<html><head><meta charset='utf-8'><title>Interpreting CLIP's Image Representation via Text-Based Decomposition</title></head><body><h1>Interpreting CLIP's Image Representation via Text-Based Decomposition</h1><h3>By: ['Yossi Gandelsman', 'Alexei Efros', 'Jacob Steinhardt']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19791</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that scalable understanding of transformer models is attainable and can be used to repair and improve models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=5Ca9sSzuDp</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=5Ca9sSzuDp</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2B</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5D</div></body></html>