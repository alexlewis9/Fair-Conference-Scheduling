<html><head><meta charset='utf-8'><title>Batched Low-Rank Adaptation of Foundation Models</title></head><body><h1>Batched Low-Rank Adaptation of Foundation Models</h1><h3>By: ['Yeming Wen', 'Swarat Chaudhuri']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19716</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.To address this, we introduce FLoRA (Fast LoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that \flora/ retains the performance merits of \lora/, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4A</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=w4abltTZ2f</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=w4abltTZ2f</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4A</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4A</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1C</div></body></html>