<html><head><meta charset='utf-8'><title>METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction</title></head><body><h1>METRA_ Scalable Unsupervised RL with Metric-Aware Abstraction</h1><h3>By: ['Seohong Park', 'Oleh Rybkin', 'Sergey Levine']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19745</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Our main idea is, instead of directly covering the entire state space, to only cover a compact latent space $\mathcal{Z}$ that is metrically connected to the state space $\mathcal{S}$ by temporal distances. By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments. Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments, being the first unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid. Our code and videos are available at https://seohong.me/projects/metra/</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4C</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=c5pwL0Soay</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=c5pwL0Soay</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4C</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4C</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4C</div></body></html>