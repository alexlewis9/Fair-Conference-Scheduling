<html><head><meta charset='utf-8'><title>Improved Techniques for Training Consistency Models</title></head><body><h1>Improved Techniques for Training Consistency Models</h1><h3>By: ['Yang Song', 'Prafulla Dhariwal']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19754</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2C</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=WNzy9bRDvG</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=WNzy9bRDvG</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2C</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2C</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5A</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6A</div></body></html>