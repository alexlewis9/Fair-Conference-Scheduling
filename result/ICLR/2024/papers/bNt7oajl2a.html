<html><head><meta charset='utf-8'><title>Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</title></head><body><h1>Phenomenal Yet Puzzling_ Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement</h1><h3>By: ['Linlu Qiu', 'Liwei Jiang', 'Ximing Lu', 'Melanie Sclar', 'Valentina Pyatkin', 'Chandra Bhagavatula', 'Bailin Wang', 'Yoon Kim', 'Yejin Choi', 'Nouha Dziri', 'Xiang Ren']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19747</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\textit{inductive reasoners}$, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=bNt7oajl2a</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=bNt7oajl2a</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1C</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4D</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4A</div></body></html>