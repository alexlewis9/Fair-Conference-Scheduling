<html><head><meta charset='utf-8'><title>LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models</title></head><body><h1>LoftQ_ LoRA-Fine-Tuning-aware Quantization for Large Language Models</h1><h3>By: ['Yixiao Li', 'Yifan Yu', 'Chen Liang', 'Nikos Karampatziakis', 'Pengcheng He', 'Weizhu Chen', 'Tuo Zhao']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19765</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=LzPWWPAdY4</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=LzPWWPAdY4</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3A</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3A</div></body></html>