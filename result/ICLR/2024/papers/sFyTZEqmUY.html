<html><head><meta charset='utf-8'><title>Learning Interactive Real-World Simulators</title></head><body><h1>Learning Interactive Real-World Simulators</h1><h3>By: ['Sherry Yang', 'Yilun Du', 'Seyed Ghasemipour', 'Jonathan Tompson', 'Leslie Kaelbling', 'Dale Schuurmans', 'Pieter Abbeel']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19722</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by x,y” from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1B</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=sFyTZEqmUY</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=sFyTZEqmUY</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1B</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1B</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1B</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1B</div></body></html>