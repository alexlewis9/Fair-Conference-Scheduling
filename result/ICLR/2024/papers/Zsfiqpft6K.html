<html><head><meta charset='utf-8'><title>Diffusion Model for Dense Matching</title></head><body><h1>Diffusion Model for Dense Matching</h1><h3>By: ['Jisu Nam', 'Gyuseong Lee', 'Seonwoo Kim', 'In√®s Hyeonsu Kim', 'Hyoungwon Cho', 'Seyeon Kim', 'Seungryong Kim']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19751</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> The objective for establishing dense correspondence between paired images con- sists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, re- cent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explic- itly takes matching cost and injects the prior within generative process. However, limited input resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. Code and pretrained weights are available at https://ku-cvlab.github.io/DiffMatch.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5A</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Zsfiqpft6K</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Zsfiqpft6K</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5A</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5A</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5A</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5A</div></body></html>