<html><head><meta charset='utf-8'><title>Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks</title></head><body><h1>Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks</h1><h3>By: ['Jie Hu', 'Vishwaraj Doshi', 'Do Young Eun']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19780</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard *linear* Markovian token by one which follows a *non-linear* Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar $\\alpha$, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves $O(1/\\alpha)$ decrease in the asymptotic variance for sampling. We propose the use of a `generalized' version of the SRRW to drive token algorithms for distributed stochastic optimization in the form of stochastic approximation, termed SA-SRRW. We prove that the optimization iterate errors of the resulting SA-SRRW converge to zero almost surely and prove a central limit theorem, deriving the explicit form of the resulting asymptotic covariance matrix corresponding to iterate errors. This asymptotic covariance is always smaller than that of an algorithm driven by the base Markov chain and decreases at rate $O(1/\\alpha^2)$ - the performance benefit of using SRRW thereby *amplified* in the stochastic optimization context. Empirical results support our theoretical findings.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 8B</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=BV1PHbTJzd</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=BV1PHbTJzd</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 8B</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 7A</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3C</div></body></html>