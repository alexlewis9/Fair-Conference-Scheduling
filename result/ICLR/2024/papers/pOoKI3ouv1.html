<html><head><meta charset='utf-8'><title>Robust agents learn causal world models</title></head><body><h1>Robust agents learn causal world models</h1><h3>By: ['Jonathan Richens', 'Tom Everitt']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19724</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1D</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=pOoKI3ouv1</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=pOoKI3ouv1</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1D</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6C</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1D</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1D</div></body></html>