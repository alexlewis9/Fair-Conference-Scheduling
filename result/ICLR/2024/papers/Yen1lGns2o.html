<html><head><meta charset='utf-8'><title>Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video</title></head><body><h1>Is ImageNet worth 1 video_ Learning strong image encoders from 1 long unlabelled video</h1><h3>By: ['Shashank Venkataramanan', 'Mamshad Nayeem Rizve', 'Joao Carreira', 'Yuki Asano', 'Yannis Avrithis']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2024/oral/19752</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a Walking Tours'' dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning. Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a tracking to learn to recognize'' approach. Our method called DoRA, leads to attention maps that D isc O ver and t RA ck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Yen1lGns2o</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Yen1lGns2o</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5D</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4C</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5D</div></body></html>