<html><head><meta charset='utf-8'><title>Pre-training via Denoising for Molecular Property Prediction</title></head><body><h1>Pre-training via Denoising for Molecular Property Prediction</h1><h3>By: ['Sheheryar Zaidi', 'Michael Schaarschmidt', 'James Martens', 'Hyunjik Kim', 'Yee Whye Teh', 'Alvaro Sanchez Gonzalez', 'Peter Battaglia', 'Razvan Pascanu', 'Jonathan Godwin']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12772</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -- on pre-training.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=tYIMtogyee</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3 Track 5: Deep Learning and representational learning & Neuroscience and Cognitive Science</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1 Track 2: Machine Learning for Sciences</div></body></html>