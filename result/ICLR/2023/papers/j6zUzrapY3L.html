<html><head><meta charset='utf-8'><title>DIFFormer_ Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</title></head><body><h1>DIFFormer_ Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</h1><h3>By: ['Qitian Wu', 'Chenxiao Yang', 'Wentao Zhao', 'Yixuan He', 'David Wipf', 'Junchi Yan']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12621</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction. The codes are available at https://github.com/qitianwu/DIFFormer.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6 Track 5: Applications- & Deep Learning and representational learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=j6zUzrapY3L</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6 Track 5: Applications- & Deep Learning and representational learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6 Track 5: Applications- & Deep Learning and representational learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6 Track 5: Applications- & Deep Learning and representational learning</div></body></html>