<html><head><meta charset='utf-8'><title>Fisher-Legendre (FishLeg) optimization of deep neural networks</title></head><body><h1>Fisher-Legendre (FishLeg) optimization of deep neural networks</h1><h3>By: ['Jezabel R. Garcia', 'Federica Freddi', 'Stathi Fotiadis', 'Maolin Li', 'Sattar Vakili', 'Alberto Bernacchia', 'Guillaume Hennequin']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12665</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Incorporating second-order gradient information (curvature) into optimization can dramatically reduce the number of iterations required to train machine learning models. In natural gradient descent, such information comes from the Fisher information matrix which yields a number of desirable properties. As exact natural gradient updates are intractable for large models, successful methods such as KFAC and sequels approximate the Fisher in a structured form that can easily be inverted. However, this requires model/layer-specific tensor algebra and certain approximations that are often difficult to justify. Here, we use ideas from Legendre-Fenchel duality to learn a direct and efficiently evaluated model for the product of the inverse Fisher with any vector, in an online manner, leading to natural gradient steps that get progressively more accurate over time despite noisy gradients. We prove that the resulting “Fisher-Legendre” (FishLeg) optimizer converges to a (global) minimum of non-convex functions satisfying the PL condition, which applies in particular to deep linear networks. On standard auto-encoder benchmarks, we show empirically that FishLeg outperforms standard first-order optimization methods, and performs on par with or better than other second-order methods, especially when using small batches. Thanks to its generality, we expect our approach to facilitate the handling of a variety  neural network layers in future work.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1 Track 6: Deep Learning and representational learning II</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=c9lAOPvQHS</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1 Track 6: Deep Learning and representational learning II</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 3: Deep Learning and representational learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 6: Deep Learning and representational learning- Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div></body></html>