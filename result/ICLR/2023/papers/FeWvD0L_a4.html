<html><head><meta charset='utf-8'><title>Learnable Behavior Control_ Breaking Atari Human World Records via Sample-Efficient Behavior Selection</title></head><body><h1>Learnable Behavior Control_ Breaking Atari Human World Records via Sample-Efficient Behavior Selection</h1><h3>By: ['Jiajun Fan', 'Yuzheng Zhuang', 'Yuecheng Liu', 'Jianye HAO', 'Bin Wang', 'Jiangcheng Zhu', 'Hao Wang', 'Shu-Tao Xia']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12618</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity.  In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=FeWvD0L_a4</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div></body></html>