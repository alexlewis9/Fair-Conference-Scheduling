<html><head><meta charset='utf-8'><title>Multifactor Sequential Disentanglement via Structured Koopman Autoencoders</title></head><body><h1>Multifactor Sequential Disentanglement via Structured Koopman Autoencoders</h1><h3>By: ['Nimrod Berman', 'Ilan Naiman', 'Omri Azencot']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12605</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches. The code is available at https://github.com/azencot-group/SKD.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 6: Deep Learning and representational learning- Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=6fuPIe9tbnC</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 6: Deep Learning and representational learning- Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1 Track 6: Deep Learning and representational learning II</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1 Track 3: Neuroscience and Cognitive Science & General Machine Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 6: Deep Learning and representational learning- Reinforcement Learning</div></body></html>