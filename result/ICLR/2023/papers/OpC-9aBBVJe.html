<html><head><meta charset='utf-8'><title>Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier</title></head><body><h1>Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier</h1><h3>By: ["Pierluca D'Oro", 'Max Schwarzer', 'Evgenii Nikishin', 'Pierre-Luc Bacon', 'Marc G Bellemare', 'Aaron Courville']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12655</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=OpC-9aBBVJe</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div></body></html>