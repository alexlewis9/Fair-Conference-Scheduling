<html><head><meta charset='utf-8'><title>Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</title></head><body><h1>Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</h1><h3>By: ['Zeyuan Allen-Zhu', 'Yuanzhi Li']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12664</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We formally study how \emph{ensemble} of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using \emph{knowledge distillation}. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the \emph{same} architecture, trained using the \emph{same} algorithm on the \emph{same} data set, and they only differ by the random seeds used in the initialization.We show that ensemble/knowledge distillation in \emph{deep learning} works very differently from traditional learning theory (such as boosting or NTKs). We develop a theory showing that when data has a structure we refer to as multi-view'', then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the dark knowledge'' is hidden in the outputs of the ensemble and can be used in distillation.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6 Track 1: Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Uuf2q9TfXGA</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6 Track 1: Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 2: Infrastructure & Social Aspects of Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 1: Unsupervised and Self-supervised learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div></body></html>