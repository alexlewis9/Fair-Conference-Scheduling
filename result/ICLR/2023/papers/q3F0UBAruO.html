<html><head><meta charset='utf-8'><title>Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games_ A Communication Perspective</title></head><body><h1>Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games_ A Communication Perspective</h1><h3>By: ['Yiming Gao', 'Feiyu Liu', 'Liang Wang', 'Zhenjie Lian', 'Weixuan Wang', 'Siqin Li', 'Xianliang Wang', 'Xianhan Zeng', 'Rundong Wang', 'jiawei wang', 'QIANG FU', 'Yang Wei', 'Lanxiao Huang', 'Wei Liu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12770</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=q3F0UBAruO</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 4: Reinforcement Learning II</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5 Track 3: Deep Learning and representational learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3 Track 1: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5 Track 5: Deep Learning and representational learning & Reinforcement Learning</div></body></html>