<html><head><meta charset='utf-8'><title>Is the Performance of My Deep Network Too Good to Be True_ A Direct Approach to Estimating the Bayes Error in Binary Classification</title></head><body><h1>Is the Performance of My Deep Network Too Good to Be True_ A Direct Approach to Estimating the Bayes Error in Binary Classification</h1><h3>By: ['Takashi Ishida', 'Ikko Yamane', 'Nontawat Charoenphakdee', 'Gang Niu', 'Masashi Sugiyama']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12692</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer may have reached, or is about to reach, the Bayes error for benchmark datasets. Finally, we discuss how we can study the inherent difficulty of the acceptance/rejection decision for scientific articles, by estimating the Bayes error of the ICLR papers from 2017 to 2023.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1 Track 4: Social Aspects of Machine Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=FZdJQgy05rz</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1 Track 4: Social Aspects of Machine Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1 Track 3: Neuroscience and Cognitive Science & General Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 1: Unsupervised and Self-supervised learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div></body></html>