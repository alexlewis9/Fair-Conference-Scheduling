<html><head><meta charset='utf-8'><title>Quantifying Memorization Across Neural Language Models</title></head><body><h1>Quantifying Memorization Across Neural Language Models</h1><h3>By: ['Nicholas Carlini', 'Daphne Ippolito', 'Matthew Jagielski', 'Katherine Lee', 'Florian Tramer', 'Chiyuan Zhang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12637</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1 Track 4: Social Aspects of Machine Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=TatRHT_1cK</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1 Track 4: Social Aspects of Machine Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5 Track 4: Applications & Optimization</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 6: Applications & Social Aspects of Machine Learning</div></body></html>