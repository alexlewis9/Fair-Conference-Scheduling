<html><head><meta charset='utf-8'><title>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</title></head><body><h1>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</h1><h3>By: ['Eoin Kenny', 'Mycal Tucker', 'Julie Shah']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12596</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an "interpretable-by-design" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5 Track 1: Unsupervised and Self-supervised learning & Social Aspects of Machine Learning-</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=hWwY_Jq0xsN</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5 Track 1: Unsupervised and Self-supervised learning & Social Aspects of Machine Learning-</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div></body></html>