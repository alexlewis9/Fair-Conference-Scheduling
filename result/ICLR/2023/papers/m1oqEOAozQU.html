<html><head><meta charset='utf-8'><title>Graph Neural Networks for Link Prediction with Subgraph Sketching</title></head><body><h1>Graph Neural Networks for Link Prediction with Subgraph Sketching</h1><h3>By: ['Benjamin Chamberlain', 'Sergey Shirobokov', 'Emanuele Rossi', 'Fabrizio Frasca', 'Thomas Markovich', 'Nils Hammerla', 'Michael Bronstein', 'Max Hansmire']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12595</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs).  It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5 Track 3: Deep Learning and representational learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=m1oqEOAozQU</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5 Track 3: Deep Learning and representational learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3 Track 5: Deep Learning and representational learning & Neuroscience and Cognitive Science</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 3: Deep Learning and representational learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6 Track 5: Applications- & Deep Learning and representational learning</div></body></html>