<html><head><meta charset='utf-8'><title>Sequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting</title></head><body><h1>Sequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting</h1><h3>By: ['Xiajun Jiang', 'Ryan Missel', 'Zhiyuan Li', 'Linwei Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12551</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Modern applications increasingly require learning and forecasting latent dynamics from high-dimensional time-series. Compared to univariate time-series forecasting, this adds a new challenge of reasoning about the latent dynamics of an unobserved abstract state. Sequential latent variable models (LVMs) present an attractive solution, although existing works either struggle with long-term forecasting or have difficulty learning across diverse dynamics. In this paper, we first present a conceptual framework of sequential LVMs to unify existing works, contrast their fundamental limitations, and identify an intuitive solution to long-term forecasting for diverse dynamics via meta-learning. We then present the first framework of few-shot forecasting for high-dimensional time-series: instead of learning a single dynamic function, we leverage data of diverse dynamics and learn to adapt latent dynamic functions to few-shot support series. This is realized via Bayesian meta-learning underpinned by: 1) a latent dynamic function conditioned on knowledge derived from few-shot support series, and 2) a meta-model that learns to extract such dynamic-specific knowledge via feed-forward embedding of support set. We compared the presented framework with a comprehensive set of baseline models trained 1) globally on the large meta-training set with diverse dynamics, and 2) individually on single dynamics, both with and without fine-tuning to k-shot support series used by the meta-models. We demonstrated that the presented framework is agnostic to the latent dynamic function of choice and, at meta-test time, is able to forecast for new dynamics given variable-shot of support series.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=7C9aRX2nBf2</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1 Track 2: Machine Learning for Sciences</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1 Track 2: Machine Learning for Sciences</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3 Track 1: Reinforcement Learning</div></body></html>