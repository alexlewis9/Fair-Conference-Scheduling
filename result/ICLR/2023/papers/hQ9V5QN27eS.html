<html><head><meta charset='utf-8'><title>Pink Noise Is All You Need_ Colored Noise Exploration in Deep Reinforcement Learning</title></head><body><h1>Pink Noise Is All You Need_ Colored Noise Exploration in Deep Reinforcement Learning</h1><h3>By: ['Onno Eberhard', 'Jakob Hollenstein', 'Cristina Pinneri', 'Georg Martius']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12540</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> In off-policy deep reinforcement learning with continuous action spaces, exploration is often implemented by injecting action noise into the action selection process. Popular algorithms based on stochastic policies, such as SAC or MPO, inject white noise by sampling actions from uncorrelated Gaussian distributions. In many tasks, however, white noise does not provide sufficient exploration, and temporally correlated noise is used instead. A common choice is Ornstein-Uhlenbeck (OU) noise, which is closely related to Brownian motion (red noise). Both red noise and white noise belong to the broad family of colored noise. In this work, we perform a comprehensive experimental evaluation on MPO and SAC to explore the effectiveness of other colors of noise as action noise. We find that pink noise, which is halfway between white and red noise, significantly outperforms white noise, OU noise, and other alternatives on a wide range of environments. Thus, we recommend it as the default choice for action noise in continuous control.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=hQ9V5QN27eS</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 2: Probabilistic Methods</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div></body></html>