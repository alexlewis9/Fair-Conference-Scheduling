<html><head><meta charset='utf-8'><title>Encoding Recurrence into Transformers</title></head><body><h1>Encoding Recurrence into Transformers</h1><h3>By: ['Feiqing Huang', 'Kexin Lu', 'Yuxi Cai', 'Zhen Qin', 'Yanwen Fang', 'Guangjian Tian', 'Guodong Li']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12649</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6 Track 3: Deep Learning and representational learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=7YfHla7IxBJ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6 Track 3: Deep Learning and representational learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 5: Deep Learning and representational learning & Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div></body></html>