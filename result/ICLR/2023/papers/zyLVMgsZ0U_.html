<html><head><meta charset='utf-8'><title>Sampling is as easy as learning the score_ theory for diffusion models with minimal data assumptions</title></head><body><h1>Sampling is as easy as learning the score_ theory for diffusion models with minimal data assumptions</h1><h3>By: ['Sitan Chen', 'Sinho Chewi', 'Jerry Li', 'Yuanzhi Li', 'Adil Salim', 'Anru Zhang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12680</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=zyLVMgsZ0U_</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6 Track 1: Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1 Track 5: Reinforcement Learning</div></body></html>