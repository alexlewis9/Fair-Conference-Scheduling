<html><head><meta charset='utf-8'><title>Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent</title></head><body><h1>Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent</h1><h3>By: ['Avrajit Ghosh', 'HE LYU', 'Xitong Zhang', 'Rongrong Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12593</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> It is well known that the finite step-size ($h$) in Gradient descent (GD) implicitly regularizes solutions to flatter minimas. A natural question to ask is \textit{Does the momentum parameter $\beta$ (say) play a role in implicit regularization in Heavy-ball (H.B) momentum accelerated gradient descent (GD+M)?}. To answer this question, first, we show that  the trajectory traced by discrete H.B momentum update (GD+M) is $O(h^2)$ close to a continuous trajectory induced by a modified loss, which consists of an original loss and an implicit regularizer. This implicit regularizer for (GD+M) is indeed stronger than that of (GD) by factor of $(\frac{1+\beta}{1-\beta})$, thus explaining why (GD+M) shows better generalization performance and higher test accuracy than (GD). Furthermore, we extend our analysis to stochastic version of gradient descent with momentum (SGD+M) and propose a deterministic continuous trajectory that is $O(h^2)$ close to the discrete update of (SGD+M) in a strong approximation sense. We explore the implicit regularization in (SGD+M) and (GD+M) through a series of experiments validating our theory.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5 Track 5: Deep Learning and representational learning & Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=ZzdBhtEH9yB</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5 Track 5: Deep Learning and representational learning & Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 4: Applications & Social Aspects of Machine Learning & General Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 1: Unsupervised and Self-supervised learning & Social Aspects of Machine Learning-</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3 Track 2: Deep Learning and representational learning</div></body></html>