<html><head><meta charset='utf-8'><title>The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry</title></head><body><h1>The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry</h1><h3>By: ['Dian Wang', 'Jung Yeon Park', 'Neel Sortur', 'Lawson Wong', 'Robin Walters', 'Robert Platt']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12585</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=P4MUGRM4Acu</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 6: Deep Learning and representational learning- Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 2: Probabilistic Methods</div></body></html>