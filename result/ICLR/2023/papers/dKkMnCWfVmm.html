<html><head><meta charset='utf-8'><title>Multi-Objective Online Learning</title></head><body><h1>Multi-Objective Online Learning</h1><h3>By: ['Jiyan Jiang', 'Wenpeng Zhang', 'Shiji Zhou', 'Lihong Gu', 'Xiaodong Zeng', 'Wenwu Zhu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12729</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper presents a systematic study of multi-objective online learning. We first formulate the framework of Multi-Objective Online Convex Optimization, which encompasses a novel multi-objective regret. This regret is built upon a sequence-wise extension of the commonly used discrepancy metric Pareto suboptimality gap in zero-order multi-objective bandits. We then derive an equivalent form of the regret, making it amenable to be optimized via first-order iterative methods. To motivate the algorithm design, we give an explicit example in which equipping OMD with the vanilla min-norm solver for gradient composition will incur a linear regret, which shows that merely regularizing the iterates, as in single-objective online learning, is not enough to guarantee sublinear regrets in the multi-objective setting. To resolve this issue, we propose a novel min-regularized-norm solver that regularizes the composite weights. Combining min-regularized-norm with OMD results in the Doubly Regularized Online Mirror Multiple Descent algorithm. We further derive the multi-objective regret bound for the proposed algorithm, which matches the optimal bound in the single-objective setting. Extensive experiments on real-world datasets verify the effectiveness of the proposed algorithm.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=dKkMnCWfVmm</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 5 Track 1: Unsupervised and Self-supervised learning & Social Aspects of Machine Learning-</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div></body></html>