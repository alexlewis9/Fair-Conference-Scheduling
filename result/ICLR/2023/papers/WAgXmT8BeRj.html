<html><head><meta charset='utf-8'><title>MARS_ Meta-learning as Score Matching in the Function Space</title></head><body><h1>MARS_ Meta-learning as Score Matching in the Function Space</h1><h3>By: ['Kruno Lehman', 'Jonas Rothfuss', 'Andreas Krause']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12709</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score function of the data-generating process marginals instead of parameter space priors. In a comprehensive benchmark, we demonstrate that our method achieves state-of-the-art performance in terms of predictive accuracy and substantial improvements in the quality of uncertainty estimates.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=WAgXmT8BeRj</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4 Track 5: Machine Learning for Sciences & Probabilistic Methods</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 2: General Machine Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 2: General Machine Learning</div></body></html>