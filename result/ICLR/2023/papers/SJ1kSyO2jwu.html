<html><head><meta charset='utf-8'><title>Human Motion Diffusion Model</title></head><body><h1>Human Motion Diffusion Model</h1><h3>By: ['Guy Tevet', 'Sigal Raab', 'Brian Gordon', 'Yonatan Shafir', 'Daniel Cohen-Or', 'Amit Bermano']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12654</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Natural and expressive human motion generation is the holy grail of computer animation.It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models are promising candidates for the human motion domain since theyhave already shown remarkable generative capabilities in other domains, and their many-to-many nature. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data.  MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=SJ1kSyO2jwu</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 3: Generative models</div></body></html>