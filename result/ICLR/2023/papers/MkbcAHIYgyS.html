<html><head><meta charset='utf-8'><title>Mass-Editing Memory in a Transformer</title></head><body><h1>Mass-Editing Memory in a Transformer</h1><h3>By: ['Kevin Meng', 'Arnab Sen Sharma', 'Alex J Andonian', 'Yonatan Belinkov', 'David Bau']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12726</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=MkbcAHIYgyS</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 1: Applications</div></body></html>