<html><head><meta charset='utf-8'><title>Learning multi-scale local conditional probability models of images</title></head><body><h1>Learning multi-scale local conditional probability models of images</h1><h3>By: ['Zahra Kadkhodaie', 'Florentin Guth', 'St√©phane Mallat', 'Eero Simoncelli']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12697</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Deep neural networks can learn powerful prior probability models for images, as evidenced by the high-quality generations obtained with recent score-based diffusion methods. But the means by which these networks capture complex global statistical structure, apparently without suffering from the curse of dimensionality, remain a mystery. To study this, we incorporate diffusion methods into a multi-scale decomposition, reducing dimensionality by assuming a stationary local Markov model for wavelet coefficients conditioned on coarser-scale coefficients. We instantiate this model using convolutional neural networks (CNNs) with local receptive fields, which enforce both the stationarity and Markov properties. Global structures are captured using a CNN with receptive fields covering the entire (but small) low-pass image. We test this model on a dataset of face images, which are highly non-stationary and contain large-scale geometric structures.Remarkably, denoising, super-resolution, and image synthesis results all demonstrate that these structures can be captured with significantly smaller conditioning neighborhoods than required by a Markov model implemented in the pixel domain. Our results show that score estimation for large complex images can be reduced to low-dimensional Markov conditional models across scales,  alleviating the curse of dimensionality.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3 Track 3: Generative models</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=VZX2I_VVJKH</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3 Track 3: Generative models</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 4: Applications & Social Aspects of Machine Learning & General Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3 Track 3: Generative models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 5: Generative models & Theory</div></body></html>