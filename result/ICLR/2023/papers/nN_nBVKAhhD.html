<html><head><meta charset='utf-8'><title>Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform</title></head><body><h1>Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform</h1><h3>By: ['Yubei Chen', 'Zeyu Yun', 'Yi Ma', 'Bruno Olshausen', 'Yann LeCun']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12555</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We describe a minimalistic and interpretable method for unsupervised representation learning that does not require data augmentation, hyperparameter tuning, or other engineering designs, but nonetheless achieves performance close to the state-of-the-art (SOTA) SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic (one training epoch) sparse manifold transform, it is possible to achieve $99.3\%$ KNN top-1 accuracy on MNIST, $81.1\%$ KNN top-1 accuracy on CIFAR-10, and $53.2\%$ on CIFAR-100. With simple gray-scale augmentation, the model achieves $83.2\%$ KNN top-1 accuracy on CIFAR-10 and $57\%$ on CIFAR-100. These results significantly close the gap between simplistic ``white-box'' methods and SOTA methods. We also provide visualization to illustrate how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though a small performance gap remains between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised representation learning, which has potential to significantly improve learning efficiency.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 1: Unsupervised and Self-supervised learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=nN_nBVKAhhD</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 1: Unsupervised and Self-supervised learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3 Track 4: General Machine Learning & Unsupervised and Self-supervised learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3 Track 4: General Machine Learning & Unsupervised and Self-supervised learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3 Track 4: General Machine Learning & Unsupervised and Self-supervised learning</div></body></html>