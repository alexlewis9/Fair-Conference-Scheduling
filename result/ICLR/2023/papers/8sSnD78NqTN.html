<html><head><meta charset='utf-8'><title>Learning Soft Constraints From Constrained Expert Demonstrations</title></head><body><h1>Learning Soft Constraints From Constrained Expert Demonstrations</h1><h3>By: ['Ashish Gaurav', 'Kasra Rezaee', 'Guiliang Liu', 'Pascal Poupart']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12699</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving scenarios.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1 Track 5: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=8sSnD78NqTN</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1 Track 5: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 2: Optimization</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div></body></html>