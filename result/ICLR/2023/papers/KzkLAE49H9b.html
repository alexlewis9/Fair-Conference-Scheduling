<html><head><meta charset='utf-8'><title>Training language models to summarize narratives improves brain alignment</title></head><body><h1>Training language models to summarize narratives improves brain alignment</h1><h3>By: ['Khai Loong Aw', 'Mariya Toneva']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12698</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing (NLP). Towards this goal, recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts. However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task. This work investigates this further by turning to the one language processing system that truly understands complex language: the human brain. We show that training language models for deeper narrative understanding results in richer representations that have improved alignment to human brain activity. We further find that the improvements in brain alignment are larger for character names than for other discourse features, which indicates that these models are learning important narrative elements. Taken together, these results suggest that this type of training can indeed lead to deeper language understanding. These findings have consequences both for cognitive neuroscience by revealing some of the significant factors behind brain-NLP alignment, and for NLP by highlighting that understanding of long-range context can be improved beyond language modeling.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3 Track 5: Deep Learning and representational learning & Neuroscience and Cognitive Science</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=KzkLAE49H9b</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3 Track 5: Deep Learning and representational learning & Neuroscience and Cognitive Science</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 2: Infrastructure & Social Aspects of Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 6: Applications & Social Aspects of Machine Learning</div></body></html>