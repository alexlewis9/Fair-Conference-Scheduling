<html><head><meta charset='utf-8'><title>Choreographer_ Learning and Adapting Skills in Imagination</title></head><body><h1>Choreographer_ Learning and Adapting Skills in Imagination</h1><h3>By: ['Pietro Mazzaglia', 'Tim Verbelen', 'Bart Dhoedt', 'Alexandre Lacoste', 'Sai Rajeswar']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12667</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exploration policy. The skills can be used to effectively adapt to downstream tasks, as we show in the URL benchmark, where we outperform previous approaches from both pixels and states inputs. The skills also explore the environment thoroughly, finding sparse rewards more frequently, as shown in goal-reaching tasks from the DMC Suite and Meta-World. Project website: https://skillchoreographer.github.io/</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=PhkWyijGi5b</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1 Track 5: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4 Track 3: Reinforcement Learning I</div></body></html>