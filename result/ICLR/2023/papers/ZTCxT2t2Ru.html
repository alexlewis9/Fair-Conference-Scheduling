<html><head><meta charset='utf-8'><title>DocPrompting_ Generating Code by Retrieving the Docs</title></head><body><h1>DocPrompting_ Generating Code by Retrieving the Docs</h1><h3>By: ['Shuyan Zhou', 'Uri Alon', 'Frank F Xu', 'Zhengbao Jiang', 'Graham Neubig']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12584</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of codeto keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5 Track 4: Applications & Optimization</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=ZTCxT2t2Ru</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5 Track 4: Applications & Optimization</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 6: Applications & Social Aspects of Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3 Track 2: Deep Learning and representational learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2 Track 6: Applications & Social Aspects of Machine Learning</div></body></html>