<html><head><meta charset='utf-8'><title>Optimal Transport for Offline Imitation Learning</title></head><body><h1>Optimal Transport for Offline Imitation Learning</h1><h3>By: ['Yicheng Luo', 'Zhengyao Jiang', 'samuel cohen', 'Edward Grefenstette', 'Marc Deisenroth']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12580</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> With the advent of large datasets, offline reinforcement learning is a promising framework for learning good decision-making policies without the need to interact with the real environment.However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive.In this paper, we introduce Optimal Transport Relabeling (OTR), an imitation learning algorithm that can automatically relabel offline data of mixed and unknown quality with rewards from a few good demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we demonstrate that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2 Track 5: Generative models & Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=MhuFzFsrfvH</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2 Track 5: Generative models & Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2 Track 2: General Machine Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 4: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3 Track 1: Reinforcement Learning</div></body></html>