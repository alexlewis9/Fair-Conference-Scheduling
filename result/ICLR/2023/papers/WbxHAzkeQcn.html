<html><head><meta charset='utf-8'><title>Neural Networks and the Chomsky Hierarchy</title></head><body><h1>Neural Networks and the Chomsky Hierarchy</h1><h3>By: ['Gregoire Deletang', 'Anian Ruoss', 'Jordi Grau-Moya', 'Tim Genewein', 'Li Kevin Wenliang', 'Elliot Catt', 'Chris Cundy', 'Marcus Hutter', 'Shane Legg', 'Joel Veness', 'Pedro Ortega']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12616</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=WbxHAzkeQcn</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3 Track 2: Deep Learning and representational learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5 Track 5: Deep Learning and representational learning & Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1 Track 1: Deep Learning and representational learning I</div></body></html>