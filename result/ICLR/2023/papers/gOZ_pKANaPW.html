<html><head><meta charset='utf-8'><title>Unsupervised Model Selection for Time Series Anomaly Detection</title></head><body><h1>Unsupervised Model Selection for Time Series Anomaly Detection</h1><h3>By: ['Mononito Goswami', 'Cristian Challu', 'Laurent Callot', 'Lenon Minorics', 'Andrey Kan']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2023/oral/12666</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question \textit{i.e.} Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, \textit{prediction error}, \textit{model centrality}, and \textit{performance on injected synthetic anomalies}, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multiple imperfect surrogate metrics as a robust rank aggregation problem. We then provide theoretical justification behind the proposed approach. Large-scale experiments on multiple real-world datasets demonstrate that our proposed unsupervised approach is as effective as selecting the most accurate model based on partially labeled data.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=gOZ_pKANaPW</div><h4>openreview_url</h4><div style='margin-bottom:1em'>nan</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2 Track 2: General Machine Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6 Track 6: Deep Learning</div></body></html>