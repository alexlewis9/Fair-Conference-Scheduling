<html><head><meta charset='utf-8'><title>Finetuned Language Models are Zero-Shot Learners</title></head><body><h1>Finetuned Language Models are Zero-Shot Learners</h1><h3>By: ['Jason Wei', 'Maarten Bosma', 'Vincent Zhao', 'Kelvin Guu', 'Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew Dai', 'Quoc V Le']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6255</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=gEZrGCozdqR</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=gEZrGCozdqR</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div></body></html>