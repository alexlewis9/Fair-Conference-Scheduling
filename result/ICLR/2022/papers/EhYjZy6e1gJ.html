<html><head><meta charset='utf-8'><title>PiCO_ Contrastive Label Disambiguation for Partial Label Learning</title></head><body><h1>PiCO_ Contrastive Label Disambiguation for Partial Label Learning</h1><h3>By: ['Haobo Wang', 'Ruixuan Xiao', 'Yixuan Li', 'Lei Feng', 'Gang Niu', 'Gang Chen', 'Junbo Zhao']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6039</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=EhYjZy6e1gJ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=EhYjZy6e1gJ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div></body></html>