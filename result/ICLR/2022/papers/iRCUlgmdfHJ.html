<html><head><meta charset='utf-8'><title>DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS</title></head><body><h1>DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS</h1><h3>By: ['Huiqi Deng', 'Qihan Ren', 'Hao Zhang', 'Quanshi Zhang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6623</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=iRCUlgmdfHJ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=iRCUlgmdfHJ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div></body></html>