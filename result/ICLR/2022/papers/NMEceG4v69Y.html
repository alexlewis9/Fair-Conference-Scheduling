<html><head><meta charset='utf-8'><title>CycleMLP_ A MLP-like Architecture for Dense Prediction</title></head><body><h1>CycleMLP_ A MLP-like Architecture for Dense Prediction</h1><h3>By: ['Shoufa Chen', 'Enze Xie', 'Chongjian GE', 'Runjian Chen', 'Ding Liang', 'Ping Luo']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6274</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can copewith various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like modelsâ€™ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2: Structured learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=NMEceG4v69Y</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=NMEceG4v69Y</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2: Structured learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2: Structured learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div></body></html>