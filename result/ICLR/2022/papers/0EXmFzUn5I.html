<html><head><meta charset='utf-8'><title>Pyraformer_ Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting</title></head><body><h1>Pyraformer_ Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting</h1><h3>By: ['Shi Zhan Liu', 'Hang Yu', 'Cong Liao', 'Jianguo Li', 'Weiyao Lin', 'Alex Liu Â·']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6828</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=0EXmFzUn5I</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=0EXmFzUn5I</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2: Structured learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div></body></html>