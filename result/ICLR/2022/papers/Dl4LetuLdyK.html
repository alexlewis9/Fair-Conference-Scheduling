<html><head><meta charset='utf-8'><title>A Fine-Grained Analysis on Distribution Shift</title></head><body><h1>A Fine-Grained Analysis on Distribution Shift</h1><h3>By: ['Olivia Wiles', 'Sven Gowal', 'Florian Stimberg', 'Sylvestre-Alvise Rebuffi', 'Ira Ktena', 'Krishnamurthy Dvijotham', 'Ali Taylan Cemgil']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/7003</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets.  Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. Code is available at github.com/deepmind/distribution shift framework.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Dl4LetuLdyK</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Dl4LetuLdyK</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div></body></html>