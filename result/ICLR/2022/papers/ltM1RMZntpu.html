<html><head><meta charset='utf-8'><title>Weighted Training for Cross-Task Learning</title></head><body><h1>Weighted Training for Cross-Task Learning</h1><h3>By: ['Shuxiao Chen', 'Koby Crammer', 'Hangfeng He', 'Dan Roth', 'Weijie J Su']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/7204</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=ltM1RMZntpu</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=ltM1RMZntpu</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3: Meta-learning and adaptation</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div></body></html>