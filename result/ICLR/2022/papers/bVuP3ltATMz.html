<html><head><meta charset='utf-8'><title>Large Language Models Can Be Strong Differentially Private Learners</title></head><body><h1>Large Language Models Can Be Strong Differentially Private Learners</h1><h3>By: ['Xuechen Li', 'Florian Tramer', 'Percy Liang', 'Tatsunori Hashimoto']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6895</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=bVuP3ltATMz</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=bVuP3ltATMz</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2: Structured learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div></body></html>