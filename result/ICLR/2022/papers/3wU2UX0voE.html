<html><head><meta charset='utf-8'><title>The Information Geometry of Unsupervised Reinforcement Learning</title></head><body><h1>The Information Geometry of Unsupervised Reinforcement Learning</h1><h3>By: ['Benjamin Eysenbach', 'Ruslan Salakhutdinov', 'Sergey Levine']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6207</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=3wU2UX0voE</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=3wU2UX0voE</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div></body></html>