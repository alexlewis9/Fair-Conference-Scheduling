<html><head><meta charset='utf-8'><title>The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks_ an Exact Characterization of Optimal Solutions</title></head><body><h1>The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks_ an Exact Characterization of Optimal Solutions</h1><h3>By: ['Yifei Wang', 'Jonathan Lacotte', 'Mert Pilanci']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/7125</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys.Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Z7Lk2cQEG8a</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Z7Lk2cQEG8a</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2: Understanding Deep Learning</div></body></html>