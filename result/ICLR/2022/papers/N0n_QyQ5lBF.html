<html><head><meta charset='utf-8'><title>Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling</title></head><body><h1>Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling</h1><h3>By: ['Bo Wan', 'Wenjuan Han', 'Zilong Zheng', 'Tinne Tuytelaars']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6111</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=N0n_QyQ5lBF</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=N0n_QyQ5lBF</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4: Sequence modeling</div></body></html>