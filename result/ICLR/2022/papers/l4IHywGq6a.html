<html><head><meta charset='utf-8'><title>Data-Efficient Graph Grammar Learning for Molecular Generation</title></head><body><h1>Data-Efficient Graph Grammar Learning for Molecular Generation</h1><h3>By: ['Minghao Guo', 'Veronika Thost', 'Beichen Li', 'Payel Das', 'Jie Chen', 'Wojciech Matusik']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/7012</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=l4IHywGq6a</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=l4IHywGq6a</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2: AI applications</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4: Probablistic Models, Vision</div></body></html>