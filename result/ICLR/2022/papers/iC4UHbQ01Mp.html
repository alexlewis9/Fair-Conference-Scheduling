<html><head><meta charset='utf-8'><title>Poisoning and Backdooring Contrastive Learning</title></head><body><h1>Poisoning and Backdooring Contrastive Learning</h1><h3>By: ['Nicholas Carlini', 'Andreas Terzis']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6317</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=iC4UHbQ01Mp</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=iC4UHbQ01Mp</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1: AI Applications</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div></body></html>