<html><head><meta charset='utf-8'><title>Transform2Act_ Learning a Transform-and-Control Policy for Efficient Agent Design</title></head><body><h1>Transform2Act_ Learning a Transform-and-Control Policy for Efficient Agent Design</h1><h3>By: ['Ye Yuan', 'Yuda Song', 'Zhengyi Luo', 'Wen Sun', 'Kris Kitani']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6197</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=UcDUxjPYWSr</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=UcDUxjPYWSr</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1: AI Applications</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div></body></html>