<html><head><meta charset='utf-8'><title>Vision-Based Manipulators Need to Also See from Their Hands</title></head><body><h1>Vision-Based Manipulators Need to Also See from Their Hands</h1><h3>By: ['Kyle Hsu', 'Moo Kim', 'Rafael Rafailov', 'Jiajun Wu', 'Chelsea Finn']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>ICLR</div><h4>url</h4><div style='margin-bottom:1em'>https://iclr.cc/virtual/2022/oral/6103</div><h4>year</h4><div style='margin-bottom:1em'>2022</div><h4>abstract</h4><div style='margin-bottom:1em'> We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1: AI Applications</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=RJkAHKp7kNZ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=RJkAHKp7kNZ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1: AI Applications</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1: AI Applications</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1: Learning in the wild,  Reinforcement learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3: Learning from distribution shift</div></body></html>