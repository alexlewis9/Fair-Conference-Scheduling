<html><head><meta charset='utf-8'><title>Bayesian-guided Label Mapping for Visual Reprogramming</title></head><body><h1>Bayesian-guided Label Mapping for Visual Reprogramming</h1><h3>By: ['Chengyi Cai', 'Zesheng Ye', 'Lei Feng', 'Jianzhong Qi', 'Feng Liu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/98002</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained vision models by adapting their input or output interfaces to solve downstream tasks whose labels (i.e., downstream labels) might be totally different from the labels associated with the pretrained models (i.e., pretrained labels). When adapting the output interface, label mapping methods transform the pretrained labels to downstream labels by establishing a gradient-free one-to-one correspondence between the two sets of labels.However, in this paper, we reveal that one-to-one mappings may overlook the complex relationship between pretrained and downstream labels. Motivated by this observation, we propose a B ayesian-guided L abel M apping (BLM) method. BLM constructs an iteratively-updated probabilistic label mapping matrix, with each element quantifying a pairwise relationship between pretrained and downstream labels.The assignment of values to the constructed matrix is guided by Bayesian conditional probability, considering the joint distribution of the downstream labels and the labels predicted by the pretrained model on downstream samples. Experiments conducted on both pretrained vision models (e.g., ResNeXt) and vision-language models (e.g., CLIP) demonstrate the superior performance of BLM over existing label mapping methods. The success of BLM also offers a probabilistic lens through which to understand and analyze the effectiveness of VR.Our code is available at https://github.com/tmlr-group/BayesianLM.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=135eKqDoRR</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=135eKqDoRR</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>tsne_0</h4><div style='margin-bottom:1em'>0.5911841988563538</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-2.3038270473480225</div></body></html>