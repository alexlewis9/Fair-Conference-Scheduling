<html><head><meta charset='utf-8'><title>PV-Tuning_ Beyond Straight-Through Estimation for Extreme LLM Compression</title></head><body><h1>PV-Tuning_ Beyond Straight-Through Estimation for Extreme LLM Compression</h1><h3>By: ['Vladimir Malinovskii', 'Denis Mazur', 'Ivan Ilin', 'Denis Kuznedelev', 'Konstantin Burlachenko', 'Kai Yi', 'Dan Alistarh', 'Peter Richtarik']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97970</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> There has been significant interest in "extreme" compression of large language models (LLMs), i.e. to 1-2 bits per parameter, which allows such models to be executed efficiently on resource-constrained devices.  Existing work focused on improved one-shot quantization techniques and weight representations; yet, purely post-training  approaches are reaching diminishing returns in terms of the accuracy-vs-bit-width trade-off. State-of-the-art quantization methods such as QuIP# and AQLM include fine-tuning (part of) the compressed parameters over a limited amount of calibration data; however, such fine-tuning techniques over compressed weights often make exclusive use of straight-through estimators (STE), whose performance is not well-understood in this setting. In this work, we question the use of STE for extreme LLM compression, showing that it can be sub-optimal, and perform a systematic study of quantization-aware fine-tuning strategies for LLMs.We propose PV-Tuning - a representation-agnostic framework that generalizes and improves upon existing fine-tuning strategies, and provides convergence guarantees in restricted cases.On the practical side, when used for 1-2 bit vector quantization, PV-Tuning outperforms prior techniques for highly-performant models such as Llama and Mistral. Using PV-Tuning, we achieve the first Pareto-optimal quantization for Llama-2 family models at  2 bits per parameter.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 3C: Natural Language Processing</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=YvA8UF0I37</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=YvA8UF0I37</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 3C: Natural Language Processing</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 3C: Natural Language Processing</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>tsne_0</h4><div style='margin-bottom:1em'>0.4640346169471741</div><h4>tsne_1</h4><div style='margin-bottom:1em'>0.4053511917591095</div></body></html>