<html><head><meta charset='utf-8'><title>OpenMathInstruct-1_ A 1.8 Million Math Instruction Tuning Dataset</title></head><body><h1>OpenMathInstruct-1_ A 1.8 Million Math Instruction Tuning Dataset</h1><h3>By: ['Shubham Toshniwal', 'Ivan Moshkov', 'Sean Narenthiran', 'Daria Gitman', 'Fei Jia', 'Igor Gitman']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/98022</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We will release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://arxiv.org/pdf/2402.10176</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Mbd3QxXjq5</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 3A: Generative Models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 5D: Machine Learning and Science</div></body></html>