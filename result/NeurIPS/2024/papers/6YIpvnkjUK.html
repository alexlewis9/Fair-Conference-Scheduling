<html><head><meta charset='utf-8'><title>The Sample-Communication Complexity Trade-off in Federated Q-Learning</title></head><body><h1>The Sample-Communication Complexity Trade-off in Federated Q-Learning</h1><h3>By: ['Sudeep Salgia', 'Yuejie Chi']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97994</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> We consider the problem of Federated Q-learning, where $M$ agents aim to collaboratively learn the optimal Q-function of an unknown infinite horizon Markov Decision Process with finite state and action spaces. We investigate the trade-off between sample and communication complexity for the widely used class of intermittent communication algorithms. We first establish the converse result, where we show that any Federated Q-learning that offers a linear speedup with respect to number of agents in sample complexity needs to incur a communication cost of at least $\Omega(\frac{1}{1-\gamma})$, where $\gamma$ is the discount factor. We also propose a new Federated Q-learning algorithm, called Fed-DVR-Q, which is the first Federated Q-learning algorithm to simultaneously achieve order-optimal sample and communication complexities. Thus, together these results provide a complete characterization of the sample-communication complexity trade-off in Federated Q-learning.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=6YIpvnkjUK</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=6YIpvnkjUK</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div></body></html>