<html><head><meta charset='utf-8'><title>DuQuant_ Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</title></head><body><h1>DuQuant_ Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs</h1><h3>By: ['Haokun Lin', 'Haobo Xu', 'Yichen WU', 'Jingzhi Cui', 'Yingtao Zhang', 'Linzhan Mou', 'Linqi Song', 'Zhenan Sun', 'Ying Wei']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97956</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 3D: Natural Language Processing</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=mp8u2Pcmqz</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=mp8u2Pcmqz</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 3D: Natural Language Processing</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 3C: Natural Language Processing</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div></body></html>