<html><head><meta charset='utf-8'><title>The Road Less Scheduled</title></head><body><h1>The Road Less Scheduled</h1><h3>By: ['Aaron Defazio', 'Xingyu Yang', 'Ahmed Khaled', 'Konstantin Mishchenko', 'Harsh Mehta', 'Ashok Cutkosky']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/98003</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Existing learning rate schedules that do not require specification of the optimization stopping step $T$ are greatly out-performed by learning rate schedules that depend on $T$. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available at https://github.com/facebookresearch/schedule_free. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=0XeNkkENuI</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=0XeNkkENuI</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6C: New Data</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>tsne_0</h4><div style='margin-bottom:1em'>1.3785514831542969</div><h4>tsne_1</h4><div style='margin-bottom:1em'>2.2786338329315186</div></body></html>