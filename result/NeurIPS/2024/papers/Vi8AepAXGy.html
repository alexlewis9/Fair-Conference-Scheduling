<html><head><meta charset='utf-8'><title>Cambrian-1_ A Fully Open, Vision-Centric Exploration of Multimodal LLMs</title></head><body><h1>Cambrian-1_ A Fully Open, Vision-Centric Exploration of Multimodal LLMs</h1><h3>By: ['Peter Tong', 'Ellis Brown', 'Penghao Wu', 'Sanghyun Woo', 'Adithya Jairam Vedagiri IYER', 'Sai Charitha Akula', 'Shusheng Yang', 'Jihan Yang', 'Manoj Middepogu', 'Ziteng Wang', 'Xichen Pan', 'Rob Fergus', 'Yann LeCun', 'Saining Xie']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97972</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures—self-supervised, strongly supervised, or combinations thereof—based on experiments with over 15 vision models. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks. To further improve visual grounding, we propose spatial vision aggregator (SVA), a dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of distribution balancing. Collectively, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 5C: Machine Vision</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Vi8AepAXGy</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Vi8AepAXGy</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 5C: Machine Vision</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 4B: Diffusion-based Models</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>tsne_0</h4><div style='margin-bottom:1em'>-0.19546853005886078</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-3.0151710510253906</div></body></html>