<html><head><meta charset='utf-8'><title>Scale Equivariant Graph Metanetworks</title></head><body><h1>Scale Equivariant Graph Metanetworks</h1><h3>By: ['Ioannis Kalogeropoulos', 'Giorgos Bouritsas', 'Yannis Panagakis']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97993</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> This paper pertains to an emerging machine learning paradigm: learning higher- order functions, i.e. functions whose inputs are functions themselves, particularly when these inputs are Neural Networks (NNs). With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure ofNNs. However, are these the sole symmetries present in NN parameterizations? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as scaling symmetries, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 5A: Graph Neural Networks</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=8Fxqn1tZM1</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=8Fxqn1tZM1</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 5A: Graph Neural Networks</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 5A: Graph Neural Networks</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 3A: Generative Models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 5C: Machine Vision</div><h4>tsne_0</h4><div style='margin-bottom:1em'>3.377854824066162</div><h4>tsne_1</h4><div style='margin-bottom:1em'>0.13267813622951508</div></body></html>