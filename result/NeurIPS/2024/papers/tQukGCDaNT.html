<html><head><meta charset='utf-8'><title>Improved Distribution Matching Distillation for Fast Image Synthesis</title></head><body><h1>Improved Distribution Matching Distillation for Fast Image Synthesis</h1><h3>By: ['Tianwei Yin', 'Michaël Gharbi', 'Taesung Park', 'Richard Zhang', 'Eli Shechtman', 'Fredo Durand', 'Bill Freeman']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97949</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent approaches have shown promises distilling expensive diffusion models into efficient one-step generators.Amongst them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, i.e., the distillation process does not enforce a one-to-one correspondence with the sampling trajectories of their teachers.However, to ensure stable training in practice, DMD requires an additional regression loss computed using a large set of noise--image pairs, generated by the teacher with many steps of a deterministic sampler.This is not only computationally expensive for large-scale text-to-image synthesis, but it also limits the student's quality, tying it too closely to the teacher's original sampling paths.We introduce DMD2, a set of techniques that lift this limitation and improve DMD training.First, we eliminate the regression loss and the need for expensive dataset construction.We show that the resulting instability is due to the "fake" critic not estimating the distribution of generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy.Second, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images.This lets us train the student model on real data, thus mitigating the imperfect "real" score estimation from the teacher model, and thereby enhancing quality.Third, we introduce a new training procedure that enables multi-step sampling in the student, andaddresses the training--inference input mismatch of previous work, by simulating inference-time generator samples during training. Taken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost.Further, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods, and surpassing the teacher. We release our code and pretrained models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=tQukGCDaNT</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=tQukGCDaNT</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 3D: Natural Language Processing</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 3D: Natural Language Processing</div></body></html>