<html><head><meta charset='utf-8'><title>Enhancing Preference-based Linear Bandits via Human Response Time</title></head><body><h1>Enhancing Preference-based Linear Bandits via Human Response Time</h1><h3>By: ['Shen Li', 'Yuyang Zhang', 'Zhaolin Ren', 'Claire Liang', 'Na Li', 'Julie A Shah']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97969</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Interactive preference learning systems infer human preferences by presenting queries as pairs of options and collecting binary choices. Although binary choices are simple and widely used, they provide limited information about preference strength. To address this, we leverage human response times, which are inversely related to preference strength, as an additional signal. We propose a computationally efficient method that combines choices and response times to estimate human utility functions, grounded in the EZ diffusion model from psychology. Theoretical and empirical analyses show that for queries with strong preferences, response times complement choices by providing extra information about preference strength, leading to significantly improved utility estimation. We incorporate this estimator into preference-based linear bandits for fixed-budget best-arm identification. Simulations on three real-world datasets demonstrate that using response times significantly accelerates preference learning compared to choice-only approaches. Additional materials, such as code, slides, and talk video, are available at https://shenlirobot.github.io/pages/NeurIPS24.html.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2A: Agents</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=aIPwlkdOut</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=aIPwlkdOut</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2A: Agents</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1B: Human-AI Interaction</div></body></html>