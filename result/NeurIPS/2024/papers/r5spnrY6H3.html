<html><head><meta charset='utf-8'><title>RG-SAN_ Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation</title></head><body><h1>RG-SAN_ Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation</h1><h3>By: ['Changli Wu', 'qi chen', 'Jiayi Ji', 'Haowei Wang', 'Yiwei Ma', 'You Huang', 'Gen Luo', 'Hao Fei', 'Xiaoshuai Sun', 'Rongrong Ji']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97951</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> 3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by correlating referring expressions with point clouds. However, traditional approaches frequently encounter issues like over-segmentation or mis-segmentation, due to insufficient emphasis on spatial information of instances. In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision. This approach enables the network to accurately depict the spatial relationships among all entities described in the text, thus enhancing the reasoning capabilities. The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM initially locates all mentioned instances and iteratively refines their positional information. The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instanceâ€™s positioning. Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity. All codes are available at https://github.com/sosppxo/RG-SAN.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 1B: Human-AI Interaction</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=r5spnrY6H3</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=r5spnrY6H3</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 1B: Human-AI Interaction</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6C: New Data</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 4B: Diffusion-based Models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4B: Diffusion-based Models</div></body></html>