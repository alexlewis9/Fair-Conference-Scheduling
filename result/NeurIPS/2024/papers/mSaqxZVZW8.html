<html><head><meta charset='utf-8'><title>SeeA__ Efficient Exploration-Enhanced A_ Search by Selective Sampling</title></head><body><h1>SeeA__ Efficient Exploration-Enhanced A_ Search by Selective Sampling</h1><h3>By: ['Dengwei Zhao', 'Shikui Tu', 'Lei Xu']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97957</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Monte-Carlo tree search (MCTS) and reinforcement learning contributed crucially to the success of AlphaGo and AlphaZero, and A$^*$ is a tree search algorithm among the most well-known ones in the classical AI literature. MCTS and  A$^*$ both perform heuristic search and are mutually beneficial. Efforts have been made to the renaissance of A$^*$ from three possible aspects, two of which have been confirmed by studies in recent years, while the third is about the OPEN list that consists of open nodes of A$^*$ search, but still lacks deep investigation. This paper aims at the third, i.e., developing the Sampling-exploration enhanced A$^*$ (SeeA$^*$) search by constructing a dynamic subset of OPEN through a selective sampling process, such that the node with the best heuristic value in this subset instead of in the OPEN is expanded. Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables SeeA$^*$ to explore other promising branches. Three sampling techniques are presented for comparative investigations. Moreover, under the assumption about the distribution of prediction errors, we have theoretically shown the superior efficiency of SeeA$^*$ over A$^*$ search, particularly when the accuracy of the guiding heuristic function is insufficient. Experimental results on retrosynthetic planning in organic chemistry, logic synthesis in integrated circuit design, and the classical Sokoban game empirically demonstrate the efficiency of SeeA$^*$, in comparison with the state-of-the-art heuristic search algorithms.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=mSaqxZVZW8</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=mSaqxZVZW8</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>tsne_0</h4><div style='margin-bottom:1em'>1.522563099861145</div><h4>tsne_1</h4><div style='margin-bottom:1em'>0.9703575372695923</div></body></html>