<html><head><meta charset='utf-8'><title>MetaLA_ Unified Optimal Linear Approximation to Softmax Attention Map</title></head><body><h1>MetaLA_ Unified Optimal Linear Approximation to Softmax Attention Map</h1><h3>By: ['YUHONG CHOU', 'Man Yao', 'Kexin Wang', 'Yuqi Pan', 'Rui-Jie Zhu', 'Jibin Wu', 'Yiran Zhong', 'Yu Qiao', 'Bo Xu', 'Guoqi Li']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97971</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: (1) Dynamic memory ability; (2) Static approximation ability; (3) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Y8YVCOMEpz</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Y8YVCOMEpz</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>tsne_0</h4><div style='margin-bottom:1em'>0.4856506884098053</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-0.8769094347953796</div></body></html>