<html><head><meta charset='utf-8'><title>Optimal Parallelization of Boosting</title></head><body><h1>Optimal Parallelization of Boosting</h1><h3>By: ['Arthur da Cunha', 'Mikael Møller Høgsgaard', 'Kasper Green Larsen']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97950</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$.These works have also presented highly non-trivial parallel algorithms that shed light on different regions of this tradeoff.Despite these advancements, a significant gap persists between the theoretical lower bounds and the performance of these algorithms across much of the tradeoff space.In this work, we essentially close this gap by providing both improved lower bounds on the parallel complexity of weak-to-strong learners, and a parallel Boosting algorithm whose performance matches these bounds across the entire $p$ vs. $t$ compromise spectrum, up to logarithmic factors.Ultimately, this work settles the parallel complexity of Boosting algorithms that are nearly sample-optimal.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=rtz4df9IF1</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=rtz4df9IF1</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1B: Human-AI Interaction</div><h4>tsne_0</h4><div style='margin-bottom:1em'>2.8204751014709473</div><h4>tsne_1</h4><div style='margin-bottom:1em'>2.7330546379089355</div></body></html>