<html><head><meta charset='utf-8'><title>Statistical Efficiency of Distributional Temporal Difference Learning</title></head><body><h1>Statistical Efficiency of Distributional Temporal Difference Learning</h1><h3>By: ['Yang Peng', 'Liangyu Zhang', 'Zhihua Zhang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97962</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Distributional reinforcement learning (DRL) has achieved empirical success in various domains.One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$.The distributional temporal difference learning has been accordingly proposed, whichis an extension of the temporal difference learning (TD) in the classic RL area.In the tabular case,  Rowland et al. [2018] and Rowland et al. [2023] proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (QTD), respectively.In this paper, we go a step further and analyze the finite-sample performance of distributional TD.To facilitate theoretical analysis, we propose a non-parametric distributional TD learning (NTD).For a $\gamma$-discounted infinite-horizon tabular Markov decision process,we show that for NTD we need $\widetilde O\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)$ iterations to achieve an $\varepsilon$-optimal estimator with high probability, when the estimation error is measured by the $p$-Wasserstein distance.This sample complexity bound is minimax optimal (up to logarithmic factors) in the case of the $1$-Wasserstein distance.To achieve this, we establish a novel Freedman's inequality in Hilbert spaces, which would be of independent interest.In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the $p$-Wasserstein distance.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=eWUM5hRYgH</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=eWUM5hRYgH</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div></body></html>