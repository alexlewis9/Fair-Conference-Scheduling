<html><head><meta charset='utf-8'><title>Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</title></head><body><h1>Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures</h1><h3>By: ['Xin Chen', 'Anderson Ye Zhang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97961</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> We study clustering under anisotropic Gaussian Mixture Models (GMMs), where covariance matrices from different clusters are unknown and are not necessarily the identity matrix. We analyze two anisotropic scenarios: homogeneous, with identical covariance matrices, and heterogeneous, with distinct matrices per cluster. For these models, we derive minimax lower bounds that illustrate the critical influence of covariance structures on clustering accuracy. To solve the clustering problem, we consider a variant of Lloyd's algorithm, adapted to estimate and utilize covariance information iteratively. We prove that the adjusted algorithm not only achieves the minimax optimality but also converges within a logarithmic number of iterations, thus bridging the gap between theoretical guarantees and practical efficiency.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=ge8GZn8Gtu</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=ge8GZn8Gtu</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 5C: Machine Vision</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 5C: Machine Vision</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1D: Learning Theory</div></body></html>