<html><head><meta charset='utf-8'><title>Reinforcement Learning Under Latent Dynamics_ Toward Statistical and Algorithmic Modularity</title></head><body><h1>Reinforcement Learning Under Latent Dynamics_ Toward Statistical and Algorithmic Modularity</h1><h3>By: ['Philip Amortila', 'Dylan J Foster', 'Nan Jiang', 'Akshay Krishnamurthy', 'Zak Mhammedi']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97952</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Real-world applications of reinforcement learning often involve  environments where agents operate on complex, high-dimensional observations, but the underlying (``latent'')  dynamics are comparatively simple. However, beyond restrictive settings  such as tabular latent dynamics,  the fundamental statistical requirements and algorithmic principles for reinforcement learning under latent dynamics are poorly  understood.  This paper addresses the question of reinforcement learning under general latent dynamics from a  statistical and algorithmic perspective.  On the statistical side, our main negativeresult shows that most well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying latent pushforward coverability as ageneral condition that enables statistical tractability. Algorithmically, we develop provably efficient observable-to-latent reductions ---that is, reductions that transform an arbitrary algorithm for the  latent MDP into an algorithm that can operate on rich observations--- in two settings: one where the agent has access to hindsightobservations of the latent dynamics (Lee et al., 2023) and onewhere the agent can estimate self-predictive latent models (Schwarzer et al., 2020). Together, our results serve as a  first step toward a unified statistical and algorithmic theory forreinforcement learning under latent dynamics.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=qf2uZAdy1N</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=qf2uZAdy1N</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 1C: Optimization and Learning Theory</div></body></html>