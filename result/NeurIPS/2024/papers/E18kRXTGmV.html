<html><head><meta charset='utf-8'><title>CVQA_ Culturally-diverse Multilingual Visual Question Answering Benchmark</title></head><body><h1>CVQA_ Culturally-diverse Multilingual Visual Question Answering Benchmark</h1><h3>By: ['David Romero', 'Chenyang Lyu', 'Haryo Wibowo', 'Santiago Góngora', 'Aishik Mandal', 'Sukannya Purkayastha', 'Jesus-German Ortiz-Barajas', 'Emilio Cueva', 'Jinheon Baek', 'Soyeong Jeong', 'Injy Hamed', 'Yong Zheng-Xin', 'Zheng Wei Lim', 'Paula Silva', 'Jocelyn Dunstan', 'Mélanie Jouitteau', 'David LE MEUR', 'Joan Nwatu', 'Ganzorig Batnasan', 'Munkh-Erdene Otgonbold', 'Munkhjargal Gochoo', 'Guido Ivetta', 'Luciana Benotti', 'Laura Alonso Alemany', 'Hernán Maina', 'Jiahui Geng', 'Tiago Timponi Torrent', 'Frederico Belcavello', 'Marcelo Viridiano', 'Jan Christian Blaise Cruz', 'Dan John Velasco', 'Oana Ignat', 'Zara Burzo', 'Chenxi Whitehouse', 'Artem Abzaliev', 'Teresa Clifford', 'Gráinne Caulfield', 'Teresa Lynn', 'Christian Salamea-Palacios', 'Vladimir Araujo', 'Yova Kementchedjhieva', 'Mihail Mihaylov', 'Israel Azime', 'Henok Ademtew', 'Bontu Balcha', 'Naome A. Etori', 'David Adelani', 'Rada Mihalcea', 'Atnafu Lambebo Tonja', 'Maria Cabrera', 'Gisela Vallejo', 'Holy Lovenia', 'Ruochen Zhang', 'Marcos Estecha-Garitagoitia', 'Mario Rodríguez-Cantelar', 'Toqeer Ehsan', 'Rendi Chevi', 'Muhammad Adilazuarda', 'Ryandito Diandaru', 'Samuel Cahyawijaya', 'Fajri Koto', 'Tatsuki Kuribayashi', 'Haiyue Song', 'Aditya Khandavally', 'Thanmay Jayakumar', 'Raj Dabre', 'Mohamed Imam', 'Kumaranage Nagasinghe', 'Alina Dragonetti', 'Luis Fernando D&#x27;Haro', 'Niyomugisha Olivier', 'Jay Gala', 'Pranjal Chitale', 'Fauzan Farooqui', 'Thamar Solorio', 'Alham Aji']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/98024</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Visual Question Answering~(VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://arxiv.org/pdf/2406.05967</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=E18kRXTGmV</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 6B: Safety, New Data</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>tsne_0</h4><div style='margin-bottom:1em'>-0.7408440113067627</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-3.980191469192505</div></body></html>