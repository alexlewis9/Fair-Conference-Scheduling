<html><head><meta charset='utf-8'><title>LINGOLY_ A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages</title></head><body><h1>LINGOLY_ A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages</h1><h3>By: ['Andrew M. Bean', 'Simi Hellsten', 'Harry Mayne', 'Jabez Magomere', 'Ethan Chi', 'Ryan Chi', 'Scott Hale', 'Hannah Rose Kirk']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/98020</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://arxiv.org/pdf/2406.06196</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=cLga8GStdk</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 4A: Natural Language Processing</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2A: Agents</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2A: Agents</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>tsne_0</h4><div style='margin-bottom:1em'>-2.5379843711853027</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-1.6981960535049438</div></body></html>