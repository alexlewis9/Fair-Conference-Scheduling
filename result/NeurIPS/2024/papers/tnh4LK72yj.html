<html><head><meta charset='utf-8'><title>Get Rid of Isolation_ A Continuous Multi-task Spatio-Temporal Learning Framework</title></head><body><h1>Get Rid of Isolation_ A Continuous Multi-task Spatio-Temporal Learning Framework</h1><h3>By: ['Zhongchao Yi', 'Zhengyang Zhou', 'Qihe Huang', 'Yanjiang Chen', 'Liheng Yu', 'Xu Wang', 'Yang Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97948</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower  collective urban intelligence, which  reforms the urban spatiotemporal learning from single-domain  to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as  self-interactions within spatial and temporal aspects  to be  exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAda) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks  by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at https://github.com/DILab-USTCSZ/CMuST.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 5D: Machine Learning and Science</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=tnh4LK72yj</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=tnh4LK72yj</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 5D: Machine Learning and Science</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 5D: Machine Learning and Science</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 4B: Diffusion-based Models</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div></body></html>