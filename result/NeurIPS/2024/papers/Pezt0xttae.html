<html><head><meta charset='utf-8'><title>DapperFL_ Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</title></head><body><h1>DapperFL_ Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices</h1><h3>By: ['Yongzhe Jia', 'Xuyun Zhang', 'Hongsheng Hu', 'Kim-Kwang Raymond Choo', 'Lianyong Qi', 'Xiaolong Xu', 'Amin Beheshti', 'Wanchun Dou']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97981</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> Federated learning (FL) has emerged as a prominent machine learning paradigm in edge computing environments, enabling edge devices to collaboratively optimize a global model without sharing their private data. However, existing FL frameworks suffer from efficacy deterioration due to the system heterogeneity inherent in edge computing, especially in the presence of domain shifts across local data. In this paper, we propose a heterogeneous FL framework DapperFL, to enhance model performance across multiple domains. In DapperFL, we introduce a dedicated Model Fusion Pruning (MFP) module to produce personalized compact local models for clients to address the system heterogeneity challenges. The MFP module prunes local models with fused knowledge obtained from both local and remaining domains, ensuring robustness to domain shifts. Additionally, we design a Domain Adaptive Regularization (DAR) module to further improve the overall performance of DapperFL. The DAR module employs regularization generated by the pruned model, aiming to learn robust representations across domains. Furthermore, we introduce a specific aggregation algorithm for aggregating heterogeneous local models with tailored architectures and weights. We implement DapperFL on a real-world FL platform with heterogeneous clients. Experimental results on benchmark datasets with multiple domains demonstrate that DapperFL outperforms several state-of-the-art FL frameworks by up to 2.28%, while significantly achieving model volume reductions ranging from 20% to 80%. Our code is available at: https://github.com/jyzgh/DapperFL.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Pezt0xttae</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Pezt0xttae</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 6D: Deep Learning Architecture, Infrastructure</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 3A: Generative Models</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 4C: Diffusion-based Models, Mathematics</div><h4>tsne_0</h4><div style='margin-bottom:1em'>1.8636860847473145</div><h4>tsne_1</h4><div style='margin-bottom:1em'>-1.288759708404541</div></body></html>