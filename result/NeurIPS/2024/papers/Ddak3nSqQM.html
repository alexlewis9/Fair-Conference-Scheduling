<html><head><meta charset='utf-8'><title>Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</title></head><body><h1>Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</h1><h3>By: ['Xiong-Hui Chen', 'Ziyan Wang', 'Yali Du', 'Shengyi Jiang', 'Meng Fang', 'Yang Yu', 'Jun Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2024/oral/97989</div><h4>year</h4><div style='margin-bottom:1em'>2024</div><h4>abstract</h4><div style='margin-bottom:1em'> When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMsâ€™ systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network.  We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a 37% while using GPT-based agent can only achieve a 6\% winning rate. The project page: https://plfb-football.github.io.</div><h4>session</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Ddak3nSqQM</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Ddak3nSqQM</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral Session 2C: Reinforcement Learning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral Session 2B: Reinforcement Learning</div></body></html>