<html><head><meta charset='utf-8'><title>Entropic Neural Optimal Transport via Diffusion Processes</title></head><body><h1>Entropic Neural Optimal Transport via Diffusion Processes</h1><h3>By: ['Nikita Gushchin', 'Alexander Kolesov', 'Alexander Korotin', 'Dmitry Vetrov', 'Evgeny Burnaev']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73836</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr√∂dinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks. The code for the ENOT solver can be found at https://github.com/ngushchin/EntropicNeuralOptimalTransport</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3C Diffusion Models</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=fHyLsfMDIs</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=fHyLsfMDIs</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3C Diffusion Models</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3C Diffusion Models</div></body></html>