<html><head><meta charset='utf-8'><title>Monarch Mixer_ A Simple Sub-Quadratic GEMM-Based Architecture</title></head><body><h1>Monarch Mixer_ A Simple Sub-Quadratic GEMM-Based Architecture</h1><h3>By: ['Dan Fu', 'Simran Arora', 'Jessica Grogan', 'Isys Johnson', 'Evan Sabri Eyuboglu', 'Armin Thomas', 'Benjamin Spector', 'Michael Poli', 'Atri Rudra', 'Christopher Ré']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73841</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE—showing for the first time that it may be possible to match Transformer quality without attention or MLPs.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=cB0BImqSS9</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=cB0BImqSS9</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2C Causality</div></body></html>