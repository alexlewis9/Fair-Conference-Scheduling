<html><head><meta charset='utf-8'><title>A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization</title></head><body><h1>A Single-Loop Accelerated Extra-Gradient Difference Algorithm with Improved Complexity Bounds for Constrained Minimax Optimization</h1><h3>By: ['Yuanyuan Liu', 'Fanhua Shang', 'Weixin An', 'Junhao Liu', 'Hongying Liu', 'Zhouchen Lin']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73815</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> In this paper, we propose a novel extra-gradient difference acceleration algorithm for solving constrained nonconvex-nonconcave (NC-NC) minimax problems. In particular, we design a new extra-gradient difference step to obtain an important quasi-cocoercivity property, which plays a key role to significantly improve the convergence rate in the constrained NC-NC setting without additional structural assumption. Then momentum acceleration is also introduced into our dual accelerating update step. Moreover, we prove that, to find an $\epsilon$-stationary point of the function $f$, our algorithm attains the complexity $\mathcal{O}(\epsilon^{-2})$ in the constrained NC-NC setting, while the best-known complexity bound is $\widetilde{\mathcal{O}}(\epsilon^{-4})$, where $\widetilde{\mathcal{O}}(\cdot)$ hides logarithmic factors compared to $\mathcal{O}(\cdot)$. As the special cases of the constrained NC-NC setting, our algorithm can also obtain the same complexity $\mathcal{O}(\epsilon^{-2})$ for both the nonconvex-concave (NC-C) and convex-nonconcave (C-NC) cases, while the best-known complexity bounds are $\widetilde{\mathcal{O}}(\epsilon^{-2.5})$ for the NC-C case and $\widetilde{\mathcal{O}}(\epsilon^{-4})$ for the C-NC case. For fair comparison with existing algorithms, we also analyze the complexity bound to find $\epsilon$-stationary point of the primal function $\phi$ for the constrained NC-C problem, which shows that our algorithm can improve the complexity bound from $\widetilde{\mathcal{O}}(\epsilon^{-3})$ to $\mathcal{O}(\epsilon^{-2})$. To the best of our knowledge, this is the first time that the proposed algorithm improves the best-known complexity bounds from $\mathcal{O}(\epsilon^{-4})$ and $\widetilde{\mathcal{O}}(\epsilon^{-3})$ to $\mathcal{O}(\epsilon^{-2})$ in both the NC-NC and NC-C settings.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=wIlmx4bHrO</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=wIlmx4bHrO</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div></body></html>