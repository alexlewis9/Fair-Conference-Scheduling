<html><head><meta charset='utf-8'><title>Visual Instruction Tuning</title></head><body><h1>Visual Instruction Tuning</h1><h3>By: ['Haotian Liu', 'Chunyuan Li', 'Qingyang Wu', 'Yong Jae Lee']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73817</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 5D Vision</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=w0H2xGHlkw</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=w0H2xGHlkw</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 5D Vision</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6C Vision</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div></body></html>