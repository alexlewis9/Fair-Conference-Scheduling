<html><head><meta charset='utf-8'><title>Fine-Tuning Language Models with Just Forward Passes</title></head><body><h1>Fine-Tuning Language Models with Just Forward Passes</h1><h3>By: ['Sadhika Malladi', 'Tianyu Gao', 'Eshaan Nichani', 'Alex Damian', 'Jason Lee', 'Danqi Chen', 'Sanjeev Arora']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73844</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12× memory reduction and up to 2× GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Vota6rFhBQ</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Vota6rFhBQ</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6A LLMs</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6A LLMs</div></body></html>