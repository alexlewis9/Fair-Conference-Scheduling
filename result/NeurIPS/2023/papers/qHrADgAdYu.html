<html><head><meta charset='utf-8'><title>Towards Revealing the Mystery behind Chain of Thought_ A Theoretical Perspective</title></head><body><h1>Towards Revealing the Mystery behind Chain of Thought_ A Theoretical Perspective</h1><h3>By: ['Guhao Feng', 'Bohang Zhang', 'Yuntian Gu', 'Haotian Ye', 'Di He', 'Liwei Wang']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73822</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. By using circuit complexity theory, we first give impossibility results showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly used math language format. Moreover, we show LLMs with CoT can handle a general class of decision-making problems known as Dynamic Programming, thus justifying their power in tackling complex real-world tasks. Finally, an extensive set of experiments show that, while Transformers always fail to directly predict the answers, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=qHrADgAdYu</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=qHrADgAdYu</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 4C COT/reasoning</div></body></html>