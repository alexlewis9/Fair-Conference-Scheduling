<html><head><meta charset='utf-8'><title>Bridging Discrete and Backpropagation_ Straight-Through and Beyond</title></head><body><h1>Bridging Discrete and Backpropagation_ Straight-Through and Beyond</h1><h3>By: ['Liyuan Liu', 'Chengyu Dong', 'Xiaodong Liu', 'Bin Yu', 'Jianfeng Gao']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73827</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Backpropagation, the cornerstone of deep learning, is limited to computing gradients for continuous variables. This limitation poses challenges for problems involving discrete latent variables. To address this issue, we propose a novel approach to approximate the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose ReinMax, which achieves second-order accuracy by integrating Heunâ€™s method, a second-order numerical method for solving ODEs. ReinMax does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Extensive experimental results on various tasks demonstrate the superiority of ReinMax over the state of the art.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=mayAyPrhJI</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=mayAyPrhJI</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3C Diffusion Models</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2A Efficient Learning</div></body></html>