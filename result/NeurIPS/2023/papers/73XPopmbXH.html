<html><head><meta charset='utf-8'><title>Smoothing the Landscape Boosts the Signal for SGD_ Optimal Sample Complexity for Learning Single Index Models</title></head><body><h1>Smoothing the Landscape Boosts the Signal for SGD_ Optimal Sample Complexity for Learning Single Index Models</h1><h3>By: ['Alex Damian', 'Eshaan Nichani', 'Rong Ge', 'Jason Lee']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73873</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We focus on the task of learning a single index model $\sigma(w^\star \cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\star$ is governed by the information exponent $k^\star$ of the link function $\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\sigma$. Ben Arous et al. (2021) showed that $n \gtrsim d^{k^\star-1}$ samples suffice for learning $w^\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \gtrsim d^{k^\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\star$ with $n \gtrsim d^{k^\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=73XPopmbXH</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=73XPopmbXH</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 4A Optimization</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div></body></html>