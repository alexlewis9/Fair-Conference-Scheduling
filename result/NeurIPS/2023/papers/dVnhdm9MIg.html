<html><head><meta charset='utf-8'><title>Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language</title></head><body><h1>Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language</h1><h3>By: ['Kevin Ellis']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73840</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> A core tension in models of concept learning is that the model must carefully balance the tractability of inference against the expressivity of the hypothesis class. Humans, however, can efficiently learn a broad range of concepts. We introduce a model of inductive learning that seeks to be human-like in that sense.It implements a Bayesian reasoning process where a language model first proposes candidate hypotheses expressed in natural language, which are then re-weighed by a prior and a likelihood.By estimating the prior from human data, we can predict human judgments on learning problems involving numbers and sets, spanning concepts that are generative, discriminative, propositional, and higher-order.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 3A Neuro</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=dVnhdm9MIg</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=dVnhdm9MIg</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 3A Neuro</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div></body></html>