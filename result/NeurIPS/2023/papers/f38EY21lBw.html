<html><head><meta charset='utf-8'><title>Privacy Auditing with One (1) Training Run</title></head><body><h1>Privacy Auditing with One (1) Training Run</h1><h3>By: ['Thomas Steinke', 'Milad Nasr', 'Matthew Jagielski']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73837</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We propose a scheme for auditing differentially private machine learning systems with a single training run. This exploits the parallelism of being able to add or remove multiple training examples independently. We analyze this using the connection between differential privacy and statistical generalization, which avoids the cost of group privacy. Our auditing scheme requires minimal assumptions about the algorithm and can be applied in the black-box or white-box setting. We demonstrate the effectiveness of our framework by applying it to DP-SGD, where we can achieve meaningful empirical privacy lower bounds by training only one model. In contrast, standard methods would require training hundreds of models.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=f38EY21lBw</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=f38EY21lBw</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div></body></html>