<html><head><meta charset='utf-8'><title>Tester-Learners for Halfspaces_ Universal Algorithms</title></head><body><h1>Tester-Learners for Halfspaces_ Universal Algorithms</h1><h3>By: ['Aravind Gollakota', 'Adam Klivans', 'Konstantinos Stavropoulos', 'Arsen Vasilyan']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73861</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We give the first tester-learner for halfspaces that succeeds universally over a wide class of structured distributions. Our universal tester-learner runs in fully polynomial time and has the following guarantee: the learner achieves error $O(\mathrm{opt}) + \epsilon$ on any labeled distribution that the tester accepts, and moreover, the tester accepts whenever the marginal is any distribution that satisfies a Poincare inequality. In contrast to prior work on testable learning, our tester is not tailored to any single target distribution but rather succeeds for an entire target class of distributions. The class of Poincare distributions includes all strongly log-concave distributions, and, assuming the Kannan--Lovasz--Simonovits (KLS) conjecture, includes all log-concave distributions. In the special case where the label noise is known to be Massart, our tester-learner achieves error $\mathrm{opt} + \epsilon$ while accepting all log-concave distributions unconditionally (without assuming KLS).Our tests rely on checking hypercontractivity of the unknown distribution using a sum-of-squares (SOS) program, and crucially make use of the fact that Poincare distributions are certifiably hypercontractive in the SOS framework.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6D Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Kv8GJkV19S</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Kv8GJkV19S</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6D Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2D Privacy</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div></body></html>