<html><head><meta charset='utf-8'><title>A U-turn on Double Descent_ Rethinking Parameter Counting in Statistical Learning</title></head><body><h1>A U-turn on Double Descent_ Rethinking Parameter Counting in Statistical Learning</h1><h3>By: ['Alicia Curth', 'Alan Jeffares', 'Mihaela van der Schaar']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73856</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a _U-shaped curve_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count $p$ grows past sample size $n$  -- a  phenomenon dubbed  _double descent_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include _linear regression, trees, and boosting_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of  double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to _what is being plotted_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and _only_) when and where the transition between these underlying axes occurs, and that its location is thus _not_ inherently tied to the interpolation threshold $p=n$. We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as _smoothers_ and propose a generalized measure for the _effective_ number of parameters they use _on unseen examples_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes -- providing a resolution to the ostensible tension between double descent and traditional statistical intuition.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=O0Lz8XZT2b</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=O0Lz8XZT2b</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 2C Causality</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div></body></html>