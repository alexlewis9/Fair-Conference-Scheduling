<html><head><meta charset='utf-8'><title>Bridging RL Theory and Practice with the Effective Horizon</title></head><body><h1>Bridging RL Theory and Practice with the Effective Horizon</h1><h3>By: ['Cassidy Laidlaw', 'Stuart J Russell', 'Anca Dragan']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73859</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy—i.e., when it is optimal to act greedily with respect to the random's policy Q function—deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the effective horizon , which roughly corresponds to how many steps of lookahead search would be needed in that MDP in order to identify the next optimal action, when leaf nodes are evaluated with random rollouts. Using BRIDGE, we show that the effective horizon-based bounds are more closely reflective of the empirical performance of PPO and DQN than prior sample complexity bounds across four metrics. We also show that, unlike existing bounds, the effective horizon can predict the effects of using reward shaping or a pre-trained exploration policy. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 6B RL</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Lr2swAfwff</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Lr2swAfwff</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 6B RL</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6B RL</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6B RL</div></body></html>