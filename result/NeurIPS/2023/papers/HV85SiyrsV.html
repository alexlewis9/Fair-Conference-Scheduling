<html><head><meta charset='utf-8'><title>Online RL in Linearly $q^_pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</title></head><body><h1>Online RL in Linearly $q^_pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore</h1><h3>By: ['Gellert Weisz', 'András György', 'Csaba Szepesvari']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73864</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be  expressed as linear functions of state-action features. This class is known to be more general than  linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have  approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorithm on the linear MDP hidden in the problem. The method returns an $\epsilon$-optimal policy after $\text{polylog}(H, d)/\epsilon^2$ interactions with the MDP, where $H$ is the time horizon and $d$ is the dimension of the feature vectors, giving the first polynomial-sample-complexity online RL algorithm for this setting. The results are proved for the misspecified case, where the sample complexity is shown to degrade gracefully with the misspecification error.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=HV85SiyrsV</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=HV85SiyrsV</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1A RL</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 6D Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 6B RL</div></body></html>