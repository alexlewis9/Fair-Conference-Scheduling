<html><head><meta charset='utf-8'><title>LeanDojo_ Theorem Proving with Retrieval-Augmented Language Models</title></head><body><h1>LeanDojo_ Theorem Proving with Retrieval-Augmented Language Models</h1><h3>By: ['Kaiyu Yang', 'Aidan Swope', 'Alex Gu', 'Rahul Chalamala', 'Peiyang Song', 'Shixing Yu', 'Saad Godil', 'Ryan J Prenger', 'Animashree Anandkumar']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73738</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selectionâ€”a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1B Datasets & Benchmarks</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=g7OX2sOJtn</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=g7OX2sOJtn</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1B Datasets & Benchmarks</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1B Datasets & Benchmarks</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 3B NLP/Tools</div></body></html>