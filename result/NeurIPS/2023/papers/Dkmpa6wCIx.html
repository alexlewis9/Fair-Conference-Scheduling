<html><head><meta charset='utf-8'><title>Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization</title></head><body><h1>Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization</h1><h3>By: ['Kaiyue Wen', 'Zhiyuan Li', 'Tengyu Ma']</h3><hr><h4>publisher</h4><div style='margin-bottom:1em'>NeurIPS</div><h4>url</h4><div style='margin-bottom:1em'>https://neurips.cc/virtual/2023/oral/73867</div><h4>year</h4><div style='margin-bottom:1em'>2023</div><h4>abstract</h4><div style='margin-bottom:1em'> Despite extensive studies, the underlying reason as to why overparameterizedneural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thusa natural potential explanation is that flatness implies generalization. This workcritically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1)flatness provably implies generalization; (2) there exist non-generalizing flattestmodels and sharpness minimization algorithms fail to generalize poorly, and (3)perhaps most strikingly, there exist non-generalizing flattest models, but sharpnessminimization algorithms still generalize. Our results suggest that the relationshipbetween sharpness and generalization subtly depends on the data distributionsand the model architectures and sharpness minimization algorithms do not onlyminimize sharpness to achieve better generalization. This calls for the search forother explanations for the generalization of over-parameterized neural networks</div><h4>session</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>pdf_url</h4><div style='margin-bottom:1em'>https://openreview.net/pdf?id=Dkmpa6wCIx</div><h4>openreview_url</h4><div style='margin-bottom:1em'>https://openreview.net/forum?id=Dkmpa6wCIx</div><h4>Baseline</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>GreedyCohesive</h4><div style='margin-bottom:1em'>Oral 1D DL Theory</div><h4>KMedoids</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div><h4>KMeans</h4><div style='margin-bottom:1em'>Oral 5C Probability/Sampling</div></body></html>