Published as a conference paper at ICLR 2021
BEHAVIORAL CLONING FROM NOISY DEMONSTRA-
TIONS
Fumihiro Sasaki & Ryota Yamashina
Ricoh Company, Ltd.
{fumihiro.fs.sasaki,ryohta.yamashina}@jp.ricoh.com
ABSTRACT
We consider the problem of learning an optimal expert behavior policy given noisy
demonstrations that contain observations from both optimal and non-optimal ex-
pert behaviors. Popular imitation learning algorithms, such as generative adversar-
ial imitation learning, assume that (clean) demonstrations are given from optimal
expert policies but not the non-optimal ones, and thus often fail to imitate the op-
timal expert behaviors given the noisy demonstrations. Prior works that address
the problem require (1) learning policies through environment interactions in the
same fashion as reinforcement learning, and (2) annotating each demonstration
with conï¬dence scores or rankings. However, such environment interactions and
annotations in real-world settings take impractically long training time and a sig-
niï¬cant human effort. In this paper, we propose an imitation learning algorithm to
address the problem without any environment interactions and annotations associ-
ated with the non-optimal demonstrations. The proposed algorithm learns ensem-
ble policies with a generalized behavioral cloning (BC) objective function where
we exploit another policy already learned by BC. Experimental results show that
the proposed algorithm can learn behavior policies that are much closer to the
optimal policies than ones learned by BC.
1
INTRODUCTION
Imitation learning (IL) has become a widely used approach to obtain autonomous robotics control
systems. IL is often more applicable in real-world problems than reinforcement learning (RL) since
expert demonstrations are often easier than designing appropriate rewards that RL requires. There
have been several IL methods that involve RL (Ziebart et al., 2008; Ng et al., 2000; Abbeel &
Ng, 2004; Ho & Ermon, 2016). Those IL methods inherit sample complexity from RL in terms
of environment interactions during training. The complexity restricts applicabilities in real-world
problems since a number of environment interactions in real-world settings often take a long time
and cause damage to the robot or the environment. Therefore, we are interested in IL methods that
do not require the environment interactions, such as behavioral cloning (BC) (Pomerleau, 1991)
which learns an expert policy in a supervised fashion.
BC as well as popular IL methods, such as generative adversarial imitation learning (GAIL) (Ho
& Ermon, 2016), assume the expert demonstration is optimal. Unfortunately, it is often difï¬cult to
obtain optimal demonstrations for many tasks in real-world problems because the expert who tries to
operate the robot so that it can achieve tasks often makes mistakes due to various reasons, such as the
difï¬culty of the task, difï¬culty in handling the controller, limited observability of the environment,
or the presence of distraction. The mistakes include unnecessary and/or incorrect operations to
achieve the tasks. Given such noisy expert demonstrations, which contain records of both optimal
and non-optimal behavior, BC as well as the popular IL methods fails to imitate the optimal policy
due to the optimal assumption on the demonstrations as shown in (Wu et al., 2019).
A naive solution to cope with the noisy demonstrations is discarding the non-optimal demonstrations
among the ones that were already collected. This screening process is often impractical because it
involves a signiï¬cant human effort. Most of recent IL works suppose settings where a very limited
number of clean expert demonstrations, which are composed of only the optimal behavior records,
are available. Those methods are also vulnerable to the noisy demonstrations due to the optimal
1

Published as a conference paper at ICLR 2021
assumption on the demonstrations. Thus they implicitly suppose such impractical screening process
if they were applied in real-world problems, where a number of the noisy demonstrations other than
the clean ones can be easily obtained. There have been IL methods addressing the noisy demonstra-
tions. Instead of the screening process, they require to annotate each demonstration with conï¬dence
scores (Wu et al., 2019) or rankings (Brown et al., 2019). Even though they cope well with the
noisy demonstrations to obtain the optimal behavior policies, such annotation costs a signiï¬cant
human effort as it is for the screening. Hence, we desire IL methods that can cope well with the
noisy demonstrations, which can be easily obtained in real-world settings, without any screening
and annotation processes associated with the non-optimal behaviors.
In this paper, we propose a novel imitation learning algorithm to address the noisy demonstrations.
The proposed algorithm does not require (1) any environment interactions during training, and (2)
any screening and annotation processes associated with the non-optimality of the expert behaviors.
Our algorithm learns ensemble policies with a generalized BC objective function where we exploit
another policy already learned by BC. Experimental results show that the proposed algorithm can
learn policies that are much closer to the optimal than ones learned by BC.
2
RELATED WORKS
A wide variety of IL methods have been proposed in these last few decades. BC (Pomerleau, 1991)
is the simplest IL method among those and thus BC could be the ï¬rst IL option when enough clean
demonstrations are available. Ross & Bagnell (2010) have theoretically pointed out a downside of
the BC which is referred to as compounding error â€“ the small errors of the learners trained by BC
could compound over time and bring about the deterioration of their performance. On the other hand,
experimental results in (Sasaki et al., 2018) show that BC given the clean demonstrations of sufï¬cient
amounts can easily obtain the optimal behavior even for complex continuous control tasks. Hence,
the effect of the compounding error is negligible in practice if the amount of clean demonstrations is
sufï¬cient. However, even if the amount of the demonstrations is large, BC cannot obtain the optimal
policy given the noisy demonstrations due to the optimal assumption on the demonstrations. Another
widely used IL approaches are inverse reinforcement learning (IRL) (Ziebart et al., 2008; Ng et al.,
2000; Abbeel & Ng, 2004) and adversarial imitation learning (AIL) (Ho & Ermon, 2016). Since
those approaches also assume the optimality of the demonstrations, they are also not able to obtain
the optimal policy given the noisy demonstrations, as shown in (Wu et al., 2019). As we will show
in Section 6, our algorithm successfully can learn near-optimal policies if noisy demonstrations of
sufï¬cient amounts are given.
There have been several works that address the noisy demonstrations (Wu et al., 2019; Brown et al.,
2019; Tangkaratt et al., 2019; Kaiser et al., 1995; Grollman & Billard, 2012; Kim et al., 2013).
Those works address the noisy demonstrations by either screening the non-optimal demonstrations
with heuristic non-optimal assessments (Kaiser et al., 1995), annotations associated with the non-
optimality (Wu et al., 2019; Brown et al., 2019; Grollman & Billard, 2012), or training through the
environment interactions (Kim et al., 2013; Wu et al., 2019; Brown et al., 2019; Tangkaratt et al.,
2019). Our algorithm does not require any screening processes, annotations associated with the
non-optimality, and the environment interactions during training.
Ofï¬‚ine RL methods (Lange et al., 2012; Fujimoto et al., 2019; Kumar et al., 2020) train the learner
agents without any environment interactions, and allow the training dataset to have non-optimal
trajectories as in our problem setting. A drawback of ofï¬‚ine RL methods for the real-world applica-
tions is the requirement to design reward functions, which often involves a signiï¬cant human efforts
for its success, since those methods assume that the reward for each state-action pair is known. Our
algorithm does not require to design reward functions as in standard IL methods.
Disagreement regularized imitation learning (DRIL) (Brantley et al., 2019) is a state-of-the-art IL
algorithm which employs an ensemble of policies as our algorithm does. The aims of employing the
ensemble is different between DRIL and our algorithm. DRIL uses the disagreement in predictions
made by policies in the ensemble to evaluate whether the states observed during training the learner
are ones observed in the expert demonstrations. On the other hand, our algorithm uses the ensemble
to encourage the learner to take optimal actions on each state as described in 5.3. In addition, DRIL
fundamentally requires the environment interactions during training whereas our algorithm does not.
2

Published as a conference paper at ICLR 2021
3
PRELIMINARIES AND PROBLEM SETUP
In this work, we consider an episodic ï¬xed-horizon Markov decision process (MDP) which is for-
malized as a tuple {S, A, P, R, d0, T}, where S is a set of states, A is a set of possible actions
agents can take, P : SÃ—AÃ—S â†’[0, 1] is a transition probability, R : SÃ—A â†’[0, 1] is a reward
function, d0 : S â†’[0, 1] is a distribution over initial states, and T is an episode horizon. The agentâ€™s
behavior is deï¬ned by a stochastic policy Ï€ : SÃ—A â†’[0, 1] and Î  denotes a set of the stochas-
tic policies. The expected one-step immediate reward for a policy Ï€ given a state s is deï¬ned as
RÏ€(s) = Eaâˆ¼Ï€(Â·|s)

R(s, a)

.
Let dÏ€
t and dÏ€ =
1
T
PT
t=1 dÏ€
t denote the distribution over states at time step t and the aver-
age distribution over T time steps induced by Ï€, respectively. The distributions dÏ€
1 at the ï¬rst
step correspond to d0 for any Ï€.
When following a policy Ï€ throughout an episode, the ex-
pected one-step immediate reward at time step t and the expected T-step reward are deï¬ned as
RÏ€
t = Esâˆ¼dÏ€
t ,aâˆ¼Ï€(Â·|s)

R(s, a)

= Esâˆ¼dÏ€
t

RÏ€(s)

and J (Ï€, R) = PT
t=1 RÏ€
t = TEsâˆ¼dÏ€
RÏ€(s)

,
respectively. We refer to J (Ï€, R) as on-policy expected T-step reward. We also consider another
T-step reward deï¬ned by JÎ²(Ï€, R) = TEsâˆ¼dÎ²

RÏ€(s)

, which we call off-policy expected T-step
reward, where Î² âˆˆÎ  is a policy that can differ from Ï€.
In our problem setting, the functions R is not given. Instead, we observe noisy demonstrations.
We refer to the agent that generates the noisy demonstrations as the noisy expert. The decision
process turns to be MDP\{R} as in the common imitation learning settings, and our problem can
be formalized as to ï¬nd an optimal policy in MDP\{R}. Here we refer to the true expert policy
Ï€âˆ—
e as ones being able to take the optimal (thus not noisy) behavior in episodic tasks. We make the
following four assumptions to further formalize our problem setting:
Assumption 1. The T-step expected reward of Ï€âˆ—
e satisï¬es J (Ï€, R) â‰¤J (Ï€âˆ—
e, R); JÎ²(Ï€, R) â‰¤
JÎ²(Ï€âˆ—
e, R); and JÎ²(Ï€âˆ—
e, R) â‰¤J (Ï€âˆ—
e, R) for any non-optimal policies Ï€, Î² âˆˆÎ  \ {Ï€âˆ—
e}.
Assumption 2. With small probability Ïµ, which we call non-optimal probability, the policies Ï€e
the noisy experts follow during demonstrations are sampled at each time step as Ï€e = Ï€ âˆ¼pÎ  if
Ïµ â‰¥z âˆ¼U(0, 1), otherwise Ï€e = Ï€âˆ—
e, where pÎ  is an unknown distribution over the set of policies,
z is a random variable, and U(0, 1) is a uniform distribution with range [0, 1].
Assumption 3. The reward RÏ€e
t
is at least zero if the noisy expert has followed a policy Ï€ âˆˆÎ \{Ï€âˆ—
e}
once or more so far, otherwise RÏ€e
t
= Esâˆ¼dÏ€e
t

ÏµEÏ€âˆ¼pÎ [RÏ€(s)] + (1 âˆ’Ïµ)RÏ€âˆ—
e (s)

.
Assumption 4. The sequence {RÏ€e
1 , ..., RÏ€e
T } has monotonically decreasing property RÏ€e
t
â‰¥RÏ€e
t+1.
Assumption 1 indicates that both on-policy and off-policy expected T-step reward following Ï€âˆ—
e are
always greater than or equal to ones following any other policies. In other words, we assume the true
expert policy is an optimal one in the MDP, and the agent following the policy is able to behave so
that the expected immediate rewards at any states are maximized. Under Assumption 1, the problem
that we would like to solve in this work can be said to learn a parameterized policy Ï€Î¸ to maximize
its on-policy expected T-step reward J (Ï€Î¸, R) to J (Ï€âˆ—
e, R). Assumption 2 indicates that the noisy
expert occasionally adopts non-optimal policies, which results in the noisy demonstrations, due to
random events, such as the presence of distractions, associated with the random variable z. The
noisy expert is going to visit states that would be never visited by the true expert if the noisy expert
followed non-optimal policies even once. Assumption 3 indicates that those states are less rewarded
and their rewards are at least zero. Assumption 3 also indicates that the noisy demonstrations have
a number of episodes where the noisy expert has reached the same state s where the noisy expert
has adopted both Ï€âˆ—
e and Ï€ âˆˆÎ  \ {Ï€âˆ—
e} with the probability Ïµ. Assumption 4 indicates that, since
the probability the noisy expert consecutively follows Ï€âˆ—
e decreases as time step increases according
to Assumption 2, the divergence between dÏ€e
t
and dÏ€âˆ—
e
t
becomes greater as the number of time step
t increases, and thus the one-step expected immediate reward RÏ€e
t
= Esâˆ¼dÏ€e
t
,aâˆ¼Ï€e(Â·|s)

R(s, a)

decreases as t increases.
3

Published as a conference paper at ICLR 2021
4
ANALYSIS OF PERFORMANCE DETERIORATION
In this section, we ï¬rstly describe BC objective in 4.1. Then, we analyze why the learner trained
by BC deteriorates its performance when using the noisy demonstrations from the expected T-step
reward maximization and KL-divergence minimization perspectives in 4.2 and 4.3, respectively.
4.1
BEHAVIORAL CLONING OBJECTIVE
Let Ï€Î¸ âˆˆÎ  is a learner policy parameterized by Î¸ to be optimized by IL algorithms. The objective
of BC in common is as follows:
arg max
Î¸
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)].
(1)
The objective (1) aims to mimic the expert behavior which follows Ï€e. It can be interpreted that (1)
is to maximize the expected one-step immediate reward RÏ€Î¸(s) to RÏ€e(s) at each state s âˆ¼dÏ€e.
Since the state distribution dÏ€e is not induced by Ï€Î¸, it can also be said that (1) is to maximize the
off-policy expected T-step rewards JÏ€e(Ï€Î¸, R) to J (Ï€e, R).
4.2
THE EXPECTED T-STEP REWARD MAXIMIZATION
We obtain the lower bound of the expected on-policy T-step reward for the noisy expert policy in
almost the same way to derive Theorem 2.1 in (Ross & Bagnell, 2010) where they showed the lower
bound for the learner policies given the â€œcleanâ€œ expert demonstrations.
Theorem 1. If the Assumptions 1 - 4 hold, J (Ï€e, R) has the following lower bound:
J (Ï€e, R) â‰¥
 1
T
T âˆ’1
X
t=0
(1 âˆ’Ïµ)t

Â· EÏ€âˆ¼pÎ [JÏ€e(Ï€, R)].
(2)
The detailed derivation can be found in Appendix A.1. Assume that the learner policy Ï€Î¸ has a
probability of non-optimal behavior Ë†Ïµ = Ïµ + Î¶ at most as the result of BC, where Î¶ âˆˆ[0, 1 âˆ’Ïµ] is
an additional probability of non-optimal behavior due to the remained loss in (1). Note that Î¶ may
become greater than zero due to the difï¬culty in the optimization of (1) even if Ïµ = 0. The learner
following Ï€Î¸ with Ë†Ïµ can be deemed as another noisy expert who samples a policy at each time step
Ï€Î¸ = Ï€ âˆ¼pÏ€Î¸ if Ë†Ïµ â‰¥z âˆ¼U(0, 1), otherwise Ï€Î¸ = Ï€âˆ—
e, where pÏ€Î¸ is a (special) distribution from
which the same policy is always sampled. By replacing Ë†Ïµ and pÏ€Î¸ from Ïµ and pÎ  in Theorem 1
respectively, we obtain the following corollary.
Corollary 1. If the Assumptions 1 - 4 hold and the policy Ï€Î¸ has a probability of non-optimal
behavior Ë†Ïµ = Ïµ + Î¶, J (Ï€Î¸, R) has the following lower bound:
J (Ï€Î¸, R) â‰¥
 1
T
T âˆ’1
X
t=0
(1 âˆ’Ë†Ïµ)t

Â· JÏ€e(Ï€Î¸, R).
(3)
Recall that the BC objective (1) is to maximize JÏ€e(Ï€Î¸, R). If Ë†Ïµ = 0, Corollary 1 indicates that the
on-policy expected T-step reward J (Ï€Î¸, R), which corresponds to the actual learner performance,
is boosted by maximizing JÏ€e(Ï€Î¸, R) through the optimization of the BC objective (1). On the
other hand, if Ïµ > 0 and thus Ë†Ïµ > 0, the ï¬rst factor on the RHS in (3) becomes much smaller as Ïµ
becomes larger. Corollary 1 thus shows that the probability of non-optimal behavior Ïµ of the noisy
expert signiï¬cantly negates the improvement of learner performance J (Ï€Î¸, R) by BC even if Î¶ can
be sufï¬ciently minimized through the optimization. Hence, the learner trained by BC is not able to
boost the learner performance enough if the noisy demonstrations were given.
4.3
KL DIVERGENCE MINIMIZATION
Let SÏ€e be a set of states that are observed in the noisy demonstration. SÏ€e can be thought of
as the domain of (empirical) state distributions dÏ€e. SÏ€e can be deï¬ned with two state sets of
states as SÏ€e = SÏ€e
e
âˆªSÏ€e
e+âˆ—, where SÏ€e
e
contains states that are observed if the noisy expert has
followed a policy Ï€ âˆˆÎ  \ {Ï€âˆ—
e} once or more so far in the episode, and SÏ€e
e+âˆ—contains states
4

Published as a conference paper at ICLR 2021
at which the noisy expert has followed a policy Ï€ âˆˆÎ  \ {Ï€âˆ—
e} at the ï¬rst time in the episode.
Under Assumption 3, the rewards RÏ€e
t
for the states s âˆˆSÏ€e
e
are at least zero whereas RÏ€e
t
=
Esâˆ¼dÏ€e
t

ÏµEÏ€âˆ¼pÎ [RÏ€(s)]+(1âˆ’Ïµ)RÏ€âˆ—
e (s)

for the states s âˆˆSÏ€e
e+âˆ—. Note that the noisy expert adopts
Ï€ âˆˆÎ  \ {Ï€âˆ—
e} with a probability Ïµ at the states s âˆˆSÏ€e
e+âˆ—. Let dÏ€e
e and dÏ€e
e+âˆ—be the state distributions
the noisy expert policy induces in SÏ€e
e
and SÏ€e
e+âˆ—, respectively. Then we can deï¬ne dÏ€e as a mixture
of those distributions as
dÏ€e(s) = Î±dÏ€e
e (s) + Î²dÏ€e
e+âˆ—(s),
(4)
where Î± and Î² are ratios the noisy expert entered states that belong to SÏ€e
e
and SÏ€e
e+âˆ—during demon-
strations, respectively. In addition, Î± + Î² = 1 is satisï¬ed. Using Equation (4), the upper bound of
the objective function in Equation (1) is derived as follows:
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)]
â‰¤
âˆ’Î±â„¦e(Î¸) âˆ’Î²â„¦e+âˆ—(Î¸),
(5)
â„¦e(Î¸)
=
Esâˆ¼dÏ€e
e [DKL[Ï€e(Â·|s)||Ï€Î¸(Â·|s)]],
(6)
â„¦e+âˆ—(Î¸)
=
ÏµEsâˆ¼dÏ€e
e+âˆ—,Ï€âˆ¼pÎ [DKL[Ï€(Â·|s)||Ï€Î¸(Â·|s)]]
+
(1 âˆ’Ïµ)Esâˆ¼dÏ€e
e+âˆ—[DKL[Ï€âˆ—
e(Â·|s)||Ï€Î¸(Â·|s)]],
(7)
where DKL is forward Kullback-Leibler (KL) divergence. The full derivation can be found in
Appendix A.2. The inequality (5) shows that the BC objective (1) with the noisy demonstrations
is to minimizes the sum of KL divergences. The ï¬rst term on the RHS in (7) leads the learner
to imitate some non-optimal behaviors whereas the second term is to learn Ï€âˆ—
e on the same states.
The optimization to maximize the RHS in (7) is difï¬cult because minimizing KL divergences with
different target distributions at the same time is difï¬cult in general. The ï¬rst term on the RHS in (7)
thus works as a â€œnoisyâ€ regularizer with a coefï¬cient Ïµ that makes the learner confused to learn Ï€âˆ—
e.
The difï¬culty in the optimization due to the noisy regularizer increases Î¶ as Ïµ increases.
As mentioned in 4.1 and 4.2, BC is to maximize JÏ€e(Ï€Î¸, R) to J (Ï€e, R). Hence, minimizing
â„¦e(Î¸) in (6) corresponds to maximize Esâˆ¼dÏ€e
e [RÏ€Î¸(s)] to Esâˆ¼dÏ€e
e [RÏ€e(s)].
Since the rewards
RÏ€e(s) are at least zero for the states s âˆ¼dÏ€e
e
according to Assumption 3 and the deï¬nition of
SÏ€e
e , Esâˆ¼dÏ€e
e [RÏ€Î¸(s)] becomes at least zero by minimizing â„¦e(Î¸). Hence JÏ€e(Ï€Î¸, R) becomes at
least zero as the rate Î± increases, while the rate Î± increases as the probabilities of non-optimal be-
havior Ïµ increases. Thus, the larger the probability Ïµ is, the more difï¬cult it is to boost the learner
performance by BC.
If the inï¬‚uence of the noisy regularizer can be reduced, probabilities the learner follows Ï€âˆ—
e at the
state s âˆˆSÏ€e
e+âˆ—will increase. In addition, as probabilities the learner follows Ï€âˆ—
e at the states s âˆˆ
SÏ€e
e+âˆ—increase, the rate (corresponding to Î±) for the states s âˆˆSÏ€e
e
will decrease. Thus, it can be
said that, the more often learner follows Ï€âˆ—
e at the states s âˆˆSÏ€e
e+âˆ—, the more rewards RÏ€âˆ—
e (s) the
learner obtains according to Assumption 3. To summarize the above analysis, reducing the inï¬‚uence
of the noisy regularizer for states s âˆˆSÏ€e
e+âˆ—, which leads the learner to imitate some non-optimal
behaviors, might boost the learner performance.
5
ALGORITHM
The analyses in Section 4 describe that the learner trained by standard BC deteriorates its perfor-
mance when the noisy demonstrations are given. Based on both analyses in 4.2 and 4.3, the learner
performance will be boosted if the learner imitates the optimal policy Ï€âˆ—
e but not the non-optimal
ones Ï€ âˆˆÎ  \ {Ï€âˆ—
e} for the states s âˆˆSÏ€e
e+âˆ—. In other words, the learner performance will be boosted
if Ë†Ïµ of the learner can be reduced. In this section, we ï¬rst propose our algorithm that avoids learning
Ï€ âˆˆÎ \{Ï€âˆ—
e} while learning Ï€âˆ—
e in 5.1. Then we describe how our algorithm works to avoid learning
Ï€ âˆˆÎ \{Ï€âˆ—
e} from mode seeking and reward maximization perspectives in 5.2 and 5.3, respectively.
We lastly provide limitations of our algorithm in 5.4.
5.1
PROPOSED ALGORITHM
We consider a generalization of the BC objective as follows:
arg max
Î¸
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s) Â· Ë†R(s, a)],
(8)
5

Published as a conference paper at ICLR 2021
Algorithm 1 Behavioral Cloning from Noisy Demonstrations
1: Given the expert demonstrations D.
2: Set Ë†R(s, a) = 1 for âˆ€(s, a) âˆˆD.
3: Split D into K disjoint sets {D1, D2, ..., DK}.
4: for iteration = 1, M do
5:
for k = 1, K do
6:
Initialize parameters Î¸k.
7:
for l = 1, L do
8:
Sample a random minibatch of N state-action pairs (sn, an) from Dk.
9:
Calculate a sampled gradient 1
N
PN
n=1 âˆ‡Î¸k log Ï€Î¸k(sn, an) Â· Ë†R(sn, an).
10:
Update Î¸k by gradient ascent using the sampled gradient.
11:
end for
12:
end for
13:
Copy Ï€Î¸old â†Ï€Î¸.
14:
Set Ë†R(s, a) = Ï€Î¸old(a|s) for âˆ€(s, a) âˆˆD.
15: end for
16: return Ï€Î¸.
where Ë†R : SÃ—A â†’[0, 1] denotes an arbitrary function which can differ from R. If Ë†R(s, a) = 1
for âˆ€(s, a) âˆˆS Ã— A, the objective (8) corresponds to the BC objective (1). If
R
A Ë†R(s, a)da = 1
for âˆ€s âˆˆS is satisï¬ed, Ë†R(s, a) can be interpreted as weights for action samples obtained by the
demonstrations so that the actions are sampled according to their relative weights. The objective (8)
can also be deemed as that of the off-policy actor-critic (Off-PAC) algorithm1 (Degris et al., 2012)
with reward functions Ë†R(s, a) and zero discount factors.
Let Ï€Î¸1, Ï€Î¸2, ..., Ï€Î¸K be K parameterized policies with different initial parameters Î¸1, Î¸2, ..., Î¸K, and
Ï€Î¸(a|s) = PK
k=1 Ï€Î¸k(a|s)/K denotes an ensemble of the parameterized policies with parameters
Î¸ = {Î¸1, Î¸2, ..., Î¸K}. Let Ï€Î¸old be a parameterized policy with Î¸old which was already optimized
with the noisy demonstrations. The main idea of our algorithm is to reuse the old policy Ï€Î¸old as
Ë†R(s, a) in the generalized BC objective (8).
arg max
Î¸
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s) Â· Ï€Î¸old(a|s)].
(9)
The overview of our algorithm is described in Algorithm 1.
5.2
WEIGHTED ACTION SAMPLING FOR Ï€âˆ—
e MODE SEEKING
Since Ï€Î¸old satisï¬es
R
A Ï€Î¸old(a|s)da = 1 for âˆ€s âˆˆS, Ï€Î¸old can be interpreted as the weights for
the weighted action sampling. We below explain the weighted action sampling procedure in our
algorithm on SÏ€e
e+âˆ—. Figure 1 depicts a toy example of the sampling procedure. The distribution
of the noisy expert actions on SÏ€e
e+âˆ—is a mixture of two distributions as shown in Equation (7).
If Ïµ is sufï¬ciently small, Ï€Î¸ is optimized so that its mode is closer to that of Ï€âˆ—
e than Ï€ âˆˆÎ  \
{Ï€âˆ—
e} according to mode seeking properties of the forward KL divergence (Ghasemipour et al.,
2020). Given the sampling weights Ï€Î¸old(a|s) = Ï€Î¸(a|s) for the empirical action samples, the
weighted action distribution distorts so that its mode also gets closer to the mode of Ï€âˆ—
e. By iteratively
distorting the weighted action distribution with the same procedure, its mode ï¬ts to near the mode of
Ï€eâˆ—. The weights for actions sampled from Ï€ âˆˆÎ \{Ï€âˆ—
e} eventually become much smaller, and thus
the learner will not learn Ï€ âˆˆÎ  \ {Ï€âˆ—
e}. The mode seeking procedure of our algorithm is analogous
to the mean shift algorithm (Fukunaga & Hostetler, 1975) so that the mode of Ï€Î¸ shifts towards that
of Ï€âˆ—
e by minimizing the KL divergence between Ï€Î¸ and the weighted action distribution.
1Although Off-PAC multiplies log Ï€Î¸(a|s) by a density ratio Ï€e(s|a)/Ï€Î¸(s|a), Ï€Î¸(s|a) is empirically ap-
proximated to be one in popular off-policy RL algorithms such as DDPG (Lillicrap et al., 2015).
6

Published as a conference paper at ICLR 2021
iteration 1
iteration 5
iteration 10
iteration 15
weighted distribution
Figure 1: A toy example of the weighted action sampling procedure at each iteration in our algorithm
when given a state s âˆˆSÏ€e
e+âˆ—. On both rows, the horizontal lines are the action domains. The left
and right dotted lines on the top row describe Ï€ âˆˆÎ  \ {Ï€âˆ—
e} and Ï€âˆ—
e(a|s), respectively. The dotted
lines on the bottom row describe the mixture distribution Ï€e(a|s) = ÏµÏ€(a|s) + (1 âˆ’Ïµ)Ï€âˆ—
e(a|s) with
Ïµ = 0.4. The solid lines on the top row describe Ï€Î¸(a|s) that are optimized with objective (8) at
each iteration. The solid lines on the bottom row describe distributions which draw actions, that were
already drawn by Ï€e(a|s) in the noisy demonstrations, according to the current importance weight
Ï€Î¸(a|s) at each iteration. Ï€Î¸(a|s) are optimized at each iteration so that the weighted distribution at
the previous iteration is the target distribution.
5.3
REWARD MAXIMIZATION
As the Off-PAC objective, the objective (9) maximizes the expected (one-step) reward Ë†R(s, a) =
Ï€Î¸old(a|s). Recall that the learner policy Ï€Î¸(a|s) = PK
k=1 Ï€Î¸k(a|s)/K is an ensemble of the pa-
rameterized policies in our algorithm. Following the work in (Perrone, 1993), we obtain
1
K
K
X
k=1
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸k(a|s) Â· Ë†R(s, a)] â‰¤Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)

log Ï€Î¸(a|s) Â· Ë†R(s, a)

,
(10)
where we use Jensenâ€™s inequality with the concave property of logarithm :
1
K
PK
k=1 log Ï€Î¸k(a|s) â‰¤
log Ï€Î¸(a|s). The inequality (10) indicates that the ensemble of policies Ï€Î¸1, Ï€Î¸2, ..., Ï€Î¸K, each of
which was learned with (8), has greater or equal values of the objective function in (8) than the
averaged values over the policies in the ensemble. As mentioned in 5.2, Ë†R(s, a) = Ï€Î¸old(a|s)
becomes higher near the mode of Ï€âˆ—
e. Thus, making Ï€Î¸ as the ensemble further encourages to shift
its mode to that of Ï€âˆ—
e and avoid learning Ï€ âˆˆÎ  \ {Ï€âˆ—
e}.
5.4
LIMITATIONS
Our algorithm has three limitations. First, K Ã— M times computational cost is required in com-
parison with BC, where M is the number of iterations in Algorithm 1. Second, the compounding
error due to the probability of non-optimal behavior Î¶ still remains unless sufï¬cient amounts of the
demonstrations are given. Lastly, Ï€Î¸ is ï¬tting to Ï€ âˆˆÎ  \ {Ï€âˆ—
e} rather than Ï€âˆ—
e if the major mode of
ÏµÏ€(a|s) + (1 âˆ’Ïµ)Ï€âˆ—
e(a|s) is nearer to the mode of Ï€(a|s) than that of Ï€âˆ—
e. It may be caused due to
the higher kurtosis of Ï€(a|s) or Ïµ of large values.
6
EXPERIMENTS
In our experiments, we aim to answer the following three questions:
Q1. Does our algorithm improve the learner performance more than BC given the noisy demon-
strations?
Q2. Can the compounding error due to Î¶ be reduced as the number of noisy demonstrations
increase?
Q3. Is our algorithm competitive to the existing IL methods if both annotations associated with
the non-optimality and environment interactions are allowed?
7

Published as a conference paper at ICLR 2021
6.1
SETUP
To answer Q1 and Q2, we evaluated our algorithm against BC on four continuous control tasks that
are simulated with MuJoCo physics simulator (Todorov et al., 2012). We train an agent on each task
by proximal policy optimization (PPO) algorithm (Schulman et al., 2017) using the rewards deï¬ned
in the OpenAI Gym (Brockman et al., 2016). We use the resulting stochastic policy as the true
expert policy Ï€âˆ—
e. We generate the noisy expert demonstrations using Ï€âˆ—
e while randomly adopting
non-optimal policies Ï€ with probabilities of the non-optimal behavior Ïµ. The non-optimal policies
Ï€ are selected from uniform distributions a âˆ¼U(âˆ’u, u), Gaussian distributions a âˆ¼N(aâˆ—, I)
with a âˆ¼Ï€âˆ—
e(Â·|s), or a deterministic policy a = 0, where u âˆˆR|A| denotes all-ones vectors and
I âˆˆR|A|Ã—|A| denotes identity matrices. Ïµ are selected from {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}. The noisy
expert takes actions following Ï€âˆ—
e if z â‰¥Ïµ otherwise Ï€ which is ï¬xed to a selected one through an
episode, where z âˆ¼U(0, 1). Each noisy demonstration with the selected Ïµ consists of N state-action
pairs, where N is selected from {5000, 10000, 50000, 100000}. Then we perform our algorithm
as well as BC to train the learners using each noisy demonstration. We also conducted the same
experiments on four low-dimensional discrete control tasks (see Appendix A.4).
To answer Q3, we evaluated our algorithm against IC-GAIL (Wu et al., 2019), 2IWIL (Wu et al.,
2019), T-REX (Brown et al., 2019), GAIL and DRIL on three continuous control tasks.
IC-
GAIL, 2IWIL and T-REX require both annotations associated with the non-optimality and envi-
ronment interactions. GAIL and DRIL require the environment interactions for the training, but
they do not address the noisy demonstration problem.
The true expert policy Ï€âˆ—
e are obtained
in the same way as mentioned above. The non-optimal policies Ï€ are ï¬xed to a âˆ¼U(âˆ’u, u).
We generate the noisy expert demonstrations which consists of 10000 state-action pairs for each
Ïµ âˆˆ{0.05, 0.1, 0.15, ...., 1.0}. Then we perform our algorithm and the baselines using all noisy
demonstrations. The detailed description of this experimental setup can be found in Appendix A.3.
In both experiments, the performance of the learners is measured by cumulative rewards they earned
in an episode. The cumulative reward is normalized with ones earned by Ï€âˆ—
e and a random policy
a âˆ¼U(âˆ’u, u) so that 1.0 and 0.0 indicate the performance of Ï€âˆ—
e and the random policy, respectively.
We run ï¬ve experiments on each task and setup, and measure the mean and standard deviation of
the normalized cumulative rewards for each learner over the ï¬ve experiments. In all experiments,
we set the number of policies K = 5 in the ensemble learner policy Ï€Î¸ and the number of iterations
M = 5. The implementation details of our algorithm can be found in Appendix A.5.
6.2
RESULTS
Figure 2 depicts the experimental results against BC. Over all tasks, our algorithm obtains much
better learner performance than BC-Single, which is a single (thus not an ensemble) policy learned
by BC. It suggests that the policies learned by our algorithm are closer to Ï€âˆ—
e than ones learned by
BC. The compounding error due to Î¶ is expected to be reduced as the number of demonstrations
increase. Whereas BC-Ensemble which denotes the ensemble of policies learned by BC yields
signiï¬cant performance gains against BC-Single, increasing the number of noisy demonstrations
has a little effect to boost the learner performance trained by BC-Ensemble as shown in Figure 2-
(D). It indicates that BC-Ensemble can not reduce the compounding error due to Ïµ. On the other
hand, our algorithm can boost the learner performance up to that of Ï€âˆ—
e as increasing the number of
demonstrations. It suggests that our algorithm can reduce the compounding error due to both Ïµ and Î¶
if sufï¬cient amounts of the noisy expert demonstrations are given, as is the case for BC with the clean
expert demonstrations. The results with the deterministic non-optimal policy Ï€ âˆˆÎ  \ {Ï€âˆ—
e} which
always takes an action a = 0 are worse than those with other non-optimal policies. It corresponds to
the limitation of our algorithm as mentioned in 5.4, since the major mode of ÏµÏ€(a|s)+(1âˆ’Ïµ)Ï€âˆ—
e(a|s)
might be around a = 0. We also conducted ablation experiments where the number of policies K
are selected from {1, 5} in our algorithm. See Appendix A.6 for details. The ablation experimental
results show that the learner obtains better performance if K increases. In addition, the performance
of the learner trained by our algorithm is signiï¬cantly better than that of BC-Single even though
K = 1. It suggests that our algorithm improves the learner performance by not only the ensemble
approach but also using the old policies Ï€Î¸old.
Table 1 shows the experimental results against IC-GAIL, 2IWIL, T-REX, GAIL and DRIL. Over all
tasks, 2IWIL and our algorithm can successfully obtain the true expert performance while others can
8

Published as a conference paper at ICLR 2021
Noisy Expert
BC-Ensemble
BC-Single
Ours
Ant-v2
HalfCheetah-v2
Hopper-v2
Walker2d-v2
Normalized Return
Probability of Non-Optimal Behavior 
Normalized Return
Normalized Return
Probability of Non-Optimal Behavior 
Probability of Non-Optimal Behavior 
(A)
(B)
Number of State-Action Pairs
(D)
(C)
Noisy Expert
BC-Ensemble
BC-Single
Ours
Noisy Expert
BC-Ensemble
BC-Single
Ours
Noisy Expert
BC-Ensemble
BC-Single
Ours
Normalized Return
Figure 2:
(A)-(C) The performance of policies vs. Ïµ given 50000 state-action pairs of the noisy
expert demonstrations where the non-optimal policies Ï€ âˆˆÎ \{Ï€âˆ—
e} are (A) U(âˆ’u, u), (B) N(aâˆ—, I)
with a âˆ¼Ï€âˆ—
e(Â·|s), and (C) the deterministic one a = 0, respectively. (D) The performance of policies
vs. the number of state-action pairs N of the noisy demonstrations with Ïµ = 0.3 where Ï€(a|s) =
U(âˆ’u, u). BC-Single is a policy learned by BC. BC-Ensemble is an ensemble of policies, each of
which was learned by BC. Shaded regions indicate the standard deviation over ï¬ve experiments.
not. It suggests that our algorithm can obtain competitive results with that of existing IL methods
even though the annotation and the environment interactions are not used.
Table 1: The experimental results against IL methods that require the environment interactions.
IC-GAIL
2IWIL
T-REX
GAIL
DRIL
Ours
Ant-v2
0.631 Â± 0.162
1.042 Â± 0.021
0.586 Â± 0.124
0.003 Â± 0.004
1.071 Â± 0.023
1.055 Â± 0.053
HalfCheetah-v2
0.941 Â± 0.103
1.024 Â± 0.059
0.001 Â± 0.113
0.106 Â± 0.003
0.065 Â± 0.006
1.093 Â± 0.092
Hopper-v2
1.233 Â± 0.152
1.223 Â± 0.135
0.441 Â± 0.219
0.000 Â± 0.001
0.910 Â± 0.099
1.003 Â± 0.045
7
CONCLUSION
In this paper, we proposed an imitation learning algorithm to cope with the noisy expert demon-
strations. Experimental results showed that our algorithm can learn behavior policies that are much
closer to the true expert policies than ones learned by BC. Since our algorithm cope well with the
noisy expert demonstrations while not requiring any environment interactions and annotations asso-
ciated with the non-optimal demonstrations, our algorithm is more applicable to real-world problems
than the prior works. Although our algorithm has a few limitations as mentioned in 5.4, we believe
that the analysis of performance deterioration detailed in Section 4 contributes to step forward for
solving the noisy demonstration problems. In future work, we will consider the setting where the
probability of non-optimal behavior is state-dependent, which often occurs in the real world more
than the state-independent case that we have considered in this paper.
9

Published as a conference paper at ICLR 2021
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-ï¬rst international conference on Machine learning, pp. 1, 2004.
KiantÂ´e Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
International Conference on Learning Representations, 2019.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Daniel S Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-
optimal demonstrations via inverse reinforcement learning from observations.
arXiv preprint
arXiv:1904.06387, 2019.
Thomas Degris, Martha White, and Richard S Sutton.
Off-policy actor-critic.
arXiv preprint
arXiv:1205.4839, 2012.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052â€“2062, 2019.
Keinosuke Fukunaga and Larry Hostetler. The estimation of the gradient of a density function, with
applications in pattern recognition. IEEE Transactions on information theory, 21(1):32â€“40, 1975.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259â€“1277.
PMLR, 2020.
Xavier Glorot and Yoshua Bengio. Understanding the difï¬culty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiï¬cial intelligence and
statistics, pp. 249â€“256, 2010.
Daniel H Grollman and Aude G Billard. Robot learning from failed demonstrations. International
Journal of Social Robotics, 4(4):331â€“342, 2012.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565â€“4573, 2016.
Michael Kaiser, Holger Friedrich, and Rudiger Dillmann.
Obtaining good performance from a
bad teacher. In Programming by Demonstration vs. Learning from Examples Workshop at ML,
volume 95, 1995.
Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup. Learning from limited
demonstrations. In Advances in Neural Information Processing Systems, pp. 2859â€“2867, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofï¬‚ine
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-
ment learning, pp. 45â€“73. Springer, 2012.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml,
volume 1, pp. 2, 2000.
Michael Peter Perrone. Improving Regression Estimation: Averaging Methods for Variance Reduc-
tion with Extensions to General Convex Measure Optimization. PhD thesis, Brown University,
1993.
10

Published as a conference paper at ICLR 2021
Dean A Pomerleau. Efï¬cient training of artiï¬cial neural networks for autonomous navigation. Neu-
ral computation, 3(1):88â€“97, 1991.
StÂ´ephane Ross and Drew Bagnell. Efï¬cient reductions for imitation learning. In Proceedings of the
thirteenth international conference on artiï¬cial intelligence and statistics, pp. 661â€“668, 2010.
Fumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi. Sample efï¬cient imitation learning for
continuous control. In International Conference on Learning Representations, 2018.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Voot Tangkaratt, Bo Han, Mohammad Emtiyaz Khan, and Masashi Sugiyama. Vild: Variational
imitation learning with diverse-quality demonstrations. arXiv preprint arXiv:1909.06769, 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026â€“5033.
IEEE, 2012.
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Im-
itation learning from imperfect demonstration. arXiv preprint arXiv:1901.09387, 2019.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433â€“1438. Chicago, IL, USA, 2008.
11

Published as a conference paper at ICLR 2021
A
APPENDIX
A.1
DETAILED DEVIATION OF THEOREM 1
Proof. Let qt = (1 âˆ’Ïµ)t denotes the probability the noisy expert consecutively follows Ï€âˆ—
e in the
ï¬rst t step, and Ï‡ = PT
t=1 qtâˆ’1 denotes sum of qtâˆ’1 over time steps. Then we obtain:
J (Ï€e, R) â‰¥
T
X
t=1
qtâˆ’1RÏ€e
t
+ (1 âˆ’qtâˆ’1) Â· 0
(11)
â‰¥
T
 1
T
T
X
t=1
qtâˆ’1
 1
T
T
X
t=1
RÏ€e
t

(12)
=
Ï‡
T
 T
X
t=1
Esâˆ¼dÏ€e
t

ÏµEÏ€âˆ¼pÎ [RÏ€(s)] + (1 âˆ’Ïµ)RÏ€âˆ—
e (s)

=
Ï‡
T

ÏµEÏ€âˆ¼pÎ [JÏ€e(Ï€, R)] + (1 âˆ’Ïµ)JÏ€e(Ï€âˆ—
e, R)

â‰¥
Ï‡
T

ÏµEÏ€âˆ¼pÎ [JÏ€e(Ï€, R)] + (1 âˆ’Ïµ)EÏ€âˆ¼pÎ [JÏ€e(Ï€, R)]

(13)
=
 1
T
T âˆ’1
X
t=0
(1 âˆ’Ïµ)t

Â· EÏ€âˆ¼pÎ [JÏ€e(Ï€, R)]
The ï¬rst inequality (11) is from Assumption 2 and 3. The second inequality (12) is from Cheby-
shevâ€™s sum inequality with the monotonically decreasing properties according to Assumption 4. The
third inequality (13) is from Assumption 1 : JÎ²(Ï€, R) â‰¤JÎ²(Ï€âˆ—
e, R) for any Ï€, Î² âˆˆÎ  \ {Ï€âˆ—
e}.
A.2
DETAILED DERIVATION OF THE KL DIVERGENCES
From the deï¬nition of (4), we obtain:
Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)]
=
Î±Esâˆ¼dÏ€e
e ,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)]
(14)
+
Î²Esâˆ¼dÏ€e
e+âˆ—,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)]
(15)
The forward Kullback-Leibler (KL) divergence DKL between Ï€e and Ï€Î¸ over a state distribution
dÏ€e is deï¬ned as Esâˆ¼dÏ€e [DKL(Ï€e(Â·|s)||Ï€Î¸(Â·|s))] = âˆ’Esâˆ¼dÏ€e [Eaâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)]+H[Ï€e(Â·|s)]],
where H denotes the entropy. Since H[Ï€e(Â·|s)] always takes positive value and is not associated with
Î¸, we obtain an inequality : Esâˆ¼dÏ€e,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)] â‰¤âˆ’Esâˆ¼dÏ€e [DKL(Ï€e(Â·|s)||Ï€Î¸(Â·|s))]. The
same goes with (14) as
Î±Esâˆ¼dÏ€e
e ,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)] â‰¤âˆ’Î±Esâˆ¼dÏ€e [DKL(Ï€e(Â·|s)||Ï€Î¸(Â·|s))].
(16)
Since Ï€e adopts both Ï€âˆ—
e and Ï€ âˆˆÎ  \ {Ï€âˆ—
e} following the probability Ïµ, the third term (15) can be
expanded as:
Î²Esâˆ¼dÏ€e
e+âˆ—,aâˆ¼Ï€e(Â·|s)[log Ï€Î¸(a|s)] = Î²Esâˆ¼dÏ€e
e+âˆ—

ÏµEÏ€âˆ¼pÎ ,aâˆ¼Ï€(Â·|s)[log Ï€Î¸(a|s)]
+(1 âˆ’Ïµ)Eaâˆ¼Ï€âˆ—e(Â·|s)[log Ï€Î¸(a|s)]
	
â‰¤âˆ’Î²

ÏµEsâˆ¼dÏ€e
e+âˆ—,Ï€âˆ¼pÎ [DKL(Ï€(Â·|s)||Ï€Î¸(Â·|s))]
+(1 âˆ’Ïµ)Esâˆ¼dÏ€e
e+âˆ—[DKL(Ï€âˆ—
e(Â·|s)||Ï€Î¸(Â·|s))]
	
(17)
A.3
DETAILED DESCRIPTION OF THE EXPERIMENTAL SETUP
We annotate conï¬dence scores for the noisy demonstrations so that the conï¬dence is one if the
demonstrations are obtained with Ïµ = 0 otherwise zero. The conï¬dence scores are used IC-GAIL
as well as 2IWIL. We use publicly available code 2 for the implementation of both IC-GAIL and
2https://github.com/kristery/Imitation-Learning-from-Imperfect-Demonstration
12

Published as a conference paper at ICLR 2021
CartPole-v1
Acrobot-v1
MountainCar-v0
LunarLander-v2
0.0
1.0
0.0
1.0
0.0
1.0
0.0
1.0
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
0.0
0.2
0.4
Normalized Cumulative Reward
Probability of Non-Optimal Behavior
Noisy Expert
BC-Ensemble
BC-Single
Ours
Figure 3:
The performance of policies vs. Ïµ given 50000 state-action pairs of the noisy expert
demonstrations where the non-optimal policies Ï€ âˆˆÎ  \ {Ï€âˆ—
e} select actions uniformly at random.
BC-Single is a policy learned by BC. BC-Ensemble is an ensemble of policies, each of which was
learned by BC. Shaded regions indicate the standard deviation over ï¬ve experiments.
2IWIL. We follow the training procedure of both methods as described in Section 5 in (Wu et al.,
2019).
We annotate rankings for the noisy demonstrations so that the smaller Ïµ correspond to higher rank-
ings. Then, we train the learner by T-REX given the ranked demonstration data. We use publicly
available code 3 for the implementation of T-REX.
For training the learner with GAIL and DRIL, we use all noisy demonstrations without any screening
process. We use publicly available code 4 for the implementation of GAIL and DRIL.
A.4
EXPERIMENTAL RESULTS ON DISCRETE CONTROL TASKS
Figure 3 shows the experimental results on four discrete control tasks. Over all tasks, our algorithm
obtain much better results than BC.
A.5
IMPLEMENTATION DETAILS OF OUR ALGORITHM
We implement our algorithm using K neural networks with two hidden layers to represent policies
Ï€Î¸1, Ï€Î¸2, ..., Ï€Î¸K in the ensemble. The input of the networks is vector representations of the state.
Each neural network has 100 hidden units in each hidden layer followed by hyperbolic tangent
nonlinearity, and the dimensionality of its ï¬nal output corresponds to that of action space. The
ï¬nal output is followed by softmax function in the discrete control tasks. As for the continuous
control tasks, the ï¬nal output represents the mean of a Gaussian policy as Ï€Î¸k = N(ÂµÎ¸k(s), Ïƒ2
Î¸k),
where Ïƒ2
Î¸k is implemented as a trainable independent vector from the networks. The neural network
architecture for the policy trained by BC is the same as the ones for a single policy in our algorithm.
We employ Adam (Kingma & Ba, 2014) for learning parameters with a learning rate of Î· âˆ—10âˆ’4
where Î· = K/ PK
k=1 Ï€Î¸k
old(ÂµÎ¸k
old(s)|s) is a scaling parameter. The parameter Î· plays a role in
scaling Ë†R = Ï€Î¸old(a|s) to avoid the training being slow due to Ï€Î¸old(a|s) of small values.
The parameters in all layers are initialized by Xavier initialization (Glorot & Bengio, 2010). The
mini-batch size and the number of training epochs are 128 and 500, respectively.
A.6
ABLATION EXPERIMENTS
We conducted ablation experiments where we evaluate how the number of policies K in the ensem-
ble policy Ï€Î¸ as well as the number of the policies Kold used in the old ensemble policies Ï€Î¸old
affect the performance. Table 2 summarizes the ablation experimental results. Even if our algorithm
uses K = 1 as BC-Single does, the results of our algorithm are better than BC. It indicates that the
3https://github.com/hiwonjoon/ICML2019-TREX
4https://github.com/xkianteb/dril
13

Published as a conference paper at ICLR 2021
weighted action sampling described in 5.2 works to avoid learning the non-optimal policies without
relying on the ensemble approach. The same goes with K = 5. Our algorithm with K = 5 and
Kold = 1 obtain much better performance than BC-Ensemble with K = 5. This result also sup-
ports the weighted action sampling works. The learner performance with ï¬xed K increases as Kold
increases. Similarly, the learner performance with ï¬xed Kold increases as K increases. It suggests
that both K and Kold affect the performance in our algorithm.
Table 2: The performance of policies on the ablation experiment. The number of state-action pairs
of the noisy expert demonstrations is N = 50000. The non-optimal policies Ï€ âˆˆÎ \{Ï€âˆ—
e} is U(0, I).
BC-Single is a policy learned by BC. BC-Ensemble is an ensemble of ï¬ve policies, each of which
was learned by BC. K denotes the number of policies in the ensemble policy Ï€Î¸. Kold denotes the
number of policies used in the old ensemble policy Ï€Î¸old. The mean and standard deviation of the
normalized cumulative rewards over three experiments are described.
Ant-v2
HalfCheetah-v2
Hopper-v2
Walker2d-v2
Average
BC-Single
0.149 Â± 0.001
0.305 Â± 0.006
0.258 Â± 0.017
0.071 Â± 0.004
0.196 Â± 0.105
BC-Ensemble(K = 5)
0.664 Â± 0.043
0.459 Â± 0.014
0.352 Â± 0.028
0.279 Â± 0.039
0.438 Â± 0.167
Ours(K = 1, Kold = 1)
0.272 Â± 0.184
0.505 Â± 0.279
0.405 Â± 0.206
0.281 Â± 0.306
0.366 Â± 0.111
Ours(K = 5, Kold = 1)
0.903 Â± 0.048
1.057 Â± 0.008
0.602 Â± 0.130
0.345 Â± 0.049
0.731 Â± 0.320
Ours(K = 1, Kold = 5)
0.517 Â± 0.015
0.907 Â± 0.007
0.778 Â± 0.093
0.414 Â± 0.085
0.654 Â± 0.227
Ours(K = 5, Kold = 5)
0.995 Â± 0.053
1.058 Â± 0.053
0.573 Â± 0.079
0.364 Â± 0.044
0.747 Â± 0.334
14

