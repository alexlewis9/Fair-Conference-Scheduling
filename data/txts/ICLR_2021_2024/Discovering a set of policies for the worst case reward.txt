Published as a conference paper at ICLR 2021
DISCOVERING A SET OF POLICIES FOR THE WORST
CASE REWARD
Tom Zahavy∗, Andre Barreto, Daniel J Mankowitz, Shaobo Hou, Brendan O’Donoghue,
Iurii Kemaev and Satinder Singh
DeepMind
ABSTRACT
We study the problem of how to construct a set of policies that can be composed
together to solve a collection of reinforcement learning tasks. Each task is a
different reward function deﬁned as a linear combination of known features. We
consider a speciﬁc class of policy compositions which we call set improving
policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition
of the former whose performance is at least as good as that of its constituents
across all the tasks. We focus on the most conservative instantiation of SIPs, set-
max policies (SMPs), so our analysis extends to any SIP. This includes known
policy-composition operators like generalized policy improvement. Our main
contribution is a policy iteration algorithm that builds a set of policies in order to
maximize the worst-case performance of the resulting SMP on the set of tasks.
The algorithm works by successively adding new policies to the set. We show
that the worst-case performance of the resulting SMP strictly improves at each
iteration, and the algorithm only stops when there does not exist a policy that
leads to improved performance. We empirically evaluate our algorithm on a grid
world and also on a set of domains from the DeepMind control suite. We conﬁrm
our theoretical results regarding the monotonically improving performance of our
algorithm. Interestingly, we also show empirically that the sets of policies computed
by the algorithm are diverse, leading to different trajectories in the grid world and
very distinct locomotion skills in the control suite.
1
INTRODUCTION
Reinforcement learning (RL) is concerned with building agents that can learn to act so as to maximize
reward through trial-and-error interaction with the environment. There are several reasons why it can
be useful for an agent to learn about multiple ways of behaving, i.e., learn about multiple policies.
The agent may want to achieve multiple tasks (or subgoals) in a lifelong learning setting and may
learn a separate policy for each task, reusing them as needed when tasks reoccur. The agent may have
a hierarchical architecture in which many policies are learned at a lower level while an upper level
policy learns to combine them in useful ways, such as to accelerate learning on a single task or to
transfer efﬁciently to a new task. Learning about multiple policies in the form of options (Sutton
et al., 1999a) can be a good way to achieve temporal abstraction; again this can be used to quickly
plan good policies for new tasks. In this paper we abstract away from these speciﬁc scenarios and ask
the following question: what set of policies should the agent pre-learn in order to guarantee good
performance under the worst-case reward? A satisfactory answer to this question could be useful in
all the scenarios discussed above and potentially many others.
There are two components to the question above: (i) what policies should be in the set, and (ii)
how to compose a policy to be used on a new task from the policies in the set. To answer (ii),
we propose the concept of a set improving policy (SIP). Given any set of n policies, a SIP is any
composition of these policies whose performance is at least as good as, and generally better than,
that of all of the constituent policies in the set. We present two policy composition (or improvement)
operators that lead to a SIP. The ﬁrst is called set-max policy (SMP). Given a distribution over states,
a SMP chooses from n policies the one that leads to the highest expected value. The second SIP
operator is generalized policy improvement (Barreto et al., 2017, GPI). Given a set of n policies and
their associated action-value functions, GPI is a natural extension of regular policy improvement in
which the agent acts greedily in each state with respect to the maximum over the set of action-values
∗tomzahavy@google.com
1

Published as a conference paper at ICLR 2021
functions. Although SMP provides weaker guarantees than GPI (we will show this below), it is more
amenable to analysis and thus we will use it exclusively for our theoretical results. However, since
SMP’s performance serve as a lower bound to GPI’s, the results we derive for the former also apply
to the latter. In our illustrative experiments we will show this result empirically.
Now that we have ﬁxed the answer to (ii), i.e., how to compose pre-learned policies for a new reward
function, we can leverage it to address (i): what criterion to use to pre-learn the policies. Here, one
can appeal to heuristics such as the ones advocating that the set of pre-learned policies should be as
diverse as possible (Eysenbach et al., 2018; Gregor et al., 2016; Grimm et al., 2019; Hansen et al.,
2019). In this paper we will use the formal criterion of robustness, i.e., we will seek a set of policies
that do as well as possible in the worst-case scenario. Thus, the problem of interest to this paper is as
follows: how to deﬁne and discover a set of n policies that maximize the worst possible performance
of the resulting SMP across all possible tasks? Interestingly, as we will discuss, the solution to this
robustness problem naturally leads to a diverse set of policies.
To solve the problem posed above we make two assumptions: (A1) that tasks differ only in their reward
functions, and (A2) that reward functions are linear combinations of known features. These two
assumptions allow us to leverage the concept of successor features (SFs) and work in apprenticeship
learning. As our main contribution in this paper, we present an algorithm that iteratively builds a set
of policies such that SMP’s performance with respect to the worst case reward provably improves in
each iteration, stopping when no such greedy improvement is possible. We also provide a closed-form
expression to compute the worst-case performance of our algorithm at each iteration. This means
that, given tasks satisfying Assumptions A1 and A2, we are able to provably construct a SIP that can
quickly adapt to any task with guaranteed worst-case performance.
Related Work. The proposed approach has interesting connections with hierarchical RL (HRL) (Sut-
ton et al., 1999b; Dietterich, 2000). We can think of SMP (and GPI) as a higher-level policy-selection
mechanism that is ﬁxed a priori. Under this interpretation, the problem we are solving can be seen as
the deﬁnition and discovery of lower-level policies that will lead to a robust hierarchical agent.
There are interesting parallels between robustness and diversity. For example, diverse stock portfolios
have less risk. In robust least squares (El Ghaoui & Lebret, 1997; Xu et al., 2009), the goal is to ﬁnd
a solution that will perform well with respect to (w.r.t) data perturbations. This leads to a min-max
formulation, and there are known equivalences between solving a robust (min-max) problem and the
diversity of the solution (via regularization) (Xu & Mannor, 2012). Our work is also related to robust
Markov decision processes (MDPs) (Nilim & El Ghaoui, 2005), but our focus is on a different aspect
of the problem. While in robust MDPs the uncertainty is w.r.t the dynamics of the environment, here
we focus on uncertainty w.r.t the reward and assume that the dynamics are ﬁxed. More importantly,
we are interested in the hierarchical aspect of the problem – how to discover and compose a set of
policies. In contrast, solutions to robust MDPs are typically composed of a single policy.
In Apprenticeship Learning (AL; Abbeel & Ng, 2004) the goal is also to solve a min-max problem in
which the agent is expected to perform as well as an expert w.r.t any reward. If we ignore the expert,
AL algorithms can be used to ﬁnd a single policy that performs well w.r.t any reward. The solution
to this problem (when there is no expert) is the policy whose SFs have the smallest possible norm.
When the SFs are in the simplex (as in tabular MDPs) the vector with the smallest ℓ2 norm puts
equal probabilities on its coordinates, and is therefore "diverse" (making an equivalence between the
robust min-max formulation and the diversity perspective). In that sense, our problem can be seen as
a modiﬁed AL setup where: (a) no expert demonstrations are available (b) the agent is allowed to
observe the reward at test time, and (c) the goal is to learn a set of constituent policies.
2
PRELIMINARIES
We will model our problem of interest using a family of Markov Decision Processes (MDPs).
An MDP is a tuple M ≜(S, A, P, r, γ, D), where S is the set of states, A is the set of actions,
P = {P a | a ∈A} is the set of transition kernels, γ ∈[0, 1] is the discount factor and D is the
initial state distribution. The function r : S × A × S 7→R deﬁnes the rewards, and thus the agent’s
objective; here we are interested in multiple reward functions, as we explain next.
Let φ(s, a, s′) ∈[0, 1]d be an observable vector of features (our analysis only requires the features to
be bounded; we use [0, 1] for ease of exposition). We are interested in the set of tasks induced by all
possible linear combinations of the features φ. Speciﬁcally, for any w ∈Rd, we can deﬁne a reward
function rw(s, a, s′) = w · φ(s, a, s′). Given w, the reward rw is well deﬁned and we will use the
terms w and rw interchangeably to refer to the RL task induced by it. Formally, we are interested in
2

Published as a conference paper at ICLR 2021
the following set of MDPs:
Mφ ≜{(S, A, P, rw, γ, D) | w ∈W}.
(1)
In general, W is any convex set, but we will focus on the ℓ2 d-dimensional ball denoted by W = B2.
This choice is not restricting, since the optimal policy in an MDP is invariant with respect to the scale
of the rewards and the ℓ2 ball contains all the directions.
A policy in an MDP M ∈Mφ, denoted by π ∈Π, is a mapping π : S →P(A), where P(A) is the
space of probability distributions over A. For a policy π we deﬁne the successor features (SFs) as
ψπ(s, a) ≜(1 −γ) · E
h X∞
t=0 γtφ(st, at, st+1)|P, π, st = s, at = a
i
.
(2)
The multiplication by 1 −γ together with the fact that the features φ are in [0, 1] assures that
ψπ(s, a) ∈[0, 1]d for all (s, a) ∈S × A.1 We also deﬁne SFs that are conditioned on the initial state
distribution D and the policy π as: ψπ ≜E[ψπ(s, a)|D, π] = Es∼D,a∼π(s)ψπ(s, a). It should be
clear that the SFs are conditioned on D and π whenever they are not written as a function of states
and actions like in Eq. (2). Note that, given a policy π, ψπ is simply a vector in [0, 1]d. Since we will
be dealing with multiple policies, we will use superscripts to refer to them—that is, we use πi to refer
to the i-th policy. To keep the notation simple, we will refer to the SFs of policy πi as ψi. We deﬁne
the action-value function (or Q-function) of policy π under reward rw as
Qπ
w(s, a) ≜(1 −γ)E
h X∞
t=0 γtφ(st, at, st+1) · w|P, π, st = s, at = a
i
= ψπ(s, a) · w.
We deﬁne the value of a policy π as vπ
w ≜(1 −γ)E
h P∞
t=0 γtw · φ(st)|π, P, D
i
= ψπ · w. Note
that vπ
w is a scalar, corresponding to the expected value of policy π under the initial state distribution
D, given by
vπ
w = E[Qπ
w(s, a)|D, π] = Es∼D,a∼π(s)Qπ
w(s, a).
(3)
3
COMPOSING POLICIES TO SOLVE A SET OF MDPS
As described, we are interested in solving all the tasks w ∈W in the set of MDPs Mφ deﬁned in (1).
We will approach this problem by learning policies associated with speciﬁc rewards w and then
composing them to build a higher-level policy that performs well across all the tasks. We call this
higher-level policy a generalized policy, deﬁned as (Barreto et al., 2020):
Deﬁnition 1 (Generalized policy). Given a set of MDPs Mφ, a generalized policy is a function
π : S × W 7→P(A) that maps a state s and a task w onto a distribution over actions.
We can think of a generalized policy as a regular policy parameterized by a task, since for a ﬁxed w
we have π(·; w) : S 7→P(A). We now focus our attention on a speciﬁc class of generalized policies
that are composed of other policies:
Deﬁnition 2 (SIP). Given a set of MDPs Mφ and a set of n policies Πn = {πi}n
i=1, a set improving
policy (SIP) πSIP is any generalized policy such that:
vSIP
Πn,w ≥vi
w for all πi ∈Πn and all w ∈W,
(4)
where vSIP
Πn,w and vi
w are the value functions of πSIP
Πn(·; w) and the policies πi ∈Πn under reward
rw.
We have been deliberately vague about the speciﬁc way the policies πi ∈Πn are combined to form a
SIP to have as inclusive a concept as possible. We now describe two concrete ways to construct a SIP.
Deﬁnition 3 (SMP). Let Πn = {πi}n
i=1 be a set of n policies and let vi be the corresponding value
functions deﬁned analogously to (3) for an arbitrary reward. A set-max policy (SMP) is deﬁned as
πSMP
Πn (s; w) = πk(s), with k = arg
max
i∈[1,...,n] vi
w.
1While we focus on the most common, discounted RL criteria, all of our results will hold in the ﬁnite horizon
and average reward criteria (see, for example, Puterman (1984)). Concretely, in these scenarios there exist
normalizations for the SFs whose effect are equivalent to that of the multiplication by 1 −γ. In the ﬁnite-horizon
case we can simply multiply the SFs by 1/H. In the average reward case, there is no multiplication (Zahavy
et al., 2020b) and the value function is measured under the stationary distribution (instead of D).
3

Published as a conference paper at ICLR 2021
Combining the concepts of SMP and SFs we can build a SIP for Mφ. Given the SFs of the policies
πi ∈Πn, {ψi}n
i=1, we can quickly compute a generalized SMP as
πSMP
Πn (s; w) = πk(s), with k = arg
max
i∈[1,...,n]{w · ψi}.
(5)
Since the value of a SMP under reward w is given by vSMP
Πn,w = maxi∈[1,...,n] vi
w, it trivially qualiﬁes
as a SIP as per Deﬁnition 2. In fact, the generalized policy πSMP
Πn deﬁned in (5) is in some sense the
most conservative SIP possible, as it will always satisfy (4) with equality. This means that any other
SIP will perform at least as well as the SIP induced by SMP. We formalize this notion below:
Lemma 1. Let πSMP
Πn be a SMP deﬁned as in (5) and let π : S × W 7→P(A) be any generalized
policy. Then, given a set of n policies Πn, π is a SIP if and only if vπ
Πn,w ≥vSMP
Πn,w for all w ∈W.
Due to space constraints, all the proofs can be found in the supplementary material. Lemma 1
allows us to use SMP to derive results that apply to all SIPs. For example, a lower bound for vSMP
Πn,w
automatically applies to all possible vSIP
Πn,w. Lemma 1 also allows us to treat SMP as a criterion to
determine whether a given generalized policy qualiﬁes as a SIP. We illustrate this by introducing
a second candidate to construct a SIP called generalized policy improvement (Barreto et al., 2017;
2018; 2020, GPI):
Deﬁnition 4 (GPI policy). Given a set of n policies Πn = {πi}n
i=1 and corresponding Q-functions
Qi
w computed under an arbitrary reward w, the GPI policy is deﬁned as
πGPI
Πn (s; w) = arg max
a
max
i
Qi
w(s, a).
Again, we can combine GPI and SFs to build a generalized policy. Given the SFs of the poli-
cies πi ∈Πn, {ψi}n
i=1, we can quickly compute the generalized GPI policy as πGPI
Πn (s; w) =
arg maxa maxi ψi(s, a) · w. Note that the maximization in GPI is performed in each state and uses
the Q-functions of the constituent policies. In contrast, SMP maximizes over value functions (not
Q-functions), with an expectation over states taken with respect to the initial state distribution D. For
this reason, GPI is a stronger composition than SMP. We now formalize this intuition:
Lemma 2. For any reward w ∈W and any set of policies Πn, we have that vGPI
Πn,w ≥vSMP
Πn,w.
Lemma 2 implies that for any set of policies it is always better to use a GPI policy rather than an
SMP (as we will conﬁrm in the experiments). As a consequence, it also certiﬁes that the generalized
GPI policy πGPI
Πn (s; w) qualiﬁes as a SIP (Lemma 1).
We have described two ways of constructing a SIP by combining SMP and GPI with SFs. Other
similar strategies might be possible, for example by using local SARSA (Russell & Zimdars, 2003;
Sprague & Ballard, 2003) as the basic mechanism to compose a set of value functions. We also note
that in some cases it is possible to deﬁne a generalized policy (Deﬁnition 1), that is not necessarily
a SIP (Eq. (5)), but is guaranteed to perform better than any SIP in expectation. For example, a
combination of maximization, randomization and local search have been shown to be optimal in
expectation among generalized policies in tabular MDPs with collectible rewards (Zahavy et al.,
2020c). That said, we note that some compositions of policies that may at ﬁrst seem like a SIP do not
qualify as such. For example, a mixed policy is a linear (convex) combination of policies that assigns
probabilities to the policies in the set and samples from them. When the mixed policy is mixing the
best policy in the set with a less performant policy then it will result in a policy that is not as good as
the best single policy in the set (Zahavy et al., 2020c).
Problem formulation. We are now ready to formalize the problem we are interested in. Given a set
of MDPs Mφ, as deﬁned in (1), we want to construct a set of n policies Πn = {πi}n
i=1, such that the
performance of the SMP deﬁned on that set πSMP
Πn will have the optimal worst-case performance over
all rewards w ∈W. That is, we want to solve the following problem:
arg max
Πn⊆Π min
w vSMP
Πn,w.
(6)
Note that, since vSMP
Πn,w ≤vSIP
Πn,w for any SIP, Πn and w, as shown in Lemma 1, by ﬁnding a good set
for (6) we are also improving the performance of all SIPs (including GPI).
4

Published as a conference paper at ICLR 2021
4
AN ITERATIVE METHOD TO CONSTRUCT A SET-MAX POLICY
We now present and analyze an iterative algorithm to solve problem (6). We begin by deﬁning the
worst case or adversarial reward associated with the generalized SMP policy:
Deﬁnition 5 (Adversarial reward for an SMP). Given a set of policies Πn, we denote by ¯wSMP
Πn =
arg minw∈B2 vSMP
Πn,w the worst case reward w.r.t the SMP πSMP
Πn deﬁned in (5). In addition, the value
of the SMP w.r.t to ¯wSMP
Πn is deﬁned by ¯vSMP
Πn = minw∈B2 vSMP
Πn,w.
We are interested in ﬁnding a set of policies Πn such that the performance of the resulting SMP will
be optimal w.r.t its adversarial reward ¯wSMP
Πn . This leads to a reformulation of (6) as a max-min-max
optimization for discovering robust policies:
arg max
Πn⊆Π ¯vSMP
Πn = arg max
Πn⊆Π min
w∈B2 vSMP
Πn,w = arg max
Πn⊆Π min
w∈B2
max
i∈[1,..,n] ψi · w.
(7)
Algorithm 1 SMP worst case policy iteration
Initialize: Sample w
∼
N(¯0, ¯1), Π0
←
{ }, π1 ←arg maxπ∈Π w · ψπ, t ←1
¯vSMP
Π1
←−||ψ1||
repeat
Πt ←Πt−1 + {πt}
¯wSMP
Πt
←solution to (8)
πt+1 ←solution of the RL task ¯wSMP
Πt
t ←t + 1
until vt
¯wSMP
Πn ≤¯vSMP
Πt−1
return Πt−1
The order in which the maximizations and the
minimization are performed in (7) is important.
(i) The inner maximization over policies (or
SFs), by the SMP, is performed last. This means
that, for a ﬁxed set of policies Πn and a ﬁxed
reward w, SMP selects the best policy in the set.
(ii) The minimization over rewards w happens
second, that is, for a ﬁxed set of policies Πn,
we compute the value of the generalized SMP
πSMP
Πn (·; w) for any reward w, and then minimize
the maximum of these values. (iii) Finally, for
any set of policies, there is an associated worst
case reward for the SMP, and we are looking for
policies that maximize this value.
The inner maximization (i) is simple: it comes down to computing n dot-products ψi · w, i =
1, 2, . . . , n, and comparing the resulting values. The minimization problem (ii) is slightly more
complicated, but fortunately easy to solve. To see this, note that this problem can be rewritten as:
¯wSMP
Πn = arg min
w∈B2
max
i∈[1,...,n]{w · ψ1, . . . , w · ψn}. s.t. ||w||2 −1 ≤0.
(8)
Eq. (8) is a convex optimization problem that can be easily solved using standard techniques, like
gradient descent, and off-the-shelf solvers (Diamond & Boyd, 2016; Boyd et al., 2004). We note that
the minimizer of Eq. (8) is a function of policy set. As a result, the set forces the worst case reward
to make a trade-off – it has to “choose” the coordinates it “wants” to be more adversarial for. This
trade-off is what encourages the worst case reward to be diverse across iterations (w.r.t different sets).
We note that this property holds since we are optimizing over B2 but it will not necessary be the case
for other convex sets. For example, in the case of B∞the internal minimization problem in the above
has a single solution - a vector with -1 in all of its coordinates.
The outer maximization problem (iii) can be difﬁcult to solve if we are searching over all possible
sets of policies Πn ⊆Π. Instead, we propose an incremental approach in which policies πi are
successively added to an initially empty set Π0. This is possible because the solution ¯wSMP
Πn of (8) gives
rise to a well-deﬁned RL problem in which the rewards are given by rw(s, a, s′) = ¯wSMP
Πn · φ(s, a, s′).
This problem can be solved using any standard RL algorithm. So, once we have a solution ¯wSMP
Πn
for (8), we solve the induced RL problem using any algorithm and add the resulting policy πn+1 to
Πn (or, rather, the associated SFs ψn+1).
Algorithm 1 has a step by step description of the proposed method. The algorithm is initialized by
adding a policy π1 that maximizes a random reward vector w to the set Π0, such that Π1 = {π1}. At
each subsequent iteration t the algorithm computes the worst case reward ¯wSMP
Πt
w.r.t to the current set
Πt by solving (8). The algorithm then ﬁnds a policy πt+1 that solves the task induced by ¯wSMP
Πt . If the
value of πt+1 w.r.t ¯wSMP
Πt
is strictly larger than ¯vSMP
Πt
the algorithm continues for another iteration, with
πt+1 added to the set. Otherwise, the algorithm stops. As mentioned before, the set of policies Πt
computed by Algorithm 1 can also be used with GPI. The resulting GPI policy will do at least as well
as the SMP counterpart on any task w (Lemma 2); in particular, the GPI’s worst-case performance
will be lower bounded by ¯vSMP
Πn .
5

Published as a conference paper at ICLR 2021
4.1
THEORETICAL ANALYSIS
Algorithm 1 produces a sequence of policy sets Π1, Π2, . . . The deﬁnition of SMP guarantees that
enlarging a set of policies always leads to a soft improvement in performance, so ¯vSMP
Πt+1 ≥¯vSMP
Πt
≥
. . . ≥¯vSMP
{π1}. We now show that the improvement in each iteration of our algorithm is in fact strict.
Theorem 1 (Strict improvement). Let Π1, . . . , Πt be the sets of policies constructed by Algorithm 1.
We have that the worst-case performance of the SMP induced by these set is strictly improving in each
iteration, that is: ¯vSMP
Πt+1 > ¯vSMP
Πt . Furthermore, when the algorithm stops, there does not exist a single
policy πt+1 such that adding it to Πt will result in improvement: ∄πt+1 ∈Π s.t. ¯vSMP
Πt+{π} > ¯vSMP
Πt .
In general we cannot say anything about the value of the SMP returned by Algorithm 1. However, in
some special cases we can upper bound it. One such case is when the SFs lie in the simplex.
Lemma 3 (Impossibility result). For the special case where the SFs associated with any policy are
in the simplex, the value of the SMP w.r.t the worst case reward for any set of policies is less than or
equal to −1/
√
d. In addition, there exists an MDP where this upper bound is attainable.
One example where the SFs are in the simplex is when the features φ are “one-hot vectors”, that is,
they only have one nonzero element. This happens for example in a tabular representation, in which
case the SFs correspond to stationary state distributions. Another example are the features induced by
state aggregation, since these are simple indicator functions associating states to clusters (Singh et al.,
1995). We will show in our experiments that when state aggregation is used our algorithm achieves
the upper bound of Lemma 3 in practice.
Finally, we observe that not all the policies in the set Πt are needed at each point in time, and we can
guarantee strict improvement even if we remove the "inactive" policies from Πt, as we show below.
Deﬁnition 6 (Active policies). Given a set of n policies Πn, and an associated worst case reward
¯wSMP
Πn , the subset of active policies Πa(Πn) are the policies in Πn that achieve ¯vSMP
Πn w.r.t ¯wSMP
Πn :
Πa(Πn) =

π ∈Πn : ψπ · ¯wSMP
Πn = ¯vSMP
Πn
	
.
Theorem 2 (Sufﬁciency of Active policies). For any set of policies Πn, πSMP
Πa(Πn) achieves the same
value w.r.t the worst case reward as πSMP
Πn , that is, ¯vSMP
Πn = ¯vSMP
Πa(Πn).
Theorem 2 implies that once we have found ¯wSMP
Πn we can remove the inactive policies from the set
and still guarantee the same worst case performance. Furthermore, we can continue with Algorithm 1
to ﬁnd the next policy by maximizing ¯wSMP
Πn and guarantee strict improvement via Theorem 1. This is
important in applications that have memory constraints, since it allows us to store fewer policies.
5
EXPERIMENTS
We begin with a 10 × 10 grid-world environment (Fig. 1(d)), where the agent starts in a random
place in the grid (marked in a black color) and gains/loses reward from collecting items (marked
with white color). Each item belongs to one of d −1 classes (here with d = 5) and is associated
with a marker: 8, O, X, Y . In addition, there is one "no item" feature (marked in gray color). The
features are one-hot vectors, i.e., for i ∈[1, d −1], φi(s) equals one when item i is in state s and zero
otherwise (similarly φd(s) equals one when there is no item in state s). The objective of the agent is
to pick up the “good” objects and avoid “bad” objects, depending on the weights of the vector w.
In Fig. 1(a) we report the performance of the SMP πSMP
Πt
w.r.t ¯wSMP
Πt
for d = 5. At each iteration
(x-axis) of Algorithm 1 we train a policy for 5 · 105 steps to maximize ¯wSMP
Πt . We then compute the
SFs of that policy using additional 5 · 105 steps and evaluate it w.r.t ¯wSMP
Πt .
As we can see, the performance of SMP strictly improves as we add more policies to the set (as we
stated in Theorem 1). In addition, we compare the performance of SMP with that of GPI, deﬁned on
the same sets of policies (Πt) that were discovered by Algorithm 1. Since we do not know how to
compute ¯wGPI
Πt (the worst case reward for GPI), we evaluate GPI w.r.t ¯wSMP
Πn (the blue line in Fig. 1(a)).
Inspecting Fig. 1(a), we can see that the GPI policy indeed performs better than the SMP as Lemma 2
indicates. We note that the blue line (in Fig. 1(a)) does not correspond to the worst case performance
of the GPI policy. Instead, we can get a good approximation for it because we have that: ¯wSMP
Πn ·
6

Published as a conference paper at ICLR 2021
1
2
3
4
5
# iterations
0.60
0.55
0.50
0.45
Value
SMP
GPI
SMP Upper Bound
(a) SMP vs. GPI
1
2
3
4
5
# iterations
10
4
10
3
10
2
10
1
Upper bound - SMP
Worst case
Orthogonal
Random
(b) Baselines
(c) SFs
(d) Trajectories
Figure 1: Experimental results in a 2D grid world. Fig. 1(a) presents the performance of the SMP and
GPI w.r.t the worst case reward. Fig. 1(b) compares Algorithm 1 with two baselines, where we show
the worst case performance, relative to the upper bound, in a logarithmic scale. Fig. 1(c) visualizes the
SFs of the policies in the set and Fig. 1(d) presents trajectories that were taken by different policies.
ψ(πSMP
Πn ) ≤¯wGPI
Πn ·ψ(πGPI
Πn ) ≤¯wSMP
Πn ·ψ(πGPI
Πn ); i.e., the worst case performance of GPI (in the middle)
is guaranteed to be between the green and blue lines in Fig. 1(a). This also implies that the upper
bound in Lemma 3 does not apply for the blue line.
We also compare our algorithm to two baselines in Fig. 1(b) (for d = 10): (i) Orthogonal - at
iteration t we train policy πt to maximize the reward w = et (a vector of zeroes with a one on the
t-th coordinate) such that a matrix with the vectors w in its columns forms the identity matrix; (ii)
Random: at iteration t we train policy πt to maximize reward w ∼˜N(¯0, ¯1), i.e., we sample a vector
of dimension d from a Normal Gaussian distribution and normalize it to have a norm of 1. While all
the methods improve as we add policies to the set, Algorithm 1 clearly outperforms the baselines.
In Fig. 1(c) and Fig. 1(d) we visualize the policies that were discovered by Algorithm 1. Fig. 1(c)
presents the SFs of the discovered policies, where each row (color) corresponds to a different policy
and the columns correspond to the different features. We do not enumerate the features from 1 to
d, but instead we label them with markers that correspond to speciﬁc items (the x-axis labels). In
Fig. 1(d) we present a trajectory from each policy. We note that both the colors and the markers match
between the two ﬁgures: the red color corresponds to the same policy in both ﬁgures, and the item
markers in Fig. 1(d) correspond to the coordinates in the x-axis of Fig. 1(c).
Inspecting the ﬁgures we can see that the discovered policies are qualitatively diverse: in Fig. 1(c)
we can see that the SFs of the different policies have different weights for different items, and in
Fig. 1(d) we can see that the policies visit different states. For example, we can see that the teal policy
has a larger weight for the no item feature (Fig. 1(c)) and visits only no-item states (Fig. 1(d)) and
that the green policy has higher weights for the ’Y’ and ’X’ items (Fig. 1(c)) and indeed visits them
(Fig. 1(d)).
(a) SMP vs. GPI, d=5
(b) SMP vs. GPI, d=5
Figure 2: Experimental results with regularized w.
Finally, in Fig. 2, we compare the performance of our algorithm with that of the baseline methods
over a test set of rewards. The only difference is in how we evaluate the algorithms. Speciﬁcally, we
sampled 500 reward signals from the uniform distribution over the unit ball. Recall that at iteration
t each algorithm has a set of policies Πt, so we evaluate the SMP deﬁned on this set, πSMP
Πt , w.r.t
each one of the test rewards. Then, for each method, we report the mean value obtained over the
test rewards and repeat this procedure for 10 different seeds. Finally, we report the mean and the
conﬁdence interval over the seeds. Note that the performance in this experiment will necessarily be
better than the in Fig. 1(a) because here we evaluate average performance rather than worst-case
performance. Also note that our algorithm was not designed to optimize the performance on this "test
set", but to optimize the performance w.r.t the worst case. Therefore it is not necessarily expected to
outperform the baselines when measured on this metric.
7

Published as a conference paper at ICLR 2021
Inspecting Figure Fig. 2(a) we can see that our algorithm (denoted by SMP) performs better than the
two baselines. This is a bit surprising for the reasons mentioned above, and suggests that optimising
for the worst case also improves the performance w.r.t the entire distribution (transfer learning result).
At ﬁrst glance, the relative gain in performance might seem small. Therefore, the baselines might
seem preferable to some users due to their simplicity. However, recall that the computational cost
for computing the worst case reward is small compared to ﬁnding the policy the maximizes it, and
therefore the relative cost of the added complexity is low.
The last observation suggests that we should care about how many policies are needed by each
method to achieve the same value. We present these results in Fig. 2(b). Note that we use exactly the
same data as in Fig. 2(a) but present it in a different manner. Inspecting the ﬁgure, we can see that the
baselines require more policies to achieve the same value. For example, to achieve a value of 0.07,
the SMP required 2 policies, while the baselines needed 4; and for a value of 0.1 the SMP required 4
policies while the baselines needed 7 and 9 respectively.
DeepMind Control Suite. Next, we conducted a set of experiments in the DM Control Suite (Tassa
et al., 2018). We focused on the setup where the agent is learning from feature observations cor-
responding to the positions and velocities of the “body” in the task (pixels were only used for
visualization). We considered the following six domains: ’Acrobot’, ’Cheetah’, ’Fish’, ’Hopper’,
’Pendulum’, and ’Walker’. In each of these tasks we do not use the extrinsic reward that is deﬁned by
the task, but instead consider rewards that are linear in the observations (of dimensions 6, 17, 21, 15,
3, and 24 respectively). At each iteration of Algorithm 1 we train a policy for 2 · 106 steps using an
actor-critic (and speciﬁcally STACX (Zahavy et al., 2020d)) to maximize ¯wSMP
Πt , add it to the set, and
compute a new ¯wSMP
Πt+1.
1
2
3
4
5
6
7
8
9
0.7
0.6
0.5
Value
Acrobot
1
2
3
4
5
6
7
8
9
1.2
1.0
0.8
0.6
0.4
Cheetah
1
2
3
4
5
6
7
8
9
1.00
0.75
0.50
0.25
0.00
Fish
1
2
3
4
5
6
7
8
9
# of policies in the set
2.5
2.0
1.5
Value
Hopper
1
2
3
4
5
6
7
8
9
# of policies in the set
0.8
0.6
0.4
0.2
Pendulum
1
2
3
4
5
6
7
8
9
# of policies in the set
1.5
1.0
0.5
Walker
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
# of active policies
0
1
2
3
4
5
0
1
2
3
4
5
0
1
2
3
4
5
# of active policies
(a) Convergence
Explained Variance:1.0
1
2
3
Acrobot
Explained Variance:1.0
1
2
Cheetah
Explained Variance:0.998
1
2
3
4
5
Fish
Explained Variance:0.992
1
2
3
4
Hopper
Explained Variance:1.0
1
2
Pendulum
Explained Variance:0.967
1
2
3
4
5
6
Walker
(b) Diversity
Figure 3: Experimental results in Deepmind Control Suite.
Fig. 3(a) presents the performance of SMP in each iteration w.r.t ¯wSMP
Πt . As we can see, our algorithm
is indeed improving in each iteration. In addition, we present the average number of active policies
(Deﬁnition 6) in each iteration with bars. All the results are averaged over 10 seeds and presented
with 95% Gaussian conﬁdence intervals. Fig. 3(b) presents the SFs of the active policies at the end
of training (the seed with the maximum number of active policies was selected). We perform PCA
dimensionality reduction such that each point in the scatter plot corresponds to the SFs of one of
the active policies. We also report the variance explained by PCA: values close to 1 indicate that the
dimensionality reduction has preserved the original variance. Examining the ﬁgures we can see that
our algorithm is strictly improving (as Theorem 1 predicts) and that the active policies in the set are
indeed diverse; we can also see that adding more policies is correlated with improving performance.
Finally, in Fig. 4(a), Fig. 4(b) and Fig. 4(c) we visualize the trajectories of the discovered policies in
the Cheetah, Hopper and Walker environments. Although the algorithm was oblivious to the extrinsic
reward of the tasks, it was still able to discover different locomotion skills, postures, and even some
"yoga poses" (as noted by the label we gave each policy on the left). The other bodies (Acrobot,
Pendulum and Fish) have simpler bodies and exhibited simpler movement in various directions and
velocities, e.g. the Pendulum learned to balance itself up and down. The supplementary material
contains videos from all the bodies.
6
CONCLUSION
We have presented an algorithm that incrementally builds a set of policies to solve a collection of
tasks deﬁned as linear combinations of known features. The policies returned by our algorithm can
be composed in multiple ways. We have shown that when the composition is a SMP its worst-case
8

Published as a conference paper at ICLR 2021
performance on the set of tasks will strictly improve at each iteration of our algorithm. More generally,
the performance guarantees we have derived also serve as a lower bound for any composition of
policies that qualiﬁes as a SIP. The composition of policies has many applications in RL, such as for
example to build hierarchical agents or to tackle a sequence of tasks in a continual learning scenario.
Our algorithm provides a simple and principled way to build a diverse set of policies that can be used
in these and potentially many other scenarios.
7
ACKNOWLEDGEMENTS
We would like to thank Remi Munos and Will Dabney for their comments and feedback on this paper.
Head Stand
Standing Tall
Walk Backward
Run Forward
(a) Cheetah
Quad Stretch
V Crunch
Plow Pose
Head Stand
(b) Hopper
Kneel
Salta Jump
Pistol Squat
Break-dance
Crawl
(c) Walker
Figure 4: Experimental results in Deepmind Control Suite.
9

Published as a conference paper at ICLR 2021
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-ﬁrst international conference on Machine learning, pp. 1. ACM, 2004.
André Barreto, Will Dabney, Rémi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and
David Silver. Successor features for transfer in reinforcement learning. In Advances in neural
information processing systems, pp. 4055–4065, 2017.
Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel, Daniel Mankowitz,
Augustin Zidek, and Remi Munos. Transfer in deep reinforcement learning using successor features
and generalised policy improvement. In International Conference on Machine Learning, pp. 501–
510. PMLR, 2018.
André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement
learning with generalized policy updates. Proceedings of the National Academy of Sciences, 2020.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.
T. G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition.
Journal of Artiﬁcial Intelligence Research, 13:227–303, 2000.
Laurent El Ghaoui and Hervé Lebret. Robust solutions to least-squares problems with uncertain data.
SIAM Journal on matrix analysis and applications, 18(4):1035–1064, 1997.
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:
Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95–110, 1956.
Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications
to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013.
Karol Gregor, Danilo Jimenez Rezende, and Daan Wierstra. Variational intrinsic control. arXiv
preprint arXiv:1611.07507, 2016.
Christopher Grimm, Irina Higgins, Andre Barreto, Denis Teplyashin, Markus Wulfmeier, Tim Her-
tweck, Raia Hadsell, and Satinder Singh. Disentangled cumulants help successor representations
transfer to new tasks. arXiv preprint arXiv:1911.10866, 2019.
Jacques Guélat and Patrice Marcotte. Some comments on wolfe’s ‘away step’. Mathematical
Programming, 35(1):110–119, 1986.
Steven Hansen, Will Dabney, Andre Barreto, Tom Van de Wiele, David Warde-Farley, and
Volodymyr Mnih. Fast task inference with variational intrinsic successor features. arXiv preprint
arXiv:1906.05030, 2019.
Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. 2013.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798, 2005.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 1984.
Stuart J Russell and Andrew Zimdars. Q-decomposition for reinforcement learning agents. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03), pp. 656–663,
2003.
Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Reinforcement learning with soft state
aggregation. In Advances in neural information processing systems, pp. 361–368, 1995.
Nathan Sprague and Dana Ballard. Multiple-goal reinforcement learning with modular sarsa (0).
2003.
10

Published as a conference paper at ICLR 2021
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for
temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2):181–211, 1999a.
Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a framework
for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112:181–211, August
1999b. doi: http://dx.doi.org/10.1016/S0004-3702(99)00052-1.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018.
Philip Wolfe. Convergence theory in nonlinear programming. Integer and nonlinear programming,
pp. 1–36, 1970.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391–423, 2012.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. In Advances in
neural information processing systems, pp. 1801–1808, 2009.
Tom Zahavy, Alon Cohen, Haim Kaplan, and Yishay Mansour. Apprenticeship learning via frank-
wolfe. AAAI, 2020, 2020a.
Tom Zahavy, Alon Cohen, Haim Kaplan, and Yishay Mansour. Average reward reinforcement
learning with unknown mixing times. The Conference on Uncertainty in Artiﬁcial Intelligence
(UAI), 2020b.
Tom Zahavy, Avinatan Hasidim, Haim Kaplan, and Yishay Mansour. Planning in hierarchical
reinforcement learning: Guarantees for using local policies. In Algorithmic Learning Theory, pp.
906–934, 2020c.
Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado van Hasselt, David
Silver, and Satinder Singh. A self-tuning actor-critic algorithm. Advances in neural information
processing systems, 2020d.
11

Published as a conference paper at ICLR 2021
A
PROOFS
Lemma 1. Let πSMP
Πn be a SMP deﬁned as in (5) and let π : S × W 7→P(A) be any generalized
policy. Then, given a set of n policies Πn, π is a SIP if and only if vπ
Πn,w ≥vSMP
Πn,w for all w ∈W.
Proof. We ﬁrst show that the fact that π is a SIP implies that vπ
Πn,w ≥vSMP
Πn,w for all w. For any
w ∈W, we have
vπ
Πn,w ≥vi
w for all πi ∈Πn
(SIP as in Deﬁnition 2)
≥
max
i∈[1,...,n] vi
w
= vSMP
Πn,w.
We now show the converse:
vπ
Πn,w ≥vSMP
Πn,w
=
max
i∈[1,...,n] vi
w
(SMP as in Deﬁnition 3)
≥vi
w for all πi ∈Πn.
■
Lemma 2. For any reward w ∈W and any set of policies Πn, we have that vGPI
Πn,w ≥vSMP
Πn,w.
Proof. We know from previous results in the literature Barreto et al. (2017) that QGPI(Πn)(s, a) ≥
Qπ(s, a) for all (s, a) ∈S × A and any π ∈Πn.
Thus, we have that ∀s ∈S:
vGPI
Πn,w(s) = QGPI
Πn,w(s, πGPI(s))
≥
max
π∈Πn,a∈A Qπ
w(s, a)
≥max
π∈Πn Ea∼π[Qπ
w(s, a)]
= max
π∈Πn vπ
w(s)
= vSMP
Πn,w(s),
where the second inequality is due to Jensen’s inequality.
Therefore:
vGPI
Πn,w(s) ≥vSMP
Πn,w(s)
ED[vGPI
Πn,w(s)] ≥ED[vSMP
Πn,w(s)]
vGPI
Πn,w ≥vSMP
Πn,w
■
Lemma 3 (Impossibility result). For the special case where the SFs associated with any policy are
in the simplex, the value of the SMP w.r.t the worst case reward for any set of policies is less than or
equal to −1/
√
d. In addition, there exists an MDP where this upper bound is attainable.
12

Published as a conference paper at ICLR 2021
Proof. For the impossibility result. we have that
max
Πn⊆Π min
w∈B2 vSMP
Πn,w = min
w∈B2 max
π∈Π vπ
w
(9)
= min
w∈B2 max
π∈Π ψ(π) · w
≤min
w∈B2
max
ψ∈∆d−1 ψ(π) · w
(10)
= min
w∈B2 max
i
wi
(11)
= −1
√
d
.
(12)
The equality in Eq. (9) follows from the fact that Π is set of all possible policies and therefore the
largest possible subset (the maximizer of the ﬁrst maximization). In that case the second maximization
(by the SMP) is equivalent to selecting the optimal policy in the MDP. Notice that the order of
maximization-minimization here is in the reversed when compared to AL, i.e., for each reward the
SMP chooses the best policy in the MDP, while in AL the reward is chosen to be the worst possible
w.r.t any policy. The inequality in Eq. (10) follows from the fact that we increase the size of the
optimization set in the inner loop, and the equality in Eq. (12) follows from the fact that a maximizer
in the inner loop puts maximal distribution on the largest component of w.
Feasibility. To show the feasibility of the upper bound in the previous impossibility result we give an
example of an MDP in which a set of d policies achieves the upper bound. The d policies are chosen
such that their stationary distributions form an orthogonal basis.
min
w∈B2 vSMP
Πn,w = min
w∈B2
max
ψ∈{ψ1,...,ψd} w · ψ = min
w∈B2
max
ψ∈∆d−1 w · ψ = −1
√
d
,
(13)
which follows from the fact that the maximization over the simplex is equivalent to a maximization
over pure strategies.
■
Lemma 4 (Reformulation of the worst-case reward for an SMP). Let {ψi}n
i=1 be n successor feature
vectors. Let w∗be the adversarial reward, w.r.t the SMP deﬁned given these successor features. That
is, w∗is the solution for
arg min
w
max
i∈[1,...,n]{w · ψ1, . . . , w · ψn}
s.t.
||w||2 −1 ≤0
(14)
Let w∗
i be the solution to the following problem for i ∈[1, . . . , n]:
arg min
w
w · ψi
s.t.
||w||2 −1 ≤0
w · (ψj −ψi) ≤0
(15)
Then, w∗= arg mini w∗
i .
Proof. For any solution w∗to Eq. (8) there is some policy i in the set that is one of its maximizers.
Since it is the maximizer w.r.t w∗, its value w.r.t w∗is bigger or equal to that of any other policy
in the set. Since we are checking the solution among all i ∈[1, . . . , n], one of them must be the
solution.
■
Theorem 2 (Sufﬁciency of Active policies). For any set of policies Πn, πSMP
Πa(Πn) achieves the same
value w.r.t the worst case reward as πSMP
Πn , that is, ¯vSMP
Πn = ¯vSMP
Πa(Πn).
Proof. Let Πn = {πi}n
i=1 . Denote by J a subset of the indices [1, . . . , n] that corresponds to the
indices of the active policies such that Πa(Πn) = {πj}j∈J . We can rewrite problem Eq. (14) as
follows:
minimize
γ
s.t.
γ ≥w · ψi
i = 1, . . . , n
∥w∥2 ≤1.
(16)
13

Published as a conference paper at ICLR 2021
Let (γ⋆, w⋆) be any optimal points. The set of inactive policies i ̸∈J satisfy γ⋆> w⋆· ψi. Since
these constraints are not binding we can drop them from the formulation and maintain the same
optimal objective value, i.e.,
minimize
γ
s.t.
γ ≥w · ψj
j ∈J
∥w∥2 ≤1,
(17)
has the same optimal objective value, ¯vSMP
Πn , as the full problem. This in turn can be rewritten
minimize
maxj∈J w · ψj
s.t.
∥w∥2 ≤1,
(18)
with optimal value ¯vSMP
Πa(Πn), which is therefore equal to ¯vSMP
Πn .
■
Lemma 5 (κ is binding).
Proof. Denote by ˙w a possible solution where the constraint ∥˙w∥2 ≤1 is not binding, i.e., (∥˙w∥2 <
1, ˙κ = 0). In addition, denote the primal objective for ˙w by ˙v = maxi∈[1,...,n]{ ˙w · ψi}. To prove the
lemma, we are going to inspect two cases: (i) ˙v ≥0 and (ii) ˙v < 0. For each of these two cases we
will show that there exists another feasible solution ˜w that achieves a lower value ˜w for the primal
objective ( ˜w < ˙v), and therefore ˙w is not the minimizer.
For the ﬁrst case ˙v ≥0, consider the vector
˜w = (−1, −1, . . . , −1)/
√
d.
˜w is a feasible solution to the problem, since ∥˜w∥2 = 1. Since all the SFs have positive coordinates,
we have that if they are not all exactly 0, then the primal objective evaluated at ˜w is stictly negative:
maxi∈[1,...,n]{ ˜w · ψ1, . . . , ˜w · ψn} < 0.
We now consider the second case of ˙v < 0. Notice that multiplying ˙w by a positive constant c would
not change the maximizer, i.e., arg maxi∈[1,...,n]{c ˙w ·ψi} = arg maxi∈[1,...,n]{ ˙w ·ψi}. Since ˙v < 0,
it means that ˙w/∥˙w∥(c = 1/∥˙w∥) is a feasible solution and a better minimizer than ˙w. Therefore ˙w
is not the minimizer.
We conclude that the constraint κ is always binding, i.e. ∥w∥2 = 1, κ > 0.
■
Theorem 1 (Strict improvement). Let Π1, . . . , Πt be the sets of policies constructed by Algorithm 1.
We have that the worst-case performance of the SMP induced by these set is strictly improving in each
iteration, that is: ¯vSMP
Πt+1 > ¯vSMP
Πt . Furthermore, when the algorithm stops, there does not exist a single
policy πt+1 such that adding it to Πt will result in improvement: ∄πt+1 ∈Π s.t. ¯vSMP
Πt+{π} > ¯vSMP
Πt .
Proof. We have that
vSMP
Πt
= min
w∈B2 max
ψ∈Ψt ψ · w ≤max
ψ∈Ψt ψ · ¯wSMP
Πt+1 ≤
max
ψ∈Ψt+1 ψ · ¯wSMP
Πt+1 = vSMP
Πt+1.
(19)
The ﬁrst inequality is true because we replace the minimization over w with ¯wSMP
Πt+1, and the second
inequality is true because we add a new policy to the set. Thus, we will focus on showing that
the ﬁrst inequality is strict. We do it in two steps. In the ﬁrst step, we will show that the problem
minw∈B2 maxψ∈Ψt ψ · w has a unique solution w⋆
t . Thus, for the ﬁrst inequality to hold with
equality it must be that ¯wSMP
Πt+1 = ¯wSMP
Πt . However, we know that, since the algorithm did not stop,
ψt+1 · ¯wSMP
Πt
> vSMP
Πt , hence a contradiction.
We will now show that minw∈B2 maxψ∈Ψt ψ · w has a unique solution. Before we begin, we refer
the reader to Lemma 4 and Theorem 2 where we reformulate the problem to a form that is simpler to
analyze. We begin by looking at the partial Lagrangian of Eq. (17):
L(w, γ, κ, λ) = γ +
X
j∈J
λj(ψj · w −γ) + κ(∥w∥2 −1).
14

Published as a conference paper at ICLR 2021
The variable κ ≥0 is associated with the constraint ∥w∥2 ≤1. Denote by (λ⋆, κ⋆) any optimal
dual variables and note that by complementary slackness we know that either κ⋆> 0 and ∥w∥2 = 1
or κ⋆= 0 and ∥w∥2 < 1. Lemma 5 above, guarantees that the constraint is in fact binding – only
solutions with κ⋆> 0 and ∥w∥2 = 1 are possible solutions. Notice that this is correct due to the fact
that the SFs have positive coordinates and not all of them are 0 (as in our problem formulation).
Consequently we focus on the case where κ⋆> 0 under which the Lagrangian is strongly convex in
w, and therefore the problem
min
w,γ L(w, γ, λ⋆, κ⋆)
has a unique solution. Every optimizer of the original problem must also minimize the Lagrangian
evaluated at an optimal dual value, and since this minimizer is unique, it implies that the minimizer
of the original problem is unique (Boyd et al., 2004, Sect. 5.5.5).
For the second part of the proof, notice that if the new policy πt+1 does not achieve better reward
w.r.t vSMP
Πt
than the policies in Πt then we have that:
vSMP
Πt+1 = min
w∈B2 max
π∈Πt+1 ψ(π) · w ≤max
π∈Πt+1 ψ(π) · ¯wSMP
Πt
= max
π∈Πt ψ(π) · ¯wSMP
Πt
= vSMP
Πt ;
thus, it is necessary that the policy πt+1 will achieve better reward w.r.t vSMP
Πt
to guarantee strict
improvement.
■
B
AL
In AL there is no reward signal, and the goal is to observe and mimic an expert. The literature on AL
is quite vast and dates back to the work of (Abbeel & Ng, 2004), who proposed a novel framework
for AL. In this setting, an expert demonstrates a set of trajectories that are used to estimate the SFs of
its policy πE, denoted by ψE. The goal is to ﬁnd a policy π, whose SFs are close to this estimate, and
hence will have a similar return with respect to any weight vector w, given by
arg max
π
min
w∈B2 w ·
 ψπ −ψE
= arg max
π
−||ψπ −ψE||
= arg min
π ||ψπ −ψE||.
(20)
The projection algorithm (Abbeel & Ng, 2004) solves this problem in the following manner. The
algorithm starts with an arbitrary policy π0 and computes its feature expectation ψ0. At step t, the
reward function is deﬁned using weight vector wt = ψE −¯ψt−1 and the algorithm ﬁnds a policy
πt that maximizes it, where ¯ψt is a convex combination of SFs of previous (deterministic) policies
¯ψt = Pt
j=1 αjψj. In order to get that ∥¯ψT −ψE∥≤ϵ, the authors show that it sufﬁces to run the
algorithm for T = O(
k
(1−γ)2ϵ2 log(
k
(1−γ)ϵ)) iterations.
Recently, it was shown that this algorithm can be viewed as a Frank-Wolfe method, also known as
the Conditional Gradient (CG) algorithm (Zahavy et al., 2020a). The idea is that solving Eq. (20) can
be seen as a constrained convex optimization problem, where the optimization variable is the SFs, the
objective is convex, and the SFs are constrained to be in the SFs polytope K, given as the following
convex set:
Deﬁnition 7 (The SFs polytope). K =
n
x : Pk+1
i=1 aiψi, ai ≥0, Pk+1
i=1 ai = 1, πi ∈Π
o
.
In general, convex optimization problems can be solved via the more familiar projected gradient
descent algorithm. This algorithm takes a step in the reverse gradient direction zt+1 = xt+αt∇h(xt),
and then projects zt+1 back into K to obtain xt+1. However, in some cases, computing this projection
may be computationally hard. In our case, projecting into K is challenging since it has |A||S| vertices
(feature expectations of deterministic policies). Thus, computing the projection explicitly and then
ﬁnding π whose feature expectations are close to this projection, is computationally prohibitive.
The CG algorithm (Frank & Wolfe, 1956) (Algorithm 2) avoids this projection by ﬁnding a point
yt ∈K that has the largest correlation with the negative gradient. In AL, this step is equivalent to
ﬁnding a policy whose SFs has the maximal inner product with the current gradient, i.e., solve an
MDP whose reward vector w is the negative gradient. This is a standard RL (planning) problem and
15

Published as a conference paper at ICLR 2021
can be solved efﬁciently, for example, with policy iteration. We also know that there exists at least
one optimal deterministic policy for it and that PI will return a solution that is a deterministic policy
(Puterman, 1984).
Algorithm 2 The CG method Frank & Wolfe (1956)
1: Input: a convex set K, a convex function h, learning rate schedule αt.
2: Initiation: let x0 ∈K
3: for t = 1, . . . , T do
4:
yt = arg maxy∈K −∇h(xt−1) · y
5:
xt = (1 −αt)xt−1 + αtyt
6: end for
For smooth functions, CG requires O(1/t2) iterations to ﬁnd an ϵ−optimal solution to Eq. (20). This
gives a logarithmic improvement on the result of (Abbeel & Ng, 2004). In addition, it was shown in
(Zahavy et al., 2020a) that since the optimization objective is strongly convex, and the constraint set
is a polytope, it is possible to use a variant of the CG algorithm, known as Away steps conditional
gradient (ASCG) (Wolfe, 1970). ASCG attains a linear rate of convergence when the set is a polytope
(Guélat & Marcotte, 1986; Garber & Hazan, 2013; Jaggi, 2013), i.e., it converges after O(log(1/ϵ)
iterations. See (Zahavy et al., 2020a) for the exact constants and analysis.
There are some interesting relations between our problem and AL with "no expert", that is, solving
arg min
π ||ψπ||
(21)
In terms of optimization, this problem is equivalent to Eq. (20), and the same algorithms can be used
to solve them.
Both AL with "no expert" and our algorithm can be used to solve the same goal: achieve good
performance w.r.t the worst case reward. However, AL is concerned with ﬁnding a single policy, while
our algorithm is explicitly designed to ﬁnd a set of policies. There is no direct connection between the
policies that are discovered from following these two processes. This is because the intrinsic rewards
that are maximised by each algorithm are essentially different. Another way to think about this is
that since the policy that is returned by AL is a mixed policy, its goal is to return a set of policies
that are similar to the expert, but not diverse from one another. From a geometric perspective, the
policies returned by AL are the nodes of the face in the polytope that is closest to the demonstrated
SFs. Even more concretely, if the SFs of the expert are given exactly (instead of being approximated
from trajectories), then the AL algorithm would return a single vertex (policy). Finally, while a mixed
policy can be viewed as a composition of policies, it is not a SIP. Therefore, it does not encourage
diversity in the set.
C
REGULARIZING W
In this section we experimented with constraining the set of rewards to include only vectors w whose
mean is zero. Since we are using CVXPY (Diamond & Boyd, 2016) to optimize for w (Eq. (8)), this
requires adding a simple constraint Pd
i=1 wi = 0 to the minimization problem. Note that constraining
the mean to be zero does not change the overall problem qualitatively, but it does potentially increase
the difference in the relative magnitude of the elements in w. Since it makes the resulting w’s have
more zero elements, i.e., it makes the w’s more sparse, it can also be viewed as a method to regularize
the worst case reward. Adding this constraint increased the number of w’s (and corresponding
policies) that made a difference to the optimal value (Deﬁnition 5). To see this, note that the green
curve in Fig. 5(a) converges to the optimal value in 2 iterations while the the green curve in Fig. 1(a))
does so in 3 iterations. As a result, the policies that were discovered by the algorithm are more
diverse. To see this observe that the SFs in Fig. 5(b) are more focused on speciﬁc items than the SFs
in Fig. 1(c). In Fig. 5(c) and Fig. 5(d) we veriﬁed that this increased diversity continues to be the case
when we increase the feature dimension d.
16

Published as a conference paper at ICLR 2021
(a) SMP vs. GPI, d=5
(b) SFs, d=5
(c) SMP vs. GPI, d=9
(d) SFs, d=9
Figure 5: Experimental results with regularized w.
17

