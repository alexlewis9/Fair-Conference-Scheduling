Published as a conference paper at ICLR 2021
DECIPHERING AND OPTIMIZING MULTI-TASK
LEARNING: A RANDOM MATRIX APPROACH
Malik Tiomoko
Laboratoire des Signaux et Syst`emes
Universit´e Paris-Sud
Orsay, France
malik.tiomoko@u-psud.fr
Haﬁz Tiomoko Ali
Huawei Technologies Research and Development (UK)
London, UK
hafiz.tiomoko.ali@huawei.com
Romain Couillet
Gipsa Lab
Universit´e Grenoble-Alpes
Saint Martin d’H`eres, France
romain.couillet@gipsa-lab.grenoble-inp.fr
ABSTRACT
This article provides theoretical insights into the inner workings of multi-task and
transfer learning methods, by studying the tractable least-square support vector
machine multi-task learning (LS-SVM MTL) method, in the limit of large (p) and
numerous (n) data. By a random matrix analysis applied to a Gaussian mixture
data model, the performance of MTL LS-SVM is shown to converge, as n, p →∞,
to a deterministic limit involving simple (small-dimensional) statistics of the data.
We prove (i) that the standard MTL LS-SVM algorithm is in general strongly
biased and may dramatically fail (to the point that individual single-task LS-SVMs
may outperform the MTL approach, even for quite resembling tasks): our analysis
provides a simple method to correct these biases, and that we reveal (ii) the
sufﬁcient statistics at play in the method, which can be efﬁciently estimated, even
for quite small datasets. The latter result is exploited to automatically optimize the
hyperparameters without resorting to any cross-validation procedure.
Experiments on popular datasets demonstrate that our improved MTL LS-SVM
method is computationally-efﬁcient and outperforms sometimes much more elabo-
rate state-of-the-art multi-task and transfer learning techniques.
1
INTRODUCTION
The advent of elaborate learning machines capable to surpass human performances on dedicated
tasks has reopened past challenges in machine learning. Transfer learning, and multitask learning
(MTL) in general, by which known tasks are used to help a machine learn other related tasks, is one
of them. The particularly interesting aspects of multi-task learning lie in the possibility (i) to exploit
the resemblance between the datasets associated to each task so the tasks “help each other” and (ii) to
train a machine on a speciﬁc target dataset comprised of few labelled data by exploiting much larger
labelled datasets, however composed of different data. Practical applications are numerous, ranging
from the prediction of student test results for a collection of schools (Aitkin & Longford, 1986), to
survival of patients in different clinics, to the value of many possibly related ﬁnancial indicators
(Allenby & Rossi, 1998), to the preference modelling of individuals in a marketing context, etc.
Since MTL seeks to improve the performance of a task with the help of related tasks, a central issue
to (i) understand the functioning of MTL, (ii) adequately adapt its hyperparameters and eventually
(iii) improve its performances consists in characterizing how MTL relates tasks to one another and in
identifying which features are “transferred”. The article aims to decipher these fundamental aspects
for sufﬁciently general data models.
1

Published as a conference paper at ICLR 2021
Several data models may be accounted for to enforce relatedness between tasks. A common assump-
tion is that the data lie close to each other in a geometrical sense (Evgeniou & Pontil, 2004), live
in a low dimensional manifold (Agarwal et al., 2010), or share a common prior (Daum´e III, 2009).
We follow here the latter assumption in assuming that, for each task, the data arise from a 2-class
Gaussian mixture.1
Methodologically, in its simplest approach, MTL algorithms can be obtained from a mere extension
of support vector machines (SVM), accounting for more than one task. That is, instead of ﬁnding
the hyperplane (through its normal vector ω) best separating the two classes of a unique dataset,
(Evgeniou & Pontil, 2004) proposes to produce best separating hyperplanes (or normal vectors)
ω1, . . . , ωk for each pair of data classes of k tasks, with the additional constraint that the normal
vectors take the form ωi = ω0 + vi for some common vector w0 and dedicated vectors vi. The
amplitude of the vectors vi is controlled (through an additional hyperparameter) to enforce or relax
task relatedness. We study this approach here. Yet, to obtain explicit and thus more insightful
results, we speciﬁcally resort to a least-square SVM (as proposed e.g., in (Xu et al., 2013)) rather
than a margin-based SVM. This only marginally alters the overall behavior of the MTL algorithm
and has no impact on the main insights drawn in the article. Moreover, by a now well-established
universality argument in large dimensional statistics, (Mai et al., 2019) show that quadratic (least-
square) cost functions are asymptotically optimal and uniformly outperform alternative costs (such as
margin-based methods or logistic approaches), even in a classiﬁcation setting. This argument further
motivates the choice of considering ﬁrst and foremost the LS-SVM version of MTL-SVM.
Technically, the article exploits the powerful random matrix theory to study the performance of the
MTL least-square SVM algorithm (MTL LS-SVM) for data arising from a Gaussian mixture model,
assuming the total number n and dimension p of the data are both large, i.e., as n, p →∞with
p/n →c ∈(0, ∞). As such, our work follows after the recent wave of interest into the asymptotics of
machine learning algorithms, such as studied lately in e.g., (Liao & Couillet, 2019; Deng et al., 2019;
Mai & Couillet, 2018; El Karoui et al., 2010). Our analysis reveals the following major conclusions:
• we exhibit the sufﬁcient statistics, which concretely enable task comparison in the MTL
LS-SVM algorithm; we show that, even when data are of large dimensions (p ≫1), these
statistics remain small dimensional (they only scale with the number k of tasks);
• while it is conventional to manually set labels associated to each dataset within {−1, 1},
we prove that this choice is largely suboptimal and may even cause MTL to severely fail
(causing “negative transfer”); we instead provide the optimal values for the labels of each
dataset, which depend on the sought-for objective: these optimal values are furthermore
easily estimated from very few training data (i.e., no cross-validation is needed);
• for unknown new data x, the MTL LS-SVM algorithm allocates a class based on the
comparison of a score g(x) to a threshold ζ, usually set to zero. We prove that, depending
on the statistics and number of elements of the training dataset, a bias is naturally induced
that makes ζ = 0 a largely suboptimal choice in general. We provide a correction for this
bias, which again can be estimated from the training data alone;
• we demonstrate on popular real datasets that our proposed optimized MTL LS-SVM is both
resilient to real data and also manages, despite its not being a best-in-class MTL algorithm,
to rival and sometimes largely outperform competing state-of-the-art algorithms.
These conclusions thus allow for an optimal use of MTL LS-SVM with performance-maximizing
hyperparameters and strong theoretical guarantees. As such, the present article offers through MTL
LS-SVM a viable fully-controlled (even better performing) alternative to state-of-the-art MTL.
Reproducibility. Matlab and Julia codes for reproducing the results of the article are available in the
supplementary materials.
Notation. e[n]
m ∈Rn is the canonical vector of Rn with [e[n]
m ]i = δmi. Moreover, e[2k]
ij
= e[2k]
2(i−1)+j.
Similarly, E[n]
ij
∈Rn×n is the matrix with [E[n]
ij ]ab = δiaδjb. The notations A ⊗B and A ⊙B
1In the supplementary material, we extend this setting to a much broader and more realistic scope, and justify
in passing the relevance of a Gaussian mixture modelling to address multi-task learning with real data.
2

Published as a conference paper at ICLR 2021
for matrices or vectors A, B are respectively the Kronecker and Hadamard products. Dx is the
diagonal matrix containing on its diagonal the elements of the vector x and Ai· is the i-th row
of A. The notation ˚
A is used when a centering operation is performed on the matrix or vector
A. Uppercase calligraphic letters (A, K, Γ, M, V,...) are used for deterministic small dimensional
matrices. Finally, 1m and Im are respectively the vector of all one’s of dimension m and the identity
matrix of dimension m × m. The index pair i, j generally refers to Class j in Task i.
2
RELATED WORKS
Let us ﬁrst point out the difference between MTL and transfer learning: while MTL makes no
distinction between tasks and aims to improve the performance of all tasks, transfer learning aims to
maximize the performance of a target task with the help of all source tasks. Yet, both methods mostly
sharing the same learning process, in this section, we mainly focus on the MTL literature, which is
divided into parameter-based versus feature-based MTL.
In the parameter-based MTL approach, the tasks are assumed to share some parameters (e.g., the
hyperplanes best separating each class) or their hyperparameters have a common prior distribution.
Existing learning methods (SVM, logistic regression, etc.) can then be appropriately modiﬁed to
incorporate these relatedness assumptions. In this context, (Evgeniou & Pontil, 2004; Xu et al.,
2013; Parameswaran & Weinberger, 2010) respectively adapt the SVM, LS-SVM, and Large Margin
Nearest Neighbor (LMNN) algorithms to the MTL paradigm. The present article borrows ideas from
Evgeniou & Pontil (2004); Xu et al. (2013).
In the feature-based MTL approach, the tasks data are instead assumed to share a low-dimensional
common representation. In this context, most of the works aim to determine a mapping of the ambient
data space into a low-dimensional subspace (through sparse coding, deep neural networks, principal
component analysis, etc.) in which the tasks have high similarity (Argyriou et al., 2007; Maurer et al.,
2013; Zhang et al., 2016; Pan et al., 2010); other works simply use a feature selection method by
merely extracting a subset of the original feature space (Obozinski et al., 2006; Wang & Ye, 2015;
Gong et al., 2012). We must insist that, in the present work, our ultimate objective is to study and
improve “data-generic” MTL mechanisms under no structural assumption on the data; this approach
is quite unlike recent works exploiting convolutive techniques in deep neural nets or low dimensional
feature-based methods to perform transfer or multi-task learning mostly for computer vision.
From a theoretical standpoint though, few works have provided a proper understanding of the
various MTL algorithms. To our knowledge, the only such results arise from elementary learning
theory (Rademacher complexity, VC dimension, covering number, stability) and only provide loose
performance bounds (Baxter, 2000; Ben-David & Schuller, 2003; Baxter, 1997). As such, the present
work ﬁlls a long-standing gap in the MTL research.
3
THE MULTI-TASK LEARNING SETTING
Let X ∈Rp×n be a collection of n independent data vectors of dimension p. The data are divided
into k subsets attached to individual “tasks”. Speciﬁcally, letting X = [X1, . . . , Xk], Task i is a
binary classiﬁcation problem from the training samples Xi = [X(1)
i
, X(2)
i
] ∈Rp×ni with X(j)
i
=
[x(j)
i1 , . . . , x(j)
inij] ∈Rp×nij the nij vectors of class j ∈{1, 2} for Task i. In particular, n = Pk
i=1 ni
and ni = ni1 + ni2 for each i ∈{1, . . . , k}.
To each xil ∈Rp of the training set is attached a corresponding “label” (or score) yil ∈R. We denote
yi = [yi1, . . . , yini]T ∈Rni the vector of all labels for Task i, and y = [yT
1 , . . . , yT
k ]T ∈Rn the
vector of all labels. These labels are generally chosen to be ±1 but, for reasons that will become clear
in the course of the article, we voluntarily do not enforce binary labels here.
Before detailing the multitask classiﬁcation scheme, a preliminary task-wise centering operation is
performed on the data, i.e., we consider in the following the datasets
˚
Xi = Xi

Ini −1
ni
1ni1T
ni

,
∀i ∈{1, . . . , k}.
3

Published as a conference paper at ICLR 2021
As such, we systematically work on the labeled datasets ( ˚
X1, y1), . . . , ( ˚
Xk, yk). Remark 1 in the
supplementary material motivates this choice, which avoids extra biases produced by the algorithm.
3.1
THE OPTIMIZATION FRAMEWORK
The multitask learning least square support vector machine (MTL LS-SVM) aims to predict, for
input vectors x ∈Rp not belonging to the training set, their associated score y upon which a decision
on the class allocation of x is taken, for a given target task. To this end, based on the labeled sets
( ˚
X1, y1), . . . , ( ˚
Xk, yk), MTL LS-SVM determines the normal vectors W = [ω1, ω2, . . . , ωk] ∈Rp×k
and intercepts b = [b1, b2, . . . , bk]T ∈Rk deﬁning k separating hyperplanes for the corresponding
k binary classiﬁcation tasks. In order to account for task relatedness, each ωi assumes the form
ωi = ω0 + vi for some common ω0 ∈Rp and task-dedicated vi ∈Rp.
Formally, writing V = [v1, . . . , vk] ∈Rp×k (so that W = ω01⊤
k + V ) and following the work of
(Evgeniou & Pontil, 2004; Xu et al., 2013), the optimization function is given by
min
(ω0,V,b)∈
Rp×Rp×k×Rk
1
2λ∥ω0∥2 + 1
2
k
X
i=1
∥vi∥2
γi
+ 1
2
k
X
i=1
∥ξi∥2,
ξi = yi −( ˚
XT
i ωi + bi1ni),
1 ≤i ≤k.
In this expression, the parameter λ enforces more task relatedness while the parameters γ1, . . . , γk
enforce better classiﬁcation of the data in their respective classes.
Being a quadratic optimization problem under linear equality constraints, ω0, V, b are obtained
explicitly (see details in Section 1 of the supplementary material). The solution is best described
through the expression of the hyperplanes ω1, . . . , ωk ∈Rp which take the form:
ωi =

e[k]T
i
⊗Ip

AZα,
and b = (P TQP)−1P TQy, where α = Q(y−Pb) = Q
1
2 (In −Q
1
2 P(P TQP)−1P TQ
1
2 )Q
1
2 y ∈Rn
is the Lagrangian dual and
Q =
 1
kpZTAZ + In
−1
∈Rn×n,
Z =
k
X
i=1
E[k]
ii ⊗˚
Xi ∈Rpk×n
A =
 Dγ + λ11T
⊗Ip ∈Rkp×kp,
P =
k
X
i=1
E[k]
ii ⊗1ni ∈Rn×k
with γ = [γ1, . . . , γk]T and Dγ = diag(γ).
MTL LS-SVM differs from a single-task joint LS-SVM for all data in that the data ˚
X1, . . . , ˚
Xk are
not treated simultaneously but through k distinct ﬁlters: this explains why Z ∈Rkp×n is not the
mere concatenation [ ˚
X1, . . . , ˚
Xk] but a block-diagonal structure isolating each ˚
Xi. As such, the
˚
Xi’s-relating matrix A plays an important role in the MTL learning process.
With this formulation for the solution (W, b), the prediction of the class of any new data point x ∈Rp
for the target Task i is then obtained from the classiﬁcation score
gi(x) = 1
kp

e[k]
i
⊗˚x
T
AZα + bi
(1)
where ˚x = x −1
ni Xi1ni is a centered version of x with respect to the training dataset for Task i.
3.2
LARGE DIMENSIONAL STATISTICAL MODELLING
The ﬁrst objective of the article is to quantify the MTL performance, and thus of the (a priori intricate)
statistics of gi(x), under a sufﬁciently simple but telling Gaussian mixture model for training and test
data.
Assumption 1 (Distribution of X and x). The columns of [X, x] are independent Gaussian random
variables. Speciﬁcally, the nij samples x(j)
i1 , . . . , x(j)
inij of class j for Task i are independent N(µij, Ip)
vectors, and we let ∆µi ≡µi1 −µi2. As for x, it follows an independent N(µx, Ip) vector.
4

Published as a conference paper at ICLR 2021
In the supplementary material, Assumption 1 is relaxed to [X, x] arising from a generative model
of the type x(j)
il
= hij(z(j)
il ) for z(j)
il
∼N(0, Ip) and hij : Rp →Rp a 1-Lipschitz function. This
model encompasses extremely realistic data models, including data arising from generative networks
(e.g., GANs (Goodfellow et al., 2014)) and is shown in the supplementary material to be universal
in the sense that, as n, p →∞, the asymptotic performances of MTL LS-SVM only depend on the
statistical means and covariances of the x(j)
il : the performances under complex mixtures are thus
proved to coincide with those under an elementary Gaussian mixture. This generalized study however
comes at the expense of more complex deﬁnitions and formulas, which impedes readability; hence
the simpler isotropic Gaussian mixture model here.
Our central technical approach for the performance evaluation of the MTL LS-SVM algorithm
consists in placing ourselves under the large p, n regime of random matrix theory.
Assumption 2 (Growth Rate). As n →∞, n/p →c0 > 0 and, for 1 ≤i ≤k, 1 ≤j ≤2,
nij
n →cij > 0. We let ci = ci1 + ci2, c = [c1, . . . , ck]T ∈Rk.
With these notations and assumptions, we are in position to present our main theoretical results.
4
THE MULTI-TASK LEARNING ANALYSIS
4.1
TECHNICAL STRATEGY AND NOTATIONS
To evaluate the statistics of gi(x) (equation 1), we resort to ﬁnding so-called deterministic equivalents
for the matrices Q, AZQ, etc., which appear at the core of the formulation of gi(x). Those are
provided in Lemma 1 of the supplementary material. Our strategy then consists in “decoupling”
the effect of the data statistics from those of the MTL hyperparameters λ, γ1, . . . , γk. Speciﬁcally,
we extract two fundamental quantities for our analysis: a data-related matrix M ∈R2k×2k and a
hyperparameter matrix A ∈Rk×k:
M =
k
X
i,j=1
∆µT
i ∆µj

E[k]
ij ⊗cicT
j

, ci =


ci2
ci
q
ci1
ci
−ci1
ci
q
ci2
ci

, A =

Ik + D
−1
2
˜∆
 Dγ + λ1k1T
k
−1 D
−1
2
˜∆
−1
where ˜∆= [ ˜∆1, . . . , ˜∆k]T are the unique positive solutions to the implicit system ˜∆i = ci
c0 −Aii
(this is implicit because A is a function of the ˜∆j’s). In passing, it will appear convenient to use the
shortcut notation ¯∆= [ ¯∆11, . . . , ¯∆k2]T ∈R2k where ¯∆ij = cij
ci c0 ˜∆i.
We will see that M plays the role, in the limit of large p, n, of a sufﬁcient statistic for the performance
of the MTL LS-SVM algorithm only involving (i) the data statistics µ11, . . . , µk2 and (ii) the (limiting)
relative number c11/c1, . . . , ck2/ck of elements per class in each task. As for A, it captures the
information about the impact of the hyperparameters λ, γ1, . . . , γk and of the dimension ratios
c1, . . . , ck and c0. These two matrices will be combined in the core matrix Γ ∈R2k×2k of the
upcoming MTL LS-SVM performance analysis, deﬁned as Γ =
 I2k +
 A ⊗121T
2

⊙M
−1
where we recall that ‘⊙’ is the Hadamard (element-wise) matrix product.
We raised in the introduction of Section 3 that we purposely relax the binary “labels” yij associated
to each datum xij in each task to become “scores” yij ∈R. This will have fundamental consequences
to the MTL performance. Yet, since xi1, . . . , xini are i.i.d. data vectors, we impose equal scores
yi1 = . . . = yini within each class. As such, we may reduce the complete score vector y ∈Rn
under the form y =

˜y111T
n11, . . . , ˜yk21T
nk2
T for ˜y = [˜y11, . . . , ˜yk2]T ∈R2k. From Remark 1 of the
supplementary material, the performances of MTL are insensitive to a constant shift in the scores
yi1 and yi2 of every given Task i: as such, the recentered version ˚˜y = [˚˜y11, . . . ,˚˜yk2]T of ˜y, where
˚˜yij = ˜yij −( ni1
ni ˜yi1 + ni2
ni ˜yi2) will be central in the upcoming results.
4.2
MAIN RESULTS
Theorem 1 (Asymptotics of gi(x)). Under Assumptions 1–2, for x ∼N(µx, Ip) with µx = µij,
gi(x) −Gij →0,
Gij ∼N(mij, σ2
i )
5

Published as a conference paper at ICLR 2021
in distribution where, letting m = [m11, . . . , mk2]T,
m = ˜y −D
−1
2
¯∆ΓD
1
2
¯∆˚˜y,
σ2
i = 1
˜∆i
˚˜yTD
1
2
¯∆Γ

D Ki.
c ⊗12 + Vi

ΓD
1
2
¯∆˚˜y
with Vi =
1
c0 (AD c0Ki·
c
+e[k]
i A ⊗121T
2 ) ⊙M and K = [A ⊙A](Ik −D c0
kc [A ⊙A])−1.
As anticipated, the statistics of the classiﬁcation scores gi(x) mainly depend on the data statistics µi′j′
and on the hyperparameters λ and γ1, . . . , γk through the matrix Γ (and more marginally through Vi
and K for the variances).
Since gi(x) has a Gaussian limit centered about mij, the (asymptotic) standard decision for x to be
allocated to Class 1 (x →C1) or Class 2 (x →C2) for Task i is obtained by the “averaged-mean” test
gi(x)
C1≷
C2
1
2 (mi1 + mi2)
(2)
the classiﬁcation error rate ϵi1 ≡P(x →C2|x ∈C1) of which is then
ϵi1 ≡P

gi(x) ≥mi1 + mi2
2
x ∈C1

= Q
mi1 −mi2
2σi

+ o(1)
(3)
with mij, σi as in Theorem 1 and Q(t) =
R ∞
t
e−u2
2 du.
Further comment on Γ is due before moving to practical consequences of Theorem 1. From the
expression (A ⊗121T
2 )−1 ⊙M, we observe that: (i) if λ ≪1, then A is diagonal dominant and thus
“ﬁlters out” in the Hadamard product all off-diagonal entries of M, i.e., all cross-terms ∆µT
i ∆µj for
i ̸= j, therefore refusing to exploit the correlation between tasks; (ii) if instead λ is not small, A may
be developed (using the Sherman-Morrison matrix inverse formulas) as the sum of a diagonal matrix,
which again ﬁlters out the ∆µT
i ∆µj for i ̸= j, and of a rank-one matrix which instead performs
a weighted sum (through the γi’s and the ˜∆i’s) of the entries of M. Speciﬁcally, letting γ−1 =
(γ−1
1 , . . . , γ−1
k )T in the expression of A, we have
 Dγ + λ1k1T
k
−1 = D−1
γ
−
λγ−1γ−1T
1+λ 1
k
Pk
i=1 γ−1
i
. As
such, disregarding the regularization effect of the ˜∆i’s, the off-diagonal ∆µT
i ∆µj entry of M is
weighted with coefﬁcient (γiγj)−1: the impact of the γi’s is thus strongly associated to the relevance
of the correlation between tasks.
A fundamental aspect of Theorem 1 is that it concludes that the performances of the large dimensional
(n, p ≫1) classiﬁcation problem at hand merely boils down to 2k-dimensional statistics, as all
objects deﬁned in the theorem statement are at most of size 2k. More importantly from a practical
perspective, these “sufﬁcient statistics” are easily amenable to fast and efﬁcient estimation: it only
requires a few training samples to estimate all quantities involved in the theorem. This, as a corollary,
lets one envision the possibility of efﬁcient transfer learning methods based on very scarce data
samples as discussed in Remark 1.
Estimating mij and σi not only allows one to anticipate theoretical performances but also enables the
actual estimation of the decision threshold 1
2(mi1 + mi2) in equation 2 and opens the possibility to
largely optimize MTL LS-SVM through an (asymptotically) optimal choice of the training scores
˜y. Indeed, the asymptotics in Theorem 1 depend in an elegant manner on the training data labels
(scores) ˜y. Since the variance σ2
i is independent of the classes, we easily determine the vector ˜y = ˜y⋆
minimizing the misclassiﬁcation probability for Task i as
˜y⋆= arg max
˜y∈R2k
(mi1 −mi2)2
σ2
i
= arg max
˜y∈R2k
∥˜yT(I2k −D
1
2
¯∆ΓD
−1
2
¯∆)(e[2k]
i1
−e[2k]
i2 )∥2
˜yTD
1
2
¯∆Γ(D Ki.
c ⊗12 + Vi)ΓD
1
2
¯∆˜y
the solution of which is explicit
˜y⋆= D
−1
2
¯∆Γ−1H[(A⊗121T
2 ) ⊙M]D
−1
2
¯∆(e[2k]
i1
−e[2k]
i2 ),
H ≡(D Ki.
c ⊗12 + Vi)−1
(4)
with corresponding (asymptotically) optimal classiﬁcation error ϵi1 (equation 3) given by
ϵ⋆
i1 = Q
1
2
q
(e[2k]
i1
−e[2k]
i2 )TG(e[2k]
i1
−e[2k]
i2 )

, G = D
1
2
¯∆[(A⊗121T
2 ) ⊙M]V−1
i
[(A⊗121T
2 ) ⊙M]D
1
2
¯∆.
6

Published as a conference paper at ICLR 2021
The only non-diagonal matrices in equation 4 are Γ and Vi in which M plays the role of a “variance
proﬁle” matrix. In particular, assuming ∆µT
i ∆µℓ= 0 for all ℓ̸= i (i.e., the statistical means of
all tasks are orthogonal to those of Task i), then the two rows and columns of M associated to
Task i are all zero but on the 2 × 2 diagonal block. Therefore, ˜y⋆must be ﬁlled with zero entries
but on its Task i two elements. All other values at the zero entry locations of ˜y⋆(such as the usual
˜y = [1, −1, . . . , 1, −1]T) would be suboptimal and possibly severely detrimental to the classiﬁcation
performance of Task i (not by altering the means mi1, mi2 but by increasing the variance σ2
i ). This
extreme example strongly suggests that, in order to maximize the MTL performance on Task i, one
must impose low scores ˜yjl to all Tasks j strongly different from Task i.
The choice ˜y = [1, −1, . . . , 1, −1]T can also be very detrimental when ∆µT
i ∆µj < 0 for some i, j:
that is, when the mapping of the two classes within each task is reversed (e.g., if Class 1 in Task 1 is
closer to Class 2 than Class 1 in Task 2). In this setting, it is easily seen that ˜y = [1, −1, . . . , 1, −1]T
works against the classiﬁcation and performs much worse than a single-task LS-SVM.
Another interesting conclusion arises from the simpliﬁed setting of equal number of samples per task
and per class, i.e., n11 = . . . = nk2. In this case, ˜y⋆= Γ−1H
 (A ⊗121T
2 ) ⊙M

(e[2k]
i1
−e[2k]
i2 )
in which all matrices are organized in 2 × 2 blocks of equal entries. This immediately implies that
˜y⋆
j1 = −˜y⋆
j2 for all j. So in particular the detection threshold 1
2(mi1 + mi2) of the averaged-mean
test (equation 2) is zero, as conventionally assumed. In all other settings for the njl’s, it is very
unlikely that ˜y⋆
i1 = −˜y⋆
i2 and the optimal decision threshold must also be estimated.
These various conclusions give rise to an improved MTL LS-SVM algorithm. A pseudo-code (Algo-
rithm 1) along with discussions on (i) the estimation of the statistics met in Theorem 1 (Remark 1)
and on (ii) its multi-class extension (Remark 2) are covered next. Matlab and Julia implementations
of Algorithm 1 and its extensions are proposed in the supplementary material.
5
EXPERIMENTS
Our theoretical results (the data-driven optimal tuning of MTL LS-SVM, as well as the anticipation
of classiﬁcation performance) ﬁnd various practical applications and consequences. We exploit them
here in the context of transfer learning, ﬁrst on a binary decision on synthetic data, and then on a
multiclass classiﬁcation on real data.
To this end, and before proceeding to our experimental setup, a few key remarks to practically exploit
Theorem 1 are in order.
Remark 1 (On the estimation of mij and σi). All quantities deﬁned in Theorem 1 are a priori known,
apart from the k2 inner products ∆µT
i ∆µj for 1 ≤i, j ≤k. For these, deﬁne Sil ⊂{1, . . . , nil}
(l = 1, 2) and the corresponding indicator vector jil ∈Rni with [jij]a = δa∈Sij. For i = j, further
let S′
il ⊂{1, . . . , nil} with S′
il ∩Sil = ∅and the corresponding indicator vector j′
il ∈Rni. Then,
the following estimates hold:
∆µT
i ∆µj −
 ji1
|Si1| −ji2
|Si2|
T
XT
i Xj
 jj1
|Sj1| −jj2
|Sj2|

= O

(p min
l∈{1,2}{|Sil|, |Sjl|})−1
2

∆µT
i ∆µi −
 ji1
|Si1| −ji2
|Si2|
T
XT
i Xi
 j′
i1
|S′
i1| −j′
i2
|S′
i2|

= O

(p min
l∈{1,2}{|Sil|, |S′
il|})−1
2

.
Observe in particular that a single sample (two when i = j) per task and per class (|Sil| = 1) is
sufﬁcient to obtain a consistent estimate for all quantities, so long that p is large.
Remark 2 (From binary to multiclass MTL). Accessing the vector m in Theorem 1 allows for
the extension of the MTL framework to a multiclass-per-task MTL by discarding the well known
inherent biases of multiclass SVM. In the context of Li classes for Task i, using a one-versus-
all approach (for each ℓ∈{1, . . . , Li}, one MTL LS-SVM algorithms with Class “1” being the
target class ℓand Class “2” all other classes at once), one needs to access Li pairs of values
(mi1(ℓ), mi2(ℓ)) and, for a new x, decide on the genuine class of x based on the largest value among
gi(x; 1) −mi1(1), . . . , gi(x; Li) −mi1(Li) with gi(x; ℓ) the output score for “Class ℓversus all”.
For simplicity, from Remark 1 in the supplementary material, one may choose smart shift vectors
¯y(ℓ) ∈Rk for the scores ˜y(ℓ) ∈R2k (i.e., replace ˜y(ℓ) by ˜y(ℓ) + ¯y(ℓ) ⊗12) so that mi1(ℓ) = 0 for
each ℓ. Under this shift, the selected class is the one for which gi(x; ℓ) is maximum.
7

Published as a conference paper at ICLR 2021
−1
0
1
0
0.5
1
1.5
2
91.3%
g2(x), ˜y ∈{±1}2k
β = −1
−1
0
1
0
0.5
1
1.5
34.4%
β = 0
−2 −1
0
1
0
0.5
1
5.2%
β = 0.5
−2
0
2
0
0.5
1
1.5
0.3%
β = 1
−4 −2 0
2
4
0
0.2
0.4
0.6
0.4%
g2(x), optimal ˜y = ˜y⋆
−2
0
0
0.2
0.4
0.6
0.8
6.4%
−2 −1
0
1
0
0.5
1
1.5
3.2%
−2
0
2
0
0.5
1
1.5
0.3%
Figure 1: Scores g2(x) [empirical histogram vs. theory in solid lines] for x of Class C1 (red) or Class
C2 (blue) for Task 2 in a 2-task (k = 2) setting of isotropic Gaussian mixtures for: (top) classical
MTL-LSSVM with y ∈{±1} and threshold ζ = 0; (bottom) proposed optimized MTL-LSSVM
with ˜y⋆and estimated threshold ζ; decision thresholds ζ represented in dashed vertical lines; red
numbers are misclassiﬁcation rates; task relatedness with β = 0 for orthogonal tasks, β > 0 for
positively correlated tasks, β < 0 for negatively correlated tasks; p = 100, [c11, c12, c21, c22] =
[0.3, 0.4, 0.1, 0.2], γ = 12, λ = 10. Histograms drawn from 1 000 test samples of each class.
5.1
EFFECT OF INPUT SCORE (LABEL) AND THRESHOLD DECISION CHOICES
In order to support the theoretical insights drawn in the article, our ﬁrst experiment illustrates the
effects of the bias in the decision threshold for gi(x) (in general not centered on zero) and of the
input score (label) optimization ˜y⋆on synthetic data.
Speciﬁcally, MTL-LSSVM is ﬁrst applied to the following two-task (k = 2) setting: for Task 1,
x(j)
1l ∼N((−1)jµ1, Ip) and for Task 2, x(j)
2l ∼N((−1)jµ2, Ip), where µ2 = βµ1 +
p
1 −β2µ⊥
1
and µ⊥
1 is any vector orthogonal to µ1 and β ∈[0, 1]. This setting allows us to tune, through β, the
similarity between tasks. For four different values of β, Figure 1 depicts the distribution of the binary
output scores gi(x) both for the classical MTL-LSSVM (top displays) and for our proposed random
matrix improved scheme, with optimized input labels (bottom displays).
As a ﬁrst remark, note that both theoretical prediction and empirical outputs closely ﬁt for all values
of β, thereby corroborating our theoretical ﬁndings. In practical terms, the ﬁgure supports (i) the
importance to estimate the threshold decision which is non-trivial (not always close to zero) and (ii)
the relevance of an appropriate choice of the input labels to improve the discrimination performance
between both classes, especially when the two tasks are not quite related as shown by the classiﬁcation
error presented in red in the ﬁgure.
5.2
MULTICLASS TRANSFER LEARNING
We next turn to the classical Ofﬁce+Caltech256 (Saenko et al., 2010; Grifﬁn et al., 2007) real data
(images) benchmark for transfer learning, consisting of the 10 categories shared by both datasets. For
fair comparison with previous works, we compare images using p = 4096 VGG features. Half of
the samples of the target is randomly selected for the test data and the accuracy is evaluated over 20
trials. We use here Algorithm 1, the results of which (Proposed) are reported in Table 1 against the
non-optimized LS-SVM (Xu et al., 2013) and alternative state-of-the-art algorithms: MMDT, CDLS
and ILS.3
3MMDT: Max Margin Domain Transform, proposed in (Hoffman et al., 2013), applies a weighted SVM
on a learnt transformation of the source and target. CDLS: Cross-Domain Landmark Selection, proposed in
(Hubert Tsai et al., 2016), derives a domain invariant feature space. ILS: Invariant Latent Space, proposed in
(Herath et al., 2017), learns an invariant latent space to reduce the discrepancy between source and target.
8

Published as a conference paper at ICLR 2021
Algorithm 1 Proposed Multi Task Learning algorithm.
Input: Training samples X = [X1, . . . , Xk] with Xi = [X(1)
i
, . . . , X(Li)
i
], Xℓ
i ∈Rp×niℓand test
data x.
Output: Estimated class ˆℓ∈{1, . . . , Lt} of x for target Task t.
for j = 1 to Lt do
Center and normalize data per task: for all i′ ∈{1, . . . , k},
•
˚
Xi′ ←Xi′

Ini′ −
1
ni′ 1ni′1T
ni′

•
˚
Xi′ ←˚
Xi′/
1
ni′ptr( ˚
Xi′ ˚
XT
i′)
Estimate: Matrix M from Remark 1 and ˜∆, using X(j)
1 , . . . , X(j)
k
as data of class 1 and
X \ {X(j)
1 , . . . , X(j)
k } as data of class 2.
Create scores
˜y⋆(j) = D
−1
2
¯∆Γ−1H[(A⊗121T
2 ) ⊙M]D
−1
2
¯∆(e[2k]
t1
−e[2k]
t2 ).
Estimate and center m = m(j) from Theorem 1 as per Remark 1.
(Optional) Estimate the theoretical classiﬁcation error ϵt (λ, γ) minimize over (λ, γ)2.
Compute classiﬁcation scores gt(x; j) according to equation 1.
end for
Output: ˆℓ= arg maxℓ∈{1,...,Lt} gt(x; ℓ).
Table 1: Classiﬁcation accuracy over Ofﬁce+Caltech256 database.
c(Caltech), w(Webcam),
a(Amazon), d(dslr), for different “Source to target” task pairs (S →T) based on VGG features. Best
score in boldface, second-to-best in italic.
S/T
c →
w
w →
c
c →
a
a →
c
w →
a
a →
d
d →
a
w →
d
c →
d
d →
c
a →
w
d →
w
Mean
score
LSSVM
96.69
89.90
92.90
90.00
93.80
78.70
93.50
95.00
85.00
90.20
94.70
100
91.70
MMDT
93.90
87.05
90.83
84.40
94.17
86.25
94.58
97.50
86.25
87.23
92.05
97.35
90.96
ILS
77.89
73.55
86.85
76.22
86.22
71.34
74.53
82.80
68.15
63.49
78.98
92.88
77.74
CDLS
97.60
88.30
93.54
88.30
93.54
92.50
93.54
93.75
93.75
88.30
97.35
96.70
93.10
Ours
98.68
89.90
94.40
90.60
94.40
93.80
94.20
100
92.50
89.90
98.70
99.30
94.70
Table 1 demonstrates that our proposed improved MTL LS-SVM, despite its simplicity and unlike
the competing methods used for comparison, has stable performances and is quite competitive. We
further recall that, in additions to these high levels of performance, the method comes along with
theoretical guarantees, which none of the competing works are able to provide.
6
CONCLUSION: BEYOND MTL
Through the example of transfer learning (and more generally multitask learning), we have demon-
strated the capability of random matrix theory to (i) predict and improve the performance of machine
learning algorithms and, most importantly, to (ii) turn simplistic (and in theory largely suboptimal)
methods, such as here LS-SVM, into competitive state-of-the-art algorithms. As Gaussian mixtures
are quite “universal” and thus already appropriate to handle real data (as shown in supplementary
material), one may surmise the optimality of the least square approach, thereby opening the possibility
to prove that MTL LS-SVM is likely close-to-optimal even in real data settings.
This is yet merely a ﬁrst step into a generalized use of random matrix theory and large dimensional
statistics to devise much-needed low computational cost and explainable, yet highly competitive,
machine learning methods from elementary optimization schemes.
ACKNOWLEDGMENTS
Couillet’s work is partially supported by MIAI at University Grenoble-Alpes (ANR-19-P3IA-0003)
and the HUAWEI LarDist project.
9

Published as a conference paper at ICLR 2021
REFERENCES
Arvind Agarwal, Samuel Gerber, and Hal Daume. Learning multiple tasks using manifold regulariza-
tion. In Advances in neural information processing systems, pp. 46–54, 2010.
Murray Aitkin and Nicholas Longford. Statistical modelling issues in school effectiveness studies.
Journal of the Royal Statistical Society: Series A (General), 149(1):1–26, 1986.
Greg M Allenby and Peter E Rossi. Marketing models of consumer heterogeneity. Journal of
econometrics, 89(1-2):57–78, 1998.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In
Advances in neural information processing systems, pp. 41–48, 2007.
Jonathan Baxter. A bayesian/information theoretic model of learning to learn via multiple task
sampling. Machine learning, 28(1):7–39, 1997.
Jonathan Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence research, 12:
149–198, 2000.
Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Learning
Theory and Kernel Machines, pp. 567–580. Springer, 2003.
Hal Daum´e III. Bayesian multitask learning with latent hierarchies. arXiv preprint arXiv:0907.0783,
2009.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensional binary linear classiﬁcation. arXiv preprint arXiv:1911.05822, 2019.
Noureddine El Karoui et al. On information plus noise kernel random matrices. The Annals of
Statistics, 38(5):3191–3216, 2010.
Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In Proceedings of
the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
109–117. ACM, 2004.
Pinghua Gong, Jieping Ye, and Chang-shui Zhang. Multi-stage multi-task feature learning. In
Advances in neural information processing systems, pp. 1988–1996, 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672–2680, 2014.
Gregory Grifﬁn, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. technical
report, 2007.
Samitha Herath, Mehrtash Harandi, and Fatih Porikli. Learning an invariant hilbert space for domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3845–3854, 2017.
Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efﬁcient learning of
domain-invariant image representations. arXiv preprint arXiv:1301.3224, 2013.
Yao-Hung Hubert Tsai, Yi-Ren Yeh, and Yu-Chiang Frank Wang. Learning cross-domain landmarks
for heterogeneous domain adaptation. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 5081–5090, 2016.
Zhenyu Liao and Romain Couillet. A large dimensional analysis of least squares support vector
machines. IEEE Transactions on Signal Processing, 67(4):1065–1074, 2019.
Xiaoyi Mai and Romain Couillet. A random matrix analysis and improvement of semi-supervised
learning for large dimensional data. The Journal of Machine Learning Research, 19(1):3074–3100,
2018.
10

Published as a conference paper at ICLR 2021
Xiaoyi Mai, Zhenyu Liao, and Romain Couillet. A large scale analysis of logistic regression:
Asymptotic performance and new insights. In ICASSP 2019-2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 3357–3361. IEEE, 2019.
Andreas Maurer, Massi Pontil, and Bernardino Romera-Paredes. Sparse coding for multitask and
transfer learning. In International conference on machine learning, pp. 343–351, 2013.
Guillaume Obozinski, Ben Taskar, and Michael Jordan. Multi-task feature selection. Statistics
Department, UC Berkeley, Tech. Rep, 2(2.2):2, 2006.
Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2010.
Shibin Parameswaran and Kilian Q Weinberger. Large margin multi-task metric learning. In Advances
in neural information processing systems, pp. 1867–1875, 2010.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In European conference on computer vision, pp. 213–226. Springer, 2010.
Jie Wang and Jieping Ye. Safe screening for multi-task feature learning with multiple data matrices.
arXiv preprint arXiv:1505.04073, 2015.
Shuo Xu, Xin An, Xiaodong Qiao, Lijun Zhu, and Lin Li. Multi-output least-squares support vector
regression machines. Pattern Recognition Letters, 34:1078–1084, 07 2013. doi: 10.1016/j.patrec.
2013.01.015.
Wenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, and Shuiwang Ji. Deep
model based transfer and multi-task learning for biological image analysis. IEEE transactions on
Big Data, 2016.
11

