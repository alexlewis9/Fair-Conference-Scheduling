Published as a conference paper at ICLR 2024
ACCELERATING
DISTRIBUTED
STOCHASTIC
OPTI-
MIZATION VIA SELF-REPELLENT RANDOM WALKS
Jie Hu∗1, Vishwaraj Doshi∗2, Do Young Eun1
1North Carolina State University, 2IQVIA Inc.
{jhu29,dyeun}@ncsu.edu, vishwaraj.doshi@iqvia.com
ABSTRACT
We study a family of distributed stochastic optimization algorithms where gradi-
ents are sampled by a token traversing a network of agents in random-walk fash-
ion. Typically, these random-walks are chosen to be Markov chains that asymp-
totically sample from a desired target distribution, and play a critical role in the
convergence of the optimization iterates. In this paper, we take a novel approach
by replacing the standard linear Markovian token by one which follows a non-
linear Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined
for any given ‘base’ Markov chain, the SRRW, parameterized by a positive scalar
α, is less likely to transition to states that were highly visited in the past, thus
the name. In the context of MCMC sampling on a graph, a recent breakthrough in
Doshi et al. (2023) shows that the SRRW achieves O(1/α) decrease in the asymp-
totic variance for sampling. We propose the use of a ‘generalized’ version of the
SRRW to drive token algorithms for distributed stochastic optimization in the form
of stochastic approximation, termed SA-SRRW. We prove that the optimization it-
erate errors of the resulting SA-SRRW converge to zero almost surely and prove a
central limit theorem, deriving the explicit form of the resulting asymptotic covari-
ance matrix corresponding to iterate errors. This asymptotic covariance is always
smaller than that of an algorithm driven by the base Markov chain and decreases
at rate O(1/α2) - the performance benefit of using SRRW thereby amplified in the
stochastic optimization context. Empirical results support our theoretical findings.
1
INTRODUCTION
Stochastic optimization algorithms solve optimization problems of the form
θ∗∈arg min
θ∈Rd
f(θ),
where f(θ) ≜EX∼µ [F(θ, X)] =
X
i∈N
µiF(θ, i),
(1)
with the objective function f : Rd →R and X taking values in a finite state space N with distri-
bution µ ≜[µi]i∈N . Leveraging partial gradient information per iteration, these algorithms have
been recognized for their scalability and efficiency with large datasets (Bottou et al., 2018; Even,
2023). For any given noise sequence {Xn}n≥0 ⊂N, and step size sequence {βn}n≥0 ⊂R+, most
stochastic optimization algorithms can be classified as stochastic approximations (SA) of the form
θn+1 = θn + βn+1H(θn, Xn+1),
∀n ≥0,
(2)
where, roughly speaking, H(θ, i) contains gradient information ∇θF(θ, i), such that θ∗solves
h(θ) ≜EX∼µ[H(θ, X)] = P
i∈N µiH(θ, i) = 0. Such SA iterations include the well-known
stochastic gradient descent (SGD), stochastic heavy ball (SHB) (Gadat et al., 2018; Li et al., 2022),
and some SGD-type algorithms employing additional auxiliary variables (Barakat et al., 2021).1
These algorithms typically have the stochastic noise term Xn generated by i.i.d. random variables
with probability distribution µ in each iteration. In this paper, we study a stochastic optimization
algorithm where the noise sequence governing access to the gradient information is generated from
general stochastic processes in place of i.i.d. random variables.
*Equal contributors.
1Further illustrations of stochastic optimization algorithms of the form (2) are deferred to Appendix A.
1

Published as a conference paper at ICLR 2024
This is commonly the case in distributed learning, where {Xn} is a (typically Markovian) random
walk, and should asymptotically be able to sample the gradients from the desired probability dis-
tribution µ. This is equivalent to saying that the random walker’s empirical distribution converges
to µ almost surely (a.s.); that is, xn ≜
1
n+1
Pn
k=0 δXk
a.s.
−−−−→
n→∞µ for any initial X0 ∈N, where
δXk is the delta measure whose Xk’th entry is one, the rest being zero. Such convergence is most
commonly achieved by employing the Metropolis Hastings random walk (MHRW) which can be
designed to sample from any target measure µ and implemented in a scalable manner (Sun et al.,
2018). Unsurprisingly, convergence characteristics of the employed Markov chain affect that of the
SA sequence (2), and appear in both finite-time and asymptotic analyses. Finite-time bounds typ-
ically involve the second largest eigenvalue in modulus (SLEM) of the Markov chain’s transition
kernel P, which is critically connected to the mixing time of a Markov chain (Levin & Peres, 2017);
whereas asymptotic results such as central limit theorems (CLT) involve asymptotic covariance ma-
trices that embed information regarding the entire spectrum of P, i.e., all eigenvalues as well as
eigenvectors (Br´emaud, 2013), which are key to understanding the sampling efficiency of a Markov
chain. Thus, the choice of random walker can significantly impact the performance of (2), and sim-
ply ensuring that it samples from µ asymptotically is not enough to achieve optimal algorithmic
performance. In this paper, we take a closer look at the distributed stochastic optimization problem
through the lens of a non-linear Markov chain, known as the Self Repellent Random Walk (SRRW),
which was shown in Doshi et al. (2023) to achieve asymptotically minimal sampling variance for
large values of α, a positive scalar controlling the strength of the random walker’s self-repellence
behaviour. Our proposed modification of (2) can be implemented within the settings of decentral-
ized learning applications in a scalable manner, while also enjoying significant performance benefit
over distributed stochastic optimization algorithms driven by vanilla Markov chains.
Token Algorithms for Decentralized Learning. In decentralized learning, agents like smartphones
or IoT devices, each containing a subset of data, collaboratively train models on a graph G(N, E) by
sharing information locally without a central server (McMahan et al., 2017). In this setup, N =|N|
agents correspond to nodes i ∈N, and an edge (i, j) ∈E indicates direct communication between
agents i and j. This decentralized approach offers several advantages compared to the traditional
centralized learning setting, promoting data privacy and security by eliminating the need for raw data
to be aggregated centrally and thus reducing the risk of data breach or misuse (Bottou et al., 2018;
Nedic, 2020). Additionally, decentralized approaches are more scalable and can handle vast amounts
of heterogeneous data from distributed agents without overwhelming a central server, alleviating
concerns about single point of failure (Vogels et al., 2021).
Among decentralized learning approaches, the class of ‘Token’ algorithms can be expressed as
stochastic approximation iterations of the type (2), wherein the sequence {Xn} is realized as the
sample path of a token that stochastically traverses the graph G, carrying with it the iterate θn for any
time n ≥0 and allowing each visited node (agent) to incrementally update θn using locally available
gradient information. Token algorithms have gained popularity in recent years (Hu et al., 2022; Tri-
astcyn et al., 2022; Hendrikx, 2023), and are provably more communication efficient (Even, 2023)
when compared to consensus-based algorithms - another popular approach for solving distributed
optimization problems (Boyd et al., 2006; Morral et al., 2017; Olshevsky, 2022). The construction
of token algorithms means that they do not suffer from expensive costs of synchronization and com-
munication that are typical of consensus-based approaches, where all agents (or a subset of agents
selected by a coordinator (Boyd et al., 2006; Wang et al., 2019)) on the graph are required to take
simultaneous actions, such as communicating on the graph at each iteration. While decentralized
Federated learning has indeed helped mitigate the communication overhead by processing multiple
SGD iterations prior to each aggregation (Lalitha et al., 2018; Ye et al., 2022; Chellapandi et al.,
2023), they still cannot overcome challenges such as synchronization and straggler issues.
Self Repellent Random Walk. As mentioned earlier, sample paths {Xn} of token algorithms are
usually generated using Markov chains with µ ∈Int(Σ) as their limiting distribution. Here, Σ
denotes the N-dimensional probability simplex, with Int(Σ) representing its interior. A recent work
by Doshi et al. (2023) pioneers the use of non-linear Markov chains to, in some sense, improve upon
any given time-reversible Markov chain with transition kernel P whose stationary distribution is µ.
2

Published as a conference paper at ICLR 2024
They show that the non-linear transition kernel2 K[·] : Int(Σ) →[0, 1]N×N, given by
Kij[x] ≜
Pij(xj/µj)−α
P
k∈N Pik(xk/µk)−α ,
∀i, j ∈N,
(3)
for any x ∈Int(Σ), when simulated as a self-interacting random walk (Del Moral & Miclo, 2006;
Del Moral & Doucet, 2010), can achieve smaller asymptotic variance than the base Markov chain
when sampling over a graph G, for all α > 0. The argument x for the kernel K[x] is taken to
be the empirical distribution xn at each time step n ≥0. For instance, if node j has been visited
more often than other nodes so far, the entry xj becomes larger (than target value µj), resulting in
a smaller transition probability from i to j under K[x] in (3) compared to Pij. This ensures that
a random walker prioritizes more seldom visited nodes in the process, and is thus ‘self-repellent’.
This effect is made more drastic by increasing α, and leads to asymptotically near-zero variance at
a rate of O(1/α). Moreover, the polynomial function (xi/µi)−α chosen to encode self-repellent
behaviour is shown in Doshi et al. (2023) to be the only one that allows the SRRW to inherit the so-
called ‘scale-invariance’ property of the underlying Markov chain – a necessary component for the
scalable implementation of a random walker over a large network without requiring knowledge of
any graph-related global constants. Conclusively, such attributes render SRRW especially suitable
for distributed optimization.3
Effect of Stochastic Noise - Finite time and Asymptotic Approaches. Most contemporary token
algorithms driven by Markov chains are analyzed using the finite-time bounds approach for obtain-
ing insights into their convergence rates (Sun et al., 2018; Doan et al., 2019; 2020; Triastcyn et al.,
2022; Hendrikx, 2023). However, as also explained in Even (2023), in most cases these bounds are
overly dependent on mixing time properties of the specific Markov chain employed therein. This
makes them largely ineffective in capturing the exact contribution of the underlying random walk
in a manner which is qualitative enough to be used for algorithm design; and performance enhance-
ments are typically achieved via application of techniques such as variance reduction (Defazio et al.,
2014; Schmidt et al., 2017), momentum/Nesterov’s acceleration (Gadat et al., 2018; Li et al., 2022),
adaptive step size (Kingma & Ba, 2015; Reddi et al., 2018), which work by modifying the algorithm
iterations themselves, and never consider potential improvements to the stochastic input itself.
Complement to finite-time approaches, asymptotic analysis using CLT has proven to be an excellent
tool to approach the design of stochastic algorithms (Hu et al., 2022; Devraj & Meyn, 2017; Morral
et al., 2017; Chen et al., 2020a; Mou et al., 2020; Devraj & Meyn, 2021). Hu et al. (2022) shows
how asymptotic analysis can be used to compare the performance of SGD algorithms for various
stochastic inputs using their notion of efficiency ordering, and, as mentioned in Devraj & Meyn
(2017), the asymptotic benefits from minimizing the limiting covariance matrix are known to be a
good predictor of finite-time algorithmic performance, also observed empirically in Section 4.
From the perspective of both finite-time analysis as well as asymptotic analysis of token algorithms,
it is now well established that employing ‘better’ Markov chains can enhance the performance of
stochastic optimization algorithm. For instance, Markov chains with smaller SLEMs yield tighter
finite-time upper bounds (Sun et al., 2018; Ayache & El Rouayheb, 2021; Even, 2023). Similarly,
Markov chains with smaller asymptotic variance for MCMC sampling tasks also provide better
performance, resulting in smaller covariance matrix of SGD algorithms (Hu et al., 2022). Therefore,
with these breakthrough results via SRRW achieving near-zero sampling variance, it is within reason
to ask: Can we achieve near-zero variance in distributed stochastic optimization driven by SRRW-
like token algorithms on any general graph?4 In this paper, we answer in the affirmative.
SRRW Driven Algorithm and Analysis Approach. For any ergodic time-reversible Markov chain
with transition probability matrix P ≜[Pij]i,j∈N and stationary distribution µ ∈Int(Σ), we con-
sider a general step size version of the SRRW stochastic process analysed in Doshi et al. (2023) and
2Here, non-linearity in the transition kernel implies that K[x] takes probability distribution x as the argu-
ment (Andrieu et al., 2007), as opposed to the kernel being a linear operator K[x] = P for a constant stochastic
matrix P in a standard (linear) Markovian setting.
3Recently, Guo et al. (2020) introduce an optimization scheme, which designs self-repellence into the per-
turbation of the gradient descent iterates (Jin et al., 2017; 2018; 2021) with the goal of escaping saddle points.
This notion of self-repellence is distinct from the SRRW, which is a probability kernel designed specifically for
a token to sample from a target distribution µ over a set of nodes on an arbitrary graph.
4This near-zero sampling variance implies a significantly smaller variance than even an i.i.d. sampling
counterpart, while adhering to graph topological constraints of token algorithms.
3

Published as a conference paper at ICLR 2024
Stochastic 
Optimization 
Algorithm
Asymptotic Covariance 𝑽𝜃
High Variance
Near-Zero Variance (Our Result)
𝜃𝑛+1 = 𝜃𝑛+ 𝛽𝑛+1𝐻𝜃𝑛, 𝑋𝑛+1
𝛽𝑛
−1/2 𝜃𝑛−𝜃∗՜
𝑑𝑁(0, 𝑽𝜃)
Nonlinear MC (SRRW [Doshi et al. 2023])
Traditional MC, e.g., MHRW
1
2
4
1
1
4
1
2
4
1
1
4
Token’s trajectory 𝑋𝑛𝑛≥0
?
Figure 1: Visualization of token algorithms using SRRW versus traditional MC in distributed learn-
ing. Our CLT analysis, extended from SRRW itself to distributed stochastic approximation, leads to
near-zero variance for the SA iteration θn. Node numbers on the left denote visit counts.
use it to drive the noise sequence in (2). Our SA-SRRW algorithm is as follows:
Draw:
Xn+1 ∼KXn,·[xn]
(4a)
Update:
xn+1 = xn + γn+1(δXn+1 −xn),
(4b)
θn+1 = θn + βn+1H(θn, Xn+1),
(4c)
where {βn} and {γn} are step size sequences decreasing to zero, and K[x] is the SRRW kernel in
(3). Current non-asymptotic analyses require globally Lipschitz mean field function (Chen et al.,
2020b; Doan, 2021; Zeng et al., 2021; Even, 2023) and is thus inapplicable to SA-SRRW since
the mean field function of the SRRW iterates (4b) is only locally Lipschitz (details deferred to
Appendix B). Instead, we successfully obtain non-trivial results by taking an asymptotic CLT-based
approach to analyze (4). This goes beyond just analyzing the asymptotic sampling covariance5 as
in Doshi et al. (2023), the result therein forming a special case of ours by setting γn =1/(n+1) and
considering only (4a) and (4b), that is, in the absence of optimization iteration (4c). Specifically,
we capture the effect of SRRW’s hyper-parameter α on the asymptotic speed of convergence of the
optimization error term θn −θ∗to zero via explicit deduction of its asymptotic covariance matrix.
See Figure 1 for illustration.
Our Contributions.
1. Given any time-reversible ‘base’ Markov chain with transition kernel P and stationary distribution
µ, we generalize first and second order convergence results of xn to target measure µ (Theorems
4.1 and 4.2 in Doshi et al., 2023) to a class of weighted empirical measures, through the use of more
general step sizes γn. This includes showing that the asymptotic sampling covariance terms decrease
to zero at rate O(1/α), thus quantifying the effect of self-repellent on xn. Our generalization is not
for the sake thereof and is shown in Section 3 to be crucial for the design of step sizes βn, γn.
2. Building upon the convergence results for iterates xn, we analyze the algorithm (4) driven by the
SRRW kernel in (3) with step sizes βn and γn separated into three disjoint cases:
(i) βn = o(γn), and we say that θn is on the slower timescale compared to xn;
(ii) βn =γn, and we say that θn and xn are on the same timescale;
(iii) γn = o(βn), and we say that θn is on the faster timescale compared to xn.
For any α ≥0 and let k = 1, 2 and 3 refer to the corresponding cases (i), (ii) and (iii), we show that
θn
a.s.
−−−−→
n→∞θ∗
and
(θn −θ∗)/
p
βn
dist.
−−−−→
n→∞N

0, V(k)
θ (α)

,
featuring distinct asymptotic covariance matrices V(1)
θ (α), V(2)
θ (α) and V(3)
θ (α), respectively. The
three matrices coincide when α = 0,6. Moreover, the derivation of the CLT for cases (i) and (iii),
for which (4) corresponds to two-timescale SA with controlled Markov noise, is the first of its kind
and thus a key technical contribution in this paper, as expanded upon in Section 3.
3. For case (i), we show that V(1)
θ (α) decreases to zero (in the sense of Loewner ordering introduced
in Section 2.1) as α increases, with rate O(1/α2). This is especially surprising, since the asymptotic
performance benefit from using the SRRW kernel with α in (3), to drive the noise terms Xn, is
amplified in the context of distributed learning and estimating θ∗; compared to the sampling case,
for which the rate is O(1/α) as mentioned earlier. For case (iii), we show that V(3)
θ (α) = V(3)
θ (0)
for all α ≥0, implying that using the SRRW in this case provides no asymptotic benefit than the
5Sampling covariance corresponds to only the empirical distribution xn in (4b).
6The α = 0 case is equivalent to simply running the base Markov chain, since from (3) we have K[·] = P,
thus bypassing the SRRW’s effect and rendering all three cases nearly the same.
4

Published as a conference paper at ICLR 2024
original base Markov chain, and thus performs worse than case (i). In summary, we deduce that
V(1)
θ (α2)<L V(1)
θ (α1)<L V(1)
θ (0)=V(3)
θ (0)=V(3)
θ (α) for all α2 > α1 > 0 and α > 0.7
4. We numerically simulate our SA-SRRW algorithm on various real-world datasets, focusing on
a binary classification task, to evaluate its performance across all three cases. By carefully choos-
ing the function H in SA-SRRW, we test the SGD and algorithms driven by SRRW. Our findings
consistently highlight the superiority of case (i) over cases (ii) and (iii) for diverse α values, even in
their finite time performance. Notably, our tests validate the variance reduction at a rate of O(1/α2)
for case (i), suggesting it as the best algorithmic choice among the three cases.
2
PRELIMINARIES AND MODEL SETUP
In Section 2.1, we first standardize the notations used throughout the paper, and define key mathe-
matical terms and quantities used in our theoretical analyses. Then, in Section 2.2, we consolidate
the model assumptions of our SA-SRRW algorithm (4). We then go on to discuss our assumptions,
and provide additional interpretations of our use of generalized step-sizes.
2.1
BASIC NOTATIONS AND DEFINITIONS
Vectors are denoted by lower-case bold letters, e.g., v ≜[vi] ∈RD, and matrices by upper-case
bold, e.g., M ≜[Mij] ∈RD×D. M−T is the transpose of the matrix inverse M−1. The diagonal
matrix Dv is formed by vector v with vi as the i’th diagonal entry. Let 1 and 0 denote vectors of
all ones and zeros, respectively. The identity matrix is represented by I, with subscripts indicating
dimensions as needed. A matrix is Hurwitz if all its eigenvalues possess strictly negative real parts.
1{·} denotes an indicator function with condition in parentheses. We use ∥·∥to denote both the
Euclidean norm of vectors and the spectral norm of matrices. Two symmetric matrices M1, M2
follow Loewner ordering M1 <L M2 if M2 −M1 is positive semi-definite and M1 ̸= M2. This
slightly differs from the conventional definition with ≤L, which allows M1 =M2.
Throughout the paper, the matrix P ≜[Pi,j]i,j∈N and vector µ ≜[µi]i∈N are used exclusively
to denote an N × N-dimensional transition kernel of an ergodic Markov chain, and its stationary
distribution, respectively. Without loss of generality, we assume Pij > 0 if and only if aij > 0.
Markov chains satisfying the detailed balance equation, where µiPij = µjPji for all i, j ∈N, are
termed time-reversible. For such chains, we use (λi, ui) (resp. (λi, vi)) to denote the i’th left (resp.
right) eigenpair where the eigenvalues are ordered: −1 < λ1 ≤· · · ≤λN−1 < λN = 1, with uN = µ
and vN = 1 in RN. We assume eigenvectors to be normalized such that uT
i vi = 1 for all i, and we
have ui =Dµvi and uT
i vj =0 for all i, j ∈N. We direct the reader to Aldous & Fill (2002, Chapter
3.4) for a detailed exposition on spectral properties of time-reversible Markov chains.
2.2
SA-SRRW: KEY ASSUMPTIONS AND DISCUSSIONS
Assumptions: All results in our paper are proved under the following assumptions.
(A1) The function H : RD × N →RD, is a continuous at every θ ∈RD, and there exists a
positive constant L such that ∥H(θ, i)∥≤L(1 + ∥θ∥) for every θ ∈RD, i ∈N.
(A2) Step sizes βn and γn follow βn =(n+1)−b, and γn =(n+1)−a, where a, b ∈(0.5, 1].
(A3) Roots of function h(·) are disjoint, which comprise the globally attracting set Θ ≜
n
θ∗|h(θ∗)=0, ∇h(θ∗) +
1{b=1}
2
I is Hurwitz
o
̸= ∅of the associated ordinary differential
equation (ODE) for iteration (4c), given by dθ(t)/dt=h(θ(t)).
(A4) For any (θ0, x0, X0) ∈RD × Int(Σ) × N, the iterate sequence {θn}n≥0 (resp. {xn}n≥0)
is Pθ0,x0,X0-almost surely contained within a compact subset of RD (resp. Int(Σ)).
Discussions on Assumptions: Assumption A1 requires H to only be locally Lipschitz albeit with
linear growth, and is less stringent than the globally Lipschitz assumption prevalent in optimization
literature (Li & Wai, 2022; Hendrikx, 2023; Even, 2023).
7In particular, this is the reason why we advocate for a more general step size γn = (n+1)−a in the SRRW
iterates with a < 1, allowing us to choose βn = (n + 1)−b with b ∈(a, 1] to satisfy βn = o(γn) for case (i).
5

Published as a conference paper at ICLR 2024
Assumption A2 is the general umbrella assumption under which cases (i), (ii) and (iii) mentioned
in Section 1 are extracted by setting: (i) a < b, (ii) a = b, and (iii) a > b. Cases (i) and (iii)
render θn, xn on different timescales; the polynomial form of βn, γn widely assumed in the two-
timescale SA literature (Mokkadem & Pelletier, 2006; Zeng et al., 2021; Hong et al., 2023). Case (ii)
characterizes the SA-SRRW algorithm (4) as a single-timescale SA with polynomially decreasing
step size, and is among the most common assumptions in the SA literature (Borkar, 2022; Fort,
2015; Li et al., 2023). In all three cases, the form of γn ensures γn ≤1 such that the SRRW iterates
xn in (4b) is within Int(Σ), ensuring that K[xn] is well-defined for all n ≥0.
In Assumption A3, limiting dynamics of SA iterations {θn}n≥0 closely follow trajectories
{θ(t)}t≥0 of their associated ODE, and assuming the existence of globally stable equilibria is stan-
dard (Borkar, 2022; Fort, 2015; Li et al., 2023). In optimization problems, this is equivalent to
assuming the existence of at most countably many local minima.
Assumption A4 assumes almost sure boundedness of iterates θn and xn, which is a common as-
sumption in SA algorithms (Kushner & Yin, 2003; Chen, 2006; Borkar, 2022; Karmakar & Bhatna-
gar, 2018; Li et al., 2023) for the stability of the SA iterations by ensuring the well-definiteness of all
quantities involved. Stability of the weighted empirical measure xn of the SRRW process is prac-
tically ensured by studying (4b) with a truncation-based procedure (see Doshi et al., 2023, Remark
4.5 and Appendix E for a comprehensive explanation), while that for θn is usually ensured either as
a by-product of the algorithm design, or via mechanisms such as projections onto a compact subset
of RD, depending on the application context. We now provide additional discussions regarding the
step-size assumptions and their implications on the SRRW iteration (4b).
SRRW with General Step Size: As shown in Benaim & Cloez (2015, Remark 1.1), albeit for a
completely different non-linear Markov kernel driving the algorithm therein, iterates xn of (4b) can
also be expressed as weighted empirical measures of {Xn}n≥0, in the following form:
xn =
Pn
i=1 ωiδXi + ω0x0
Pn
i=0 ωi
,
where ω0 = 1, and ωn =
γn
Qn
i=1(1 −γi),
(5)
for all n > 0. For the special case when γn = 1/(n+1) as in Doshi et al. (2023), we have ωn = 1 for
all n ≥0 and xn is the typical, unweighted empirical measure. For the additional case considered
in our paper, when a < 1 for γn as in assumption A2, we can approximate 1 −γn ≈e−γn and
ωn ≈n−aen(1−a)/(1−a). This implies that ωn will increase at sub-exponential rate, giving more
weight to recent visit counts and allowing it to quickly ‘forget’ the poor initial measure x0 and shed
the correlation with the initial choice of X0. This ‘speed up’ effect by setting a < 1 is guaranteed
in case (i) irrespective of the choice of b in Assumption A2, and in Section 3 we show how this can
lead to further reduction in covariance of optimization error θn = θ∗in the asymptotic regime.
Additional assumption for case (iii): Before moving on to Section 3, we take another look at the
case when γn = o(βn), and replace A3 with the following, stronger assumption only for case (iii).
(A3′) For any x∈Int(Σ), there exists a function ρ : Int(Σ)→RD such that ∥ρ(x)∥≤L2(1+∥x∥)
for some L2 >0, Ei∼π[x][H(ρ(x), i)]=0 and Ei∼π[x][∇H(ρ(x), i)] +
1{b=1}
2
I is Hurwitz.
While Assumption A3′ for case (iii) is much stronger than A3, it is not detrimental to the overall
results of our paper, since case (i) is of far greater interest as impressed upon in Section 1. This is
discussed further in Appendix C.
3
ASYMPTOTIC ANALYSIS OF THE SA-SRRW ALGORITHM
In this section, we provide the main results for the SA-SRRW algorithm (4). We first present the
a.s. convergence and the CLT result for SRRW with generalized step size, extending the results in
Doshi et al. (2023). Building upon this, we present the a.s. convergence and the CLT result for the
SA iterate θn under different settings of step sizes. We then shift our focus to the analysis of the
different asymptotic covariance matrices emerging out of the CLT result, and capture the effect of α
and the step sizes, particularly in cases (i) and (iii), on θn −θ∗via performance ordering.
Almost Sure convergence and CLT: The following result establishes first and second order conver-
gence of the sequence {xn}n≥0, which represents the weighted empirical measures of the SRRW
process {Xn}n≥0, based on the update rule in (4b).
6

Published as a conference paper at ICLR 2024
Lemma 3.1. Under Assumptions A1, A2 and A4, for the SRRW iterates (4b), we have
xn
a.s.
−−−−→
n→∞µ,
and
γ−1/2
n
(xn −µ)
dist.
−−−−→
n→∞N(0, Vx(α)),
where
Vx(α) =
N−1
X
i=1
1
2α(1 + λi) + 2 −1{a=1}
· 1 + λi
1 −λi
uiuT
i .
(6)
Moreover, for all α2 > α1 > 0, we have Vx(α2) <L Vx(α1) <L Vx(0).
Lemma 3.1 shows that the SRRW iterates xn converges to the target distribution µ a.s. even under
the general step size γn = (n+1)−a for a ∈(0.5, 1]. We also observe that the asymptotic covariance
matrix Vx(α) decreases at rate O(1/α). Lemma 3.1 aligns with Doshi et al. (2023, Theorem 4.2
and Corollary 4.3) for the special case of a = 1, and is therefore more general. Critically, it helps us
establish our next result regarding the first-order convergence for the optimization iterate sequence
{θn}n≥0 following update rule (4c), as well as its second-order convergence result, which follows
shortly after. The proofs of Lemma 3.1 and our next result, Theorem 3.2, are deferred to Appendix
D. In what follows, k = 1, 2, and 3 refer to cases (i), (ii), and (iii) in Section 2.2, respectively. All
subsequent results are proven under Assumptions A1 to A4, with A3′ replacing A3 only when the
step sizes βn, γn satisfy case (iii).
Theorem 3.2. For k ∈{1, 2, 3}, and any initial (θ0, x0, X0) ∈RD×Int(Σ)×N, we have θn →θ∗
as n →∞for some θ∗∈Θ, Pθ0,x0,X0-almost surely.
In the stochastic optimization context, the above result ensures convergence of iterates θn to a local
minimizer θ∗. Loosely speaking, the first-order convergence of xn in Lemma 3.1 as well as that of
θn are closely related to the convergence of trajectories {z(t) ≜(θ(t), x(t))}t≥0 of the (coupled)
mean-field ODE, written in a matrix-vector form as
d
dtz(t) = g(z(t)) ≜

H(θ(t))T π[x(t)]
π[x(t)] −x(t)

∈RD+N.
(7)
where matrix H(θ) ≜[H(θ, 1),· · ·, H(θ, N)]T ∈RN×D for any θ ∈RD. Here, π[x] ∈Int(Σ)
is the stationary distribution of the SRRW kernel K[x] and is shown in Doshi et al. (2023) to be
given by πi[x] ∝P
j∈N µiPij(xi/µi)−α(xj/µj)−α. The Jacobian matrix of (7) when evaluated at
equilibria z∗= (θ∗, µ) for θ∗∈Θ captures the behaviour of solutions of the mean-field in their
vicinity, and plays an important role in the asymptotic covariance matrices arising out of our CLT
results. We evaluate this Jacobian matrix J(α) as a function of α ≥0 to be given by
J(α)≜∇g(z∗)=

∇h(θ∗)
−αH(θ∗)T (PT+ I)
0N×D
2αµ1T−αPT−(α+1)I

≜

J11
J12(α)
J21
J22(α)

.
(8)
The derivation of J(α) is referred to Appendix E.1.8 Here, J21 is a zero matrix since π[x] −x
is devoid of θ. While matrix J22(α) is exactly of the form in Doshi et al. (2023, Lemma 3.4)
to characterize the SRRW performance, our analysis includes an additional matrix J12(α), which
captures the effect of x(t) on θ(t) in the ODE (7), which translates to the influence of our generalized
SRRW empirical measure xn on the SA iterates θn in (4).
For notational simplicity, and without loss of generality, all our remaining results are stated while
conditioning on the event that {θn →θ∗}, for some θ∗∈Θ. We also adopt the shorthand notation
H to represent H(θ∗). Our main CLT result is as follows, with its proof deferred to Appendix E.
Theorem 3.3. For any α ≥0, we have: (a) There exists V(k)(α) for all k ∈{1, 2, 3} such that
"
β−1/2
n
(θn −θ∗)
γ−1/2
n
(xn −µ)
#
dist.
−−−−→
n→∞N

0, V(k)(α)

.
(b) For k = 2, matrix V(2)(α) solves the Lyapunov equation J(α)V(2)(α) + V(2)(α)J(α)T +
1{b=1}V(2)(α) = −U, where the Jacobian matrix J(α) is in (8), and
U ≜
N−1
X
i=1
1 + λi
1 −λi
·

HT uiuT
i H
HT uiuT
i
uiuT
i H
uiuT
i

≜

U11
U12
U21
U22

.
(9)
(c) For k ∈{1, 3}, V(k)(α) becomes block diagonal, which is given by
V(k)(α) =

V(k)
θ (α)
0D×N
0N×D
Vx(α)

,
(10)
8The Jacobian J(α) is (D+N)×(D+N)– dimensional, with J11 ∈RD×D and J22(α)∈RN×N. Following
this, all matrices written in a block form, such as matrix U in (9), will inherit the same dimensional structure.
7

Published as a conference paper at ICLR 2024
where Vx(α) is as in (6), and V(1)
θ (α) and V(3)
θ (α) can be written in the following explicit form:
V(1)
θ (α) =
R ∞
0
et(∇θh(θ∗)+
1{b=1}
2
I)Uθ(α)et(∇θh(θ∗)+
1{b=1}
2
I)T dt,
V(3)
θ (α) =
R ∞
0
et∇θh(θ∗)U11et∇θh(θ∗)dt,
where Uθ(α) =
N−1
X
i=1
1
(α(1 + λi) + 1)2 · 1 + λi
1 −λi
HT uiuT
i H.
(11)
For k ∈{1, 3}, SA-SRRW in (4) is a two-timescale SA with controlled Markov noise. While a few
works study the CLT of two-timescale SA with the stochastic input being a martingale-difference
(i.i.d.) noise (Konda & Tsitsiklis, 2004; Mokkadem & Pelletier, 2006), a CLT result covering the
case of controlled Markov noise (e.g., k ∈{1, 3}), a far more general setting than martingale-
difference noise, is still an open problem. Thus, we here prove our CLT for k ∈{1, 3} from scratch
by a series of careful decompositions of the Markovian noise, ultimately into a martingale-difference
term and several non-vanishing noise terms through repeated application of the Poisson equation
(Benveniste et al., 2012; Fort, 2015). Although the form of the resulting asymptotic covariance
looks similar to that for the martingale-difference case in (Konda & Tsitsiklis, 2004; Mokkadem
& Pelletier, 2006) at first glance, they are not equivalent. Specifically, V(k)
θ (α) captures both the
effect of SRRW hyper-parameter α, as well as that of the underlying base Markov chain via eigen-
pairs (λi, ui) of its transition probability matrix P in matrix U, whereas the latter only covers the
martingale-difference noise terms as a special case.
When k = 2, that is, βn = γn, algorithm (4) can be regarded as a single-timescale SA algorithm.
In this case, we utilize the CLT in Fort (2015, Theorem 2.1) to obtain the implicit form of V(2)(α)
as shown in Theorem 3.3. However, J12(α) being non-zero for α > 0 restricts us from obtaining
an explicit form for the covariance term corresponding to SA iterate errors θn −θ∗. On the other
hand, for k ∈{1, 3}, the nature of two-timescale structure causes θn and xn to become asymptoti-
cally independent with zero correlation terms inside V(k)(α) in (10), and we can explicitly deduce
V(k)
θ (α). We now take a deeper dive into α and study its effect on V(k)
θ (α).
Covariance Ordering of SA-SRRW: We refer the reader to Appendix F for proofs of all remaining
results. We begin by focusing on case (i) and capturing the impact of α on V(1)
θ (α).
Proposition 3.4. For all α2 > α1 > 0, we have V(1)
θ (α2) <L V(1)
θ (α1) <L V(1)
θ (0). Furthermore,
V(1)
θ (α) decreases to zero at a rate of O(1/α2).
Proposition 3.4 proves a monotonic reduction (in terms of Loewner ordering) of V(1)
θ (α) as α in-
creases. Moreover, the decrease rate O(1/α2) surpasses the O(1/α) rate seen in Vx(α) and the
sampling application in Doshi et al. (2023, Corollary 4.7), and is also empirically observed in our
simulation in Section 4.9 Suppose we consider the same SA now driven by an i.i.d. sequence {Xn}
with the same marginal distribution µ. Then, our Proposition 3.4 asserts that a token algorithm em-
ploying SRRW (walk on a graph) with large enough α on a general graph can actually produce better
SA iterates with its asymptotic covariance going down to zero, than a ‘hypothetical situation’ where
the walker is able to access any node j with probability µj from anywhere in one step (more like a
random jumper). This can be seen by noting that for large time n, the scaled MSE E[∥θn−θ∗∥2]/βn
is composed of the diagonal entries of the covariance matrix Vθ, which, as we discuss in detail in
Appendix F.2, are decreasing in α as a consequence of the Loewner ordering in Proposition 3.4. For
large enough α, the scaled MSE for SA-SRRW becomes smaller than its i.i.d. counterpart, which is
always a constant. Although Doshi et al. (2023) alluded this for sampling applications with Vx(α),
we broaden its horizons to distributed optimization problem with Vθ(α) using tokens on graphs.
Our subsequent result concerns the performance comparison between cases (i) and (iii).
Corollary 3.5. For any α > 0, we have V(1)
θ (α) <L V(3)
θ (α) = V(3)
θ (0).
We show that case (i) is asymptotically better than case (iii) for α > 0. In view of Proposition 3.4
and Corollary 3.5, the advantages of case (i) become prominent.
9Further insights of O(1/α2) are tied to the two-timescale structure, particularly βn = o(γn) in case (i),
which places θn on the slow timescale so that the correlation terms J12(α), J22(α) in the Jacobian matrix
J(α) in (8) come into play. Technical details are referred to Appendix E.2, where we show the form of Uθ(α).
8

Published as a conference paper at ICLR 2024
102
103
104
105
106
Number of steps (n)
10
3
10
2
10
1
100
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(a) SGD-SRRW.
102
103
104
105
106
Number of steps (n)
10
3
10
2
10
1
100
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(b) SHB-SRRW.
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
value of 
0.0005
0.0010
0.0015
0.0020
0.0025
0.0030
0.0035
MSE 
n
*
2 at n = 107
(
MSE points 
Fitted curve 
g( ) = 
       
0.0078
+ 1.588)2 + 0.0003
(c) Curve fitting for MSE.
Figure 2: Simulation results under case (i): (a) and (b) show the performance of SGD-SRRW and
SHB-SRRW for various α values. (c) shows that MSE decreases at O(1/α2) speed.
102
103
104
105
106
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(a) α = 1, SGD-SRRW
102
103
104
105
106
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(b) α = 5, SGD-SRRW
102
103
104
105
106
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(c) α = 10, SGD-SRRW
Figure 3: Comparison of the performance among cases (i) - (iii) for α ∈{1, 5, 10}.
4
SIMULATION
In this section, we simulate our SA-SRRW algorithm on the wikiVote graph (Leskovec & Krevl,
2014), comprising 889 nodes and 2914 edges. We configure the SRRW’s base Markov chain P as
the MHRW with a uniform target distribution µ =
1
N 1. For distributed optimization, we consider
the following L2 regularized binary classification problem:
minθ∈RD
n
f(θ) ≜1
N
PN
i=1 log

1 + eθT si

−yi
 θT si

+ κ
2 ∥θ∥2o
,
(12)
where {(si, yi)}N
i=1 is the ijcnn1 dataset (with 22 features, i.e., si ∈R22) from LIBSVM (Chang
& Lin, 2011), and penalty parameter κ = 1. Each node in the wikiVote graph is assigned one
data point, thus 889 data points in total. We perform SRRW driven SGD (SGD-SRRW) and SRRW
driven stochastic heavy ball (SHB-SRRW) algorithms (see (13) in Appendix A for its algorithm).
We fix the step size βn = (n + 1)−0.9 for the SA iterates and adjust γn = (n + 1)−a in the SRRW
iterates to cover all three cases discussed in this paper: (i) a = 0.8; (ii) a = 0.9; (iii) a = 1. We use
mean square error (MSE), i.e., E[∥θn−θ∗∥2], to measure the error on the SA iterates.
Our results are presented in Figures 2 and 3, where each experiment is repeated 100 times. Figures
2a and 2b, based on wikiVote graph, highlight the consistent performance ordering across different
α values for both algorithms over almost all time (not just asymptotically). Notably, curves for
α ≥5 outperform that of the i.i.d. sampling (in black) even under the graph constraints. Figure 2c
on the smaller Dolphins graph (Rossi & Ahmed, 2015) - 62 nodes and 159 edges - illustrates that
the points of (α, MSE) pair arising from SGD-SRRW at time n = 107 align with a curve in the form
of g(x)=
c1
(x+c2)2 +c3 to showcase O(1/α2) rates. This smaller graph allows for longer simulations
to observe the asymptotic behaviour. Additionally, among the three cases examined at identical α
values, Figures 3a - 3c confirm that case (i) performs consistently better than the rest, underscoring
its superiority in practice. Further results, including those from non-convex functions and additional
datasets, are deferred to Appendix H due to space constraints.
5
CONCLUSION
In this paper, we show both theoretically and empirically that the SRRW as a drop-in replacement
for Markov chains can provide significant performance improvements when used for token algo-
rithms, where the acceleration comes purely from the careful analysis of the stochastic input of the
algorithm, without changing the optimization iteration itself. Our paper is an instance where the
asymptotic analysis approach allows the design of better algorithms despite the usage of uncon-
ventional noise sequences such as nonlinear Markov chains like the SRRW, for which traditional
finite-time analytical approaches fall short, thus advocating their wider adoption.
9

Published as a conference paper at ICLR 2024
6
ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING
We thank the anonymous reviewers for their constructive comments, especially Reviewer DxLx for
raising future directions. This work was supported in part by National Science Foundation under
Grant Nos. CNS-2007423 and IIS-1910749.
REFERENCES
David Aldous and James Allen Fill. Reversible markov chains and random walks on graphs, 2002.
Unfinished monograph, recompiled 2014, available at http://www.stat.berkeley.
edu/˜aldous/RWG/book.html.
Christophe Andrieu, Ajay Jasra, Arnaud Doucet, and Pierre Del Moral. Non-linear markov chain
monte carlo. In Esaim: Proceedings, volume 19, pp. 79–84. EDP Sciences, 2007.
Ghadir Ayache and Salim El Rouayheb. Private weighted random walk stochastic gradient descent.
IEEE Journal on Selected Areas in Information Theory, 2(1):452–463, 2021.
Anas Barakat and Pascal Bianchi. Convergence and dynamical behavior of the adam algorithm for
nonconvex stochastic optimization. SIAM Journal on Optimization, 31(1):244–274, 2021.
Anas Barakat, Pascal Bianchi, Walid Hachem, and Sholom Schechtman. Stochastic optimization
with momentum: convergence, fluctuations, and traps avoidance. Electronic Journal of Statistics,
15(2):3892–3947, 2021.
M Benaim and Bertrand Cloez. A stochastic approximation approach to quasi-stationary distribu-
tions on finite spaces. Electronic Communications in Probability 37 (20), 1-14.(2015), 2015.
Albert Benveniste, Michel M´etivier, and Pierre Priouret. Adaptive algorithms and stochastic ap-
proximations, volume 22. Springer Science & Business Media, 2012.
V.S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint: Second Edition. Texts
and Readings in Mathematics. Hindustan Book Agency, 2022. ISBN 9788195196111.
L´eon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM review, 60(2):223–311, 2018.
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
IEEE transactions on information theory, 52(6):2508–2530, 2006.
Pierre Br´emaud. Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31.
Springer Science & Business Media, 2013.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM trans-
actions on intelligent systems and technology (TIST), 2(3):1–27, 2011.
VijaySekhar Chellaboina and Wassim M Haddad. Nonlinear dynamical systems and control: A
Lyapunov-based approach. Princeton University Press, 2008.
Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, and Stanislaw H Zak. On the con-
vergence of decentralized federated learning under imperfect information sharing. arXiv preprint
arXiv:2303.10695, 2023.
Han-Fu Chen.
Stochastic approximation and its applications, volume 64.
Springer Science &
Business Media, 2006.
Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds
for monte-carlo and linear stochastic approximation. In International Conference on Artificial
Intelligence and Statistics, pp. 4173–4183. PMLR, 2020a.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam.
Finite-
sample analysis of stochastic approximation using smooth convex envelopes.
arXiv preprint
arXiv:2002.00874, 2020b.
10

Published as a conference paper at ICLR 2024
Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-
sample analysis of nonlinear stochastic approximation with applications in reinforcement learn-
ing. Automatica, 146:110623, 2022.
Burgess Davis. On the intergrability of the martingale square function. Israel Journal of Mathemat-
ics, 8:187–190, 1970.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: a fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, volume 1, 2014.
Pierre Del Moral and Arnaud Doucet. Interacting markov chain monte carlo methods for solving
nonlinear measure-valued equations1. The Annals of Applied Probability, 20(2):593–639, 2010.
Pierre Del Moral and Laurent Miclo.
Self-interacting markov chains.
Stochastic Analysis and
Applications, 24(3):615–660, 2006.
Bernard Delyon. Stochastic approximation with decreasing gain: Convergence and asymptotic the-
ory. Technical report, Universit´e de Rennes, 2000.
Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochastic approximation
version of the em algorithm. Annals of statistics, pp. 94–128, 1999.
Adithya M Devraj and Sean P Meyn. Zap q-learning. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 2232–2241, 2017.
Adithya M. Devraj and Sean P. Meyn. Q-learning with uniformly bounded variance. IEEE Trans-
actions on Automatic Control, 2021.
Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed td (0) with linear
function approximation on multi-agent reinforcement learning. In International Conference on
Machine Learning, pp. 1626–1635. PMLR, 2019.
Thinh T Doan. Finite-time convergence rates of nonlinear two-time-scale stochastic approximation
under markovian noise. arXiv preprint arXiv:2104.01627, 2021.
Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of ac-
celerated markov gradient descent with applications in reinforcement learning. arXiv preprint
arXiv:2002.02873, 2020.
Vishwaraj Doshi, Jie Hu, and Do Young Eun. Self-repellent random walks on general graphs–
achieving minimal sampling variance via nonlinear markov chains. In International Conference
on Machine Learning. PMLR, 2023.
Marie Duflo. Algorithmes stochastiques, volume 23. Springer, 1996.
Mathieu Even. Stochastic gradient descent under markovian sampling schemes. In International
Conference on Machine Learning, 2023.
Gersende Fort. Central limit theorems for stochastic approximation with controlled markov chain
dynamics. ESAIM: Probability and Statistics, 19:60–80, 2015.
S´ebastien Gadat, Fabien Panloup, and Sofiane Saadane. Stochastic heavy ball. Electronic Journal
of Statistics, 12:461–529, 2018.
Xin Guo, Jiequn Han, Mahan Tajrobehkar, and Wenpin Tang. Escaping saddle points efficiently
with occupation-time-adapted perturbations. arXiv preprint arXiv:2005.04507, 2020.
P. Hall, C.C. Heyde, Z.W. Birnbauam, and E. Lukacs. Martingale Limit Theory and Its Application.
Communication and Behavior. Elsevier Science, 2014.
Hadrien Hendrikx. A principled framework for the design and analysis of token algorithms. In
International Conference on Artificial Intelligence and Statistics, pp. 470–489. PMLR, 2023.
11

Published as a conference paper at ICLR 2024
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm
framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM
Journal on Optimization, 33(1):147–180, 2023.
Jie Hu, Vishwaraj Doshi, and Do Young Eun. Efficiency ordering of stochastic gradient descent. In
Advances in Neural Information Processing Systems, 2022.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In International conference on machine learning, pp. 1724–1732. PMLR, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. In Conference On Learning Theory, pp. 1042–1085. PMLR,
2018.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex
optimization for machine learning: Gradients, stochasticity, and saddle points. Journal of the
ACM (JACM), 68(2):1–29, 2021.
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai. Non-asymptotic analysis of
biased stochastic approximation scheme. In Conference on Learning Theory, pp. 1944–1974.
PMLR, 2019.
Prasenjit Karmakar and Shalabh Bhatnagar. Two time-scale stochastic approximation with con-
trolled markov noise and off-policy temporal-difference learning. Mathematics of Operations
Research, 43(1):130–151, 2018.
Ahmed Khaled and Peter Richt´arik. Better theory for SGD in the nonconvex world. Transactions
on Machine Learning Research, 2023. ISSN 2835-8856.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic approxi-
mation. The Annals of Applied Probability, 14(2):796–819, 2004.
Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and appli-
cations, volume 35. Springer Science & Business Media, 2003.
Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, and Farinaz Koushanfar. Fully decentralized
federated learning. In Advances in neural information processing systems, 2018.
Jure Leskovec and Andrej Krevl. Snap datasets: Stanford large network dataset collection, 2014.
David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathe-
matical Soc., 2017.
Qiang Li and Hoi-To Wai. State dependent performative prediction with stochastic approximation.
In International Conference on Artificial Intelligence and Statistics, pp. 3164–3186. PMLR, 2022.
Tiejun Li, Tiannan Xiao, and Guoguo Yang. Revisiting the central limit theorems for the sgd-type
methods. arXiv preprint arXiv:2207.11755, 2022.
Xiang Li, Jiadong Liang, and Zhihua Zhang. Online statistical inference for nonlinear stochastic
approximation with markovian data. arXiv preprint arXiv:2302.07690, 2023.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli-
gence and statistics, pp. 1273–1282. PMLR, 2017.
Sean Meyn. Control systems and reinforcement learning. Cambridge University Press, 2022.
Abdelkader Mokkadem and Mariane Pelletier. The compact law of the iterated logarithm for mul-
tivariate stochastic approximation algorithms. Stochastic analysis and applications, 23(1):181–
203, 2005.
12

Published as a conference paper at ICLR 2024
Abdelkader Mokkadem and Mariane Pelletier. Convergence rate and averaging of nonlinear two-
time-scale stochastic approximation algorithms. Annals of Applied Probability, 16(3):1671–1702,
2006.
Gemma Morral, Pascal Bianchi, and Gersende Fort. Success and failure of adaptation-diffusion
algorithms with decaying step size in multiagent networks. IEEE Transactions on Signal Pro-
cessing, 65(11):2798–2813, 2017.
Wenlong Mou, Chris Junchi Li, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. On
linear stochastic approximation: Fine-grained polyak-ruppert and non-asymptotic concentration.
In Conference on Learning Theory, pp. 2947–2997. PMLR, 2020.
Angelia Nedic. Distributed gradient methods for convex machine learning problems in networks:
Distributed optimization. IEEE Signal Processing Magazine, 37(3):92–101, 2020.
Alex Olshevsky.
Asymptotic network independence and step-size for a distributed subgradient
method. Journal of Machine Learning Research, 23(69):1–32, 2022.
Mariane Pelletier. On the almost sure asymptotic behaviour of stochastic algorithms. Stochastic
processes and their applications, 78(2):217–244, 1998.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
and visualization. In AAAI, 2015.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162:83–112, 2017.
Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. In Advances in neural
information processing systems, volume 31, 2018.
Aleksei Triastcyn, Matthias Reisser, and Christos Louizos. Decentralized learning with random
walks and communication-efficient adaptive optimization. In Workshop on Federated Learning:
Recent Advances and New Challenges (in Conjunction with NeurIPS 2022), 2022.
Thijs Vogels, Lie He, Anastasiia Koloskova, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich,
and Martin Jaggi. Relaysum for decentralized deep learning on heterogeneous data. In Advances
in Neural Information Processing Systems, volume 34, pp. 28004–28015, 2021.
Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. Matcha: Speed-
ing up decentralized sgd via matching decomposition sampling. In 2019 Sixth Indian Control
Conference (ICC), pp. 299–300. IEEE, 2019.
Vinayaka G Yaji and Shalabh Bhatnagar. Stochastic recursive inclusions in two timescales with
nonadditive iterate-dependent markov noise. Mathematics of Operations Research, 45(4):1405–
1444, 2020.
Hao Ye, Le Liang, and Geoffrey Ye Li. Decentralized federated learning with unreliable communi-
cations. IEEE Journal of Selected Topics in Signal Processing, 16(3):487–500, 2022.
Sihan Zeng, Thinh T Doan, and Justin Romberg. A two-time-scale stochastic optimization frame-
work with applications in control and reinforcement learning. arXiv preprint arXiv:2109.14756,
2021.
13

Published as a conference paper at ICLR 2024
A
EXAMPLES OF STOCHASTIC ALGORITHMS OF THE FORM (2).
In the literature of stochastic optimizations, many SGD variants have been proposed by introducing
an auxiliary variable to improve convergence. In what follows, we present two SGD variants with
decreasing step size that can be presented in the form of (2): SHB (Gadat et al., 2018; Li et al., 2022)
and momentum-based algorithm (Barakat et al., 2021; Barakat & Bianchi, 2021).
θn+1 =θn−βn+1mn
mn+1 =mn+βn+1(∇F(θn, Xn+1)−mn),



vn+1 =vn+βn+1(∇F(θn, Xn+1)2−vn),
mn+1 =mn+βn+1(∇F(θn, Xn+1)−mn),
θn+1 =θn−βn+1mn/√vn + ϵ,
(a). SHB
(b). Momentum-based Algorithm
(13)
where ϵ > 0, θn, mn, vn, ∇F(θ, X) ∈Rd, and the square and square root in (13) (b) are element-
wise operators.10
For SHB, we introduce an augmented variable zn and function H(zn, Xn+1) defined as follows:
zn ≜

θn
mn

∈R2d,
H(zn, Xn+1) ≜

−mn
∇F(θn, Xn+1) −mn

∈R2d.
For the general momentum-based algorithm, we define
zn ≜
" vn
mn
θn
#
∈R3d,
H(zn, X) ≜


∇F(θn, Xn+1)2 −vn
∇F(θn, Xn+1) −mn
−mn/√vn + ϵ

∈R3d.
Thus, we can reformulate both algorithms in (13) as zn+1 = zn + βn+1H(zn, Xn+1). This aug-
mentation approach was previously adopted in (Gadat et al., 2018; Barakat et al., 2021; Barakat &
Bianchi, 2021; Li et al., 2022) to analyze the asymptotic performance of algorithms in (13) using an
i.i.d. sequence {Xn}n≥0. Consequently, the general SA iteration (2) includes these SGD variants.
However, we mainly focus on the CLT for the general SA driven by SRRW in this paper. Pursuing
the explicit CLT results of these SGD variants with specific form of function H(θ, X) driven by the
SRRW sequence {Xn} is out of the scope of this paper.
When we numerically test the SHB algorithm in Section 4, we use the exact form of (13) (a) and
the stochastic sequence {Xn} is now driven by the SRRW. Specifically, we consider MHRW with
transition kernel P as the base Markov chain of the SRRW process, e.g.,
Pij =
(
min
n
1
di , 1
dj
o
if node j is the neighbor of node i,
0
otherwise,
Pii = 1 −
X
j∈N
Pij.
Then, at each time step n,
Draw:
Xn+1 ∼KXn,·[xn],
where
Kij[x] ≜
Pij(xj/µj)−α
P
k∈N Pik(xk/µk)−α ,
∀i, j ∈N,
Update:
xn+1 = xn + γn+1(δXn+1 −xn),
θn+1 = θn −βn+1mn,
mn+1 = mn + βn+1(∇F(θn, Xn+1) −mn).
10For ease of expression, we simplify the original SHB and momentum-based algorithms from Gadat et al.
(2018); Li et al. (2022); Barakat et al. (2021); Barakat & Bianchi (2021), setting all tunable parameters to 1 and
resulting in (13).
14

Published as a conference paper at ICLR 2024
B
DISCUSSION ON MEAN FIELD FUNCTION OF SRRW ITERATES (4b)
Non-asymptotic analyses have seen extensive attention recently in both single-timescale SA litera-
ture (Sun et al., 2018; Karimi et al., 2019; Chen et al., 2020b; 2022) and two-timescale SA literature
(Doan, 2021; Zeng et al., 2021). Specifically, single-timescale SA has the following form:
xn+1 = xn + βn+1H(xn, Xn+1),
and function h(x) ≜EX∼µ[H(x, X)] is the mean field of function H(x, X). Similarly, for two-
timescale SA, we have the following recursions:
xn+1 = xn + βn+1H1(xn, yn, Xn+1),
yn+1 = yn + γn+1H2(xn, yn, Xn+1),
where {βn} and {γn} are on different timescales, and function hi(x, y) ≜EX∼µ[Hi(x, y, X)] is
the mean field of function Hi(x, y, X) for i = {1, 2}. All the aforementioned works require the
mean field function h(x) in the single-timescale SA (or h1(x, y), h2(x, y) in the two-timescale SA)
to be globally Lipschitz with a Lipschitz constant L to proceed with the derivation of finite-time
bounds including the constant L.
Here, we show that the mean field function π[x] −x in the SRRW iterates (4b) is not globally
Lipschitz, where π[x] is the stationary distribution of the SRRW kernel K[x] defined in (3). To this
end, we show that each entry of Jacobian matrix of π[x]−x goes unbounded because a multivariate
function is Lipschitz if and only if it has bounded partial derivatives. Note that from Doshi et al.
(2023, Proposition 2.1), for the i-th entry of π[x], we have
πi[x] =
P
j∈N µiPij (xi/µi)−α (xj/µj)−α
P
i∈N
P
j∈N µiPij (xi/µi)−α (xj/µj)−α .
(16)
Then, the Jacobian matrix of the mean field function π[x] −x , which has been derived in Doshi
et al. (2023, Proof of Lemma 3.4 in Appendix B), is given as follows:
∂(πi[x] −xi)
∂xj
= 2α
xj
· (P
k∈N µiPik (xi/µi)−α (xk/µk)−α)(P
k∈N µjPjk (xj/µj)−α (xk/µk)−α)
(P
l∈N
P
k∈N µlPlk (xl/µl)−α (xk/µk)−α)2
−α
xj
·
µiPij (xi/µi)−α (xj/µj)−α
P
l∈N
P
k∈N µlPlk (xl/µl)−α (xk/µk)−α
(17)
for i, j ∈N, i ̸= j, and
∂(πi[x] −xi)
∂xi
= 2α
xi
·
(P
k∈N µiPik (xi/µi)−α (xk/µk)−α)2
(P
l∈N
P
k∈N µlPlk (xl/µl)−α (xk/µk)−α)2
−α
xi
·
P
k∈N µiPik (xi/µi)−α (xk/µk)−α + µiPii(xi/µi)−2α
P
l∈N
P
k∈N µlPlk (xl/µl)−α (xk/µk)−α
−1
(18)
for i ∈N. Since the empirical distribution x ∈Int(Σ), we have xi ∈(0, 1) for all i ∈N. For
fixed i, assume xi = xj and as they approach zero, the terms (xi/µi)−α, (xj/µj)−α dominate the
fraction in (17) and both the numerator and the denominator of the fraction have the same order in
terms of xi, xj. Thus, we have
∂(πi[x] −xi)
∂xj
= O
 1
xj

such that the (i, j)-th entry of the Jacobian matrix can go unbounded as xj →0. Consequently,
π[x] −x is not globally Lipschitz for x ∈Int(Σ).
15

Published as a conference paper at ICLR 2024
C
DISCUSSION ON ASSUMPTION A3′
When γn = o(βn), iterates xn has smaller step size compared to θn, thus converges ‘slower’ than
θn. From Assumption A3′, θn will intuitively converge to some point ρ(x) with the current value
x from the iteration xn, i.e., EX∼π[x][H(ρ(x), X)] = 0, while the Hurwitz condition is to ensure
the stability around ρ(x). We can see that Assumption A3 is less stringent than A3′ in that it only
assumes such condition when x = µ such that ρ(µ) = θ∗rather than for all x ∈Int(Σ).
One special instance of Assumption A3′ is by assuming the linear SA, e.g., H(θ, i) = Aiθ + bi. In
this case, EX∼π[x][H(ρ(x), X)] = 0 is equivalent to Ei∼π[x][Ai]ρ(x)+Ei∼π[x][bi] = 0. Under the
condition that for every x ∈Int(Σ), matrix Ei∼π[x][Ai] is invertible, we then have
ρ(x) = −(Ei∼π[x][Ai])−1 · Ei∼π[x][bi].
However, this condition is quite strict. Loosely speaking, Ei∼π[x][Ai] being invertible for any x
is similar to saying that any convex combination of {Ai} is invertible. For example, if we assume
{Ai}i∈N are negative definite and they all share the same eigenbasis {ui}, e.g., Ai = PD
j=1 λi
juiuT
i
and λi
j < 0 for all i ∈N, j ∈[D]. Then, Ei∼π[x][Ai] is invertible.
Another example for Assumption A3′ is when H(θ, i) = H(θ, j) for all i, j ∈N, which implies
that each agent in the distributed learning has the same local dataset to collaboratively train the
model. In this example, ρ(x) = θ∗such that
Ei∼π[x][H(ρ(x), i)] = h(θ∗) = 0,
Ei∼π[x][∇H(ρ(x), i)] + 1{b=1}
2
I = ∇h(θ∗) + 1{b=1}
2
I
being Hurwitz.
D
PROOF OF LEMMA 3.1 AND LEMMA 3.2
In this section, we demonstrate the almost sure convergence of both θn and xn together. This proof
naturally incorporates the almost certain convergence of the SRRW iteration in Lemma 3.1, since
xn is independent of θn (as indicated in (4)), allowing us to separate out its asymptotic results. The
same reason applies to the CLT analysis of SRRW iterates and we refer the reader to Section E.1 for
the CLT result of xn in Lemma 3.1.
We will use different techniques for different settings of step sizes in Assumption A2. Specifically,
for step sizes γn = (n + 1)−a, βn = (n + 1)−b, we consider the following scenarios:
Scenario 1: We consider case(ii): 1/2 < a = b ≤1, and will apply the almost sure convergence
result of the single-timescale stochastic approximation in Theorem G.8 and verify all the
conditions therein.
Scenario 2: We consider both case(i): 1/2 < a < b ≤1 and case (iii): 1/2 < b < a ≤1. In these
two cases, step sizes γn, βn decrease at different rates, thereby putting iterates xn, θn on
different timescales and resulting in a two-timescale structure. We will apply the existing
almost sure convergence result of the two-timescale stochastic approximation with iterate-
dependent Markov chain in Yaji & Bhatnagar (2020, Theorem 4) where our SA-SRRW
algorithm can be regarded as a special instance.11
D.1
SCENARIO 1
In Scenario 1, we have βn = γn. First, we rewrite (4) as

θn+1
xn+1

=

θn
xn

+ γn+1

H(θn.Xn+1)
δXn+1 −xn

.
(19)
11However, Yaji & Bhatnagar (2020) paper only analysed the almost sure convergence. The central limit
theorem analysis remains unknown in the literature for the two-timescale stochastic approximation with iterate-
dependent Markov chains. Thus, our CLT analysis in Section E for this two-timescale structure with iterate-
dependent Markov chain is still novel and recognized as our contribution.
16

Published as a conference paper at ICLR 2024
By augmentations, we define the variable zn ≜

θn
xn

∈R(N+D)×1 and the function G(zn, i) ≜

H(θn, i)
δi −xn

∈R(N+d)×1. In addition, we define a new Markov chain {Yn}n≥0 in the same state
space N as SRRW sequence {Xn}n≥0. With slight abuse of notation, the transition kernel of {Yn}
is denoted by K′[zn] ≡K[xn] and its stationary distribution π′[zn] ≡π[xn], where K[xn] and
π(xn) are the transition kernel and its corresponding stationary distribution of SRRW, with π[x] of
the form
πi[x] ∝
X
j∈N
µiPij(xi/µi)−α(xj/µj)−α.
(20)
Recall that µ is the fixed point, i.e., π[µ] = µ, and P is the base Markov chain inside SRRW (see
(3)). Then, the mean field
g(z) = EY ∼π′(z)[G(z, Y )] =
P
i∈N πi[x]H(θ, i)
π[x] −x

,
and z∗= (θ∗, µ) for θ∗∈Θ in Assumption A3 is the root of g(z), i.e., g(z∗) = 0. The augmented
iteration (19) becomes
zn+1 = zn + γn+1G(zn, Yn+1)
(21)
with the goal of solving g(z) = 0. Therefore, we can treat (21) as an SA algorithm driven by
a Markov chain {Yn}n≥0 with its kernel K′[z] and stationary distribution π′[z], which has been
widely studied in the literature (e.g., Delyon (2000); Benveniste et al. (2012); Fort (2015); Li et al.
(2023)). In what follows, we demonstrate that for any initial point z0 = (θ0, x0) ∈RD × Int(Σ),
the SRRW iteration {xn}n≥0 will almost surely converge to the target distribution µ, and the SA
iteration {θn}n≥0 will almost surely converge to the set Θ.
Now we verify conditions C1 - C4 in Theorem G.8. Our assumption A4 is equivalent to condition
C1 and assumption A2 corresponds to condition C2. For condition C3, we set ∇w(z) ≡−g(z),
and the set S ≡{z∗|θ∗∈Θ, x∗= µ}, including disjoint points. For condition C4, since K′[z],
or equivalently K[x], is ergodic and time-reversible for a given z, as shown in the SRRW work
Doshi et al. (2023), it automatically ensures a solution to the Poisson equation, which has been well
discussed in Chen et al. (2020a, Section 2) and Benveniste et al. (2012); Meyn (2022). To show (97)
and (98) in condition C4, for each given z and any i ∈N, we need to give the explicit solution mz(i)
to the Poisson equation mz(i) −(K′
zmz)(i) = G(z, i) −g(z) in (96). The notation (K′
zmz)(i) is
defined as follows.
(K′
zmz)(i) =
X
j∈N
K′
z(i, j)m(z, j).
Let G(z) ≜[G(z, 1), · · · , G(z, N)]T ∈RN×D. We use [A]:,i to denote the i-th column of matrix
A. Then, we let mz(i) such that
mz(i) =
∞
X
k=0
 [G(z)(K′[z]k)T ][:,i] −g(z)

=
∞
X
k=0
[G(z)((K′[z]k)T −π′[z]1T )][:,i].
(22)
In addition,
(K′
zmz)(i) =
∞
X
k=1
[G(z)(K′[z]T −π′[z]1T )][:,i].
(23)
We can check that the mz(i) form in (22) is indeed the solution of the above Poisson equation.
Now, by induction, we get K′[z]k −1π′[z]T = (K′[z] −1π′[z]T )k for k ≥1 and for k = 0,
K′[z]0 −1π′[z]T = (K′[z] −1π′[z]T )0 −1π′[z]T . Then,
mz(i) =
∞
X
k=0
[G(z)(K′[z]T −π′[z]1T )k][:,i] −g(z)
=
"
G(z)
∞
X
k=0
(K′[z]T −π′[z]1T )k
#
[:,i]
−g(z)
=

G(z)(I −K′[z]T + π′[z]1T )−1
[:,i] −g(z)
=
X
j∈N
(I −K′[z] + 1π′[z]T )−1(i, j)G(z, j) −g(z).
(24)
17

Published as a conference paper at ICLR 2024
Here, (I −K′[z] + 1π′[z]T )−1 is well defined because K′[z] is ergodic and time-reversible for any
given z (proved in Doshi et al. (2023, Appendix A)). Now that both functions H(θ, i) and δi −x
are bounded for each compact subset of RD × Σ by our assumption A1, function G(z, i) is also
bounded within the compact subset of its domain. Thus, function mz(i) is bounded, and (97) is
verified. Moreover, for a fixed i ∈N,
X
j∈N
(I −K′[z] + 1π′[z]T )−1(i, j)δj = (I −K′[z] + 1π′[z]T )−1
[:,i] = (I −K[x] + 1π[x]T )−1
[:,i]
and this vector-valued function is continuous in x because K[x], π[x] are continuous. We then
rewrite (24) as
mz(i) =
"P
j∈N (I −K[x] + 1π[x]T )−1(i, j)H(x, j)
(I −K[x]T + π[x]1T )−1
[:,i]
#
−
P
i∈N πi[x]H(θ, i)
π[x] −x

.
With continuous functions H(θ, i), K[x], π[x], we have mz(i) continuous with respect to z, so does
(K′
zmz)(i). This implies that functions mz(i) and (K′
zmz)(i) are locally Lipschitz, which satisfies
(98) with ϕC(x) = CCx for some constant CC that depends on the compact set C. Therefore,
condition C4 is checked, and we can apply Theorem G.8 to show the almost convergence result of
(19), i.e., almost surely,
lim
n→∞xn = µ,
and
lim sup
n→∞
inf
θ∗∈Θ ∥θn −θ∗∥= 0.
Therefore, the almost sure convergence of xn in Lemma 3.1 is also proved. This finishes the proof
in Scenario 1.
D.2
SCENARIO 2
Now in this subsection, we consider the steps sizes γn, βn with 1/2 < a < b ≤1 and 1/2 < b <
a ≤1. We will frequently use assumptions (B1) - (B5) in Section G.3 and Theorem G.10 to prove
the almost sure convergence.
D.2.1
CASE (I): 1/2 < a < b ≤1
In case (i), θn is on the slow timescale and xn is on the fast timescale because iteration θn has
smaller step size than xn, making θn converge slower than xn. Here, we consider the two-timescale
SA of the form:
θn+1 = θn + βn+1H(θ, Xn+1),
xn+1 = xn + γn+1(δXn+1 −x).
Now, we verify assumptions (B1) - (B5) listed in Section G.3.
• Assumptions (B1) and (B5) are satisfied by our assumptions A2 and A4.
• Our assumption A3 shows that the function H(θ, X) is continuous and differentiable w.r.t
θ and grows linearly with ∥θ∥. In addition, δX −x also satisfies this property. Therefore,
(B2) is satisfied.
• Now that the function π[x] −x is independent of θ, we can set ρ(θ) = µ for any θ ∈RD
such that π[µ] −µ = 0 from Doshi et al. (2023, Proposition 3.1), and
∇x(π(x) −x)|x=µ = 2αu1T −αPT −(α + 1)I
from Doshi et al. (2023, Lemma 3.4), which is Hurwitz. Furthermore, ρ(θ) = µ inherently
satisfies the condition ∥ρ(θ)∥≤L2(1+∥θ∥) for any L2 ≥∥µ∥. Thus, conditions (i) - (iii)
in (B3) are satisfied. Additionally, P
i∈N πi[ρ(θ)]H(θ, i) = P
i∈N πi[x] = h(θ) such
that for θ∗∈Θ defined in assumption A3, P
i∈N πi[ρ(θ∗)]H(θ∗, i) = h(θ∗) = 0, and
∇θh(θ∗) is Hurwitz. Therefore, (B3) is checked.
• Assumption (B4) is verified by the nature of SRRW, i.e., its transition kernel K[x] and the
corresponding stationary distribution π[x] with π[µ] = µ.
18

Published as a conference paper at ICLR 2024
Consequently, assumptions (B1) - (B5) are satisfied by our assumptoins A1 - A4 and by Theorem
G.10, we have limn→∞xn = µ and θn →Θ almost surely.
Next, we consider 1/2 < b < a ≤1. As discussed before, (B1), (B2), (B4) and (B5) are satisfied by
our assumptions A1 - A4 and the properties of SRRW. The only difference for this step size setting,
compared to the previous one 1/2 < a < b ≤1, is that the roles of θn, xn are now flipped, that is,
θn is now on the fast timescale while xn is on the slow timescale. By a much stronger Assumption
A3′, for any x ∈Int(Σ), (i) EX∼π[x][H(ρ(x), X)] = 0; (ii) EX∼π[x][∇H(ρ(x), X)] is Hurwitz;
(iii) ∥ρ(x)∥≤L2(1 + ∥x∥). Hence, conditions (i) - (iii) in (B3) are satisfied. Moreover, we have
π[µ] −µ = 0, ∇(π[x] −x)|x=µ being Hurwitz, as mentioned in the previous part. Therefore, (B3)
is verified. Accordingly, (B1) - (B5) are checked by our assumptions A1, A2, A3′, A4. By Theorem
G.10, we have limn→∞xn = µ and θn →Θ almost surely.
E
PROOF OF THEOREM 3.3
This section is devoted to the proof of Theorem 3.3, which also includes the proof of the CLT results
for the SRRW iteration xn in Lemma 3.1. We will use different techniques depending on the step
sizes in Assumption A2. Specifically, for step sizes γn = (n + 1)−a, βn = (n + 1)−b, we will
consider three cases: case (i): βn = o(γn); case (ii): βn = γn; and case (iii): γn = o(βn). For case
(ii), we will use the existing CLT result for single-timescale SA in Theorem G.9. For cases (i) and
(iii), we will construct our own CLT analysis for the two-timescale structure. We start with case (ii).
E.1
CASE (II): βn = γn
In this part, we stick to the notations for single-timescale SA studied in Section D.1. To utilize
Theorem G.9, apart from Conditions C1 - C4 that have been checked in Section D.1, we still need
to check conditions C5 and C6 listed in Section G.2.
Assumption A3 corresponds to condition C5. For condition C6, we need to obtain the explicit form
of function Qz to the Poisson equation defined in (96), that is,
Qz(i) −(K′
zQz)(i) = ψ(z, i) −Ej∼π[z][ψ(z, j)]
where
ψ(z, i) ≜
X
j∈N
K′
z(i, j)mz(j)mz(j)T −(K′
zmz)(i)(K′
zmz)(i)T .
Following the similar steps in the derivation of mz(i) from (22) to (24), we have
Qz(i) =
X
j∈N
(I −K′[z] + 1π′[z]T )−1(i, j)mz(j) −π′
j[z]mz(j).
We also know that Qz(i) and (K′
zQz)(i) are continuous in z for any i ∈N. For any z in a compact
set Ω, Qz(i) and (K′
zQz)(i) are bounded because function mz(i) is bounded. Therefore, C6 is
checked. By Theorem G.9, assume zn =

θn
xn

converges to a point z∗=

θ∗
µ

for θ∗∈Θ, we
have
γ−1/2
n
(zn −z∗)
dist.
−−−−→
n→∞N(0, V),
(26)
where V is the solution of the following Lyapunov equation
V
1{b=1}
2
I + ∇g(z∗)T

+
1{b=1}
2
I + ∇g(z∗)

V + U = 0,
(27)
and U = P
i∈N µi
 mz∗(i)mz∗(i)T −(Kz∗mz∗)(i)(Kz∗mz∗)(i)T 
.
19

Published as a conference paper at ICLR 2024
By algebraic calculations of derivative of π[x] with respect to x in (20),12 we can rewrite ∇g(z∗) in
terms of x, θ, i.e.,
J(α) ≜∇g(z∗) =
"
∂P
i∈N πi[x]H(θ,i)
∂θ
∂P
i∈N πi[x]H(θ,i)
∂x
∂(π[x]−x)
∂θ
∂π[x]−x
∂x
#
z=z∗
=

∇h(θ∗)
−αHT (PT + I)
0
2αµ1T −αPT −(α + 1)I

≜

J11
J12(α)
J21
J22(α)

,
where matrix H = [H(θ∗, 1), · · · , H(θ, N)]T . Then, we further clarify the matrix U. Note that
mz∗(i) =
∞
X
k=0
[G(z∗)((Pk)T −µ1T )][:,i] =
∞
X
k=0
[G(z∗)(Pk)T ][:,i] = E
" ∞
X
k=0
[G(z∗, Xk)]
 X0 = i
#
,
(28)
where the first equality holds because K′[µ] = P from the definition of SRRW kernel (3), the
second equality stems from G(z∗)µ = g(z∗) = 0, and the last term is a conditional expectation
over the base Markov chain {Xk}k≥0 (with transition kernel P) conditioned on X0 = i. Similarly,
with (K′
zmz)(i) in the form of (23), we have
(K′
zmz)(i) = E
" ∞
X
k=1
[G(z∗, Xk)]
 X0 = i
#
.
From the form ‘P
i∈N µi’ inside the matrix U, the Markov chain {Xk}k≥0 is in its stationary regime
from the beginning, i.e., Xk ∼µ for any k ≥0. Hence,
U = E


 ∞
X
k=0
[G(z∗, Xk)]
!  ∞
X
k=0
[G(z∗, Xk)]
!T 

−E


 ∞
X
k=1
[G(z∗, Xk)]
!  ∞
X
k=1
[G(z∗, Xk)]
!T 

= E

G(z∗, X0)G(z∗, X0)T 
+E

G(z∗, X0)
 ∞
X
k=1
G(z∗, Xk)
!T 

+ E
" ∞
X
k=1
G(z∗, Xk)
!
G(z∗, X0)T
#
= Cov(G(z∗, X0), G(z∗, X0))
+
∞
X
k=1
[Cov(G(z∗, X0), G(z∗, Xk)) + Cov(G(z∗, Xk), G(z∗, X0))] ,
(29)
where the covariance between G(z∗, X0) and G(z∗, Xk) for the Markov chain {Xn} in the station-
ary regime is Cov(G(z∗, X0), G(z∗, Xk)). By Br´emaud (2013, Theorem 6.3.7), it is demonstrated
that U is the sampling covariance of the base Markov chain P for the test function G(z∗, ·). More-
over, Br´emaud (2013, equation (6.34)) states that this sampling covariance U can be rewritten in the
following form:
U =
N−1
X
i=1
G(z∗)T uiuiG(z∗) =
N−1
X
i=1
1 + λi
1 −λi

HT uiuT
i H
HT uiuT
i
uiuT
i H
uiuT
i

≜

U11
U12
U21
U22

,
(30)
where {(λi, ui)}i∈N is the eigenpair of the transition kernel P of the ergodic and time-reversible
base Markov chain. This completes the proof of case 1.
12One may refer to Doshi et al. (2023, Appendix B, Proof of Lemma 3.4) for the computation of ∂π[x]−x
∂x
.
20

Published as a conference paper at ICLR 2024
Remark E.1. For the CLT result (26), we can look further into the asymptotic covariance matrix V
as in (27). For convenience, we denote V =

V11
V12
V21
V22

and U in the form of (30) such that

V11
V12
V21
V22
 1{b=1}
2
I + J(α)T

+
1{b=1}
2
I + J(α)
 
V11
V12
V21
V22

+ U = 0.
(31)
For the SRRW iteration xn, from (26) we know that γ−1/2
n
(xn −µ)
dist.
−−−−→
n→∞N(0, V4). Thus, in this
remark, we want to obtain the closed form of V22. By algebraic computations of the bottom-right
sub-block matrix, we have

2αµ1T −αPT −

α + 1−1{a=1}
2

I

V22
+ V22

2αµ1T −αPT −

α + 1−1{a=1}
2

I
T
+ U22 = 0.
By using result of the closed form solution to the Lyapunov equation (e.g., Lemma G.1) and the
eigendecomposition of P, we have
V22 =
N−1
X
i=1
1
2α(1 + λi) + 2 −1{a=1}
· 1 + λi
1 −λi
uiuT
i .
(32)
E.2
CASE (I): βn = o(γn)
In this part, we mainly focus on the CLT of the SA iteration θn because the SRRW iteration xn is
independent of θn and its CLT result has been shown in Remark E.1.
E.2.1
DECOMPOSITION OF SA-SRRW ITERATION (4)
We slightly abuse the math notation and define the function
h(θ, x) ≜Ei∼π[x]H(θ, i) =
X
i∈N
πi[x]H(θ, i)
such that h(θ, µ) ≡h(θ). Then, we reformulate (25) as
θn+1 = θn + βn+1h(θn, xn) + βn+1(H(θn, Xn+1) −h(θn, xn)).
(33a)
xn+1 = xn + γn+1(π[xn] −xn) + γn+1(δXn+1) −π[xn]).
(33b)
There exist functions qx : N →RN, ˜Hθ,x : N →RD satisfying the following Poisson equations
δi −π(x) = qx(i) −(Kxqx)(i)
(34a)
H(θ, i) −h(θ, x) = ˜Hθ,x(i) −(Kx ˜Hθ,x)(i),
(34b)
for any θ ∈RD, x ∈Int(Σ) and i ∈N, where (Kxqx)(i) ≜P
j∈N Kij[x]qx(j), (Kx ˜Hθ,x)(j) ≜
P
j∈N Kij[x] ˜Hθ,x(j). The existence and explicit form of the solutions qx, ˜Hθ,x, which are contin-
uous w.r.t x, θ, follow the similar steps that can be found in Section D.1 from (22) to (24). Thus, we
can further decompose (33) into
θn+1 =θn + βn+1h(θn, xn) + βn+1 ( ˜Hθn,xn(Xn+1) −(Kxn ˜Hθn,xn)(Xn))
|
{z
}
M (θ)
n+1
+ βn+1 ((Kxn+1 ˜Hθn+1,xn+1)(Xn+1) −(Kxn ˜Hθn,xn)(Xn+1))
|
{z
}
r(θ,1)
n
+ βn+1 ((Kxn ˜Hθn,xn)(Xn) −(Kxn+1 ˜Hθn+1,xn+1)(Xn+1))
|
{z
}
r(θ,2)
n
,
(35a)
21

Published as a conference paper at ICLR 2024
xn+1 =xn + γn+1(π(xn) −xn) + γn+1 (qxn(Xn+1) −(Kxnqxn)(Xn))
|
{z
}
M (x)
n+1
+ γn+1 ([Kxnqxn](Xn+1) −[Kxnqxn+1](Xn+1))
|
{z
}
r(x,1)
n
+ γn+1 ((Kxnqxn)(Xn) −(Kxn+1qxn+1)(Xn+1))
|
{z
}
r(x,2)
n
.
(35b)
such that
θn+1 = θn + βn+1h(θn, xn) + βn+1M (θ)
n+1 + βn+1r(θ,1)
n
+ βn+1r(θ,2)
n
,
(36a)
xn+1 = xn + γn+1(π(xn) −xn) + γn+1M (x)
n+1 + γn+1r(x,1)
n
+ γn+1r(x,2)
n
.
(36b)
We can observe that (36) differs from the expression in Konda & Tsitsiklis (2004); Mokkadem &
Pelletier (2006), which studied the two-timescale SA with Martingale difference noise. Here, due to
the presence of the iterate-dependent Markovian noise and the application of the Poisson equation
technique, we have additional non-vanishing terms r(θ,2)
n
, r(x,2)
n
, which will be further examined in
Lemma E.2. Additionally, when we apply the Poisson equation to the Martingale difference terms
M (θ)
n+1, M (x)
n+1, we find that there are some covariances that are also non-vanishing as in Lemma E.1.
We will mention this again when we obtain those covariances. These extra non-zero noise terms
make our analysis distinct from the previous ones since the key assumption (A4) in Mokkadem &
Pelletier (2006) is not satisfied. We demonstrate that the long-term average performance of these
terms can be managed so that they do not affect the final CLT result.
Analysis of Terms M (θ)
n+1, M (x)
n+1
Consider the filtration Fn ≜σ(θ0, x0, X0, · · · , θn, xn, Xn), it is evident that M (θ)
n+1, M (x)
n+1 are
Martingale difference sequences adapted to Fn. Then, we have
E
h
M (x)
n+1(M (x)
n+1)T  Fn
i
= E[qxn(Xn+1)qxn(Xn+1)T |Fn] + (Kxnqxn)(Xn) ((Kxnqxn)(Xn))T
−E[qxn(Xn+1)|Fn] (Kxnqxn)(Xn))T −(Kxnqxn)(Xn)E[qxn(Xn+1)T |Fn]
= E[qxn(Xn+1)qxn(Xn+1)T |Fn] −(Kxnqxn)(Xn) ((Kxnqxn)(Xn))T .
(37)
Similarly, we have
E
h
M (θ)
n+1(M (θ)
n+1)T  Fn
i
= E[ ˜Hθn,xn(Xn+1) ˜Hθn,xn(Xn+1)T |Fn] −(Kxn ˜Hθn,xn)(Xn)

(Kxn ˜Hθn,xn)(Xn)
T
,
(38)
and
E
h
M (x)
n+1(M (θ)
n+1)T  Fn
i
= E[qxn(Xn+1) ˜Hθn,xn(Xn+1)T |Fn] −(Kxnqxn)(Xn)

(Kxn ˜Hθn,xn)(Xn)
T
.
We now focus on E
h
M (x)
n+1(M (x)
n+1)T  Fn
i
. Denote by
V1(x, i) ≜
X
j∈N
Ki,j[x]qx(j)qx(j)T −(Kxqx)(i) ((Kxqx)(i))T ,
(39)
and let its expectation w.r.t the stationary distribution π(x) be v1(x) ≜Ei∼π(x)[V1(x, i)], we can
construct another Poisson equation, i.e.,
E
h
M (x)
n+1(M (x)
n+1)T  Fn
i
−
X
Xn∈N
πXn(xn)E
h
M (x)
n+1(M (x)
n+1)T  Fn
i
= V1(xn, Xn+1) −v1(xn)
= φ(1)
x (Xn+1) −(Kxnφ(1)
xn )(Xn+1),
22

Published as a conference paper at ICLR 2024
for some matrix-valued function φ(1)
x
: N →RN×N. Since qx and K[x] are continuous in x,
functions V1, v1 are also continuous in x. Then, we can decompose (39) into
V1(xn, Xn+1) = v1(µ)
| {z }
U22
+ v1(xn) −v1(µ)
|
{z
}
D(1)
n
+ φ(1)
xn (Xn+1) −(Kxnφ(1)
xn )(Xn)
|
{z
}
J(1,a)
n
+ (Kxnφ(1)
xn )(Xn) −(Kxnφ(1)
xn )(Xn+1)
|
{z
}
J(1,b)
n
.
(40)
Thus, we have
E[M (x)
n+1(M (x)
n+1)T |Fn] = U22 + D(1)
n
+ J(1)
n ,
(41)
where J(1)
n
= J(1,a)
n
+ J(1,b)
n
.
Following
the
similar
steps
above,
we
can
decompose
E
h
M (x)
n+1(M (θ)
n+1)T  Fn
i
and
E
h
M (θ)
n+1(M (θ)
n+1)T  Fn
i
as
E
h
M (x)
n+1(M (θ)
n+1)T  Fn
i
= U21 + D(2)
n
+ J(2)
n ,
(42a)
E
h
M (θ)
n+1(M (θ)
n+1)T  Fn
i
= U11 + D(3)
n
+ J(3)
n .
(42b)
where J(2)
n
= J(2,a)
n
+J(2,b)
n
and J(3)
n
= J(3,a)
n
+J(3,b)
n
. Here, we note that matrices Ji
n for i = 1, 2, 3
are in presence of the current CLT analysis of the two-timescale SA with Martingale difference noise.
In addition, U11, U12 and U22 inherently include the information of the underlying Markov chain
(with its eigenpair (λi, ui)), which is an extension of the previous works (Konda & Tsitsiklis, 2004;
Mokkadem & Pelletier, 2006).
Lemma E.1. For M (θ)
n+1, M (x)
n+1 defined in (35) and their decomposition in (41) and (42), we have
U11 =
N−1
X
i=1
1 + λi
1 −λi
uiuT
i ,
U21 =
N−1
X
i=1
1 + λi
1 −λi
uiuT
i H,
U11 =
N−1
X
i=1
1 + λi
1 −λi
HT uiuT
i H,
(43a)
lim
n→∞D(i)
n = 0 a.s.
for
i = 1, 2, 3,
(43b)
lim
n→∞γnE
"
n
X
k=1
J(i)
k

#
= 0,
for
i = 1, 2, 3.
(43c)
Proof. We now provide the properties of the four terms inside (41) as an example. Note that
U11 = Ei∼µ[V1(µ, i)] =
X
i∈N
µi

X
j∈N
P(i, j)qµ(j)qµ(j)T −(Pqµ)(i) ((Pqµ)(i))T


=
X
j∈N
µjqµ(j)qµ(j)T −(Pqµ)(j) ((Pqµ)(j))T .
We can see that it has exactly the same structure as matrix U in (27). Following the similar steps in
deducing the explicit form of U from (28) to (30), we get
U11 =
N−1
X
i=1
1 + λi
1 −λi
uiuT
i .
(44)
By the almost sure convergence result xn →µ in Lemma 3.1, v1(xn) →v1(µ) a.s. such that
limn→∞D(1)
n
= 0 a.s.
We next prove that limn→∞γnE
hPn
k=1 J(1,a)
k

i
= 0 and limn→∞γnE
hPn
k=1 J(1,b)
k

i
= 0.
23

Published as a conference paper at ICLR 2024
Since {J(1,a)
n
} is a Martingale difference sequence adapted to Fn, with the Burkholder inequality in
Lemma G.2 and p = 1, we show that
E
"
n
X
k=1
J(1,a)
k

#
≤C1E


v
u
u
t
 n
X
k=1
J(1,a)
k

2
!
.
(45)
By assumption A4, xn is always within some compact set Ωsuch that supn ∥J(1,a)
n
∥≤CΩ< ∞
and for a given trajectory ω of xn(ω),
γnCp
v
u
u
t
 n
X
k=1
J(1,a)
k

2
!
≤CpCΩγn
√n,
(46)
and the last term decreases to zero in n since a > 1/2.
For J(1,b)
n
, we use Abel transformation and obtain
n
X
k=1
J(1,b)
k
=
n
X
k=1
((Kxkφ(1)
xk )(Xk−1) −(Kxk−1φ(1)
xk−1)(Xk−1))
+ (Kx0φ(1)
x0 )(X0) −(Kxnφ(1)
xn )(Xn).
Since (Kxφ(1)
x )(X) is continuous in x, for xn within a compact set Ω(assumption A4), it is local
Lipschitz with a constant LΩsuch that
∥(Kxkφ(1)
xk )(Xk−1) −Kxk−1φ(1)
xk−1)(Xk−1)∥≤LΩ∥xk −xk−1∥≤2LΩγk.
where the last inequality arises from (4b), i.e., xk−xk−1 = γk(δXk −xk−1) and ∥δXk −xk−1∥≤2
because xn ∈Int(Σ). Also, ∥(Kx0φ(1)
x0 )(X0)∥+ ∥(Kxnφ(1)
xn )(Xn)∥are upper-bounded by some
positive constant C′
Ω. This implies that

n
X
k=1
J(1,b)
k
 ≤C′
Ω+ 2LΩ
n
X
k=1
γk.
Note that
γn

n
X
k=1
J(1,b)
k
 ≤γnC′
Ω+ 2LΩγn
n
X
k=1
γk ≤γnC′
Ω+ 2LΩ
a n1−2a,
(47)
where the last inequality is from Pn
k=1 γk <
1
an1−a. We observe that the last term in (47) is
decreasing to zero in n because a > 1/2.
Note that J(1)
k
= J(1,a)
k
+ J(1,b)
k
, by triangular inequality we have
γnE
"
n
X
k=1
J(11)
k

#
≤γnE
"
n
X
k=1
J(11,A)
k

#
+ γnE
"
n
X
k=1
J(11,B)
k

#
≤γnC1E


v
u
u
t
 n
X
k=1
J(11,A)
k

2
!
+ γnE
"
n
X
k=1
J(11,B)
k

#
= E

γnC1
v
u
u
t
 n
X
k=1
J(11,A)
k

2
!
+ γn

n
X
k=1
J(11,B)
k


,
(48)
where the second inequality comes from (45). By (46) and (47) we know that both terms in the last
line of (48) are uniformly bounded by constants over time n that depend on the set Ω. Therefore, by
dominated convergence theorem, taking the limit over the last line of (48) gives
lim
n→∞E

γnC1
v
u
u
t
 n
X
k=1
J(11,A)
k

2
!
+γn

n
X
k=1
J(11,B)
k



= E

lim
n→∞γnC1
v
u
u
t
 n
X
k=1
J(11,A)
k

2
!
+γn

n
X
k=1
J(11,B)
k


=0.
24

Published as a conference paper at ICLR 2024
Therefore, we have
lim
n→∞γnE
"
n
X
k=1
J(1)
k

#
= 0,
In sum, in terms of E[M (x)
n+1(M (x)
n+1)T |Fn] in (41), we have U11 in (44), limn→∞D(1)
n
= 0 a.s. and
limn→∞γnE
hPn
k=1 J(1)
k

i
= 0.
We can apply the same steps as above for the other two terms i = 2, 3 in (42) and obtain the
results.
Analysis of Terms r(θ,1)
n
, r(θ,2)
n
, r(x,1)
n
, r(x,2)
n
Lemma E.2. For r(θ,1)
n
, r(θ,2)
n
, r(x,1)
n
, r(x,2)
n
defined in (35), we have the following results:
∥r(θ,1)
n
∥= O(γn) = o(
p
βn),
√γn

n
X
k=1
r(θ,2)
k
 = O(√γn) = o(1).
(49a)
∥r(x,1)
n
∥= O(γn) = o(
p
βn),
√γn

n
X
k=1
r(x,2)
k
 = O(√γn) = o(1).
(49b)
Proof. For r(θ,1)
n
, note that
r(θ,1)
n
= (Kxn+1 ˜Hθn+1,xn+1)(Xn+1) −(Kxn ˜Hθn,xn)(Xn+1)
=
X
j∈N

KXn,j[xn+1] ˜Hθn+1,xn+1(j) −KXn,j[xn] ˜Hθn,xn(j)

≤
X
j∈N
LC(∥θn+1 −θn∥+ ∥xn+1 −xn∥)
≤NLC(CCβn+1 + 2γn+1)
(50)
where the second last inequality is because Ki,j[x] ˜Hθ,x(j) is continuous in θ, x K[x], which
stems from continuous functions K[x] and ˜Hθ,x. The last inequality is from update rules (4) and
(θn, xn) ∈Ωfor some compact subset Ωby assumption A4. Then, we have ∥r(θ,1)
n
∥= O(γn) =
o(√βn) because of a > 1/2 ≥b/2 by assumption A2.
We let νn ≜(Kxn ˜Hθn,xn)(Xn) such that r(θ,2)
n
= νn −νn+1. Note that Pn
k=1 r(θ,2)
k
= ν1 −νn+1,
and by assumption A4, ∥νn∥is upper bounded by a constant dependent on the compact set, which
leads to
√γn

n
X
k=1
r(θ,2)
k
 = √γn∥ν1 −νn+1∥= O(√γn) = o(1).
Similarly, we can also obtain ∥r(x,1)
n
∥= o(√βn) and √γn
Pn
k=1 r(x,2)
k
 = O(√γn) = o(1).
E.2.2
EFFECT OF SRRW ITERATION ON SA ITERATION
In view of the almost sure convergence results in Lemma 3.1 and Lemma 3.2, for large enough n so
that both iterations θn, xn are close to the equilibrium (θ∗, µ), we can apply the Taylor expansion
to functions h(θ, x) and π[x] −x in (36) at the point (θ∗, µ), which results in
h(θ, x) = h(θ∗, µ)+∇θh(θ∗, µ)(θ−θ∗)+∇xh(θ∗, µ)(x−µ)+O(∥θ−θ∗∥2+∥x−µ∥2), (51a)
π[x] −x = π[µ] −µ + ∇x(π(x) −x)|x=µ(x −µ) + O(∥x −µ∥2).
(51b)
With matrix J(α), we have the following:
J11 = ∇θh(θ∗, µ) = ∇h(θ∗),
J12(α) = ∇xh(θ∗, µ) = −αHT (PT + I),
J22(α) = ∇x(π(x) −x)|x=µ = 2αµ1T −αPT −(α + 1)I.
(52)
25

Published as a conference paper at ICLR 2024
Then, (36) becomes
θn+1 = θn + βn+1(J11(θn −θ∗) + J12(α)(xn −µ) + r(θ,1)
n
+ r(θ,2)
n
+ M (θ)
n+1 + η(θ)
n ), (53a)
xn+1 = xn + γn+1(J22(α)(xn −µ) + r(x,1)
n
+ r(x,2)
n
+ M (x)
n+1 + η(x)
n ),
(53b)
where η(θ)
n
= O(∥xn∥2 + ∥θn∥2) and η(x)
n
= O(∥xn∥2).
Then, inspired by Mokkadem & Pelletier (2006), we decompose iterates {xn} and {θn} into xn =
L(x)
n
+ ∆(x)
n
and θn = L(θ)
n
+ R(θ)
n
+ ∆(θ)
n . Rewriting (53b) gives
xn −µ = γ−1
n+1J22(α)−1(xn+1 −xn) −J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ M (x)
n+1 + η(x)
n ),
and substituting the above equation back in (53a) gives
θn+1 −θ∗= θn −θ∗+ βn+1

J11(θn −θ∗) + γ−1
n+1J12(α)J22(α)−1(xn+1 −xn)
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ M (x)
n+1 + η(x)
n ) + r(θ,1)
n
+ r(θ,2)
n
+ M (θ)
n+1 + η(θ)
n

= (I + βn+1J11)(θn −θ∗) + [βn+1γ−1
n+1J12(α)J22(α)−1(xn+1 −xn)]
+ βn+1(M (θ)
n+1 −J12(α)J22(α)−1M (x)
n+1)
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ η(x)
n )),
(54)
From (54) we can see the iteration {θn} implicitly embeds the recursions of three sequences
• βn+1γ−1
n+1J12(α)J22(α)−1(xn+1 −xn);
• βn+1(M (θ)
n+1 −J12(α)J22(α)−1M (x)
n+1);
• βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ η(x)
n ))).
Let un ≜Pn
k=1 βk and sn ≜Pn
k=1 γk. Below we define two iterations:
L(θ)
n
= eβnJ11L(θ)
n−1 + βn(M (θ)
n
−J12(α)J22(α)−1M (x)
n )
=
n
X
k=1
e(un−uk)J11βk(M (θ)
k
−J12(α)J22(α)−1M (x)
k
)
(55a)
R(θ)
n
= eβnJ11R(θ)
n−1 + βnγ−1
n J12(α)J22(α)−1(xn −xn−1)
=
n
X
k=1
e(un−uk)J11βkγ−1
k J12(α)J22(α)−1(xk −xk−1)
(55b)
and a remaining term ∆(θ)
n
≜θn −θ∗−L(θ)
n
−R(θ)
n .
Similarly, for iteration xn, define the sequence L(x)
n
such that
L(x)
n
= eγnJ22(α)L(x)
n−1 + γnM (x)
n
=
n
X
k=1
e(sn−sk)J22(α)γkM (x)
k
,
(56)
and a remaining term
∆(x)
n
≜xn −µ −L(θ)
n
(57)
The decomposition of θn −θ∗and xn −µ in the above form is also standard in the single-timescale
SA literature (Delyon, 2000; Fort, 2015).
Characterization of Sequences {L(θ)
n } and {L(x)
n }
we set a Martingale Z(n) = {Z(n)
k
}k≥1 such that
Z(n)
k
=
 
β−1/2
n
eunJ11
0
0
γ−1/2
n
esnJ22(α)
!
×
k
X
j=1
 
e−ukJ11βk(M (θ)
k
−J12(α)J22(α)−1M (x)
k
)
e−skJ22(α)γkM (x)
k
!
.
26

Published as a conference paper at ICLR 2024
Then, the Martingale difference array Z(n)
k
−Z(n)
k−1 becomes
Z(n)
k
−Z(n)
k−1 =
 
β−1/2
n
e(un−uk)J11βk(M (θ)
k
−J12(α)J22(α)−1M (x)
k
)
γ−1/2
n
e(sn−sk)J22(α)γkM (x)
k
!
and
n
X
k=1
E
h
(Z(n)
k
−Z(n)
k−1)(Z(n)
k
−Z(n)
k−1)T |Fk−1
i
=
A1,n
A2,n
AT
2,n
A4,n

,
where, in view of decomposition of M (θ)
n
and M (x)
n
in (41) and (42), respectively,
A1,n = β−1
n
n
X
k=1
β2
ke(un−uk)J11

U22+D(1)
k +J(1)
k −(U21+D(2)
k +J(2)
k )(J12(α)J22(α)−1)T
+ J12(α)J22(α)−1(U11 + D(3)
k
+ J(3)
k )(J12(α)J22(α)−1)T
−J12(α)J22(α)−1(U21 + D(2)
k
+ J(2)
k )T

e(un−uk)(J11)T ,
(58a)
A2,n = β−1/2
n
γ−1/2
n
n
X
k=1
βkγke(un−uk)J11(U21 −J12(α)J22(α)−1U11)e(sn−sk)J22(α)T ,
(58b)
A4,n = γ−1
n
n
X
k=1
γ2
ke(sn−sk)J22(α)(U11 + D(3)
k
+ J(3)
k )e(sn−sk)J22(α)T .
(58c)
We further decompose A1,n into three parts:
A1,n =β−1
n
n
X
k=1

β2
ke(un−uk)J11(U22 −U21(J12(α)J22(α)−1)T
−J12(α)J22(α)−1U12 + J12(α)J22(α)−1U11(J12(α)J22(α)−1)T )e(un−uk)(J11)T 
+ β−1
n
n
X
k=1

β2
ke(un−uk)J11(D(1)
k
+ J12(α)J22(α)−1D(3)
k (J12(α)J22(α)−1)T
−D(2)
k (J12(α)J22(α)−1)T −J12(α)J22(α)−1(D(2)
k )T )e(un−uk)(J11)T 
+ β−1
n
n
X
k=1

β2
ke(un−uk)J11(J(1)
k
+ J12(α)J22(α)−1J(3)
k (J12(α)J22(α)−1)T
−J(2)
k (J12(α)J22(α)−1)T −J12(α)J22(α)−1(J(2)
k )T )e(un−uk)(J11)T 
≜A(a)
1,n + A(b)
1,n + A(c)
1,n.
(59)
Here,
we define Uθ(α)
≜
U22 −U21(J12(α)J22(α)−1)T −J12(α)J22(α)−1U12 +
J12(α)J22(α)−1U11(J12(α)J22(α)−1)T . By (52) and (43a) in Lemma E.1, we have
Uθ(α) =
N−1
X
i=1
1
(α(1 + λi) + 1)2 · 1 + λi
1 −λi
HT uiuT
i H.
(60)
Then, we have the following lemma.
Lemma E.3. For A(a)
1,n, A(b)
1,n, A(c)
1,n defined in (59), we have
lim
n→∞A(a)
1,n = Vθ(α),
lim
n→∞∥A(b)
1,n∥= 0,
lim
n→∞∥A(c)
1,n∥= 0,
(61)
where Vθ(α) is the solution to the Lyapunov equation

J11 + 1{b=1}
2
I

Vθ(α) + Vθ(α)

J11 + 1{b=1}
2
I
T
+ Uθ(α) = 0.
27

Published as a conference paper at ICLR 2024
Proof. First, from Lemma G.4, we have for some c, T > 0 such that
∥A(b)
1,n∥≤β−1
n
n
X
k=1
D(1)
k
+ J12(α)J22(α)−1D(3)
k (J12(α)J22(α)−1)T −D(2)
k (J12(α)J22(α)−1)T
−J12(α)J22(α)−1(D(2)
k )T
 · β2
kc2e−2T (un−uk).
Applying Lemma G.6, together with D(i)
n →0 a.s. in Lemma E.1, gives
lim sup
n
∥A(b)
1,n∥
≤
1
C(b, p) lim sup
n
∥(D(1)
n
+ J12(α)J22(α)−1D(4)
n (J12(α)J22(α)−1)T
−D(2)
n (J12(α)J22(α)−1)T −J12(α)J22(α)−1D(3)
n )∥
= 0.
We now consider ∥A(c)
1,n∥. Set
Ξn ≜
n
X
k=1
 J(1)
n
+ J12(α)J22(α)−1J(3)
k (J12(α)J22(α)−1)T
−J(2)
k (J12(α)J22(α)−1)T −J12(α)J22(α)−1(J(2)
k )T 
,
we can rewrite A(c)
1,n as
A(c)
1,n = β−1
n
n
X
k=1
β2
ke(un−uk)J11(Ξk −Ξk−1)e(un−uk)(J11)T .
By the Abel transformation, we have
A(c)
1,n = βnΞn + β−1
n
n−1
X
k=1
h
β2
ke(un−uk)J11Ξke(un−uk)(J11)T
−β2
k+1e(un−uk+1)J11Ξke(un−uk+1)(J11)T i
.
(62)
We know from Lemma E.1 that βnΞn →0 a.s. because Ξn = o(γ−1
n ). Besides,
∥βke(un−uk)J11 −βk+1e(un−uk+1)J11∥
= ∥(βk −βk+1)e(un−uk)J11 + βk+1e(un−uk)J11(I −e−βk+1J11)∥
≤C1β2
ke−(un−uk)T
for some constant C1 > 0 because βn −βn+1 ≤C2β2
n and ∥I −e−βk+1J11∥≤C3βk+1. Moreover,
∥βke(un−uk)J11∥+ ∥βk+1e(un−uk+1)J11∥
≤βk∥e(un−uk)J11∥+ βk∥e(un−uk)J11∥· ∥e−βk+1J11∥
≤C4βke−(un−uk)T .
Using Lemma G.7 on (62) gives
∥A(c)
1,n∥≤C1C4β−1
n
n−1
X
k=1
β2
ke−2(un−uk)T ∥βkΞk∥+ ∥βnΞn∥.
Applying Lemma G.6 again gives
lim sup
n
∥A(c)
1,n∥≤C5 lim sup
n
∥βnΞn∥= 0
for some constant C5 > 0.
Finally, we provide an existing lemma below.
28

Published as a conference paper at ICLR 2024
Lemma E.4 (Mokkadem & Pelletier (2005) Lemma 4). For a sequence with decreasing step size
βn = (n + 1)−b for b ∈(1/2, 1], un = Pn
k=1 βk, a positive semi-definite matrix Γ and a Hurwitz
matrix Q, which is given by
β−1
n
n
X
k=1
β2
ne(un−uk)QΓe(un−uk)QT ,
we have
lim
n→∞β−1
n
n
X
k=1
β2
ne(un−uk)QΓe(un−uk)QT = V
where V is the solution of the Lyapunov equation

Q + 1{b=1}
2
I

V + V

QT + 1{b=1}
2
I

+ Γ = 0.
Then, limn→∞A(a)
1,n = Vθ(α) is a direct application of Lemma E.4.
We can follow the similar steps in Lemma E.3 to obtain
lim
n→∞A4,n = Vx(α),
where Vx(α) is in the form of (32).
The last step is to show limn→∞A2,n = 0. Note that
∥A2,n∥= O
 
β−1/2
n
γ−1/2
n
n
X
k=1
βkγk∥e(un−uk)J11∥∥e(sn−sk)J22(α)T ∥
!
= O
 
β−1/2
n
γ−1/2
n
n
X
k=1
βkγke−(un−uk)T e−(sn−sk)T ′
!
= O
 
β−1/2
n
γ−1/2
n
n
X
k=1
βkγke−(sn−sk)T ′
!
,
where the second equality is from Lemma G.4. Then, we use Lemma G.6 with p = 0 to obtain
n
X
k=1
βkγke−(sn−sk)T ′ = O(βn)
(63)
Additionally, since βn = o(γn), we have
β−1/2
n
γ−1/2
n
n
X
k=1
βkγ−1/2
k
γ3/2
k
e−(sn−sk)T ′ = O(β1/2
n
γ−1/2
n
) = o(1).
Then, it follows that limn→∞A2,n = 0. Therefore, we obtain
lim
n→∞
n
X
k=1
E
h
(Z(n)
k
−Z(n)
k−1)(Z(n)
k
−Z(n)
k−1)T |Fk−1
i
=

Vθ(α)
0
0
Vx(α)

.
Now, we turn to verifying the conditions in Theorem G.3. For some τ > 0, we have
n
X
k=1
E
h
∥Z(n)
k
−Z(n)
k−1∥2+τ|Fk−1
i
= O
 
β
−(1+ τ
2 )
n
n
X
k=1
β
2+ τ
2
k
β
τ
2
k e−(2+τ)(un−uk)T + γ
−(1+ τ
2 )
n
n
X
k=1
γ
2+ τ
2
k
γ
τ
2
k e−(2+τ)(sn−sk)T ′
!
= O

β
τ
2n + γ
τ
2n

(64)
29

Published as a conference paper at ICLR 2024
where the last equality comes from Lemma G.6. Since (64) also holds for τ = 0, we have
n
X
k=1
E
h
∥Z(n)
k
−Z(n)
k−1∥2|Fk−1
i
= O(1) < ∞.
Therefore, all the conditions in Theorem G.3 are satisfied and its application then gives
Z(n) =
 p
β−1
n L(θ)
n
p
γ−1
n L(x)
n
!
n→∞
−−−−→
dist.
N

0,

Vθ(α)
0
0
Vx(α)

.
(65)
Furthermore, we have the following lemma about the strong convergence rate of {L(θ)
n } and {L(x)
n }.
Lemma E.5.
∥L(θ)
n ∥= O
p
βn log(un)

a.s.
(66a)
∥L(x)
n ∥= O
p
γn log(sn)

a.s.
(66b)
Proof. This proof follows Pelletier (1998, Lemma 1). We only need the special case of Pelletier
(1998, Lemma 1) that fits our scenario; e.g., we let the two types of step sizes therein to be the same.
Specifically, we attach the following lemma.
Lemma E.6 (Pelletier (1998) Lemma 1). Consider a sequence
Ln+1 = eunH
n
X
k=1
e−ukHβkMk+1,
where βn = n−b, 1/2 < b ≤1, and {Mn} is a Martingale difference sequence adapted to the
filtration F such that, almost surely, lim supn E[∥Mn+1∥2|Fn] ≤M 2 and there exists τ ∈(0, 2),
b(2 + τ) > 2, such that supn E[∥Mn+1∥2+τ|Fn] < ∞. Then, almost surely,
lim sup
n
∥Ln∥
p
βn log(un)
≤CM,
(67)
where CM is a constant dependent on M.
By assumption A4, the iterates (θn, xn) are bounded within a compact subset Ω. Recall the form
of M (θ)
n+1, M (x)
n+1 defined in (35), it comprises the functions ˜Hθn,xn(i) and (Kxn ˜Hθn,xn)(i), which
in turn include the function H(θ, i). We know that H(θ, i) is bounded for θ in some compact set
C. Thus, for any (θn, xn) ∈Ωfor some compact set Ω, M (θ)
n+1, M (x)
n+1 are bounded and we denote
by cθ and cx as their upper bounds, i.e., E[∥M (θ)
n+1∥2|Fn] ≤c(θ)
Ω
and E[∥M (x)
n+1∥2|Fn] ≤c(x)
Ω. We
only need to replace the upper bound c in Lemma E.6 by c(θ)
Ω
for the sequence {L(θ)
n } (resp. c(x)
Ω
for the sequence {L(x)
n }), i.e.,
lim sup
n
∥L(θ)
n ∥
p
βn log(un)
≤C(θ)
Ω,
(68a)
lim sup
n
∥L(x)
n ∥
p
γn log(sn)
≤C(x)
Ω,
(68b)
such that ∥L(θ)
n ∥= O(
p
βn log(un)) a.s. and ∥L(x)
n ∥= O(
p
γn log(sn)) a.s. which completes the
proof.
Note that we have xn −µ and L(x)
n
weakly converge to the same Gaussian distribution from Remark
E.1 and (65). Then, γ−1/2
n
∆(x)
n
weakly converges to zero, implying that γ−1/2
n
∆(x)
n
converges to
zero with probability 1. Therefore, together with {γn} being strictly positive, we have
∆(x)
n
= o(√γn)
a.s.
(69)
30

Published as a conference paper at ICLR 2024
Characterization of Sequences {R(θ)
n } and {∆(θ)
n }
We first consider the sequence {R(θ)
n }. We assume a positive real-valued bounded sequence {wn}
under the same conditions as in Mokkadem & Pelletier (2006, Definition 1), i.e.,
Definition E.1. In the case b < 1,
wn
wn+1 = 1 + o(βn), which also implies
wn
wn+1 = 1 + o(γn).
In the case b = 1, there exist ϵ ≥0 and a nondecreasing slowly varying function l(n) such that
wn = n−ϵl(n). When ϵ = 0, we require function l(n) to be bounded.
Since ∥xn −µ∥= o(1) by a.s. convergence result, we can assume that there exists {wn} such that
∥xn −µ∥= O(wn). Then, from (55b), we can use the Abel transformation and obtain
R(θ)
n
= βnγ−1
n J12(α)J22(α)−1(xn −µ) −e(un−u1)J11β1γ−1
1 U11J12(α)J22(α)−1(x1 −µ)
+ eunJ11
n−1
X
k=1
 e−ukJ11βkγ−1
k
−e−uk+1J11βk+1γ−1
k+1

J12(α)J22(α)−1(xk+1 −µ),
where the last term on the RHS can be rewritten as
Wn =
n−1
X
k=1
e(un−uk+1)J11βk+1γ−1
k+1
 eβk+1J11βkβ−1
k+1γ−1
k γk+1 −I

J12(α)J22(α)−1(xk+1 −µ).
Using Lemma G.6 on Wn gives ∥Wn∥= O(γ−1
n ∥eβnJ11 −I∥∥xn −µ∥) = O(γ−1
n βnωn). Then,
it follows that for some T > 0,
∥R(θ)
n ∥= O
 βnγ−1
n ωn + ∥eunJ11∥

= O(βnγ−1
n ωn + e−unT )
(70)
with the application of Lemma G.4 to the second equality.
Then, we shift our focus on {∆(θ)
n }. Specifically, we take (54), (55a), and (56) back to ∆(θ)
n
=
θn −θ∗−L(θ)
n
−R(θ)
n , and obtain
∆(θ)
n+1 =(I + βn+1J11)(θn −θ∗)
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ η(x)
n ))
−eβn+1J11L(θ)
n
−eβn+1J11R(θ)
n
=(I + βn+1J11)(θn −θ∗)
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ η(x)
n ))
−(I + βn+1J11 + O(β2
n+1))L(θ)
n
−(I + βn+1J11 + O(β2
n+1))R(θ)
n
=(I + βn+1J11)∆(θ)
n
+ O(β2
n+1)(L(θ)
n
+ R(θ))
n
)
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n
−J12(α)J22(α)−1(r(x,1)
n
+ r(x,2)
n
+ η(x)
n )),
(71)
where the second equality is by taking the Taylor expansion eβn+1J11 = I + βn+1J11 + O(β2
n+1).
Define Φk,n ≜Qn
j=k+1(I + βjJ11) and by convention Φn,n = I. Then, we rewrite (71) as
∆(θ)
n+1 =
n
X
k=1
Φk,nβk+1

O(βk+1)L(θ)
k
+ O(βk+1)R(θ)
k

+
n
X
k=1
Φk,nβk+1(r(θ,1)
k
+ r(θ,2)
k
+ η(θ)
k
−J12(α)J22(α)−1(r(x,1)
k
+ r(x,2)
k
+ η(x)
k ))
=
n
X
k=1
Φk,nβk+1

O(βk+1)L(θ)
k
+ O(βk+1)R(θ)
k

+
n
X
k=1
Φk,nβk+1(r(θ,1)
k
+ η(θ)
k
−J12(α)J22(α)−1(r(x,1)
k
+ η(x)
k ))
+
n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
).
(72)
31

Published as a conference paper at ICLR 2024
From (72), we can indeed decompose ∆(x)
n+1 into two parts ∆(θ)
n+1 = ∆(θ,1)
n+1 + ∆(θ,2)
n+1 , where
∆(θ,1)
n+1 ≜
n
X
k=1
Φk,nβk+1

O(βk+1)L(θ)
k
+ O(βk+1)R(θ)
k

+
n
X
k=1
Φk,nβk+1(r(θ,1)
k
+ η(θ)
k
−J12(α)J22(α)−1(r(x,1)
k
+ η(x)
k )),
(73a)
∆(θ,2)
n+1 ≜
n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
).
(73b)
This term ∆(θ,1)
n+1 shares the same recursive form as in the sequence defined in Mokkadem & Pelletier
(2006, Lemma 6), which is given below.
Lemma E.7 (Mokkadem & Pelletier (2006) Lemma 6). For ∆(θ,1)
n+1 in the form of (73a), assume
∥xn −µ∥= O(ωn) and ∥∆(x)
n ∥= O(δn) for the sequences ωn, δn defined in (E.1). Then, we have
∥∆(θ,1)
n+1 ∥= O(β2
nγ−2
n ω2
n + βnγ−1
n δn) + o(
p
βn)
a.s.
Since we already have ∆(x)
n
= o(√γn) in (69), together with Lemma E.7, we have
∥∆(θ,1)
n+1 ∥= O(β2
nγ−2
n ω2
n) + o(βnγ−1/2
n
) + o(
p
βn) = O(β2
nγ−2
n ω2
n) + o(
p
βn)
where the second equality comes from o(βnγ−1/2
n
) = o(β1/2
n
(βnγ−1
n )1/2) = o(β1/2
n
).
We now focus on ∆(θ,2)
n+1 . Define a sequence
Ψn ≜
n
X
k=1
r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
,
(74)
and we have
β−1/2
n+1
n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
)
=β−1/2
n+1
n
X
k=1
Φk,nβk+1(Ψk −Ψk−1)
=β1/2
n+1Ψn + β−1/2
n+1
n−1
X
k=1
(βkΦk,n −βk+1Φk+1,n)Ψk
where the last equality comes from the Abel transformation. Note that
∥βkΦk,n −βk+1Φk+1,n∥≤βk+1∥Φk,n −Φk+1,n∥+ (βk −βk+1)∥Φk,n∥
≤βk+1∥Φk+1,n∥βk∥J11∥+ C7β2
k∥Φk,n∥
≤C8β2
ke−(un−uk)T
for some constant C7, C8 > 0, where the last inequality is from Lemma G.4 and ∥Φk+1,n∥≤
C9∥Φk,n∥for some constant C9 > 0 that depends on eβ0T . Then,
β−1/2
n+1

n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
)

≤∥β1/2
n+1Ψn∥+
βn+1
βn
1/2
β−1/2
n
n
X
k=1
∥βkΦk,n −βk+1Φk+1,n∥∥Ψk∥
≤∥β1/2
n+1Ψn∥+ C8
βn+1
βn
1/2
β−1/2
n
n
X
k=1
β3/2
k
e−(un−uk)T ∥β1/2
k
Ψk∥.
32

Published as a conference paper at ICLR 2024
By Lemma E.1, we have β1/2
n
Ψn →0 a.s. such that by Lemma G.6, it follows that
lim sup
n
β−1/2
n+1

n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
)
 ≤lim supn ∥β1/2
n
Ψn∥
C(T, 1/2)
= 0.
Therefore, we have
∆(θ,2)
n+1 =
n
X
k=1
Φk,nβk+1(r(θ,2)
k
−J12(α)J22(α)−1r(x,2)
k
) = o(
p
βn).
(75)
Consequently, ∆(θ)
n+1 = O(β2
nγ−2
n ω2
n) + o(√βn) almost surely.
Now we are dealing with xn −µ and its related sequence ωn. Note that by Lemma E.5 and (69),
we have almost surely,
∥xn −µ∥= O(∥L(x)
n ∥+ ∥∆x
n∥)
= O(
p
γn log(sn) + o(√γn))
= O(
p
γn log(sn)).
(76)
Thus, we can set ωn ≡O(
p
γn log(sn)) such that ∥R(θ)
n ∥in (70) can be written as
∥R(θ)
n ∥= O(na/2−bp
log(sn) + e−unT ),
and
∥∆(θ)
n+1∥= O(na−2blog(sn)) + o(
p
βn).
In view of assumption A2 and βn = o(γn), a/2−b < −b/2 and a−2b < −b, there exists a c > b/2
such that almost surely,
∥R(θ)
n ∥= O(n−s),
∥∆(θ)
n+1∥= o(
p
βn).
Therefore, β−1/2
n
(R(θ)
n
+ ∆(θ)
n+1) →0 almost surely. This completes the proof of Scenario 2.
E.3
CASE (III): γn = o(βn)
For γn = o(βn), we can see that the roles of θn and xn are flipped, i.e., θn is now on fast timescale
while xn is on slow timescale.
We still decompose xn as xn −µ = L(x)
n
+ ∆(x)
n , where L(x)
n , ∆(x)
n
are defined in (56) and (57),
respectively. Since xn is independent of θn, the results of L(x)
n
and ∆(x)
n
remain the same, i.e.,
almost surely, L(x)
n
= O(
p
γn log(sn)) from Lemma E.5 and ∆(x)
n
= o(√γn) from (69). Then, we
define sequences ˆL(θ)
n
and ˆR(θ)
n
as follows.
ˆL(θ)
n
≜eβnJ11 ˆL(θ)
n−1 + βnM (θ)
n
=
n
X
k=1
e(un−uk)J11βkM (θ)
k
,
(77a)
ˆR(θ)
n
≜eβnJ11 ˆR(θ)
n−1 + βnJ12(α)(L(x)
n−1 + R(x)
n−1) =
n
X
k=1
e(un−uk)J11βkJ12(α)(L(x)
k−1 + R(x)
k−1).
(77b)
Moreover, the remaining term ˆ∆(θ)
n
≜θn −θ∗−ˆL(θ)
n
−ˆR(θ)
n .
The proof outline is the same as in the previous scenario:
• We first show β−1/2
n
ˆ∆(θ)
n
weakly converges to the distribution N(0, V(3)
θ )(α);
• We analyse ˆL(θ)
n
and ˆR(θ)
n
to ensure that these two terms decrease faster than the CLT scale
β−1/2
n
, i.e., limn→∞β−1/2
n
(ˆL(θ)
n
−ˆR(θ)
n ) = 0;
33

Published as a conference paper at ICLR 2024
• With above two steps, we can show that β−1/2
n
(θn −θ∗) weakly converges to the distribu-
tion N(0, V(3)
θ )(α).
Analysis of ˆL(θ)
n
We first focus on ˆL(θ)
n
and follow similar steps as we did when we analysed L(θ)
n
in the previous
scenario. We set a Martingale Z(n) = {Z(n)
k
}k≥1 such that
Z(n)
k
= β−1/2
n
n
X
k=1
e(un−uk)J11βkM (θ)
k .
Then,
An ≜
n
X
k=1
E
h
(Z(n)
k
−Z(n)
k−1)(Z(n)
k
−Z(n)
k−1)T  Fk−1
i
.
Following the similar steps in (59) to decompose M (θ)
k
with (42b), we have
An = β−1
n
n
X
k=1
β2
ke(un−uk)J11 
U11 + D(3)
k
+ J(3)
k

e(un−uk)JT
11
= β−1
n
n
X
k=1
β2
ke(un−uk)J11U11e(un−uk)JT
11
|
{z
}
A(a)
n
+ β−1
n
n
X
k=1
β2
ke(un−uk)J11D(3)
k e(un−uk)JT
11
|
{z
}
A(b)
n
+ β−1
n
n
X
k=1
β2
ke(un−uk)J11J(3)
k e(un−uk)JT
11
|
{z
}
A(c)
n
(78)
Since A(a)
n , A(b)
n , A(c)
n
share similar forms as in Lemma E.3, we follow the same steps as the proof
therein, with the application of Lemma E.1. To avoid repetition, we omit the proof and directly give
the following lemma.
Lemma E.8. For A(a)
n , A(b)
n , A(c)
n defined in (78), we have
lim
n→∞A(a)
n
= V(3)
θ (α),
lim
n→∞∥A(b)
n ∥= 0,
lim
n→∞∥A(c)
n ∥= 0,
(79)
where V(3)
θ (α) is the solution to the Lyapunov equation
J11V + VJT
11 + U11 = 0.
Note that here we don’t have the term
1{b=1}
2
I in above lemma, compared to Lemma E.3, because
in the case of γn = o(βn), b < 1 such that 1{b=1} = 0. Then, applying Lemma G.1 to derive the
closed form of V(3)
θ (α) gives
V(3)
θ (α) =
R ∞
0
et∇θh(θ∗)U11et∇θh(θ∗)dt.
Thus, it follows that
lim
n→∞
n
X
k=1
E
h
(Z(n)
k
−Z(n)
k−1)(Z(n)
k
−Z(n)
k−1)T |Fk−1
i
= V(3)
θ (α).
Again, we use the Martingale CLT result in Theorem G.3 and have the following result.
Zn = β−1/2
n
ˆL(θ)
n
n→∞
−−−−→
dist.
N

0, V(3)
θ (α)

.
Moreover, similar to the tighter upper bound of L(x)
n
proved in Lemma E.5, we utilize the tighter
upper bound Lemma E.6 in the proof thereof, and obtain ˆL(θ)
n
= O(
p
βn log(un)).
34

Published as a conference paper at ICLR 2024
Analysis of ˆR(θ)
n
Next, we turn to the term ˆR(θ)
n
in (77b). Taking the norm gives the following inequality for some
constant C, T > 0 by applying Lemma G.4,
∥ˆR(θ)
n ∥≤C
n
X
k=1
e−(un−uk)T βk(∥L(x)
k−1∥+ ∥R(x)
k−1∥).
Using Lemma G.6 gives
n
X
k=1
e−(un−uk)T βk(∥L(x)
k−1∥+ ∥R(x)
k−1∥) = O(∥L(x)
k−1∥+ ∥R(x)
n−1∥).
Thus, β−1/2
n
∥ˆR(θ)
n ∥= o(
p
γnβ−1
n ) + O
q
γnβ−1
n log(sn)

. Since γn = o(βn), γnβ−1
n
= (n +
1)b−a, where b −a < 0. Then, there exists some s > 0 such that b −a < −s < 0. Together with
log(sn) = O(log(n)), we have O
q
γnβ−1
n log(sn)

= O(
p
n−s log(n)) = o(1). Therefore, we
have
lim
n→∞β−1/2
n
ˆR(θ)
n
= 0.
Analysis of ˆ∆(θ)
n
Lastly, let’s focus on the term ˆ∆(θ)
n . We have
ˆ∆(θ)
n+1 = θn+1 −θ∗−ˆL(θ)
n+1 −ˆR(θ)
n+1
= θn −θ∗+ βn+1

J11(θn −θ∗) + J12(α)(xn −µ) + M (θ)
n+1 + r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n

−eβn+1J11 ˆL(θ)
n
−βn+1M (θ)
n+1 −eβn+1J11 ˆR(θ)
n
−βn+1J12(α)(L(x)
n
+ R(x)
n )
= (I + βn+1J11)(θn −θ∗) + βn+1J12(α)∆(x)
n
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n )
−(I + βn+1J11 + O(β2
n+1))(ˆL(θ)
n
+ ˆR(θ)
n )
= (I + βn+1J11) ˆ∆(θ)
n
+ βn+1J12(α)∆(x)
n
+ βn+1(r(θ,1)
n
+ r(θ,2)
n
+ η(θ)
n )
+ O(β2
n+1)(ˆL(θ)
n
+ ˆR(θ)
n ).
where the second equality is from (53a), the third equality stems from the approximation of eβn+1J11.
Then, we again use the definition Φk,n ≜Qn
j=k+1(I + βjJ11) and reiterate the above equation as
ˆ∆(θ)
n+1 =
n
X
k=1
Φk,nβk+1

O(βk+1)L(θ)
k
+ O(βk+1)R(θ)
k

+
n
X
k=1
Φk,nβk+1J12(α)∆(x)
n
+
n
X
k=1
Φk,nβk+1(r(θ,1)
k
+ η(θ)
k )
+
n
X
k=1
Φk,nβk+1r(θ,2)
k
≜ˆ∆(θ,1)
n+1 + ˆ∆(θ,2)
n+1 ,
where ˆ∆(θ,2)
n+1 = Pn
k=1 Φk,nβk+1r(θ,2)
k
and
ˆ∆(θ,1)
n+1 =
n
X
k=1
Φk,nβk+1

O(βk+1)L(θ)
k
+ O(βk+1)R(θ)
k

+
n
X
k=1
Φk,nβk+1(r(θ,1)
k
+ η(θ)
k
+ J12(α)∆(x)
n ).
(80)
35

Published as a conference paper at ICLR 2024
For ˆ∆(θ,2)
n+1 , we follow the same steps from (74) to (75), and obtain ˆ∆(θ,2)
n+1 = o(√βn).
Next, we consider ˆ∆(θ,1)
n+1 and want to show that ˆ∆(θ,1)
n+1 = o(√βn). Again, we utilize Mokkadem &
Pelletier (2006, Lemma 6) for ˆ∆(θ,1)
n+1 and adapt the notation here for the case γn = o(βn).
Lemma E.9. For ˆ∆(θ,1)
n+1 in the form of (80), assume ∥θn −θ∗∥= O(ωn) and ∥ˆ∆(θ,1)
n
∥= O(δn)
for the sequences ωn, δn defined in (E.1). Then, we have
∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−2
n ω2
n + γnβ−1
n δn) + o(√γn)
a.s.
(81)
Now we need to further analyse δn and tighten its big O form, starting from δn ≡1, so that we
can finally obtain the big O form of ∥ˆ∆(θ,1)
n+1 ∥. The following steps are borrowed from the ideas in
Mokkadem & Pelletier (2006, Section 2.3.2).
By almost sure convergence result limn→∞θn = θ∗, we have limn→∞∆(θ)
n
= 0 a.s. such that we
can first set δn ≡1, and ∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−2
n ω2
n + γnβ−1
n ) + o(√γn). Then, we redefine
δn ≡O(γ2
nβ−2
n ω2
n + γnβ−1
n ) + o(√γn),
and notice that it still satisfies definition E.1. Then, reapplying this δn form to (81) gives
∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−2
n ω2
n + [γnβ−1
n ]2) + o(√γn)
and by induction we have for all integers k ≥1,
∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−2
n ω2
n + [γnβ−1
n ]k) + o(√γn).
Since [γnβ−1
n ]k = n(b−a)k, there exists k0 > a/2(a −b) such that [γnβ−1
n ]k0 = o(√γn), and
∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−2
n ω2
n) + o(√γn).
(82)
Then, as suggested in Mokkadem & Pelletier (2006, Section 2.3.2), we can choose ωn
=
O(
p
βn log(un) + [γnβ−1
n ]k), which also satisfies definition E.1. Then,
∥θn −θ∗∥= ∥ˆL(θ)
n
+ ˆR(θ)
n
+ ˆ∆(θ)
n ∥
=O
p
βn log(un)+
q
γnβ−1
n log(sn)+

[γnβ−1
n ]k+1+γnβ−1
n
p
βn log(un)
2
+ o(
p
βn + √γn)
=O(
p
βn log(un) + [γnβ−1
n ]k+1).
By induction, this holds for all k ≥1 such that there exists k0, [γnβ−1
n ]k0 = o(√βn) and ∥θn −
θ∗∥= O(
p
βn log(un)). Equivalently, ωn =
p
βn log(un). Therefore, from (82) we have
∥ˆ∆(θ,1)
n+1 ∥= O(γ2
nβ−1
n log(un)) + o(√γn) = o(√γn).
Together with ∥ˆ∆(θ,2)
n+1 ∥= o(√βn), we have β−1/2
n
∥ˆ∆(θ)
n+1∥= o(
p
γnβ−1
n ) + 1) such that
lim
n→∞β−1/2
n
ˆ∆(θ)
n+1 = 0.
Thus, we have finished the proof according to the proof outline mentioned at the beginning of this
part.
F
DISCUSSION OF COVARIANCE ORDERING OF SA-SRRW
F.1
PROOF OF PROPOSITION 3.4
For any α > 0 and any vector x ∈Rd, we have
xT V(1)
θ (α)x =
Z ∞
0
xT et(∇θh(θ∗)+
1{b=1}
2
I)Uθ(α)et(∇θh(θ∗)+
1{b=1}
2
I)T x dt
36

Published as a conference paper at ICLR 2024
where the first equality is from the form of V(1)
θ (α) in Theorem 3.3. Let y ≜et(∇θh(θ∗)+
1{b=1}
2
I)x,
with the dependence on variable t left implicit. The matrix Uθ(α), given explicitly in (11) positive
semi definite, since λi ∈(−1, 1) for all i ∈{1, · · · , N −1}. Thus, the terms yT Uθ(α)y inside the
integral are non-negative, and it is enough to provide an ordering on yT Uθ(α)y with respect to α.
For any α2 > α1 > 0,
yT Uθ(α2)y =
N−1
X
i=1
1
(α2(1 + λi) + 1)2 · 1 + λi
1 −λi
yT HT uiuT
i Hy
<
N−1
X
i=1
1
(α1(1 + λi) + 1)2 · 1 + λi
1 −λi
yT HT uiuT
i Hy = yT Uθ(α1)y
<
N−1
X
i=1
·1 + λi
1 −λi
yT HT uiuT
i Hy = yT Uθ(0)y,
where the inequality13 is because α(1 + λi) > 0 for all i ∈{1, · · · , N} and any α > 0. In fact,
the ordering is monotone in α, and yT Uθ(α2)y decreases at rate 1/α2 as seen form its form in the
equation above. This completes the proof.
F.2
DISCUSSION REGARDING PROPOSITION 3.4 AND MSE ORDERING
We can use Proposition 3.4 to show that the MSE of SA iterates of (4c) driven by SRRW eventually
becomes smaller than that SA iterates when the stochastic noise is driven by an i.i.d. sequence of ran-
dom variables. The diagonal entries of V(1)
θ (α) are obtained by evaluating eT
i V(1)
θ (α)ei, where ei is
the i’th standard basis vector.14 These diagonal entries are the asymptotic variance corresponding to
the element-wise iterate errors, and for large enough n, we have eT
i V(1)
θ (α)ei ≈E[(θn −θ∗)2
i ]/βn
for all i ∈{1, · · · , D}. Thus, the trace of matrix V(1)
θ (α) approximates the scaled MSE, that is
Tr(V(1)
θ (α)) = P
i eT
i V(1)
θ (α)ei ≈P
i E[(θn −θ∗)2
i ]/βn = E[∥θn −θ∗∥2]/βn for large n. Since
all entries of V(1)
θ (α) go to zero as α increases, they get smaller than the corresponding term for the
SA algorithm with i.i.d. input for large enough α, which achieves a constant MSE in the similarly
scaled limit, since the asymptotic covariance is not a function of α. Moreover, the value of α only
needs to be moderately large, since the asymptotic covariance terms decrease at rate O(1/α2) as
shown in Proposition 3.4.
F.3
PROOF OF COROLLARY 3.5
We see that V(3)
θ (α) = V(3)
θ (0) for all α > 0, because the form of V(3)
θ (α) in Theorem 3.3 is
independent of α. To prove that V(1)
θ (α) <L V(3)
θ (0), it is enough to show that V(1)
θ (0) = V(3)
θ (0),
since V(1)
θ (α) <L V(1)
θ (0) from Proposition 3.4. This is easily checked by substituting α = 0 in 11,
for which Uθ(0) = U11. Substituting in the respective forms of V(1)
θ (0) and V(3)
θ (0) in Theorem
3.3, we get equivalence. This completes the proof.
G
BACKGROUND THEORY
G.1
TECHNICAL LEMMAS
Lemma G.1 (Solution to the Lyapunov Equation). If all the eigenvalues of matrix M have negative
real part, then for every positive semi-definite matrix U there exists a unique positive semi-definite
13The inequality may not be strict when H is low rank, however it will always be true for some choice of x,
since H is not a zero matrix. Thus, the ordering derived still follows our definition of <L in Section 1, footnote
6.
14D-dimensional vector of all zeros except at the i’th position which is 1.
37

Published as a conference paper at ICLR 2024
matrix V satisfying the Lyapunov equation U+MV+VMT = 0. The explicit solution V is given
as
V =
Z ∞
0
eMtUe(MT )tdt.
(83)
Chellaboina & Haddad (2008, Theorem 3.16) states that for a positive definite matrix U, there exists
a positive definite matrix V. The reason they focus on the positive definite matrix U is that they
require the related autonomous ODE system to be asymptotically stable. However, in this paper we
don’t need this requirement. The same steps therein can be used to prove Lemma G.1 and show that
if U is positive semi-definite, then V in the form of (83) is unique and also positive semi-definite.
Lemma G.2 (Burkholder Inequality, Davis (1970), Hall et al. (2014) Theorem 2.10). Given a Mar-
tingale difference sequence {Mi,n}n
i=1, for p ≥1 and some positive constant Cp, we have
E
"
n
X
i=1
Mi,n

p#
≤CpE


 n
X
i=1
∥Mi,n∥2
!p/2

(84)
Theorem G.3 (Martingale CLT, Delyon (2000) Theorem 30). If a Martingale difference array
{Xn,i} satisfies the following condition: for some τ > 0,
n
X
k=1
E

∥Xn,k∥2+τ|Fk−1
 P−→0,
(85)
sup
n
n
X
k=1
E

∥Xn,k∥2|Fk−1

< ∞,
(86)
and
n
X
k=1
E

Xn,kXT
n,k|Fk−1
 P−→V ,
(87)
then
n
X
i=1
Xn,i
dist.
−−−→N(0, V ).
(88)
Lemma G.4 (Duflo (1996) Proposition 3.I.2). For a Hurwitz matrix H, there exist some positive
constants C, b such that for any n,
eHn ≤Ce−bn.
(89)
Lemma G.5 (Fort (2015) Lemma 5.8). For a Hurwitz matrix A, denote by −r, r > 0, the largest
real part of its eigenvalues. Let a positive sequence {γn} such that limn γn = 0. Then for any
0 < r′ < r, there exists a positive constant C such that for any k < n,

n
Y
j=k
(I + γjA)

≤Ce−r′ Pn
j=k γj.
(90)
Lemma G.6 (Fort (2015) Lemma 5.9, Mokkadem & Pelletier (2006) Lemma 10). Let {γn} be a
positive sequence such that limn γn = 0 and P
n γn = ∞. Let {ϵn, n ≥0} be a nonnegative
sequence. Then, for b > 0, p ≥0,
lim sup
n
γ−p
n
n
X
k=1
γp+1
k
e−b Pn
j=k+1 γjϵk ≤
1
C(b, p) lim sup
n
ϵn
(91)
for some constant C(b, p) > 0.
When p = 0 and define a positive sequence {wn} satisfying wn−1/wn = 1 + o(γn), we have
n
X
k=1
γke−b Pn
j=k+1 γjϵk =
O(wn),
if ϵn = O(wn),
o(wn),
if ϵn = o(wn).
(92)
Lemma G.7 (Fort (2015) Lemma 5.10). For any matrices A, B, C,
∥ABAT −CBCT ∥≤∥A −C∥∥B∥(∥A∥+ ∥C∥).
(93)
38

Published as a conference paper at ICLR 2024
G.2
ASYMPTOTIC RESULTS OF SINGLE-TIMESCALE SA
Consider the stochastic approximation in the form of
zn+1 = zn + γn+1G(zn, Xn+1).
(94)
Let Kz be the transition kernel of the underlying Markov chain {Xn}n≥0 with stationary distribu-
tion π(z) such that g(z) ≜EX∼π(z)[G(z, X)] with domain O ⊆Rd. Define an operator Kzf for
any function f : N →RD such that
(Kzf)(i) =
X
j∈N
f(j)Kz(i, j).
(95)
Assume that
C1. W.p.1, the closure of {zn}n≥0 is a compact subset of O.
C2. γn = γ0/na, a ∈(1/2, 1].
C3. Function g is continuous on O and there exists a non-negative C1 function w and a compact
set K ⊂O such that
• ∇w(z)T g(z) ≤0 for all z ∈O and ∇w(z)T g(z) < 0 if z /∈K;
• the set S ≜{z | ∇w(z)T g(z) = 0} is such that w(S) has an empty interior;
C4. For every z, there exists a solution mz : N →Rd for the following Poisson equation
mz(i) −(Kzmz)(i) = G(z, i) −g(z)
(96)
for any i ∈N; for any compact set C ⊂O,
sup
z∈C,i∈N
∥(Kzmz)(i)∥+ ∥mz(i)∥< ∞
(97)
and there exist a continuous function ϕC, ϕC(0) = 0, such that for any z, z′ ∈C,
sup
i∈N
∥(Kzmz)(i) −(Kz′mz′)(i)∥≤ϕC(∥z −z′∥).
(98)
C5. Denote by −r the largest real part of the eigenvalues of the Jacobian matrix ∇g(z∗) and
assume r >
1{a=1}
2
.
C6. For every z, there exists a solution Qz : N →Rd×d for the following Poisson equation
Qz(i) −(KzQz)(i) = F(z, i) −Ej∼π(z)[F(z, j)]
(99)
for any i ∈N, where
F(z, i) ≜
X
j∈N
mz(j)mz(j)T Kz(i, j) −(Kzmz)(i)(Kzmz)(i)T .
(100)
For any compact set C ⊂O,
sup
z∈C,i∈N
∥Qz(i)∥+ ∥(KzQz)(i)∥< ∞
(101)
and there exist p, CC > 0, such that for any z, z′ ∈C,
sup
i∈N
∥(KzQz)(i) −(Kz′Qz′)(i)∥≤CC∥z −z′∥p.
(102)
Theorem G.8 (Delyon et al. (1999) Theorem 2). Consider (94) and assume C1 - C4. Then, w.p.1,
lim supn d(zn, S) = 0.
Theorem G.9 (Fort (2015) Theorem 2.1 & Proposition 4.1). Consider (94) and assume C1 - C6.
Then, given the condition that zn converges to one point z∗∈S, we have
γ−1/2
n
(zn −z∗)
dist.
−−−−→
n→∞N(0, V),
(103)
where
V
1{b=1}
2
I + ∇g(z∗)T

+
1{b=1}
2
I + ∇g(z∗)

V + U = 0,
(104)
and
U ≜
X
i∈N
µi
 mz∗(i)mz∗(i)T −(Kz∗mz∗)(i)(Kz∗mz∗)(i)T 
.
(105)
39

Published as a conference paper at ICLR 2024
G.3
ASYMPTOTIC RESULTS OF TWO-TIMESCALE SA
For the two-timescale SA with iterate-dependent Markov chain, we have the following iterations:
zn+1 = zn + βn+1G1(zn, ynXn+1),
(106a)
yn+1 = yn + γn+1G2(zn, yn, Xn+1),
(106b)
with the goal of finding the root (z∗, y∗) such that
g1(z∗, y∗) = EX∼µ[G1(z∗, y∗, X)] = 0,
g2(z∗, y∗) = EX∼µ[G2(z∗, y∗, X)] = 0.
(107)
We present here a simplified version of the assumptions for single-valued functions G1, G2 that
are necessary for the almost sure convergence result in Yaji & Bhatnagar (2020, Theorem 4). The
original assumptions are intended for more general set-valued functions G1, G2.
(B1) The step sizes βn ≜n−b and γn ≜n−a, where 0.5 < a < b ≤1.
(B2) Assume the function G1(z, y, X) is continuous and differentiable with respect to z, y.
There exists a positive constant L1 such that ∥G1(z, y, X)∥≤L1(1 + ∥z∥+ ∥y∥) for
every z ∈Rd1, y ∈Rd2, X ∈N. The same condition holds for the function G2 as well.
(B3) Assume there exists a function ρ : Rd1 →Rd2 such that the following three properties hold:
(i) ∥ρ(z)∥≤L2(1 + ∥z∥) for some positive constant L2; (ii) the ODE ˙y = g2(z, y) has
a globally asymptotically stable equilibrium λ(z) such that g2(z, ρ(z)) = 0. Additionally,
let ˆg1(z) ≜g1(z, ρ(z)), there exists a set of disjoint roots Λ ≜{z∗: ˆg1(z∗) = 0}, which
is the set of globally asymptotically stable equilibria of the ODE ˙z = ˆg1(z).
(B4) {Xn}n≥0 is an iterate-dependent Markov process in finite state space N.
For every
n ≥0, P(Xn+1 = j|zm, ym, Xm, 0 ≤m ≤n) = P(Xn+1 = j|zn, yn, Xn = i) =
Pi,j[zn, yn], where the transition kernel P[z, y] is continuous in z, y, and the Markov
chain generated by P[z, y] is ergodic so that it admits a stationary distribution π(z, y), and
π(z∗, ρ(z∗)) = µ.
(B5) supn≥0(∥zn∥+ ∥yn∥) < ∞a.s.
Yaji & Bhatnagar (2020) included assumptions A1 - A9 and A11 for the following Theorem G.10.
We briefly show the correspondence of our assumptions (B1) - (B5) and theirs: (B1) with A5, (B2)
with A1 and A2, (B3) with A9 and A11, (B4) with A3 and A4, and (B5) with A8. Given that our
two-timescale SA framework (106) excludes additional noises (setting them to zero), A6 and A7
therein are inherently met.
Theorem G.10 (Yaji & Bhatnagar (2020) Theorem 4). Under Assumptions (B1) - (B5), iterations
(zn, yn) in (106) almost surely converge to a set of roots, i.e., (zn, yn) →S
z∗∈Λ(z∗, ρ(z∗)) a.s.
H
ADDITIONAL SIMULATION RESULTS
H.1
BINARY CLASSIFICATION ON ADDITIONAL DATASETS
In this part, we perform the binary classification task as in Section 4 on additional datasets, i.e.,
a9a (with 123 features) and splice (with 60 features) from LIBSVM (Chang & Lin, 2011). Figure
4 provides the performance ordering of different α values, and we empirically demonstrate that the
curves with α ≥5 still outperform the i.i.d. counterpart. Additionally, Figure 5 compare cases (i) -
(iii) under both a9a and splice datasets, and case (i) consistently perform the best.
H.2
NON-CONVEX LINEAR REGRESSION
We further test SGD-SRRW and SHB-SRRW algorithms with a non-convex function to demonstrate
the efficiency of our SA-SRRW algorithm beyond the convex setting. In this task, we simulate the
following linear regression problem in Khaled & Richt´arik (2023) with non-convex regularization
min
θ∈Rd


f(θ) ≜1
N
N
X
i=1
li(θ) + κ
d
X
j=1
θ2
j
θ2
j + 1



(108)
40

Published as a conference paper at ICLR 2024
102
103
104
105
Number of steps (n)
10
2
10
1
100
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(a) SGD-SRRW, a9a
102
103
104
105
Number of steps (n)
10
1
100
101
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(b) SGD-SRRW, splice
Figure 4: Simulation results with various α values in a9a and splice datasets.
102
103
104
105
Number of steps (n)
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(a) α = 5, SGD-SRRW, a9a
102
103
104
105
Number of steps (n)
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(b) α = 10, SGD-SRRW, a9a
102
103
104
105
Number of steps (n)
10
2
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(c) α = 20, SGD-SRRW, a9a
102
103
104
105
Number of steps (n)
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(d) α = 5, SGD-SRRW, splice
102
103
104
105
Number of steps (n)
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(e) α = 10, SGD-SRRW, splice
102
103
104
105
Number of steps (n)
10
1
100
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(f) α = 20, SGD-SRRW, splice
Figure 5: Performance comparison among cases (i) - (iii) for α ∈{5, 10, 20} in a9a and splice
datasets.
where the loss function li(θ) = ∥sT
i θ −yi∥2 and κ = 1, with the data points {(si, yi)}i∈N from the
ijcnn1 dataset of LIBVIM (Chang & Lin, 2011). We still perform the optimization over the wikiVote
graph, as done in Section 4.
The numerical results for the non-convex linear regression taks are presented in Figures 6 and 7,
where each experiment is repeated 100 times. Figures 6a and 6b show that the performance ordering
across different α values is still preserved for both algorithms over almost all time, and curves
for α ≥5 outperform that of the i.i.d. sampling (in black) under the graph topological constraints.
Additionally, among the three cases examined at identical α values, Figures 7a - 7c confirm that case
(i) performs consistently better than the other two cases, implying that case (i) can even become the
best choice for non-convex distributed optimization tasks.
41

Published as a conference paper at ICLR 2024
102
103
104
105
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(a) SGD-SRRW
102
103
104
105
Number of steps (n)
10
3
10
2
10
1
MSE 
n
*
2
CASE (i): a = 0.8, b = 0.9
MHRW (
= 0)
= 1
= 5
= 10
= 20
i.i.d. sampling
(b) SHB-SRRW
Figure 6: Simulation results for non-convex linear regression under case (i) with various α values.
102
103
104
105
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(a) α = 5, SGD-SRRW
102
103
104
105
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(b) α = 10, SGD-SRRW
102
103
104
105
Number of steps (n)
10
2
10
1
MSE 
n
*
2
CASE (i):a = 0.8, b = 0.9
CASE (ii):a = 0.9, b = 0.9
CASE (iii):a = 1, b = 0.9
(c) α = 20, SGD-SRRW
Figure 7: Performance comparison among cases (i) - (iii) for non-convex regression.
42

