Published as a conference paper at ICLR 2021
WHAT ARE THE STATISTICAL LIMITS OF OFFLINE RL
WITH LINEAR FUNCTION APPROXIMATION?
Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu
Dean P. Foster
University of Pennsylvania and Amazon
dean@foster.net
Sham M. Kakade
University of Washington, Seattle and Microsoft Research
sham@cs.washington.edu
ABSTRACT
OfÔ¨Çine reinforcement learning seeks to utilize ofÔ¨Çine (observational) data to guide
the learning of (causal) sequential decision making strategies. The hope is that
ofÔ¨Çine reinforcement learning coupled with function approximation methods (to
deal with the curse of dimensionality) can provide a means to help alleviate the
excessive sample complexity burden in modern sequential decision making prob-
lems. However, the extent to which this broader approach can be effective is not
well understood, where the literature largely consists of sufÔ¨Åcient conditions.
This work focuses on the basic question of what are necessary representational and
distributional conditions that permit provable sample-efÔ¨Åcient ofÔ¨Çine reinforce-
ment learning. Perhaps surprisingly, our main result shows that even if: i) we have
realizability in that the true value function of every policy is linear in a given set
of features and 2) our off-policy data has good coverage over all features (under a
strong spectral condition), any algorithm still (information-theoretically) requires
a number of ofÔ¨Çine samples that is exponential in the problem horizon to non-
trivially estimate the value of any given policy. Our results highlight that sample-
efÔ¨Åcient ofÔ¨Çine policy evaluation is not possible unless signiÔ¨Åcantly stronger con-
ditions hold; such conditions include either having low distribution shift (where
the ofÔ¨Çine data distribution is close to the distribution of the policy to be evaluated)
or signiÔ¨Åcantly stronger representational conditions (beyond realizability).
1
INTRODUCTION
OfÔ¨Çine methods (also known as off-policy methods or batch methods) are a promising methodol-
ogy to alleviate the sample complexity burden in challenging reinforcement learning (RL) settings,
particularly those where sample efÔ¨Åciency is paramount (Mandel et al., 2014; Gottesman et al.,
2018; Wang et al., 2018; Yu et al., 2019). Off-policy methods are often applied together with func-
tion approximation schemes; such methods take sample transition data and reward values as inputs,
and approximate the value of a target policy or the value function of the optimal policy. Indeed,
many practical deep RL algorithms Ô¨Ånd their prototypes in the literature of ofÔ¨Çine RL. For exam-
ple, when running on off-policy data (sometimes termed as ‚Äúexperience replay‚Äù), deep Q-networks
(DQN) (Mnih et al., 2015) can be viewed as an analog of Fitted Q-Iteration (Gordon, 1999) with
neural networks being the function approximators. More recently, there are an increasing number
of both model-free (Laroche et al., 2019; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al.,
2019; Agarwal et al., 2020) and model-based (Ross & Bagnell, 2012; Kidambi et al., 2020) ofÔ¨Çine
RL methods, with steady improvements in performance (Fujimoto et al., 2019; Kumar et al., 2019;
Wu et al., 2020; Kidambi et al., 2020).
However, despite the importance of these methods, the extent to which data reuse is possible, espe-
cially when off-policy methods are combined with function approximation, is not well understood.
For example, deep Q-network requires millions of samples to solve certain Atari games (Mnih et al.,
2015). Also important is that in some safety-critical settings, we seek guarantees when ofÔ¨Çine-
1

Published as a conference paper at ICLR 2021
trained policies can be effective (Thomas, 2014; Thomas et al., 2019). A basic question here is
that if there are fundamental statistical limits on such methods, where sample-efÔ¨Åcient ofÔ¨Çine RL is
simply not possible without further restrictions on the problem.
In the context of supervised learning, it is well-known that empirical risk minimization is sample-
efÔ¨Åcient if the hypothesis class has bounded complexity. For example, suppose the agent is given
a d-dimensional feature extractor, and the ground truth labeling function is a (realizable) linear
function with respect to the feature mapping. Here, it is well-known that a polynomial number
of samples in d sufÔ¨Åce for a given target accuracy. Furthermore, in this realizable case, provided
the training data has a good feature coverage, then we will have good accuracy against any test
distribution.1
In the more challenging ofÔ¨Çine RL setting, it is unclear if sample-efÔ¨Åcient methods are possible,
even under analogous assumptions. This is our motivation to consider the following question:
What are the statistical limits for ofÔ¨Çine RL with linear function approximation?
Here, one may hope that value estimation for a given policy is possible in the ofÔ¨Çine RL setting
under the analogous set of assumptions that enable sample-efÔ¨Åcient supervised learning, i.e., 1)
(realizability) the features can perfectly represent the value functions and 2) (good coverage) the
feature covariance matrix of our off-policy data has lower bounded eigenvalues.
The extant body of provable methods on ofÔ¨Çine RL either make representational assumptions that are
far stronger than realizability or assume distribution shift conditions that are far stronger than having
coverage with regards to the spectrum of the feature covariance matrix of the data distribution. For
example, Szepesv¬¥ari & Munos (2005) analyze ofÔ¨Çine RL methods by assuming a representational
condition where the features satisfy (approximate) closedness under Bellman updates, which is a far
stronger representation condition than realizability. Recently, Xie & Jiang (2020a) propose a ofÔ¨Çine
RL algorithm that only requires realizability as the representation condition. However, the algorithm
in (Xie & Jiang, 2020a) requires a more stringent data distribution condition. Whether it is possible
to design a sample-efÔ¨Åcient ofÔ¨Çine RL method under the realizability assumption and a reasonable
data coverage assumption ‚Äî an open problem in (Chen & Jiang, 2019) ‚Äî is the focus of this work.
Our Contributions.
Perhaps surprisingly, our main result shows that, under only the above two
assumptions, it is information-theoretically not possible to design a sample-efÔ¨Åcient algorithm to
non-trivially estimate the value of a given policy. The following theorem is an informal version of
the result in Section 4.
Theorem 1.1 (Informal). In the ofÔ¨Çine RL setting, suppose the data distributions have (polynomi-
ally) lower bounded eigenvalues, and the Q-functions of every policy are linear with respect to a
given feature mapping. Any algorithm requires an exponential number of samples in the horizon
H to output a non-trivially accurate estimate of the value of any given policy œÄ, with constant
probability.
This hardness result states that even if the Q-functions of all polices are linear with respect to the
given feature mapping, we still require an exponential number of samples to evaluate any given
policy. Note that this representation condition is signiÔ¨Åcantly stronger than assuming realizability
with regards to only a single target policy; it assumes realizability for all policies. Regardless, even
under this stronger representation condition, it is hard to evaluate any policy, as speciÔ¨Åed in our
hardness result.
This result also formalizes a key issue in ofÔ¨Çine reinforcement learning with function approxima-
tion: geometric error ampliÔ¨Åcation. To better illustrate the issue, in Section 5, we analyze the clas-
sical Least-Squares Policy Evaluation (LSPE) algorithm under the realizability assumption, which
demonstrates how the error propagates as the algorithm proceeds. Here, our analysis shows that,
if we only rely on the realizability assumption, then a far more stringent condition is required for
sample-efÔ¨Åcient ofÔ¨Çine policy evaluation: the off-policy data distribution must be quite close to the
distribution induced by the policy to be evaluated.
1SpeciÔ¨Åcally, if the features have a uniformly bounded norm and if the minimum eigenvalue of the feature
covariance matrix of our data is bounded away from 0, say by 1/poly(d), then we have good accuracy on any
test distribution. See Assumption 2 and the comments thereafter.
2

Published as a conference paper at ICLR 2021
Our results highlight that sample-efÔ¨Åcient ofÔ¨Çine RL is simply not possible unless either the distri-
bution shift condition is sufÔ¨Åciently mild or we have stronger representation conditions that go well
beyond realizability. See Section 5 for more details.
Furthermore, our hardness result implies an exponential separation on the sample complexity be-
tween ofÔ¨Çine RL and supervised learning, since supervised learning (which is equivalent to ofÔ¨Çine
RL with H = 1) is possible with polynomial number of samples under the same set of assumptions.
A few additional points are worth emphasizing with regards to our lower bound construction:
‚Ä¢ Our results imply that Least-Squares Policy Evaluation (LSPE, i.e., using Bellman backups with
linear regression) will fail. Interestingly, while LSPE will provide an unbiased estimator, our
results imply that it will have exponential variance in the problem horizon.
‚Ä¢ Our construction is simple and does not rely on having a large state or action space: the size of
the state space is only O(d ¬∑ H) where d is the feature dimension and H is the planning horizon,
and the size of the action space is only is 2. This stands in contrast to other RL lower bounds,
which typically require state spaces that are exponential in the problem horizon (e.g. see (Du
et al., 2020)).
‚Ä¢ We provide two hard instances, one with a sparse reward (and stochastic transitions) and another
with deterministic dynamics (and stochastic rewards). These two hard instances jointly imply that
both the estimation error on reward values and the estimation error on the transition probabilities
could be geometrically ampliÔ¨Åed in ofÔ¨Çine RL.
‚Ä¢ Of possibly broader interest is that our hard instances are, to our knowledge, the Ô¨Årst concrete
examples showing that geometric error ampliÔ¨Åcation is real in RL problems (even with realizabil-
ity). While this is a known concern in the analysis of RL algorithms, there have been no concrete
examples exhibiting such behavior under only a realizability assumption.
2
RELATED WORK
We now survey prior work on ofÔ¨Çine RL, largely focusing on theoretical results. We also discuss
results on the error ampliÔ¨Åcation issue in RL. Concurrent to this work, Xie & Jiang (2020a) propose
a ofÔ¨Çine RL algorithm under the realizability assumption, which requires stronger distribution shift
conditions. We will discuss this work shortly.
Existing Algorithms and Analysis.
OfÔ¨Çine RL with value function approximation is closely re-
lated to Approximate Dynamic Programming (Bertsekas & Tsitsiklis, 1995). Existing work (Munos,
2003; Szepesv¬¥ari & Munos, 2005; Antos et al., 2008; Munos & Szepesv¬¥ari, 2008; Tosatto et al.,
2017; Xie & Jiang, 2020b; Duan & Wang, 2020) that analyze the sample complexity of approximate
dynamic programming-based approaches usually make the following two categories of assumptions:
(i) representation conditions that assume the function class approximates the value functions well
and (ii) distribution shift conditions that assume the given data distribution has sufÔ¨Åcient coverage
over the state-action space. As mentioned in the introduction, the desired representation condition
would be realizability, which only assumes the value function of the policy to be evaluated lies in
the function class (for the case of ofÔ¨Çine policy evaluation) or the optimal value function lies in
the function class (for the case of Ô¨Ånding near-optimal policies), and existing works usually make
stronger assumptions. For example, Szepesv¬¥ari & Munos (2005); Duan & Wang (2020) assume (ap-
proximate) closedness under Bellman updates, which is much stronger than realizability. Whether
it is possible to design a sample-efÔ¨Åcient ofÔ¨Çine RL method under the realizability assumption and
reasonable data coverage assumption, is left as an open problem in (Chen & Jiang, 2019).
To measure the coverage over the state-action space of the given data distribution, existing works
assume the concentrability coefÔ¨Åcient (introduced by Munos (2003)) to be bounded. The concen-
trability coefÔ¨Åcient, informally speaking, is the largest possible ratio between the probability for a
state-action pair (s, a) to be visited by a policy, and the probability that (s, a) appears on the data
distribution. Since we work with linear function approximation in this work, we measure the distri-
bution shift in terms of the spectrum of the feature covariance matrices (see Assumption 2), which
is a well-known sufÔ¨Åcient condition in the context of supervised learning and is much more natural
for the case of linear function approximation.
3

Published as a conference paper at ICLR 2021
Concurrent to this work, Xie & Jiang (2020a) propose an algorithm that works under the realizabil-
ity assumption instead of other stronger representation conditions used in prior work. However, the
algorithm in (Xie & Jiang, 2020a) requires a much stronger data distribution condition which as-
sumes a stringent version of concentrability coefÔ¨Åcient introduced by (Munos, 2003) to be bounded.
In contrast, in this work we measure the distribution shift in terms of the spectrum of the feature
covariance matrix of the data distribution, which is more natural than the concentrability coefÔ¨Åcient
for the case of linear function approximation.
Recently, there has been great interest in approaching ofÔ¨Çine policy evaluation (Precup, 2000) via
importance sampling. For recent work on this topic, see (Dud¬¥ƒ±k et al., 2011; Mandel et al., 2014;
Thomas et al., 2015; Li et al., 2015; Jiang & Li, 2016; Thomas & Brunskill, 2016; Guo et al.,
2017; Wang et al., 2017; Liu et al., 2018; Farajtabar et al., 2018; Xie et al., 2019; Kallus & Ue-
hara, 2019; Liu et al., 2019; Uehara & Jiang, 2019; Kallus & Uehara, 2020; Jiang & Huang, 2020;
Feng et al., 2020). OfÔ¨Çine policy evaluation with importance sampling incurs exponential variance
in the planning horizon when the behavior policy is signiÔ¨Åcantly different from the policy to be
evaluated. Bypassing such exponential dependency requires non-trivial function approximation as-
sumptions (Jiang & Huang, 2020; Feng et al., 2020; Liu et al., 2018). Finally, Kidambi et al. (2020)
provide a model-based ofÔ¨Çine RL algorithm, with a theoretical analysis based on hitting times.
Hardness Results.
Algorithm-speciÔ¨Åc hardness results have been known for a long time in the
literature of Approximate Dynamic Programming. See Chapter 4 in (Van Roy, 1994) and also (Gor-
don, 1995; Tsitsiklis & Van Roy, 1996). These works demonstrate that certain approximate dynamic
programming-based methods will diverge on hard cases. However, such hardness results only hold
for a restricted class of algorithms, and to demonstrate the fundamental difÔ¨Åculty of ofÔ¨Çine RL, it is
more desirable to obtain information-theoretic lower bounds as initiated by Chen & Jiang (2019).
Existing (information-theoretic) exponential lower bounds (Krishnamurthy et al., 2016; Sun et al.,
2017; Chen & Jiang, 2019) usually construct unstructured MDPs with an exponentially large state
space. Du et al. (2020) prove an exponential lower bound under the assumption that the optimal Q-
function is approximately linear. The condition that the optimal Q-function is only approximately
linear is crucial for the hardness result in Du et al. (2020). The techniques in (Du et al., 2020) are
later generalized to other settings (Kumar et al., 2020; Wang et al., 2020; Mou et al., 2020).
Error AmpliÔ¨Åcation In RL.
Error ampliÔ¨Åcation induced by distribution shift and long planning
horizon is a known issue in the theoretical analysis of RL algorithms. See (Gordon, 1995; 1996;
Munos & Moore, 1999; Ormoneit & Sen, 2002; Kakade, 2003; Zanette et al., 2019) for papers on
this topic and additional assumptions that mitigate this issue. Error ampliÔ¨Åcation in ofÔ¨Çine RL is
also observed in empirical works (see e.g. (Fujimoto et al., 2019)). In this work, we provide the Ô¨Årst
information-theoretic lower bound showing that geometric error ampliÔ¨Åcation is real in ofÔ¨Çine RL.
3
THE OFFLINE POLICY EVALUATION PROBLEM
Throughout this paper, for a given integer H, we use [H] to denote the set {1, 2, . . . , H}.
Episodic Reinforcement Learning.
Let M = (S, A, P, R, H) be a Markov Decision Process
(MDP) where S is the state space, A is the action space, P : S √ó A ‚Üí‚àÜ(S) is the transition
operator which takes a state-action pair and returns a distribution over states, R : S √ó A ‚Üí‚àÜ(R)
is the reward distribution, H ‚ààZ+ is the planning horizon. For simplicity, we assume a Ô¨Åxed initial
state s1 ‚ààS. A (stochastic) policy œÄ : S ‚Üí‚àÜ(A) chooses an action a randomly based on the
current state s. The policy œÄ induces a (random) trajectory s1, a1, r1, s2, a2, r2, . . . , sH, aH, rH,
where a1 ‚àºœÄ1(s1), r1 ‚àºR(s1, a1), s2 ‚àºP(s1, a1), a2 ‚àºœÄ2(s2), etc. To streamline our analysis,
for each h ‚àà[H], we use Sh ‚äÜS to denote the set of states at level h, and we assume Sh do not
intersect with each other. We assume, almost surely, that rh ‚àà[‚àí1, 1] for all h ‚àà[H].
Value Functions.
Given a policy œÄ, h ‚àà[H] and (s, a) ‚ààSh √ó A, deÔ¨Åne QœÄ
h(s, a) =
E
hPH
h‚Ä≤=h rh‚Ä≤ | sh = s, ah = a, œÄ
i
and V œÄ
h (s) = E
hPH
h‚Ä≤=h rh‚Ä≤ | sh = s, œÄ
i
. For a policy œÄ, we
deÔ¨Åne V œÄ = V œÄ
1 (s1) to be the value of œÄ from the Ô¨Åxed initial state s1.
4

Published as a conference paper at ICLR 2021
Linear Function Approximation.
When applying linear function approximation schemes, it is
commonly assumed that the agent is given a feature extractor œÜ : S √ó A ‚ÜíRd which can either be
hand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional
embedding, and the Q-functions can be predicted by linear functions of the features. In this paper,
we are interested in the following realizability assumption.
Assumption 1 (Realizable Linear Function Approximation). For every policy œÄ : S ‚Üí‚àÜ(A), there
exists Œ∏œÄ
1 , . . . Œ∏œÄ
H ‚ààRd such that for all (s, a) ‚ààS √ó A and h ‚àà[H], QœÄ
h(s, a) = (Œ∏œÄ
h)‚ä§œÜ(s, a).
Note that our assumption is much stronger than assuming realizability with regards to a single policy
œÄ (say the policy that we wish to evaluate); our assumption imposes realizability for all policies.
OfÔ¨Çine Reinforcement Learning.
This paper is concerned with the ofÔ¨Çine RL setting. In this
setting, the agent does not have direct access to the MDP and instead is given access to data dis-
tributions {¬µh}H
h=1 where for each h ‚àà[H], ¬µh ‚àà‚àÜ(Sh √ó A). The inputs of the agent are H
datasets {Dh}H
h=1, and for each h ‚àà[H], Dh consists i.i.d. samples of the form (s, a, r, s‚Ä≤) ‚àà
Sh √ó A √ó R √ó Sh+1 tuples, where (s, a) ‚àº¬µh, r ‚àºr(s, a), s‚Ä≤ ‚àºP(s, a).
In this paper, we focus on the ofÔ¨Çine policy evaluation problem with linear function approximation:
given a policy œÄ : S ‚Üí‚àÜ(A) and a feature extractor œÜ : S √ó A ‚ÜíRd, the goal is to output an
accurate estimate of the value of œÄ (i.e., V œÄ) approximately, using the collected datasets {Dh}H
h=1,
with as few samples as possible.
Notation.
For a vector x ‚ààRd, we use ‚à•x‚à•2 to denote its ‚Ñì2 norm. For a positive semideÔ¨Ånite
matrix A, we use ‚à•A‚à•2 to denote its operator norm, and œÉmin(A) to denote its smallest eigenvalue.
For two positive semideÔ¨Ånite matrices A and B, we write A ‚™∞B to denote the L¬®owner partial
ordering of matrices, i.e, A ‚™∞B if and only if A ‚àíB is positive semideÔ¨Ånite. For a policy œÄ : S ‚Üí
‚àÜ(A), we use ¬µœÄ
h to denote the marginal distribution of sh under œÄ, i.e., ¬µœÄ
h(s) = Pr[sh = s | œÄ].
For a vector x ‚ààRd and a positive semideÔ¨Ånite matrix A ‚ààRd√ód, we use ‚à•x‚à•A to denote
‚àö
x‚ä§Ax.
4
THE LOWER BOUND: REALIZABILITY AND COVERAGE ARE INSUFFICIENT
We now present our main hardness result for ofÔ¨Çine policy evaluation with linear function approxi-
mation. It should be evident that without feature coverage in our dataset, realizability alone is clearly
not sufÔ¨Åcient for sample-efÔ¨Åcient estimation. Here, we will make the strongest possible assumption,
with regards to the conditioning of the feature covariance matrix.
Assumption 2 (Feature Coverage). For all (s, a) ‚ààS √ó A, assume our feature map is bounded
such that ‚à•œÜ(s, a)‚à•2 ‚â§1. Furthermore, suppose for each h ‚àà[H], the data distributions ¬µh satisfy
the following minimum eigenvalue condition: œÉmin
 E(s,a)‚àº¬µh[œÜ(s, a)œÜ(s, a)‚ä§]

= 1/d.2
Clearly, for the case where H = 1, the realizability assumption (Assumption 1), and feature cov-
erage assumption (Assumption 2) imply that the ordinary least squares estimator will accurately
estimate Œ∏1.3 Our main result now shows that these assumptions are not sufÔ¨Åcient for ofÔ¨Çine policy
evaluation for long horizon problems.
Theorem 4.1. Suppose Assumption 2 holds. Fix an algorithm that takes as input both a policy and
a feature mapping. There exists a (deterministic) MDP satisfying Assumption 1, such that for any
policy œÄ : S ‚Üí‚àÜ(A), the algorithm requires ‚Ñ¶((d/2)H) samples to output the value of œÄ up to
constant additive approximation error with probability at least 0.9.
Although we focus on ofÔ¨Çine policy evaluation in this work, our hardness result also holds for
Ô¨Ånding near-optimal policies under Assumption 1 in the ofÔ¨Çine RL setting. Below we give a simple
reduction. At the initial state, if the agent chooses action a1, then the agent receives a Ô¨Åxed reward
value (say 0.5) and terminates. If the agent chooses action a2, then the agent transits to our hard
2Note that 1/d is the largest possible minimum eigenvalue due to that, for any data distribution e¬µh,
œÉmin(E(s,a)‚àºe¬µh[œÜ(s, a)œÜ(s, a)‚ä§]) ‚â§1/d since ‚à•œÜ(s, a)‚à•2 ‚â§1 for all (s, a) ‚ààS √ó A.
3For H = 1, the ordinary least squares estimator will satisfy that ‚à•Œ∏1 ‚àíbŒ∏OLS‚à•2
2 ‚â§O(d/n) with high
probability. See e.g. (Hsu et al., 2012b).
5

Published as a conference paper at ICLR 2021
instance. In order to Ô¨Ånd a policy with suboptimality at most 0.5, the agent must evaluate the value
of the optimal policy in our hard instance up to an error of 0.5, and hence the hardness result holds.
Remark 1 (The sparse reward case). As stated, the theorem uses a deterministic MDP (with stochas-
tic rewards). See Appendix C for another hard case where the transition is stochastic and the reward
is deterministic and sparse (only occurring at two states at h = H).
Remark 2 (Least-Squares Policy Evaluation (LSPE) has exponential variance). For ofÔ¨Çine policy
evaluation with linear function approximation, the most na¬®ƒ±ve algorithm here would be LSPE, i.e.,
using ordinary least squares (OLS) to estimate Œ∏œÄ, starting at level h = H and then proceeding
backwards to level h = 1, using the plug-in estimator from the previous level. Here, LSPE will pro-
vide an unbiased estimate (provided the feature covariance matrices are full rank, which will occur
with high probability). As a direct corollary, the above theorem implies that LSPE has exponential
variance in H. See Section 5 for a more detailed discussion on LSPE. More generally, our theorem
implies that there is no estimator that can avoid such exponential dependence in the ofÔ¨Çine setting.
Remark 3 (Least-Squares Value Iteration (LSVI) versus Least-Squares Policy Iteration (LSPI)). In
the ofÔ¨Çine setting, under Assumptions 1 and 2, in order to Ô¨Ånd a near-optimal policy, the most na¬®ƒ±ve
algorithm would be LSVI, i.e., using ordinary least squares (OLS) to estimate Œ∏‚àó, starting at level
h = H and then proceeding backwards to level h = 1, using the plug-in estimator from the previous
level and the bellman operator. The above theorem implies that LSVI will require an exponential
number of samples to Ô¨Ånd a near-optimal policy. On the other hand, if the regression targets are
collected by using rollouts (i.e. on-policy sampling) as in LSPI (Lagoudakis & Parr, 2003), then a
polynomial number of samples sufÔ¨Åce. See Section D in (Du et al., 2020) for an analysis. Thus,
Theorem 4.1 implies an exponential separation on the sample complexity between LSVI and LSPI.
Of course, LSPI requires adaptive data samples and thus does not work in the ofÔ¨Çine setting.
One may wonder if Theorem 4.1 still holds when the data distributions {¬µh}H
h=1 are induced by
a policy. In Appendix C, we prove another exponential sample complexity lower bound under the
additional assumption that the data distributions are induced by a Ô¨Åxed policy œÄ. However, under
such an assumption, it is impossible to prove a hardness result as strong as Theorem 4.1 (which
shows that evaluating any policy is hard), since one can at least evaluate the policy œÄ that induces
the data distributions. Nevertheless, we are able to prove the hardness of ofÔ¨Çine policy evaluation,
under a weaker version of Assumption 1. See Appendix C for more details.
In the remaining part of this section, we give the hard instance construction and the proof of Theo-
rem 4.1. We use d the denote the feature dimension, and we assume d is even for simplicity. We use
ÀÜd to denote d/2 for convenience. We also provide an illustration of the construction in Figure 1.
State Space, Action Space and Transition Operator.
The action space A = {a1, a2}. For each
h ‚àà[H], Sh contains ÀÜd + 1 states s1
h, s2
h, . . . , s ÀÜd
h and s
ÀÜd+1
h
. For each h ‚àà[H ‚àí1], for each
c ‚àà{1, 2, . . . , ÀÜd + 1}, we have P(sc
h, a1) = s
ÀÜd+1
h+1 and P(sc
h, a1) = sc
h+1.
Reward Distributions.
Let 0 ‚â§r0 ‚â§ÀÜd‚àíH/2 be a parameter to be determined. For each (h, c) ‚àà
[H ‚àí1] √ó [ ÀÜd] and a ‚ààA, we set R(sc
h, a) = 0 and R(s
ÀÜd+1
h
, a) = r0 ¬∑ ( ÀÜd1/2 ‚àí1) ¬∑ ÀÜd(H‚àíh)/2. For
the last level, for each c ‚àà[ ÀÜd] and a ‚ààA, we set R(sc
H, a) =
1
with probability (1 + r0)/2
‚àí1
with probability (1 ‚àír0)/2
so that E[R(sc
H, a)] = r0. Moreover, for all actions a ‚ààA, R(s
ÀÜd+1
H
, a) = r0 ¬∑ ÀÜd1/2.
Feature Mapping.
Let e1, e2, . . . , ed be a set of orthonormal vectors in Rd. Here, one possible
choice is to set e1, e2, . . . , ed to be the standard basis vectors. For each (h, c) ‚àà[H] √ó [ ÀÜd], we set
œÜ(sc
h, a1) = ec, œÜ(sc
h, a2) = ec+ ÀÜd, and œÜ(s
ÀÜd+1
h
, a) = P
c‚ààÀÜd ec/ ÀÜd1/2 for all a ‚ààA.
Verifying Assumption 1.
The following lemma shows that Assumption 1 holds for our construc-
tion. The formal proof can be found in Appendix A.
Lemma 4.2. For every policy œÄ : S ‚Üí‚àÜ(A), for each h ‚àà[H], for all (s, a) ‚ààSh √ó A, we have
QœÄ
h(s, a) = (Œ∏œÄ
h)‚ä§œÜ(s, a) for some Œ∏œÄ
h ‚ààRd.
6

Published as a conference paper at ICLR 2021
‚Ä¶
‚Ä¶
‚Ä¶
‚Ä¶
ùëÑùë†, ùëé! = ùëü" 'ùëë($%!)/(
ùëÖùë†, ùëé= 0
ùëÑùë†, ùëé! = ùëü" 'ùëë($%()/(
ùëÖùë†, ùëé= 0
ùëÑùë†, ùëé! = ùëü" 'ùëë!/(
ùëÖùë†, ùëé= 0
ùëÑùë†, ùëé= ùëü"
ùîº[ùëÖùë†, ùëé] = ùëü"
ùëÑùë†!
)*+!, ùëé= ùëü" 'ùëë$/(
ùëÖùë†!
)*+!, ùëé= ùëü"( 'ùëë$/( ‚àí'ùëë($%!)/()
ùëÑùë†(
)*+!, ùëé= ùëü" 'ùëë($%!)/(
ùëÖùë†(
)*+!, ùëé= ùëü"( 'ùëë($%!)/( ‚àí'ùëë($%()/()
ùëÑùë†$%!
)*+!, ùëé= ùëü" 'ùëë
ùëÖùë†$%!
)*+!, ùëé= ùëü"( 'ùëë‚àí'ùëë!/()
ùëÑùë†$
)*+!, ùëé= ùëü" 'ùëë!/(
ùëÖùë†$
)*+!, ùëé= ùëü" 'ùëë!/(
ùëé!
ùëé(
ùúôùë†,
-, ùëé! = ùëí-
ùúôùë†,
-, ùëé( = ùëí-+ )*
ùúôùë†,
)*+!, ùëé= (ùëí! + ùëí( + ‚ãØ+ ùëí)*)/ 'ùëë!/(
ùë†!
!
ùë†!
(
ùë†!
.
ùë†!
)*
ùë†!
)*+!
ùë†(
!
ùë†(
(
ùë†(
.
ùë†(
)*
ùë†(
)*+!
ùë†$%!
!
ùë†$%!
)*+!
ùë†$%!
)*
ùë†$%!
(
ùë†$%!
.
ùë†$
!
ùë†$
(
ùë†$
.
ùë†$
)*
ùë†$
)*+!
‚ãØ
‚ãØ
‚ãØ
‚ãØ
‚ãØ
‚Ä¶
ùëÑùë†,
)*+!, ùëé= ùëü" 'ùëë($%,+!)/(
ùëÖùë†,
)*+!, ùëé= ùëü"( 'ùëë($%,+!)/( ‚àí'ùëë($%,)/()
ùëÑùë†, ùëé! = ùëü" 'ùëë($%,)/(
ùëÖùë†, ùëé= 0
Figure 1: An illustration of the hard instance. Recall that ÀÜd = d/2. States on the top are those in
the Ô¨Årst level (h = 1), while states at the bottom are those in the last level (h = H). Solid line
(with arrow) corresponds to transitions associated with action a1, while dotted line (with arrow)
corresponds to transitions associated with action a2. For each level h ‚àà[H], reward values and
Q-values associated with s1
h, s2
h, . . . , s ÀÜd
h are marked on the left, while reward values and Q-values
associated with s
ÀÜd+1
h
are mark on the right. Rewards and transitions are all deterministic, except
for the reward distributions associated with s1
H, s2
H, . . . , s ÀÜd
H. We mark the expectation of the reward
value when it is stochastic. For each level h ‚àà[H], for the data distribution ¬µh, the state is chosen
uniformly at random from those states in the dashed rectangle, i.e., {s1
h, s2
h, . . . , s ÀÜd
h}, while the action
is chosen uniformly at random from {a1, a2}. Suppose the initial state is s
ÀÜd+1
1
. When r0 = 0, the
value of the policy is 0. When r0 = ÀÜd‚àíH/2, the value of the policy is r0 ¬∑ ÀÜdH/2 = 1.
The Data Distributions.
For each level h ‚àà[H], the data distribution ¬µh is a uniform distribution
over {(s1
h, a1), (s1
h, a2), (s2
h, a1), (s2
h, a2), . . . , (s ÀÜd
h, a1), (s ÀÜd
h, a2)}. Notice that (s
ÀÜd+1
h
, a) is not in the
support of ¬µh for all a ‚ààA. It can be seen that, E(s,a)‚àº¬µh

œÜ(s, a)œÜ(s, a)‚ä§
= 1
d
Pd
c=1 ece‚ä§
c = 1
dI.
The Lower Bound.
We show that it is information-theoretically hard for any algorithm to distin-
guish the case r0 = 0 and r0 = ÀÜd‚àíH/2. We Ô¨Åx the initial state to be s
ÀÜd+1
1
, and consider any policy œÄ.
When r0 = 0, all reward values will be zero, and thus the value of œÄ is zero. On the other hand, when
r0 = ÀÜd‚àíH/2, the value of œÄ would be r0 ¬∑ ÀÜdH/2 = 1. Thus, if the algorithm approximates the value
of the policy up to an error of 1/2, then it must distinguish the case that r0 = 0 and r0 = ÀÜd‚àíH/2.
We Ô¨Årst notice that for the case r0 = 0 and r0 = ÀÜd‚àíH/2, the data distributions {¬µh}H
h=1, the fea-
ture mapping œÜ : S √ó A ‚ÜíRd, the policy œÄ to be evaluated and the transition operator P are the
same. Thus, in order to distinguish the case r0 = 0 and r0 = ÀÜd‚àíH/2, the only way is to query the
reward distribution by using sampling taken from the data distributions. For all state-action pairs
(s, a) in the support of the data distributions of the Ô¨Årst H ‚àí1 levels, the reward distributions will
be identical. This is because for all s ‚ààSh \ {s
ÀÜd+1
h
} and a ‚ààA, we have R(s, a) = 0. For
the case r0 = 0 and r0 = ÀÜd‚àíH/2, for all state-action pairs (s, a) in the support of the data dis-
tribution of the last level, R(s, a) =
1
with probability (1 + r0)/2
‚àí1
with probability (1 ‚àír0)/2 . Therefore, to distinguish
7

Published as a conference paper at ICLR 2021
Algorithm 1 Least-Squares Policy Evaluation
1: Input: policy œÄ to be evaluated, number of samples N, regularization parameter Œª > 0
2: Let QH+1(¬∑, ¬∑) = 0 and VH+1(¬∑) = 0
3: for h = H, H ‚àí1, . . . , 1 do
4:
Take samples (si
h, ai
h) ‚àº¬µh, ri
h ‚àºr(si
h, ai
h) and si
h ‚àºP(si
h, ai
h) for each i ‚àà[N]
5:
Let ÀÜŒõh = P
i‚àà[N] œÜ(si
h, ai
h)œÜ(si
h, ai
h)‚ä§+ ŒªI
6:
Let ÀÜŒ∏h = ÀÜŒõ‚àí1
h
PN
i=1 œÜ(si
h, ai
h) ¬∑ (ri
h + ÀÜVh+1(si
h))

7:
Let ÀÜQh(¬∑, ¬∑) = œÜ(¬∑, ¬∑)‚ä§ÀÜŒ∏h and ÀÜVh(¬∑) = ÀÜQ(¬∑, œÄ(¬∑))
the case that r0 = 0 and r0 = ÀÜd‚àíH/2, the agent needs to distinguish two reward distributions
r1 =
1
with probability 1/2
‚àí1
with probability 1/2 and r2 =
(
1
with probability (1 + ÀÜd‚àíH/2)/2
‚àí1
with probability (1 ‚àíÀÜd‚àíH/2)/2 . Now we in-
voke Lemma B.1 in Section B by setting Œµ = ÀÜd‚àíH/2/2 and Œ¥ = 0.9. By Lemma B.1, in order to
distinguish r1 and r2 with probability at least 0.9, any algorithm requires ‚Ñ¶( ÀÜdH) samples.
Remark 4. The key in our construction is the state s
ÀÜd+1
h
in each level, whose feature vector is
deÔ¨Åned to be P
c‚ààÀÜd ec/ ÀÜd1/2. In each level, s
ÀÜd+1
h
ampliÔ¨Åes the Q-value by a ÀÜd1/2 factor, due to the
linearity of the Q-function. After all the H levels, the value will be ampliÔ¨Åed by a ÀÜdH/2 factor. Since
s
ÀÜd+1
h
is not in the support of the data distribution, the only way to estimate the value of the policy is
to estimate the expected reward value in the last level. Our construction forces the estimation error
of the last level to be ampliÔ¨Åed exponentially and thus implies an exponential lower bound.
5
UPPER BOUNDS: LOW DISTRIBUTION SHIFT OR POLICY COMPLETENESS
ARE SUFFICIENT
In order to illustrate the error ampliÔ¨Åcation issue and discuss conditions that permit sample-efÔ¨Åcient
ofÔ¨Çine RL, in this section, we analyze Least-Squares Policy Evaluation when applied to the ofÔ¨Çine
policy evaluation problem under the realizability assumption. The algorithm is presented in Algo-
rithm 1. For simplicity here we assume the policy œÄ to be evaluated is deterministic.
Notation.
For each h ‚àà[H], deÔ¨Åne Œõh = E(s,a)‚àº¬µh

œÜ(s, a)œÜ(s, a)‚ä§
to be the feature covariance
matrix at level h. For each h ‚àà[H‚àí1], deÔ¨Åne Œõh+1 = E(s,a)‚àº¬µh,s‚àºP (¬∑|s,a)

œÜ(s, œÄ(s))œÜ(s, œÄ(s))‚ä§
to be the feature covariance matrix of the one-step lookahead distribution at level h. Moreover, deÔ¨Åne
Œõ1 = œÜ(s1, œÄ(s1))œÜ(s1, œÄ(s1))‚ä§. We deÔ¨Åne Œ¶h to be a N √ó d matrix, whose i-th row is œÜ(si
h, ai
h),
and deÔ¨Åne Œ¶h+1 to be another N √ó d matrix whose i-th row is œÜ(si
h, œÄ(si
h)). For each h ‚àà[H] and
i ‚àà[N], deÔ¨Åne Œæi
h = ri
h + V (si
h) ‚àíQ(si
h, ai
h). We use Œæh to denote a vector whose i-th entry is Œæi
h.
Now we present a general lemma that characterizes the estimation error of Algorithm 1 by an equal-
ity. The proof can be found in Appendix D. Later, we apply this general lemma to special cases.
Lemma 5.1. Suppose Œª > 0 in Algorithm 1, and for the given policy œÄ, there exists Œ∏1, Œ∏2, . . . , Œ∏d ‚àà
Rd such that for each h ‚àà[H], QœÄ
h(s, a) = œÜ(s, a)‚ä§Œ∏h for all (s, a) ‚ààSh √ó A. Then we have
(QœÄ(s1, œÄ(s1)) ‚àíÀÜQ(s1, œÄ(s1)))2 =

H
X
h=1
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 ¬∑ ¬∑ ¬∑ (ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh ‚àíŒªÀÜŒõ‚àí1
h Œ∏h)

2
Œõ1
. (1)
Now we consider two special cases where the estimation error in Equation (1) can be upper bounded.
Low Distribution Shift.
The Ô¨Årst special we focus on is the case where the distribution shift
between the data distributions and the distribution induced by the policy to be evaluated is low. To
measure the distribution shift formally, our main assumption is as follows.
Assumption 3. We assume that for each h ‚àà[H], there exists Ch ‚â•1 such that Œõh ‚™ØChŒõh.
8

Published as a conference paper at ICLR 2021
Remark 5. For each h ‚àà[H], if œÉmin(Œõh) ‚™∞
1
Ch I for some Ch ‚â•1, we have Œõh ‚™ØI ‚™ØChŒõh.
Therefore, Assumption 3 can be replaced with the assumption that ChŒõh ‚™∞I. However, we stick
to the original version of Assumption 3 as it gives a tighter characterization of the distribution shift.
Now we state the theoretical guarantee of Algorithm 1. The proof can be found in Appendix D.
Theorem 5.2. Suppose for the given policy œÄ, there exists Œ∏1, Œ∏2, . . . , Œ∏d ‚ààRd such that for
each h ‚àà[H], QœÄ
h(s, a) = œÜ(s, a)‚ä§Œ∏h for all (s, a) ‚ààSh √ó A and ‚à•Œ∏h‚à•2 ‚â§H
‚àö
d.4
Let
Œª = CH
p
d log(dH/Œ¥)N for some C > 0. With probability at least 1 ‚àíŒ¥, for some c > 0,
(QœÄ
1(s1, œÄ(s1)) ‚àíÀÜQ1(s1, œÄ(s1)))2 ‚â§c ¬∑ QH
h=1 Ch ¬∑ dH5 ¬∑
p
d log(dH/Œ¥)/N.
Remark 6. The factor QH
h=1 Ch in Theorem 5.2 implies that the estimation error will be ampliÔ¨Åed
geometrically. Now we discuss how the error is ampliÔ¨Åed when running Algorithm 1 on the instance
in Section 4 to better illustrate the issue. If we run Algorithm 1 on the hard instance in Section 4,
when h = H, the estimation error on V (sc
H) would be roughly N ‚àí1/2 for each c ‚àà[ ÀÜd]. When using
the linear predictor at level H to predict the value of s‚àó
H, the error will be ampliÔ¨Åed by ÀÜd1/2. When
h = H ‚àí1, the dataset contains only sc
H‚àí1 for c ‚àà[ ÀÜd], and the estimation error on the value of sc
H‚àí1
will be the same as that of s‚àó
H, which is roughly ( ÀÜd/N)1/2. Again, the estimation error on the value
of s‚àó
H‚àí1 will be ( ÀÜd2/N)1/2 when using the linear predictor at level H ‚àí1. The error will eventually
be ampliÔ¨Åed by a factor of ÀÜdH/2, which corresponds to the factor QH
h=1 Ch in Theorem 5.2.
Policy Completeness.
In ofÔ¨Çine RL, another representation condition is closedness under Bellman
update (Szepesv¬¥ari & Munos, 2005; Duan & Wang, 2020), which is stronger than realizability. In
the context of ofÔ¨Çine policy evaluation, we have the following policy completeness assumption.
Assumption 4. For the given policy œÄ, for any h > 1 and Œ∏h ‚ààRd, there exists Œ∏‚Ä≤ ‚ààRd such that
for any (s, a) ‚ààSh‚àí1 √ó A, E[R(s, a)] + P
s‚Ä≤‚ààSh P(s‚Ä≤ | s, a)œÜ(s‚Ä≤, œÄ(s‚Ä≤))‚ä§Œ∏h = œÜ(s, a)‚ä§Œ∏‚Ä≤.
Under Assumption 4 and the assumption that œÉmin(Œõh) ‚â•Œª0 for all h ‚àà[H] for some Œª0 > 0,
Duan & Wang (2020) have shown that for Algorithm 1, by taking N = poly(H, d, 1/Œµ, 1/Œª0), we
have (QœÄ
1(s1, œÄ(s1)) ‚àíÀÜQ1(s1, œÄ(s1)))2 ‚â§Œµ. We refer interested readers to (Duan & Wang, 2020).
We remark that the above analysis again implies that geometric error ampliÔ¨Åcation is a real issue in
ofÔ¨Çine RL, and sample-efÔ¨Åcient ofÔ¨Çine RL is impossible unless the distribution shift is sufÔ¨Åciently
low, i.e., QH
h=1 Ch is bounded, or strong representation condition (e.g. policy completeness) holds.
6
CONCLUSION
While the extant body of provable results in the literature largely focus on sufÔ¨Åcient conditions for
sample-efÔ¨Åcient ofÔ¨Çine RL, this work focuses on obtaining a better understanding of the necessary
conditions, where we seek to understand to what extent mild assumptions can imply sample-efÔ¨Åcient
ofÔ¨Çine RL. This work shows that for off-policy evaluation, even if we are given a representation that
can perfectly represent the value function of the given policy and the data distribution has good
coverage over the features, any provable algorithm still requires an exponential number of samples
to non-trivially approximate the value of the given policy. These results highlight that provable
sample-efÔ¨Åcient ofÔ¨Çine RL is simply not possible unless either the distribution shift condition is
sufÔ¨Åciently mild or we have stronger representation conditions that go well beyond realizability.
ACKNOWLEDGMENTS
The authors would like to thank Akshay Krishnamurthy, Alekh Agarwal, Wen Sun, and Nan Jiang
for numerous helpful discussion.
Sham M. Kakade gratefully acknowledges funding from the
ONR award N00014-18-1-2247, and NSF Awards CCF-1703574 and CCF-1740551.
Ruosong
Wang was supported in part by the NSF IIS1763562, US Army W911NF1920104, and ONR Grant
N000141812861. Research performed while Ruosong Wang was an intern at Microsoft Research.
4Without loss of generality, we can work in a coordinate system such that ‚à•Œ∏h‚à•2 ‚â§H
‚àö
d and ‚à•œÜ(s, a)‚à•2 ‚â§
1 for all (s, a) ‚ààS √ó A. This follows due to John‚Äôs theorem (e.g. see (Ball, 1997; Bubeck et al., 2012)).
9

Published as a conference paper at ICLR 2021
REFERENCES
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofÔ¨Çine
reinforcement learning. In International Conference on Machine Learning, 2020.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Andr¬¥as Antos, Csaba Szepesv¬¥ari, and R¬¥emi Munos. Learning near-optimal policies with bellman-
residual minimization based Ô¨Åtted policy iteration and a single sample path. Machine Learning,
71(1):89‚Äì129, 2008.
Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1‚Äì58,
1997.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Pro-
ceedings of 1995 34th IEEE Conference on Decision and Control, volume 1, pp. 560‚Äì564. IEEE,
1995.
S¬¥ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade.
Towards minimax policies for
online linear optimization with bandit feedback. In COLT 2012 - The 25th Annual Conference on
Learning Theory, June 25-27, 2012, Edinburgh, Scotland, volume 23 of JMLR Proceedings, pp.
41.1‚Äì41.14, 2012.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, pp. 1042‚Äì1051, 2019.
Herman Chernoff. Sequential analysis and optimal design. SIAM, 1972.
Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation suf-
Ô¨Åcient for sample efÔ¨Åcient reinforcement learning?
In International Conference on Learning
Representations, 2020.
Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approx-
imation. arXiv preprint arXiv:2002.09516, 2020.
Miroslav Dud¬¥ƒ±k, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, pp. 1097‚Äì1104, 2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, pp. 1447‚Äì1456, 2018.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. arXiv preprint arXiv:2008.06668, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052‚Äì2062, 2019.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning
Proceedings 1995, pp. 261‚Äì268. Elsevier, 1995.
Geoffrey J Gordon. Stable Ô¨Åtted reinforcement learning. In Advances in neural information pro-
cessing systems, pp. 1052‚Äì1058, 1996.
Geoffrey J Gordon.
Approximate solutions to markov decision processes.
Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 1999.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,
Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, Jiayu Yao, Isaac Lage, Christopher Mosch,
Li wei H. Lehman, Matthieu Komorowski, Matthieu Komorowski, Aldo Faisal, Leo Anthony
Celi, David Sontag, and Finale Doshi-Velez. Evaluating reinforcement learning algorithms in
observational health settings, 2018.
10

Published as a conference paper at ICLR 2021
Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems, pp.
2492‚Äì2501, 2017.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012a.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9‚Äì1, 2012b.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning
of human preferences in dialog, 2020.
URL https://openreview.net/forum?id=
rJl5rRVFvH.
Nan Jiang and Jiawei Huang. Minimax conÔ¨Ådence interval for off-policy evaluation and policy
optimization. arXiv preprint arXiv:2002.02081, 2020.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In
International Conference on Machine Learning, pp. 652‚Äì661. PMLR, 2016.
Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,
University of London London, England, 2003.
Nathan Kallus and Masatoshi Uehara. EfÔ¨Åciently breaking the curse of horizon in off-policy evalu-
ation with double reinforcement learning. arXiv preprint arXiv:1909.05850, 2019.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efÔ¨Åcient off-policy evalu-
ation in markov decision processes. Journal of Machine Learning Research, 21(167):1‚Äì63, 2020.
Emilie Kaufmann, Olivier Capp¬¥e, and Aur¬¥elien Garivier. On the complexity of best-arm identiÔ¨Å-
cation in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1‚Äì42,
2016.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel : Model-
based ofÔ¨Çine reinforcement learning, 2020.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, pp. 1840‚Äì1848, 2016.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. neural information processing systems, pp. 11761‚Äì
11771, 2019.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107‚Äì1149, 2003.
Romain Laroche, Paul Trichelair, and R¬¥emi Tachet des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning
(ICML), 2019.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In
ArtiÔ¨Åcial Intelligence and Statistics, pp. 608‚Äì616, 2015.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: InÔ¨Ånite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356‚Äì
5366, 2018.
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy
evaluation via conditional importance sampling. arXiv preprint arXiv:1910.06508, 2019.
11

Published as a conference paper at ICLR 2021
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. OfÔ¨Çine policy
evaluation across representations with applications to educational games. In AAMAS, pp. 1077‚Äì
1084, 2014.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. Journal of Machine Learning Research, 5(Jun):623‚Äì648, 2004.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning with
policy space generalization. arXiv preprint arXiv:2008.07353, 2020.
R¬¥emi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pp. 560‚Äì567,
2003.
Remi Munos and Andrew W Moore. Barycentric interpolators for continuous space and time re-
inforcement learning. In Advances in neural information processing systems, pp. 1024‚Äì1030,
1999.
R¬¥emi Munos and Csaba Szepesv¬¥ari. Finite-time bounds for Ô¨Åtted value iteration. Journal of Machine
Learning Research, 9(May):815‚Äì857, 2008.
Dirk Ormoneit and ¬¥Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161‚Äì178, 2002.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
St¬¥ephane Ross and Drew Bagnell. Agnostic system identiÔ¨Åcation for model-based reinforcement
learning. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply ag-
grevated: Differentiable imitation learning for sequential prediction. In International Conference
on Machine Learning, pp. 3309‚Äì3318, 2017.
Csaba Szepesv¬¥ari and R¬¥emi Munos. Finite time bounds for sampling based Ô¨Åtted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pp. 880‚Äì887, 2005.
Philip Thomas and Emma Brunskill. Data-efÔ¨Åcient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139‚Äì2148, 2016.
Philip S. Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts, Amherst,
2014.
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-conÔ¨Ådence off-
policy evaluation. In Twenty-Ninth AAAI Conference on ArtiÔ¨Åcial Intelligence, 2015.
Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Stephen Giguere, Yuriy Brun, and
Emma Brunskill. Preventing undesirable behavior of intelligent machines. Science, 366(6468):
999‚Äì1004, 2019. ISSN 0036-8075. doi: 10.1126/science.aag3311.
Samuele Tosatto, Matteo Pirotta, Carlo d‚ÄôEramo, and Marcello Restelli. Boosted Ô¨Åtted q-iteration.
In International Conference on Machine Learning, pp. 3434‚Äì3443. PMLR, 2017.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends R‚Éùin
Machine Learning, 8(1-2):1‚Äì230, 2015.
J Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximation
(technical report lids-p-2322). Laboratory for Information and Decision Systems, 1996.
Masatoshi Uehara and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.
arXiv preprint arXiv:1910.12809, 2019.
12

Published as a conference paper at ICLR 2021
Benjamin Van Roy. Feature-based methods for large scale dynamic programming. PhD thesis,
Massachusetts Institute of Technology, 1994.
L. Wang, Wei Zhang, Xiaofeng He, and H. Zha. Supervised reinforcement learning with recurrent
neural network for dynamic treatment recommendation. Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, 2018.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2006.11274, 2020.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudƒ±k. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning, pp. 3589‚Äì3597. PMLR,
2017.
Yifan Wu, George Tucker, and OÔ¨År Nachum. Behavior regularized ofÔ¨Çine reinforcement learning,
2020. URL https://openreview.net/forum?id=BJg9hTNKPH.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020a.
Tengyang Xie and Nan Jiang. Q‚ãÜapproximation schemes for batch reinforcement learning: A
theoretical comparison. arXiv preprint arXiv:2003.03924, 2020b.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for rein-
forcement learning with marginalized importance sampling. In Advances in Neural Information
Processing Systems, pp. 9668‚Äì9678, 2019.
C. Yu, G. Ren, and J. Liu. Deep inverse reinforcement learning for sepsis treatment. In 2019 IEEE
International Conference on Healthcare Informatics (ICHI), pp. 1‚Äì3, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Limiting ex-
trapolation in linear approximate value iteration. In Advances in Neural Information Processing
Systems, pp. 5616‚Äì5625, 2019.
13

Published as a conference paper at ICLR 2021
A
PROOF OF LEMMA 4.2
Proof. We Ô¨Årst verify QœÄ is linear for the Ô¨Årst H ‚àí1 levels. For each (h, c) ‚àà[H ‚àí1] √ó [ ÀÜd], we
have
QœÄ
h(sc
h, a1) =R(sc
h, a1) + R(s
ÀÜd+1
h+1, a1) + R(s
ÀÜd+1
h+2, a1) + . . . + R(s
ÀÜd+1
H
, a1) = r0 ¬∑ ÀÜd(H‚àíh)/2.
Moreover, for all a ‚ààA,
QœÄ
h(s
ÀÜd+1
h
, a) =R(s
ÀÜd+1
h
, a) + R(s
ÀÜd+1
h+1, a1) + R(s
ÀÜd+1
h+2, a1) + . . . + R(s
ÀÜd+1
H
, a1) = r0 ¬∑ ÀÜd(H‚àíh+1)/2.
Therefore, if we deÔ¨Åne
Œ∏œÄ
h =
ÀÜd
X
c=1
r0 ¬∑ ÀÜd(H‚àíh)/2 ¬∑ ec +
ÀÜd
X
c=1
QœÄ
h(sc
h, a2) ¬∑ ec+ ÀÜd,
then QœÄ
h(s, a) = (Œ∏œÄ
h)‚ä§œÜ(s, a) for all (s, a) ‚ààSh √ó A.
Now we verify that the Q-function is linear for the last level. Clearly, for all c ‚àà[ ÀÜd] and a ‚ààA,
QœÄ
H(sc
H, a) = r0 and QœÄ
H(s
ÀÜd+1
H
, a) = r0 ¬∑
p
ÀÜd. Thus by deÔ¨Åning Œ∏œÄ
H = Pd
c=1 r0 ¬∑ ec, we have
QœÄ
H(s, a) = (Œ∏œÄ
H)‚ä§œÜ(s, a) for all (s, a) ‚ààSH √ó A.
B
A TECHNICAL LEMMA
We need the following lemma in the proof of our hardness results.
Lemma B.1. Let Œ± be a random variable uniformly distributed on {Œ±+, Œ±‚àí}, where Œ±‚àí= 1/2 and
Œ±+ = 1/2 + Œµ with 0 < Œµ < 1. Suppose that Œæ1, Œæ2, . . . , Œæm are i.i.d. {+1, ‚àí1}-valued random
variables with Pr[Œæi = +1] = Œ± for all i ‚àà[m]. Let f be a function from {+1, ‚àí1}m to {Œ±+, Œ±‚àí}.
Suppose m ‚â§C/Œµ2 log(1/Œ¥) for some Ô¨Åxed constant C. Then
Pr[f(Œæ1, Œæ2, . . . , Œæm) Ã∏= Œ±] > Œ¥.
To our best knowledge, Lemma B.1 was Ô¨Årst proved in (Chernoff, 1972) and has enormous ap-
plications in statistical learning theory (see, e.g., Chapter 5 in (Anthony & Bartlett, 2009)) and
bandits (Mannor & Tsitsiklis, 2004).
To prove Lemma B.1, one can Ô¨Årst prove that the maximum likelihood estimator (MLE) is optimal
and then show that MLE requires ‚Ñ¶(1/Œµ2 log(1/Œ¥)) samples to correctly output Œ± with probability
1 ‚àíŒ¥ by anti-concentration. Lemma B.1 can also be proved by using information theory. See, e.g.,
(Kaufmann et al., 2016) for such a proof.
C
ANOTHER HARD INSTANCE
In this section, we present another hard case under a weaker version of Assumption 1. Here the
transition operator is stochastic and the reward is deterministic and sparse, meaning that the reward
value is non-zero only for the last level. Moreover, the data distributions {¬µh}H
h=1 are induced by a
Ô¨Åxed policy œÄdata. We also illustrate the construction in Figure 2. Throughout this section, we use
d the denote the feature dimension, and we assume d is an even integer for simplicity. We use ÀÜd to
denote d/2 ‚àí1.
In this section, we adopt the following realizability assumption, which is a weaker version of As-
sumption 1.
Assumption 5 (Realizability). For the policy œÄ : S ‚Üí‚àÜ(A) to be evaluated, there exists
Œ∏1, . . . Œ∏H ‚ààRd such that for all (s, a) ‚ààS √ó A and h ‚àà[H],
QœÄ
h(s, a) = Œ∏‚ä§
h œÜ(s, a).
14

Published as a conference paper at ICLR 2021
‚Ä¶
‚Ä¶
‚Ä¶
ùëÑùë†!
", ùëé= ùëü# 'ùëë(%&')/!
ùëÑùë†%&*
"
, ùëé= ùëü#
ùëÉùë†%
+ ùë†%&*
"
, ùëé) = (1 + ùëü#)/2
ùëÉùë†%
& ùë†%&*
"
, ùëé) = (1 ‚àíùëü#)/2
ùëÑùë†%
" , ùëé= 0
ùëÖùë†%
+, ùëé= 1
ùëÖùë†%
&, ùëé= ‚àí1
ùëÉùë†'
+ ùë†!
,-+*, ùëé-) = (1 + ùëü# 'ùëë(%&!)/!)/2
ùëÉùë†'
& ùë†!
,-+*, ùëé-) = (1 ‚àíùëü# 'ùëë(%&!)/!)/2
ùëÑùë†!
,-+*, ùëé- = ùëü# 'ùëë(%&!)/!
ùëÉùë†%
+ ùë†%&*
,-+*, ùëé-) = (1 + ùëü# 'ùëë*/!)/2
ùëÉùë†%
& ùë†%&*
,-+*, ùëé-) = (1 ‚àíùëü# 'ùëë*/!)/2
ùëÑùë†%&*
,-+*, ùëé- = ùëü# 'ùëë*/!
ùë†%&*
,-+*
ùë†%&*
&
ùë†%&*
+
ùë†%&*
,-
ùë†%&*
!
ùë†%&*
*
‚ãØ
‚ãØ
‚ãØ
‚ãØ
‚ãØ
‚Ä¶
ùë†%
*
ùë†%
!
ùë†%
,-
ùë†%
+
ùë†%
&
ùë†%
,-+*
ùë†!
!
ùë†!
*
ùë†!
,-+*
ùë†*
ùë†!
&
ùë†!
+
ùë†!
,-
‚ãØ
ùëÑùë†*, ùëé- = ùëü# 'ùëë(%&!)/!
ùëÑùë†.
", ùëé= ùëü# 'ùëë(%&.&*)/!
ùëÉùë†.+*
+
ùë†.
,-+*, ùëé-) = (1 + ùëü# 'ùëë(%&.)/!)/2
ùëÉùë†.+*
&
ùë†.
,-+*, ùëé-) = (1 ‚àíùëü# 'ùëë(%&.)/!)/2
ùëÑùë†.
,-+*, ùëé-
= ùëü# 'ùëë(%&.)/!
ùëé*, ùëé!, ‚Ä¶ , ùëé,-
ùëé,-+*, ùëé,-+!, ‚Ä¶ , ùëé-
Figure 2: An illustration of the hard instance. Recall that ÀÜd = d/2 ‚àí1. States on the top are those
in the Ô¨Årst level (h = 1), while states at the bottom are those in the last level (h = H). Dotted
line (with arrow) corresponds to transitions associated with actions a1, a2, . . . , a ÀÜd, while solid line
(with arrow) corresponds to transitions associated with actions a ÀÜd+1, a ÀÜd+2, . . . , ad. We omit the
transition associated with a1, a2, . . . , a ÀÜd in the Ô¨Ågure if all actions give the same transition. For
each level h ‚àà[H], Q-values associated with s1
h, s2
h, . . . , s ÀÜd
h, s+
h , s‚àí
h are marked on the left, while
transition distributions and Q-values associated with s
ÀÜd+1
h
are marked on the right. Rewards are all
deterministic, and the only two states (s+
H and s‚àí
H) with non-zero reward values are marked in black
and grey. Consider the Ô¨Åxed policy that returns ad for all input states. When r0 = 0, the value of the
policy is 0. When r0 = ÀÜd‚àí(H‚àí2)/2, the value of the policy is = r0 ÀÜd(H‚àí2)/2 = 1.
State Space, Action Space and Transition Operator.
In this hard case, the action space A =
{a1, a2, . . . , ad} contains d elements. S1 contains a single state s1. For each h ‚â•2, Sh contains
ÀÜd + 3 states s1
h, s2
h, . . . , s ÀÜd
h, s
ÀÜd+1
h
, s+
h and s‚àí
h .
Let 0 ‚â§r0 ‚â§ÀÜd‚àí(H‚àí2)/2 be a parameter to be determined. We Ô¨Årst deÔ¨Åne the transition operator for
the Ô¨Årst level. We have
P(s1, a) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
sc
2
a = ac, c ‚àà[ ÀÜd]
s+
2
a = a ÀÜd+1
s‚àí
2
a = a ÀÜd+2
s
ÀÜd+1
2
a ‚àà

a ÀÜd+3, a ÀÜd+4, . . . , ad
	
.
Now we deÔ¨Åne the transition operator when h ‚àà{2, 3, . . . , H ‚àí2}. For each h ‚àà{2, 3, . . . , H ‚àí2},
a ‚ààA and c ‚àà[ ÀÜd], we have P(sc
h, a) = s
ÀÜd+1
h+1, P(s+
h , a) = s+
h+1 and P(s‚àí
h , a) = s‚àí
h+1. For each
h ‚àà{2, 3, . . . , H ‚àí2} and c ‚àà[ ÀÜd], we have P(s
ÀÜd+1
h
, ac) = sc
h+1. For all a ‚àà

a ÀÜd+1, a ÀÜd+2, . . . , ad
	
,
15

Published as a conference paper at ICLR 2021
we have
P

s
ÀÜd+1
h
, a

=
(
s+
h+1
with probability (1 + r0 ¬∑ ÀÜd(H‚àíh)/2)/2
s‚àí
h+1
with probability (1 ‚àír0 ¬∑ ÀÜd(H‚àíh)/2)/2 .
Now we deÔ¨Åne the transition operator for the second last level. For all c ‚àà[ ÀÜd] and a ‚ààA, we have
P(sc
H‚àí1, a) =
s+
H
with probability (1 + r0)/2
s‚àí
H
with probability (1 ‚àír0)/2 .
For all a ‚ààA, we have P(s+
H‚àí1, a) = s+
H and P(s‚àí
H‚àí1, a) = s‚àí
H. For each c ‚àà[ ÀÜd], we have
P(s
ÀÜd+1
H‚àí1, ac) = sc
H. For all a ‚àà

a ÀÜd+1, a ÀÜd+2, . . . , ad
	
, we have
P(s
ÀÜd+1
H‚àí1, a) =
Ô£±
Ô£≤
Ô£≥
s+
H
with probability

1 + r0 ¬∑
p
ÀÜd

/2
s‚àí
H
with probability

1 ‚àír0 ¬∑
p
ÀÜd

/2
.
Reward Values.
In this hard case, all reward values are deterministic, and reward values can be
non-zero only for the last level. Formally, we have
R(s, a) =
Ô£±
Ô£≤
Ô£≥
1
s = s+
H
‚àí1
s = s‚àí
H
0
otherwise
.
Feature Mapping.
As in the in hard instance in Section 4, let e1, e2, . . . , ed be a set of of orthonor-
mal vectors in Rd. For the initial state, for each c ‚àà[d], we have œÜ(s1, ac) = ec.
Now we deÔ¨Åne the feature mapping when h ‚àà{2, 3, . . . , H}. For each h ‚àà{2, 3, . . . , H}, a ‚ààA
and c ‚àà[ ÀÜd], œÜ(sc
h, a) = ec, œÜ(s+
h , a) = e ÀÜd+1 and œÜ(s‚àí
h , a) = e ÀÜd+2. Moreover, for all actions a ‚ààA,
œÜ(s
ÀÜd+1
h
, a) =
(
e ÀÜd+2+c
a = ac, c ‚àà[ ÀÜd]
1
ÀÜd1/2
 e1 + e2 + . . . + e ÀÜd

a ‚àà

a ÀÜd+1, a ÀÜd+2, . . . , ad
	 .
Clearly, for all (s, a) ‚ààS √ó A, ‚à•œÜ(s, a)‚à•2 ‚â§1.
Verifying Assumption 5.
Now we consider the deterministic policy œÄ : S ‚ÜíA, which is deÔ¨Åned
to be œÄ(s) = ad for all s ‚ààS. We show that Assumption 5 holds.
When h = 1, deÔ¨Åne
Œ∏1 =
ÀÜd
X
c=1
r0 ¬∑ ÀÜd(H‚àí3)/2 ¬∑ ec + e ÀÜd+1 ‚àíe ÀÜd+2 +
ÀÜd
X
c=1
r0 ¬∑ ÀÜd(H‚àí2)/2 ¬∑ e ÀÜd+2+c.
For each h ‚àà{2, 3, . . . , H ‚àí2}, deÔ¨Åne
Œ∏h =
ÀÜd
X
c=1
r0 ¬∑ ÀÜd(H‚àíh‚àí1)/2 ¬∑ ec + e ÀÜd+1 ‚àíe ÀÜd+2 +
ÀÜd
X
c=1
r0 ¬∑ ÀÜd(H‚àíh‚àí2)/2 ¬∑ e ÀÜd+2+c.
For the second last level h = H ‚àí1, deÔ¨Åne
Œ∏H‚àí1 =
ÀÜd
X
c=1
r0 ¬∑ ec + e ÀÜd+1 ‚àíe ÀÜd+2.
Finally, for the last level h = H, deÔ¨Åne
Œ∏H = e ÀÜd+1 ‚àíe ÀÜd+2.
It can be veriÔ¨Åed that for each h ‚àà[H], QœÄ
h(s, a) = Œ∏‚ä§
h œÜ(s, a) for all (s, a) ‚ààSh √ó A.
16

Published as a conference paper at ICLR 2021
The Data Distributions.
For the Ô¨Årst level, the data distribution ¬µ1 is deÔ¨Åned to be the uniform
distribution over {(s1, ac) | c ‚àà[d]}. For each h ‚â•2, the data distribution ¬µh is a uniform
distribution over
{(s1
h, a1), (s2
h, a1), . . . , (s
ÀÜd
h, a1), (s+
h , a1), (s‚àí
h , a1), (s
ÀÜd+1
h
, a1), (s
ÀÜd+1
h
, a2), . . . , (s
ÀÜd+1
h
, a ÀÜd)}.
Notice that again (s
ÀÜd+1
h
, a) is not in the support of ¬µh for all actions a ‚àà

a ÀÜd+1, a ÀÜd+2, . . . , ad
	
. It
can be seen that for all h ‚àà[H],
E(s,a)‚àº¬µh[œÜ(s, a)œÜ(s, a)‚ä§] = 1
d
d
X
c=1
ece‚ä§
c = 1
dI.
Moreover, by deÔ¨Åning
œÄdata(s) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Uniform(A)
s = s1
a1
s ‚àà{sc
h | h ‚àà{2, 3, . . . , H}, c ‚àà[ ÀÜd]}
a1
s ‚àà{s+
h | h ‚àà{2, 3, . . . , H}}
a1
s ‚àà{s‚àí
h | h ‚àà{2, 3, . . . , H}}
Uniform({a1, a2, . . . , a ÀÜd})
s ‚àà{s
ÀÜd+1
h
| h ‚àà{2, 3, . . . , H}}
,
we have ¬µh = ¬µœÄdata
h
for all h ‚àà[H].
The Lower Bound.
Now we show that it is information-theoretically hard for any algorithm to
distinguish the case r0 = 0 and r0 = ÀÜd‚àí(H‚àí2)/2 in the ofÔ¨Çine setting by taking samples from the
data distributions {¬µh}H
h=1. Here we consider the above policy œÄ deÔ¨Åned above which returns action
ad for all input states. Notice that when r0 = 0, the value of the policy would be zero. On the other
hand, when r0 = ÀÜd‚àí(H‚àí2)/2, the value of the policy would be r0 ¬∑ ÀÜd(H‚àí2)/2 = 1. Therefore, if the
algorithm approximates the value of the policy up to an approximation error of 1/2, then it must
distinguish the case that r0 = 0 and r0 = ÀÜd‚àí(H‚àí2)/2.
We Ô¨Årst notice that for the case r0 = 0 and r0 = ÀÜd‚àí(H‚àí2)/2, the data distributions {¬µh}H
h=1, the
feature mapping œÜ : S √ó A ‚ÜíRd, the policy œÄ to be evaluated and the reward distributions R are
the same. Thus, in order to distinguish the case r0 = 0 and r0 = ÀÜd‚àí(H‚àí2)/2, the only way is to
query the transition operator P by using sampling taken from the data distributions.
Now, for all state-action pairs (s, a) in the support of the data distributions of the Ô¨Årst H ‚àí2 lev-
els (namely ¬µ1, ¬µ2, . . . , ¬µH‚àí2), the transition operator will be identical. This is because chang-
ing r0 only changes the transition distributions of (s
ÀÜd+1
h
, a ÀÜd+1), (s
ÀÜd+1
h
, a ÀÜd+2), . . . , (s
ÀÜd+1
h
, ad), and
such state-actions are not in the support of ¬µh for all h ‚àà[H ‚àí2].
Moreover, for any
(s, a) ‚àà{s+
H‚àí1, s‚àí
H‚àí1, s
ÀÜd+1
H‚àí1} √ó A in the support of ¬µH‚àí1, P(s, a) will also be identical no mat-
ter r0 = 0 or r0 = ÀÜd‚àí(H‚àí2)/2. For those state-action pairs (s, a) in the support of ¬µH‚àí1 with
s /‚àà{s+
H‚àí1, s‚àí
H‚àí1, s
ÀÜd+1
H‚àí1}, we have
P(s, a) =
s+
H
with probability (1 + r0)/2
s‚àí
H
with probability (1 ‚àír0)/2 .
Again, this is because (s
ÀÜd+1
H‚àí1, a) is not in the support of ¬µH‚àí1 for all a ‚àà

a ÀÜd+1, a ÀÜd+2, . . . , ad
	
.
Therefore, in order to distinguish the case r0 = 0 and r0 = ÀÜd‚àí(H‚àí2)/2, the agent needs distinguish
two transition distributions
p1 =
s+
H
with probability 1/2
s‚àí
H
with probability 1/2
and
p2 =
(
s+
H
with probability (1 + ÀÜd‚àí(H‚àí2)/2)/2
s‚àí
H
with probability (1 ‚àíÀÜd‚àí(H‚àí2)/2)/2 .
Again, by Lemma B.1, in order to distinguish p1 and p2 with probability at least 0.9, one needs
‚Ñ¶( ÀÜdH‚àí2) samples. Formally, we have the following theorem.
17

Published as a conference paper at ICLR 2021
Theorem C.1. Suppose Assumption 2 holds, and rewards are deterministic and could be none-
zero only for state-action pairs in the last level. Fix an algorithm that takes as input both a policy
and a feature mapping. There exists an MDP satisfying Assumption 5, such that for a Ô¨Åxed policy
œÄ : S ‚ÜíA, the algorithm requires ‚Ñ¶((d/2‚àí1)H/2) samples to output the value of œÄ up to constant
additive approximation error with probability at least 0.9.
D
ANALYSIS OF ALGORITHM 1
D.1
PROOF OF LEMMA 5.1
Clearly,
ÀÜŒ∏h = ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + ÀÜVh+1(si
h))
!
= ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + ÀÜQh+1(si
h, œÄ(si
h)))
!
= ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + œÜ(si
h, œÄ(si
h))‚ä§ÀÜŒ∏h+1)
!
= ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + œÜ(si
h, œÄ(si
h))‚ä§Œ∏h+1) +
N
X
i=1
œÜ(si
h, ai
h) ¬∑ œÜ(si
h, œÄ(si
h))‚ä§(ÀÜŒ∏h+1 ‚àíŒ∏h+1)
!
= ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + œÜ(si
h, œÄ(si
h))‚ä§Œ∏h+1)
!
+ ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ œÜ(si
h, œÄ(si
h))‚ä§(ÀÜŒ∏h+1 ‚àíŒ∏h+1)
!
.
For the Ô¨Årst term, we have
ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + œÜ(si
h, œÄ(si
h))‚ä§Œ∏h+1)
!
=ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + QœÄ(si
h, œÄ(si
h)))
!
=ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (ri
h + V œÄ(si
h))
!
=ÀÜŒõ‚àí1
h
 N
X
i=1
œÜ(si
h, ai
h) ¬∑ (QœÄ(si
h, ai
h) + Œæi
h)
!
=ÀÜŒõ‚àí1
h
N
X
i=1
œÜ(si
h, ai
h) ¬∑ Œæi
h + ÀÜŒõ‚àí1
h
N
X
i=1
œÜ(si
h, ai
h) ¬∑ œÜ(si
h, ai
h)‚ä§Œ∏h
=ÀÜŒõ‚àí1
h
N
X
i=1
œÜ(si
h, ai
h) ¬∑ Œæi
h + ÀÜŒõ‚àí1
h (Œ¶‚ä§
h Œ¶h)Œ∏h
=ÀÜŒõ‚àí1
h Œ¶hŒæh + Œ∏h ‚àíŒªÀÜŒõ‚àí1
h Œ∏h.
Therefore,
ÀÜŒ∏1 ‚àíŒ∏1 = (ÀÜŒõ‚àí1
1 Œ¶1Œæ1 ‚àíŒªÀÜŒõ‚àí1
1 Œ∏1) + ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2(Œ∏2 ‚àíÀÜŒ∏2)
= (ÀÜŒõ‚àí1
1 Œ¶1Œæ1 ‚àíŒªÀÜŒõ‚àí1
1 Œ∏1) + ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2(ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œæ2 ‚àíŒªÀÜŒõ‚àí1
2 Œ∏2)
+ ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3(Œ∏3 ‚àíÀÜŒ∏3)
= . . .
=
H
X
h=1
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ (ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh ‚àíŒªÀÜŒõ‚àí1
h Œ∏h).
18

Published as a conference paper at ICLR 2021
Also note that
(QœÄ(s1, œÄ(s1)) ‚àíÀÜQ(s1, œÄ(s1)))2 = ‚à•Œ∏1 ‚àíÀÜŒ∏1‚à•2
Œõ1.
D.2
PROOF OF THEOREM 5.2
By matrix concentration inequality (Tropp, 2015), we have the following lemma.
Lemma D.1. For each h ‚àà[H], with probability 1 ‚àíŒ¥/(4H), for some universal constant C, we
have

1
N Œ¶‚ä§
h Œ¶h ‚àíŒõh

2
‚â§C
p
d log(dH/Œ¥)/N.
and

1
N Œ¶h+1Œ¶h+1 ‚àíŒõh+1

2
‚â§C
p
d log(dH/Œ¥)/N.
Therefore, since Œª = CH
p
d log(dH/Œ¥)N, with probability 1 ‚àíŒ¥/(4H), we have
ÀÜŒõh = Œ¶‚ä§
h Œ¶h + ŒªI ‚™∞NŒõh.
Note that
(QœÄ(s1, œÄ(s1)) ‚àíÀÜQ(s1, œÄ(s1)))2
‚â§H ¬∑
 H
X
h=1
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ (ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh ‚àíŒªÀÜŒõ‚àí1
h Œ∏h)

2
Œõ1
!
‚â§2H ¬∑
 H
X
h=1
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh

2
Œõ1 +
H
X
h=1
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ŒªÀÜŒõ‚àí1
h Œ∏h

2
Œõ1
!
.
For each h ‚àà[H],
‚à•ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh‚à•2
Œõ1
‚â§‚à•Œ¶1ÀÜŒõ‚àí1
1 Œõ1ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 ‚à•2 ¬∑ ‚à•Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh‚à•2
2
‚â§‚à•ÀÜŒõ‚àí1/2
1
Œõ1ÀÜŒõ‚àí1/2
1
‚à•2 ¬∑ ‚à•Œ¶1ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 ‚à•2 ¬∑ ‚à•Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶2 ¬∑ ¬∑ ¬∑ ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh‚à•2
2
‚â§‚à•ÀÜŒõ‚àí1/2
1
Œõ1ÀÜŒõ‚àí1/2
1
‚à•2 ¬∑
h‚àí1
Y
h‚Ä≤=1

‚à•Œ¶h‚Ä≤ ÀÜŒõ‚àí1
h‚Ä≤ Œ¶‚ä§
h‚Ä≤‚à•2 ¬∑ ‚à•ÀÜŒõ‚àí1/2
h‚Ä≤+1(Œ¶
‚ä§
h‚Ä≤+1Œ¶h‚Ä≤+1)ÀÜŒõ‚àí1/2
h‚Ä≤+1‚à•2

¬∑ ‚à•Œæh‚à•2
Œ¶h ÀÜŒõ‚àí1
h Œ¶‚ä§
h .
Similarly,
‚à•ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ŒªÀÜŒõ‚àí1
h Œ∏h‚à•2
Œõ1
‚â§‚à•ÀÜŒõ‚àí1/2
1
Œõ1ÀÜŒõ‚àí1/2
1
‚à•2 ¬∑
h‚àí1
Y
h‚Ä≤=1

‚à•Œ¶h‚Ä≤ ÀÜŒõ‚àí1
h‚Ä≤ Œ¶‚ä§
h‚Ä≤‚à•2 ¬∑ ‚à•ÀÜŒõ‚àí1/2
h‚Ä≤+1(Œ¶
‚ä§
h‚Ä≤+1Œ¶h‚Ä≤+1)ÀÜŒõ‚àí1/2
h+1 ‚à•2

¬∑ Œª2 ¬∑ ‚à•Œ∏h‚à•2
ÀÜŒõ‚àí1
h
‚â§‚à•ÀÜŒõ‚àí1/2
1
Œõ1ÀÜŒõ‚àí1/2
1
‚à•2 ¬∑
h‚àí1
Y
h‚Ä≤=1

‚à•Œ¶h‚Ä≤ ÀÜŒõ‚àí1
h‚Ä≤ Œ¶‚ä§
h‚Ä≤‚à•2 ¬∑ ‚à•ÀÜŒõ‚àí1/2
h‚Ä≤+1(Œ¶
‚ä§
h‚Ä≤+1Œ¶h‚Ä≤+1)ÀÜŒõ‚àí1/2
h‚Ä≤+1‚à•2

¬∑ Œª ¬∑ H2d.
For all h ‚àà[H], we have
‚à•Œ¶hÀÜŒõ‚àí1
h Œ¶‚ä§
h ‚à•2 ‚â§1
and
‚à•ÀÜŒõ‚àí1/2
h
(Œ¶
‚ä§
h Œ¶h)ÀÜŒõ‚àí1/2
h
‚à•2 ‚â§‚à•N ÀÜŒõ‚àí1/2
h
ŒõhÀÜŒõ‚àí1/2
h
‚à•2 + ‚à•ÀÜŒõ‚àí1/2
h
(Œ¶
‚ä§
h Œ¶h ‚àíNŒõh)ÀÜŒõ‚àí1/2
h
‚à•2.
Conditioned on the event in Lemma D.1,
ÀÜŒõh ‚™∞NŒõh ‚™∞N
Ch
Œõh,
19

Published as a conference paper at ICLR 2021
which implies ‚à•N ÀÜŒõ‚àí1/2
h
ŒõhÀÜŒõ‚àí1/2
h
‚à•‚â§Ch. Moreover, conditioned on the event in Lemma D.1,
‚à•ÀÜŒõ‚àí1/2
h
(Œ¶
‚ä§
h Œ¶h ‚àíNŒõh)ÀÜŒõ‚àí1/2
h
‚à•2 ‚â§C
p
d log(dH/Œ¥)N/Œª.
Thus,
‚à•ÀÜŒõ‚àí1/2
1
Œõ1ÀÜŒõ‚àí1/2
1
‚à•2 ‚â§C1/N.
and
‚à•ÀÜŒõ‚àí1/2
h
(Œ¶
‚ä§
h Œ¶h)ÀÜŒõ‚àí1/2
h
‚à•2 ‚â§Ch + C
p
d log(dH/Œ¥)N/Œª.
Finally, by Theorem 1.2 in (Hsu et al., 2012a), with probability 1 ‚àíŒ¥/(4H), for some constant C‚Ä≤,
we have
‚à•Œæh‚à•2
Œ¶h ÀÜŒõ‚àí1
h Œ¶‚ä§
h ‚â§C‚Ä≤H2d log(H/Œ¥).
Therefore,
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ÀÜŒõ‚àí1
h Œ¶‚ä§
h Œæh

2
Œõ1 +
ÀÜŒõ‚àí1
1 Œ¶‚ä§
1 Œ¶2ÀÜŒõ‚àí1
2 Œ¶‚ä§
2 Œ¶3 ¬∑ ¬∑ ¬∑ ŒªÀÜŒõ‚àí1
h Œ∏h

2
Œõ1
‚â§C1
N (C2 + C
p
d log(d/Œ¥)N/Œª) √ó ¬∑ ¬∑ ¬∑ √ó (Ch + C
p
d log(d/Œ¥)N/Œª) √ó (C‚Ä≤H2d log(H/Œ¥) + ŒªH2d)
‚â§C1
N (C2 + 1/H) √ó ¬∑ ¬∑ ¬∑ √ó (Ch + 1/H) √ó (C‚Ä≤H2d log(H/Œ¥) + ŒªH2d)
‚â§e
N C1 √ó C2 √ó ¬∑ ¬∑ ¬∑ √ó Ch √ó (C‚Ä≤H2d log(H/Œ¥) + CdH3p
d log(dH/Œ¥)N).
Let c > 0 be a large enough constant. We now have
Es1[(QœÄ
1(s1, œÄ(s1)) ‚àíÀÜQ1(s1, œÄ(s1)))2] ‚â§c ¬∑
 H
Y
h=1
Ch
!
¬∑ dH5 ¬∑
r
d log(dH/Œ¥)
N
.
20

