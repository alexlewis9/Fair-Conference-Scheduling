Published as a conference paper at ICLR 2021
DIFFWAVE: A VERSATILE DIFFUSION MODEL FOR
AUDIO SYNTHESIS
Zhifeng Kong ∗
Computer Science and Engineering, UCSD
z4kong@eng.ucsd.edu
Wei Ping
NVIDIA
wping@nvidia.com
Jiaji Huang, Kexin Zhao
Baidu Research
{huangjiaji,kexinzhao}@baidu.com
Bryan Catanzaro
NVIDIA
bcatanzaro@nvidia.com
ABSTRACT
In this work, we propose DiffWave, a versatile diffusion probabilistic model for con-
ditional and unconditional waveform generation. The model is non-autoregressive,
and converts the white noise signal into structured waveform through a Markov
chain with a constant number of steps at synthesis. It is efﬁciently trained by
optimizing a variant of variational bound on the data likelihood. DiffWave pro-
duces high-ﬁdelity audio in different waveform generation tasks, including neural
vocoding conditioned on mel spectrogram, class-conditional generation, and un-
conditional generation. We demonstrate that DiffWave matches a strong WaveNet
vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing
orders of magnitude faster. In particular, it signiﬁcantly outperforms autoregressive
and GAN-based waveform models in the challenging unconditional generation task
in terms of audio quality and sample diversity from various automatic and human
evaluations. 1
1
INTRODUCTION
Deep generative models have produced high-ﬁdelity raw audio in speech synthesis and music
generation. In previous work, likelihood-based models, including autoregressive models (van den
Oord et al., 2016; Kalchbrenner et al., 2018; Mehri et al., 2017) and ﬂow-based models (Prenger
et al., 2019; Ping et al., 2020; Kim et al., 2019), have predominated in audio synthesis because of
the simple training objective and superior ability of modeling the ﬁne details of waveform in real
data. There are other waveform models, which often require auxiliary losses for training, such as
ﬂow-based models trained by distillation (van den Oord et al., 2018; Ping et al., 2019), variational
auto-encoder (VAE) based model (Peng et al., 2020), and generative adversarial network (GAN)
based models (Kumar et al., 2019; Bi´nkowski et al., 2020; Yamamoto et al., 2020).
Most of previous waveform models focus on audio synthesis with informative local conditioner (e.g.,
mel spectrogram or aligned linguistic features), with only a few exceptions for unconditional genera-
tion (Mehri et al., 2017; Donahue et al., 2019). It has been noticed that autoregressive models (e.g.,
WaveNet) tend to generate made-up word-like sounds (van den Oord et al., 2016), or inferior sam-
ples (Donahue et al., 2019) under unconditional settings. This is because very long sequences need to
be generated (e.g., 16,000 time-steps for one second speech) without any conditional information.
Diffusion probabilistic models (diffusion models for brevity) are a class of promising generative
models, which use a Markov chain to gradually convert a simple distribution (e.g., isotropic Gaussian)
into complicated data distribution (Sohl-Dickstein et al., 2015; Goyal et al., 2017; Ho et al., 2020).
Although the data likelihood is intractable, diffusion models can be efﬁciently trained by optimizing
the variational lower bound (ELBO). Most recently, a certain parameterization has been shown suc-
cessful in image synthesis (Ho et al., 2020), which is connected with denoising score matching (Song
∗Contributed to the work during an internship at Baidu Research, USA.
1Audio samples are in: https://diffwave-demo.github.io/
1

Published as a conference paper at ICLR 2021
· · · · · ·
x0
x1
x2
xT −1
xT
diffusion process
reverse process
q(x1|x0)
q(x2|x1)
q(xT |xT −1)
pθ(x0|x1)
pθ(x1|x2)
pθ(xT −1|xT )
qdata(x0)
platent(xT )
Figure 1: The diffusion and reverse process in diffusion probabilistic models. The reverse process
gradually converts the white noise signal into speech waveform through a Markov chain pθ(xt−1|xt).
& Ermon, 2019). Diffusion models can use a diffusion (noise-adding) process without learnable
parameters to obtain the “whitened” latents from training data. Therefore, no additional neural
networks are required for training in contrast to other models (e.g., the encoder in VAE (Kingma &
Welling, 2014) or the discriminator in GAN (Goodfellow et al., 2014)). This avoids the challenging
“posterior collapse” or “mode collapse” issues stemming from the joint training of two networks, and
hence is valuable for high-ﬁdelity audio synthesis.
In this work, we propose DiffWave, a versatile diffusion probabilistic model for raw audio synthesis.
DiffWave has several advantages over previous work: i) It is non-autoregressive thus can synthesize
high-dimensional waveform in parallel. ii) It is ﬂexible as it does not impose any architectural
constraints in contrast to ﬂow-based models, which need to keep the bijection between latents and
data (e.g., see more analysis in Ping et al. (2020)). This leads to small-footprint neural vocoders that
still generate high-ﬁdelity speech. iii) It uses a single ELBO-based training objective without any
auxiliary losses (e.g., spectrogram-based losses) for high-ﬁdelity synthesis. iv) It is a versatile model
that produces high-quality audio signals for both conditional and unconditional waveform generation.
Speciﬁcally, we make the following contributions:
1. DiffWave uses a feed-forward and bidirectional dilated convolution architecture motivated
by WaveNet (van den Oord et al., 2016). It matches the strong WaveNet vocoder in terms of
speech quality (MOS: 4.44 vs. 4.43), while synthesizing orders of magnitude faster as it only
requires a few sequential steps (e.g., 6) for generating very long waveforms.
2. Our small DiffWave has 2.64M parameters and synthesizes 22.05 kHz high-ﬁdelity speech
(MOS: 4.37) more than 5× faster than real-time on a V100 GPU without engineered kernels.
Although it is still slower than the state-of-the-art ﬂow-based models (Ping et al., 2020;
Prenger et al., 2019), it has much smaller footprint. We expect further speed-up by optimizing
its inference mechanism in the future.
3. DiffWave signiﬁcantly outperforms WaveGAN (Donahue et al., 2019) and WaveNet in the
challenging unconditional and class-conditional waveform generation tasks in terms of audio
quality and sample diversity measured by several automatic and human evaluations.
We organize the rest of the paper as follows. We present the diffusion models in Section 2, and intro-
duce DiffWave architecture in Section 3. Section 4 discusses related work. We report experimental
results in Section 5 and conclude the paper in Section 6.
2
DIFFUSION PROBABILISTIC MODELS
We deﬁne qdata(x0) as the data distribution on RL, where L is the data dimension. Let xt ∈RL
for t = 0, 1, · · · , T be a sequence of variables with the same dimension, where t is the index for
diffusion steps. Then, a diffusion model of T steps is composed of two processes: the diffusion
process, and the reverse process (Sohl-Dickstein et al., 2015). Both of them are illustrated in Figure 1.
2

Published as a conference paper at ICLR 2021
Algorithm 1 Training
for i = 1, 2, · · · , Niter do
Sample x0 ∼qdata, ϵ ∼N(0, I), and
t ∼Uniform({1, · · · , T})
Take gradient step on
∇θ∥ϵ −ϵθ(√¯αtx0 + √1 −¯αtϵ, t)∥2
2
according to Eq. (7)
end for
Algorithm 2 Sampling
Sample xT ∼platent = N(0, I)
for t = T, T −1, · · · , 1 do
Compute µθ(xt, t) and σθ(xt, t) using Eq. (5)
Sample xt−1 ∼pθ(xt−1|xt) =
N(xt−1; µθ(xt, t), σθ(xt, t)2I)
end for
return x0
The diffusion process is deﬁned by a ﬁxed Markov chain from data x0 to the latent variable xT :
q(x1, · · · , xT |x0) =
T
Y
t=1
q(xt|xt−1),
(1)
where each of q(xt|xt−1) is ﬁxed to N(xt; √1 −βtxt−1, βtI) for a small positive constant βt. The
function of q(xt|xt−1) is to add small Gaussian noise to the distribution of xt−1. The whole process
gradually converts data x0 to whitened latents xT according to a variance schedule β1, · · · , βT . 2
The reverse process is deﬁned by a Markov chain from xT to x0 parameterized by θ:
platent(xT ) = N(0, I), and pθ(x0, · · · , xT −1|xT ) =
T
Y
t=1
pθ(xt−1|xt),
(2)
where platent(xT ) is isotropic Gaussian, and the transition probability pθ(xt−1|xt) is parameterized
as N(xt−1; µθ(xt, t), σθ(xt, t)2I) with shared parameter θ. Note that both µθ and σθ take two inputs:
the diffusion-step t ∈N, and variable xt ∈RL. µθ outputs an L-dimensional vector as the mean,
and σθ outputs a real number as the standard deviation. The goal of pθ(xt−1|xt) is to eliminate the
Gaussian noise (i.e. denoise) added in the diffusion process.
Sampling: Given the reverse process, the generative procedure is to ﬁrst sample an xT ∼N(0, I),
and then sample xt−1 ∼pθ(xt−1|xt) for t = T, T −1, · · · , 1. The output x0 is the sampled data.
Training: The likelihood pθ(x0) =
R
pθ(x0, · · · , xT −1|xT ) · platent(xT ) dx1:T is intractable to
calculate in general. The model is thus trained by maximizing its variational lower bound (ELBO):
Eqdata(x0) log pθ(x0) = Eqdata(x0) log Eq(x1,··· ,xT |x0)
hpθ(x0, · · · , xT −1|xT ) × platent(xT )
q(x1, · · · , xT |x0)
i
≥Eq(x0,··· ,xT ) log pθ(x0, · · · , xT −1|xT ) × platent(xT )
q(x1, · · · , xT |x0)
:= ELBO.
(3)
Most recently, Ho et al. (2020) showed that under a certain parameterization, the ELBO of the
diffusion model can be calculated in closed-form. This accelerates the computation and avoids Monte
Carlo estimates, which have high variance. This parameterization is motivated by its connection
to denoising score matching with Langevin dynamics (Song & Ermon, 2019; 2020). To introduce
this parameterization, we ﬁrst deﬁne some constants based on the variance schedule {βt}T
t=1 in the
diffusion process as in Ho et al. (2020):
αt = 1 −βt, ¯αt =
tY
s=1
αs,
˜βt = 1 −¯αt−1
1 −¯αt
βt for t > 1 and ˜β1 = β1.
(4)
Then, the parameterizations of µθ and σθ are deﬁned by
µθ(xt, t) =
1
√αt

xt −
βt
√1 −¯αt
ϵθ(xt, t)

, and σθ(xt, t) = ˜β
1
2
t ,
(5)
where ϵθ : RL × N →RL is a neural network also taking xt and the diffusion-step t as inputs. Note
that σθ(xt, t) is ﬁxed to a constant ˜β
1
2
t for every step t under this parameterization. In the following
proposition, we explicitly provide the closed-form expression of the ELBO.
2One can ﬁnd that q(xT |x0) approaches to isotropic Gaussian with large T in Eq. (11) in the Appendix A.
3

Published as a conference paper at ICLR 2021
Input
Conv1×1
=  Conv1×1
Bi-DilConv-2$
=  Bi-directional Dilated Conv
(dilation = 2$)
FC
=  Fully connected
=  Broadcast over length
Conditioner
↔
=  Element-wise addition
+
=  Element-wise multiplication
Diffusion-step 
embedding
Residual layer ' = 0
Residual layer ' = 1
Residual layer ' = * −1
⋯
Skip connections
Output
⋯
FC
FC
FC
Conv1×1
Conv1×1
Conv1×1
Conv1×1
Conv1×1
Bi-DilConv-2- ./0 1
↔
+
+
+
+
⋅
⋅
tanh
7
=  Connect to next residual layer
=  Input of each residual layer
Conv1×1
ReLU
swish
ReLU
swish
Figure 2: The network architecture of DiffWave in modeling ϵθ : RL × N →RL.
Proposition 1. (Ho et al., 2020) Suppose a series of ﬁxed schedule {βt}T
t=1 are given. Let ϵ ∼
N(0, I) and x0 ∼qdata. Then, under the parameterization in Eq. (5), we have
−ELBO = c +
T
X
t=1
κtEx0,ϵ ∥ϵ −ϵθ(√¯αtx0 +
√
1 −¯αtϵ, t)∥2
2
(6)
for some constants c and κt, where κt =
βt
2αt(1−¯αt−1) for t > 1, and κ1 =
1
2α1 .
Note that c is irrelevant for optimization purpose. The key idea in the proof is to expand the ELBO
into a sum of KL divergences between tractable Gaussian distributions, which have a closed-form
expression. We refer the readers to look at Section A in the Appendix for the full proof.
In addition, Ho et al. (2020) reported that minimizing the following unweighted variant of the ELBO
leads to higher generation quality:
min
θ
Lunweighted(θ) = Ex0,ϵ,t ∥ϵ −ϵθ(√¯αtx0 +
√
1 −¯αtϵ, t)∥2
2
(7)
where t is uniformly taken from 1, · · · , T. Therefore, we also use this training objective in this paper.
We summarize the training and sampling procedures in Algorithm 1 and 2, respectively.
Fast sampling: Given a trained model from Algorithm 1, we noticed that the most effective denoising
steps at sampling occur near t = 0 (see Section IV on demo website). This encourages us to design
a fast sampling algorithm with much fewer denoising steps Tinfer (e.g., 6) than T at training (e.g.,
200). The key idea is to “collapse” the T-step reverse process into a Tinfer-step process with carefully
designed variance schedule. We provide the details in Appendix B.
3
DIFFWAVE ARCHITECTURE
In this section, we present the architecture of DiffWave (see Figure 2 for an illustration). We build the
network ϵθ : RL × N →RL in Eq. (5) based on a bidirectional dilated convolution architecture that
is different from WaveNet (van den Oord et al., 2016), because there is no autoregressive generation
constraint. 3 The similar architecture has been applied for source separation (Rethage et al., 2018;
Lluís et al., 2018). The network is non-autoregressive, so generating an audio x0 with length L from
latents xT requires T rounds of forward propagation, where T (e.g., 50) is much smaller than the
waveform length L. The network is composed of a stack of N residual layers with residual channels
3Indeed, we found the causal dilated convolution architecture leads to worse audio quality in DiffWave.
4

Published as a conference paper at ICLR 2021
C. These layers are grouped into m blocks and each block has n = N
m layers. We use a bidirectional
dilated convolution (Bi-DilConv) with kernel size 3 in each layer. The dilation is doubled at each
layer within each block, i.e., [1, 2, 4, · · · , 2n−1]. We sum the skip connections from all residual layers
as in WaveNet. More details including the tensor shapes are included in Section C in the Appendix.
3.1
DIFFUSION-STEP EMBEDDING
It is important to include the diffusion-step t as part of the input, as the model needs to output different
ϵθ(·, t) for different t. We use an 128-dimensional encoding vector for each t (Vaswani et al., 2017):
tembedding =
h
sin

10
0×4
63 t

, · · · , sin

10
63×4
63 t

, cos

10
0×4
63 t

, · · · , cos

10
63×4
63 t
i
(8)
We then apply three fully connected (FC) layers on the encoding, where the ﬁrst two FCs share
parameters among all residual layers. The last residual-layer-speciﬁc FC maps the output of the
second FC into a C-dimensional embedding vector. We next broadcast this embedding vector over
length and add it to the input of every residual layer.
3.2
CONDITIONAL GENERATION
Local conditioner: In speech synthesis, a neural vocoder can synthesize the waveform conditioned
on the aligned linguistic features (van den Oord et al., 2016; Arık et al., 2017b), the mel spectrogram
from a text-to-spectrogram model (Ping et al., 2018; Shen et al., 2018), or the hidden states within the
text-to-wave architecture (Ping et al., 2019; Donahue et al., 2020). In this work, we test DiffWave as a
neural vocoder conditioned on mel spectrogram. We ﬁrst upsample the mel spectrogram to the same
length as waveform through transposed 2-D convolutions. After a layer-speciﬁc Conv1×1 mapping
its mel-band into 2C channels, the conditioner is added as a bias term for the dilated convolution in
each residual layer. The hyperparameters can be found in Section 5.1.
Global conditioner: In many generative tasks, the conditional information is given by global discrete
labels (e.g., speaker IDs or word IDs). We use shared embeddings with dimension dlabel = 128 in all
experiments. In each residual layer, we apply a layer-speciﬁc Conv1×1 to map dlabel to 2C channels,
and add the embedding as a bias term after the dilated convolution in each residual layer.
3.3
UNCONDITIONAL GENERATION
In unconditional generation task, the model needs to generate consistent utterances without conditional
information. It is important for the output units of the network to have a receptive ﬁeld size (denoted
as r) larger than the length L of the utterance. Indeed, we need r ≥2L, thus the left and right-most
output units have receptive ﬁelds covering the whole L-dimensional inputs as illustrated in Figure 4
in Appendix. This posts a challenge for architecture design even with the dilated convolutions.
For a stack of dilated convolution layers, the receptive ﬁeld size of the output is up to: r =
(k −1) P
i di + 1, where k is the kernel size and di is the dilation at i-th residual layer. For example,
30-layer dilated convolution has a receptive ﬁeld size r = 6139, with k = 3 and dilation cycle
[1, 2, · · · , 512]. This only amounts to 0.38s of 16kHz audio. We can further increase the number of
layers and the size of dilation cycles; however, we found degraded quality with deeper layers and
larger dilation cycles. This is particularly true for WaveNet. In fact, previous study (Shen et al., 2018)
suggests that even a moderate large receptive ﬁeld size (e.g., 6139) is not effectively used in WaveNet
and it tends to focus on much shorter context (e.g., 500). DiffWave has an advantage in enlarging the
receptive ﬁelds of output x0: by iterating from xT to x0 in the reverse process, the receptive ﬁeld
size can be increased up to T × r, which makes DiffWave suitable for unconditional generation.
4
RELATED WORK
In the past years, many neural text-to-speech (TTS) systems have been introduced. An incomplete
list includes WaveNet (van den Oord et al., 2016), Deep Voice 1 & 2 & 3 (Arık et al., 2017a;b; Ping
et al., 2018), Tacotron 1 & 2 (Wang et al., 2017; Shen et al., 2018), Char2Wav (Sotelo et al., 2017),
VoiceLoop (Taigman et al., 2018), Parallel WaveNet (van den Oord et al., 2018), WaveRNN (Kalch-
brenner et al., 2018), ClariNet (Ping et al., 2019), ParaNet (Peng et al., 2020), FastSpeech (Ren et al.,
2019), GAN-TTS (Bi´nkowski et al., 2020), and Flowtron (Valle et al., 2020). These systems ﬁrst
5

Published as a conference paper at ICLR 2021
generate intermediate representations (e.g., aligned linguistic features, mel spectrogram, or hidden
representations) conditioned on text, then use a neural vocoder to synthesize the raw waveform.
Neural vocoder plays the most important role in the recent success of speech synthesis. Autoregressive
models like WaveNet and WaveRNN can generate high-ﬁdelity speech, but in a sequential way of
generation. Parallel WaveNet and ClariNet distill parallel ﬂow-based models from WaveNet, thus can
synthesize waveform in parallel. In contrast, WaveFlow (Ping et al., 2020), WaveGlow (Prenger et al.,
2019) and FloWaveNet (Kim et al., 2019) are trained by maximizing likelihood. There are other
waveform models, such as VAE-based models (Peng et al., 2020), GAN-based models (Kumar et al.,
2019; Yamamoto et al., 2020; Bi´nkowski et al., 2020), and neural signal processing models (Wang
et al., 2019; Engel et al., 2020; Ai & Ling, 2020). In contrast to likelihood-based models, they often
require auxiliary training losses to improve the audio ﬁdelity. The proposed DiffWave is another
promising neural vocoder synthesizing the best quality of speech with a single objective function.
Unconditional generation of audio in the time domain is a challenging task in general. Likelihood-
based models are forced to learn all possible variations within the dataset without any conditional
information, which can be quite difﬁcult with limited model capacity. In practice, these models
produce made-up word-like sounds or inferior samples (van den Oord et al., 2016; Donahue et al.,
2019). VQ-VAE (van den Oord et al., 2017) circumvents this issue by compressing the waveform
into compact latent code, and training an autoregressive model in latent domain. GAN-based models
are believed to be suitable for unconditional generation (e.g., Donahue et al., 2019) due to the “mode
seeking” behaviour and success in image domain (Brock et al., 2018). Note that unconditional
generation of audio in the frequency domain is considered easier, as the spectrogram is much shorter
(e.g., 200×) than waveform (Vasquez & Lewis, 2019; Engel et al., 2019; Palkama et al., 2020).
In this work, we demonstrate the superior performance of DiffWave in unconditional generation
of waveform. In contrast to the exact-likelihood models, DiffWave maximizes a variational lower
bound of the likelihood, which can focus on the major variations within the data and alleviate the
requirements for model capacity. In contrast to GAN or VAE-based models (Donahue et al., 2019;
Peng et al., 2020), it is much easier to train without mode collapse, posterior collapse, or training
instability stemming from the joint training of two networks. There is a concurrent work (Chen et al.,
2020) that uses diffusion probabilistic models for waveform generation. In contrast to DiffWave, it
uses a neural architecture similar to GAN-TTS and focuses on the neural vocoding task only. Our
DiffWave vocoder has much fewer parameters than WaveGrad – 2.64M vs. 15M for Base models
and 6.91M vs. 23M for Large models. The small memory footprint is preferred in production TTS
systems, especially for on-device deployment. In addition, DiffWave requires a smaller batch size (16
vs. 256) and fewer computational resources for training.
5
EXPERIMENTS
We evaluate DiffWave on neural vocoding, unconditional and class-conditional generation tasks.
5.1
NEURAL VOCODING
Data: We use the LJ speech dataset (Ito, 2017) that contains ∼24 hours of audio recorded in home
environment with a sampling rate of 22.05 kHz. It contains 13,100 utterances from a female speaker.
Models: We compare DiffWave with several state-of-the-art neural vocoders, including WaveNet,
ClariNet, WaveGlow and WaveFlow. Details of baseline models can be found in the original papers.
Their hyperparameters can be found in Table 1. Our DiffWave models have 30 residual layers, kernel
size 3, and dilation cycle [1, 2, · · · , 512]. We compare DiffWave models with different number of
diffusion steps T ∈{20, 40, 50, 200} and residual channels C ∈{64, 128}. We use linear spaced
schedule for βt ∈[1 × 10−4, 0.02] for DiffWave with T = 200, and βt ∈[1 × 10−4, 0.05] for
DiffWave with T ≤50. The reason to increase βt for smaller T is to make q(xT |x0) close to platent.
In addition, we compare the fast sampling algorithm with smaller Tinfer (see Appendix B), denoted as
DiffWave (Fast), with the regular sampling (Algorithm 2). Both of them use the same trained models.
Conditioner: We use the 80-band mel spectrogram of the original audio as the conditioner to test
these neural vocoders as in previous work (Ping et al., 2019; Prenger et al., 2019; Kim et al., 2019).
We set FFT size to 1024, hop size to 256, and window size to 1024. We upsample the mel spectrogram
256 times by applying two layers of transposed 2-D convolution (in time and frequency) interleaved
6

Published as a conference paper at ICLR 2021
Table 1: The model hyperparameters, model footprint, and 5-scale Mean Opinion Score (MOS) with
95% conﬁdence intervals for WaveNet, ClariNet, WaveFlow, WaveGlow and the proposed DiffWave
on the neural vocoding task. ↑means the number is the higher the better, and ↓means the number is
the lower the better.
Model
T
Tinfer
layers
res. channels
#param(↓)
MOS(↑)
WaveNet
—
—
30
128
4.57M
4.43 ± 0.10
ClariNet
—
—
60
64
2.17M
4.27 ± 0.09
WaveGlow
—
—
96
256
87.88M
4.33 ± 0.12
WaveFlow
—
—
64
64
5.91M
4.30 ± 0.11
WaveFlow
—
—
64
128
22.25M
4.40 ± 0.07
DiffWave BASE
20
20
30
64
2.64M
4.31 ± 0.09
DiffWave BASE
40
40
30
64
2.64M
4.35 ± 0.10
DiffWave BASE
50
50
30
64
2.64M
4.38 ± 0.08
DiffWave LARGE
200
200
30
128
6.91M
4.44 ± 0.07
DiffWave BASE (Fast)
50
6
30
64
2.64M
4.37 ± 0.07
DiffWave LARGE (Fast)
200
6
30
128
6.91M
4.42 ± 0.09
Ground-truth
—
—
—
—
—
4.52 ± 0.06
with leaky ReLU (α = 0.4). For each layer, the upsamling stride in time is 16 and 2-D ﬁlter sizes are
[32, 3]. After upsampling, we use a layer-speciﬁc Conv1×1 to map the 80 mel bands into 2× residual
channels, then add the conditioner as a bias term for the dilated convolution before the gated-tanh
nonlinearities in each residual layer.
Training: We train DiffWave on 8 Nvidia 2080Ti GPUs using random short audio clips of 16,000
samples from each utterance. We use Adam optimizer (Kingma & Ba, 2015) with a batch size of 16
and learning rate 2 × 10−4. We train all DiffWave models for 1M steps. For other models, we follow
the training setups as in the original papers.
Results: We use the crowdMOS tookit (Ribeiro et al., 2011) for speech quality evaluation, where the
test utterances from all models were presented to Mechanical Turk workers. We report the 5-scale
Mean Opinion Scores (MOS), and model footprints in Table 1 4. Our DiffWave LARGE model with
residual channels 128 matches the strong WaveNet vocoder in terms of speech quality (MOS: 4.44
vs. 4.43). The DiffWave BASE with residual channels 64 also generates high quality speech (e.g.,
MOS: 4.35) even with small number of diffusion steps (e.g., T = 40 or 20). For synthesis speed,
DiffWave BASE (T = 20) in FP32 generates audio 2.1× faster than real-time, and DiffWave BASE (T =
40) in FP32 is 1.1× faster than real-time on a Nvidia V100 GPU without engineering optimization.
Meanwhile, DiffWave BASE (Fast) and DiffWave LARGE (Fast) can be 5.6× and 3.5× faster than real-
time respectively and still obtain good audio ﬁdelity. In contrast, a WaveNet implementation can
be 500× slower than real-time at synthesis without engineered kernels. DiffWave is still slower
than the state-of-the-art ﬂow-based models (e.g., a 5.91M WaveFlow is > 40× faster than real-time
in FP16), but has smaller footprint and slightly better quality. Because DiffWave does not impose
any architectural constraints as in ﬂow-based models, we expect further speed-up by optimizing the
architecture and inference mechanism in the future.
5.2
UNCONDITIONAL GENERATION
In this section, we apply DiffWave to an unconditional generation task based on raw waveform only.
Data: We use the Speech Commands dataset (Warden, 2018), which contains many spoken words by
thousands of speakers under various recording conditions including some very noisy environment.
We select the subset that contains spoken digits (0∼9), which we call the SC09 dataset. The SC09
dataset contains 31,158 training utterances (∼8.7 hours in total) by 2,032 speakers, where each audio
has length equal to one second under sampling rate 16kHz. Therefore, the data dimension L is 16,000.
Note that the SC09 dataset exhibits various variations (e.g., contents, speakers, speech rate, recording
conditions); the generative models need to model them without any conditional information.
Models: We compare DiffWave with WaveNet and WaveGAN. We also tried to remove the mel
conditioner in a state-of-the-art GAN-based neural vocoder (Yamamoto et al., 2020), but found it could
4The MOS evaluation for DiffWave(Fast) with Tinfer = 6 was done after paper submission and may not be
directly comparable to previous scores.
7

Published as a conference paper at ICLR 2021
Table 2: The automatic evaluation metrics (FID, IS, mIS, AM, and NDB/K), and 5-scale MOS with
95% conﬁdence intervals for WaveNet, WaveGAN, and DiffWave on the unconditional generation
task. ↑means the number is the higher the better, and ↓means the number is the lower the better.
Model
FID(↓)
IS(↑)
mIS(↑)
AM(↓)
NDB/K(↓)
MOS(↑)
WaveNet-128
3.279
2.54
7.6
1.368
0.86
1.34 ± 0.29
WaveNet-256
2.947
2.84
10.0
1.260
0.86
1.43 ± 0.30
WaveGAN
1.349
4.53
36.6
0.796
0.78
2.03 ± 0.33
DiffWave
1.287
5.30
59.4
0.636
0.74
3.39 ± 0.32
Trainset
0.000
8.48
281.4
0.164
0.00
—
Testset
0.011
8.47
275.2
0.166
0.10
3.72 ± 0.28
Table 3: The automatic evaluation metrics (Accuracy, FID-class, IS, mIS), and 5-scale MOS with
95% conﬁdence intervals for WaveNet and DiffWave on the class-conditional generation task.
Model
Accuracy(↑)
FID-class(↓)
IS(↑)
mIS(↑)
MOS(↑)
WaveNet-128
56.20%
7.876±2.469
3.29
15.8
1.46 ± 0.30
WaveNet-256
60.70%
6.954±2.114
3.46
18.9
1.58 ± 0.36
DiffWave
91.20%
1.113±0.569
6.63
117.4
3.50 ± 0.31
DiffWave (deep & thin)
94.00%
0.932±0.450
6.92
133.8
3.44 ± 0.36
Trainset
99.06%
0.000±0.000
8.48
281.4
—
Testset
98.76%
0.044±0.016
8.47
275.2
3.72 ± 0.28
not generate intelligible speech in this unconditional task. We use 30 layer-WaveNet models with
residual channels 128 (denoted as WaveNet-128) and 256 (denoted as WaveNet-256), respectively.
We tried to increase the size of the dilation cycle and the number of layers, but these modiﬁcations
lead to worse quality. In particular, a large dilation cycle (e.g., up to 2048) leads to unstable training.
For WaveGAN, we use their pretrained model on Google Colab. We use a 36-layer DiffWave model
with kernel size 3 and dilation cycle [1, 2, · · · , 2048]. We set the number of diffusion steps T = 200
and residual channels C = 256. We use linear spaced schedule for βt ∈[1 × 10−4, 0.02].
Training: We train WaveNet and DiffWave on 8 Nvidia 2080Ti GPUs using full utterances. We use
Adam optimizer with a batch size of 16. For WaveNet, we set the initial learning rate as 1 × 10−3
and halve the learning rate every 200K iterations. For DiffWave, we ﬁx the learning rate to 2 × 10−4.
We train WaveNet and DiffWave for 1M steps.
Evaluation: For human evaluation, we report the 5-scale MOS for speech quality similar to Section
5.1. To automatically evaluate the quality of generated audio samples, we train a ResNeXT classiﬁer
(Xie et al., 2017) on the SC09 dataset according to an open repository (Xu & Tuguldur, 2017). The
classiﬁer achieves 99.06% accuracy on the trainset and 98.76% accuracy on the testset. We use the
following evaluation metrics based on the 1024-dimensional feature vector and the 10-dimensional
logits from the ResNeXT classiﬁer (see Section D in the Appendix for the detailed deﬁnitions):
• Fréchet Inception Distance (FID) (Heusel et al., 2017) measures both quality and diversity
of generated samples, and favors generators that match moments in the feature space.
• Inception Score (IS) (Salimans et al., 2016) measures both quality and diversity of gener-
ated samples, and favors generated samples that can be clearly determined by the classiﬁer.
• Modiﬁed Inception Score (mIS) (Gurumurthy et al., 2017) measures the within-class
diversity of samples in addition to IS.
• AM Score (Zhou et al., 2017) takes into consideration the marginal label distribution of
training data compared to IS.
• Number of Statistically-Different Bins (NDB) (Richardson & Weiss, 2018) measures
diversity of generated samples.
Results: We randomly generate 1,000 audio samples from each model for evaluation. We report
results in Table 2. Our DiffWave model outperforms baseline models under all metrics, including
both automatic and human evaluation. Notably, the quality of audio samples generated by DiffWave
is much higher than WaveNet and WaveGAN baselines (MOS: 3.39 vs. 1.43 and 2.03). Note that the
quality of ground-truth audio exhibits large variations. The automatic evaluation metrics also indicate
that DiffWave is better at quality, diversity, and matching marginal label distribution of training data.
8

Published as a conference paper at ICLR 2021
5.3
CLASS-CONDITIONAL GENERATION
In this section, we provide the digit labels as the conditioner in DiffWave and compare our model to
WaveNet. We omit the comparison with conditional WaveGAN due to its noisy output audio (Lee
et al., 2018). For both DiffWave and WaveNet, the label conditioner is added to the model according to
Section 3.2. We use the same dataset, model hyperparameters, and training settings as in Section 5.2.
Evaluation: We use slightly different automatic evaluation methods in this section because audio
samples are generated according to pre-speciﬁed discrete labels. The AM score and NDB are removed
because they are less meaningful when the prior label distribution of generated data is speciﬁed. We
keep IS and mIS because IS favors sharp, clear samples and mIS measures within-class diversity. We
modify FID to FID-class: for each digit from 0 to 9, we compute FID between the generated audio
samples that are pre-speciﬁed as this digit and training utterances with the same digit labels, and
report the mean and standard deviation of these ten FID scores. We also report classiﬁcation accuracy
based on the ResNeXT classiﬁer used in Section 5.2.
Results: We randomly generate 100 audio samples for each digit (0 to 9) from all models for
evaluation. We report results in Table 3. Our DiffWave model signiﬁcantly outperforms WaveNet on
all evaluation metrics. It produces superior quality than WaveNet (MOS: 3.50 vs. 1.58), and greatly
decreases the gap to ground-truth (the gap between DiffWave and ground-truth is ∼10% of the gap
between WaveNet and ground-truth). The automatic evaluation metrics indicate that DiffWave is
much better at speech clarity (> 91% accuracy) and within-class diversity (its mIS is 6× higher
than WaveNet). We additionally found a deep and thin version of DiffWave with residual channels
C = 128 and 48 residual layers can achieve slightly better accuracy but lower audio quality. One may
also compare quality of generated audio samples between conditional and unconditional generation
based on IS, mIS, and MOS. For both WaveNet and DiffWave, IS increases by >20%, mIS almost
doubles, and MOS increases by ≥0.11. These results indicate that the digit labels reduces the
difﬁculty of the generative task and helps improving the generation quality of WaveNet and DiffWave.
5.4
ADDITIONAL RESULTS
Zero-shot speech denoising: The unconditional DiffWave model can readily perform speech denois-
ing. The SC09 dataset provides six types of noises for data augmentation in recognition tasks: white
noise, pink noise, running tap, exercise bike, dude miaowing, and doing the dishes. These noises are
not used during the training phase of our unconditional DiffWave in Section 5.2. We add 10% of
each type of noise to test data, feed these noisy utterances into the reverse process at t = 25, and
then obtain the outputs x0’s. The audio samples are in Section V on the demo website. Note that our
model is not trained on a denoising task and has zero knowledge about any noise type other than the
white noise added in diffusion process. It indicates DiffWave learns a good prior of raw audio.
Interpolation in latent space: We can do interpolation with the digit conditioned DiffWave model
in Section 5.3 on the SC09 dataset. The interpolation of voices xa
0, xb
0 between two speakers a, b is
done in the latent space at t = 50. We ﬁrst sample xa
t ∼q(xt|xa
0) and xb
t ∼q(xt|xb
0) for the two
speakers. We then do linear interpolation between xa
t and xb
t: xλ
t = (1 −λ)xa
t + λxb
t for 0 < λ < 1.
Finally, we sample xλ
0 ∼pθ(xλ
0|xλ
t ). The audio samples are in Section VI on the demo website.
6
CONCLUSION
In this paper, we present DiffWave, a versatile generative model for raw waveform. In the neural
vocoding task, it readily models the ﬁne details of waveform conditioned on mel spectrogram and
matches the strong autoregressive neural vocoder in terms of speech quality. In unconditional
and class-conditional generation tasks, it properly captures the large variations within the data and
produces realistic voices and consistent word-level pronunciations. To the best of our knowledge,
DiffWave is the ﬁrst waveform model that exhibits such versatility. DiffWave raises a number of open
problems and provides broad opportunities for future research. For example, it would be meaningful
to push the model to generate longer utterances, as DiffWave potentially has very large receptive ﬁelds.
Second, optimizing the inference speed would be beneﬁcial for applying the model in production
TTS, because DiffWave is still slower than ﬂow-based models. We found the most effective denoising
steps in the reverse process occur near x0, which suggests an even smaller T is possible in DiffWave.
In addition, the model parameters θ are shared across the reverse process, so the persistent kernels
that stash the parameters on-chip would largely speed-up inference on GPUs (Diamos et al., 2016).
9

Published as a conference paper at ICLR 2021
REFERENCES
Yang Ai and Zhen-Hua Ling. A neural vocoder with hierarchical generation of amplitude and phase
spectra for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and
Language Processing, 28:839–851, 2020.
Sercan Ö. Arık, Mike Chrzanowski, Adam Coates, Gregory Diamos, Andrew Gibiansky, Yongguo
Kang, Xian Li, John Miller, Jonathan Raiman, Shubho Sengupta, and Mohammad Shoeybi. Deep
Voice: Real-time neural text-to-speech. In ICML, 2017a.
Sercan Ö. Arık, Gregory Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan
Raiman, and Yanqi Zhou. Deep Voice 2: Multi-speaker neural text-to-speech. In NIPS, 2017b.
Mikołaj Bi´nkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande,
Luis C Cobo, and Karen Simonyan. High ﬁdelity speech synthesis with adversarial networks. In
ICLR, 2020.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity
natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveG-
rad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.
Greg Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski, Adam Coates, Erich Elsen,
Jesse Engel, Awni Hannun, and Sanjeev Satheesh. Persistent rnns: Stashing recurrent weights
on-chip. In International Conference on Machine Learning, pp. 2024–2033, 2016.
Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR, 2019.
Jeff Donahue, Sander Dieleman, Mikołaj Bi´nkowski, Erich Elsen, and Karen Simonyan. End-to-end
adversarial text-to-speech. arXiv preprint arXiv:2006.03575, 2020.
Jesse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue, and Adam
Roberts. Gansynth: Adversarial neural audio synthesis. In ICLR, 2019.
Jesse Engel, Lamtharn Hantrakul, Chenjie Gu, and Adam Roberts. Ddsp: Differentiable digital signal
processing. arXiv preprint arXiv:2001.04643, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems, pp. 2672–2680, 2014.
Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational
walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural
Information Processing Systems, pp. 4392–4402, 2017.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. Deligan: Generative
adversarial networks for diverse and limited data. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 166–174, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural
information processing systems, pp. 6626–6637, 2017.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020.
Keith Ito. The LJ speech dataset. 2017.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,
Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient
neural audio synthesis. In ICML, 2018.
Sungwon Kim, Sang-gil Lee, Jongyoon Song, and Sungroh Yoon. FloWaveNet: A generative ﬂow
for raw audio. In ICML, 2019.
10

Published as a conference paper at ICLR 2021
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Brébisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. In Advances in Neural Information Processing
Systems, pp. 14881–14892, 2019.
Chae Young Lee, Anoop Toffy, Gue Jun Jung, and Woo-Jin Han. Conditional wavegan. arXiv
preprint arXiv:1809.10636, 2018.
Francesc Lluís, Jordi Pons, and Xavier Serra. End-to-end music source separation: is it possible in
the waveform domain? arXiv preprint arXiv:1810.12187, 2018.
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron
Courville, and Yoshua Bengio. SampleRNN: An unconditional end-to-end neural audio generation
model. In ICLR, 2017.
Kasperi Palkama, Lauri Juvela, and Alexander Ilin. Conditional spoken digit generation with stylegan.
In Interspeech, 2020.
Kainan Peng, Wei Ping, Zhao Song, and Kexin Zhao. Non-autoregressive neural text-to-speech. In
ICML, 2020.
Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O Arik, Ajay Kannan, Sharan Narang, Jonathan
Raiman, and John Miller. Deep Voice 3: Scaling text-to-speech with convolutional sequence
learning. In ICLR, 2018.
Wei Ping, Kainan Peng, and Jitong Chen. ClariNet: Parallel wave generation in end-to-end text-to-
speech. In ICLR, 2019.
Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song. WaveFlow: A compact ﬂow-based model for
raw audio. In ICML, 2020.
Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A ﬂow-based generative network for
speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2019.
Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech:
Fast, robust and controllable text to speech. arXiv preprint arXiv:1905.09263, 2019.
Dario Rethage, Jordi Pons, and Xavier Serra. A wavenet for speech denoising. In 2018 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5069–5073.
IEEE, 2018.
Flávio Ribeiro, Dinei Florêncio, Cha Zhang, and Michael Seltzer. CrowdMOS: An approach for
crowdsourcing mean opinion score studies. In ICASSP, 2011.
Eitan Richardson and Yair Weiss. On gans and gmms. In Advances in Neural Information Processing
Systems, pp. 5847–5858, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems, pp.
2234–2242, 2016.
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by conditioning
WaveNet on mel spectrogram predictions. In ICASSP, 2018.
Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918–11930, 2019.
11

Published as a conference paper at ICLR 2021
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
arXiv preprint arXiv:2006.09011, 2020.
Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe Santos, Kyle Kastner, Aaron Courville, and
Yoshua Bengio. Char2wav: End-to-end speech synthesis. ICLR workshop, 2017.
Yaniv Taigman, Lior Wolf, Adam Polyak, and Eliya Nachmani. VoiceLoop: Voice ﬁtting and
synthesis via a phonological loop. In ICLR, 2018.
Rafael Valle, Kevin Shih, Ryan Prenger, and Bryan Catanzaro. Flowtron: an autoregressive ﬂow-based
generative network for text-to-speech synthesis. arXiv preprint arXiv:2005.05957, 2020.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for
raw audio. arXiv preprint arXiv:1609.03499, 2016.
Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning.
In Advances in Neural Information Processing Systems, pp. 6306–6315, 2017.
Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu,
George van den Driessche, Edward Lockhart, Luis C Cobo, Florian Stimberg, et al. Parallel
WaveNet: Fast high-ﬁdelity speech synthesis. In ICML, 2018.
Sean Vasquez and Mike Lewis. Melnet: A generative model for audio in the frequency domain. arXiv
preprint arXiv:1906.01083, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.
Xin Wang, Shinji Takaki, and Junichi Yamagishi. Neural source-ﬁlter-based waveform model for
statistical parametric speech synthesis. In ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 5916–5920. IEEE, 2019.
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng
Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark,
and Rif A. Saurous. Tacotron: Towards end-to-end speech synthesis. In Interspeech, 2017.
Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint
arXiv:1804.03209, 2018.
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492–1500, 2017.
Yuan
Xu
and
Erdene-Ochir
Tuguldur.
Convolutional
neural
networks
for
Google
speech commands data set with PyTorch, 2017.
https://github.com/tugstugi/
pytorch-speech-commands.
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim. Parallel wavegan: A fast waveform generation
model based on generative adversarial networks with multi-resolution spectrogram. In ICASSP
2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 6199–6203. IEEE, 2020.
Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Yong Yu, and Jun Wang.
Activation maximization generative adversarial nets. arXiv preprint arXiv:1703.02000, 2017.
12

Published as a conference paper at ICLR 2021
A
PROOF OF PROPOSITION 1
Proof. We expand the ELBO in Eq. (3) into the sum of a sequence of tractable KL divergences below.
ELBO = Eq log pθ(x0, · · · , xT −1|xT ) × platent(xT )
q(x1, · · · , xT |x0)
= Eq
 
log platent(xT ) −
T
X
t=1
log pθ(xt−1|xt)
q(xt|xt−1)
!
= Eq
 
log platent(xT ) −log pθ(x0|x1)
q(x1|x0) −
T
X
t=2

log pθ(xt−1|xt)
q(xt−1|xt, x0) + log q(xt−1|x0)
q(xt|x0)
!
= Eq
 
log platent(xT )
q(xT |x0) −log pθ(x0|x1) −
T
X
t=2
log pθ(xt−1|xt)
q(xt−1|xt, x0)
!
= −Eq
 
KL (q(xT |x0)∥platent(xT )) +
T
X
t=2
KL (q(xt−1|xt, x0)∥pθ(xt−1|xt)) −log pθ(x0|x1)
!
(9)
Before we calculate these terms individually, we ﬁrst derive q(xt|x0) and q(xt−1|xt, x0). Let ϵi’s be
independent standard Gaussian random variables. Then, by deﬁnition of q and using the notations of
constants introduced in Eq. (4), we have
xt
= √αtxt−1 + √βtϵt
= √αtαt−1xt−2 +
p
αtβt−1ϵt−1 + √βtϵt
= √αtαt−1αt−1xt−3 +
p
αtαt−1βt−2ϵt−2 +
p
αtβt−1ϵt−1 + √βtϵt
= · · ·
= √¯αtx0 +
p
αtαt−1 · · · α2β1ϵ1 + · · · +
p
αtβt−1ϵt−1 + √βtϵt
Note that q(xt|x0) is still Gaussian, and the mean of xt is √¯αtx0, and the variance matrix is
(αtαt−1 · · · α2β1 + · · · + αtβt−1 + βt)I = (1 −¯αt)I. Therefore,
q(xt|x0) = N(xt; √¯αtx0, (1 −¯αt)I).
(10)
It is worth mentioning that,
q(xT |x0) = N(xT ; √¯αT x0, (1 −¯αT )I),
(11)
where ¯αT = QT
t=1(1 −βt) approaches zero with large T.
Next, by Bayes rule and Markov chain property,
q(xt−1|xt, x0)
= q(xt|xt−1) q(xt−1|x0)
q(xt|x0)
= N(xt; √αtxt−1, βtI) N(xt−1; √¯αt−1x0, (1 −¯αt−1)I)
N(xt; √¯αtx0, (1 −¯αt)I)
= (2πβt)−d
2 (2π(1 −¯αt−1))−d
2 (2π(1 −¯αt))
d
2 ×
exp

−∥xt −√αtxt−1∥2
2βt
−∥xt−1 −√¯αt−1x0∥2
2(1 −¯αt−1)
+ ∥xt −√¯αtx0∥2
2(1 −¯αt)

= (2π ˜βt)−d
2 exp
 
−1
2˜βt
xt−1 −
√¯αt−1βt
1 −¯αt
x0 −
√αt(1 −¯αt−1)
1 −¯αt
xt

2!
Therefore,
q(xt−1|xt, x0) = N(xt−1;
√¯αt−1βt
1 −¯αt
x0 +
√αt(1 −¯αt−1)
1 −¯αt
xt, ˜βtI).
(12)
Now, we calculate each term of the ELBO expansion in Eq. (9). The ﬁrst constant term is
Eq KL (q(xT |x0)∥platent(xT ))
= Ex0KL
 N(√¯αT x0, (1 −¯αT )I)∥N(0, I)

= 1
2Ex0∥√¯αT x0 −0∥2 + d

log
1
√1 −¯αT
+ 1 −¯αT −1
2

= ¯αT
2 Ex0∥x0∥2 −d
2(¯αT + log(1 −¯αT ))
13

Published as a conference paper at ICLR 2021
Next, we compute KL (q(xt−1|xt, x0)∥pθ(xt−1|xt)). Because both q(xt−1|xt, x0) and pθ(xt−1|xt)
are Gaussian with the same covariance matrix ˜βtI, the KL divergence between them is
1
2 ˜βt times
the squared ℓ2 distance between their means. By the expression of q(xt|x0), we have xt = √¯αtx0 +
√1 −¯αtϵ. Therefore, we have
Eq KL (q(xt−1|xt, x0)∥pθ(xt−1|xt))
=
1
2˜βt
Ex0

√¯αt−1βt
1 −¯αt
x0 +
√αt(1 −¯αt−1)
1 −¯αt
xt −
1
√αt

xt −
βt
√1 −¯αt
ϵθ(xt, t)

2
=
1
2˜βt
Ex0,ϵ

√¯αt−1βt
1 −¯αt
· xt −√1 −¯αtϵ
√¯αt
+
√αt(1 −¯αt−1)
1 −¯αt
xt −
1
√αt

xt −
βt
√1 −¯αt
ϵθ(xt, t)

2
=
1
2˜βt
·
β2
t
αt(1 −¯αt)Ex0,ϵ ∥0 · xt + ϵ −ϵθ(xt, t)∥2
=
β2
t
2 1−¯αt−1
1−¯αt βtαt(1 −¯αt)
Ex0,ϵ ∥ϵ −ϵθ(xt, t)∥2
=
βt
2αt(1 −¯αt−1)Ex0,ϵ ∥ϵ −ϵθ(xt, t)∥2
Finally, as x1 = √¯α1x0 + √1 −¯α1ϵ = √α1x0 + √1 −α1ϵ, we have
Eq log pθ(x0|x1)
= Eq log N

x0;
1
√α1

x1 −
β1
√1 −α1
ϵθ(x1, 1)

, β1I

= Eq
 
−d
2 log 2πβ1 −
1
2β1
x0 −
1
√α1

x1 −
β1
√1 −α1
ϵθ(x1, 1)

2!
= −d
2 log 2πβ1 −
1
2β1
Ex0,ϵ
x0 −
1
√α1
√α1x0 +
√
1 −α1ϵ −
β1
√1 −α1
ϵθ(x1, 1)

2
= −d
2 log 2πβ1 −
1
2β1
Ex0,ϵ

√β1
√α1
(ϵ −ϵθ(x1, 1))

2
= −d
2 log 2πβ1 −
1
2α1
Ex0,ϵ∥ϵ −ϵθ(x1, 1)∥2
The computation of the ELBO is now ﬁnished.
14

Published as a conference paper at ICLR 2021
B
DETAILS OF THE FAST SAMPLING ALGORITHM
Let Tinfer ≪T be the number of steps in the reverse process (sampling) and {ηt}Tinfer
t=1
be the user-
deﬁned variance schedule, which can be independent with the training variance schedule {βt}T
t=1.
Then, we compute the corresponding constants in the same way as Eq. (4):
γt = 1 −ηt, ¯γt =
tY
s=1
γs, ˜ηt = 1 −¯γt−1
1 −¯γt
ηt for t > 1 and ˜η1 = η1.
(13)
As step s during sampling, we need to select an t and use ϵθ(·, t) to eliminate noise. This is realized
by aligning the noise levels from the user-deﬁned and the training variance schedules. Ideally, we
want √¯αt = √¯γs. However, since this is not always possible, we interpolate √¯γs between two
consecutive training noise levels √¯αt+1 and √¯αt, if √¯γs is between them. We therefore obtain the
desired aligned diffusion step t, which we denote talign
s
, via the following equation:
talign
s
= t +
√¯αt −√¯γs
√¯αt −√¯αt+1
if √¯γs ∈[ √¯αt+1, √¯αt ].
(14)
Note that, talign
s
is ﬂoating-point number, which is different from the integer diffusion-step at training.
Finally, the parameterizations of µθ and σθ are deﬁned in a similar way as Eq. (5):
µfast
θ
(xs, s) =
1
√γs

xs −
ηs
√1 −¯γs
ϵθ(xs, talign
s
)

, and σfast
θ
(xs, s) = ˜η
1
2s .
(15)
The fast sampling algorithm is summarized in Algorithm 3.
Algorithm 3 Fast Sampling
Sample xTinfer ∼platent = N(0, I)
for s = Tinfer, Tinfer −1, · · · , 1 do
Compute µfast
θ
(xs, s) and σfast
θ
(xs, s) using Eq. (15)
Sample xs−1 ∼N(xs−1; µfast
θ
(xs, s), σfast
θ
(xs, s)2I)
end for
return x0
In neural vocoding task, we use user-deﬁned variance schedules {0.0001, 0.001, 0.01, 0.05, 0.2, 0.7}
for DiffWave LARGE and {0.0001, 0.001, 0.01, 0.05, 0.2, 0.5} for DiffWave BASE in Section 5.1.
The fast sampling algorithm is similar to the sampling algorithm in Chen et al. (2020) in the sense of
considering the noise levels as a controllable variable during sampling. However, the fast sampling
algorithm for DiffWave does not need to modify the training procedure (Algorithm 1), and can just
reuse the trained model checkpoint with large T.
15

Published as a conference paper at ICLR 2021
C
DETAILS OF THE MODEL ARCHITECTURE
(", $, %)
(", $, %)
(", 1, %)
Input
Conditioner
Residual layer ( = 0
Residual layer ( = 1
Residual layer ( = + −1
⋯
Skip connections
Output
⋯
FC
FC
FC
Conv1×1
Conv1×1
Conv1×1
Conv1×1
Conv1×1
Bi-DilConv-20 123 4
↔
+
+
+
+
⋅
tanh
<
Identity map
Conv1×1
ReLU
swish
ReLU
swish
(", 1, %)
(", $, %)
(", $, %)
(", 2$, %)
(", $, %)
(", $, %)
(", $, %)
(", 128)
(", 512)
(", 512)
(", $)
(", $, %)
Diffusion-step 
embedding
Figure 3: The network architecture of DiffWave in modeling ϵθ(xt, t), including tensor shapes at
each stage and activation functions. B is the batch size, C is the number of residual/skip channels of
the network, and L is data dimension.
1-st Bi-DilConv
input
!-th Bi-DilConv
⋯
⋯
receptive fields
#-dimensional data
⋯
Figure 4: The Receptive ﬁelds of the output units within DiffWave network.
16

Published as a conference paper at ICLR 2021
D
DETAILS OF AUTOMATIC EVALUATION METRICS IN SECTION 5.2 AND 5.3
The automatic evaluation metrics used in Section 5.2 and 5.3 are described as follows. Given an input
audio x, an 1024-dimensional feature vector (denoted as Ffeature(x)) is computed by the ResNeXT
F, and is then transformed to the 10-dimensional multinomial distribution (denoted as pF(x)) with
a fully connected layer and a softmax layer. Let Xtrain be the trainset, pgen be the distribution of
generated data, and Xgen ∼pgen(i.i.d.) be the set of generated audio samples. Then, we compute
the following automatic evaluation metrics:
• Fréchet Inception Distance (FID) (Heusel et al., 2017) computes the Wasserstein-2 dis-
tance between Gaussians ﬁtted to Ffeature(Xtrain) and Ffeature(Xgen). That is,
FID = ∥µg −µt∥2 + Tr

Σt + Σg −2(ΣtΣg)
1
2

,
where µt, Σt are the mean vector and covariance matrix of Ffeature(Xtrain), and where
µg, Σg are the mean vector and covariance matrix of Ffeature(Xgen).
• Inception Score (IS) (Salimans et al., 2016) computes the following:
IS = exp
 Ex∼pgenKL
 pF(x)∥Ex′∼pgenpF(x′)

,
where Ex′∼pgenpF(x′) is the marginal label distribution.
• Modiﬁed Inception Score (mIS) (Gurumurthy et al., 2017) computes the following:
mIS = exp
 Ex,x′∼pgenKL (pF(x)∥pF(x′))

.
• AM Score (Zhou et al., 2017) computes the following:
AM = KL
 Ex′∼qdatapF(x′)∥Ex∼pgenpF(x)

+ Ex∼pgenH(pF(x)),
where H(·) computes the entropy. Compared to IS, AM score takes into consideration the
the prior distribution of pF(Xtrain).
• Number of Statistically-Different Bins (NDB) (Richardson & Weiss, 2018): First, Xtrain
is clustered into K bins by K-Means in the feature space (where K = 50 in our evaluation).
Next, each sample in Xgen is assigned to its nearest bin. Then, NDB is the number of
bins that contain statistically different proportion of samples between training samples and
generated samples.
17

