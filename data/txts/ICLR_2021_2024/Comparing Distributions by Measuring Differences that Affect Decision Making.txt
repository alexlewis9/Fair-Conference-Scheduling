Published as a conference paper at ICLR 2022
COMPARING DISTRIBUTIONS BY MEASURING DIF-
FERENCES THAT AFFECT DECISION MAKING
Shengjia Zhao∗, Abhishek Sinha∗, Yutong He∗, Aidan Perreault, Jiaming Song, Stefano Ermon
Department of Computer Science
Stanford University
{sjzhao,a7b23,kellyyhe,aperr,tsong,ermon}@stanford.edu
ABSTRACT
Measuring the discrepancy between two probability distributions is a fundamental
problem in machine learning and statistics. We propose a new class of discrepan-
cies based on the optimal loss for a decision task – two distributions are different
if the optimal decision loss is higher on their mixture than on each individual
distribution. By suitably choosing the decision task, this generalizes the Jensen-
Shannon divergence and the maximum mean discrepancy family. We apply our
approach to two-sample tests, and on various benchmarks, we achieve superior
test power compared to competing methods. In addition, a modeler can directly
specify their preferences when comparing distributions through the decision loss.
We apply this property to understanding the effects of climate change on different
economic activities and selecting features targeting different decision tasks.
1
INTRODUCTION
Quantifying the difference between two probability distributions is a fundamental problem in ma-
chine learning. Modelers choose different types of discrepancies (or probability divergences) to
encode their prior knowledge about which aspects are relevant to evaluate the difference. Inte-
gral probability metrics (IPMs, M¨uller (1997)) and f-divergences (Csisz´ar, 1964) are widely used
discrepancies in machine learning. IPMs, such as the Wasserstein distance, maximum mean discrep-
ancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012), are based on the idea that if two
distributions are identical, any function should have the same expectation under both distributions.
IPMs are used to deﬁne training objectives for generative models (Arjovsky et al., 2017), perform
independence tests (Doran et al., 2014), robust optimization (Esfahani & Kuhn, 2018) among many
other applications. f-divergences, such as the KL divergence and the Jensen Shannon divergence,
are based on the idea that if two distributions are identical, they assign the same likelihood to every
point. One can then deﬁne a discrepancy based on how different the likelihood ratio is from one.
KL divergence underlies some of the most commonly used training objectives for both supervised
and unsupervised machine learning algorithms, such as cross entropy loss.
We propose a third category of divergences called H-divergences that overlaps with but also extends
the set of integral probability metrics or the set f-divergences. Intuitively, H-divergence compares
two distributions in terms of the optimal loss for a certain decision task. This optimal loss corre-
sponds to a generalized notion of entropy (DeGroot et al., 1962). Instead of measuring the best
average code length of any encoding scheme (Shannon entropy), the generalized entropy uses arbi-
trary loss function (rather than code length) and set of actions (rather than encoding schemes), and
is deﬁned as the best expected loss among the set of actions. In particular, given two distribution p
and q, we compare the generalized entropy of the mixture distribution (p+q)/2 and the generalized
entropy of p and q individually. Intuitively, if p and q are different, it is more difﬁcult to minimize
expected loss under the mixture distribution (p + q)/2, and hence the mixture distribution should
have higher generalized entropy; if p and q are identical, then the mixture distribution is identical to
p or q, and hence should have the same generalized entropy.
Our divergence strictly generalizes the maximum mean discrepancy family and the Jensen Shannon
divergence, which can be obtained with speciﬁc choices of the loss function. We illustrate this via
∗Co-ﬁrst author
1

Published as a conference paper at ICLR 2022
Figure 1: Relationship between H-divergence (this paper) and existing divergences. The Jensen
Shannon divergence is an f-divergence but not an IPM; the MMD is an IPM but not always an
f-divergence; both are H-divergences. There are H-divergences that are not f-divergences or IPMs.
the Venn diagram in Figure 1. Our formulation allows us to choose alternative losses to leverage
inductive biases and machine learning models from different problem domains. For example, if we
choose the generalized entropy as the maximum log likelihood of deep generative models, we are
able to leverage recent progress in modeling high dimensional images.
We demonstrate the effectiveness of H-divergence in two sample tests, i.e. to decide whether two
sets of samples come from the same distribution or not. A test based on a probability discrepancy
declares two sets of samples different if their discrepancy exceeds some threshold. We use H-
divergences based on generalized entropy deﬁned by the log likelihood of off-the-shelf generative
models. Compared to state-of-the-art tests based on MMD with deep kernels (Liu et al., 2020), tests
based on the H-divergence achieve better test power (given identical type I error) on a large set of
benchmarks.
More importantly, scientists and policy makers are often interested not only in if two distributions are
different, but how two distributions are different and whether the differences affect decision making.
Typical divergence measures (such as KL) or two sample tests only quantify if two distributions are
different, while we show that H-divergence is a useful tool for quantifying how distributions are
different with three application examples: studying the effect of climate change, feature selection,
and sample quality evaluation. In each of these examples, we compare different aspects of the
distributions by choosing speciﬁc decision loss functions. For example, climate change (Figure 3)
might impact agriculture in a region but not energy production, or vice versa. By choosing suitable
loss functions (related to agriculture, energy, etc) we can quantify and test if the change in climate
distribution impact different economic activities.
2
BACKGROUND
2.1
PROBABILITY DIVERGENCES
Let X denote a ﬁnite set or a ﬁnite dimensional vector space, and P(X) denote the set of probability
distributions on X that have a density. We consider the problem of deﬁning a probability divergence
between any two distributions in P(X), where a probability divergence is any function D : P(X) ×
P(X) →R that satisﬁes D(p∥q) ≥0, D(p∥p) = 0, ∀p, q ∈P(X). We call the divergence D
“strict” if D(p∥q) > 0 ∀p ̸= q, and “non-strict“ otherwise. In this paper we consider both types of
divergences.
Integral Probability Metrics
Let F denote a set of functions X →R. An integral probability
metric is deﬁned as IPMF(p∥q) = supf∈F |Ep[f(X)] −Eq[f(X)]|. Several important divergences
belong to integral probability metrics. Examples include the Wasserstein distance, where F is the set
of 1-Lipschitz functions; the total variation distance, where F is the set of functions X →[−1, 1].
The maximum mean discrepancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012)
chooses a kernel function k : X × X →R+ and is deﬁned by
MMD2(p∥q) = Ep,pk(X, Y ) + Eq,qk(X, Y ) −2Ep,qk(X, Y )
MMD is an IPM where F is the unit norm functions in the reproducing kernel Hilbert space (RKHS)
associated with the kernel k.
2

Published as a conference paper at ICLR 2022
f-Divergences
Given any convex continuous function f : R+ →R such that f(1) = 0, the
f-Divergence is deﬁned as (assuming densities exist) Df(p∥q) = Eq[f(p(X)/q(X))]. Examples
include the KL divergence, where f : t 7→t log t and the Jensen Shannon divergence, where f :
t 7→(t + 1) log

2
t+1

+ t log t.
2.2
H-ENTROPY
For any action space A and any loss function ℓ: X × A →R, the H-entropy (DeGroot et al., 1962;
DeGroot, 2005; Gr¨unwald et al., 2004) is deﬁned as
Hℓ(p) = inf
a∈A Ep[ℓ(X, a)]
In words, H-entropy is the Bayes optimal loss of a decision maker who must select some action a
not for a particular x, but in expectation for a random x drawn from p(x). H-entropy generalizes
several important notions of uncertainty. Examples include: Shannon Entropy, where A as the set of
probabilities P(X), and ℓ(x, a) = −log a(x); Variance where A = X, and ℓ(x, a) = ∥x−a∥2
2; Pre-
dictive V-entropy, where A ⊂P(X) is some subset of distributions, and ℓ(x, a) = −log a(x) (Xu
et al., 2020).
A key property we will use is that H-entropy is concave (DeGroot et al., 1962).
Lemma 1. For any choice of ℓ: X × A →R, Hℓis a concave function.
This Lemma can be proved by observing that inf is a concave function: it is always better to pick an
optimal action for p and q separately rather than a single one for both.
Hℓ
 αp + (1 −α)q

= inf
a (αEp[ℓ(X, a)] + (1 −α)Eq[ℓ(X, a)])
≥α inf
a Ep[ℓ(X, a)] + (1 −α) inf
a Eq[ℓ(X, a)] = αHℓ(p) + (1 −α)Hℓ(q)
This Lemma reﬂects why Hℓcan be thought of as a measurement of entropy or uncertainty. If the
distribution is more uncertain (e.g. a mixture of p and q, rather than p or q separately) then decisions
made under higher uncertainty will suffer a higher loss.
3
DEFINITION AND THEORETICAL PROPERTIES
3.1
H-JENSEN SHANNON DIVERGENCE
As a warm up, we present a special case of our divergence.
Deﬁnition 1 (H-Jensen Shannon divergence).
DJS
ℓ(p, q) = Hℓ
p + q
2

−1
2
 Hℓ(p) + Hℓ(q)

(1)
DJS
ℓ
is always non-negative because H-entropy is concave (Lemma 1), and clearly DJS
ℓ(p, q) = 0
whenever p = q. Therefore, DJS
ℓ
is a valid probability divergence. In particular, if we choose Hℓ
as the Shannon entropy, Deﬁnition 1 recovers the Jensen Shannon divergence. Other special loss
function choices can recover deﬁnitions in (Burbea & Rao, 1982).
3.2
GENERAL H-DIVERGENCE
In addition to the H-Jensen Shannon divergence, there are other functions based on the H-entropy
that satisfy the requirements of a divergence. For example,
DMin
ℓ
= Hℓ
p + q
2

−min(Hℓ(p), Hℓ(q))
(2)
is also a valid divergence (this will be proved later as a special case of Lemma 2). We can deﬁne a
general set of divergences that includes the above two divergences with the following deﬁnition:
3

Published as a conference paper at ICLR 2022
Deﬁnition 2 (H-divergence). For two distributions p, q on X, given any continuous function φ :
R2 →R such that φ(θ, λ) > 0 whenever θ + λ > 0 and φ(0, 0) = 0, deﬁne
Dφ
ℓ(p∥q) = φ

Hℓ
p + q
2

−Hℓ(p), Hℓ
p + q
2

−Hℓ(q)

Intuitively Hℓ
  p+q
2

−Hℓ(p) and Hℓ
  p+q
2

−Hℓ(q) measure how much more difﬁcult it is to
minimize loss on the mixture distribution (p+q)/2 than on p and q respectively. φ is a general class
of functions that map these differences into a scalar divergence, while satisfying some desirable
properties described in the next section.
The following proposition shows that the H-divergence generalizes the previous deﬁnitions (1) and
(2). Therefore, any property of H-divergence is inherited by e.g. the H-Jensen Shannon divergence.
Proposition 1. If φ(θ, λ) = θ+λ
2
then Dφ
ℓ(p, q) is the H-Jensen Shannon divergence in Eq.(1). If
φ(θ, λ) = max(θ, λ) then Dφ
ℓ(p, q) is the H-Min divergence in Eq.(2).
3.3
PROPERTIES OF THE H-DIVERGENCE
We ﬁrst verify that Dφ
ℓis indeed a (strict or non-strict) probability divergence.
Lemma 2. For any choice of ℓand for any choice of φ that satisfy Deﬁnition 2, Dφ
ℓis non-negative
and Dφ
ℓ(p, q) = 0 whenever p = q. Furthermore, Dφ
ℓis symmetric whenever φ is symmetric.
Depending on the choice of ℓ, H-divergence may or may not be strict (i.e. whenever p ̸= q,
D(p∥q) > 0). The following proposition characterizes conditions for a strict divergence.
Proposition 2 (Strict Divergence). For any choice of φ the following are equivalent 1) ∀p ̸= q,
Dℓ(p∥q) > 0. 2) The H-entropy Hℓ(p) := infa Ep[ℓ(X, a)] is strictly convex in p. 3) ∀p ̸= q,
arg infa Ep[ℓ(X, a)] ∩arg infa Eq[ℓ(X, a)] = ∅.
In particular, this proposition can be used to characterize all strict H-divergences, because the set of
all losses ℓthat induces strict H-entropy functions Hℓcan be characterized by Fenchel duality (Duchi
et al., 2018).
One important property of the H-divergence is that two distributions have non-zero divergence if and
only if they have different optimal actions, i.e. the optimal solutions for their respective H-entropy
are different. This is shown in the following proposition (proof in Appendix A).
Proposition 3. Dφ
ℓ(p∥q) > 0 if and only if arg infa Ep[ℓ(X, a)] ∩arg infa Eq[ℓ(X, a)] = ∅.
Intuitively, Dφ
ℓonly takes into account differences between distributions that lead to different opti-
mal action choices. This property allows us to incorporate prior domain knowledge. By choosing A
and ℓwe can specify which differences between distributions lead to different optimal actions, and
which differences do not. For example, we can choose A as a set of generative models (e.g., mixture
of Gaussians) and ℓ(x, a) as the negative log likelihood of x under generative model a. If under
two distributions we end up learning the same generative model (by maximizing log likelihood), the
H-divergence between them is zero.
3.4
RELATIONSHIP TO MMD
An important special case of the H-divergence is the set of squared Maximum Mean Discrepencies
(MMD), as shown by the following theorem:
Theorem 1. The set of H-Jensen Shannon Divergences is strictly larger than the MMD2 distances.
To prove this theorem, we show that for each choice of kernel k : X × X →R, there exists an
action space A and loss ℓsuch that the corresponding squared MMD distance and H-divergence are
the same (see proof in Appendix A). In particular, this equivalence can be achieved by choosing A
to be the RKHS H of k(·, ·), and ℓ(x, a) = 4∥k(x, ·) −a∥2
H. Inclusion is strict because the Jensen
Shannon divergence is a H-Jensen Shannon Divergence but not a squared MMD distance.
4

Published as a conference paper at ICLR 2022
3.5
ESTIMATION AND CONVERGENCE
Many machine learning tasks can be reduced to the problem of estimating the divergence between
two distributions given samples. Speciﬁcally, suppose we are provided with a set of m i.i.d. samples
ˆpm = (x1, · · · , xm) drawn from distribution p and ˆqm = (x′
1, · · · , x′
m) drawn from distribution q,
and would like to obtain an estimate of Dφ
ℓ(p∥q) based on the samples. Here, ˆpm and ˆqm denote
empirical distributions drawn from p and q respectively. In this section we propose an empirical
estimator for the H-divergence and show that it has favorable convergence properties.
Let ˆDφ
ℓ(ˆpm∥ˆqm) be the empirical (random) estimator for Dφ
ℓ(p∥q) deﬁned by
ˆDφ
ℓ(ˆpm∥ˆqm) =φ
 
inf
a
1
m
m
X
i=1
ℓ(x′′
i , a) −inf
a
1
m
m
X
i=1
ℓ(xi, a), inf
a
1
m
m
X
i=1
ℓ(x′′
i , a) −inf
a
1
m
m
X
i=1
ℓ(x′
i, a)
!
where x′′
i = xibi + x′
i(1 −bi) and bi are i.i.d uniformly sampled from {0, 1}, so that x′′
i is a sample
from the mixture distribution (p + q)/2 of size m.
Using x′′
i as deﬁned above is crucial for the convergence properties we will prove in Theorem 2. It
might be tempting to replace the term
1
m
Pm
i=1 ℓ(x′′
i , a) with
1
2m
Pm
i=1(ℓ(xi, a) + ℓ(x′
i, a)) to use
all the available samples. However, optimizing the action based on a ﬁnite set of samples (instead of
in expectation) is prone to overﬁtting, and introduces bias. Intuitively, using m samples (x′′
i ) ensures
the bias for the mixture is comparable to that of p and q. Without this, Theorem 2 is no longer true,
and empirical performance also degrades.
Before presenting the convergence results, we ﬁrst must deﬁne several assumptions that make con-
vergence possible. In particular, we are going to assume that the loss function ℓis C-bounded, i.e.
there exists some C such that 0 ≤ℓ(x, a) ≤C, ∀a, x. This assumption seemingly exclude important
special cases such as the Jensen-Shannon divergence (which is associated with the unbounded log
loss). However, we show in the appendix that the Jensen-Shannon divergence cannot be consistently
estimated in general, hence correctly excluded by our theorem. One practical solution is to clip the
log likelihood, which is the approach adopted in (Song & Ermon, 2019) for improved divergence
estimation (for a similar KL divergence estimation problem).
In addition, we assume that φ is 1-Lipschitz under the ∞-norm, i.e. |φ(θ +dθ, λ+dλ)−φ(θ, λ)| ≤
max(dθ, dλ), ∀θ, λ, dθ, dλ ∈R . Both φ(θ, λ) = θ+λ
2
and φ(θ, λ) = max{θ + λ} are 1-Lipschitz
under the ∞-norm. This is a mild assumption because if φ is not 1-Lipschitz we can rescale φ to
make it 1-Lipschitz. Finally, deﬁne the Radamacher complexity
Rp
m(ℓ) = EXi∼p,ϵi∼Uniform({−1,1})
"
sup
a∈A
1
m
m
X
i=1
ϵiℓ(Xi, a)
#
We deﬁne Rq
m(ℓ) analogously. Based on these assumptions and deﬁnitions we can bound the dif-
ference between ˆDφ
ℓ(ˆpm∥ˆqm) and Dφ
ℓ(p∥q).
Theorem 2. If ℓis C-bounded, and φ is 1-Lipschitz under the ∞-norm, for any choice of distribution
p, q ∈P(X) and t > 0 we have
1. Pr[ ˆDφ
ℓ(ˆpm∥ˆqm) ≥t] ≤4e−t2m
2C2 if p = q.
2. Pr
h ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
 ≥4 max(Rp
m(ℓ), Rq
m(ℓ)) + t
i
≤4e−t2m
2C2
Corollary 1.
q
Var[ ˆDφ
ℓ(ˆpm∥ˆqm)] ≤4 max(Rp
m(ℓ), Rq
m(ℓ)) +
p
2C2/m
For proof see Appendix A. Note that when p = q, the convergence of ˆDφ
ℓ(ˆpm∥ˆqm) does not depend
on the Radamacher complexity of ℓ, and converges to 0 very quickly. When p ̸= q the estimator
ˆDφ
ℓ(ˆpm∥ˆqm) is still consistent (under regularity assumptions)
Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a ﬁnite
set 2. A is a bounded subset of Rd for some d ∈N and ℓis Lipschitz w.r.t. a, then almost surely
limm→∞ˆDφ
ℓ(ˆpm∥ˆqm) = Dφ
ℓ(p∥q).
5

Published as a conference paper at ICLR 2022
For both cases in Corollary 2 the Radamacher complexity Rp
m(ℓ) goes to zero (as sample size
m →∞) at a rate of O(1/√m). In other words we can conclude that the estimation error in
Theorem 2 is bounded by O(1/√m) and the variance of the estimator is also bounded by O(1/√m)
when the sample size m →∞.
4
EXPERIMENT: TWO SAMPLE TEST
The ﬁrst application is to design more powerful two sample tests. We aim to show that H-divergence
allow us to leverage inductive biases for each data type (e.g. image, bio, text) by choosing suitable
actions A and loss ℓ, which leads to improved test power. 1
4.1
TWO SAMPLE TEST
For the task of two sample test, we would like to decide if two sets of samples are drawn from
the same distribution or not. Speciﬁcally, given two sets of samples ˆpm := (x1, · · · , xm)
i.i.d.
∼p
and ˆqm := (x′
1, · · · , x′
m)
i.i.d.
∼q we would like to decide if p = q. Typical approaches estimate a
divergence ˆD(ˆpm∥ˆqm) and output p ̸= q if the divergence exceeds some threshold.
There are two types of errors: Type I error happens when the algorithm incorrectly outputs p ̸= q;
the probability of type I errors is called the signiﬁcance level. Type II error happens when the
algorithm incorrectly outputs p = q; the probability of not making a Type II error is called the
test power (higher is better). Note that both the signiﬁcance level and the test power are relative to
distributions p and q.
We follow the typical setup where we guarantee a certain signiﬁcance level while empirically mea-
suring the test power. In particular, the signiﬁcance level can be guaranteed with a permutation
test (Ernst et al., 2004). In a permutation test, in addition to the original set of samples ˆpm and ˆqm,
we also uniformly randomly swap elements between ˆpm and ˆqm, and sample multiple randomly
swapped datasets (ˆp1
m, ˆq1
m), (ˆp2
m, ˆq2
m), · · · . The testing algorithm outputs p ̸= q if ˆD(ˆpm∥ˆqm) is
in the top α-quantile among { ˆD(ˆp1
m∥ˆq1
m), ˆD(ˆp2
m∥ˆq2
m), · · · }. Permutation test guarantees the sig-
niﬁcance level (i.e. low Type I error) because if p = q then swapping elements between ˆpm and
ˆqm should not change its distribution, so each pair (ˆpm, ˆqm), (ˆp1
m, ˆq1
m), · · · should have the same
distribution. Therefore, ˆD(ˆpm∥ˆqm) happens to be in the top α-quantile with at most α probability.
Note that the signiﬁcance level guarantee does not rely on accurate estimation of H-divergence in
Theorem 2 (accurate H-divergence estimation is still important because the test power does depends
on it).
When the choice of Dℓis not a strict divergence (See Proposition 2) we may falsely conclude p = q
(D(p∥q) = 0) when in reality p ̸= q. This is true but inconsequential in ﬁnite data scenarios.
With ﬁnite data, it is generally impossible to guarantee the test power (i.e. bounding the probability
of concluding p = q when in reality p ̸= q for any p, q) and prior literature do not provide such
guarantees. Hence our guarantee is no weaker than prior two sample test literature.
4.2
EXPERIMENT SETUP
Baselines
We compare our proposed approach with six other divergences.
All methods are
based on the permutation test explained in Section 4.1.
MMD-D (Liu et al., 2020) measures
the MMD distance with a deep kernel, while MMD-O (Gretton et al., 2012) measures the MMD
distance with a Gaussian kernel. Mean embedding (ME) and smoothed characteristic functions
(SCF) (Chwialkowski et al., 2015; Jitkrittum et al., 2016) are distances based on the difference in
Gaussian kernel mean embedding at a set of optimized points, or a set of optimized frequencies.
C2STS-S & C2ST-L (Lopez-Paz & Oquab, 2017; Cheng & Cloninger, 2019) use a classiﬁer’s accu-
racy distinguishing between the two distributions.
Comparison Metrics and Setup
All methods have the same signiﬁcance level (which is provably
equal to α = 0.05 because of the permutation test), therefore we only consider the test power. We
follow Liu et al. (2020) and consider four datasets: Blob (Liu et al., 2020), HDGM (Liu et al.,
1The code to reproduce our experiments can be found here.
6

Published as a conference paper at ICLR 2022
2020), HIGGS (Adam-Bourdarios et al., 2014) and MNIST (LeCun & Cortes, 2010). Our method
and all the baseline methods have hyper-parameters. To ensure fair comparison, we follow the same
evaluation setup as (Liu et al., 2020) for all methods. We split each dataset into two equal partitions:
a training set to tune hyper-parameters, and a validation set to compute the ﬁnal test output.
Implementation Details
We choose φ(θ, λ) =
  θs+λs
2
1/s for s > 1 (which includes the H-
Jensen Shannon divergence when s = 1 and the H-Min divergence when s = ∞). We deﬁne l(x, a)
as the negative log likelihood of x under distribution a, where a is in a certain model family A. We
experiment with mixture of Gaussian distributions, Parzen density estimtor and Variational Autoen-
coder (Kingma & Welling, 2013). Our hyper-parameters consist of the best parameter s and also the
best generative model family. Choosing these hyper-parameters might seem cumbersome, but com-
pared to the second best baseline (MMD-D which chooses thousands of deep kernel parameters),
we have much fewer hyper-parameters.
We use α = 0.05 in all two-sample test experiments. Each permutation test uses 100 permutations,
and we run each test 100 times to compute the test power (i.e. the percent of times it correctly
outputs p ̸= q). Finally we plot and report the performance standard deviation by repeating the
entire experiment 10 times.
4.3
EXPERIMENT RESULTS
The average test powers are reported in Figure 4, Figure 2, Table 1 and Table 3. Our approach
achieves superior test power across the board. Notably on Higgs we achieve the same test power
with 2x fewer samples than the second best test, and on MNIST we can achieve perfect test power
even on the smallest sample size evaluated in (Liu et al., 2020).
Following (Liu et al., 2020) we also evaluate the test power as the dimension of the problem increases
(Figure 2). Our test power decreases gracefully as the dimension of the problem increases. We
hypothesize that the test power improvements come from leveraging progress in generative model
research: for each type of data (e.g. bio, image, text) there has been decades of research ﬁnding
suitable generative models; we use commonly used generative models (in modern literature) for
each data type (e.g. KDE for low dimensional physics/bio data, VAE for simple images).
Figure 2: Average test power on HDGM dataset. Left: results with the same sample size (4000)
and different data dimensions. Right: results with the same sample dimension (10) and different
sample sizes. Our method (H-Div, dashed line) achieve better test power for almost every setup.
All tests have high test power for low data dimensions, but our method scales better for higher data
dimensions.
N
ME
SCF
C2ST-S
C2ST-L
MMD-O
MMD-D
H-Div
1000
0.120±0.007
0.095±0.007
0.082±0.015
0.097±0.014
0.132±0.005
0.113±0.013
0.240±0.020
2000
0.165±0.019
0.130±0.019
0.183±0.026
0.232±0.032
0.291±0.017
0.304±0.012
0.380±0.040
3000
0.197±0.012
0.142±0.025
0.257±0.049
0.399±0.058
0.376±0.022
0.403±0.050
0.685±0.015
5000
0.410±0.041
0.261±0.044
0.592±0.037
0.447±0.045
0.659±0.018
0.699±0.047
0.930±0.010
8000
0.691±0.067
0.467±0.038
0.892±0.029
0.878±0.020
0.923±0.013
0.952±0.024
1.000±0.000
10000
0.786±0.041
0.603±0.066
0.974±0.007
0.985±0.005
1.000±0.000
1.000±0.000
1.000±0.000
Avg.
0.395
0.283
0.497
0.506
0.564
0.579
0.847
Table 1: Average test power ± standard error for N samples over the HIGGS dataset. The results
on MNIST is similar and presented in Table 3, Appendix B.1.
7

Published as a conference paper at ICLR 2022
Figure 3: Example plots of H-divergence across different geographical locations for losses ℓrelated
to agriculture (left) and energy production (right). Darker color indicates larger H-divergence. Com-
pared to divergences such as KL, H-divergence measures changes relevant to different social and
economic activities (by selecting appropriate loss functions ℓ). For example, even though climate
change signiﬁcantly impact the high latitude or high altitude areas, this change has less relevance to
agriculture (because few agriculture activities are possible in these areas).
5
EXPERIMENT: DECISION DEPENDENT DISCREPANCY MEASUREMENT
5.1
ASSESSING CLIMATE CHANGE
As an illustrative example of how H-divergence can facilitate decision making, we use climate data
and study how climate change affects decision making through the lens of H-divergence. Scien-
tists and policy makers are often interested in how climate change disparately affect different geo-
graphical locations. Existing methods (Preston et al., 2011) focus on one aspect of climate change
(such as the expected economic loss (Burke et al., 2018)) using tailor-designed analysis, while H-
divergence provides a general tool for hypothesis testing and visualization for different aspects of
climate change. In our example, we choose suitable loss functions to quantitatively measure as-
pects of climate change that are relevant to decision making in agriculture and renewable energy
production.2
Setup
We use the NOAA database which contains daily weather from thousands of weather sta-
tions at different geographical locations. For each location, we summarize the weather sequence of
each year into a few summary statistics (average yearly temperature, humidity, wind speed and rainy
days). We are interested in assessing changes in weather over this period at each location, from the
perspective of agriculture and renewable energy activities. Further details of these experiments are
in Appendix C.2.
Example: Agriculture
It is known that climate changes affect crop suitability (Lobell et al.,
2008). Let A denote the set of possible crops to plant at each location (e.g. wheat/barley/rice),
and ℓ(x, a) denote the loss of planting crop a if the yearly weather is x. We estimate the function ℓ
by matching geographical locations in the FAO crop yield dataset (FAOSTAT et al., 2006) to weather
stations in the NOAA database, and learn a function to predict crop yield from weather data with
kernel ridge regression.
The H-divergence has a natural interpretation: a geographical location could either (1) plant the
same crop for the entire period 1981-2019 that is optimal for the local climate (i.e.
choose
a∗= arg mina∈A E(p+q)/2[ℓ(X, a)]); (2) plant the optimal crops for 1981-1999 and for 2000-2019
respectively. H divergence measures the additional loss of option (1) compared to option (2). In
other words, it is the excess loss of not adapting crop type to climate change. For each geographical
location we can compute the H-divergence DJS
ℓ
for the estimated ℓ(plotted in Figure 3 left).
Example: Energy production
Changes in weather also affect electricity generation, since cli-
mate change could affect the amount of wind/solar energy available. Let A denote the number
of wind/solar/fossil fuel power plants built, and ℓ(x, a) denote the loss (negative utility) when the
weather is x. We obtain the function ℓusing empirical formulas for energy production (Npower,
2012). The H-divergence for this loss function is shown in Figure 3 (right). Intuitively the H di-
2Designing loss functions ℓthat capture the effect of climate on human activities is a well studied topic in
economics, and beyond the scope of this work. Our results should be taken as an illustrative example of how
domain experts might use H-divergence with more realistic loss functions.
8

Published as a conference paper at ICLR 2022
Loss Selection
Selected Features
Neutral
education, cap-gain, sex, age, occupation
Upweight low income
education, cap-gain, relationship, marital-status, sex
Upweight high income
education, cap-gain, sex, age, race
Table 2: Features selected by different approaches. With H-Divergence we can select different
features that are important in different decision problems. For example, if we assign a high / low
penalty to making incorrect prediction for higher income groups, we select a different set of features.
vergence measures the excess loss of using the same energy generation infrastructure for the entire
time period vs. using different infrastructure that adapts to climate change. While this is only an
illustrative example, comparing the two maps we see that regions and industries are affected by
climate change in different ways – H divergence provides a quantitative framework for this kind of
assessments.
5.2
FEATURE SELECTION
In a feature selection task, we wish to know which input features are most predictive of the label.
Feature selection provides information on which features have the biggest inﬂuence on the label,
and can be used in scientiﬁc discovery (Jovi´c et al., 2015; Zhang et al., 2015).
Off-the-shelf feature selection algorithms often do not take into account problem speciﬁc require-
ments. For example, denote the input features as X1, · · · , XK and label as Y , the mutual in-
formation feature selection algorithms estimate the Shannon mutual information I(Xi, Y ) :=
KL(p(Xi, Y )∥p(Xi)p(Y )) and select features with largest mutual information. However, scien-
tists and policies makers often need ﬁne-grained control to answer their speciﬁc scientiﬁc or policy
questions. For example, social scientists might want to know which features are more important for
high-income as compared to low-income groups (e.g. to understand potential glass ceilings).
With H-Divergence we can select features with large Dφ
ℓ(p(Xi, Y )∥p(Xi)p(Y )) (i.e. the optimal
action is different under the joint p(Xi, Y ) and the product of marginals p(Xi)p(Y )). By choosing
different loss functions ℓwe can get different feature selection results, each reﬂecting important
features for that decision problem. For example, in Table 2 we show the features selected for the
UCI income prediction dataset (Blake, 1998). For this dataset, we choose A as the set of logistic
regression functions, l(x, a) as the cross entropy loss for regression function a on the sample x and
φ(θ, λ) = max(θ, λ). If we want to focus on high income groups, we can assign a higher weight to
the loss of high income samples, and vice versa. We observe that gender/race is more predictive of
income for high-income groups, while relationship or marital status is more predictive of income for
lower-income groups. This can help us identify potential inequality or suggest further investigation
into the cause of low income and poverty. For example, our results suggest a connection between
family and relationship status and poverty, and a connection between gender/race and high income.
These connections merit further investigation into the cause and policy remedy.
6
ACKNOWLEDGEMENTS
SE acknowledges support by NSF(#1651565, #1522054, #1733686), ONR (N000141912145),
AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship.
REFERENCES
Claire Adam-Bourdarios, Glen Cowan, C´ecile Germain, Isabelle Guyon, Bal´azs K´egl, and David
Rousseau. The higgs boson machine learning challenge. In Proceedings of the 2014 International
Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML’14, pp. 19–55.
JMLR.org, 2014.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214–223. PMLR, 2017.
Peter Bartlett.
Theoretical statistics, lecture 13.
https://www.stat.berkeley.edu/
˜bartlett/courses/2013spring-stat210b/notes/13notes.pdf, 2013.
9

Published as a conference paper at ICLR 2022
Catherine Blake.
Uci repository of machine learning databases.
http://www. ics. uci. edu/˜
mlearn/MLRepository. html, 1998.
Jacob Burbea and C Radhakrishna Rao. Entropy differential metric, distance and divergence mea-
sures in probability spaces: A uniﬁed approach. Journal of Multivariate Analysis, 12(4):575–596,
1982.
Jacob Burbea and C Radhakrishna Rao. Differential metrics in probability spaces. Probab. Math.
Stat, 3:241–258, 1984.
Marshall Burke, W Matthew Davis, and Noah S Diffenbaugh. Large potential reduction in economic
damages under un mitigation targets. Nature, 557(7706):549–553, 2018.
X. Cheng and A. Cloninger. Classiﬁcation logit two-sample testing by neural networks. ArXiv,
abs/1909.11298, 2019.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems
28, pp. 1981–1989. Curran Associates, Inc., 2015.
Imre Csisz´ar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der er-
godizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85–108, 1964.
Morris H DeGroot. Optimal statistical decisions, volume 82. John Wiley & Sons, 2005.
Morris H DeGroot et al.
Uncertainty, information, and sequential experiments.
The Annals of
Mathematical Statistics, 33(2):404–419, 1962.
Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Sch¨olkopf. A permutation-based kernel
conditional independence test. In UAI, pp. 132–141, 2014.
John Duchi, Khashayar Khosravi, Feng Ruan, et al. Multiclass classiﬁcation, information, diver-
gence and surrogate risk. Annals of Statistics, 46(6B):3246–3275, 2018.
Michael D Ernst et al. Permutation methods: a basis for exact inference. Statistical Science, 19(4):
676–685, 2004.
Peyman Mohajerin Esfahani and Daniel Kuhn.
Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115–166, 2018.
FAO FAOSTAT et al. Fao statistical databases. Rome: Food and Agriculture Organization of the
United Nations, 2006.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012.
Peter D Gr¨unwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. the Annals of Statistics, 32(4):1367–1433, 2004.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626–6637, 2017.
Wittawat Jitkrittum, Zolt´an Szab´o, Kacper P Chwialkowski, and Arthur Gretton. Interpretable dis-
tribution features with maximum testing power. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
181–189. Curran Associates, Inc., 2016.
10

Published as a conference paper at ICLR 2022
Alan Jovi´c, Karla Brki´c, and Nikola Bogunovi´c. A review of feature selection methods with appli-
cations. In 2015 38th international convention on information and communication technology,
electronics and microelectronics (MIPRO), pp. 1200–1205. Ieee, 2015.
Diederik P Kingma and Max Welling.
Auto-Encoding variational bayes.
arXiv preprint
arXiv:1312.6114v10, December 2013.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, A. Gretton, and D. Sutherland. Learning deep
kernels for non-parametric two-sample tests. ArXiv, abs/2002.09116, 2020.
David B Lobell, Marshall B Burke, Claudia Tebaldi, Michael D Mastrandrea, Walter P Falcon, and
Rosamond L Naylor. Prioritizing climate change adaptation needs for food security in 2030.
Science, 319(5863):607–610, 2008.
David Lopez-Paz and M. Oquab. Revisiting classiﬁer two-sample tests. arXiv: Machine Learning,
2017.
Tengyu Ma. Machine learning theory. https://github.com/tengyuma/cs229m_notes/
blob/main/Winter2021/pdf/02-08-2021.pdf, 2021.
Alfred M¨uller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, pp. 429–443, 1997.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Proceedings of the 30th International Conference
on Neural Information Processing Systems, pp. 271–279, 2016.
Npower. Wind turbine power calculations. Mechanical and Electrical Engineering Power Industry,
The Royal Academy of Engineering, 2012.
Benjamin L Preston, Emma J Yuen, and Richard M Westaway. Putting vulnerability to climate
change on the map: a review of approaches, beneﬁts, and risks. Sustainability science, 6(2):
177–202, 2011.
C Radhakrishna Rao.
Diversity and dissimilarity coefﬁcients: a uniﬁed approach.
Theoretical
population biology, 21(1):24–43, 1982.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch¨olkopf, and Gert RG
Lanckriet.
On integral probability metrics,\phi-divergences and binary classiﬁcation.
arXiv
preprint arXiv:0901.2698, 2009.
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.
Yudong Zhang, Zhengchao Dong, Preetha Phillips, Shuihua Wang, Genlin Ji, Jiquan Yang, and Ti-
Fei Yuan. Detection of subjects and brain regions related to alzheimer’s disease using 3d mri
scans based on eigenbrain and machine learning. Frontiers in computational neuroscience, 9:66,
2015.
11

Published as a conference paper at ICLR 2022
A
PROOFS
Lemma 2. For any choice of ℓand for any choice of φ that satisfy Deﬁnition 2, Dφ
ℓis non-negative and Dφ
ℓ(p, q) = 0
whenever p = q. Furthermore, Dφ
ℓis symmetric whenever φ is symmetric.
Proof of Lemma 2. For any choice of p, q by the concavity of the H-entropy in Lemma 1 we have
Hℓ
p + q
2

−Hℓ(p) ≥1
2(Hℓ(q) −Hℓ(p))
Hℓ
p + q
2

−Hℓ(q) ≥1
2(Hℓ(p) −Hℓ(q))
Therefore by summing the two inequalities we have

Hℓ
p + q
2

−Hℓ(p)

+

Hℓ
p + q
2

−Hℓ(q)

≥0
By the requirement on φ we know that Dφ
ℓ(p∥q) ≥0. In addition when p = q since (p + q)/2 = p = q we have
Dφ
ℓ(p∥q) = φ(0, 0) = 0.
To show it is symmetric, note that
Dφ
ℓ(p∥q) = φ

Hℓ
p + q
2

−Hℓ(p), Hℓ
p + q
2

−Hℓ(q)

= φ

Hℓ
p + q
2

−Hℓ(q), Hℓ
p + q
2

−Hℓ(p)

= Dφ
ℓ(q∥p)
whenever φ is symmetric.
Proposition 3. Dφ
ℓ(p∥q) > 0 if and only if arg infa Ep[ℓ(X, a)] ∩arg infa Eq[ℓ(X, a)] = ∅.
Proof of Proposition 3. Denote A∗
p = arg infa Ep[ℓ(X, a)] and A∗
q = arg infa Eq[ℓ(X, a)]. Also compute
Hℓ
p + q
2

= inf
a E p+q
2 [ℓ(X, a)] = inf
a
1
2Ep[ℓ(X, a)] + 1
2Eq[ℓ(X, a)]

(3)
If A∗
p ∩A∗
q = ∅, for any action a′ such that Ep[ℓ(X, a′)] = Hℓ(p), we must have a′ ∈A∗
p so a′ ̸∈A∗
q and Eq[ℓ(X, a′)] >
Hℓ(q). Similar if we choose a′′ such that Eq[ℓ(X, a′′)] = Hℓ(q) we have similarly have Ep[ℓ(X, a′′)] > Hℓ(p). In other
words, for any choice of action a ∈A either a /∈A∗
p and Ep[l(X, a)] > Hℓ(p) or a ∈A∗
p and Eq[l(X, a)] > Hℓ(q).
Therefore
inf
a
1
2Ep[ℓ(X, a)] + 1
2Eq[ℓ(X, a)]

> 1
2Hℓ(p) + 1
2Hℓ(q)
(4)
Combining Eq.(3) and Eq.(4) we have
1
2

Hℓ
p + q
2

−Hℓ(p)

+ 1
2

Hℓ
p + q
2

−Hℓ(q)

> 0
By Deﬁnition 2 this implies (for any choice of φ that satisﬁes the requirements in Deﬁnition 2) that Dφ
ℓ(p∥q) > 0.
To prove the converse simply obverse that if A∗
p ∩A∗
q ̸= φ, let a∗∈A∗
p ∩A∗
q we have a∗= arg infa∈A E p+q
2 [l(X, a)]. This
implies that
2Hℓ
p + q
2

−Hℓ(q) −Hℓ(p) = 2E p+q
2 [l(X, a∗)] −Eq[l(X, a∗)] −Ep[l(X, a∗)] = 0
By Deﬁnition 2 we can conclude that Dφ
ℓ(p∥q) = 0.
Theorem 1. The set of H-Jensen Shannon Divergences is strictly larger than the MMD2 distances.
12

Published as a conference paper at ICLR 2022
Proof of Theorem 1. Let k(x, y) be some kernel on an input space X, and let H be the RKHS induced by the kernel. The
(squared) MMD distance is deﬁned by
MMD2(p, q) = EX∼p,Y ∼pk(X, Y ) + EX∼q,Y ∼qk(X, Y ) −2EX∼p,Y ∼qk(X, Y )
which we write more compactly as MMD2(p, q) = Ep,pk(X, Y ) + Eq,qk(X, Y ) −2Ep,qk(X, Y ).
Deﬁne φ(x, y) = ∥k(x, ·) −k(y, ·)∥2
H. We can rewrite this in the following form:
MMD2(p, q) = Ep,qφ(X, Y ) −1
2Ep,pφ(X, Y ) −1
2Eq,qφ(X, Y )
(5)
= Ep,q∥k(X, ·)∥2
H + ∥k(Y, ·)∥2
H −2k(X, Y ) −1
2Ep,p∥k(X, ·)∥2
H + ∥k(Y, ·)∥2
H −2k(X, Y )
−1
2Eq,q∥k(X, ·)∥2
H + ∥k(Y, ·)∥2
H −2k(X, Y ) = Ep,pk(X, Y ) + Eq,qk(X, Y ) −2Ep,qk(X, Y )
We also observe an algebraic relationship for any function f(x, y) such that f(x, y) = f(y, x) for all x, y:
E p+q
2
, p+q
2 f(X, Y ) = 1
4Ep,pf(X, Y ) + 1
4Ep,qf(X, Y ) + 1
4Eq,pf(X, Y ) + 1
4Eq,qf(X, Y )
= 1
4Ep,pf(X, Y ) + 1
4Ep,qf(X, Y ) + 1
4Eq,pf(Y, X) + 1
4Eq,qf(X, Y )
= 1
4Ep,pf(X, Y ) + 1
4Eq,qf(X, Y ) + 1
2Ep,qf(X, Y )
(6)
Furthermore, we have that
Ep,p∥k(X, ·) −k(Y, ·)∥2
H = 2Ep∥k(X, ·) −Epk(Y, ·)∥2
H
(7)
Based on the above, noting that φ(x, y) = φ(y, x), we can derive
MMD2(p, q) = Ep,q∥k(X, ·) −k(Y, ·)∥2
H −1
2Ep,p∥k(X, ·) −k(Y, ·)∥2
H −1
2Eq,q∥k(X, ·) −k(Y, ·)∥2
H
Eq (5)
= 2E p+q
2
, p+q
2 ∥k(X, ·) −k(Y, ·)∥2
H −Ep,p∥k(X, ·) −k(Y, ·)∥2
H −Eq,q∥k(X, ·) −k(Y, ·)∥2
H
Eq (6)
= 4E p+q
2 ∥k(X, ·) −E p+q
2 k(Y, ·)∥2
H −2Ep∥k(X, ·) −Epk(Y, ·)∥2
H −2Eq∥k(X, ·) −Eqk(Y, ·)∥2
H
Eq (7)
= 4 inf
a∈H E p+q
2 ∥k(X, ·) −a∥2
H −2 inf
a∈H Ep∥k(X, ·) −a∥2
H −2 inf
a∈H Eq∥k(X, ·) −a∥2
H.
mean def.
Therefore we can deﬁne a loss ℓ: X × H →R where
ℓ(x, a) = 4∥k(x, ·) −a∥2
H
Under the new notation we have
MMD2(p, q) = inf
a∈H E p+q
2 l(X, a) −1
2

inf
a∈H Epl(X, a) + inf
a∈H Eql(X, a)

= Hℓ
p + q
2

−1
2(Hℓ(p) + Hℓ(q)) = DJS
ℓ(p∥q)
Conversely we want to show that not every H-Jensen Shannon divergence is a MMD. For example, take Hℓto be the Shan-
non entropy, then the corresponding DJS
ℓ
is the Jensen-Shannon divergence, which is not a MMD. This is because the JS
divergence is a type of f-divergence, and the only f-divergence that is also an IPM is total variation distance Sriperumbudur
et al. (2009). Therefore, the set of H-Jensen Shannon Divergences is strictly larger than the set of MMDs.
Theorem 2. If ℓis C-bounded, and φ is 1-Lipschitz under the ∞-norm, for any choice of distribution p, q ∈P(X) and t > 0
we have
1. Pr[ ˆDφ
ℓ(ˆpm∥ˆqm) ≥t] ≤4e−t2m
2C2 if p = q.
2. Pr
h ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
 ≥4 max(Rp
m(ℓ), Rq
m(ℓ)) + t
i
≤4e−t2m
2C2
13

Published as a conference paper at ICLR 2022
Proof of Theorem 2. Let ˆpm be a sequence of n samples (x1, · · · , xm) drawn from p, and ˆqm a sequence of n sam-
ples (x′
1, · · · , x′
m) drawn from q.
Let ˆrm the sub-sampling mixture (x′′
1, · · · , x′′
m) deﬁned in Section 3.5 (i.e.
x′′
i
=
xibi + x′
i(1 −bi) where bi is uniformly sampled from {0, 1}). We also overload the notation Hℓby deﬁning Hℓ(ˆpm) =
infa∈A 1
m
Pm
i=1 l(xi, a), and deﬁne Hℓ(ˆqm), Hℓ(ˆrm) similarly.
Before proving this theorem we need the following Lemmas
Lemma 3. Under the assumptions of Theorem 2
Pr [Hℓ(ˆpm) −E[Hℓ(ˆpm)] ≥t] ≤e−2t2m
C2
Pr [Hℓ(ˆpm) −E[Hℓ(ˆpm)] ≤−t] ≤e−2t2m
C2
Lemma 4. Under the assumptions of Theorem 2
Pr [|Hℓ(p) −Hℓ(ˆpm)| ≥2Rm(ℓ) + t] ≤e−2t2m
C2
To prove the ﬁrst statement of the Theorem, when p = q we can denote µ = E[Hℓ(ˆpm)] = E[Hℓ(ˆqm)] = E[Hℓ(ˆrm)], and
we have
Pr
h
ˆDφ
ℓ(ˆpm∥ˆqm) ≥t
i
= Pr[φ(Hℓ(ˆrm) −Hℓ(ˆpm), Hℓ(ˆrm) −Hℓ(ˆqm)) ≥t]
Def 2
≤Pr [max(Hℓ(ˆrm) −Hℓ(ˆpm), Hℓ(ˆrm) −Hℓ(ˆqm)) ≥t]
φ 1-Lipschitz
≤Pr [Hℓ(ˆrm) −Hℓ(ˆpm) ≥t] + Pr [Hℓ(ˆrm) −Hℓ(ˆqm) ≥t]
Union bound
≤Pr [Hℓ(ˆpm) −µ ≤−t/2] + 2 Pr [Hℓ(ˆrm) −µ ≥t/2] + Pr [Hℓ(ˆqm) −µ ≤−t/2]
Union bound
≤4e
−
t2
2C2/m
Lemma 3
The third inequality is because if Hℓ(ˆrm) −Hℓ(ˆpm) ≥t then it must be either Hℓ(ˆpm) −µ ≤−t/2 or Hℓ(ˆrm) −µ ≥t/2.
Similarly if Hℓ(ˆrm) −Hℓ(ˆqm) ≥t then it must be either Hℓ(ˆqm) −µ ≤−t/2 and Hℓ(ˆrm) −µ ≥t/2.
To prove the second statement of the Theorem, we observe that
| ˆDφ
ℓ(pm∥qm) −Dφ
ℓ(p∥q)|
=
φ (Hℓ(ˆrm) −Hℓ(ˆpm), Hℓ(ˆrm) −Hℓ(ˆqm)) −φ

Hℓ
p + q
2

−Hℓ(p), Hℓ
p + q
2

−Hℓ(q)

Def 2
≤max
Hℓ(ˆrm) −Hℓ(ˆpm) −Hℓ
p + q
2

+ Hℓ(p)
 ,
Hℓ(ˆrm) −Hℓ(ˆqm) −Hℓ
p + q
2

+ Hℓ(q)


φ 1-Lip
≤max
Hℓ(ˆrm) −Hℓ
p + q
2
 + |Hℓ(ˆpm) −Hℓ(p)| ,
Hℓ(ˆrm) −Hℓ
p + q
2
 + |Hℓ(ˆqm) −Hℓ(q)|

Jensen
Therefore, the event | ˆDφ
ℓ(pm∥qm) −Dφ
ℓ(p∥q)| ≥4 max(Rp
m(ℓ), Rq
m(ℓ)) + t happens only if at least one of the following
events happen
Hℓ(ˆrm) −Hℓ
p + q
2
 ≥Rp
m(ℓ) + Rq
m(ℓ) + t/2 ≥2R(p+q)/2
m
(ℓ) + t/2
R convex
|Hℓ(ˆpm) −Hℓ(p)| ≥2Rm(ℓ) + t/2
|Hℓ(ˆqm) −Hℓ(q)| ≥2Rm(ℓ) + t/2
Based on Lemma 4 each of these events only happen with probability at most e−t2m
2C2 . Therefore we can conclude by union
bound that
Pr[| ˆDφ
ℓ(p∥q) −Dφ
ℓ(p∥q)| ≥4 max(Rp
m(ℓ), Rq
m(ℓ)) + t] ≤4e−t2m
2C2
Finally we prove the two Lemmas used in the theorem. Lemma 4 is a standard result in the Radamacher complexity literature.
For a proof, see e.g. Section 26.1 in (Shalev-Shwartz & Ben-David, 2014). Lemma 3 can also be proved by standard
techniques. We provide the proof here.
14

Published as a conference paper at ICLR 2022
Proof of Lemma 3. Consider two sets of samples x1, · · · , xj, · · · , xm and x′
1, · · · , x′
j, · · · , x′
m where xi = x′
i for every
index i = 1, · · · , m except index j.
inf
a
1
m
X
i
ℓ(xi, a) −inf
a
1
m
X
i
ℓ(x′
i, a)
 ≤sup
a

1
m
X
i
ℓ(xi, a) −1
m
X
i
ℓ(x′
i, a)

= 1
m sup
a
ℓ(xj, a) −ℓ(x′
j, a)
 ≤C
m
Then we can conclude by Mcdiarmid inequality that
Pr
"
inf
a
1
m
X
i
ℓ(Xi, a) −E
"
inf
a
1
m
X
i
ℓ(Xi, a)
#
≥t
#
≤e
−
2t2
C2/m = e−2t2m
C2
Corollary 1.
q
Var[ ˆDφ
ℓ(ˆpm∥ˆqm)] ≤4 max(Rp
m(ℓ), Rq
m(ℓ)) +
p
2C2/m
Proof of Corollary 1. For notation convenience denote B = 4 max(Rp
m(ℓ), Rq
m(ℓ))
Var[ ˆDφ
ℓ(ˆpm∥ˆqm)]
= E

ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
2
=
Z ∞
t=0
Pr

ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
2
≥t

dt
=
Z ∞
t=0
Pr
h ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
 ≥
√
t
i
dt
=
Z ∞
s=0
Pr
h ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
 ≥s
i
2sds
s =
√
t
≤
Z B
s=0
2sds +
Z ∞
s=0
Pr
h ˆDφ
ℓ(ˆpm∥ˆqm) −Dφ
ℓ(p∥q)
 ≥B + s
i
2(B + s)ds
≤B2 +
Z ∞
s=0
2(B + s)e−s2m
2C2 ds
≤B2 +
Z ∞
t=0
2(B + t
r
2C2
m )e−t2
r
2C2
m dt
t = s
r m
2C2
= B2 + 2B
r
2C2
m
Z
e−t2dt + 4C2
m
Z
te−t2dt
= B2 + B
r
2πC2
m
+ 2C2
m
≤(B +
p
2C2/m)2
Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a ﬁnite set 2. A is a bounded
subset of Rd for some d ∈N and ℓis Lipschitz w.r.t. a, then almost surely limm→∞ˆDφ
ℓ(ˆpm∥ˆqm) = Dφ
ℓ(p∥q).
Proof of Corollary 2. We can prove the consistency results from Theorem 2 by observing that the expected Radamacher
complexity goes to 0 when m →∞.
The ﬁrst statement is a simple consequence of Massart’s Lemma, (e.g. see Eq.(8.44) in (Ma, 2021)). In particular, because A
is ﬁnite we have
Rp
m(ℓ) ≤
p
2 log |A|/m →m→∞0
15

Published as a conference paper at ICLR 2022
To prove the second statement, ﬁrst observe that because A is bounded, there must exist some r ∈R such that A ⊂Br :=
{a, ∥a∥2 ≤r}. In addition, without loss of generality we can assume that there exists L ∈R such that ∀x ∈X and a, a′ ∈A
|ℓ(x, a) −ℓ(x, a′)| ≤L∥a −a′∥2
There is no loss of generality because in ﬁnite dimensions all norms are equivalent, so if f is Lipschitz under any norm, then ℓ
is Lipschitz under the 2-norm. We can apply the results on Radamacher complexity for smoothly parameterized class proved
in (Bartlett, 2013), and conclude that limm→∞Rp
m(ℓ) = 0.
B
ADDITIONAL EXPERIMENTAL RESULTS
B.1
BLOB DATASET
Figure 4: Left: Average test power on the Blob dataset for different sample sizes and signiﬁcance
level α = 0.05. Our method (H-Div, dashed line) has signiﬁcantly better test power, especially for
setups with small sample sizes. Right: The same plot with signiﬁcance level α = 0.01.
B.2
EVALUATING SAMPLE QUALITY
Figure 5: The divergence between corrupted image and original image measured by H-divergence
vs. FID. For better comparison we normalize each distance to between [0, 1] by a linear transforma-
tion. For “speckle” and “impulse” corruption, both divergences are monotonically increasing with
more corruption. For “snow” corruption H-divergence is monotonic while FID is not. Other types
of corruptions are provided in Appendix B.2.
The gold standard for evaluating image generative models requires human decision, which is nevertheless expensive. Sev-
eral surrogate measurements are commonly used, such as the Frechet Inception Distance (FID) (Heusel et al., 2017) or the
inception score. Here by formulating such evaluation as an estimation of the discrepancy between the generated and the real
images, we can quantify the quality of image samples by calculating the corresponding H-Divergences.
We choose A as the set of Gaussian mixture distributions on the inception feature space, l(x, a) as the negative log likelihood
of x under distribution a and φ(θ, λ) = max(θ, λ). To evaluate the performance, we use the same setup as (Heusel et al.,
2017), where we add corruption from (Hendrycks & Dietterich, 2019) to a set of images. Intuitively, adding more corruption
degrades the sample quality, so a good measurement of sample quality should assign a lower quality score (higher divergence
from clean images). The results are plotted in Figure 5. The remaining plots of other perturbations are in Appendix B.2.
Both FID and H-divergence are generally monotonically increasing as we increase the amount of corruption. Our method is
slightly better on some perturbations (such as “snow”), where the FID fails to be monotonically increasing, but our method
is still monotonic, better aligning with human intuition.
16

Published as a conference paper at ICLR 2022
Figure 6: Additional plots that extend Figure 5.
C
ADDITIONAL THEORY AND EXPERIMENT DETAILS
C.1
CONNECTION TO OPTIMAL TRANSPORT
We ﬁrst show that H-divergence can also have a transportation interpretation. For all the intuitive interpretations we avoid
technical difﬁculty by assuming X is a ﬁnite set, even though all the formulas are applicable when X has inﬁnite cardinality.
17

Published as a conference paper at ICLR 2022
N
ME
SCF
C2ST-S
C2ST-L
MMD-O
MMD-D
H-Div
200
0.414±0.050
0.107±0.018
0.193±0.037
0.234±0.031
0.188±0.010
0.555±0.044
1.000±0.000
400
0.921±0.032
0.152±0.021
0.646±0.039
0.706±0.047
0.363±0.017
0.996±0.004
1.000±0.000
600
1.000±0.000
0.294±0.008
1.000±0.000
0.977±0.012
0.619±0.021
1.000±0.000
1.000±0.000
800
1.000±0.000
0.317±0.017
1.000±0.000
1.000±0.000
0.797±0.015
1.000±0.000
1.000±0.000
1000
1.000±0.000
0.346±0.019
1.000±0.000
1.000±0.000
0.894±0.016
1.000±0.000
1.000±0.000
Avg.
0.867
0.243
0.768
0.783
0.572
0.910
1.000
Table 3: Average test power ± standard error for N samples over the MNIST dataset.
Setup
Choose A = X, and let l(x, a) be a symmetric function (l(x, a) = l(a, x)) that denotes the cost of transporting a
unit of goods from x to a. When we say that a unit of goods is located according to p, we mean that there is 1 unit of goods
dispersed in X locations, where p(x) is the amount of goods at location x.
Optimal Transport Distance
The optimal transport distance is deﬁned by
Oℓ(p, q) =
inf
rXY ,rX=pX,rY =qY ErXY [l(X, Y )]
Intuitively the optimal transport distance measures the following cost: initially the goods are located according to p, we would
like to move them to locate according to q; O(p, q) denote the minimum cost to accomplish this transportation task.
H-Divergence as Optimal Storage
We ﬁrst consider the intuitive interpretation to the H-entropy
Hℓ(p) = inf
a Ep[ℓ(X, a)]
a∗= arg inf
a∈X Ep[ℓ(X, a)]
Suppose we want to move goods located according to p to a storage location (for example, we want to collect all the mail in a
city to a package center), then a∗is the optimal location to build the storage location, and H-entropy measures the minimum
cost to transport all goods to the storage location. Similarly 2Hℓ
  p+q
2

measures the minimum cost to transport both goods
located according to p and goods located according to q to the same storage location. The H-divergence
2DJS
ℓ(p∥q) := 2Hℓ
p + q
2

−(Hℓ(q) + Hℓ(p))
measures the reduction of transportation cost with two storage locations (one for p and one for q) rather than a single storage
location (for both p and q).
The H-Divergence is related to the optimal transport distance by the following inequality.
Proposition 4. If ℓsatisﬁes the triangle inequality ∀x, y, z ∈X, l(x, y) + l(y, z) ≥l(x, z) then DJS
ℓ(p∥q) ≤1
2O(p, q)
Proof of Proposition 4. Let a∗
q = arg inf Eq[l(X, a)] then we have
2Hℓ
p + q
2

= inf
a (Ep[ℓ(X, a)] + Eq[ℓ(X, a)]) ≤Ep[ℓ(X, a∗
q)] + Eq[ℓ(X, a∗
q)]
≤
inf
rXY ,rX=pX,rY =qY ErXY [ℓ(X, a∗
q)] + Eq[ℓ(X, a∗
q)]
≤
inf
rXY ,rX=pX,rY =qY ErXY [ℓ(X, Y ) + ℓ(Y, a∗
q)] + Eq[ℓ(X, a∗
q)]
= Oℓ(p, q) + 2Hℓ(q)
Intuitively, to move goods located according to p and goods located according to q to some storage location, one option is to
ﬁrst transport all goods from p to q (so that the goods at location x will be 2q(x)), then move the goods located according to
2q to the optimal storage location. Similarly we have
2Hℓ
p + q
2

≤O(q, p) + 2Hℓ(p)
which combined we have
2DJS
ℓ(p∥q) = 2Hℓ
p + q
2

−(Hℓ(q) + Hℓ(p)) ≤O(p, q)
18

Published as a conference paper at ICLR 2022
C.2
IMPOSSIBILITY OF JENSEN-SHANNON DIVERGENCE ESTIMATION
Suppose we have a consistent estimator for the Jenson-Shannon divergence, i.e. a function ˆJS such that for any pair of
distribution p, q and given N i.i.d. samples pN ∼p and qN ∼q we have limN→∞ˆJS(pN∥qN) = ˆJS(p∥q), then we prove a
contradiction by the probabilistic method.
Let p be a standard Gaussian distribution, let QM be a uniform distribution on a set of M i.i.d. samples from p (hence QM
is itself a random variable that depends on the i.i.d. samples). Let Q∗be the limit of QM when M →∞(i.e. it is the
uniform distribution on an inﬁnite set of samples). Let q∗denote a value that Q∗can take. Because q∗is always supported
on countably many points, hence JS(p∥q∗) = 1. Note that for any N the following two sampling process leads to identical
distribution on pN, qN:
pN ∼p, qN ∼p
Q∗∼p, pN ∼p, qN ∼Q∗
Hence, the expectation of any function (including ˆJS) is also identical.
EQ∗∼p
h
EpN∼p,qN∼Q∗[ ˆJS(pN, qN)]
i
= EpN∼p,qN∼p[ ˆJS(pN, qN)]
Hence the limit when N →∞must be identical
lim
N→∞EQ∗∼p
h
EpN∼p,qN∼Q∗[ ˆJS(pN, qN)]
i
= lim
N→∞EpN∼p,qN∼p[ ˆJS(pN, qN)]
Because the Jenson Shannon divergence is always bounded, any consistent estimator must also be bounded for sufﬁciently
large N. By the dominated convergence theorem we can exchange the expectation and the limit.
EQ∗∼p
h
lim
N→∞EpN∼p,qN∼Q∗[ ˆJS(pN, qN)]
i
= lim
N→∞EpN∼p,qN∼p[ ˆJS(pN, qN)]
By the probabilistic method (i.e. for any function f if EQ∗∼p[f(Q∗)] = 0 there must exist some q∗such that f(q∗) ≤0)
there must exist some q∗such that
lim
N→∞EpN∼p,qN∼q∗[ ˆJS(pN, qN)] ≤lim
N→∞EpN∼p,qN∼p[ ˆJS(pN, qN)] = 0 ̸= JS(p∥q∗) = 1
Therefore ˆJS cannot be consistent.
C.3
CLIMATE CHANGE EXPERIMENT DETAILS
Setup Details
In this experiment, we extract the statistics of yearly weather for each year from 1981-2019. We use the
NOAA dataset, which contains daily weather data from thousands of weather stations across the globe. For each year we
compute the following summary statistics: average yearly temperature, average yearly humidity, average yearly wind speed
and average number of rainy days in an year. For example x1990 is a 4 dimensional vector where each dimension correspond
to one of the summary statistics above.
Let p denote the uniform distribution over {x1981, · · · , x1999} and q denote the uniform distribution over {x2000, · · · , x2019}.
For example Ep[ℓ(X, a)] denote the expected loss of action a for a random year sampled from 1981-1999. Note that for many
decision problems, it is possible to make yearly decisions (e.g. decide the best crop to plant for each year). However, because
we want to measure the difference between two time periods 1981-1999 vs. 2000-2019, we choose the action space A to
be a single crop selection that will be used for the entire time period (rather than a different crop selection for each year).
Similarly for energy production we choose the action space A to be the proportion of different energy production methods
that will be used for the entire time period.
Crop yield
We obtain the crop yield loss function ℓ(x, a) with the following procedure
1. We obtain the crop yield dataset from (FAOSTAT et al., 2006), each entry we extract is the following tuple: (country
code, year, crop type, yield per hectare (kg/ha))
2. We associate each country code with the central coordinate (i.e. the average latitude and longitude) of the country. For
each central coordinate we ﬁnd the nearest weather station in the NOAA database. We use data for the nearest weather
station as the weather data for the country.
3. Based on step 2 for each (country code, year) pair we can associate a weather statistics (i.e. the 4 dimensional vector
described in Setup Details). We update each entry in step 1 to be (weather statistics, crop type, yield per hectare).
4. Based on the data entries we obtain in step 3 we train a Kernel Ridge regression model to learn the function ℓ(x, a)
where x is the weather statistics, a is the crop type, and ℓ(x, a) is learned to predict the yield (normalized by market
price) of the weather x for crop type a.
19

Published as a conference paper at ICLR 2022
Energy production
We consider three types of energy production methods: solar, wind and traditional (such as fossil
fuel). Solar energy and wind energy both depend heavily on weather, while traditional energy does not. In particular, we use
empirical formulas for solar and wind energy calculation:
solar ∝number of sunny days ∗daylight hour
wind ∝wind velocity3
D
DISCUSSIONS
Future Work
In this paper we explored the applications of H-divergence to two sample testing. Future work can explore
other applications of divergences. Potential applications include
• Generative model training. Many generative models learning algorithm minimize divergences (Nowozin et al., 2016;
Arjovsky et al., 2017), and future work can explore if the new divergence family leads to new generative model learning
algorithms.
• Independence tests. Independence tests are two sample tests between the joint distribution pXY and the product of
marginal distributions pXpY , therefore the two sample test results from this paper are applicable to independence tests.
• Robustness. Many robust optimization, estimation or prediction methods aim to achieve good performance even when
the data distribution is perturbed. Typically perturbation is measured by e.g. the KL divergence or the Lp distances.
Future work can measure perturbation with the H-divergence Hℓby choosing loss functions ℓthat are tailored for the
problem.
Computation Issues
There are several situations where estimating the H-divergence is (provably) computationally feasible:
• When A is a small ﬁnite set, in which case we can enumerate all possible values of a ∈A.
• When the loss function ℓ(y, a) is convex in a, in which case we can accurately estimate the H-divergence in polynomial
time by solving the optimization problem infa E[ℓ(Y, a)] by gradient descent.
In general, while it is difﬁcult to guarantee computational feasibility, we use a practical technique that works well in our
experiments: we use the same number of gradient descent steps for evaluating Hℓ
  p+q
2

and Hℓ(p), Hℓ(q). Intuitively, the
“sub-optimality” when estimating the three terms is approximately the same in expectation and cancels out.
20

