Published as a conference paper at ICLR 2022
UNDERSTANDING OVER-SQUASHING AND
BOTTLENECKS ON GRAPHS VIA CURVATURE
Jake Topping12†, Francesco Di Giovanni3†, Benjamin P. Chamberlain3,
Xiaowen Dong1, and Michael M. Bronstein23
1University of Oxford 2Imperial College London 3Twitter
ABSTRACT
Most graph neural networks (GNNs) use the message passing paradigm, in which
node features are propagated on the input graph. Recent works pointed to the
distortion of information ﬂowing from distant nodes as a factor limiting the efﬁ-
ciency of message passing for tasks relying on long-distance interactions. This
phenomenon, referred to as ‘over-squashing’, has been heuristically attributed to
graph bottlenecks where the number of k-hop neighbors grows rapidly with k. We
provide a precise description of the over-squashing phenomenon in GNNs and
analyze how it arises from bottlenecks in the graph. For this purpose, we introduce
a new edge-based combinatorial curvature and prove that negatively curved edges
are responsible for the over-squashing issue. We also propose and experimentally
test a curvature-based graph rewiring method to alleviate the over-squashing.
1
INTRODUCTION
Figure 1: Top: evolution of curvature on a surface may
reduce the bottleneck. Bottom: this paper shows how
the same may be done on graphs to improve GNN per-
formance. Blue/red shows negative/positive curvature.
In the past few years, deep learning on
graphs and in particular graph neural net-
works (GNNs) (Sperduti, 1994; Goller &
Kuchler, 1996; Sperduti & Starita, 1997;
Frasconi et al., 1998; Gori et al., 2005;
Scarselli et al., 2008; Bruna et al., 2014;
Defferrard et al., 2016; Kipf & Welling,
2017; Gilmer et al., 2017) have become
very popular in the machine learning com-
munity due to their ability to deal with
broad classes of systems of relations and
interactions, ranging from social networks
to particle physics (Shlomi et al., 2021).
The vast majority of GNNs follow the
message passing paradigm (Gilmer et al.,
2017), using learnable non-linear functions to diffuse information on the graph. Multiple popular
GNN architectures such as GCN (Kipf & Welling, 2017) and GAT (Veliˇckovi´c et al., 2018) can be
posed as particular ﬂavors of this scheme and considered instances of a more general framework of
geometric deep learning (Bronstein et al., 2021).
Some of the drawbacks of the message passing paradigm have now been identiﬁed and formalized,
including the limits of expressive power (Xu et al., 2019; Morris et al., 2019; Maron et al., 2019)
and the problem of over-smoothing (NT & Maehara, 2019; Oono & Suzuki, 2020). On the other
hand, much less is known about the phenomenon of over-squashing, consisting in the distortion of
messages being propagated from distant nodes. Alon & Yahav (2021) proposed rewiring the graph
as a way of reducing the bottleneck, deﬁned as those topological properties in the graph leading to
over-squashing. This approach is in line with multiple other results e.g. using connectivity diffusion
(Klicpera et al., 2019) as a preprocessing step to facilitate graph learning. Yet, the exact understanding
of the over-squashing and how it originates from the bottlenecks in the topology of the underlying
†Equal contribution
1

Published as a conference paper at ICLR 2022
graph are still elusive. Consequently, there is currently no consensus on the right method (either
based on graph rewiring or not) to address the bottleneck and hence alleviate the over-squashing.
In this paper, we address these questions using tools from differential geometry, which traditionally
is concerned with the study of manifolds. It offers an appealing framework to study the properties
of graphs, in particular arguing that graphs, like manifolds, exhibit curvature that makes them more
suitable to be realized in spaces with hyperbolic geometry (Liu et al., 2019; Chami et al., 2019;
Boguna et al., 2021). One notion of curvature that has received attention for graph learning is Ricci
curvature (Hamilton, 1988), also known in geometry for its use in Ricci ﬂow and the subsequent
proof of the Poincaré conjecture (Perelman, 2003). Certain graph analogues of the Ricci curvature
(Forman, 2003; Ollivier, 2009; Sreejith et al., 2016) were used in Ni et al. (2018) for a discrete version
of Ricci ﬂow to construct a metric between graphs. Graph Ricci ﬂow was also used in Ni et al. (2019)
for community detection. Both of these methods use the edge weights as a substitute for the metric of
a manifold, and do not change the topological structure of the graph.
Contributions and Outline.
This paper, to our knowledge, is the ﬁrst theoretical study of the
bottleneck and over-squashing phenomena in message passing neural networks from a geometric
perspective. In Section 2, we propose the Jacobian of node representations as a formal way of
measuring the over-squashing and we show that the graph topology may compromise message
propagation in graph neural networks by creating a bottleneck. In Section 3, we investigate how such
a bottleneck is induced which leads to the over-squashing of information. To this aim, we introduce a
new combinatorial edge-based curvature called Balanced Forman curvature that constitutes a sharp
lower bound to the standard Ollivier curvature on graphs, and prove that negatively curved edges are
responsible for the formation of bottlenecks (and hence for over-squashing). In Section 4, we present
a new curvature-based method for graph rewiring called Stochastic Discrete Ricci Flow. According
to the theoretical results in Section 3, this rewiring method is suited to address the graph bottleneck
and hence alleviate the over-squashing by surgically targeting the edges responsible for the issue.
By contrast, we rigorously show that a recently introduced diffusion-based rewiring scheme might
generally fail to reduce the bottleneck. Finally, in Section 5, we compare different rewiring strategies
experimentally on several standard graph learning datasets.
2
ANALYSIS OF THE OVER-SQUASHING PHENOMENON
2.1
PRELIMINARIES
Let G = (V, E) be a simple, undirected, and connected graph, where (i, j) ∈E iff i ∼j. We focus
on the unweighted case, although the theory extends to the weighted setting as well. We denote
the adjacency matrix by A and let ˜A = A + I be the adjacency matrix augmented with self-loops.
Similarly we let ˜D = D + I, with D the diagonal degree matrix, and let ˆA = ˜D−1
2 ˜A ˜D−1
2 be the
normalized augmented adjacency matrix (self-loops are commonly included in GNN architecture,
and in Section 2 we formally explain why GNNs are expected to propagate information more reliably
when self-loops are taken into account). Given i ∈V , we denote its degree by di and let
Sr(i) := {j ∈V : dG(i, j) = r},
Br(i) := {j ∈V : dG(i, j) ≤r},
where dG is the standard shortest-path distance on the graph and r ∈N. The set Br(i) represents the
receptive ﬁeld of an r-layer message passing neural network at node i.
Message passing neural networks (MPNNs).
Assume that the graph G is equipped with node
features X ∈Rn×p0 where xi ∈Rp0 is the feature vector at node i = 1, . . . , n = |V |. We denote by
h(ℓ)
i
∈Rpℓthe representation of node i at layer ℓ≥0, with h(0)
i
= xi. Given a family of message
functions ψℓ: Rpℓ× Rpℓ→Rp′
ℓand update functions φℓ: Rpℓ× Rp′
ℓ→Rpℓ+1, we can write the
(ℓ+ 1)-st layer output of a generic MPNN as follows (Gilmer et al., 2017):
h(ℓ+1)
i
= φℓ

h(ℓ)
i ,
n
X
j=1
ˆAijψℓ(h(ℓ)
i , h(ℓ)
j )

.
(1)
Here we have used the augmented normalized adjacency matrix to propagate messages from each
node to its neighbors, which simply leads to a degree normalization of the message functions ψℓ. To
2

Published as a conference paper at ICLR 2022
avoid heavy notations the node features and representations are assumed to be scalar from now on;
these assumptions simplify the discussion and the vector case leads to analogous results.
2.2
THE OVER-SQUASHING PROBLEM
Multiple recent papers observed that MPNNs tend to perform poorly in situations when the learned
task requires long-range dependencies and at the same time the structure of the graph results in
exponentially many long-range neighboring nodes. We say that a graph learning problem has
long-range dependencies when the output of a MPNN depends on representations of distant nodes
interacting with each other. If long-range dependencies are present, messages coming from non-
adjacent nodes need to be propagated across the network without being too distorted. In many cases
however (e.g. in ‘small-world’ graphs such as social networks), the size of the receptive ﬁeld Br(i)
grows exponentially with r. If this occurs, representations of exponentially many neighboring nodes
need to be compressed into ﬁxed-size vectors to propagate messages to node i, causing a phenomenon
referred to as over-squashing of information (Alon & Yahav, 2021). In line with Alon & Yahav
(2021), we refer to those structural properties of the graph that lead to over-squashing as a bottleneck1.
Sensitivity analysis.
The hidden feature h(ℓ)
i
= h(ℓ)
i (x1, . . . , xn) computed by an MPNN with ℓ
layers as in equation 1 is a differentiable function of the input node features {x1, . . . , xn} as long as
the update and message functions φℓand ψℓare differentiable. The over-squashing of information
can then be understood in terms of one node representation h(ℓ)
i
failing to be affected by some input
feature xs of node s at distance r from node i. Hence, we propose the Jacobian ∂h(r)
i /∂xs as an
explicit and formal way of assessing the over-squashing effect2.
Lemma 1. Assume an MPNN as in equation 1. Let i, s ∈V with s ∈Sr+1(i). If |∇φℓ| ≤α and
|∇ψℓ| ≤β for 0 ≤ℓ≤r, then

∂h(r+1)
i
∂xs
 ≤(αβ)r+1( ˆAr+1)is.
(2)
Lemma 1 states that if φℓand ψℓhave bounded derivatives, then the propagation of messages is
controlled by a suitable power of ˆA. For example, if dG(i, s) = r + 1 and the sub-graph induced on
Br+1(i) is a binary tree, then ( ˆAr+1)is = 2−13−r, which gives an exponential decay of the node
dependence on input features at distance r, as also heuristically argued by Alon & Yahav (2021).
The sensitivity analysis in Lemma 1 relates the over-squashing – as measured by the Jacobian of
the node representations – to the graph topology via powers of the augmented normalized adjacency
matrix. In the next section we explore this connection further by analyzing which local properties of
the graph structure affect the right hand side in equation 2, hence causing the bottleneck. We will
address this problem by introducing a new combinatorial notion of edge-based curvature and showing
that negatively curved edges are those responsible for the over-squashing phenomenon.
3
GRAPH CURVATURE AND BOTTLENECK
A natural object in Riemannian geometry is the Ricci curvature, a bilinear form determining the
geodesic dispersion, i.e. whether geodesics starting at nearby points with ‘same’ velocity remain
parallel (Euclidean space), converge (spherical space), or diverge (hyperbolic space). To motivate
the introduction of a Ricci curvature for graphs, we focus on these three cases. Consider two nodes
i ∼j and two edges starting at i and j respectively. In a discrete spherical geometry (Figure 2a),
the edges would meet at k to form a triangle (complete graph). In a discrete Euclidean geometry
(Figure 2b), the edges would stay parallel and form a 4-cycle based at i ∼j (orthogonal grid). Finally,
in a discrete hyperbolic geometry (Figure 2c), the mutual distance of the edge endpoints would have
grown compared to that of i and j (tree). Therefore, a Ricci curvature for graphs should provide us
with more sophisticated tools than the degree to analyze the neighborhood of an edge.
1We note that the over-squashing issue is different from the problem of under-reaching; the latter simply
amounts to a MPNN failing to fully explore a graph when the depth is smaller than the diameter (Barceló et al.,
2019). The over-squashing phenomenon instead may occur even in deep GNNs with the number of layers larger
than the graph diameter, as tested experimentally in Alon & Yahav (2021).
2The Jacobian of a GNN-output was also used by Xu et al. (2018) to set a similarity score among nodes.
3

Published as a conference paper at ICLR 2022
(a) Clique (> 0)
(b) Grid (= 0)
(c) Tree (< 0)
Figure 2: Different regimes of curvatures on graphs anal-
ogous to spherical (a), planar (b), and hyperbolic (c) ge-
ometries in the continuous setting.
Curvatures on graphs.
The main ex-
amples of edge-based curvature are
the Forman curvature F(i, j) (Forman,
2003) and the Ollivier curvature κ(i, j)
in Ollivier (2007; 2009) (see Appendix).
While F(i, j) is given in terms of combi-
natorial quantities (Sreejith et al., 2016),
results are scarce and the deﬁnition is bi-
ased towards negative curvature. The the-
ory on κ(i, j) instead is richer (Lin et al.,
2011; Münch, 2019) but its formulation
makes it hard to control local quantities.
Balanced Forman curvature.
We propose a new curvature to address the shortcomings of the
existing candidates. We use the following deﬁnitions to describe the neighborhood of an edge i ∼j
and we refer to the Appendix for a more complete discussion:
(i) ♯∆(i, j) := S1(i) ∩S1(j) are the triangles based at i ∼j.
(ii) ♯i
□(i, j) := {k ∈S1(i) \ S1(j), k ̸= j : ∃w ∈(S1(k) ∩S1(j)) \ S1(i)} are the neighbors
of i forming a 4-cycle based at the edge i ∼j without diagonals inside.
0
1
2
3
4
5
6
Figure 3: 4-cycle
contribution.
(iii) γmax(i, j) is the maximal number of 4−cycles based at i ∼j traversing
a common node (see Deﬁnition 4).
In line with the discussion about geodesic dispersion, one expects ♯∆to be
related to positive curvature (complete graph), ♯i
□to zero curvature (grid), and
the remaining outgoing edges to negative curvature (tree). Our new curvature
formulation reﬂects such an intuition and recovers the expected results in the
classical cases. In the example in Figure 3 we have ♯0
□(0, 1) = {2, 3} while
♯1
□(0, 1) = {5}, both without 4,6 because of the triangle 1-6-0. The degeneracy
factor γmax(0, 1) = 2, as there exist two 4-cycles passing through node 5.
Deﬁnition 1 (Balanced Forman curvature). For any edge i ∼j in a simple, unweighted graph G,
we let Ric(i, j) be zero if min{di, dj} = 1 and otherwise
Ric(i, j) := 2
di
+ 2
dj
−2 + 2
|♯∆(i, j)|
max{di, dj} +
|♯∆(i, j)|
min{di, dj} +
(γmax)−1
max{di, dj}(|♯i
□| + |♯j
□|),
(3)
where the last term is set to be zero if |♯i
□| (and hence |♯j
□|) is zero. In particular Ric(i, j) > −2.
The curvature is negative when i ∼j behaves as a bridge between S1(i) and S1(j), while it is
positive when S1(i) and S1(j) stay connected after removing i ∼j. We refer to Ric as Balanced
Forman curvature. We can relate the Balanced Forman curvature to the Jacobian of hidden features,
while also extending many results valid for the Ollivier curvature κ(i, j) thanks to our next theorem.
Theorem 2. Given an unweighted graph G, for any edge i ∼j we have κ(i, j) ≥Ric(i, j).
Graph G
RicG
Cycles
C3
3
2
C4
1
Cn≥5
0
Complete Kn
n
n−1
Grid Gn
0
Tree Tr
4
r+1 −2
Table 1: Examples of the
Balanced Forman curvature.
Theorem 2 generalizes Jost & Liu (2014, Theorem 3) (see Appendix).
We also note that the computational complexity for κ scales as
O(|E|d3
max), while for our Ric we have O(|E|d2
max), with dmax the
maximal degree. From Theorem 2 and Paeng (2012), we ﬁnd:
Corollary 3. If Ric(i, j) ≥k > 0 for any edge i ∼j, then there
exists a polynomial P such that
|Br(i)| ≤P(r),
∀i ∈V.
Therefore, when the curvature is positive everywhere, the bottleneck
effect should not play a crucial role as the receptive ﬁeld of each
node will be polynomial in the hop-distance. This limit case shows
how the curvature determines whether a learning task on a graph would suffer from over-squashing.
4

Published as a conference paper at ICLR 2022
Curvature and over-squashing.
Thanks to our new combinatorial curvature and the sensitivity
analysis in Lemma 1, we are able to relate local curvature properties to the Jacobian of the node
representations. This leads to one of the main results of this paper: negatively curved edges are
those causing the graph bottleneck and thus leading to the over-squashing phenomenon:
Theorem 4. Consider a MPNN as in equation 1. Let i ∼j with di ≤dj and assume that:
(i) |∇φℓ| ≤α and |∇ψℓ| ≤β for each 0 ≤ℓ≤L −1, with L ≥2 the depth of the MPNN.
(ii) There exists δ s.t. 0 < δ < (max{di, dj})−1
2 , δ < γ−1
max, and Ric(i, j) ≤−2 + δ.
Then there exists Qj ⊂S2(i) satisfying |Qj| > δ−1 and for 0 ≤ℓ0 ≤L −2 we have
1
|Qj|
X
k∈Qj

∂h(ℓ0+2)
k
∂h(ℓ0)
i
 < (αβ)2δ
1
4 .
(4)
Condition (i) is always satisﬁed and allows us to control the message passing functions. The
requirement (ii) instead means that the curvature of (i, j) is negative enough when compared to the
degrees of i and j (recall that Ric(i, j) > −2). The further condition on γmax is to avoid pathological
cases where we have a large number of degenerate 4-cycles passing through the same three nodes.
To understand the conclusions in Theorem 4, let us ﬁx ℓ0 = 0. The equation 4 shows that negatively
curved edges are the ones causing bottlenecks, interpreted as how the graph topology prevents a
representation h(2)
k
to be affected by non-adjacent features h(0)
i
= xi. Theorem 4 implies that if we
have a negatively curved edge as in (ii), then there exist a large number of nodes k such that GNNs -
on average - struggle to propagate messages from i to k in two layers despite these nodes k being at
distance 2 from i. In this case the over-squashing occurs as measured by the Jacobian in equation 4
and hence the propagation of information suffers. If the task at hand has long-range dependencies,
then the over-squashing caused by the negatively curved edges may compromise the performance.
Bottleneck via Cheeger constant.
We now relate the previous discussion about bottlenecks and
curvature to spectral properties of the graph. In particular, since the spectral gap of a graph can be
interpreted as a topological obstruction to the graph being partitioned into two communities, we
argue below that this quantity is related to the graph bottleneck and should hence be controllable
by the curvature. We start with an intuitive explanation: suppose we are given a graph G with two
communities separated by few edges. In this case, we see that the graph can be easily disconnected.
This property is encoded in the classical notion of the Cheeger constant (Chung & Graham, 1997)
hG := min
S⊂V hS,
hS := min
S⊂V
|∂S|
min{vol(S), vol(V \ S)}
(5)
where ∂S = {(i, j) : i ∈S, j ∈V \ S} and vol(S) = P
i∈S di. The main result about the Cheeger
constant is the Cheeger inequality (Cheeger, 2015; Chung & Graham, 1997):
2hG ≥λ1 ≥h2
G
2
(6)
where λ1 is the ﬁrst non-zero eigenvalue of the normalized graph Laplacian, often referred to as
the spectral gap. A graph with two tightly connected communities (S and V \ S := ¯S) and few
inter-community edges has a small Cheeger constant hG. For nodes in different communities to
interact with each other, all messages need to go through the same few bridges hence leading to the
over-squashing of information (a similar intuition was explored in Alon & Yahav (2021)). Therefore,
hG can be interpreted as a rough measure of graph ‘bottleneckedness’, in the sense that the smaller its
value, the more likely the over-squashing is to occur across inter-community edges. Since Theorem 4
implies that negatively curved edges induce the bottleneck, we expect a relationship between hG and
the curvature of the graph. The next proposition follows from Theorem 2 and Lin et al. (2011):
Proposition 5. If Ric(i, j) ≥k > 0 for all i ∼j, then λ1/2 ≥hG ≥k
2.
Therefore, a positive lower bound on the curvature gives us a control on hG and hence on the
spectral gap of the graph. In the next section, we show that diffusion-based graph-rewiring methods
might fail to signiﬁcantly alter hG and hence correct the graph bottleneck potentially induced by
inter-community edges. This will lead us to propose an alternative curvature-based graph rewiring.
5

Published as a conference paper at ICLR 2022
4
CURVATURE-BASED REWIRING METHODS
The traditional paradigm of message passing graph neural networks assumes that messages are
propagated on the input graph (Gilmer et al., 2017). More recently, there is a trend to decouple
the input graph from the graph used for information propagation. This can take the form of graph
subsampling or resampling to deal with scalability (Hamilton et al., 2017) or topological noise (Zhang
et al., 2019), using larger motif-based (Monti et al., 2018) or multi-hop ﬁlters (Rossi et al., 2020),
or changing the graph either as a preprocessing step (Klicpera et al., 2019; Alon & Yahav, 2021) or
adaptively for the downstream task (Wang et al., 2019; Kazi et al., 2020). Such methods are often
generically referred to as graph rewiring.
In the context of this paper, we assume that graph rewiring attempts to produce a new graph
G′ = (V, E′) with a different edge structure that reduces the bottleneck and hence potentially
alleviates the over-squashing of information. We propose a method that leverages the graph curvature
to guide the rewiring steps in a surgical way by modifying the negatively-curved edges, so to decrease
the bottleneck without signiﬁcantly compromising the statistical properties of the input graph. We
also rigorously show that a random-walk based rewiring method might generally fail to obtain an edge
set E′ with a signiﬁcant improvement in its bottleneckedness as measured by the Cheeger constant.
Curvature-based graph rewiring.
Since according to Theorem 4 negatively curved edges induce
a bottleneck and are hence responsible for over-squashing, a curvature-based rewiring method should
attempt to alleviate a graph’s strongly-negatively curved edges. To this end we implement a simple
rewiring method called Stochastic Discrete Ricci Flow (SDRF), described in Algorithm 1.
Algorithm 1: Stochastic Discrete Ricci Flow (SDRF)
Input: graph G, temperature τ > 0, max number of iterations, optional Ric upper-bound C+
Repeat
1) For edge i ∼j with minimal Ricci curvature Ric(i, j):
Calculate vector x where xkl = Rickl(i, j) −Ric(i, j), the improvement to Ric(i, j)
from adding edge k ∼l where k ∈B1(i), l ∈B1(j);
Sample index k, l with probability softmax(τx)kl and add edge k ∼l to G.
2) Remove edge i ∼j with maximal Ricci curvature Ric(i, j) if Ric(i, j) > C+.
Until convergence, or max iterations reached;
At each iteration this preprocessing step adds an edge to ‘support’ the graph’s most negatively curved
edge, and then removes the most positively curved edge. The requirement on the added edge k ∼l
that k ∈B1(i) and l ∈B1(j) ensures that we’re adding either an extra 3- or 4-cycle around the
negative edge i ∼j so that this is a local modiﬁcation. The graph edit distance between the original
and preprocessed graph is bounded above by 2 × the max number of iterations. The temperature
τ determines how stochastic the edge addition is, with τ = ∞being fully deterministic (the best
edge is always added). At each step we remove the edge with most positive curvature to balance the
distributions of curvature and node degrees. We use Balanced Forman curvature as in equation 3 for
Ric(i, j). C+ can be chosen to stop the method skewing the curvature distribution negative, including
C+ = ∞to not remove any edges. The method is inspired by the continuous (backwards) Ricci ﬂow
with the aim of homogenizing edge curvatures. This is different from more direct extensions of Ricci
ﬂow on graphs where it becomes increasingly expensive to propagate messages across negatively
curved edges (as in other applications such as Ni et al. (2019)). An example alongside its continuous
analogue can be seen in Figure 1.
Can random-walk based rewiring address bottlenecks?
A good way of understanding the effec-
tiveness of SDRF in reducing the graph bottleneck is through comparison with random-walk based
rewiring strategies. Recall that, as argued in Section 3, the Cheeger constant hG of a graph constitutes
a rough measure of its bottleneckedness as induced by the inter-community edges (a small hG is
indicative of a bottleneck). Suppose we are given a graph G with a small hG and wish to rewire it into
a graph G′ with a signiﬁcantly improved Cheeger constant in order to reduce the inter-community
bottleneck. A random-walk based rewiring method such as DIGL (Klicpera et al., 2019) acts by
smoothing out the graph adjacency and hence tends to promote connections among nodes at short
6

Published as a conference paper at ICLR 2022
diffusion distance (Coifman & Lafon, 2006). Accordingly, such a rewiring method might fail to
correct structural features like the bottleneck, which is instead more prominent for nodes that are at
long diffusion distance.3 To emphasize this point, we consider a classic example: given α ∈(0, 1),
the Personalized Page Rank (PPR) matrix is deﬁned by (Brin & Page, 1998) as
Rα :=
∞
X
k=0
θP P R
k
(D−1A)k = α
∞
X
k=0
 (1 −α)(D−1A)
k .
Assume that we rewire the graph using Rα as in Klicpera et al. (2019) with the PPR kernel, meaning
that we replace the given adjacency A with Rα. Since Rα is stochastic, the new Cheeger constant of
the rewired graph can be computed as
hS,α =
|∂S|α
volα(S) ≡1
|S|
X
i∈S
X
j∈¯S
(Rα)ij.
By applying (Chung, 2007, Lemma 5), we show that we cannot improve the Cheeger constant
(and hence the bottleneck) arbitrarily well (in contrast to a curvature-based approach). We refer to
Proposition 17 and Remark 18 in Appendix E for results that are more tailored to the actual strategy
adopted in Klicpera et al. (2019) where we also take into account the effect of the sparsiﬁcation.
Theorem 6. Let S ⊂V with vol(S) ≤vol(G)/2. Then hS,α ≤
  1−α
α
 davg(S)
dmin(S) hS, where davg(S)
and dmin(S) are the average and minimum degree on S, respectively.
The property that the new Cheeger constant is directly controlled by the old one stems from the
fact that a random-walk approach like in Klicpera et al. (2019) is meant to act more relevantly
on intra-community edges rather than inter-community edges because it prioritizes short diffusion
distance nodes. This is also why this method performs well on high-homophily datasets, as discussed
below. In particular, for a ﬁxed α ∈(0, 1), the bound in Theorem 6 can be very small. As a speciﬁc
example, consider two complete graphs Kn joined by one bridge. Then hG = (n(n −1) + 1)−1,
which means that the bound on the right hand side is O(n−2).
Theorem 6 implies that a diffusion approach such as DIGL might fail to yield a new edge set E′
with a sufﬁciently improved bottleneck. By contrast, from Theorem 4 and Proposition 5, we deduce
that a curvature-based rewiring method such as SDRF properly addresses the edges that cause the
bottleneck.
Graph structure preservation.
Although a graph-rewiring approach aims at providing a new edge
set E′ potentially more beneﬁcial for the given learning task, it is still desirable to control how far
E′ is from E. In this regard, we note that a curvature-based rewiring is surgical in nature and hence
more likely to preserve the structure of the input graph better than a random-walk based approach.
Consider, for example, that we are given ρ > 0 and wish to rewire the graph such that the new edge
set E′ is within graph-edit distance ρ from the original E. Theorem 4 tells us how to do the rewiring
under such constraints in order to best address the over-squashing: the topological modiﬁcations
need to be localized around the most negatively-curved edges. We can do this with SDRF, with the
maximum number of iterations set to ρ/2.
Secondly, we also point out that Ric(i, j) < −2 + δ implies that min{di, dj} > 2/δ. Therefore, if
we mostly modify the edge set at those nodes i, j joined by an edge with large negative curvature,
then we are perturbing nodes with high degree where such a change is relatively insigniﬁcant, and
thus overall statistical properties of the rewired graph such as degree distribution are likely to be
better preserved. Moreover, graph convolutional networks tend to be more stable to perturbations of
high degree nodes (Zügner et al., 2020; Kenlay et al., 2021), making curvature-based rewiring more
suitable for the downstream learning tasks with popular GNN architectures.
Homophily and bottleneck.
As a ﬁnal remark, note that the graph rewiring techniques considered
in this paper (both DIGL and SDRF) are based purely on the topological structure of the graph and
completely agnostic to the node features and to whether the dataset is homophilic (adjacent nodes
have same labels) or heterophilic. Nonetheless, the different nature of these rewiring methods allows
us to draw a few broad conclusions about their suitability in each of these settings. A random-walk
3We refer to the right hand side of equation 2 where the power of the normalized augmented adjacency is
measuring the number of walks of distance r from i to s.
7

Published as a conference paper at ICLR 2022
Cornell
Texas
Wisconsin
Chameleon
Squirrel
Actor
Cora
Citeseer
Pubmed
H(G)
0.11
0.06
0.16
0.25
0.22
0.24
0.83
0.71
0.79
None
52.69 ± 0.21
61.19 ± 0.49
54.60 ± 0.86
41.80 ± 0.41
39.83 ± 0.14
28.70 ± 0.09
81.89 ± 0.79
72.31 ± 0.17
78.16 ± 0.23
Undirected
53.20 ± 0.53
63.38 ± 0.87
51.37 ± 1.15
42.63 ± 0.30
40.77 ± 0.16
28.10 ± 0.11
-
-
-
+FA
58.29 ± 0.49 64.82 ± 0.29
55.48 ± 0.62
42.33 ± 0.17
40.74 ± 0.13
28.68 ± 0.16
81.65 ± 0.18
70.47 ± 0.18
79.48 ± 0.12
DIGL (PPR)
58.26 ± 0.50
62.03 ± 0.43
49.53 ± 0.27
42.02 ± 0.13
34.38 ± 0.11
30.79 ± 0.10 83.21 ± 0.27 73.29 ± 0.17
78.84 ± 0.08
DIGL + Undirected 59.54 ± 0.64
63.54 ± 0.38
52.23 ± 0.54
42.68 ± 0.12
33.36 ± 0.21
29.71 ± 0.11
-
-
-
SDRF
54.60 ± 0.39
64.46 ± 0.38
55.51 ± 0.27 43.75 ± 0.31 40.97 ± 0.14
29.70 ± 0.13
82.76 ± 0.23 72.58 ± 0.20 79.10 ± 0.11
SDRF + Undirected
57.54 ± 0.34
70.35 ± 0.60 61.55 ± 0.86 44.46 ± 0.17 41.47 ± 0.21 29.85 ± 0.07
-
-
-
Table 2: Experimental results on common node classiﬁcation benchmarks. Top two in bold.
approach such as DIGL tends to improve the connectivity among nodes that are at short diffusion
distance; since for a high-homophily dataset these nodes often share the same label, a rewiring method
like DIGL is likely to act as graph denoising and yield improved performance. On the other hand, for
datasets with low homophily, nodes at short diffusion distance are more likely to belong to different
label classes, meaning that a diffusion-based rewiring might inject noise and hence compromise
performance as also noted in Klicpera et al. (2019). Conversely, on a low-homophily dataset, a
curvature-based approach as SDRF modiﬁes the edge set mainly around the most negatively curved
edges, meaning that it decreases the bottleneck without signiﬁcantly increasing the connectivity
among nodes with different labels. In fact, long-range dependencies are often more relevant in
low-homophily settings, where nodes sharing the same labels are in general not neighbors. This
observation is largely conﬁrmed by experimental results reported in the next section.
5
EXPERIMENTAL RESULTS
Experiment setup.
To demonstrate the theoretical results in this paper we ran a suite of semi-
supervised node classiﬁcation tasks comparing our curvature-based rewiring method SDRF to DIGL
from Klicpera et al. (2019) (GDC with the PPR kernel) and the +FA method from Alon & Yahav
(2021), where the last layer of the GNN is made fully connected. We evaluate the methods on
nine datasets: Cornell, Texas and Wisconsin from the WebKB dataset4; Chameleon and Squirrel
(Rozemberczki et al., 2021) along with Actor (Tang et al., 2009); and Cora (McCallum et al., 2000),
Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012). Statistics for these datasets can be found
in Appendix F.1. Our base model is a GCN (Kipf & Welling, 2017). Following Shchur et al. (2018)
and Klicpera et al. (2019) we optimized hyperparameters for all dataset-preprocessing combinations
separately by random search over 100 data splits. Results are reported as average accuracies on a
test set used once with 95% conﬁdence intervals calculated by bootstrapping. We compared the
performance on graphs with no preprocessing, making the graph undirected, +FA, DIGL, SDRF, and
the given combinations. For DIGL + Undirected we symmetrized the diffusion matrix as in Klicpera
et al. (2019), and for SDRF + Undirected we made the graph undirected before applying SDRF. For
more details on the experiments and datasets see Appendix F, and for the hyperparameters used for
each model and preprocessing see Appendix F.4.
Node classiﬁcation results.
Table 2 shows the results of the experiments. As well as reporting
results we give a measure of homophily H(G) proposed by Pei et al. (2019) (restated in Appendix F,
equation 21), by which we can see our experiment set is diverse with respect to homophily. We see
that SDRF improves upon the baseline in all cases, and that the largest improvements are seen on the
low-homophily datasets. We also see that SDRF matches or outperforms DIGL and +FA on most
datasets, supporting our argument that curvature-based rewiring is a viable candidate for improving
GNN performance.
Graph topology change.
Furthermore, SDRF preserves the graph topology to a far greater extent
than DIGL due to its surgical nature. Table 3 shows the number of edges added / removed by
the two preprocessings on each dataset as a percentage of the original number of edges. We see
that for optimal performance DIGL makes the graph much denser, which signiﬁcantly affects the
node degrees and may negatively impact the time and space complexity of the downstream GNN,
which typically are O(E). In comparison, SDRF adds and removes a similar number of edges and
approximately preserves the degree distribution. The effect on the full degree distribution for three
4http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/
8

Published as a conference paper at ICLR 2022
0
1
2
4
8
16
32
64
128
Original
DIGL
SDRF
(a) Wisconsin:
W1(Original, DIGL) = 11.83
W1(Original, SDRF) = 0.28
0
1
2
4
8
16
32
64
128
256
512
1024
Original
DIGL
SDRF
(b) Actor:
W1(Original, DIGL) = 243.81
W1(Original, SDRF) = 1.03
0
1
2
4
8
16
32
64
128
256
512 1024 2048
Original
DIGL
SDRF
(c) Pubmed:
W1(Original, DIGL) = 247.01
W1(Original, SDRF) = 0.03
Figure 4: Comparing the degree distribution of the original graphs to the preprocessed version. The x-
axis is node degree in log2 scale, and the plots are a kernel density estimate of the degree distribution.
In the captions we see the Wasserstein distance W1 between the original and preprocessed graphs.
DIGL
SDRF
Cornell
351.1% / 0.0%
7.8% / 33.3%
Texas
483.3% / 0.0%
2.4% / 10.4%
Wisconsin
300.6% / 0.0%
1.4% / 7.5%
Chameleon
336.1% / 11.8%
6.4% / 6.4%
Squirrel
228.8% / 1.9%
0.7% / 0.7%
Actor
2444.0% / 2.3%
5.4% / 9.9%
Cora
3038.0% / 0.5%
1.0% / 1.0%
Citeseer
2568.3% / 0.0%
1.1% / 1.1%
Pubmed
2747.1% / 0.1%
0.2% / 0.2%
Table 3: % edges added / removed by method.
datasets is shown in Figure 4. We see that the de-
gree distribution following SDRF is close to, or
in some cases indistinguishable from, the orig-
inal distribution, whereas DIGL has a more no-
ticeable effect. We also compute the Wasser-
stein distance between the degree distributions,
denoted by W1 in the ﬁgure captions, to numer-
ically conﬁrm this observation. For this analysis
extended to all nine datasets see Appendix F.2.
This difference is also visually evident from Fig-
ure 6 in Appendix F.3, where we again observe
that SDRF largely preserves the topology of the
Cornell graph and that the curvature (encoded
by the edge color) is homogenized across the
graph. We also see that the entries of the trained
network’s Jacobian between a node’s prediction and the features of its 2-hop neighbors (encoded as
node colors) are increased over the graph, which we may attribute to both DIGL and SDRF’s ability
to alleviate the upper bound presented in Theorem 4 and thus reduce over-squashing.
6
CONCLUSION
In this paper, we studied the graph bottleneck and the over-squashing phenomena limiting the
performance of message passing graph neural networks from a geometric perspective. We started
with a Jacobian approach to determine how the over-squashing phenomenon is dictated by the graph
topology as in equation 2. We then investigated further how the topology induces the bottleneck
and hence causes over-squashing. We introduced a new notion of edge-based Ricci curvature called
Balanced Forman curvature, relating it to the classical Ollivier curvature (Theorem 2). We then
proved in Theorem 4 that negatively-curved edges are responsible for over-squashing, calling for a
possibility of curvature-based rewiring of the graph in order to improve its bottleneckedness. We
show one such possibility (Algorithm 1), inspired by the classical Ricci ﬂow and comment on the
advantages of surgical method such as this. We show both theoretically and experimentally that the
proposed method can be advantageous compared to a diffusion-based rewiring approach, opening the
door for curvature-based rewiring methods for improving GNN performance going forward.
Limitations and future directions.
Our paper establishes a geometric perspective on the graph
bottleneck and over-squashing, providing new tools to study and cope with these phenomena. One
limitation of our work is that the theoretical results presented here do not currently extend to multi-
graphs. In addition, the current methodology is agnostic to information beyond the graph topology,
such as node features. In future works, we will develop a notion of the curvature and the corresponding
rewiring method that can take into account such information.
Acknowledgements.
This research was supported in part by the EPSRC CDT in Modern Statistics
and Statistical Machine Learning (EP/S023151/1) and the ERC Consolidator Grant No. 724228
(LEMAN). X.D. gratefully acknowledges support from the Oxford-Man Institute of Quantitative
Finance and the EPSRC (EP/T023333/1).
9

Published as a conference paper at ICLR 2022
REFERENCES
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=i80OPhOCVH2.
Pablo Barceló, Egor V Kostylev, Mikael Monet, Jorge Pérez, Juan Reutter, and Juan Pablo Silva. The
logical expressiveness of graph neural networks. In ICLR, 2019.
Marian Boguna, Ivan Bonamassa, Manlio De Domenico, Shlomo Havlin, Dmitri Krioukov, and
M Ángeles Serrano. Network geometry. Nature Reviews Physics, pp. 1–22, 2021.
Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1-7):107–117, 1998.
Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. arXiv:2104.13478, 2021.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In 2nd International Conference on Learning Representations,
ICLR 2014, 2014.
Ines Chami, Rex Ying, Christopher Ré, and Jure Leskovec. Hyperbolic graph convolutional neural
networks. In NeurIPS, 2019.
Jeff Cheeger. A lower bound for the smallest eigenvalue of the laplacian. In Problems in analysis, pp.
195–200. Princeton University Press, 2015.
Fan Chung. Four proofs for the cheeger inequality and graph partition algorithms. In Proceedings of
ICCM, volume 2, pp. 378. Citeseer, 2007.
Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical
Soc., 1997.
Ronald R Coifman and Stéphane Lafon. Diffusion maps. Applied and computational harmonic
analysis, 21(1):5–30, 2006.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As-
sociates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
04df4d434d481c5bb723be1b6df1ee65-Paper.pdf.
Robin Forman. Discrete and computational geometry, 2003.
Paolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing
of data structures. IEEE Trans. Neural Networks, 9(5):768–786, 1998.
Linton C Freeman. A set of measures of centrality based on betweenness. Sociometry, pp. 35–41,
1977.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263–1272. PMLR, 2017.
Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations by
backpropagation through structure. In ICNN, 1996.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729–734. IEEE, 2005.
Richard Hamilton. The ricci ﬂow on surfaces. In Mathematics and general relativity, Proceedings
of the AMS-IMS-SIAM Joint Summer Research Conference in the Mathematical Sciences on
Mathematics in General Relativity, Univ. of California, Santa Cruz, California, 1986, pp. 237–262.
Amer. Math. Soc., 1988.
10

Published as a conference paper at ICLR 2022
William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeurIPS, 2017.
Jürgen Jost and Shiping Liu. Ollivier’s ricci curvature, local clustering and curvature-dimension
inequalities on graphs. Discrete & Computational Geometry, 51(2):300–322, 2014.
Anees Kazi, Luca Cosmo, Nassir Navab, and Michael Bronstein. Differentiable graph module
(dgm) graph convolutional networks. arXiv preprint arXiv:2002.04999, 2020. URL https:
//arxiv.org/pdf/2002.04999.pdf.
Henry Kenlay, Dorina Thanou, and Xiaowen Dong. Interpretable stability bounds for spectral graph
ﬁlters. arXiv preprint arXiv:2102.09587, 2021.
Thomas N. Kipf and Max Welling. Semi-Supervised Classiﬁcation with Graph Convolutional
Networks. In Proceedings of the 5th International Conference on Learning Representations, ICLR
’17, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.
Johannes Klicpera, Stefan Weißenberger, and Stephan Günnemann. Diffusion improves graph
learning. In Proceedings of the 33rd International Conference on Neural Information Processing
Systems, 2019.
Yong Lin, Linyuan Lu, and Shing-Tung Yau. Ricci curvature of graphs. Tohoku Mathematical
Journal, Second Series, 63(4):605–627, 2011.
Qi Liu, Maximilian Nickel, and Douwe Kiela.
Hyperbolic graph neural networks.
In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/
103303dd56a731e377d01f6a37badae3-Paper.pdf.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In NeurIPS, pp. 2153–2164, 2019.
Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating the
construction of internet portals with machine learning. Information Retrieval, 3(2):127–163, 2000.
F. Monti, K. Otness, and M. M. Bronstein. Motifnet: A motif-based graph convolutional network for
directed graphs. In IEEE Data Science Workshop, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In AAAI Conference on Artiﬁcial Intelligence, pp. 4602–4609. AAAI Press, 2019.
Florentin Münch. Non-negative ollivier curvature on graphs, reverse poincar\’e inequality, buser
inequality, liouville property, harnack inequality and eigenvalue estimates.
arXiv preprint
arXiv:1907.13514, 2019.
Galileo Namata, Ben London, Lise Getoor, Bert Huang, and UMD EDU. Query-driven active
surveying for collective classiﬁcation. In 10th International Workshop on Mining and Learning
with Graphs, volume 8, pp. 1, 2012.
Chien-Chun Ni, Yu-Yao Lin, Jie Gao, and Xianfeng Gu. Network alignment by discrete ollivier-ricci
ﬂow. In International Symposium on Graph Drawing and Network Visualization, pp. 447–462.
Springer, 2018. URL https://arxiv.org/abs/1809.00320.
Chien-Chun Ni, Yu-Yao Lin, Feng Luo, and Jie Gao. Community detection on networks with ricci
ﬂow. Scientiﬁc reports, 9(1):1–12, 2019.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass ﬁlters.
2019.
Yann Ollivier. Ricci curvature of metric spaces. Comptes Rendus Mathematique, 345(11):643–646,
2007.
11

Published as a conference paper at ICLR 2022
Yann Ollivier. Ricci curvature of markov chains on metric spaces. Journal of Functional Analysis,
256(3):810–864, 2009.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classiﬁcation. In ICLR, 2020.
Seong-Hun Paeng. Volume and diameter of a graph and ollivier’s ricci curvature. European Journal
of Combinatorics, 33(8):1808–1819, 2012.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. 2019.
Grisha Perelman. Finite extinction time for the solutions to the ricci ﬂow on certain three-manifolds.
arXiv preprint math/0307245, 2003.
Emanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael M. Bronstein, and
Federico Monti. Sign: Scalable inception graph neural networks. CoRR, abs/2004.11198, 2020.
URL https://arxiv.org/abs/2004.11198.
Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal
of Complex Networks, 9(2):cnab014, 2021.
Areejit Samal, RP Sreejith, Jiao Gu, Shiping Liu, Emil Saucan, and Jürgen Jost. Comparative analysis
of two discretizations of ricci curvature for complex networks. Scientiﬁc reports, 8(1):1–16, 2018.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE transactions on neural networks, 20(1):61–80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls
of graph neural network evaluation. In NIPS workshop, 2018.
Jonathan Shlomi, Peter Battaglia, and Jean-Roch Vlimant. Graph neural networks in particle physics.
Machine Learning: Science and Technology, 2(2):021001, jan 2021. doi: 10.1088/2632-2153/
abbf9a. URL https://doi.org/10.1088/2632-2153/abbf9a.
Alessandro Sperduti. Encoding labeled graphs by labeling RAAM. In NIPS, 1994.
Alessandro Sperduti and Antonina Starita. Supervised neural networks for the classiﬁcation of
structures. IEEE Trans. Neural Networks, 8(3):714–735, 1997.
R P Sreejith, Karthikeyan Mohanraj, Jürgen Jost, Emil Saucan, and Areejit Samal. Forman curvature
for complex networks.
Journal of Statistical Mechanics: Theory and Experiment, 2016(6):
063206, Jun 2016. ISSN 1742-5468. doi: 10.1088/1742-5468/2016/06/063206. URL http:
//dx.doi.org/10.1088/1742-5468/2016/06/063206.
Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social inﬂuence analysis in large-scale networks. In
Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, KDD ’09, pp. 807–816, New York, NY, USA, 2009. Association for Computing
Machinery. ISBN 9781605584959. doi: 10.1145/1557019.1557108. URL https://doi.org/
10.1145/1557019.1557108.
Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ.
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon.
Dynamic graph CNN for learning on point clouds. ACM Trans. Graphics, 38(5):1–12, 2019.
Melanie Weber, Emil Saucan, and Jürgen Jost. Coarse geometry of evolving networks. Journal of
complex networks, 6(5):706–732, 2018.
12

Published as a conference paper at ICLR 2022
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453–5462. PMLR, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR. OpenReview.net, 2019.
Y. Zhang, S. Pal, M. Coates, and D. Üstebay. Bayesian graph convolutional neural networks for
semi-supervised classiﬁcation. In AAAI Conference on Artiﬁcial Intelligence, 2019.
Daniel Zügner, Oliver Borchert, Amir Akbarnejad, and Stephan Günnemann. Adversarial attacks
on graph neural networks: Perturbations and their patterns. ACM Transactions on Knowledge
Discovery from Data (TKDD), 14(5):1–31, 2020.
13

Published as a conference paper at ICLR 2022
APPENDIX
The Appendix is structured as follows:
(i) In Appendix A we prove Lemma 1 and a side-result about the role of self-loops. We also
summarize how to extend the analysis in Lemma 1 to message passing models with sum
aggregations (not average), meaning those architectures where we do not normalize the
adjacency matrix.
(ii) In Appendix B we introduce and describe different quantities we use to characterize the
neighbourhood of a given edge i ∼j. These objects are all essential to studying the new
notion of balanced Forman curvature. The focus is on how we can distinguish 4-cycles in a
computationally tractable way without losing too much accuracy.
(iii) In Appendix C we provide a brief literature review about existing curvature candidates. In
particular, we report the deﬁnitions of (modiﬁed) Ollivier curvature and Forman curvature.
(iv) In Appendix D we prove the statements in Section 3, i.e. Theorem 2, Corollary 3, Theorem
4 and Proposition 5. We also comment on the role of the assumptions and compare the
bound in Theorem 2 with the existing literature. Finally, we relate the classical notion of
betweenness centrality to the over-squashing effect and the negatively curved edges in a
graph.
(v) In Appendix E we prove the results in Section 4, namely Theorem 6 and an analogous result.
(vi) In Appendix F we describe more fully the experiments from Section 5, including a full anal-
ysis on degree distribution (Appendix F.2) and the hyperparameters used in our experiments
(Appendix F.4).
(vii) In Appendix G we comment on hardware speciﬁcations.
A
PROOFS OF RESULTS IN SECTION 2
Lemma 1. Assume an MPNN as in equation 1. Let i, s ∈V with s ∈Sr+1(i). If |∇φℓ| ≤α and
|∇ψℓ| ≤β for 0 ≤ℓ≤r, then

∂h(r+1)
i
∂xs
 ≤(αβ)r+1( ˆAr+1)is.
(2)
Proof. Let i ∈V and s ∈Sr+1(i). We recall that to ease the notations we assume that node features
and hidden representations are scalar. The proof in the more general higher-dimensional case follows
without any modiﬁcation. We compute
∂h(r+1)
i
∂xs
= ∂1φr(. . .)∂xsh(r)
i
+ ∂2φr(. . .)
n
X
jr=1
ˆaijr

∂1ψr(h(r)
i , h(r)
jr )∂xsh(r)
i
+ ∂2ψr(h(r)
i , h(r)
jr )∂xsh(r)
jr

.
We can iterate the computation above and see that the right hand side can be expanded as
∂h(r+1)
i
∂xs
=
X
jr,...,j0
X
kr∈{i,jr}
· · ·
X
k1∈{i,jr,...,j1}
ˆaijrˆakrjr−1 . . . ˆak1j0Zijrkrjr−1...k1j0(X)∂xsh(0)
j0 ,
for some functions Zijr...k1j0 of the input features obtained as products of r + 1 partial derivatives of
the maps φℓand r + 1 partial derivatives of the maps ψℓ. Since H(0) = X, we have
∂xsh(0)
j0 = δj0s
14

Published as a conference paper at ICLR 2022
meaning that the previous sum becomes
∂h(r+1)
i
∂xs
=
X
jr,...,j1
X
kr∈{i,jr}
· · ·
X
k1∈{i,jr,...,j1}
ˆaijrˆakrjr−1 . . . ˆak1sZijrkrjr−1...k1s(X).
Since dG(i, s) = r + 1, the only non-vanishing terms in the sum above are the minimal walks from i
to s. In fact, if there existed a different choice of coefﬁcients yielding a non-zero term then we would
ﬁnd a walk joining i to s of length lesser than r + 1, which is in contradiction with the deﬁnition of
geodesic distance. Since Zi...s(X) is a product of r + 1-partial derivatives of the aggregation and
update maps and by assumption their gradients are bounded by α and β respectively, we conclude
that

∂h(r+1)
i
∂xs
 ≤(αβ)r+1
X
jr,...,j1
ˆaijrˆajrjr−1 . . . ˆaj1s = αr+1( ˆAr+1)is
which completes the proof of the Lemma.
As a byproduct of this analysis, we can also provide a rigorous motivation for the role of self-loops in
GNNs (see Appendix for details):
Corollary 7. If h(ℓ+1)
i
= P
j∼i ψℓ(h(ℓ)
j ), then h(ℓ+1)
i
only depends on nodes that can be reached
via walks of length exactly ℓ+ 1. By adding self-loops, the GNN also takes into account nodes that
can be reached via walks of length r ≤ℓ+ 1.
Proof. If h(ℓ+1)
i
= P
j∼i ψℓ(h(ℓ)
j ) for each ℓ∈[0, L −1], then we can argue as in the proof of
Lemma 1 and ﬁnd
∂h(ℓ+1)
i
∂xs
=
X
jℓ,...,j1
aijℓ. . . aj1sψ′
ℓ(h(ℓ)
jℓ) · · · ψ′
0(xs).
The combinatorial coefﬁcient aijℓ. . . aj1s is non-zero iff there exists a walk from i to s of length
exactly ℓ+1, since we are not taking into account the contribution coming from self-loops. Conversely,
if each term aijℓwas replaced by ˆaijℓthen we would ﬁnd that ˆaijℓ. . . ˆaj1s is non-zero iff there exists
a walk from i to s of length at most r + 1, since the diagonal entries are now positive.
Remark 8. As a speciﬁc instance of Corollary 7, we note that if we do not include self-loops in
the adjacency matrix, then the output of a 2-layer simpliﬁed graph neural network at node i is
independent of the features of neighbours k that do not form a triangle with i. Once again here the
dependence is precisely measured via the Jacobian of the hidden features with respect to the input
features.
Remark 9. We note that the role of self-loops has also implicitly been noted in Xu et al. (2018) where
the analysis of the Jacobian of node representations on the graph augmented with self-loops has been
related to lazy random-walks.
GNNs with different aggregations
We note that similar conclusions extend to message passing
architectures where the aggregations are sums and not averages meaning that we take the augmented
adjacency without normalizing by the degree matrices. Consistently with Lemma 1, we restrict to
the setting where features and node representations at each layer are scalars to make the discussion
simpler. In line with the Xu et al. (2018) we consider a GNN-model of the form
h(ℓ+1)
i
= ReLU

X
j∈˜
Ni
h(ℓ)
j wℓ

.
Note that the augmented neighbourhood ˜
Ni is deﬁned as Ni ∪{i}. Differently from the setting of
Theorem 1 in Xu et al. (2018), the aggregation here is not an average but a simple sum. Let us now
take nodes i and s such that s ∈Sr+1(i) as in the statement of Lemma 1. In this case, instead of
simply considering the quantity |∂h(r+1)
i
/∂xs|, we normalize the Jacobian entries - obtaining what
15

Published as a conference paper at ICLR 2022
is referred to as inﬂuence score in Xu et al. (2018):
Jr+1(i, s) :=

∂h(r+1)
i
∂xs

P
k

∂h(r+1)
i
∂xk

This of course represents now a relative importance of feature xs on the representation of node i
at layer r + 1. If - similarly to Theorem 1 in Xu et al. (2018) - we assume that all paths in the
computational graph of the model are activated with the same probability, then we obtain that on
average
Jr+1(i, s) =
˜Ar+1
is
P
k ˜Ar+1
ik
≤
˜Ar+1
is
Vol(Br+1(i)),
where ˜A = A + I and vol(S) = P
j∈S dj. In particular, we again ﬁnd that if we have a tree structure,
then the right hand side decays exponentially as 2−(r+1).
B
PRELIMINARY ANALYSIS OF AN EDGE-NEIGHBORHOOD
Given an edge i ∼j, we introduce the sets below:
(i) ♯∆(i, j) := S1(i) ∩S1(j), the number of triangles based at the edge i ∼j.
(ii) ♯i
□(i, j) := {k ∈S1(i) \ S1(j), k ̸= j : ∃w ∈(S1(k) ∩S1(j)) \ S1(i)}, the number of
nodes k ∈S1(i) forming a 4-cycle based at i ∼j without diagonals inside.
(iii) Qi(j) := S1(i) \ ({j} ∪♯∆(i, j) ∪♯i
□(i, j)), simply the complement of the neighbours of i
with respect to the sets introduced in (i) and (ii) once we also exclude j.
In the following we simply write ♯∆, ♯i
□and Qi when the edge i ∼j is clear from the context.
4-cycle contributions.
In general the sets ♯i
□and ♯j
□may differ. This may occur when there exists
a node k belonging to ♯i
□that admits multiple solutions w as in the deﬁnition of ♯i
□. This feature
needs to be taken into account when comparing Ollivier’s Ricci curvature to the new notion we
present below. We ﬁrst introduce the following class to ease the notations.
Deﬁnition 2. For any simple, undirected graph G = (V, E), if U ⊂V , then we set
D(U) := {ϕ : U →V, |U| = |ϕ(U)|, (z, ϕ(z)) ∈E, ∀z ∈U} .
We note that any ϕ ∈D(U) is injective.
We may now deﬁne a quantity which measures the maximal number of 1-1 pairings that can be
performed from ♯i
□to ♯j
□.
Deﬁnition 3. For any edge i ∼j we let
♯m
□(i, j) := max
n
|U| : U ⊂♯i
□, ∃ϕ : U →♯j
□, ϕ ∈D(U)
o
.
We often simply write ♯m
□. While the quantity ♯m
□plays a role in the derivation of the Ollivier curvature
of i ∼j it is not computationally-friendly, as to determine ♯m
□we need to identify and distinguish all
possible 4-cycles based at i ∼j and then choose a maximal pairing map. Accordingly, we consider a
looser term which is easier to compute:
Deﬁnition 4. For any pair of adjacent nodes i ∼j we deﬁne
γmax(i, j) := max
(
max
k∈♯i
□
{(Ak · (Aj −Ai ⊙Aj)) −1}, max
w∈♯j
□
{(Aw · (Ai −Aj ⊙Ai)) −1}
)
,
where As denotes the s-th row of the adjacency matrix. We usually simply write γmax.
16

Published as a conference paper at ICLR 2022
Remark 10. We note that given k ∈S1(i) \ S1(j) the term (Ak · (Aj −Ai ⊙Aj)) −1 yields the
number of nodes w forming a 4-cycle of the form i ∼k ∼w ∼j ∼i with no diagonals inside. The
value γmax measures the maximal degeneracy of edges forming 4-cycles, meaning that it is equal to 1
iff for each k ∈♯i
□there exists a unique node w ∈♯j
□such that i ∼k ∼w ∼j ∼i is a 4-cycle.
We now end the discussion about 4-cycle contributions by proving the following inequality, which
allows us to avoid to compute directly the term ♯m
□up to giving up some accuracy.
Lemma 11. For any edge i ∼j we have
|♯m
□| ≥max{|♯i
□|, |♯j
□|}
γmax
.
Proof. The proof is based on a combinatorial argument. Let ♯i
□= {k1, . . . , kr} and let ♯m
□=
{k1, . . . , kℓ}, with ℓ< r. By deﬁnition there exists ϕ : {k1, . . . , kℓ} →{w1, . . . , wℓ}, with ki ∼wi
and wi ∈♯j
□, for 1 ≤i ≤ℓ. Given k ∈♯i
□\ {k1, . . . , kℓ}, then there are no w ∈♯j
□\ {w1, . . . , wℓ}
such that k ∼w, otherwise we could extend ϕ by setting k 7→ϕ(k) := w and we would then get
|♯m
□| = ℓ+ 1. Accordingly, we have
ℓ
X
s=1
(Aws · (Ai −Aj ⊙Ai)) −1) ≥|♯i
□|,
which implies
γmax |♯m
□| ≡γmax ℓ≥
ℓ
X
s=1
(Aws · (Ai −Aj ⊙Ai)) −1) ≥|♯i
□|.
C
EXISTING CURVATURE CANDIDATES
Ollivier Ricci curvature
For i ∈V and α ∈[0, 1) we deﬁne a probability measure on B1(i) by:
µα
i : j 7→





α, j = i
1−α
di , j ∈S1(i),
0, otherwise.
Before we introduce the Ollivier curvature, we recall that the transportation distance between two
ﬁnitely supported probability measures as above can be computed as
W1(µα
i , µα
j ) := inf
M
X
k∈S1(i)
X
w∈S1(j)
MkwdG(k, w),
where dG(·, ·) is the geodesic distance on the graph and the inﬁmum is taken over all matrices M
satisfying the marginal constraints:
X
k∈S1(i)
Mkw = µα
j (w),
X
w∈S1(j)
Mkw = µα
i (k).
We are now ready to deﬁne the Ollivier Ricci curvature: the formulation below is due to Lin et al.
(2011).
Deﬁnition 5. Given i ∼j we deﬁne the α-Ollivier curvature by
κα(i, j) := 1 −W1(µα
i , µα
j ).
(7)
Since κα(1 −α)−1 is increasing and bounded the quantity below is well-deﬁned:
κ(i, j) := lim
α→1
1 −W1(µα
i , µα
j )
1 −α
.
(8)
17

Published as a conference paper at ICLR 2022
Forman Ricci curvature
In the following we report a formula for the augmented Forman Ricci
curvature on unweighted graphs Samal et al. (2018). We also note that Forman curvature on graphs
has also been studied in Sreejith et al. (2016); Weber et al. (2018).
Deﬁnition 6. For any edge i ∼j the augmented Forman curvature is given by
F(i, j) := 4 −di −dj + 3|♯∆|.
We note that such formulation of curvature does not distinguish contributions coming from 4-cycles.
In fact, for the orthogonal grid with degree d ≥4, Forman Ricci curvature is equal to 2(2 −d) < 0.
This does not reﬂect that the r-hop neighbourhood for such a graph grows polynomially in r.
We conclude this appendix by reporting a lower bound for the Ollivier Ricci curvature derived in Jost
& Liu (2014). We recall that κα, with α ∈[0, 1) was deﬁned in equation 7.
Theorem 12 (Jost & Liu (2014)). For any edge i ∼j, with di ≤dj, the following bound is satisﬁed:
κ0(i, j) ≥Φ(i, j) := −

1 −1
di
−1
dj
−|♯∆|
dj

+
−

1 −1
di
−1
dj
−|♯∆|
di

+
+ |♯∆|
dj
.
D
PROOFS OF RESULTS IN SECTION 3
We ﬁrst recall our deﬁnition of Balanced Forman:
Deﬁnition 7. For any edge i ∼j we let Ric(i, j) be zero if min{di, dj} = 1, otherwise
Ric(i, j) := 2
di
+ 2
dj
−2 + 2
|♯∆|
max{di, dj} +
|♯∆|
min{di, dj} +
(γmax)−1
max{di, dj}(|♯i
□| + |♯j
□|)
(9)
where the last term is set to be zero if |♯i
□| (and hence |♯j
□|) is zero.
We also extend the previous deﬁnition to the weighted case. In this setting we let G = (V, E, ω) be a
simple, locally ﬁnite, undirected graph with normalized weights. We ﬁrst report the formula for the
augmented Forman in the weighted case Samal et al. (2018):
F(i, j) = ω(i) + ω(j) +
X
k∈S1(i)∩S1(j)
ω2
ij
ω∆
−
X
k∈S1(i)\S1(j)
ω(i)
r ωij
ωik
−
X
k∈S1(j)\S1(i)
ω(j)
r ωij
ωjk
,
where ω∆is taken to be the Heron formula for the area of a triangle while ω(·) denotes some
weighting scheme for the nodes as well. We propose a similar deﬁnition for the weighted case, which
reduces to the one discussed above in the combinatorial setting. We recall that W is the weighted
adjacency matrix while A is the combinatorial one. Moreover, we write Ai
j = (Aj −Ai)+ and
similarly for W i
j.
Deﬁnition 8. For any pair of adjacent nodes i, j ∈V we deﬁne Ric(i, j) to be 0 if min{|di|, |dj|} =
1, otherwise we set
Ric(i, j) := 1
di

1 −
X
k∈Qi
r ωij
ωik

+ 1
dj

1 −
X
k∈Qj
r ωij
ωjk


+
1
max{di, dj}
X
k∈S1(i)∩S1(j)
ω2
ij
ω∆
+
X
k∈♯i
□
ωij
ω□
 ωij(γmax)−1
max{di, dj} −
√ωijν□
di

+
X
k∈♯j
□
ωij
ω□
 ωij(γmax)−1
max{di, dj} −
√ωijν□
dj

,
18

Published as a conference paper at ICLR 2022
with
ω∆:= 1
3(ω2
ij + ω2
ik + ω2
jk),
z ∈S1(i) ∩S1(j),
and, for a given k ∈♯i
□,
ν□:= Wz · Ai
j
Ak · Ai
j
,
ω□= 1
4
 
ω2
ij + ω2
ik + ν2
□+ Ak · W i
j
Ak · Ai
j
!
.
Important convention. Without losing generality, in the following we always assume that 1 ≤di ≤
dj. In particular, we write di .= d and dj = d + s, for some s ≥0, omitting to specify that both d and
s are of course depending on i and j. Moreover, from now on we only focus on the unweighted case.
We can now prove our main comparison theorem.
Theorem 2. Given an unweighted graph G, for any edge i ∼j we have κ(i, j) ≥Ric(i, j).
Proof. We stick to the aforementioned convention: di := d ≤dj = d + s, s ≥0. The strategy of the
proof amounts to ﬁnding a transportation plan providing an upper bound for W1(µα
i , µα
j ) and hence
a lower bound for the curvature κ(i, j). In particular, we consider plans moving the mass µα
i from
B1(i) to B1(j).
If d = 1, then the optimal transport plan consists of moving the mass α from i to j and the remaining
mass 1 −α on j to S1(j). This yields a unit Wasserstein distance between µα
i and µα
j and hence zero
Ollivier curvature κ(i, j), which coincides with the value of balanced Forman Ric(i, j).
Assume now that d ≥2. A (possibly non-optimal) transport plan from µα
i to µα
j is given by:
(i) Move mass (1 −α)/(d + s) from each node k ∈♯m
□⊂♯i
□to its unique image in ♯j
□under a
bijection ϕ as per deﬁnition of ♯m
□.
(ii) The remaining mass on each node k ∈♯m
□will need to travel by at most distance 3 to S1(j).
(iii) The extra-mass (1 −α)(1/d −1/(d + s)) on each common neighbour k ∈♯∆will need to
travel by at most distance 2 to S1(j).
(iv) Move the mass (1 −α)/d from j to S1(j).
(v) Move the mass α from i to j. This leaves left-over mass (1 −α)/(d + s) at i from the
distribution µα
j . This mass can be compensated from mass in S1(i) which is at distance one.
(vi) Finally, we move the mass (1 −α)/d of any untouched node in S1(i) to S1(j) along
a path of length lesser or equal than three. Note that the remaining mass is equal to
((1 −α)/d)(d −1 −|♯∆| −|♯m
□|) −(1 −α)(d + s), where the last terms comes from (v).
If we sum all the contributions we ﬁnd
W1(µα
i , µα
j ) ≤(1 −α)
 |♯m
□|
d + s + 3|♯m
□|
1
d −
1
d + s

+ 2|♯∆|
1
d −
1
d + s

+ 1
d

+ α + 1 −α
d + s
+ 3(1 −α)
1
d(d −1 −|♯∆| −|♯m
□|) −
1
d + s

= (1 −α)

1
d + s (−2|♯∆| −2|♯m
□| −2) + 1
d (−|♯∆| −2) + 2

+ 1
= (1 −α)

1
d + s

−3|♯∆| −s
d|♯∆| −2|♯m
□| −4 −2s
d + 2(d + s)

+ 1.
Therefore we have
κ(i, j) = lim
α→1
1 −W(µα
i , µα
j )
1 −α
≥

1
d + s

3|♯∆| + s
d|♯∆| + 2|♯m
□| + 4 + 2s
d −2(d + s)

.
19

Published as a conference paper at ICLR 2022
By using Lemma 11, we can bound the right hand side as
κ(i, j) ≥

1
d + s

3|♯∆| + s
d|♯∆| + (γmax)−1(|♯i
□| + |♯j
□|) + 4 + 2s
d −2(d + s)

= Ric(i, j)
which completes the proof.
Remark 13. By inspection Ric(i, j) ≥Φ(i, j), with Φ(i, j) as in Theorem 12. We have three cases:
(i) Φ(i, j) = 2/d + 2/(d + s) −2 + 2|♯∆|/(d + s) + |♯∆|/d ≤Ric(i, j), because Ric(i, j)
takes into account the positive contribution of 4-cycles as well.
(ii) Φ(i, j) = −1 + 1/d + 1/(d + s) + 2|♯∆|/(d + s), which happens iff
d + s −2 −s
d −|♯∆| > 0
and
d + s −2 −s
d −|♯∆| −s
d|♯∆| ≤0.
From the previous inequalities we derive
Ric(i, j) ≥Φ(i, j) +
1
d + s

2 −d −s + |♯∆| + s
d|♯∆| + s
d

≥Φ(i, j).
(iii) Φ(i, j) = |♯∆|/(d + s) which is equivalent to
d + s −2 −s
d −|♯∆| ≤0.
In this case we have
Ric(i, j) ≥Φ(i, j) + s
d
|♯∆|
d + s
Corollary 3. If Ric(i, j) ≥k > 0 for any edge i ∼j, then there exists a polynomial P such that
|Br(i)| ≤P(r),
∀i ∈V.
Proof. This follows immediately from Theorem 2 and a Bishop-Gromov type of result for discrete
Ollivier curvature on graphs Paeng (2012).
To address the proof of Theorem 4, we ﬁrst need the Lemma below.
Lemma 14. Given i ∼j, with di ≤dj, if Ric(i, j) ≤−2 + δ, for some 0 < δ < (1 + γmax)−1, then
|Qj|
|♯∆| + 1 > δ−1.
Proof. According to our convention we let di = d and dj = d + s, for s ≥0. We also recall that
Qj = S1(j)\(♯∆∪♯j
□∪{i}). If we multiply equation 3 by dj = d+s, we see that Ric(i, j) ≤−2+δ
iff
4 + 2s
d + 3|♯∆| + s
d|♯∆| + γ−1
max(|♯i
□| + |♯j
□|) ≤δ(1 + |♯∆| + |♯j
□| + |Qj|).
By assumption |♯j
□|(γ−1
max −δ) ≥0, meaning that
δ(1 + |♯∆| + |Qj|) ≥4 + 2s
d + 3|♯∆| + s
d|♯∆| ≥4 + 3|♯∆|.
Therefore, we conclude
|Qj|
|♯∆| + 1 ≥3
δ −1.
Theorem 4. Consider a MPNN as in equation 1. Let i ∼j with di ≤dj and assume that:
20

Published as a conference paper at ICLR 2022
(i) |∇φℓ| ≤α and |∇ψℓ| ≤β for each 0 ≤ℓ≤L −1, with L ≥2 the depth of the MPNN.
(ii) There exists δ s.t. 0 < δ < (max{di, dj})−1
2 , δ < γ−1
max, and Ric(i, j) ≤−2 + δ.
Then there exists Qj ⊂S2(i) satisfying |Qj| > δ−1 and for 0 ≤ℓ0 ≤L −2 we have
1
|Qj|
X
k∈Qj

∂h(ℓ0+2)
k
∂h(ℓ0)
i
 < (αβ)2δ
1
4 .
(4)
Proof. As usual we let di := d and dj := d + s, for some s ≥0. We ﬁrst observe that from the
requirement δ2(d + s) ≤1 in (ii), we derive Ric(i, j) ≤−2 + δ iff
4 + 2s
d + 3|♯∆| + s
d|♯∆| + γ−1
max(|♯i
□| + |♯j
□|) ≤δ(d + s).
Therefore we have
δ|♯∆|

3 + s
d

≤δ2(d + s),
meaning that
δ|♯∆| ≤1.
(10)
From now on we let Qj denote again the complement S1(j) \ (S1(i) ∪♯j
□∪{i}). Without loss of
generality we set ℓ0 = 0 and hence h(0)
i
= xi; the very same proof applies to any other choice of ℓ0.
Given k ∈Qj, since k ∈S2(i), we can apply Lemma 1 and derive

∂h(2)
k
∂xi
 ≤(αβ)2( ˆA)2
ik.
(11)
We may expand the power of the augmented normalized adjacency matrix as
( ˆA)2
ik =
1
p
(dk + 1)(di + 1)
X
w∈S1(k)∩S1(i)
1
dw + 1.
If we introduce the set ˆQj = {k ∈Qj : σik > 1}, we can then write
X
k∈Qj
( ˆA)2
ik =
X
k∈Qj
1
p
(dk + 1)(di + 1)
X
w∈S1(k)∩S1(i)
1
dw + 1 =
=
1
√di + 1

X
k∈Qj
1
√dk + 1
1
dj + 1 +
X
k∈ˆ
Qj
1
√dk + 1
X
w∈S1(k)∩S1(i)∩S1(j)
1
dw + 1


(12)
where in the last equality we have again used the fact that k ∈ˆQj iff there is w ∈S1(k)∩S1(i)∩S1(j).
To avoid heavy notations, we introduce Vk := S1(k) ∩S1(i) ∩S1(j). Let us ﬁrst focus on the ﬁrst
sum in equation 12. We have
1
√di + 1
X
k∈Qj
1
√dk + 1
1
dj + 1 ≤
1
√di + 1|Qj|
1
dj + 1 ≤
1
√di + 1 ≤1.
(13)
We now consider the second sum in equation 12. We assume |♯∆| ≥1, otherwise ˆQj = ∅. We let
Ω:=
(
w ∈♯∆: dw < 1
C
| ˆQj|
|♯∆| + 2
C
)
for some C > 0 to be chosen below. Then, the second sum in equation 12 can be split as
X
k∈ˆ
Qj
1
√dk + 1


X
w∈Vk∩Ω
1
dw + 1 +
X
w∈Vk\Ω
1
dw + 1

.
(14)
21

Published as a conference paper at ICLR 2022
Since any w ∈Vk has degree at least three, we can bound the ﬁrst term in equation 14 as
X
k∈ˆ
Qj
1
√dk + 1
 
X
w∈Vk∩Ω
1
dw + 1
!
≤
X
k∈ˆ
Qj
1
√dk + 1
|Vk ∩Ω|
4
.
We now observe that
X
k∈ˆ
Qj
|Vk ∩Ω|
√dk + 1 ≤
X
k∈ˆ
Qj
|Vk ∩Ω| =

n
(k, w) ∈E : k ∈ˆQj, w ∈Vk ∩Ω
o ≤

max
w∈Ωdw

|Ω|.
Since dw ≤(1/C)| ˆQj|/|♯∆| + 2/C for any w ∈Ωwe see that the ﬁrst term in equation 14 can be
bounded by
X
k∈ˆ
Qj
1
√dk + 1
X
w∈Vk∩Ω
1
dw + 1 ≤
X
k∈ˆ
Qj
1
√dk + 1
|Vk ∩Ω|
4
≤
 
1
C
| ˆQj|
|♯∆| + 2
C
!
|Ω|
4 ≤
 
1
C
| ˆQj|
|♯∆| + 2
C
!
|♯∆|
4 .
(15)
by deﬁnition of Ω. We now bound the second term in equation 14 as
X
k∈ˆ
Qj
1
√dk + 1
X
w∈Vk\Ω
1
dw + 1 ≤
X
k∈ˆ
Qj
1
√dk + 1
C|♯∆|
| ˆQj|
|Vk \ Ω|
where we have used that d−1
w ≤C|♯∆|/| ˆQj| if w ∈Vk \ Ω. Since
|Vk \ Ω|
√dk + 1 ≤
|Vk|
p
|S1(k)|
≤
|Vk|
p
|Vk|
≤
p
|Vk| ≤
p
|♯∆|
we see that
X
k∈ˆ
Qj
1
√dk + 1
C|♯∆|
| ˆQj|
|Vk \ Ω| ≤C|♯∆|
| ˆQj|
p
|♯∆|| ˆQj| = C|♯∆|
3
2 .
(16)
We are now ready to complete the proof of the theorem. According to equation 11 it sufﬁces to show
that
1
|Qj|
X
k∈Qj
( ˆA2)ik ≤δ1/4.
From equation 12 and equation 13 we derive that the left hand side of the equation above is bounded
by
1
|Qj|
X
k∈Qj
( ˆA2)ik ≤
1
|Qj| +
1
|Qj|

X
k∈ˆ
Qj
1
√dk + 1
X
w∈Vk
1
dw + 1


≤δ +
1
|Qj|

X
k∈ˆ
Qj
1
√dk + 1
X
w∈Vk
1
dw + 1


where in the last inequality we have used Lemma 14 to bound |Qj|−1 by δ. In particular we note that
if ♯∆= ∅then ˆQj = ∅, and the bound would be simply controlled by δ as claimed. When |♯∆| > 0,
we can use equation 15 and equation 16 to estimate the second term from above by
1
|Qj|

X
k∈ˆ
Qj
1
√dk + 1
X
w∈Vk
1
dw + 1

≤
1
|Qj|
  
1
C
| ˆQj|
|♯∆| + 2
C
!
|♯∆|
4
!
+
1
|Qj|

C|♯∆|
3
2

≤1
4
 1
C + 2|♯∆|
C|Qj|

+ C|♯∆|
|Qj|
p
|♯∆|.
22

Published as a conference paper at ICLR 2022
By applying Lemma 14 we get
1
4
 1
C + 2|♯∆|
C|Qj|

+ C|♯∆|
|Qj|
p
|♯∆| ≤1
4
 1
C + 2 δ
C

+ Cδ
p
|♯∆|.
We now choose C = δ−1/4, so that the previous quantity can be bounded by
1
4
 1
C + 2 δ
C

+ Cδ
p
|♯∆| ≤1
4

δ
1
4 + 2δ
5
4

+ δ
1
4 p
δ|♯∆| ≤1
4

δ
1
4 + 2δ
5
4

+ δ
1
4
where in the last inequality we have used equation 10. Therefore, we have shown that
1
|Qj|
X
k∈Qj
( ˆA2)ik ≤δ + 1
4

δ
1
4 + 2δ
5
4

+ δ
1
4 ≤3δ
1
4
where we have used that δ < 1. This completes the proof (once we absorb the extra factor 3 in the
constant αβ in equation 11).
Remark 15. The requirement δ
p
max{di, dj} < 1 can be replaced by a more general bound
δ
p
max{di, dj} < r. The argument above extends to this case up to renaming the constant αβ in
the statement so to include an extra factor r.
We note that the condition δ max{di, dj} < r would be stronger than the one appearing in (ii) of
Theorem 4. In this regard, we recall that for a d-tree the curvature satisﬁes Ric(i, j) = −2 + 4
d.
We can also prove Proposition 5:
Proposition 5. If Ric(i, j) ≥k > 0 for all i ∼j, then λ1/2 ≥hG ≥k
2.
Proof. This follows as an immediate Corollary of Theorem 2 and (Lin et al., 2011, Theorem 4.2).
Betweenness centrality to measure bottleneck.
In equation 2 we have derived how the topology
of the graph affects the dependence of the hidden node representation h(r+1)
i
on the input feature xs,
for nodes i, s at distance r + 1. We note that in this case ˆAr+1
is
is exactly measuring the number of
minimal paths from i to s. If the receptive ﬁeld Br+1(i) is a binary tree, then we have seen that the
entry of the power matrix decays exponentially. The reason for such decay stems from the existence
of exponentially many nodes in the receptive ﬁeld combined with the lack of multiple minimal paths
(shortcuts). When such conditions hold, most of the minimal paths go through the same nodes,
which is exactly what happens for the tree where each node is in the minimal paths between different
branches. Since the frequency in which a node appears in the minimal path of distinct pairs of nodes
is measured by the betweenness centrality Freeman (1977), we propose a topological characterization
of the ‘bottleneckedness’ of a graph as follows:
Deﬁnition 9 (bottleneck). The bottleneck-value of G is bG := 1
n
Pn
i=1 b(i), where b(i) denotes the
betweenness centrality on node i.
From a standard combinatorial argument it follows that if G is connected, then
bG = 1
n
X
i,j
(dG(i, j) −1) .
(17)
We note that bG = 0 iff G is the complete graph Kn. Therefore, bG determines how far the given
topology is from Kn, with the latter representing the limit case of a fully connected layer Alon &
Yahav (2021) where no bottleneck may occur as any pair of nodes would be neighbours. This further
supports our intuition that the betweenness centrality is a good topological candidate for providing a
global measurement of bottleneckedness in the graph.
It also follows from equation 17 that any update to the graph topology consisting of edge additions
would decrease bG and thus reduce the bottleneck. The quantity bG is global in nature though and
hence lacks robustness. As a pedagogical example, consider a barbell G(m, 2r + 1), with m the size
of the two cliques joined by a path of length 2r + 1 and focus on the edge i ∼j in the middle of
such path. Nodes i and j are central to the graph, in the sense that most minimal paths go through
them and indeed their betweenness centrality is b(i) = b(j) = (m + r)2 + (m + r). If now we add a
23

Published as a conference paper at ICLR 2022
single edge joining the two cliques Km, the values b(i) and b(j) decrease dramatically by Ω(m2 + r).
Since the operation is non-local, we see that the representation h(ℓ)
i
of any MPNN is unaffected by
the edge addition for any ℓ∈(0, r), and similarly for j. Eventually, if we keep adding edges, the
receptive ﬁelds Bs(i) will be affected for small values of s as well: the drawback of such approach
is that the resulting adjacency may be signiﬁcantly different. Conversely, the curvature provides a
more precise, local and hence robust way of controlling the bottleneck and hence the over-squashing
problem. Nonetheless, we relate the betweenness centrality to the Jacobian of the hidden features.
Theorem 16. Given i ∼j, let Ωj := S1(i) ∩S1(j) ∪{j}. If Ric(i, j) ≤−2 + δ, for 0 < δ <
(1 + γmax)−1, then
1
|Ωj|
X
k∈Ωj
b(k) ≥δ−1.
Proof. We rewrite the quantity in the statement as
1
|♯∆| + 1

X
k∈♯∆
b(k) + b(j)

.
By deﬁnition, given a node k ∈V , the betweenness centrality of k is given by
b(k) :=
X
s,t∈V :s̸=k,t̸=k
σst(k)
σst
where σst is the number of minimal paths between s and t while σst(k) is the number of minimal
paths from s to t passing through k. For convenience, we introduce the set ˆ
Qj ⊂Qj deﬁned by
ˆQj := {w ∈Qj : σiw > 1}.
Equivalently, ˆQj consists of those nodes in S1(j) \ S1(i) which form a 4-cycle based at i ∼j with
a diagonal inside. Indeed, if w ∈Qj and σiw > 1, then there exists more than one minimal path
between i and w, in addition to the one passing through node j. For any such path there exists
k ∈S1(i) ∩S1(w). Since w ∈Qj and Qj ∩♯j
□= ∅, we derive that k ∈S1(j) as well. We then get
X
k∈♯∆
b(k) =
X
k∈♯∆
X
s,t∈V :s̸=k,t̸=k
σst(k)
σst
≥
X
k∈♯∆
X
w∈ˆ
Qj
σiw(k)
σiw
=
X
w∈ˆ
Qj
1
σiw
X
k∈♯∆
σiw(k).
By summing σiw(k) for all k ∈♯∆we obtain all the 2-long minimal paths between i and w with the
exception of the one passing through j:
X
k∈♯∆
b(k) ≥
X
w∈ˆ
Qj
1
σiw
(σiw −1).
(18)
On the other hand we also have
b(j) =
X
s,t∈V :s̸=j,t̸=j
σst(j)
σst
≥
X
z∈Qj
σiz(j)
σiz
=
X
z∈Qj
1
σiz
.
(19)
By combining equation 18 and equation 19 we ﬁnally get
1
|♯∆| + 1

X
k∈♯∆
b(k) + b(j)

≥
1
|♯∆| + 1

X
w∈ˆ
Qj
1
σiw
(σiw −1) +
X
z∈Qj
1
σiz


=
1
|♯∆| + 1

| ˆQj| +
X
z∈Qj\ ˆ
Qj
1
σiz


=
|Qj|
|♯∆| + 1,
where in the last equality we have used that by deﬁnition σiz = 1 for all z ∈Qj \ ˆQj. By Lemma 14
the last quantity is larger than δ−1.
24

Published as a conference paper at ICLR 2022
E
PROOFS OF RESULTS IN SECTION 4
Theorem 6. Let S ⊂V with vol(S) ≤vol(G)/2. Then hS,α ≤
  1−α
α
 davg(S)
dmin(S) hS, where davg(S)
and dmin(S) are the average and minimum degree on S, respectively.
Proof. Given a signal f : V →R on the vertex set and U ⊂V , analogously to Chung (2007), we
introduce the notation
f(U) :=
X
i∈U
f(i).
Let us rewrite the new Cheeger constant hS,α as follows:
hS,α = 1
|S|
X
i∈S,j∈¯S
(Rα)ij = 1
|S|χSRα( ¯S)
with χS the characteristic function of the subset S, i.e. χS(i) = 1 iff i ∈S. Since the graph G is
connected, we can bound hS,α from above as
1
|S|χSRα( ¯S) = 1
|S|
X
i∈S,j∈¯S
(Rα)ij ≤1
|S|
X
i∈S,j∈¯S
(Rα)ij
di
dmin(S) = 1
|S|χSDRα( ¯S)
1
dmin(S).
It was proven in (Chung, 2007, Lemma 5) that
χSDRα( ¯S) ≤1 −α
α
|∂S|.
(20)
By applying equation equation 20 to the bound for the Cheeger constant hS,α we ﬁnally see that
hS,α = 1
|S|χSRα( ¯S) ≤1
|S|χSDRα( ¯S)
1
dmin(S) ≤1
|S|
1 −α
α
|∂S|
1
dmin(S)
= 1
|S|
1 −α
α
hSvol(S)
1
dmin(S) = 1 −α
α
hS
davg (S)
dmin(S) .
We also report an equivalent result, again relying on (Chung, 2007, Lemma 5).
Proposition 17. Let S ⊂V with vol(S) ≤vol(G)/2. For any k ∈N, there exists Sk,α ⊂S with
vol(Sk,α) ≥vol(S)(1 −(2k)−1) such that
X
j∈¯S
(Rα)ij ≤k 1 −α
α
hS,
for all i ∈Sk,α.
Proof. Let k ∈N. By modifying slightly the argument in (Chung, 2007, Lemma 5), we derive that
S′
k,α := {i ∈S : χiRα( ¯S) ≥k 1 −α
α
hS}
satisﬁes
1 −α
2α |∂S| ≥vol(S′
k,α)k 1 −α
α
hS.
Therefore, we obtain
vol(S′
k,α) ≤1
k
vol(S)
2
.
We then conclude that the complement of S′
k,α has volume greater or equal than vol(S)(1 −(2k)−1),
which completes the proof.
Remark 18. The previous proposition shows that after sparsifying the personalized page rank
operator Rα as suggested in Klicpera et al. (2019) by setting entries below some threshold equal to
zero, there will still be only few edges connecting different communities, once again highlighting that
random-walk based methods are generally not suited to tackle the graph bottleneck.
25

Published as a conference paper at ICLR 2022
F
EXPERIMENTS
Our experiments in this paper are semi-supervised node classiﬁcation (semi-supervised in that the
graph structure provides some unlabelled information) on nine common graph learning datasets.
Cornell, Texas and Wisconsin are small heterophilic datasets based on webpage networks from
the WebKB dataset. Chameleon and Squirrel (Rozemberczki et al., 2021) are medium heterophilic
datasets based on Wikipedia networks, along with Actor, the actor-only induced subgraph of the ﬁlm-
director-actor-writer network (Tang et al., 2009). Cora (McCallum et al., 2000), Citeseer (Sen et al.,
2008) and Pubmed (Namata et al., 2012) are medium homophilic datasets based on citation networks.
As in Klicpera et al. (2019), for all experiments we consider the largest connected component of the
graph.
When splitting the data into train/validation/test sets, we ﬁrst separate the data into a development set
and the test set. This is done once to ensure the test set is not used for any training or hyperparameter
ﬁtting before the ﬁnal evaluation. For each of the 100 random splits the development set is divided
randomly into a train set and a validation set, where we train models on the train set and evaluate
on the validation set. We ﬁt hyperparameters by random search, maximising the mean accuracy
across the validation sets. The accuracy then reported in Table 2 is the mean accuracy on the test set
from models trained on the train sets with the chosen hyperparameters, along with a 95% conﬁdence
interval calculated by bootstrapping the test set accuracies with 1000 samples. For Cora, Citeseer
and Pubmed the development set contains 1500 nodes with the rest kept for the test set, and for each
random split the train set is chosen to contain 20 nodes of each class while the rest form the validation
set. As this is the same method as Klicpera et al. (2019) and we use the same random seeds, we
are using the same test set and expect to have comparable results. For the remaining datasets we
perform a 60/20/20 split, with 20% of nodes set aside as the test set and then for each random split
the remaining 80% is split into 60% training, 20% validation.
The homophily index H(G) proposed by Pei et al. (2019) is deﬁned as
H(G) =
1
|V |
X
v∈V
Number of v’s neighbors who have the same label as v
Number of v’s neighbors
.
(21)
F.1
DATASETS
For datasets with disconnected graphs, the statistics shown here are for the largest connected compo-
nent.
Cornell
Texas
Wisconsin
Chameleon
Squirrel
Actor
Cora
Citeseer
Pubmed
H(G)
0.11
0.06
0.16
0.25
0.22
0.24
0.83
0.72
0.79
Nodes
140
135
184
832
2186
4388
2485
2120
19717
Edges
219
251
362
12355
65224
21907
5069
3679
44324
Features
1703
1703
1703
2323
2089
931
1433
3703
500
Classes
5
5
5
5
5
5
7
6
3
Directed?
Yes
Yes
Yes
Yes
Yes
Yes
No
No
No
26

Published as a conference paper at ICLR 2022
F.2
DEGREE DISTRIBUTIONS
0
1
2
4
8
16
32
64
128
Original
DIGL
SDRF
(a) Cornell:
W1(Original, DIGL) = 10.99
W1(Original, SDRF) = 1.01
0
1
2
4
8
16
32
64
128
Original
DIGL
SDRF
(b) Texas:
W1(Original, DIGL) = 17.98
W1(Original, SDRF) = 0.39
0
1
2
4
8
16
32
64
128
Original
DIGL
SDRF
(c) Wisconsin:
W1(Original, DIGL) = 11.83
W1(Original, SDRF) = 0.28
0
1
2
4
8
16
32
64
128
256
Original
DIGL
SDRF
(d) Chameleon:
W1(Original, DIGL) = 96.30
W1(Original, SDRF) = 1.93
0
1
2
4
8
16
32
64
128
256
512 1024 2048
Original
DIGL
SDRF
(e) Squirrel:
W1(Original, DIGL) = 135.41
W1(Original, SDRF) = 0.50
0
1
2
4
8
16
32
64
128
256
512
1024
Original
DIGL
SDRF
(f) Actor:
W1(Original, DIGL) = 243.81
W1(Original, SDRF) = 1.03
0
1
2
4
8
16
32
64
128
256
512
1024
Original
DIGL
SDRF
(g) Cora:
W1(Original, DIGL) = 247.84
W1(Original, SDRF) = 0.14
0
1
2
4
8
16
32
64
128
256
512
1024
Original
DIGL
SDRF
(h) Citeseer:
W1(Original, DIGL) = 178.28
W1(Original, SDRF) = 0.15
0
1
2
4
8
16
32
64
128
256
512 1024 2048
Original
DIGL
SDRF
(i) Pubmed:
W1(Original, DIGL) = 247.01
W1(Original, SDRF) = 0.03
Figure 5: Comparing the degree distribution of the original graphs to the preprocessed version. The x-
axis is node degree in log2 scale, and the plots are a kernel density estimate of the degree distribution.
In the captions we see the Wasserstein distance W1 between the original and preprocessed graphs.
27

Published as a conference paper at ICLR 2022
F.3
VISUALIZING CURVATURE AND SENSITIVITY TO FEATURES
Figure 6: Rewiring of the Cornell graph. Left-to-right: original graph, DIGL, and SDRF rewiring.
Edges are colored by curvature; nodes are colored by the maximum absolute entry of a trained 2-layer
GCN’s Jacobian between the GCN’s prediction for that node and the features of the nodes 2 hops
away in the original graph. SDRF homogenizes curvature and so lifts the upper bound on the Jacobian
from Theorem 4. DIGL also does to an extent, though at the expense of preserving graph topology.
F.4
HYPERPARAMETERS
Table 4: Hyperparameters for GCN with no preprocessing (None).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
Cornell
0.3060
1
128
0.0082
0.1570
Texas
0.2346
1
128
0.0072
0.0037
Wisconsin
0.2869
1
64
0.0281
0.0113
Chameleon
0.7304
3
128
0.0248
0.0936
Squirrel
0.5974
2
64
0.0136
0.1346
Actor
0.7605
1
64
0.0290
0.0619
Cora
0.4144
1
64
0.0097
0.0639
Citeseer
0.7477
1
128
0.0251
0.4577
Pubmed
0.4013
1
64
0.0095
0.0448
Table 5: Hyperparameters for GCN with the input graph made undirected (Undirected).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
Cornell
0.6910
1
64
0.0185
0.0285
Texas
0.2665
1
128
0.0069
0.0035
Wisconsin
0.2893
2
128
0.0142
0.0001
Chameleon
0.4657
3
64
0.0189
0.0423
Squirrel
0.5944
2
64
0.0081
0.0309
Actor
0.6626
2
64
0.0195
0.0219
28

Published as a conference paper at ICLR 2022
Table 6: Hyperparameters for GCN with the last layer made fully connected (+FA from Alon &
Yahav (2021)).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
Cornell
0.2643
1
128
0.0216
0.0760
Texas
0.2207
1
128
0.0102
0.4450
Wisconsin
0.2613
3
64
0.0057
0.0131
Chameleon
0.7783
3
64
0.0156
0.0108
Squirrel
0.3654
3
64
0.0077
0.1922
Actor
0.3824
1
64
0.0165
0.1168
Cora
0.7840
2
128
0.0149
0.1429
Citeseer
0.5460
2
64
0.0066
0.0758
Pubmed
0.3376
2
128
0.0204
0.0215
Table 7: Hyperparameters for GCN with DIGL preprocessing, or Graph Diffusion Convolution with
PPR kernel (DIGL). Descriptions for α, k and ϵ can be found in Klicpera et al. (2019).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
α
k
ϵ
Cornell
0.6294
1
64
0.0134
0.0258
0.1795
64
-
Texas
0.2382
2
128
0.0063
0.0153
0.0206
32
-
Wisconsin
0.2941
1
128
0.0083
0.0226
0.1246
-
0.0001
Chameleon
0.4191
1
128
0.0052
0.0001
0.0244
64
-
Squirrel
0.6844
1
128
0.0056
0.4537
0.0395
32
-
Actor
0.7820
1
64
0.0170
0.0102
0.1584
128
-
Cora
0.3315
1
64
0.0284
0.0572
0.0773
128
-
Citeseer
0.5561
1
64
0.0094
0.5013
0.1076
-
0.0008
Pubmed
0.4915
2
128
0.0057
0.0597
0.1155
128
-
Table 8: Hyperparameters for GCN with DIGL preprocessing followed by symmetrizing the graph
diffusion matrix (DIGL + Undirected).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
α
k
ϵ
Cornell
0.6294
1
64
0.0134
0.0258
0.1795
64
-
Texas
0.2382
2
128
0.0063
0.0153
0.0206
32
-
Wisconsin
0.2941
1
128
0.0083
0.0226
0.1246
-
0.0001
Chameleon
0.4191
1
128
0.0052
0.0001
0.0244
64
-
Squirrel
0.7094
1
64
0.0172
0.0192
0.1610
64
-
Actor
0.4012
1
64
0.0161
0.0141
0.0706
-
0.0016
Cora
0.3315
1
64
0.0284
0.0572
0.0773
128
-
Citeseer
0.5561
1
64
0.0094
0.5013
0.1076
-
0.0008
Pubmed
0.4915
2
128
0.0057
0.0597
0.1155
128
-
29

Published as a conference paper at ICLR 2022
Table 9: Hyperparameters for GCN with SDRF preprocessing (SDRF). Max iterations, τ and C+ are
the SDRF parameters described in Algorithm 1.
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
Max
iterations
τ
C+
Cornell
0.2411
1
128
0.0172
0.0125
135
130
0.25
Texas
0.5954
1
128
0.0278
0.0623
47
172
2.25
Wisconsin
0.6033
1
128
0.0295
0.1920
27
32
0.5
Chameleon
0.7265
1
128
0.0180
0.2101
832
77
3.35
Squirrel
0.7401
2
16
0.0189
0.2255
6157
178
0.5
Actor
0.6886
1
128
0.0095
0.0727
1010
69
1.22
Cora
0.3396
1
128
0.0244
0.1076
100
163
0.95
Citeseer
0.4103
1
64
0.0199
0.4551
84
180
0.22
Pubmed
0.3749
3
128
0.0112
0.0138
166
115
14.43
Table 10: Hyperparameters for GCN with the input graph made undirected followed by SDRF
preprocessing (SDRF + Undirected).
Dataset
Dropout
Hidden
depth
Hidden
dimension
Learning
rate
Weight
decay
Max
iterations
τ
C+
Cornell
0.2911
1
128
0.0056
0.0336
126
145
0.88
Texas
0.2160
1
64
0.0229
0.0137
89
22
1.64
Wisconsin
0.2452
1
64
0.0113
0.1559
136
12
7.95
Chameleon
0.4886
1
32
0.0268
0.4056
2441
252
2.84
Squirrel
0.4249
1
64
0.0295
0.1397
787
43
17.19
Actor
0.6705
1
128
0.0115
0.0447
1141
44
11.17
G
HARDWARE SPECIFICATIONS
Our experiments were performed on a server with the following speciﬁcations:
Component
Speciﬁcation
Architecture
x86_64
CPU
40x Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz
GPU
4x GeForce RTX 3090 (24268MiB/GPU)
RAM
126GB
OS
Ubuntu 20.04.2 LTS
30

