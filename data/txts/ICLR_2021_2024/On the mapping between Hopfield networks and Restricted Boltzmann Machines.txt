Published as a conference paper at ICLR 2021
ON THE MAPPING BETWEEN HOPFIELD NETWORKS
AND RESTRICTED BOLTZMANN MACHINES
Matthew Smart
Department of Physics
University of Toronto
msmart@physics.utoronto.ca
Anton Zilman
Department of Physics
and Institute for Biomedical Engingeering
University of Toronto
zilmana@physics.utoronto.ca
ABSTRACT
Hopﬁeld networks (HNs) and Restricted Boltzmann Machines (RBMs) are two
important models at the interface of statistical physics, machine learning, and neu-
roscience. Recently, there has been interest in the relationship between HNs and
RBMs, due to their similarity under the statistical mechanics formalism. An exact
mapping between HNs and RBMs has been previously noted for the special case
of orthogonal (“uncorrelated”) encoded patterns. We present here an exact map-
ping in the case of correlated pattern HNs, which are more broadly applicable to
existing datasets. Speciﬁcally, we show that any HN with N binary variables and
p < N arbitrary binary patterns can be transformed into an RBM with N binary
visible variables and p gaussian hidden variables. We outline the conditions under
which the reverse mapping exists, and conduct experiments on the MNIST dataset
which suggest the mapping provides a useful initialization to the RBM weights.
We discuss extensions, the potential importance of this correspondence for the
training of RBMs, and for understanding the performance of deep architectures
which utilize RBMs.
1
INTRODUCTION
Hopﬁeld networks (HNs) (Hopﬁeld, 1982; Amit, 1989) are a classical neural network architecture
that can store prescribed patterns as ﬁxed-point attractors of a dynamical system. In their standard
formulation with binary valued units, HNs can be regarded as spin glasses with pairwise interactions
Jij that are fully determined by the patterns to be encoded. HNs have been extensively studied in
the statistical mechanics literature (e.g. (Kanter & Sompolinsky, 1987; Amit et al., 1985)), where
they can be seen as an interpolation between the ferromagnetic Ising model (p = 1 pattern) and
the Sherrington-Kirkpatrick spin glass model (many random patterns) (Kirkpatrick & Sherrington,
1978; Barra & Guerra, 2008). By encoding patterns as dynamical attractors which are robust to
perturbations, HNs provide an elegant solution to pattern recognition and classiﬁcation tasks. They
are considered the prototypical attractor neural network, and are the historical precursor to modern
recurrent neural networks.
Concurrently, spin glasses have been used extensively in the historical machine learning literature
where they comprise a sub-class of “Boltzmann machines” (BMs) (Ackley et al., 1985). Given a
collection of data samples drawn from a data distribution, one is generally interested in “training”
a BM by tuning its weights Jij such that its equilibrium distribution can reproduce the data distri-
bution as closely as possible (Hinton, 2012). The resulting optimization problem is dramatically
simpliﬁed when the network has a two-layer structure where each layer has no self-interactions, so
that there are only inter-layer connections (Hinton, 2012) (see Fig. 1). This architecture is known as
a Restricted Boltzmann Machine (RBM), and the two layers are sometimes called the visible layer
and the hidden layer. The visible layer characteristics (dimension, type of units) are determined by
the training data, whereas the hidden layer can have binary or continuous units and the dimension
is chosen somewhat arbitrarily. In addition to generative modelling, RBMs and their multi-layer
extensions have been used for a variety of learning tasks, such as classiﬁcation, feature extraction,
and dimension reduction (e.g. Salakhutdinov et al. (2007); Hinton & Salakhutdinov (2006)).
1

Published as a conference paper at ICLR 2021
There has been extensive interest in the relationship between HNs and RBMs, as both are built
on the Ising model formalism and fulﬁll similar roles, with the aim of better understanding RBM
behaviour and potentially improving performance. Various results in this area have been recently
reviewed (Marullo & Agliari, 2021). In particular, an exact mapping between HNs and RBMs has
been previously noted for the special case of uncorrelated (orthogonal) patterns (Barra et al., 2012).
Several related models have since been studied (Agliari et al., 2013; M´ezard, 2017), which partially
relax the uncorrelated pattern constraint. However, the patterns observed in most real datasets exhibit
signiﬁcant correlations, precluding the use of these approaches.
In this paper, we demonstrate exact correspondence between HNs and RBMs in the case of corre-
lated pattern HNs. Speciﬁcally, we show that any HN with N binary units and p < N arbitrary
(i.e. non-orthogonal) binary patterns encoded via the projection rule (Kanter & Sompolinsky, 1987;
Personnaz et al., 1986), can be transformed into an RBM with N binary and p gaussian variables.
We then characterize when the reverse map from RBMs to HNs can be made. We consider a practi-
cal example using the mapping, and discuss the potential importance of this correspondence for the
training and interpretability of RBMs.
2
RESULTS
We ﬁrst introduce the classical solution to the problem of encoding N-dimensional binary {−1, +1}
vectors {ξµ}p
µ=1, termed “patterns”, as global minima of a pairwise spin glass H(s) = −1
2sT Js.
This is often framed as a pattern retrieval problem, where the goal is to specify or learn Jij such
that an energy-decreasing update rule for H(s) converges to the patterns (i.e. they are stable ﬁxed
points). Consider the N × p matrix ξ with the p patterns as its columns. Then the classical prescrip-
tion known as the projection rule (or pseudo-inverse rule) (Kanter & Sompolinsky, 1987; Personnaz
et al., 1986), J = ξ(ξT ξ)−1ξT , guarantees that the p patterns will be global minima of H(s). This
resulting spin model is commonly called a (projection) Hopﬁeld network, and has the Hamiltonian
H(s) = −1
2sT ξ(ξT ξ)−1ξT s.
(1)
Note that ξT ξ invertibility is guaranteed as long as the patterns are linearly independent (we there-
fore require p ≤N). Also note that in the special (rare) case of orthogonal patterns ξµ · ξν = Nδµν
(also called “uncorrelated”), studied in the previous work (Barra et al., 2012), one has ξT ξ = NI
and so the pseudo-inverse interactions reduce to the well-known Hebbian form J =
1
N ξξT (the
properties of which are studied extensively in Amit et al. (1985)). Additional details on the pro-
jection HN Eq. (1) are provided in Appendix A. To make progress in analyzing Eq. (1), we ﬁrst
consider a transformation of ξ which eliminates the inverse factor.
2.1
MAPPING A HOPFIELD NETWORK TO A RESTRICTED BOLTZMANN MACHINE
In order to obtain a more useful representation of the quadratic form Eq. (1) (for our purposes), we
utilize the QR-decomposition (Schott & Stewart, 1999) of ξ to “orthogonalize” the patterns,
ξ = QR,
(2)
with Q ∈RN×p, R ∈Rp×p. The columns of Q are the orthogonalized patterns, and form an
orthonormal basis (of non-binary vectors) for the p-dimensional subspace spanned by the binary
patterns. R is upper triangular, and if its diagonals are held positive then Q and R are both unique
(Schott & Stewart, 1999). Note both the order and sign of the columns of ξ are irrelevant for HN
pattern recall, so there are n = 2p · p! possible Q, R pairs. Fixing a pattern ordering, we can use the
orthogonality of Q to re-write the interaction matrix as
J = ξ(ξT ξ)−1ξT = QR(RT R)−1RT QT = QQT
(3)
(the last equality follows from (RT R)−1 = R−1(RT )−1). Eq. (3) resembles the simple Hebbian
rule but with non-binary orthogonal patterns. Deﬁning q ≡QT s in analogy to the classical pattern
overlap parameter m ≡1
N ξT s (Amit et al., 1985), we have
H(s) = −1
2sT QQT s = −1
2q(s) · q(s).
(4)
2

Published as a conference paper at ICLR 2021
Using a Gaussian integral as in Amit et al. (1985); Barra et al. (2012); M´ezard (2017) to transform
(exactly) the partition function Z ≡P
{s} e−βH(s) of Eq. (1), we get
Z =
X
{s}
e
1
2 (βq)T (β−1I)(βq)
=
X
{s}
Z
e−β
2
P
µ λ2
µ+β P
µ λµ
P
i Qiµsi Y
µ
dλµ
p
2π/β
.
(5)
The second line can be seen as the partition function of an expanded Hamiltonian for the N (binary)
original variables {si} and the p (continuous) auxiliary variables {λµ}, i.e.
HRBM({si}, {λµ}) = 1
2
X
µ
λ2
µ −
X
µ
X
i
Qiµsiλµ.
(6)
Note that this is the Hamiltonian of a binary-continuous RBM with inter-layer weights Qiµ. The
original HN is therefore equivalent to an RBM described by Eq. (6) (depicted in Fig. 1). As
mentioned above, there are many RBMs which correspond to the same HN due to the combinatorics
of choosing Q. In fact, instead of QR factorization one can use any decomposition which satisﬁes
J = UU T , with orthogonal U ∈RN×p (see Appendix B), in which case U acts as the RBM
weights. Also note the inclusion of an applied ﬁeld term −P
i bisi in Eq. (1) trivially carries
through the procedure, i.e. ˜HRBM({si}, {λµ}) = 1
2
P
µ λ2
µ −P
i bisi −P
µ
P
i Qiµsiλµ.
Figure 1:
Correspondence between Hop-
ﬁeld Networks (HNs) with correlated pat-
terns and binary-gaussian restricted boltz-
mann machines (RBMs).
The HN has N
binary units and pairwise interactions J de-
ﬁned by p < N (possibly correlated) pat-
terns {ξµ}p
µ=1. The patterns are encoded as
minima of Eq.
(1) through the projection
rule J = ξ(ξT ξ)−1ξT , where ξµ form the
columns of ξ. We orthogonalize the patterns
through a QR decomposition ξ = QR. The
HN is equivalent to an RBM with N binary
visible units and p gaussian hidden units with
inter-layer weights deﬁned as the orthogonal-
ized patterns Qiµ, and Hamiltonian Eq. (6).
See (Agliari et al., 2017) for the analogous
mapping in the uncorrelated case.
Instead of working with the joint form Eq. (6), one could take a different direction from Eq. (5) and
sum out the original variables {si}, i.e.
Z =
Z
e−β
2
P
µ λ2
µ2N Y
i
cosh
 
β
X
µ
Qiµλµ
! Y
µ
dλµ
p
2π/β
.
(7)
This continuous, p-dimensional representation is useful for numerical estimation of Z (Section 3.1).
We may write Eq. (7) as Z =
R
e−F0(λ)dλµ, where
F0({λµ}) = 1
2
X
µ
λ2
µ −1
β
X
i
ln cosh
 
β
X
µ
Qiµλµ
!
.
(8)
Eq. (8) is an approximate Lyapunov function for the mean dynamics of {λµ}; ∇λF0 describes the
effective behaviour of the stochastic dynamics of the N binary variables {si} at temperature β−1.
2.2
COMMENTS ON THE REVERSE MAPPING
With the mapping from HNs (with correlated patterns) to RBMs established, we now consider the
reverse direction. Consider a binary-continuous RBM with inter-layer weights Wiµ which couple a
3

Published as a conference paper at ICLR 2021
visible layer of N binary variables {si} to a hidden layer of p continuous variables {λµ},
H(s, λ) = 1
2
X
µ
λ2
µ −
X
i
bisi −
X
µ
X
i
Wiµsiλµ.
(9)
Here we use W instead of Q for the RBM weights to emphasize that the RBM is not necessarily
an HN. First, following Mehta et al. (2019), we transform the RBM to a BM with binary states by
integrating out the hidden variables. The corresponding Hamiltonian for the visible units alone is
(see Appendix D.1 for details),
˜H(s) = −
X
i
bisi −1
2
X
i
X
j
X
µ
WiµWjµsisj,
(10)
a pairwise Ising model with a particular coupling structure Jij = P
µ WiµWjµ, which in vector
form is
J =
X
µ
wµwT
µ = W W T ,
(11)
where {wµ} are the p columns of W .
In general, this Ising model Eq. (10) produced by integrating out the hidden variables need not
have Hopﬁeld structure (discussed below). However, it automatically does (as noted in Barra et al.
(2012)), in the very special case where Wiµ ∈{−1, +1}. In that case, the binary patterns are simply
{wµ}, so that Eq. (11) represents a Hopﬁeld network with the Hebbian prescription. This situation
is likely rare and may only arise as a by-product of constrained training; for a generically trained
RBM the weights will not be binary. It is therefore interesting to clarify when and how real-valued
RBM interactions W can be associated with HNs.
Approximate binary representation of W : In Section 2.1, we orthogonalized the binary matrix
ξ via the QR decomposition ξ = QR, where Q is an orthogonal (but non-binary) matrix, which
allowed us to map a projection HN (deﬁned by its patterns ξ, Eq. (1)) to an RBM (deﬁned by its
inter-layer weights Q, Eq. (6)).
Here we consider the reverse map. Given a trained RBM with weights W ∈RN×p, we look for an
invertible transformation X ∈Rp×p which binarizes W . We make the mild assumption that W is
rank p. If we ﬁnd such an X, then B = W X will be the Hopﬁeld pattern matrix (analogous to ξ),
with Biµ ∈{−1, +1}.
This is a non-trivial problem, and an exact solution is not guaranteed. As a ﬁrst step to study the
problem, we relax it to that of ﬁnding a matrix X ∈GLp(R) (i.e. invertible, p × p, real) which
minimizes the binarization error
arg min
X∈GLp(R)
||W X −sgn(W X)||F .
(12)
We denote the approximately binary transformation of W via a particular solution X by
Bp = W X.
(13)
We also deﬁne the associated error matrix E ≡Bp −sgn(Bp). We stress that Bp is non-binary and
approximates B ≡sgn(Bp), the columns of which will be HN patterns under certain conditions on
E. We provide an initial characterization and example in Appendix D.
3
EXPERIMENTS ON MNIST DATASET
Next we investigate whether the Hopﬁeld-RBM correspondence can provide an advantage for train-
ing binary-gaussian RBMs. We consider the popular MNIST dataset of handwritten digits (LeCun
et al., 1998) which consists of 28 × 28 images of handwritten images, with greyscale pixel values 0
to 255. We treat the sample images as N ≡784 dimensional binary vectors of {−1, +1} by setting
all non-zero values to +1. The dataset includes M ≡60, 000 training images and 10, 000 testing
images, as well as their class labels µ ∈{0, . . . , 9}.
4

Published as a conference paper at ICLR 2021
3.1
GENERATIVE OBJECTIVE
The primary task for generative models such as RBMs is to reproduce a data distribution. Given
a data distribution pdata, the generative objective is to train a model (here an RBM deﬁned by its
parameters θ), such that the model distribution pθ is as close to pdata as possible. This is often quan-
tiﬁed by the Kullback-Leibler (KL) divergence DKL(pdata∥pθ) = P
s pdata(s) ln

pdata(s)
pθ(s)

. One
generally does not have access to the actual data distribution, instead there is usually a representa-
tive training set S = {sa}M
a=1 sampled from it. As the data distribution is constant with respect to
θ, the generative objective is equivalent to maximizing L(θ) =
1
M
P
a ln pθ(sa).
3.1.1
HOPFIELD RBM SPECIFICATION
With labelled classes of training data, speciﬁcation of an RBM via a one-shot Hopﬁeld rule (“Hop-
ﬁeld RBM”) is straightforward. In the simplest approach, we deﬁne p = 10 representative patterns
via the (binarized) class means
ξµ ≡sgn

1
|Sµ|
X
s∈Sµ
s

.
(14)
where µ ∈{0, . . . , 9} and Sµ is the set of sample images for class µ. These patterns comprise the
columns of the N ×p pattern matrix ξ, which is then orthogonalized as in Eq. (2) to obtain the RBM
weights W which couple N binary visible units to p gaussian hidden units.
We also consider reﬁning this approach by considering sub-classes within each class, representing,
for example, the different ways one might draw a “7”. As a proof of principle, we split each digit
class into k sub-patterns using hierarchical clustering. We found good results with Agglomera-
tive clustering using Ward linkage and Euclidean distance (see Murtagh & Contreras (2012) for an
overview of this and related methods). In this way, we can deﬁne a hierarchy of Hopﬁeld RBMs. At
one end, k = 1, we have our simplest RBM which has p = 10 hidden units and encodes 10 patterns
(using Eq. (14)), one for each digit class. At the other end, 10k/N →1, we can specify increas-
ingly reﬁned RBMs that encode k sub-patterns for each of the 10 digit classes, for a total of p = 10k
patterns and hidden units. This approach has an additional cost of identifying the sub-classes, but is
still typically faster than training the RBM weights directly (discussed below).
The generative performance as a function of k and β is shown in Fig. 2, and increases monotonically
with k in the range plotted. If β is too high (very low temperature) the free energy basins will be
very deep directly at the patterns, and so the model distribution will not capture the diversity of
images from the data. If β is too low (high temperature), there is a “melting transition” where the
original pattern basins disappear entirely, and the data will therefore be poorly modelled. Taking
α = p/N ∼0.1 (roughly k = 8), Fig. 1 of Kanter & Sompolinsky (1987) predicts βm ≈1.5
for the theoretical melting transition for the pattern basins. Interestingly, this is quite close to our
observed peak near β = 2. Note also as k is increased, the generative performance is sustained at
lower temperatures.
Figure 2: Hopﬁeld RBM generative perfor-
mance as a function of β for varying numbers
of encoded sub-patterns k per digit. The num-
ber of hidden units in each RBM is p = 10k,
corresponding to the total number of encoded
patterns. ln Z is computed using annealed im-
portance sampling (AIS) (Neal, 2001) on the
continuous representation of Z, Eq. (7), with
500 chains for 1000 steps (see Appendix E).
Each curve displays the mean of three runs.
In situations where one already has access to the class labels, this approach to obtain RBM weights
is very fast. The class averaging has negligible computational cost O(MN) for the whole training
5

Published as a conference paper at ICLR 2021
set (M samples), and the QR decomposition has a modest complexity of O(Np2) (Schott & Stewart,
1999). Conventional RBM training, discussed below, requires signiﬁcantly more computation.
3.1.2
CONVENTIONAL RBM TRAINING
RBM training is performed through gradient ascent on the log-likelihood of the data, L(θ) =
1
M
P
a ln pθ(sa) (equivalent here to minimizing KL divergence, as mentioned above). We are fo-
cused here on the weights W in order to compare to the Hopﬁeld RBM weights, and so we neglect
the biases on both layers. As is common (Hinton, 2012), we approximate the total gradient by split-
ting the training dataset into “mini-batches”, denoted B. The resulting gradient ascent rule for the
weights is (see Appendix E)
W t+1
iµ
= W t
iµ + η


*X
j
Wjµsa
i sa
j
+
a∈B
−⟨siλµ⟩model

,
(15)
where ⟨siλµ⟩model ≡Z−1 P
{s}
R
siλµe−βH(s,λ) Q
µ dλµ is an average over the model distribution.
The ﬁrst bracketed term of Eq. (15) is simple to calculate at each iteration of the weights. The second
term, however, is intractable as it requires one to calculate the partition function Z. We instead
approximate it using contrastive divergence (CD-K) (Carreira-Perpinan & Hinton, 2005; Hinton,
2012). See Appendix E for details. Each full step of RBM weight updates involves O(KBNp)
operations (Melchior et al., 2017). Training generally involves many mini-batch iterations, such that
the entire dataset is iterated over (one epoch) many times. In our experiments we train for 50 epochs
with mini-batches of size 100 (3 · 105 weight updates), so the overall training time can be extensive
compared to the one-shot Hopﬁeld approach presented above. For further details on RBM training
see e.g. Hinton (2012); Melchior et al. (2017).
In Fig. 3, we give an example of the Hopﬁeld RBM weights (for k = 1), as well as how they evolve
during conventional RBM training. Note Fig. 3(a), (b) appear qualitatively similar, suggesting that
the proposed initialization Q from Eqs. (2), (14) may be near a local optimum of the objective.
0.15
0.10
0.05
0.00
0.05
0.10
0.15
(a) Hopfield initialized (0 epochs)
(b) Hopfield initialized (50 epochs)
Figure 3: Binary-gaussian RBM weights for p = 10 hidden units prior to and during generative
training. (a) Initial values of the columns of W (speciﬁed as the orthogonalized Hopﬁeld patterns
via Eqs. (2), (14)). (b) Same columns of W after 50 epochs of CD-20 training (see Fig. 4(a)).
In Fig. 4(a), (b), we compare conventional RBM training on four different weight initializations: (i)
random Wiµ ∼N(0, 0.01) (purple), commonly used in the literature; (ii) our proposed weights from
the projection rule Hopﬁeld mapping for correlated patterns (blue); (iii) the “Hebbian” Hopﬁeld
mapping described in previous work for uncorrelated patterns, W = N −1/2ξ (Barra et al., 2012)
(green); and (iv) the top p PCA components of the training data (pink). In Fig. 4(c), (d) we compare
generated sample images from two RBMs, each with p = 50 hidden units but different initial
weights (random in (c) and the HN mapping in (d)). The quality of samples in Fig. 4(d) reﬂect the
efﬁcient training of the Hopﬁeld initialized RBM.
Fig. 4(a), (b) show that the Hopﬁeld initialized weights provide an advantage over other approaches
during the early stages of training. The PCA and Hebbian initializations start at much lower values
of the objective and require one or more epochs of training to perform similarly (Fig. 4(a) inset),
while the randomly initialized RBM takes > 25 epochs to reach a similar level. All initializations
ultimately reach the same value. This is noteworthy because the proposed weight initialization is
fast compared to conventional RBM training. PCA performs best for intermediate training times.
6

Published as a conference paper at ICLR 2021
(c) Normal initialization, 15 epochs
(d) Hopfield initialization, 15 epochs
(a) 10 hidden units 
(b) 50 hidden units
Figure 4: Generative performance of binary-gaussian RBMs trained with (a) p = 10 and (b) p = 50
hidden units. Curves are colored according to the choice of weight initialization (see legend in (b),
and further detail in the preceding text). Each curve shows the mean and standard deviation over
5 runs. The inset in (a) details the ﬁrst two epochs. We compute ln pθ as in Fig. 2, but with 100
AIS chains. The learning rate is η0 = 10−4 except the ﬁrst 25 epochs of the randomly initialized
weights in (b), where we used η = 5η0 due to slow training. The mini-batch size is B = 100 for all
curves in (b) and the purple curve in (a), and B = 1000 otherwise. (c), (d) Samples from two RBMs
from (b) (projection HN and random) after 15 epochs, generated by initializing the visible state to
an example image from the desired class and performing 20 RBM updates with β = 2. Training
parameters: β = 2, and CD-20.
Despite being a common choice, the random initialization trains surprisingly slowly, taking roughly
40 epochs in Fig. 4(a), and in Fig. 4(b) we had to increase the basal learning rate η0 = 10−4
by a factor of 5 for the ﬁrst 25 epochs due to slow training. The non-random initializations, by
comparison, arrive at the same maximum value much sooner. The relatively small change over
training for the Hopﬁeld initialized weights supports the idea that they may be near a local optimum
of the objective, and that conventional training may simply be mildly tuning them (Fig. 3).
That the HN initialization performs well at 0 epochs suggests that the p Hopﬁeld patterns concisely
summarize the dataset. This is intuitive, as the projection rule encodes the patterns (and nearby
states) as high probability basins in the free energy landscape of Eq. (1). As the data itself is
clustered near the patterns, these basins should model the true data distribution well. Overall, our
results suggest that the HN correspondence provides a useful initialization for generative modelling
with binary-gaussian RBMs, displaying excellent performance with minimal training.
3.2
CLASSIFICATION OBJECTIVE
As with the generative objective, we ﬁnd that the Hopﬁeld initialization provides an advantage for
classiﬁcation tasks. Here we consider the closely related MNIST classiﬁcation problem. The goal
is to train a model on the MNIST Training dataset which accurately predicts the class of presented
images. The key statistic is the number of misclassiﬁed images on the MNIST Testing dataset.
We found relatively poor classiﬁcation results with the single (large) RBM architecture from the
preceding Section 3.1. Instead, we use a minimal product-of-experts (PoE) architecture as described
in Hinton (2002): the input data is ﬁrst passed to 10 RBMs, one for each class µ. This “layer of
RBMs” functions as a pre-processing layer which maps the high dimensional sample s to a feature
vector f(s) ∈R10. This feature vector is then passed to a logistic regression layer in order to predict
the class of s. The RBM layer and the classiﬁcation layer are trained separately.
The ﬁrst step is to train the RBM layer to produce useful features for classiﬁcation. As in Hinton
(2002), each small RBM is trained to model the distribution of samples from a speciﬁc digit class µ.
We use CD-20 generative training as in Section 3.1, with the caveat that each expert is trained solely
on examples from their respective class. Each RBM connects N binary visible units to k gaussian
hidden units, and becomes an “expert” at generating samples from one class. To focus on the effect
of interlayer weight initialization, we set the layer biases to 0.
After generative training, each expert should have relatively high probability p(µ)
θ (sa) for sample
digits sa of the corresponding class µ, and lower probability for digits from other classes. This idea
is used to deﬁne 10 features, one from each expert, based on the log-probability of a given sample
7

Published as a conference paper at ICLR 2021
under each expert, ln p(µ)
θ (sa) = −βH(µ)(sa)−ln Z(µ). Note that β and ln Z(µ) are constants with
respect to the data and thus irrelevant for classiﬁcation. For a binary-gaussian RBM, H(µ)(sa) has
the simple form Eq. (10), so the features we use are
f (µ)(s) =
X
i
X
j
X
ν
W (µ)
iν W (µ)
jν sisj = ||sT W (µ)||2.
(16)
With the feature map deﬁned, we then train a standard logistic regression classiﬁer (using scikit-
learn (Pedregosa et al., 2011)) on these features. In Fig. 5, we report the classiﬁcation error on the
MNIST Testing set of 10,000 images (held out during both generative and classiﬁcation training).
Note the size p = 10 of the feature vector is independent of the hidden dimension k of each RBM,
so the classiﬁer is very efﬁcient.
Figure 5:
Product-of-experts classiﬁcation
performance for the various weight initializa-
tions. For each digit model (expert), we per-
form CD-20 training according to Eq.
(15)
(as in Fig. 4) for a ﬁxed number of epochs.
A given sample image is mapped to the 10-
dimensional feature vector with elements Eq.
(16). These features are used to train a logistic
regression classiﬁer, and the average MNIST
Test set errors are reported. The initializations
considered for each expert’s weights W (µ)
init ∈
RN×k are Wiµ ∼N(0, 0.01) (purple, dash-
dot), PCA (pink, dashed), and the orthogo-
nalized Hopﬁeld sub-patterns for digit class µ
(blue, solid). The error bars show the min/max
of three runs.
Despite this relatively simple approach, the PoE initialized using the orthogonalized Hopﬁeld pat-
terns (“Hopﬁeld PoE”) performs fairly well (Fig. 5, blue curves), especially as the number of sub-
patterns is increased. We found that generative training beyond 50 epochs did not signiﬁcantly
improve performance for the projection HN or PCA. (in Fig. E.1, we train to 100 epochs and also
display the aforementioned “Hebbian” initial condition, which performs much worse for classiﬁ-
cation). Intuitively, increasing the number of hidden units k increases classiﬁcation performance
independent of weight initialization (with sufﬁcient training).
For k ﬁxed, the Hopﬁeld initialization provides a signiﬁcant beneﬁt to classiﬁcation performance
compared to the randomly initialized weights (purple curves). For few sub-patterns (circles k = 10
and squares k = 20), the Hopﬁeld initialized models perform best without additional training and
until 1 epoch, after which PCA (pink) performs better. When each RBM has k = 100 hidden
features (triangles), the Hopﬁeld and PCA PoE reach 3.0% training error, whereas the randomly
initialized PoE reaches 4.5%. However, the Hopﬁeld PoE performs much better than PCA with
minimal training, and maintains its advantage until 10 epochs, after which they are similar. Inter-
estingly, both the Hopﬁeld and PCA initialized PoE with just k = 10 encoded patterns performs
better than or equal to the k = 100 randomly initialized PoE at each epoch despite having an order
of magnitude fewer trainable parameters. Finally, without any generative training (0 epochs), the
k = 100 Hopﬁeld PoE performs slightly better (4.4%) than the k = 100 randomly initialized PoE
with 50 epochs of training.
4
DISCUSSION
We have presented an explicit, exact mapping from projection rule Hopﬁeld networks to Restricted
Boltzmann Machines with binary visible units and gaussian hidden units. This provides a gener-
alization of previous results which considered uncorrelated patterns (Barra et al., 2012), or special
cases of correlated patterns (Agliari et al., 2013; M´ezard, 2017). We provide an initial characteriza-
tion of the reverse map from RBMs to HNs, along with a matrix-factorization approach to construct
approximate associated HNs when the exact reverse map is not possible. Importantly, our HN to
8

Published as a conference paper at ICLR 2021
RBM mapping can be applied to correlated patterns such as those found in real world datasets. As
a result, we are able to conduct experiments (Section 3) on the MNIST dataset which suggest the
mapping provides several advantages.
The conversion of an HN to an equivalent RBM has practical utility: it trades simplicity of presen-
tation for faster processing. The weight matrix of the RBM is potentially much smaller than the HN
(Np elements instead of N(N −1)/2). More importantly, proper sampling of stochastic trajectories
in HNs requires asynchronous updates of the units, whereas RBM dynamics can be simulated in a
parallelizable, layer-wise fashion. We also utilized the mapping to efﬁciently estimate the partition
function of the Hopﬁeld network (Fig. 2) by summing out the spins after representing it as an RBM.
This mapping also has another practical utility. When used as an RBM weight initialization, the
HN correspondence enables efﬁcient training of generative models (Section 3.1, Fig. 4). RBMs
initialized with random weights and trained for a moderate amount of time perform worse than
RBMs initialized to orthogonalized Hopﬁeld patterns and not trained at all. Further, with mild
training of just a few epochs, Hopﬁeld RBMs outperform conventionally initialized RBMs trained
several times longer. The revealed initialization also shows advantages over alternative non-random
initializations (PCA and the “Hebbian” Hopﬁeld mapping) during early training. By leveraging
this advantage for generative tasks, we show that the correspondence can also be used to improve
classiﬁcation performance (Section 3.2, Fig. 5, Appendix E.3).
Overall, the RBM initialization revealed by the mapping allows for smaller models which perform
better despite shorter training time (for instance, using fewer hidden units to achieve similar clas-
siﬁcation performance). Reducing the size and training time of models is critical, as more realistic
datasets (e.g. gene expression data from single-cell RNA sequencing) may require orders of mag-
nitude more visible units. For generative modelling of such high dimensional data, our proposed
weight initialization based on orthogonalized Hopﬁeld patterns could be of practical use. Our the-
ory and experiments are a proof-of-principle; if they can be extended to the large family of deep
architectures which are built upon RBMs, such as deep belief networks (Hinton et al., 2006) and
deep Boltzmann machines (Salakhutdinov & Hinton, 2009), it would be of great beneﬁt. This will
be explored in future work.
More broadly, exposing the relationship between RBMs and their representative HNs helps to ad-
dress the infamous interpretability problem of machine learning which criticizes trained models
as “black boxes”. HNs are relatively transparent models, where the role of the patterns as robust
dynamical attractors is theoretically well-understood. We believe this correspondence, along with
future work to further characterize the reverse map, will be especially fruitful for explaining the
performance of deep architectures constructed from RBMs.
ACKNOWLEDGMENTS
The authors thank Duncan Kirby and Jeremy Rothschild for helpful comments and discussions.
This work is supported by the National Science and Engineering Research Council of Canada
(NSERC) through Discovery Grant RGPIN 402591 to A.Z. and CGS-D Graduate Fellowship to M.S.
REFERENCES
David H Ackley, Geoffrey E Hinton, and Terrence J. Sejnowski. A learning algorithm for boltz-
mann machines.
Cognitive Science, 9(1):147–169, 1985.
ISSN 03640213.
doi: 10.1016/
S0364-0213(85)80012-4.
Elena Agliari, Adriano Barra, Andrea De Antoni, and Andrea Galluzzi. Parallel retrieval of cor-
related patterns: From Hopﬁeld networks to Boltzmann machines. Neural Networks, 38:52–63,
2013. ISSN 08936080. doi: 10.1016/j.neunet.2012.11.010. URL http://dx.doi.org/10.
1016/j.neunet.2012.11.010.
Elena Agliari, Adriano Barra, Chiara Longo, and Daniele Tantari. Neural Networks Retrieving
Boolean Patterns in a Sea of Gaussian Ones. Journal of Statistical Physics, 168(5):1085–1104,
2017. ISSN 00224715. doi: 10.1007/s10955-017-1840-9.
9

Published as a conference paper at ICLR 2021
Daniel J. Amit. Modeling Brain Function: The World of Attractor Neural Networks. Cambridge
University Press, sep 1989. doi: 10.1017/cbo9780511623257.
Daniel J. Amit, Hanoch Gutfreund, and H. Sompolinsky. Spin-glass models of neural networks.
Physical Review A, 32(2):1007–1018, 1985. ISSN 10502947. doi: 10.1103/PhysRevA.32.1007.
Adriano Barra and Francesco Guerra. About the ergodic regime in the analogical Hopﬁeld neural
networks: Moments of the partition function. Journal of Mathematical Physics, 49(12), 2008.
ISSN 00222488. doi: 10.1063/1.3039083.
Adriano Barra, Alberto Bernacchia, Enrica Santucci, and Pierluigi Contucci. On the equivalence of
Hopﬁeld networks and Boltzmann Machines. Neural Networks, 34:1–9, 2012. ISSN 08936080.
doi: 10.1016/j.neunet.2012.06.003.
URL http://dx.doi.org/10.1016/j.neunet.
2012.06.003.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Aistats,
volume 10, pp. 33–40. Citeseer, 2005.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507, 2006.
ISSN 0036-8075.
doi: 10.1126/science.1127647.
URL
https://science.sciencemag.org/content/313/5786/504.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771–1800, 2002. ISSN 08997667. doi: 10.1162/089976602760128018.
Geoffrey E. Hinton. A practical guide to training restricted boltzmann machines. Lecture Notes
in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture
Notes in Bioinformatics), 7700 LECTU:599–619, 2012.
ISSN 03029743.
doi: 10.1007/
978-3-642-35289-8-32.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 2006. ISSN 08997667. doi: 10.1162/neco.2006.18.7.1527.
J J Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the National Academy of Sciences of the United States of America, 79(8):2554–
2558, 1982. ISSN 00278424. doi: 10.1073/pnas.79.8.2554.
I. Kanter and H. Sompolinsky. Associative recall of memory without errors. Physical Review A, 35
(1):380–392, 1987. ISSN 10502947. doi: 10.1103/PhysRevA.35.380.
Scott Kirkpatrick and David Sherrington. Inﬁnite-ranged models of spin-glasses. Physical Review
B, 17(11):4384–4403, 1978. ISSN 01631829. doi: 10.1103/PhysRevB.17.4384.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2323, 1998. ISSN 00189219. doi:
10.1109/5.726791.
Per Olov L¨owdin. On the Nonorthogonality Problem. Advances in Quantum Chemistry, 5(C):
185–199, 1970. ISSN 00653276. doi: 10.1016/S0065-3276(08)60339-1.
Chiara Marullo and Elena Agliari. Boltzmann machines as generalized hopﬁeld networks: A review
of recent results and outlooks.
Entropy, 23(1):1–16, 2021.
ISSN 10994300.
doi: 10.3390/
e23010034.
Pankaj Mehta, Marin Bukov, Ching Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K
Fisher, and David J Schwab. A high-bias, low-variance introduction to Machine Learning for
physicists, 2019. ISSN 03701573.
Jan Melchior, Nan Wang, and Laurenz Wiskott. Gaussian-binary restricted Boltzmann machines
for modeling natural image statistics. PLoS ONE, 12(2), 2017. ISSN 19326203. doi: 10.1371/
journal.pone.0171015.
Marc M´ezard. Mean-ﬁeld message-passing equations in the Hopﬁeld model and its generalizations.
Physical Review E, 95(2):1–15, 2017. ISSN 24700053. doi: 10.1103/PhysRevE.95.022117.
10

Published as a conference paper at ICLR 2021
Fionn Murtagh and Pedro Contreras. Algorithms for hierarchical clustering: An overview. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1):86–97, 2012.
ISSN
19424787. doi: 10.1002/widm.53.
Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.
ISSN 09603174. doi: 10.1023/A:1008923215028.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duch-
esnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:
2825–2830, 2011. ISSN 15324435.
L. Personnaz, I. Guyon, and G. Dreyfus. Collective computational properties of neural networks:
New learning mechanisms. Physical Review A, 34(5):4217–4228, 1986. ISSN 10502947. doi:
10.1103/PhysRevA.34.4217.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann machines. In Journal of Machine
Learning Research, volume 5, pp. 448–455, 2009.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton.
Restricted boltzmann machines for
collaborative ﬁltering. In Proceedings of the 24th International Conference on Machine Learn-
ing, ICML ’07, pp. 791–798, New York, NY, USA, 2007. Association for Computing Machin-
ery. ISBN 9781595937933. doi: 10.1145/1273496.1273596. URL https://doi.org/10.
1145/1273496.1273596.
James R. Schott and G. W. Stewart. Matrix Algorithms, Volume 1: Basic Decompositions. Journal
of the American Statistical Association, 94(448):1388, 1999. ISSN 01621459. doi: 10.2307/
2669960.
A
HOPFIELD NETWORK DETAILS
Consider p < N N-dimensional binary patterns {ξµ}p
µ=1 that are to be “stored”. From them,
construct the N ×p matrix ξ whose columns are the p patterns. If they are mutually orthogonal (e.g.
randomly sampled patterns in the large N →∞limit), then choosing interactions according to the
Hebbian rule, JHebb =
1
N ξξT , guarantees that they will all be stable minima of H(s) = −1
2sT Js,
provided α ≡p/N < αc, where αc ≈0.14 (Amit et al., 1985). If they are not mutually orthogonal
(referred to as correlated), then using the “projection rule” JProj = ξ(ξT ξ)−1ξT guarantees that
they will all be stable minima of H(s), provided p < N (Kanter & Sompolinsky, 1987; Personnaz
et al., 1986). Note JProj →JHebb in the limit of orthogonal patterns. In the main text, we use J as
shorthand for JProj.
We provide some relevant notation from Kanter & Sompolinsky (1987). Deﬁne the overlap of a state
s with the p patterns as m(s) ≡1
N ξT s, and deﬁne the projection of a state s onto the p patterns as
a(s) ≡(ξT ξ)−1ξT s ≡A−1m(s). Note A ≡ξT ξ is the overlap matrix, and mµ, aµ ∈[−1, 1].
We can re-write the projection rule Hamiltonian Eq. (1) as
H(s) = −N
2 m(s) · a(s).
(A.1)
For simplicity, we include the self-interactions rather than keeping track of their omission; the results
are the same as N →∞. From Eq. (A.1), several quadratic forms can be written depending on
which variables one wants to work with:
I. H(s) = −N2
2 mT (ξT ξ)−1m
II. H(s) = −1
2aT (ξT ξ)a
These are the starting points for the alternative Boltzmann Machines (i.e. not RBMs) presented in
Appendix C.
11

Published as a conference paper at ICLR 2021
B
ADDITIONAL HN TO RBM MAPPINGS
We used QR factorization in the main text to establish the HN to RBM mapping. However, one can
use any decomposition which satisﬁes
JProj = UU T
(B.1)
such that U ∈RN×p is orthogonal (orthogonal for tall matrices means U T U = I). In that case, U
becomes the RBM weights. We provide two simple alternatives below, and show they are all part of
the same family of orthogonal decompositions.
“Square root” decomposition: Deﬁne the matrix K ≡ξ(ξT ξ)−1/2. Note that K is orthogonal, and
that JProj = KKT .
Singular value decomposition: More generally, consider the SVD of the pattern matrix ξ:
ξ = UΣV T
(B.2)
where U ∈RN×p, V ∈Rp×p store the left and right singular vectors (respectively) of ξ as orthog-
onal columns, and Σ ∈Rp×p is diagonal and contains the singular values of ξ. Note in the limit of
orthogonal patterns, we have Σ =
√
NI. This decomposition gives several relations for quantities
of interest:
A ≡ξT ξ
= V Σ2V T
JHebb ≡1
N ξξT
= 1
N UΣ2U T
JProj ≡ξ(ξT ξ)−1ξT
= UU T .
(B.3)
The last line is simply the diagonalization of JProj, and shows that our RBM mapping is preserved
if we swap the Q from QR with U from SVD. However, since there are p degenerate eigenvalues
σ2 = 1, U is not unique - any orthogonal basis for the 1-eigenspace can be chosen. Thus U ′ = UO
where O is orthogonal is also valid, and the QR decomposition and “square root” decomposition
correspond to particular choices of O.
C
HN TO BM MAPPINGS USING ALTERNATIVE REPRESENTATIONS OF THE
HOPFIELD HAMILTONIAN
In addition to the orthogonalized representation in the main text, there are two natural representations
to consider based on the pattern overlaps and projections introduced in Appendix A. These lead to
generalized Boltzmann Machines (BMs) consisting of N original binary spins and p continuous
variables. These representations are not RBMs as the continuous variables interact with each other.
We present them for completeness.
“Overlap” Boltzmann Machine: Writing H(s) = −N2
2 mT (ξT ξ)−1m, we have
Z =
X
{s}
exp
 
1
2(β
√
Nm)T
 β
N ξT ξ
−1
(β
√
Nm)
!
.
(C.1)
Applying the gaussian integral identity,
Z =
q
det (ξT ξ)
X
{s}
Z
e−β
2N
P
µ,ν(ξT ξ)µνλµλν+
β
√
N
P
µ λµ
P
i ξiµsi Y
µ
dλµ
p
2πN/β
,
(C.2)
from which we identify the BM Hamiltonian
H(s, λ) =
1
2N
X
µ,ν
(ξT ξ)µνλµλν −
1
√
N
X
µ
X
i
ξiµsiλµ.
(C.3)
This is the analog of Eq. (6) in the main text for the “overlap” representation. Note we can also sum
out the binary variables in Eq. (C.2), which allows for an analogous expression to Eq. (8),
F0({λµ}) =
1
2N
X
µ,ν
(ξT ξ)µνλµλν −1
β
X
i
ln cosh
 
β
√
N
X
µ
ξiµλµ
!
.
(C.4)
12

Published as a conference paper at ICLR 2021
Curiously, we note that we may perform a second Gaussian integral on Eq. (C.2), introducing new
auxiliary variables τν to remove the interactions between the λµ variables:
Z =
s
det
β2
N ξT ξ
 X
{s}
Z Z
e−β
2 τ T τ+
β
√
N λT ξT s+i
β
√
N λT (ξT ξ)1/2τ Y
ν
dτν
√
2π
Y
µ
dλµ
√
2π .
(C.5)
Eq. (C.5) describes a three-layer RBM with complex interactions between the λµ and τν variables,
a representation which could be useful in some contexts.
“Projection” Boltzmann Machine: Proceeding as above but for H(s) = −1
2aT (ξT ξ)a, one ﬁnds
Z = det (ξT ξ)
−1/2 X
{s}
Z
e−β
2 λT (ξT ξ)−1λ+βλT (ξT ξ)−1ξT s Y
µ
dλµ
p
2π/β
,
(C.6)
which corresponds to the BM Hamiltonian
H(s, λ) = 1
2λT (ξT ξ)−1λ −λT (ξT ξ)−1ξT s.
(C.7)
The analogous expression to Eq. (8) in this case is
F0(λ) = 1
2λT (ξT ξ)−1λ −1
β
X
i
ln cosh
 β[ξ(ξT ξ)−1λ]i

.
(C.8)
D
RBM TO HN DETAILS
D.1
INTEGRATING OUT THE HIDDEN VARIABLES
The explanation from Mehta et al. (2019) for integrating out the hidden variables of an RBM is
presented here for completeness. For a given binary-gaussian RBM deﬁned by HRBM(s, λ) (as in
Eq. (9)), we have p(s) = Z−1 R
e−βHRBM(s,λ)dλ. Consider also that p(s) = Z−1e−βH(s) for some
unknown H(s). Equating these expressions gives,
H(s) = −
X
i
bisi −1
β
X
µ
ln
Z
e−β 1
2 λ2
µ+β P
i Wiµsiλµdλµ

.
(D.1)
Decompose the argument of ln(·) in Eq. (D.1) by deﬁning qµ(λµ), a gaussian with zero mean and
variance β−1. Writing tµ = β P
i Wiµsi, one observes that the second term (up to a constant) is a
sum of cumulant generating functions, i.e.
Kµ(tµ) ≡ln ⟨etµλµ⟩qµ = ln
Z
qµetµλµdλµ

.
(D.2)
These can be written as a cumulant expansion, Kµ(tµ) = P
n=1 κ(n)
µ
tn
µ
n! , where κ(n)
µ
= ∂Kµ
∂tµ |tµ=0
is the nth cumulant of qµ. However, since qµ(λµ) is gaussian, only the second term remains, leaving
Kµ(tµ) = 1
β
(β P
i Wiµsi)2
2
. Putting this all together, we have
H(s) = −
X
i
bisi −1
2
X
i
X
j
X
µ
WiµWjµsisj,
(D.3)
Note that in general, qµ(λµ) need not be gaussian, in which case the resultant Hamiltonian H(s)
can have higher order interactions (expressed via the cumulant expansion).
D.2
APPROXIMATE REVERSE MAPPING
Suppose one has a trial solution Bp = W X to the approximate binarization problem Eq. (12), with
error matrix E ≡Bp −sgn(Bp). We consider two cases depending on if W is orthogonal.
13

Published as a conference paper at ICLR 2021
Case 1: If W is orthogonal, then starting from Eq. (11) and applying Eq. (13), we have J =
W W T = Bp(XT X)−1BT
p . Using I = W T W , we get
J = Bp(BT
p Bp)−1BT
p .
(D.4)
Thus, the interactions between the visible units is the familiar projection rule used to store the
approximately binary patterns Bp. “Storage” means the patterns are stable ﬁxed points of the deter-
ministic update rule st+1 ≡sgn(Jst).
We cannot initialize the network to a non-binary state. Therefore, the columns of B = sgn(Bp) are
our candidate patterns. To test if they are ﬁxed points, consider
sgn(JB) = sgn(JBp −JE) = sgn(Bp −JE).
We need the error E to be such that JE will not alter the sign of Bp. Two sufﬁcient conditions are:
(a) small error: |(JE)iµ| < |(Bp)iµ|, or
(b) error with compatible sign: (JE)iµ(Bp)iµ < 0.
When either of these conditions hold for each element, we have sgn(JB) = sgn(Bp) = B, so that
the candidate patterns B are ﬁxed points. It remains to show that they are also stable (i.e. minima).
Case 2: If W is not orthogonal but its singular values remain close to one, then L¨owdin Orthogo-
nalization (also known as Symmetric Orthogonalization) (L¨owdin, 1970) provides a way to preserve
the HN mapping from Case 1 above.
Consider the SVD of W : W = UΣV T . The closest matrix to W (in terms of Frobenius norm)
with orthogonal columns is L = UV T , and the approximation W ≈L is called the L¨owdin
Orthogonalization of W . Note the approximation becomes exact when all the singular values are
one. We then write W W T ≈UU T , and the orthogonal W approach of Case 1 can then be applied.
On the other hand, W may be strongly not orthogonal (singular values far from one). If it is still
full rank, then its pseudo-inverse W † = (W T W )−1W T = V Σ−1U T is well-deﬁned. Repeating
the steps from the orthogonal case, we note here XT X = BT
p (W †)T W †Bp. Deﬁning C ≡
(W †)T W † = UΣ−2U T , we arrive at the corresponding result,
J = Bp(BT
p CBp)−1BT
p .
(D.5)
This is analogous to the projection rule but with a “correction factor” C. However, it is not imme-
diately clear how C affects pattern storage. Given the resemblance between JProj and Eq. (D.4)
(relative to Eq. (D.5)), we expect that RBMs trained with an orthogonality constraint on the weights
may be more readily mapped to HNs.
D.3
EXAMPLE OF THE APPROXIMATE REVERSE MAPPING
In the main text we introduced the approximate binarization problem Eq. (12), the solutions of
which provide approximately binary patterns through Eq. (13). To numerically solve Eq. (12) and
obtain a candidate solution X∗, we perform gradient descent on a differentiable variant.
Speciﬁcally, we replace sgn(u) with tanh(αu) for large α. Deﬁne E = W X −tanh(αW X) as
in the main text. Then the derivative of the “softened” Eq. (12) with respect to X is
G(X) = 2W T (E −αE ⊙sech2(W X))).
(D.6)
Given an initial condition X0, we apply the update rule
Xt+1 = Xt −γG(Xt)
(D.7)
until convergence to a local minimum X∗.
In the absence of prior information, we consider randomly initialized X0. Our preliminary experi-
ments using Eq. (D.7) to binarize arbitrary RBM weights W have generally led to high binarization
error. This is due in part to the difﬁculty in choosing a good initial condition X0, which will be
explored in future work.
14

Published as a conference paper at ICLR 2021
To avoid this issue, we consider the case of a Hopﬁeld-initialized RBM following CD-k training. At
epoch 0, we in fact have the exact binarization solution X∗= R (from the QR decomposition, Eq.
(2)), which recovers the encoded binary patterns ξ. We may use X0 = R, as an informed initial
condition to Eq. (D.7), to approximately binarize the weight at later epochs and monitor how the
learned patterns change.
In Fig. D.1, we give an example of this approximate reverse mapping for a Hopﬁeld-initialized
RBM following generative training (Fig. 4). Fig. D.1(a) shows the p = 10 encoded binary patterns,
denoted below by ξ0, and Fig. D.1(b) shows the approximate reverse mapping applied to the RBM
weights at epoch 10. We denote these nearly binary patterns by ξ10. Interestingly, some of the
non-binary regions in Fig. D.1(b) coincide with features that distinguish the respective pattern. For
example, the strongly “off” area in the top-right of the “six” pattern.
(a) Encoded Hopfield patterns
(b) Recovered Hopfield patterns (10 epochs)
Figure D.1: Reverse mapping example. (a) The p = 10 encoded binary patterns used to initialize
the RBM in Fig. 4(a). (b) The approximate reverse mapping applied to the RBM weights after 10
epochs of CD-k training. Parameters: α = 200 and γ = 0.05.
We also considered the associative memory performance of the projection HNs built from the the
patterns ξ0, ξ10 from Fig. D.1. Speciﬁcally, we test the extent to which images from the MNIST
Test dataset are attracted to the correct patterns. Ideally, each pattern should attract all test images
of the corresponding class. The results are displayed in Fig. D.2 and elaborated in the caption.
Net accuracy 76.8%
Net accuracy 73.7%
(a) Epoch 0 patterns
(b) Epoch 10 patterns
Figure D.2: Associative memory task using (a) ξ0, the initial Hopﬁeld patterns, and (b) ξ10, the
patterns recovered from the reverse mapping after 10 epochs. The patterns are used to construct a
projection HN as in Eq. (1). Each sample from the MNIST Test set is updated deterministically
until it reaches a local minimum of the HN. If the ﬁxed point is one of the encoded patterns, the
sample contributes value 1 to the table. Otherwise, we perform stochastic updates with β = 2.0
and ensemble size n = 20 until one of the encoded patterns is reached, deﬁned as an overlap
N −1sT sgn(ξµ) > 0.7, with each trajectory contributing 1/n to the table.
15

Published as a conference paper at ICLR 2021
The results in Fig. D.2 suggest that p = 10 patterns may be too crude for the associative memory
network to be used as an accurate MNIST classiﬁer (as compared to e.g. Fig. 5). Notably, the HN
constructed from ξ10 performs about 3% worse than the HN constructed from ξ0, although this per-
formance might improve with a more sophisticated method for the associative memory task. There
may therefore be a cost, in terms of associative memory performance, to increasing the generative
functionality of such models (Fig. 4). Our results from Appendix D.2 indicate that incorporating
an orthogonality constraint on the weights during CD-k generative training may provide a way to
preserve or increase the associative memory functionality. This will be explored in future work.
E
RBM TRAINING
Consider a general binary-gaussian RBM with N visible and p hidden units. The energy function is
H(s, λ) = 1
2
X
µ
(λµ −cµ)2 −
X
i
bisi −
X
µ
X
i
Wiµsiλµ.
(E.1)
First, we note the Gibbs-block update distributions for sampling one layer of a binary-gaussian RBM
given the other (see e.g. Melchior et al. (2017)),
Visible units: p(si = 1|λ) =
1
1+e−2βxi , where xi ≡P
µ Wiµλµ + bi deﬁnes input to si,
Hidden units: p(λµ = λ|s) ∼N(hµ, β−1), where hµ ≡P
i Wiµsi + cµ deﬁnes the input to λµ.
E.1
GENERATIVE TRAINING
For completeness, we re-derive the binary-gaussian RBM weight updates, along the lines of Mel-
chior et al. (2017). We want to maximize L ≡
1
M
P
a ln pθ(sa). The contribution for a single
datapoint sa has the form ln pθ(sa) = ln(C−1 R
e−βH(sa,λ)dλ) −ln Z with C ≡(2π/β)p/2. The
gradient with respect to the model is
∂ln pθ(sa)
∂θ
=
R
(−β ∂H(sa,λ)
∂θ
)e−βH(sa,λ) Q
µ dλµ
R
e−βH(sa,λ) Q
µ dλµ
−β
P
{s}
R
siλµe−βH(sa,λ) Q
µ dλµ
Z
(E.2)
We are focused on the interlayer weights, with ∂H(s,λ)
∂Wiµ
= −siλµ, so
∂ln pθ(sa)
∂θ
= β(sa)i
R
(λµe−βH(sa,λ) Q
µ dλµ
R
e−βH(sa,λ) Q
µ dλµ
−β
P
{s}
R
siλµe−βH(sa,λ) Q
µ dλµ
Z
= β(sa)i⟨λµ|s = sa⟩model
−β⟨siλµ⟩model.
(E.3)
The ﬁrst term is straightforward to compute: ⟨λµ|s = sa⟩model = P
i Wiµ(sa)i + cµ. The second
term is intractable and needs to be approximated. We use contrastive divergence (Carreira-Perpinan
& Hinton, 2005; Hinton, 2012): ⟨siλµ⟩model ≈s(k)
i
λ(k)
µ . Here k denotes CD-k – k steps of Gibbs-
block updates (introduced above) – from which s(k)
i
, λ(k)
µ
comprise the ﬁnal state. We evaluate both
terms over mini-batches of the training data to arrive at the weight update rule Eq. (15).
E.2
GENERATIVE PERFORMANCE
We are interested in estimating the objective function L ≡
1
M
P
a ln pθ(sa) during training. As
above, we split L into two terms,
L = ln
 
C−1
Z
e−βH(sa,λ) Y
µ
dλµ
!
−ln Z,
(E.4)
with C ≡(2π/β)p/2. The ﬁrst term evaluates to
β
M
M
X
a=1
(bT sa + 1
2||c + W T sa||2) −β
2 ||c||2,
(E.5)
16

Published as a conference paper at ICLR 2021
which can be computed deterministically. ln Z, on the other hand, needs to be estimated. For this
we perform Annealed Importance Sampling (AIS) (Neal, 2001) on its continuous representation
Z = C−12N
Z
e−β
2
P
µ(λµ−cµ)2+P
i ln cosh(β(P
µ Wiµλµ+bi)) Y
µ
dλµ.
(E.6)
For AIS we need to specify the target distribution’s un-normalized log-probabilities
ln(Zp(λ)) = N ln 2 −p
2 ln
2π
β

−β
2
X
µ
(λµ −cµ)2 +
X
i
ln cosh
 
β
 X
µ
Wiµλµ + bi
!!
,
(E.7)
as well as an initial proposal distribution to anneal from, which we ﬁx as a p-dimensional unit normal
distribution N(0, I).
E.3
CLASSIFICATION PERFORMANCE
Here we provide extended data (Fig. E.1) on the classiﬁcation performance shown in Fig. 5. As
in the main text, color denotes initial condition (introduced in Section 3.1) and shape denotes the
number of sub-patterns. Here we train for each 100 epochs, which allows convergence of most of
the curves. We also include the “Hebbian” initialization (green curves).
Figure E.1: Product-of-experts classiﬁcation performance (extended data).
Notably, the Hebbian initialization performs quite poorly in the classiﬁcation task (as compared to
direct generative objective, Fig. 4). In particular, for the 100 sub-patterns case, where the projection
17

Published as a conference paper at ICLR 2021
HN performs best, the Hebbian curve trains very slowly (still not converged after 100 epochs) and
lags behind even the 10 sub-pattern Hebbian curve for most of the training. This emphasizes the
beneﬁts of the projection rule HN over the Hebbian HN when the data is composed of correlated
patterns, which applies to most real-world datasets.
18

