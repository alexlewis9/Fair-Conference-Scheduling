Published as a conference paper at ICLR 2021
ON STATISTICAL BIAS IN ACTIVE LEARNING: HOW
AND WHEN TO FIX IT
Sebastian Farquhar†∗, Yarin Gal†, Tom Rainforth‡∗
University of Oxford, †OATML, Department of Computer Science; ‡Department of Statistics
ABSTRACT
Active learning is a powerful tool when labelling data is expensive, but it introduces
a bias because the training data no longer follows the population distribution. We
formalize this bias and investigate the situations in which it can be harmful and
sometimes even helpful. We further introduce novel corrective weights to remove
bias when doing so is beneﬁcial. Through this, our work not only provides a
useful mechanism that can improve the active learning approach, but also an
explanation of the empirical successes of various existing approaches which ignore
this bias. In particular, we show that this bias can be actively helpful when training
overparameterized models—like neural networks—with relatively little data.
1
INTRODUCTION
In modern machine learning, unlabelled data can be plentiful while labelling requires scarce resources
and expert attention, for example in medical imaging or scientiﬁc experimentation. A promising
solution to this is active learning—picking the most informative datapoints to label that will hopefully
let the model be trained in the most sample-efﬁcient way possible (Atlas et al., 1990; Settles, 2010).
However, active learning has a complication. By picking the most informative labels, the acquired
dataset is not drawn from the population distribution. This sampling bias, noted by e.g., MacKay
(1992); Dasgupta & Hsu (2008), is worrying: key results in machine learning depend on the training
data being identically and independently distributed (i.i.d.) samples from the population distribution.
For example, we train neural networks by minimizing a Monte Carlo estimator of the population risk.
If training data are actively sampled, that estimator is biased and we optimize the wrong objective.
The possibility of bias in active learning has been considered by e.g., Beygelzimer et al. (2009); Chu
et al. (2011); Ganti & Gray (2012), but the full problem is not well understood. In particular, methods
that remove active learning bias have been restricted to special cases, so it has been impossible to
even establish whether removing active learning bias is helpful or harmful in typical situations.
To this end, we show how to remove the bias introduced by active learning with minimal changes
to existing active learning methods. As a stepping stone, we build a Plain Unbiased Risk Estimator,
˜RPURE, which applies a corrective weighting to actively sampled datapoints in pool-based active
learning. Our Levelled Unbiased Risk Estimator, ˜RLURE, builds on this and has lower variance
and additional desirable ﬁnite-sample properties. We prove that both estimators are unbiased and
consistent for arbitrary functions, and characterize their variance.
Interestingly, we ﬁnd—both theoretically and empirically—that our bias corrections can simultane-
ously also reduce the variance of the estimator, with these gains becoming larger for more effective
acquisition strategies. We show that, in turn, these combined beneﬁts can sometimes lead to signif-
icant improvements for both model evaluation and training. The beneﬁts are most pronounced in
underparameterized models where each datapoint affects the learned function globally. For example,
in linear regression adopting our weighting allows better estimates of the parameters with less data.
On the other hand, in cases where the model is overparameterized and datapoints mostly affect the
learned function locally—like deep neural networks—we ﬁnd that correcting active learning bias can
be ineffective or even harmful during model training. Namely, even though our corrections typically
produce strictly superior statistical estimators, we ﬁnd that the bias from standard active learning
can actually be helpful by providing a regularising effect that aids generalization. Through this, our
work explains the known empirical successes of existing active learning approaches for training deep
models (Gal et al., 2017b; Shen et al., 2018), despite these ignoring the bias this induces.
∗Equal contribution. Corresponding author sebastian.farquhar@cs.ox.ac.uk.
1

Published as a conference paper at ICLR 2021
To summarize, our main contributions are:
1. We offer a formalization of the problem of statistical bias in active learning.
2. We introduce active learning risk estimators, ˜RPURE and ˜RLURE, and prove both are unbiased,
consistent, and with variance that can be less than the naive (biased) estimator.
3. Using these, we show that active learning bias can hurt in underparameterized cases like
linear regression but help in overparameterized cases like neural networks and explain why.
2
BIAS IN ACTIVE LEARNING
We begin by characterizing the bias introduced by active learning. In supervised learning, generally,
we aim to ﬁnd a decision rule fθ corresponding to inputs, x, and outputs, y, drawn from a population
data distribution pdata(x, y) which, given a loss function L(y, fθ(x)), minimizes the population risk:
r = Ex,y∼pdata [L(y, fθ(x))] .
The population risk cannot be found exactly, so instead we consider the empirical distribution for
some dataset of N points drawn from the population. This gives the empirical risk: an unbiased and
consistent estimator of r when the data are drawn i.i.d from pdata and are independent of θ,
ˆR = 1
N
XN
n=1 L(yn, fθ(xn)).
In pool-based active learning (Lewis & Gale, 1994; Settles, 2010), we begin with a large unlabelled
dataset, known as the pool dataset Dpool ≡{xn|1 ≤n ≤N}, and sequentially pick the most useful
points for which to acquire labels. The lack of most labels means we cannot evaluate ˆR directly, so
we use the sub-sample empirical risk evaluated using the M actively sampled labelled points:
˜R = 1
M
XM
m=1 L(ym, fθ(xm)).
(1)
Though almost all active learning research uses this estimator (see Appendix D), it is not an unbiased
estimator of either ˆR or r when the M points are actively sampled. Under active—i.e. non–uniform—
sampling the M datapoints are not drawn from the population distribution, resulting in a bias which
we formally characterize in §4. See Appendix A for a more general overview of active learning.
Note an important distinction between what we will call “statistical bias” and “overﬁtting bias.” The
bias from active learning above is a statistical bias in the sense that using ˜R biases our estimation of
r, regardless of θ. As such, optimizing θ with respect to ˜R induces bias into our optimization of θ. In
turn, this breaks any consistency guarantees for our learning process: if we keep M/N ﬁxed, take
M →∞, and optimize for θ, we no longer get the optimal θ that minimizes r. Almost all work on
active learning for neural networks currently ignores the issue of statistical bias.
However, even without this statistical bias, indeed even if we use ˆR directly, the training process
itself also creates an overﬁtting bias: evaluating the risk using training data induces a dependency
between the data and θ. This is why we usually evaluate the risk on held-out test data when doing
model selection. Dealing with overﬁtting bias is beyond the scope of our work as this would equate to
solving the problem of generalization. The small amount of prior work which does consider statistical
bias in active learning entirely ignores this overﬁtting bias without commenting on it.
In §3-6, we focus on statistical bias in active learning, so that we can produce estimators that are valid
and consistent, and let us optimize the intended objective, not so they can miraculously close the
train–test gap. From a more formal perspective, our results all assume that θ is chosen independently
of the training data; an assumption that is almost always (implicitly) made in the literature. This
ensures our estimators form valid objectives, but also has important implications that are typically
overlooked. We return to this in §7, examining the interaction between statistical and overﬁtting bias.
3
UNBIASED ACTIVE LEARNING: ˜RPURE AND ˜RLURE
We now show how to unbiasedly estimate the risk in the form of a weighted expectation over actively
sampled data points. We denote the set of actively sampled points Dtrain ≡{(xm, ym)|1 ≤m ≤M},
where ∀m : xm ∈Dpool. We begin by building a “plain” unbiased risk estimator, ˜RPURE, as a stepping
stone—its construction is quite natural in that each term is individually an unbiased estimator of the
risk. We then use it to construct a “levelled” unbiased risk estimator, ˜RLURE, which is an unbiased
2

Published as a conference paper at ICLR 2021
and consistent estimator of the population risk just like ˜RPURE, but which reweights individual terms
to produce lower variance and resolve some pathologies of the ﬁrst approach. Both estimators are
easy to implement and have trivial compute/memory requirements.
3.1
˜RPURE: PLAIN UNBIASED RISK ESTIMATOR
For our estimators, we introduce an active sampling proposal distribution over indices rather than the
more typical distribution over datapoints. This simpliﬁes our proofs, but the two are algorithmically
equivalent for pool-based active learning because of the one–to–one relationship between datapoints
and indices. We deﬁne the probability mass for each index being the next to be sampled, once
Dtrain contains m −1 points, as q(im; i1:m−1, Dpool). Because we are learning actively, the proposal
distribution depends on the indices sampled so far (i1:m−1) and the available data (Dpool, note
though that it does not depend on the labels of unsampled points). The only requirement on this
proposal distribution for our theoretical results is that it must place non-zero probability on all of the
training data: anything else necessarily introduces bias. Considerations for the acquisition proposal
distribution are discussed further in §3.3. We ﬁrst present the estimator before proving the main
results:
˜RPURE ≡1
M
XM
m=1 am;
where
am ≡wmLim + 1
N
Xm−1
t=1 Lit,
(2)
where the loss at a point Lim ≡L(yim, fθ(xim)), the weights wm ≡1/Nq(im; i1:m−1, Dpool)
and im ∼q(im; i1:m−1, Dpool). For practical implementation, ˜RPURE can further be written in the
following more computationally friendly form that avoids a double summation:
˜RPURE = 1
M
M
X
m=1

1
Nq(im; i1:m−1, Dpool) + M −m
N

Lim.
(3)
However, we focus on the ﬁrst form for our analysis because am in (2) has some beneﬁcial properties
not shared by the weighting factors in (3). In particular, in Appendix B.1 we prove that:
Lemma 1. The individual terms am of ˜RPURE are unbiased estimators of the risk: E [am] = r.
The motivation for the construction of am directly originates from constructing an estimator where
Lemma 1 holds while only making use of the observed losses Li1, . . . , Lim, taking care with the fact
that each new proposal distribution does not have support over points that have already been acquired.
Except for trivial problems, am is essentially unique in this regard; naive importance sampling
(i.e.
1
M
PM
m=1 wmLim) does not lead to an unbiased, or even consistent, estimator. However, the
overall estimator ˜RPURE is not the only unbiased estimator of the risk, as we discuss in §3.2.
We can now characterize the behaviour of ˜RPURE as follows (see Appendix B.2 for proof)
Theorem 1. ˜RPURE as deﬁned above has the properties:
E
h
˜RPURE
i
= r,
Var
h
˜RPURE
i
= Var [L(y, fθ(x))]
N
+
1
M 2
M
X
m=1
EDpool,i1:m−1 [Var [wmLim|i1:m−1, Dpool]] .
(4)
Remark 1. The ﬁrst term of (4) is the variance of the loss on the whole pool, while the second term
accounts for the variance originating from the active sampling itself given the pool. This second term
is O(N/M) times larger and so will generally dominate in practice as typically M ≪N.
Armed with Theorem 1, we can prove the consistency of ˜RPURE under standard assumptions: ˜RPURE
converges in expectation (i.e. its mean squared error tends to zero) as M →∞under the assumptions
that N > M, L(y, fθ(x)) is integrable, and q(im; i1:m−1, Dpool) is a valid proposal in the sense that
it puts non-zero mass on each unlabelled datapoint. Formally, as proved in Appendix B.3,
Theorem 2. Let α = N/M and assume that α > 1. If E

L(y, fθ(x))2
< ∞and
∃β > 0 :
min
n∈{1:N\i1:m−1} q(im = n; i1:m−1, Dpool) ≥β/N
∀N ∈Z+, m ≤N,
then ˜RPURE converges in its L2 norm to r as M →∞, i.e., limM→∞E
h
( ˜RPURE −r)2i
= 0.
3

Published as a conference paper at ICLR 2021
3.2
˜RLURE: LEVELLED UNBIASED RISK ESTIMATOR
˜RPURE is natural in that each term is an unbiased estimator of r. However, this creates surprising
behaviour given the sequential structure of active learning. For example, with a uniform proposal
distribution—equivalent to not actively learning—points sampled earlier are more highly weighted
than later ones and ˜RPURE ̸= ˜R. Speciﬁcally, a uniform proposal, q(im; i1:m−1, Dpool) =
1
N−m+1,
gives a weight on each sampled point of 1 + M−2m+1
N
̸= 1. Similarly, as M →N (such that we use
the full pool) the weights also fail to become uniform: setting M = N gives a weight for each point
of 1 + M−2m+1
N
̸= 1. ˜RLURE ﬁxes this. We ﬁrst quote the estimator before proving key results:
˜RLURE ≡1
M
M
X
m=1
vmLim; vm ≡1 + N −M
N −m

1
(N −m + 1) q(im; i1:m−1, Dpool) −1

. (5)
This estimator ensures that the expected value of the weight, vm, does not depend on the position it
was sampled in, but only on the probability with which it was sampled. That is, E [vm] = 1 for all m,
M, N, and q(im; i1:m−1; Dpool). As a consequence the variance is generally lower. Moreover, we
resolve the ﬁnite-sample behaviour shown by ˜RPURE. The weights become more even as M increases
for a given N, and when M = N, each vm = 1 such that ˜RLURE = ˜R = ˆR. Additionally, if the
proposal is uniform, all weights are always exactly 1 such that ˜RLURE = ˜R.
To derive ˜RLURE note that each am estimates r without bias so for any normalized linear combination:
E
"PM
m=1 cmam
PM
m=1 cm
#
= r,
provided that the cm are constant with respect to the data and sampled indices (they can depend on
M, N, and m). In Appendix B.4 we show that the choice of :
cm =
N(N −M)
(N −m)(N −m + 1)
produces the vm from (5) and in turn that these vm have the desired property E [vm] = 1, ∀m ∈
{1, . . . , M}. We note also that PM
m=1 cm = M, such that ˜RLURE =
1
M
PM
m=1 cmam. We further
characterise the variance and unbiasedness of ˜RLURE as follows (see Appendix B.5 for proof)
Theorem 3. ˜RLURE as deﬁned above has the following properties:
E
h
˜RLURE
i
= r,
Var
h
˜RLURE
i
= Var [L(y, fθ(x))]
N
+
1
M 2
M
X
m=1
c2
mEDpool,i1:m−1 [Var [wmLim|i1:m−1, Dpool]] . (6)
Although not obvious from inspection of (6), in Appendix B.6 we prove that the variance of ˜RLURE is
always less than that of ˜RPURE subject to a mild assumption about the proposal which we detail there.
Theorem 4. If Equation (14) in Appendix B.6 holds then Var[ ˜RLURE] ≤Var[ ˜RPURE]. If M > 1 and
EDpool [Var[wmLi1|Dpool]] > 0 also hold, then the inequality is strict: Var[ ˜RLURE] < Var[ ˜RPURE].
To provide intuition into why this result holds, remember that cm were introduced to ensure that E [vm]
are all identically one. Therefore this weighting removes the tendency of ˜RPURE to overemphasize
the earlier samples; essentially increasing the effective sample size by correcting the imbalance.
We ﬁnish by conﬁrming that ˜RLURE is a consistent estimator as M →∞(proof in Appendix B.7):
Theorem 5. Under the same assumptions as Theorem 2: limM→∞E
h  ˜RLURE −r
2i
= 0.
3.3
FROM ACTIVE LEARNING SCHEMES TO PROPOSALS
We have introduced two elements of the active learning scheme: the risk estimators— ˜RPURE and
˜RLURE—and the acquisition proposal distribution—q(im|i1:m−1, Dpool)—which has so far remained
general. So long as the acquisition proposal puts non-zero mass on all the training data, ˜RPURE and
4

Published as a conference paper at ICLR 2021
˜RLURE are unbiased and consistent as proven above. This is in contrast to the naive risk estimator ˜R,
for which the choice of proposal distribution affects the bias of the estimator.
It is easy to satisfy the requirement for non-zero mass everywhere. Even prior work which selects
points deterministically (e.g., Bayesian Active Learning by Disagreement (BALD) (Houlsby et al.,
2011) or a geometric heuristic like coreset construction (Sener & Savarese, 2018)) can be easily
adapted. Any scheme, like BALD, that selects the points with argmax can use softmax to return a
distribution. Alternatively, a distribution can be constructed analogous to epsilon-greedy exploration.
With probability ϵ we pick uniformly, otherwise we pick the point returned by an arbitrary acquisition
strategy. This adapts any deterministic active learning scheme to allow unbiased risk estimation.
It is also possible to use ˜RLURE and ˜RPURE with data collected using a proposal distribution that does
not fully support the training data, though they will not fully correct the bias in this case. Namely, if
we have a set of points, I, that are ignored by the proposal (i.e. that are assigned zero mass), we can
still use ˜RLURE and ˜RPURE in the same way but they both introduce the same following bias:
E[ ˜RI
LURE] = E[ ˜RI
PURE] = E
"
E
h
˜RLURE
Dpool
i
−E
"
1
N
X
n∈I
Ln
Dpool
##
= r −E
"
1
N
X
n∈I
Ln
#
.
Sometimes this bias will be small and may be acceptable if it enables a desired acquisition scheme,
but in general one of the stochastic adaptations described above is likely to be preferable. One
can naturally also extend this result to cases where I varies at each iteration of the active learning
(including deterministic acquisition strategies), for which we again have a non–zero bias.
Though the choices of acquisition proposal and risk estimator are algorithmically detached, choosing
a good proposal will still be critical to performance in practice. In the next section, we will discuss
how the proposal distribution can affect the variance of the estimators, and we will see that our
approaches also offer the potential to reduce the variance of the naive biased estimator. Later, in §7,
we will turn to a third element of active learning schemes—generalization—and consider the fact that
optimization introduces a bias separately from the choice of risk estimator and proposal distribution.
4
UNDERSTANDING THE EFFECT OF ˜RLURE AND ˜RPURE ON VARIANCE
In order to show that the variance of our unbiased estimators can be lower than that of the biased ˜R,
with a well-chosen acquisition function, we ﬁrst introduce an analogous result to Theorems 1 and 3
for ˜R, the proof for which is given in Appendix B.8:
Theorem 6. Let µm := E [Lim] and µm|i,D := E [Lim|i1:m−1, Dpool]. For ˜R (deﬁned in (1)):
E
h
˜R
i
= 1
M
XM
m=1 µm
(̸= r in general)
Var[ ˜R] =
1
z
}|
{
VarDpool
h
E
h
˜R
Dpool
ii
+
2
z
}|
{
1
M 2
XM
m=1 EDpool,i1:m−1 [Var [Lim|i1:m−1, Dpool]]
+
1
M 2
M
X
m=1
EDpool

Var

µm|i,D
Dpool

|
{z
}
3
+ 2 EDpool
h
Cov
h
Lim,
X
k<m Lik
Dpool
ii
|
{z
}
4
.
(7)
Examining this expression suggests that the variances of ˜RPURE and, in particular, ˜RLURE will often
be lower than that of ˜R, given a suitable proposal. Consider the terms of (7):
1 is analogous to
the shared ﬁrst term of (4) and (6), Var [L(y, fθ(x))] /N. If ˜R were an unbiased estimator of ˆR then
these would be exactly equal, but the conditional bias introduced by ˜R also varies between pool
datasets. In general,
1 will typically be larger than, or similar to, its unbiased counterparts. In any
case, recall that the ﬁrst terms of (4) and (6) tend to be small contributors to the overall variance
anyway, thus
1 provides negligible scope for ˜R to provide notable beneﬁts over our estimators.
We can also relate
2 to terms in (4) and (6): it corresponds to the second half of (4), but where we
replace of the expected conditional variances of the weighted losses wmLim with the unweighted
losses Lim. For effective proposals, wm and Lim should be anticorrelated: high loss points should
have higher density and thus lower weight. This means the expected conditional variance of wmLim
should be less than Lim for a well-designed acquisition strategy. Variation in the expected value
5

Published as a conference paper at ICLR 2021
of the weights with m can complicate this slightly for ˜RPURE, but the correction factors applied for
˜RLURE avoids this issue and ensure that the second half of (6) will be reliably smaller than
2 .
We have shown that the variance of ˜RLURE is typically smaller than
1 +
2 under sensible proposals.
Expression (7) has additional terms:
3 is trivially always positive and so contributes to higher variance
for ˜R (it comes from variation in the bias in index sampling given Dpool);
4 reﬂects correlations
between the losses at different iterations which have been eliminated by our estimators. This term is
harder to quantify and can be positive or negative depending on the problem. For example, sampling
points without replacement can cause negative correlation, while the proposal adaptation itself can
cause positive correlations (ﬁnding one high loss point can help ﬁnd others). The former effect
diminishes as N grows, for ﬁxed M, hinting that
4 may tend to be positive for N ≫M. Regardless,
if
4 is big enough to change which estimator has higher variance then correlation between losses in
different acquired points would lead to high bias in ˜R.
In contrast, we prove in Appendix B.9 that under an optimal proposal distribution both ˜RPURE and
˜RLURE become exact estimators of the empirical risk for any number of samples M—such that they
will inevitably have lower variance than ˜R in this case. A similar result holds when we are estimating
gradients of the loss, though note that the optimal proposal is different in the two cases.
Theorem 7. Given a non-negative loss, the optimal proposal distribution
q∗(im; i1:m−1, Dpool) = Lim/Σn/∈i1:m−1Ln
yields estimators exactly equal to the pool risk, that is ˜RPURE = ˜RLURE = ˆR almost surely ∀M.
In practice, it is impossible to sample using the optimal proposal distribution. However, we make this
point in order to prove that adopting our unbiased estimator is certainly capable of reducing variance
relative to standard practice if appropriate acquisition strategies are used. It also provides interesting
insights into what makes a good acquisition strategy from the perspective of the risk estimation itself.
5
RELATED WORK
Pool-based active learning (Lewis & Gale, 1994) is useful in cases where input data are prevalent
but labeling them is expensive (Atlas et al., 1990; Settles, 2010). The bias from selective sampling
was noted by MacKay (1992), but dismissed from a Bayesian perspective based on the likelihood
principle. Others have noted that the likelihood principle remains controversial (Rainforth, 2017),
and in this case would assume a well-speciﬁed model. Moreover, from a discriminative learning
perspective this bias is uncontentiously problematic. Lowell et al. (2019) observe that active learning
algorithms and datasets become coupled by active sampling and that datasets often outlive algorithms.
Despite the potential pitfalls, in deep learning this bias is generally ignored. As an informal survey,
we examined the 15 most-cited peer-reviewed papers citing Gal et al. (2017b), which considered
active learning to image data using neural networks. Of these, only two mentioned estimator bias but
did not address it while the rest either ignored or were unaware of this problem (see Appendix D).
There have been some attempts to address active learning bias, but these have generally required
fundamental changes to the active learning approach and only apply to particular setups. Beygelzimer
et al. (2009), Chu et al. (2011), and (Cortes et al., 2019) apply importance-sampling corrections
(Sugiyama, 2006; Bach, 2006) to online active learning. Unlike pool-based active learning, this
involves deciding whether or not to sample a new point as it arrives from an inﬁnite distribution.
This makes importance-sampling estimators much easier to develop, but as Settles (2010) notes, “the
pool-based scenario appears to be much more common among application papers.”
Ganti & Gray (2012) address unbiased active learning in a pool-based setting by sampling from the
pool with replacement. This effectively converts pool-based learning into a stationary online learning
setting, although it overweights data that happens to be sampled early. Sampling with replacement is
unwanted in active learning because it requires retraining the model on duplicate data which is either
impossible or wasteful depending on details of the setting. Moreover, they only prove the consistency
of their estimator under very strong assumptions (well-speciﬁed linear models with noiseless labels
and a mean-squared-error loss). Imberg et al. (2020) consider optimal proposal distributions in an
importance-sampling setting. Outside the context of active learning, Byrd & Lipton (2019) question
the value of importance-weighting for deep learning, which aligns with our ﬁndings below.
6

Published as a conference paper at ICLR 2021
6
APPLYING ˜RLURE AND ˜RPURE
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
x
−0.5
0.0
0.5
1.0
1.5
2.0
2.5
y
Ideal Fit to ̂R on pool
Unweighted ̃R on train
̃RPURE on train
̃RLURE on train
Pool Point ∈pool
Acquired Point ∈train
Figure 1: Illustrative linear regression. Ac-
tive learning deliberately over-samples unusual
points (red x’s) which no longer match the pop-
ulation (black dots). Common practice uses the
biased unweighted estimator ˜R which puts too
much emphasis on unusual points. Our unbiased
estimators ˜RPURE and ˜RLURE ﬁx this, learning
a function using only Dtrain nearly equal to the
ideal you would get if you had labels for the
whole of Dpool, despite only using a few points.
We ﬁrst verify that ˜RLURE and ˜RPURE remove the
bias introduced by active learning and examine
the variance of the estimators. We do this by tak-
ing a ﬁxed function whose parameters are inde-
pendent of Dtrain and estimating the risk using
actively sampled points. We note that this is equiv-
alent to the problem of estimating the risk of an
already trained model in a sample-efﬁcient way
given unlabelled test data. We consider two set-
tings: an inﬂexible model (linear regression) on
toy but non-linear data and an overparameterized
model (convolutional Bayesian neural network)
on a modiﬁed version of MNIST with unbalanced
classes and noisy labels.
Linear regression. For linear functions, remov-
ing active learning bias (ALB), i.e., the statistical
bias introduced by active learning, is critical. We
illustrate this in Figure 1. Actively sampled points
overrepresent unusual parts of the distribution, so
a model learned using the unweighed Dtrain differs
from the ideal function ﬁt to the whole of Dpool. Using our corrective weights more closely approxi-
mates the ideal line. The full details of the population distribution and geometric acquisition proposal
distribution are in Appendix C.1, where we also show results using an alternative epsilon-greedy
proposal. We inspect the ALB in Figure 2a by comparing the estimated risk (with squared error loss
and a ﬁxed function) to the corresponding true population risk ˆR. While M < N, the unweighted ˜R
is biased (in practice we never have M = N as then actively learning is unnecessary). ˜RPURE and
˜RLURE are unbiased throughout. However, they have high variance because the proposal is rather
poor. Shading represents the std. dev. of the bias over 1000 different acquisition trajectories.
Bayesian Neural Network. We actively classify MNIST and FashionMNIST images using a
convolutional Bayesian neural network (BNN) with roughly 80,000 parameters. In Figure 2b and 2c
we show that ˜RPURE and ˜RLURE remove the ALB. Here the variance of ˜RPURE and ˜RLURE is lower
or similar to the biased estimator. This is because the acquisition proposal distribution, a stochastic
relaxation of the Bayesian Active Learning by Disagreement (BALD) objective (Houlsby et al., 2011),
is effective (c.f. §4). A full description of the dataset and procedure is provided in Appendix C.2. Our
modiﬁed MNIST dataset is unbalanced and has noisy labels, which makes the bias more distorting.
Overall, Figure 2 shows that our estimators remove the bias introduced by active learning, as
expected, and can do so with reduced variance given an acquisition proposal distribution that puts a
high probability mass on more informative/surprising high-expected-loss points.
20
40
60
80
100
M
−0.4
−0.2
0.0
0.2
0.4
Bias: ̂R −E[⋅]
r −E[ ̃R]
r −E[RPURE]
r −E[ ̃RLURE]
(a) Linear regression.
0
10
20
30
40
50
60
70
M
−1.25
−1.00
−0.75
−0.50
−0.25
0.00
0.25
Bias: ̂R −E[⋅]
̃R
̃RPURE
̃RLURE
(b) BNN: MNIST.
0
10
20
30
40
50
60
70
M
−0.20
−0.15
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
Bias: ̂R −E[⋅]
̃R
̃RPURE
̃RLURE
(c) BNN: FashionMNIST.
Figure 2: ˜RPURE and ˜RLURE remove bias introduced by active learning, while unweighted ˜R, which
most active learning work uses, is biased. Note the sign: ˜R overestimates risk because active learning
samples the hardest points. Variance for ˜RPURE and ˜RLURE depends on the acquisition distribution
placing high weight on high-expected-loss points. In (b), the BALD-style distribution means that the
variance of the unbiased estimators is smaller. For FashionMNIST, (c), active learning bias is small
and high variance in all cases. Shading is ±1 standard deviation.
7

