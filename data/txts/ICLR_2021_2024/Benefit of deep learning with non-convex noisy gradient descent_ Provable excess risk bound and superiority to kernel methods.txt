Published as a conference paper at ICLR 2021
BENEFIT
OF
DEEP
LEARNING
WITH
NON-CONVEX
NOISY GRADIENT DESCENT: PROVABLE EXCESS RISK
BOUND AND SUPERIORITY TO KERNEL METHODS
Taiji Suzuki
Graduate School of Information Science and Technology, The University of Tokyo, Japan
Center for Advanced Intelligence Project, RIKEN, Japan
E-mail: taiji@mist.i.u-tokyo.ac.jp
Shunta Akiyama
Graduate School of Information Science and Technology, The University of Tokyo, Japan
E-mail: akiyama@mist.i.u-tokyo.ac.jp
ABSTRACT
Establishing a theoretical analysis that explains why deep learning can outperform
shallow learning such as kernel methods is one of the biggest issues in the deep
learning literature. Towards answering this question, we evaluate excess risk of
a deep learning estimator trained by a noisy gradient descent with ridge regular-
ization on a mildly overparameterized neural network, and discuss its superiority
to a class of linear estimators that includes neural tangent kernel approach, ran-
dom feature model, other kernel methods, k-NN estimator and so on. We consider
a teacher-student regression model, and eventually show that any linear estimator
can be outperformed by deep learning in a sense of the minimax optimal rate espe-
cially for a high dimension setting. The obtained excess bounds are so-called fast
learning rate which is faster than O(1/√n) that is obtained by usual Rademacher
complexity analysis. This discrepancy is induced by the non-convex geometry of
the model and the noisy gradient descent used for neural network training provably
reaches a near global optimal solution even though the loss landscape is highly
non-convex. Although the noisy gradient descent does not employ any explicit
or implicit sparsity inducing regularization, it shows a preferable generalization
performance that dominates linear estimators.
1
INTRODUCTION
In the deep learning theory literature, clarifying the mechanism by which deep learning can out-
perform shallow approaches has been gathering most attention for a long time. In particular, it is
quite important to show that a tractable algorithm for deep learning can provably achieve a better
generalization performance than shallow methods. Towards that goal, we study the rate of conver-
gence of excess risk of both deep and shallow methods in a setting of a nonparametric regression
problem. One of the difﬁculties to show generalization ability of deep learning with certain opti-
mization methods is that the solution is likely to be stacked in a bad local minimum, which prevents
us to show its preferable performances. Recent studies tackled this problem by considering opti-
mization on overparameterized networks as in neural tangent kernel (NTK) (Jacot et al., 2018; Du
et al., 2019a) and mean ﬁeld analysis (Nitanda & Suzuki, 2017; Chizat & Bach, 2018; Rotskoff &
Vanden-Eijnden, 2018; 2019; Mei et al., 2018; 2019), or analyzing the noisy gradient descent such
as stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011; Raginsky et al., 2017;
Erdogdu et al., 2018).
The NTK analysis deals with a relatively large scale initialization so that the model is well approxi-
mated by the tangent space at the initial solution, and eventually, all analyses can be reduced to those
of kernel methods (Jacot et al., 2018; Du et al., 2019b; Allen-Zhu et al., 2019; Du et al., 2019a; Arora
et al., 2019; Cao & Gu, 2019; Zou et al., 2020). Although this regime is useful to show its global
1

Published as a conference paper at ICLR 2021
convergence, the obtained estimator looses large advantage of deep learning approaches because the
estimation ability is reduced to the corresponding kernel methods. To overcome this issue, there are
several “beyond-kernel” type analyses. For example, Allen-Zhu & Li (2019; 2020) showed beneﬁt
of depth by analyzing ResNet type networks. Li et al. (2020) showed global optimality of gradient
descent by reducing the optimization problem to a tensor decomposition problem for a speciﬁc re-
gression problem, and showed the “ideal” estimator on a linear model has worse dependency on the
input dimensionality. Bai & Lee (2020) considered a second order Taylor expansion and showed
that the sample complexity of deep approaches has better dependency on the input dimensionality
than kernel methods. Chen et al. (2020) also derived a similar conclusion by considering a hierarchi-
cal representation. The analyses mentioned above actually show some superiority of deep learning,
but all of these bounds are essentially Ω(1/√n) where n is the sample size, which is not optimal
for regression problems with squared loss (Caponnetto & de Vito, 2007). The reason why only
such a sub-optimal rate is considered is that the target of their analyses is mostly the Rademacher
complexity of the set in which estimators exist for bounding the generalization gap. However, to
derive a tight excess risk bound instead of the generalization gap, we need to evaluate so called local
Rademacher complexity (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006) (see Eq. (2)
for the deﬁnition of excess risk). Moreover, some of the existing analyses should change the target
function class as the sample size n increases, for example, the input dimensionality is increased
against the sample size, which makes it difﬁcult to see how the rate of convergence is affected by
the choice of estimators.
Another promising approach is the mean ﬁeld analysis. There are also some work that showed
superiority of deep learning against kernel methods. Ghorbani et al. (2019) showed that, when
the dimensionality d of input is polynomially increasing with respect to n, the kernel methods is
outperformed by neural network approaches. Although the situation of increasing d explains well
the modern high dimensional situations, this setting blurs the rate of convergence. Actually, we can
show the superiority of deep learning even in a ﬁxed dimension setting.
There are several studies about approximation abilities of deep and shallow models. Ghorbani et al.
(2020) showed adaptivity of kernel methods to the intrinsic dimensionality in terms of approxi-
mation error and discuss difference between deep and kernel methods. Yehudai & Shamir (2019)
showed that the random feature method requires exponentially large number of nodes against the
input dimension to obtain a good approximation for a single neuron target function. These are only
for approximation errors and estimation errors are not compared.
Recently, the superiority of deep learning against kernel methods has been discussed also in the
nonparametric statistics literature where the minimax optimality of deep learning in terms of excess
risk is shown. Especially, it is shown that deep learning achieves better rate of convergence than
linear estimators in several settings (Schmidt-Hieber, 2020; Suzuki, 2019; Imaizumi & Fukumizu,
2019; Suzuki & Nitanda, 2019; Hayakawa & Suzuki, 2020). Here, the linear estimators are a general
class of estimators that includes kernel ridge regression, k-NN regression and Nadaraya-Watson
estimator. Although these analyses give clear statistical characterization on estimation ability of
deep learning, they are not compatible with tractable optimization algorithms.
In this paper, we give a theoretical analysis that uniﬁes these analyses and shows the superiority of a
deep learning method trained by a tractable noisy gradient descent algorithm. We evaluate the excess
risks of the deep learning approach and linear estimators in a nonparametric regression setting, and
show that the minimax optimal convergence rate of the linear estimators can be dominated by the
noisy gradient descent on neural networks. In our analysis, the model is ﬁxed and no explicit sparse
regularization is employed. Our contributions can be summarized as follows:
• A reﬁned analysis of excess risks for a ﬁxed model with a ﬁxed input dimension is given to com-
pare deep and shallow estimators. Although several studies pointed out the curse of dimensionality
is a key factor that separates shallow and deep approaches, we point out that such a separation ap-
pears in a rather low dimensional setting, and more importantly, the non-convexity of the model
essentially makes the two regimes different.
• A lower bound of the excess risk which is valid for any linear estimator is derived. The analysis is
considerably general because the class of linear estimators includes kernel ridge regression with
any kernel and thus it also includes estimators in the NTK regime.
• All derived convergence rate is a fast learning rate that is faster than O(1/√n). We show that
simple noisy gradient descent on a sufﬁciently wide two-layer neural network achieves a fast
2

Published as a conference paper at ICLR 2021
learning rate by using a fact that the solution converges to a Bayes estimator with a Gaussian
process prior, and the derived convergence rate can be faster than that of linear estimators. This
is much different from such existing work that compared only coefﬁcients with the same rate of
convergence with respect to the sample size n.
Other related work
Bach (2017) analyzed the model capacity of neural networks and its corre-
sponding reproducing kernel Hilbert space (RKHS), and showed that the RKHS is much larger than
the neural network model. However, separation of the estimation abilities between shallow and deep
is not proven. Moreover, the analyzed algorithm is basically the Frank-Wolfe type method which is
not typically used in practical deep learning. The same technique is also employed by Barron (1993).
The Frank-Wolfe algorithm is a kind of sparsity inducing algorithm that is effective for estimating
a function in a model with an L1-norm constraint. It has been shown that explicit or implicit sparse
regularization such as L1-regularization is beneﬁcial to obtain better performances of deep learning
under certain situations (Chizat & Bach, 2020; Chizat, 2019; Gunasekar et al., 2018; Woodworth
et al., 2020; Klusowski & Barron, 2016). For example, E et al. (2019b;a) showed that the approxi-
mation error of a linear model suffers from the curse of dimensionality in a setting where the target
function is in the Barron class (Barron, 1993), and showed an L1-type regularization avoids the curse
of dimensionality. However, our analysis goes in a different direction where a sparse regularization
is not required.
2
PROBLEM SETTING AND MODEL
In this section, we give the problem setting and notations that will be used in the theoretical analy-
sis. We consider the standard nonparametric regression problem where data are generated from the
following model for an unknown true function f o : Rd →R:
yi = f o(xi) + ϵi
(i = 1, . . . , n),
(1)
where xi is independently identically distributed from PX whose support is included in Ω= [0, 1]d,
and ϵi is an observation noise that is independent of xi and satisﬁes E[ϵi] = 0 and ϵi ∈[−U, U]
almost surely. The n i.i.d. observations are denoted by Dn = (xi, yi)n
i=1. We want to estimate the
true function f o through the training data Dn. To achieve this purpose, we employ the squared loss
ℓ(y, f(x)) = (y −f(x))2 and accordingly we deﬁne the expected and empirical risks as L(f) :=
EY,X[ℓ(Y, f(X))] and bL(f) :=
1
n
Pn
i=1 ℓ(yi, f(xi)) respectively. Throughout this paper, we are
interested in the excess (expected) risk of an estimator bf deﬁned by
(Excess risk)
L( bf) −
inf
f:measurable L(f).
(2)
Since the loss function ℓis the squared loss, the inﬁmum of inff:measurable L(f) is achieved by
f o: inff:measurable L(f) = L(f o).
The population L2(PX)-norm is denoted by ∥f∥L2(PX) :=
p
EX∼PX[f(X)2] and the sup-norm on the support of PX
is denoted by ∥f∥∞
:=
supx∈supp(PX) |f(x)|. We can easily check that for an estimator bf, the L2-distance ∥bf −f o∥2
L2(PX)
between the estimator bf and the true function f o is identical to the excess risk: L( bf) −L(f o) =
∥bf −f o∥2
L2(PX). Note that the excess risk is different from the generalization gap L( bf) −bL( bf).
Indeed, the generalization gap typically converges with the rate of O(1/√n) which is optimal in a
typical setting (Mohri et al., 2012). On the other hand, the excess risk can be faster than O(1/√n),
which is known as a fast learning rate (Mendelson, 2002; Bartlett et al., 2005; Koltchinskii, 2006;
Gin´e & Koltchinskii, 2006).
2.1
MODEL OF TRUE FUNCTIONS
To analyze the excess risk, we need to specify a function class (in other words, model) in which
the true function f o is included. In this paper, we only consider a two layer neural network model,
whereas the techniques adapted in this paper can be directly extended to deeper neural network
models. We consider a teacher-student setting, that is, the true function f o can be represented by
a neural network deﬁned as follows. For w ∈R, let ¯w be a “clipping” of w deﬁned as ¯w :=
3

Published as a conference paper at ICLR 2021
R × tanh(w/R) where R ≥1 is a ﬁxed constant, and let [x; 1] := [x⊤, 1]⊤for x ∈Rd. Then, the
teacher network is given by
fW (x) = P∞
m=1 am ¯w2,mσm(w⊤
1,m[x; 1]),
where w1,m ∈Rd+1 and w2,m ∈R (m ∈N) are the trainable parameters (where W
=
(w1,m, w2,m)∞
m=1), am ∈R (m ∈N) is a ﬁxed scaling parameter, and σm : R →R is an activation
function for the m-th node. The reason why we applied the clipping operation to the parameter of
the second layer is just for a technical reason to ensure convergence of Langevin dynamics. The dy-
namics is bounded in high probability in practical situations and the boundedness condition would
be removed if further theoretical development of inﬁnite dimensional Langevin dynamics would be
achieved.
Let H be a set of parameters W such that its squared norm is bounded:
H := {W
=
(w1,m, w2,m)∞
m=1 | P∞
m=1(∥w1,m∥2 + w2
2,m) < ∞}. Deﬁne ∥W∥H := [P∞
m=1(∥w1,m∥2 +
w2
2,m)]1/2 for W ∈H. Let (µm)∞
m=1 be a regularization parameter such that µm ↘0. Accordingly
we deﬁne Hγ := {W ∈H | ∥W∥Hγ < ∞} where ∥W∥Hγ := [P∞
m=1 µ−γ
m (∥w1,m∥2 + w2
2,m)]1/2
for a given 0 < γ. Throughout this paper, we analyze an estimation problem in which the true
function is included in the following model:
Fγ = {fW | W ∈Hγ, ∥W∥Hγ ≤1}.
This is basically two layer neural network with inﬁnite width. As assumed later, am is assumed
to decrease as m →∞. Its decreasing rate controls the capacity of the model. If the ﬁrst layer
parameters (w1,m)m are ﬁxed, this model can be regarded as a variant of the unit ball of some
reproducing kernel Hilbert space (RKHS) with basis functions amσm(w⊤
1,m[x; 1]). However, since
the ﬁrst layer (w1,m) is also trainable, there appears signiﬁcant difference between deep and kernel
approaches. The Barron class (Barron, 1993; E et al., 2019b) is relevant to this function class.
Indeed, it is deﬁned as the convex hull of w2σ(w⊤
1 [x; 1]) with norm constraints on (w1, w2) where
σ is an activation function. On the other hand, we will put an explicit decay rate on am and the
parameter W has an L2-norm constraint, which makes the model Fγ smaller than the Barron class.
3
ESTIMATORS
We consider two classes of estimators and discuss their differences: linear estimators and deep
learning estimator with noisy gradient descent (NGD).
Linear estimator A class of linear estimators, which we consider as a representative of “shallow”
learning approach, consists of all estimators that have the following form:
bf(x) = Pn
i=1 yiϕi(x1, . . . , xn, x).
Here, (ϕi)n
i=1 can be any measurable function (and L2(PX)-integrable so that the excess risk can
be deﬁned). Thus, they could be selected as the “optimal” one so that the corresponding linear esti-
mator minimizes the worst case excess risk. Even if we chose such an optimal one, the worst case
excess risk should be lower bounded by our lower bound given in Theorem 1. It should be noted that
the linear estimator does not necessarily imply “linear model.” The most relevant linear estimator
in the machine learning literature is the kernel ridge regression: bf(x) = Y ⊤(KX + λI)−1k(x)
where KX = (k(xi, xj))n
i,j=1 ∈Rn×n, k(x) = [k(x, x1), . . . , k(x, xn)]⊤∈Rn and Y
=
[y1, . . . , yn]⊤∈Rn for a kernel function k : Rd × Rd →R. Therefore, the ridge regression
estimator in the NTK regime or the random feature model is also included in the class of linear
estimators. The solution obtained in the early stopping criteria instead of regularization in the NTK
regime under the squared loss is also included in the linear estimators. Other examples include the
k-NN estimator and the Nadaraya-Watson estimator. All of them do not train the basis function in a
nonlinear way, which makes difference from the deep learning approach. In the nonparametric statis-
tics literature, linear estimators have been studied for estimating a wavelet series model. Donoho
et al. (1990; 1996) have shown that a wavelet shrinkage estimator can outperform any linear estima-
tor by showing suboptimality of linear estimators. Suzuki (2019) utilized such an argument to show
superiority of deep learning but did not present any tractable optimization algorithm.
4

Published as a conference paper at ICLR 2021
Noisy Gradient Descent with regularization As for the neural network approach, we consider a
noisy gradient descent algorithm. Basically, we minimize the following regularized empirical risk:
bL(fW ) + λ
2 ∥W∥2
H1.
Here, we employ H1-norm as the regularizer.
We note that the constant γ controls the
relative complexity of the true function f o compared to the typical solution obtained un-
der the regularization.
Here, we deﬁne a linear operator A as λ∥W∥H1
= W ⊤AW, that
is, AW
= (λµ−1
m w1,m, λµ−1
m w2,m)∞
m=1.
The regularized empirical risk can be minimized
by noisy gradient descent as Wk+1
= Wk −η∇( bL(fWk) + λ
2 ∥Wk∥2
H1) +
q
2η
β ξk, where
η
>
0 is a step size and ξk
=
(ξk,(1,m), ξk,(2,m))∞
m=1 is an inﬁnite-dimensional Gaus-
sian noise, i.e., ξk,(1,m) and ξk,(2,m) are independently identically distributed from the stan-
dard normal distribution (Da Prato & Zabczyk, 1996). Here, ∇bL(fW ) =
1
n
Pn
i=1 2(fW (xi) −
yi)( ¯w2,mam[xi; 1]σ′
m(w⊤
1,m[xi; 1]), am tanh′(w2,m/R)σm(w⊤
1,m[xi; 1]))∞
m=1.
However,
since
∇∥Wk−1∥2
H1 is unbounded which makes it difﬁcult to show convergence, we employ the semi-
implicit Euler scheme deﬁned by
Wk+1 =Wk−η∇bL(fWk)−ηAWk+1+
q
2η
β ξk ⇔Wk+1 =Sη

Wk−η∇bL(fWk)+
q
2η
β ξk

, (3)
where Sη := (I+ηA)−1. It is easy to check that this is equivalent to the following update rule: Wk =
Wk−1 −η

∇bL(fWk−1) + SηAWk−1 +
q
2η
β ξk−1

. Therefore, the implicit Euler scheme can be
seen as a naive noisy gradient descent for minimizing the empirical risk with a slightly modiﬁed
ridge regularization. This can be interpreted as a discrete time approximation of the following
inﬁnite dimensional Langevin dynamics:
dWt = −∇( bL(fWt) + λ
2 ∥Wt∥2
H1)dt +
p
2/βdξt,
(4)
where (ξt)t≥0 is the so-called cylindrical Brownian motion (see Da Prato & Zabczyk (1996) for
the details). Its application and analysis for machine learning problems with non-convex objectives
have been recently studied by, for example, Muzellec et al. (2020); Suzuki (2020).
The above mentioned algorithm is executed on an inﬁnite dimensional parameter space. In practice,
we should deal with a ﬁnite width network. To do so, we approximate the solution by a ﬁnite
dimensional one: W (M) = (w1,m, w2,m)M
m=1 where M corresponds to the width of the network.
We identify W (M) to the “zero-padded” inﬁnite dimensional one, W = (w1,m, w2,m)∞
m=1 with
w1,m = 0 and w2,m = 0 for all m > M. Accordingly, we use the same notation fW (M) to indicate
fW with zero padded vector W. Then, the ﬁnite dimensional version of the update rule is given by
W (M)
k+1 = S(M)
η

W (M)
k
−η∇bL(fW (M)
k
) +
q
2η
β ξ(M)
k

, where ξ(M)
k
is the Gaussian noise vector
obtained by projecting ξk to the ﬁrst M components and S(M)
η
is also obtained in a similar way.
4
CONVERGENCE RATE OF ESTIMATORS
In this section, we present the excess risk bounds for linear estimators and the deep learning estima-
tor. As for the linear estimators, we give its lower bound while we give an upper bound for the deep
learning approach. To obtain the result, we setup some assumptions on the model.
Assumption 1.
(i) There exists a constant cµ such that µm ≤cµm−2 (m ∈N).
(ii) There exists α1 > 1/2 such that am ≤µα1
m (m ∈N).
(iii) The activation functions (σm)m is bounded as ∥σm∥∞≤1. Moreover, they are three times
differentiable and their derivatives upto third order differentiation are uniformly bounded:
∃Cσ such that ∥σm∥1,3 := max{∥σ′
m∥∞, ∥σ′′
m∥∞, ∥σ′′′
m∥∞} ≤Cσ (∀m ∈N).
The ﬁrst assumption (i) controls the strength of the regularization, and combined with the second
assumption (ii) and deﬁnition of the model Fγ, complexity of the model is controlled. If α1 and γ
are large, the model is less complicated. Indeed, the convergence rate of the excess risk becomes
5

Published as a conference paper at ICLR 2021
faster if these parameters are large as seen later. The decay rate µm ≤cµm−2 can be generalized as
m−p with p > 1 but we employ this setting just for a technical simplicity for ensuring convergence
of the Langevin dynamics. The third assumption (iii) can be satisﬁed by several activation functions
such as the sigmoid function and the hyperbolic tangent. The assumption ∥σm∥∞≤1 could be
replaced by another one like ∥σm∥∞≤C, but we ﬁx this scaling for simple presentation.
4.1
MINIMAX LOWER BOUND FOR LINEAR ESTIMATORS
Here, we analyze a lower bound of excess risk of linear estimators, and eventually we show that
any linear estimator suffers from curse of dimensionality. To rigorously show that, we consider the
following minimax excess risk over the class of linear estimators:
Rlin(Fγ) :=
inf
b
f:linear
sup
f o∈Fγ
EDn[∥bf −f o∥2
L2(PX)],
where inf is taken over all linear estimators and EDn[·] is taken with respect to the training data Dn.
This expresses the best achievable worst case error over the class of linear estimators to estimate a
function in Fγ. To evaluate it, we additionally assume the following condition.
Assumption 2. We assume that µm = m−2 and am = µα1
m (m ∈N) (and hence cµ = 1). There
exists a monotonically decreasing sequence (bm)∞
m=1 and s ≥3 such that bm = µα2
m (∀m) with
α2 > γ/2 and σm(u) = bs
mσ(b−1
m u) (u ∈R) where σ is the sigmoid function: σ(u) = 1/(1+e−u).
Intuitively, the parameter s controls the “resolution” of each basis function σm, and the relation
between parameter α1 and α2 controls the magnitude of coefﬁcient for each basis σm. Note that the
condition s ≥3 ensures ∥σm∥1,3 is uniformly bounded and 0 < bm ≤1 ensures ∥σm∥∞≤1. Our
main strategy to obtain the lower bound is to make use of the so-called convex-hull argument. That
is, it is known that, for a function class F, the minimax risk R(F) over a class of linear estimators
is identical to that for the convex hull of F (Hayakawa & Suzuki, 2020; Donoho et al., 1990):
Rlin(F) = Rlin(conv(F)),
where conv(F) = {PN
i=1 λifi | fi ∈F, PN
i=1 λi = 1, λi ≥0, N ∈N} and conv(·) is the
closure of conv(·) with respect to L2(PX)-norm. Intuitively, since the linear estimator is linear to
the observations (yi)n
i=1 of outputs, a simple application of Jensen’s inequality yields that its worst
case error on the convex hull of the function class F does not increase compared with that on the
original one F (see Hayakawa & Suzuki (2020) for the details). This indicates that the linear es-
timators cannot distinguish the original hypothesis class F and its convex hull. Therefore, if the
class F is highly non-convex, then the linear estimators suffer from much slower convergence rate
because its convex hull conv(F) becomes much “fatter” than the original one F. To make use of
this argument, for each sample size n, we pick up appropriate mn and consider a subset generated
by the basis function σmn, i.e., F(n)
γ
:= {amn ¯w2,mnσm(w⊤
1,mn[x; 1]) ∈Fγ}. By applying the con-
vex hull argument to this set, we obtain the relation Rlin(Fγ) ≥Rlin(F(n)
γ
) = Rlin(conv(F(n)
γ
)).
Since F(n)
γ
is highly non-convex, its convex hull conv(F(n)
γ
) is much larger than the original set
F(n)
γ
and thus the minimax risk over the linear estimators would be much larger than that over all
estimators including deep learning. More intuitively, linear estimators do not adaptively select the
basis functions and thus they should prepare redundantly large class of basis functions to approx-
imate functions in the target function class. The following theorem gives the lower bound of the
minimax optimal excess risk over the class of linear estimators.
Theorem 1. Suppose that Var(ϵ) > 0, PX is the uniform distribution on [0, 1]d, and Assumption 2
is satisﬁed. Let ˜β = α1+(s+1)α2
α2−γ/2
. Then for arbitrary small κ′ > 0, we have that
Rlin(Fγ) ≳n−2 ˜
β+d
2 ˜
β+2d n−κ′.
(5)
The proof is in Appendix A. We utilized the Irie-Miyake integral representation (Irie & Miyake,
1988; Hornik et al., 1990) to show there exists a “complicated” function in the convex hull, and
then we adopted the technique of Zhang et al. (2002) to show the lower bound. The lower bound is
characterized by the decaying rate (α1) of am relative to that (α2) of the scaling factor bm. Indeed,
the faster am decays with increasing m, the faster the rate of the minimax lower bound becomes.
6

Published as a conference paper at ICLR 2021
We can see that the minimax rate of linear estimators is quite sensitive to the dimension d. Actually,
for relatively high dimensional settings, this lower bound becomes close to a slow rate Ω(1/√n),
which corresponds to the curse of dimensionality.
It has been pointed out that the sample complexity of kernel methods suffers from the curse of
dimensionality while deep learning can avoid that by a tractable algorithms (e.g., Ghorbani et al.
(2019); Bach (2017)). Among them, Ghorbani et al. (2019) showed that if the dimensionality d is
polynomial against n, then the excess risk of kernel methods is bounded away from 0 for all n. On
the other hand, our analysis can be applied to any linear estimator including kernel methods, and
it shows that even if the dimensionality d is ﬁxed, the convergence rate of their excess risk suffers
from the curse of dimensionality. This can be accomplished thanks to a careful analysis of the rate
of convergence. Bach (2017) derived an upper bound of the Rademacher complexity of the unit ball
of the RKHS corresponding to a neural network model. However, it is just an upper bound and there
is still a large gap from excess risk estimates. Allen-Zhu & Li (2019; 2020); Bai & Lee (2020); Chen
et al. (2020) also analyzed a lower bound of sample complexity of kernel methods. However, their
lower bound is not for the excess risk of the squared loss. Eventually, the sample complexities of all
methods including deep learning take a form of O(C/√n) and dependency of coefﬁcient C to the
dimensionality or other factors such as magnitude of residual components is compared. On the other
hand, our lower bound properly involves the properties of squared loss such as strong convexity and
smoothness and the bound shows the curse of dimensionality occurs even in the rate of convergence
instead of just the coefﬁcient. Finally, we would like to point out that several existing work (e.g.,
Ghorbani et al. (2019); Allen-Zhu & Li (2019)) considered a situation where the target function
class changes as the sample size n increases. However, our analysis reveals that separation between
deep and shallow occurs even if the target function class Fγ is ﬁxed.
4.2
UPPER BOUND FOR DEEP LEARNING
Here, we analyze the excess risk of deep learning trained by NGD and its algorithmic convergence
rate. Our analysis heavily relies on the weak convergence of the discrete time gradient Langevin dy-
namics to the stationary distribution of the continuous time one (Eq. (4)). Under some assumptions,
the continuous time dynamics has a stationary distribution (Da Prato & Zabczyk, 1992; Maslowski,
1989; Sowers, 1992; Jacquot & Royer, 1995; Shardlow, 1999; Hairer, 2002). If we denote the prob-
ability measure on H corresponding to the stationary distribution by π∞, then it is given by
dπ∞
dνβ (W) ∝exp(−β bL(fW )),
where νβ is the Gaussian measure in H with mean 0 and covariance (βA)−1 (see Da Prato &
Zabczyk (1996) for the rigorous deﬁnition of the Gaussian measure on a Hilbert space). Remarkably,
this can be seen as the Bayes posterior for a prior distribution νβ and a “log-likelihood” function
exp(−β bL(W)). Through this view point, we can obtain an excess risk bound of the solution Wk.
The proofs of all theorems in this section are in Appendix B.
Under Assumption 1, the distribution of Wk derived by the discrete time gradient Langevin synamics
satisﬁes the following weak convergence property to the stationary distribution π∞. This conver-
gence rate analysis depends on the techniques by Br´ehier & Kopec (2016); Muzellec et al. (2020).
Proposition 1. Assume Assumption 1 holds and β > η. Then, there exist spectral gaps Λ∗
η and
Λ∗
0 (deﬁned in Eq. (10) of Appendix B.1) and a constant C0 such that, for any 0 < a < 1/4, the
following convergence bound holds for almost sure observation Dn:
|EWk[L(fWk)|Dn] −EW ∼π∞[L(fW )|Dn]| ≤C0 exp(−Λ∗
ηηk) + C1
√β
Λ∗
0
η1/2−a =: Ξk,
(6)
where C1 is a constant depending only on cµ, R, α1, Cσ, U, a (independent of η, k, β, λ, n).
This proposition indicates that the expected risk of Wk can be almost identical to that of the “Bayes
posterior solution” obeying π∞after sufﬁciently large iterations k with sufﬁciently small step size
η even though bL(fW ) is not convex. The deﬁnition of Λ∗
η can be found in Eq. (10). We should
note that its dependency on β is exponential. Thus, if we take β = Ω(n), then the computational
cost until a sufﬁciently small error could be exponential with respect to the sample size n. The same
convergence holds also for ﬁnite dimensional one W (M)
k
with a modiﬁed stationary distribution. The
7

Published as a conference paper at ICLR 2021
constants appearing in the bound are independent of the model size M (see the proof of Proposition
1 in Appendix B). In particular, the convergence can be guaranteed even if W is inﬁnite dimensional.
This is quite different from usual ﬁnite dimensional analyses (Raginsky et al., 2017; Erdogdu et al.,
2018; Xu et al., 2018) which requires exponential dependency on the dimension, but thanks to the
regularization term, we can obtain the model size independent convergence rate. Xu et al. (2018)
also analyzed a ﬁnite dimensional gradient Langevin dynamics and obtained a similar bound where
O(η) appears in place of the second term η1/2−a which corresponds to time discretization error. In
our setting the regularization term is ∥W∥2
H1 = P
m(∥w1,m∥2 + w2
2,m)/µm with µm ≲m−2, but
if we employ ∥W∥2
Hp/2 = P
m(∥w1,m∥2 + w2
2,m)/µp/2
m
for p > 1, then the time discretization
error term would be modiﬁed to η(p−1)/p−a (Andersson et al., 2016). We can interpret the ﬁnite
dimensional setting as the limit of p →∞which leads to η(p−1)/p →η that recovers the ﬁnite
dimensional result (O(η)) as shown by Xu et al. (2018).
In addition to the above algorithmic convergence, we also have the following convergence rate for
the excess risk bound of the ﬁnite dimensional solution W (M)
k
.
Theorem 2. Assume Assumption 1 holds, assume η < β ≤min{n/(2U 2), n}, and 0 < γ <
1/2 + α1. Then, if the width satisﬁes M ≥min

λ1/4γ(α1+1)β1/2γ, λ−1/2(α1+1), n1/2γ	
, the
expected excess risk of Wk is bounded as
EDn
h
EW (M)
k
[∥fW (M)
k −f o∥2
L2(PX)|Dn]
i
≤C max

(λβ)
1/γ
1+1/2γ n−
1
1+1/2γ, λ−
1
2(α1+1) β−1, λ
γ
1+α1 	
+Ξk,
where C is a constant independent of n, β, λ, η, k. In particular, if we set β = min{n/(2U 2), n}
and λ = β−1, then for M ≥n1/2(α1+1), we obtain
EDn
h
EW (M)
k
[∥fW (M)
k
−f o∥2
L2(PX)|Dn]
i
≲n−
γ
α1+1 + Ξk.
In addition to this theorem, if we further assume Assumption 2, we obtain a reﬁned bound as follows.
Corollary 1. Assume Assumptions 1 and 2 hold and η < β, and let β = min{n/(2U 2), n} and
λ = β−1. Suppose that there exists 0 ≤q ≤s −3 such that 0 < γ < 1/2 + α1 + qα2. Then, the
excess risk bound of W (M)
k
for M ≥n1/2(α1+qα2+1) can be reﬁned as
EDn
h
EW (M)
k
[∥fW (M)
k
−f o∥2
L2(PX)|Dn]
i
≲n−
γ
α1+qα2+1 + Ξk.
(7)
These theorem and corollary shows that the tractable NGD algorithm achieves a fast convergence
rate of the excess risk bound. Indeed, if q is chosen so that γ > (α1+qα2+1)/2, then the excess risk
bound converges faster than O(1/√n). Remarkably, the convergence rate is not affected by the input
dimension d, which makes discrepancy from linear estimators. The bound of Theorem 2 is tightest
when γ is close to 1/2 + α1 (γ ≈1/2 + α1 + 3α2 for Corollary 1), and a smaller γ yields looser
bound. This relation between γ and α1 reﬂects misspeciﬁcation of the “prior” distribution. When
γ is small, the regularization λ∥W∥2
H1 is not strong enough so that the variance of the posterior
distribution becomes unnecessarily large for estimating the true function f o ∈Fγ. Therefore, the
best achievable bound can be obtained when the regularization is correctly speciﬁed. The analysis
of fast rate is in contrast to some existing work (Allen-Zhu & Li, 2019; 2020; Li et al., 2020; Bai
& Lee, 2020) that basically evaluated the Rademacher complexity. This is because we essentially
evaluated a local Rademacher complexity instead.
4.3
COMPARISON BETWEEN LINEAR ESTIMATORS AND DEEP LEARNING
Here, we compare the convergence rate of excess risks between the linear estimators and the neural
network method trained by NGD using the bounds obtained in Theorem 1 and Corollary 1 respec-
tively. We write the lower bound (5) of the minimax excess risk of linear estimators as R∗
lin and the
excess risk of the neural network approach (7) as R∗
NN. To make the discussion concise, we consider
a speciﬁc situation where s = 3, α1 = γ = 1
4α2. In this case, ˜β = 17/3 ≈5.667, which gives
R∗
lin ≳n
−

1+
d
2 ˜β+d
−1
n−κ′ ≈n
−

1+
d
11.3+d
−1
n−κ′.
8

Published as a conference paper at ICLR 2021
On the other hand, by setting q = 0, we have
R∗
NN ≲n−
α1
α1+1 = n
−

1+ 1
α1
−1
.
Thus, as long as α1 > 11.3/d + 1 ≈2˜β/d + 1, we have that
R∗
lin ≳R∗
NN, and limn→∞
R∗
NN
R∗
lin = 0.
In particular, as d gets larger, R∗
lin approaches to Ω(n−1/2) while R∗
NN is not affected by d and it
gets close to O(n−1) as α1 gets larger. Moreover, the inequality α1 > 11.3/d + 1 can be satis-
ﬁed by a relatively low dimensional setting; for example, d = 10 is sufﬁcient when α1 = 3. As
α1 becomes large, the model becomes “simpler” because (am)∞
m=1 decays faster. However, the
linear estimators cannot take advantage of this information whereas deep learning can. From the
convex hull argument, this discrepancy stems from the non-convexity of the model. We also note
that the superiority of deep learning is shown without sparse regularization while several existing
work showed favorable estimation property of deep learning though sparsity inducing regularization
(Bach, 2017; Chizat, 2019; Hayakawa & Suzuki, 2020). However, our analysis indicates that sparse
regularization is not necessarily as long as the model has non-convex geometry, i.e., sparsity is just
one sufﬁcient condition for non-convexity but not a necessarily condition. The parameter setting
above is just a sufﬁcient condition and the lower bound R∗
lin would not be tight. The superiority of
deep learning would hold in much wider situations.
5
CONCLUSION
In this paper, we studied excess risks of linear estimators, as a representative of shallow methods,
and a neural network estimator trained by a noisy gradient descent where the model is ﬁxed and no
sparsity inducing regularization is imposed. Our analysis revealed that deep learning can outperform
any linear estimator even for a relatively low dimensional setting. Essentially, non-convexity of the
model induces this difference and the curse of dimensionality for linear estimators is a consequence
of a fact that the geometry of the model becomes more “non-convex” as the dimension of input
gets higher. All derived bounds are fast rate because the analyses are about the excess risk with the
squared loss, which made it possible to compare the rate of convergence. The fast learning rate of
the deep learning approach is derived through the fact that the noisy gradient descent behaves like a
Bayes estimator with model size independent convergence rate.
ACKNOWLEDGMENTS
TS was partially supported by JSPS Kakenhi (18K19793, 18H03201, and 20H00576), Japan Digital
Design and JST-CREST.
REFERENCES
Z. Allen-Zhu and Y. Li. What can ResNet learn efﬁciently, going beyond kernels? In Advances in
Neural Information Processing Systems 32, pp. 9017–9028. Curran Associates, Inc., 2019.
Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning.
arXiv preprint arXiv:2001.04413, 2020.
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In Proceedings of International Conference on Machine Learning, pp. 242–252, 2019.
A. Andersson, R. Kruse, and S. Larsson. Duality in reﬁned Sobolev–Malliavin spaces and weak ap-
proximation of SPDE. Stochastics and Partial Differential Equations Analysis and Computations,
4(1):113–149, 2016.
S. Arora, S. S. Du, W. Hu, Z. Li, and R. Wang. Fine-grained analysis of optimization and gen-
eralization for overparameterized two-layer neural networks. arXiv preprint arXiv:1901.08584,
2019.
9

Published as a conference paper at ICLR 2021
F. Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Machine
Learning Research, 18(19):1–53, 2017.
Y. Bai and J. D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=rkllGyBFPH.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information theory, 39(3):930–945, 1993.
P. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. The Annals of Statis-
tics, 33:1487–1537, 2005.
C.-E. Br´ehier and M. Kopec. Approximation of the invariant law of SPDEs: error analysis using
a Poisson equation for a full-discretization scheme. IMA Journal of Numerical Analysis, 37(3):
1375–1410, 07 2016.
Y. Cao and Q. Gu. A generalization theory of gradient descent for learning over-parameterized deep
ReLU networks. arXiv preprint arXiv:1902.01384, 2019.
A. Caponnetto and E. de Vito. Optimal rates for regularized least-squares algorithm. Foundations
of Computational Mathematics, 7(3):331–368, 2007.
M. Chen, Y. Bai, J. D. Lee, T. Zhao, H. Wang, C. Xiong, and R. Socher. Towards understanding hier-
archical learning: Beneﬁts of neural representations. Advances in Neural Information Processing
Systems, 33, 2020.
L. Chizat.
Sparse optimization on measures with over-parameterized gradient descent.
arXiv
preprint arXiv:1907.10300, 2019.
L. Chizat and F. Bach. A note on lazy training in supervised differentiable programming. arXiv
preprint arXiv:1812.07956, 2018.
L. Chizat and F. Bach. Implicit bias of gradient descent for wide two-layer neural networks trained
with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
G. Da Prato and J. Zabczyk. Non-explosion, boundedness and ergodicity for stochastic semilinear
equations. Journal of Differential Equations, 98:181–195, 1992.
G. Da Prato and J. Zabczyk. Ergodicity for Inﬁnite Dimensional Systems. London Mathematical
Society Lecture Note Series. Cambridge University Press, 1996.
D. L. Donoho, R. C. Liu, and B. MacGibbon. Minimax risk over hyperrectangles, and implications.
The Annal of Statistics, 18(3):1416–1437, 09 1990. doi: 10.1214/aos/1176347758.
D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard. Density estimation by wavelet
thresholding. The Annals of Statistics, 24(2):508–539, 1996.
S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent ﬁnds global minima of deep neural
networks. In International Conference on Machine Learning, pp. 1675–1685, 2019a.
S. S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. International Conference on Learning Representations 7, 2019b.
W. E, C. Ma, and L. Wu. A priori estimates of the population risk for two-layer neural networks.
Communications in Mathematical Sciences, 17(5):1407–1425, 2019a.
W. E, C. Ma, and L. Wu. A comparative analysis of optimization and generalization properties of
two-layer neural network and random feature models under gradient descent dynamics. Science
China Mathematics, pp. 1–24, 2019b.
M. A. Erdogdu, L. Mackey, and O. Shamir. Global non-convex optimization with discretized diffu-
sions. In Advances in Neural Information Processing Systems 31, pp. 9671–9680. 2018.
10

Published as a conference paper at ICLR 2021
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Linearized two-layers neural networks in
high dimension. arXiv preprint arXiv:1904.12191, 2019.
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. When do neural networks outperform
kernel methods? arXiv preprint arXiv:2006.13409, 2020.
E. Gin´e and V. Koltchinskii. Concentration inequalities and asymptotic results for ratio type empir-
ical processes. The Annals of Probability, 34(3):1143–1216, 2006.
S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems, pp. 9482–9491,
2018.
M. Hairer. Exponential mixing properties of stochastic PDEs through asymptotic coupling. Probab.
Theory Related Fields, 124(3):345–380, 2002.
S. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network
learning over sparse parameter spaces. Neural Networks, 123:343–361, 2020. ISSN 0893-6080.
K. Hornik, M. Stinchcombe, and H. White. Universal approximation of an unknown mapping and
its derivatives using multilayer feedforward networks. Neural Networks, 3(5):551–560, 1990.
M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. In
K. Chaudhuri and M. Sugiyama (eds.), Proceedings of Machine Learning Research, volume 89
of Proceedings of Machine Learning Research, pp. 869–878. PMLR, 16–18 Apr 2019.
B. Irie and S. Miyake. Capabilities of three-layered perceptrons. In IEEE 1988 International Con-
ference on Neural Networks, pp. 641–648, 1988.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In Advances in Neural Information Processing Systems 31, pp. 8580–8589,
2018.
S. Jacquot and G. Royer. Ergodicit´e d’une classe d’´equations aux d´eriv´ees partielles stochastiques.
Comptes Rendus de l’Acad´emie des Sciences. S´erie I. Math´ematique, 320(2):231–236, 1995.
J. M. Klusowski and A. R. Barron. Risk bounds for high-dimensional ridge function combinations
including neural networks. arXiv preprint arXiv:1607.01434, 2016.
V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization. The
Annals of Statistics, 34:2593–2656, 2006.
Y. Li, T. Ma, and H. R. Zhang. Learning over-parametrized two-layer neural networks beyond ntk.
volume 125 of Proceedings of Machine Learning Research, pp. 2613–2682. PMLR, 09–12 Jul
2020.
B. Maslowski. Strong Feller property for semilinear stochastic evolution equations and applications.
In Stochastic systems and optimization (Warsaw, 1988), volume 136 of Lect. Notes Control Inf.
Sci., pp. 210–224. Springer, Berlin, 1989.
S. Mei, A. Montanari, and P.-M. Nguyen. A mean ﬁeld view of the landscape of two-layer neural
networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671, 2018. doi:
10.1073/pnas.1806579115.
S. Mei, T. Misiakiewicz, and A. Montanari.
Mean-ﬁeld theory of two-layers neural networks:
dimension-free bounds and kernel limit. In A. Beygelzimer and D. Hsu (eds.), Proceedings of
the Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learn-
ing Research, pp. 2388–2464, Phoenix, USA, 25–28 Jun 2019. PMLR.
S. Mendelson. Improving the sample complexity using global data. IEEE Transactions on Informa-
tion Theory, 48:1977–1991, 2002.
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. The MIT Press,
2012.
11

Published as a conference paper at ICLR 2021
B. Muzellec, K. Sato, M. Massias, and T. Suzuki. Dimension-free convergence rates for gradient
Langevin dynamics in RKHS. arXiv preprint 2003.00306, 2020.
A. Nitanda and T. Suzuki. Stochastic particle gradient descent for inﬁnite ensembles. arXiv preprint
arXiv:1712.05438, 2017.
M. Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via Stochastic Gradient Langevin
Dynamics: a nonasymptotic analysis. arXiv e-prints, pp. arXiv:1702.03849, 2017.
G. Rotskoff and E. Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in Neural Information Processing
Systems 31, pp. 7146–7155. Curran Associates, Inc., 2018.
G. M. Rotskoff and E. Vanden-Eijnden. Trainability and accuracy of neural networks: An interacting
particle system approach. arXiv preprint arXiv:1805.00915, 2019.
W. Rudin. Real and Complex Analysis (third edition). Mathematics series. McGraw-Hill, 1987.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics, 48(4), 2020.
T. Shardlow. Geometric ergodicity for stochastic PDEs. Stochastic Analysis and Applications, 17
(5):857–869, 1999.
R. Sowers. Large deviations for the invariant measure of a reaction-diffusion equation with non-
Gaussian perturbations. Probab. Theory Related Fields, 92(3):393–421, 1992. ISSN 0178-8051.
T. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=H1ebTsActm.
T. Suzuki. Generalization bound of globally optimal non-convex neural network training: Trans-
portation map estimation by inﬁnite dimensional langevin dynamics.
In Advances in Neural
Information Processing Systems 33, pp. to appear. Curran Associates, Inc., 2020.
T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic Besov space. arXiv preprint arXiv:1910.12799, 2019.
M. Welling and Y.-W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML,
pp. 681–688, 2011.
B. Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Sre-
bro. Kernel and rich regimes in overparametrized models. volume 125 of Proceedings of Machine
Learning Research, pp. 3635–3673. PMLR, 09–12 Jul 2020.
P. Xu, J. Chen, D. Zou, and Q. Gu. Global convergence of langevin dynamics based algorithms for
nonconvex optimization. In Advances in Neural Information Processing Systems, volume 31, pp.
3122–3133. Curran Associates, Inc., 2018.
G. Yehudai and O. Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems 32, pp. 6598–6608.
Curran Associates, Inc., 2019.
S. Zhang, M.-Y. Wong, and Z. Zheng. Wavelet threshold estimation of a regression function with
random design. Journal of Multivariate Analysis, 80(2):256–284, 2002.
D. Zou, Y. Cao, D. Zhou, and Q. Gu. Gradient descent optimizes over-parameterized deep ReLU
networks. Machine Learning, 109(3):467–492, 2020.
12

Published as a conference paper at ICLR 2021
A
PROOF OF THEOREM 1
We basically combine the “convex hull argument” and the minimax optimal rate analysis for linear
estimators developed by Zhang et al. (2002).
Zhang et al. (2002) essentially showed the following statement in their Theorem 1.
Proposition 2 (Theorem 1 of Zhang et al. (2002)). Let µ be the Lebesgue measure. Suppose that
the space Ωhas even partition A such that |A| = 2K for an integer K ∈N, each A has equivalent
measure µ(A) = 2−K for all A ∈A, and A is indeed a partition of Ω, i.e., ∪A∈A = Ω, A ∩A′ = ∅
for A, A′ ∈Ωand A ̸= A′. Then, if K is chosen as n−γ1 ≤2−K ≤n−γ2 for constants γ1, γ2 > 0
that are independent of n, then there exists an event E such that, for a constant C′ > 0,
P(E) ≥1 + o(1) and |{xi | xi ∈A (i ∈{1, . . . , n})}| ≤C′n/2K (∀A ∈A).
Moreover, suppose that, for a class F◦of functions on Ω, there exists ∆> 0 that satisﬁes the
following conditions:
1. There exists F > 0 such that, for any A ∈A, there exists g ∈F◦that satisﬁes g(x) ≥
1
2∆F for all x ∈A,
2. There exists K′ and C′′ > 0 such that 1
n
Pn
i=1 g(xi)2 ≤C′′∆22−K′ for any g ∈F◦on
the event E.
Then, there exists a constant F1 such that at least one of the following inequalities holds:
F 2
4F1C′′
2K′
n
≤Rlin(F◦),
(8a)
F 3
32 ∆22−K ≤Rlin(F◦),
(8b)
for sufﬁciently large n.
Before we show the main assertion, we prepare some additional lemmas. For a sigmoid function σ,
let ˜F(σ)
C,τ := {x ∈Rd 7→aσ(τ(w⊤x + b))) | |a| ≤2C, ∥w∥≤1, |b| ≤2 (a, b ∈R, w ∈Rd)} for
C > 0, τ > 0.
Lemma 1. Let ψ(x) =
1
2(σ(x + 1) −σ(x −1)) and ˆψ be its Fourier transform:
ˆψ(ω) :=
(2π)−1 R
e−iωxψ(x)dx. Let h > 0 and Dw > 0. Then, by setting τ = h−1(2
√
d + 1)Dw and
C = (2
√
d+1)Dw
πh| ˆ
ψ(1)| , the Gaussian RBF kernel can be approximated by
inf
ˇg∈conv( ˜
F(σ)
C,τ )
sup
x∈[0,1]d
ˇg(x) −exp

−∥x −c∥2
2h2

≤
4
|2π ˆψ(1)|
h
CdD2(d−2)
w
exp(−D2
w/2) + exp(−Dw)
i
for any c ∈[0, 1]d, where Cd is a constant depending only on d. In particular, the right hand side is
O(exp(−nκ)) if Dw = nκ.
Proof of Lemma 1. Let ψh(x) = ψ(h−1x). Suppose that, for f ∈L1(Rd), its Fourier transform
ˆf(ω) = (2π)−d R
e−iω⊤xf(x)dx (ω ∈Rd) gives
Z
Rd exp(iw⊤x) ˆf(w)dw = f(x),
for every x ∈Rd1. Then the Irie-Miyake itegral representation (Irie & Miyake (1988); see also the
proof of Theorem 3.1 in Hornik et al. (1990)) gives
f(x) =
Z
a∈Rd
Z
b∈R
ψ(a⊤x + b)dν(a, b) (a.e.),
1If ˆf is integrable, this inversion formula holds for almost every x ∈Rd (Rudin, 1987). However, we
assume a stronger condition that it holds for every x ∈Rd.
13

Published as a conference paper at ICLR 2021
where dν(a, b) is given by
dν(a, b) = Re
 
|ω|de−iwb
2π ˆψ(ω)
!
ˆf(wa)dadb
for any ω ̸= 0. Since the characteristic function of the multivariate normal distribution gives that
Z
Rd exp(iw⊤(x −c))
s
h2d
(2π)d exp

−h2∥w∥2
2

|
{z
}
= ˆ
f(w)
dw = exp

−∥x −c∥2
2h2

=: f(x) (∀x ∈Rd),
we have that
exp

−∥x −c∥2
2h2

=
Z
a∈Rd
Z
b∈R
ψh(a⊤(x −c) + b)Re
 
e−iwb
2π ˆψh(ω)
! s
|ωh|2d
(2π)d exp

−(ωh)2∥a∥2
2

dadb,
for all x ∈Rd. Since ψh(·) = ψ(h−1·) and ˆψh(·) = h ˆψ(h·) by its deﬁnition, the right hand side is
equivalent to
Z
a∈Rd
Z
b∈R
ψ(h−1[a⊤(x −c) + b])Re
 
e−iwb
2πh ˆψ(hω)
! s
|ωh|2d
(2π)d exp

−(ωh)2∥a∥2
2

dadb.
Here, we set ω = h−1. Let Nσ2 be the probability measure corresponding to the multivariate
normal with mean 0 and covariance σ2I, and let AD := {w ∈Rd | ∥w∥≤D}. Let Da > 0 and
Db = Da(
√
2d + 1), and deﬁne
fDa(x) :=
1
2DbN1(ADa)
Z
∥a∥≤Da,|b|≤Db
ψ(h−1[a⊤(x −c) + b])Re
 
e−ib/h
2πh ˆψ(1)
!
×
s
1
(2π)d exp

−∥a∥2
2

dadb.
Then, we can see that, for any x ∈[0, 1]d, it holds that

1
2DbN1(ADa)f(x) −fDa(x)

≤
1
2DbN1(ADa)|2πh ˆψ(1)|
"
N1(Ac
Da)
Z
2 exp(−h−1|x|)dx +
Z
|b|>Db
2 exp(−[h−1(|b| −2
√
dDa)])db
#
≤
1
2DbN1(ADa)|2πh ˆψ(1)|

4hN1(Ac
Da) + 4h exp(−Da)

=
4h
2DbN1(ADa)|2πh ˆψ(1)|
h
CdD2(d−2)
a
exp(−D2
a/2) + exp(−Da)
i
,
where Cd > 0 is a constant depending on only d, and we used |a⊤(x −c) + b| ≥|b| −|a⊤(x −
c)| ≥|b| −2
√
dDa and ψ(x) ≤2 exp(−|x|). Note that if Da = nκ, then the right hand side is
O(h exp(−nκ)). Therefore, since N1(ADa) ≤1, by setting τ = h−1Db, C =
Db
πh| ˆ
ψ(1)|, we have
that
inf
ˇg∈conv( ˜
F(σ)
C,τ )
sup
x∈[0,1]d
ˇg(x) −exp

−∥x −c∥2
2h2

≤
4
|2π ˆψ(1)|
h
CdD2(d−2)
a
exp(−D2
a/2) + exp(−Da)
i
.
Hence, by rewriting Dw ←Da, we obtain the assertion.
As noted above, the right hand is
O(exp(−nκ)) if Da = nκ.
14

Published as a conference paper at ICLR 2021
Proof of Theorem 1. For a sample size n, we ﬁx mn which will be determined later and use
Proposition 2 with F◦
=
F(n)
γ
.
If w2,mn
=
b
p
µγ
mn/2 with |b|
≤
1 and w1,m
=
µγ/2
mn [u; −u⊤c]/(
p
2(d + 1)) for u
∈
Rd such that ∥u∥
≤
1 and c
∈
[0, 1]d, then
∥(w1,mn, w2,mn)∥2 ≤µγ
mn(1/2 + (1 + |u⊤c|2)/2(d + 1)) ≤µγ
mn.
Therefore, ˜ϕu,c(x) =
amn ¯w2,mnσmn(w⊤
1,mn[x; 1])
=
µα1
mn(bµγ/2
mn /
√
2)µsα2
mn σ

µ−α2+γ/2
mn
u⊤(x −c)/
p
2(d + 1)

∈
F(n)
γ
⊂Fγ for all b ∈R with |b| ≤1, u ∈Rd with ∥u∥≤1, and c ∈[0, 1]d. In other words,
µα1+γ/2+sα2
mn
(2C)−1F(σ)
C,τ ⊂F(n)
γ
for any C > 0 and τ =
1
√
2(d+1)µ−α2+γ/2
mn
.
Therefore, by setting C = (
√
2d + 1)Dw/(πh| ˆψ(1)|) for Dw > 0, Lemma 1 yields that for any
c ∈[0, 1]d and given h > 0, there exists g ∈conv(F(n)
γ
) such that

µα1+γ/2+sα2
mn
 
2(
√
2d + 1)Dw
πh| ˆψ(1)|
!−1
exp

−∥· −c∥2
2h2

−g

∞
≤µα1+γ/2+sα2
mn
 
2(
√
2d + 1)Dw
πh| ˆψ(1)|
!−1
4
|2π ˆψ(1)|
h
CdD2(d−2)
w
exp(−D2
w/2) + exp(−Dw)
i
= µα1+γ/2+sα2
mn
h
(
√
2d + 1)Dw
h
CdD2(d−2)
a
exp(−D2
w/2) + exp(−Dw)
i
.
We let Dw = nκ for any κ > 0 and choose µmn as τ ≃µ−α2+γ/2
mn
= Dwh−1 = h−1nκ. We write
∆:= µα1+γ/2+sα2
mn
(2C)−1 ≃h
α1+sα2+γ/2
α2−γ/2
+1n−κ( α1+sα2+γ/2
α2−γ/2
+1). Then, it holds that
∆exp

−∥· −c∥2
2h2

−g

∞
≲∆exp(−nκ).
(9)
Here, we set h as h = 2−k with a positive integer k. Accordingly, we deﬁne a partition A of Ωso
that any A ∈A can be represented as A = [2−kj1, 2−k(j1 + 1)] × · · · × [2−kjd, 2−k(jd + 1)] by
non-negative integers 0 ≤ji ≤2k −1 (i = 1, . . . , d). Note that |A| = 2dk = h−d.
For each A ∈A, we deﬁne cA as cA = (2−k(j1 + 1/2), . . . , 2−k(jd + 1/2))⊤where (j1, . . . , jd)
is a set of indexes that satisﬁes A = [2−kj1, 2−k(j1 + 1)] × · · · × [2−kjd, 2−k(jd + 1)]. For each
A ∈A, we deﬁne gA ∈conv(F(n)
γ
) as a function that satisﬁes Eq. (9) for c = cA.
Now, we apply Proposition 2 with F◦= conv(F(n)
γ
) and K = K′ = dk.
Let R∗:=
Rlin(conv(F(n)
γ
)). First, we can see that there exits a constant F > 0 such that
gA(x) ≥F∆(∀x ∈A),
where we used exp(−nκ) ≪1.
Second, in the event E introduced in the statement of Proposition 2, there exists C such that |{i ∈
{1, . . . , n} | xi ∈A′}| ≤Cn/2−dk for all A′ ∈A. In this case, we can check that
1
n
n
X
i=1

∆exp

−∥xi −cA∥2
2h2
2
≲∆2hd = ∆22−kd,
by the uniform continuity of the Gaussian RBF. Therefore, we also have
1
n
n
X
i=1
gA(xi)2 ≤2
n
n
X
i=1

∆exp

−∥xi −cA∥2
2h2
2
+ c∆2 exp(−2nκ)
≲∆2(hd + exp(−2nκ)),
where c > 0 is a constant. Thus, as long as h is polynomial to n like h = Θ(n−a), the right hand
side is O(∆2hd).
15

Published as a conference paper at ICLR 2021
Now, if we write
˜β = α1 + sα2 + γ/2
α2 −γ/2
+ 1 = α1 + (s + 1)α2
α2 −γ/2
,
then we have ∆≃h ˜βn−κ ˜β by its deﬁnition.
Here, we choose k as a maximum integer that satisﬁes F 3
32 ∆22−dk > R∗. In this situation, it holds
that
h2 ˜β+dn−2κ ˜β ≃R∗.
Since Eq. (8b) is not satisﬁed, Eq. (8a) must hold, and hence we have
n−1h−d ≲R∗≃h2 ˜β+dn−2κ ˜β
⇒
h ≃n−1−2κ ˜
β
2 ˜
β+2d .
Therefore, we obtain that
R∗≳n−2 ˜
β+d
2 ˜
β+2d n−2κd ˜
β
2 ˜
β+2d
≥n−2 ˜
β+d
2 ˜
β+2d n−κ′,
by setting κ′ = κ
2d ˜β
2 ˜β+2d. This gives the assertion.
B
PROOFS OF PROPOSITION 1, THEOREM 2 AND COROLLARY 1
Proposition 1, Theorem 2 and Corollary 1 can be shown by using Propositions 3 and 4 given in
Appendix B.1 shown below.
Let T αW = (µα
mw1,m, µα
mw2,m)∞
m=1 for W = (w1,m, w2,m)∞
m=1 for α > 0, and let us consider a
model hW := fT −α/2W . Then, the training error can be rewritten as
bL(fW ) = bL(hT α/2W ).
For notational simplicity, we let bL(W) := bL(fW ).
Let H(M) be {W (M) = (w1,m, w2,m)M
m=1 | w1,m ∈Rd+1, w2,m ∈R, 1 ≤m ≤M} and
ι : H(M) →H be the zero padding of W (M), that is, ι(W (M)) = (w′
1,m, w′
2,m)∞
m=1 ∈H satisﬁes
w′
1,m = w1,m, w′
2,m = w2,m (m ≤M) and w′
1,m = 0, w′
2,m = 0 (m > M). Moreover, we
deﬁne ι∗: H →H(M) as the map that extracts ﬁrst M components. By abuse of notation, we write
fW (M) for W (M) ∈H(M) to indicate fι(W (M)). Finally, let A(M) : H(M) →H(M) be a linear
operator such that A(M)W (M) = ι∗(Aι(W (M))), which is just a truncation of A. Similarly, let
T a
MW (M) for W (M) ∈H(M) be the operator corresponding to T aW for W ∈H, i.e., T a
MW (M) =
ι∗(T aι(W (M))).
B.1
AUXILIARY LEMMAS
First, we show some key propositions to show the main results. To do so, we utilize the result by
Muzellec et al. (2020) and Suzuki (2020).
Assumption 3.
(i) There exists a constant cµ such that µm ≤cµm−2.
(ii) There exist B, U > 0 such that the following two inequalities hold for some a ∈(1/4, 1)
almost surely:
∥∇bL(W)∥H ≤B (∀W ∈H),
∥∇bL(W) −∇bL(W ′)∥H ≤L∥W −W ′∥H−a (∀W, W ′ ∈H).
16

Published as a conference paper at ICLR 2021
(iii) For any data Dn, bL is three times differentiable. Let ∇3 bL(W) be the third-order derivative
of bL(W). This can be identiﬁed with a third-order linear form and ∇3 bL(W)·(h, k) denotes
the Riesz representor of l ∈H 7→∇3 bL(W) · (h, k, l). There exists α′ ∈[0, 1), Cα′ ∈
(0, ∞) such that ∀W, h, k ∈H, ∥∇3 bL(W) · (h, k)∥H−α′ ≤Cα′∥h∥H∥k∥H, ∥∇3 bL(W) ·
(h, k)∥H ≤Cα′∥h∥Hα′∥k∥H (a.s.).
Remark 1. In the analysis of Br´ehier & Kopec (2016); Muzellec et al. (2020); Suzuki (2020),
Assumption 3-(iii) is imposed for any ﬁnite dimensional projection L(W (M)) as a function on H(M))
for all M ≥1 instead of L(W) as a function of H. However, the condition on L(W) gives a
sufﬁcient condition for any ﬁnite dimensional projection in our setting. Thus, we employed the
current version.
Assumption 4. For the loss function ℓ(y, f(x)) = (y −f(x))2, the following conditions holds:
(i) There exists C > 0 such that for any fW (W ∈H), it holds that
EX,Y [(ℓ(Y, fW (X)) −ℓ(Y, f ∗(X)))2] ≤C(L(fW ) −L(f ∗)).
(ii) β > 0 is chosen so that, for any h : Rd →R and x ∈supp(PX), it holds that
EY |X=x

exp
 −β
n(ℓ(Y, h(x)) −ℓ(Y, f ∗(x)))

≤1.
(iii) There exists Lh > 0 such that ∥∇W ℓ(Y, hW (X)) −∇W ℓ(Y, hW ′(X))∥H ≤Lh∥W −
W ′∥H (∀W, W ′ ∈H) almost surely.
(iv) There exists Ch such that ∥hW −hW ′∥∞≤Ch∥W −W ′∥H (W, W ′ ∈H).
Proposition 3. Assume Assumption 3 holds and β > η. Suppose that ∃¯R > 0, 0 ≤ℓ(Y, fW (X)) ≤
¯R for any W ∈H (a.s.). Let ρ =
1
1+λη/µ1 and b = µ1
λ B + cµ
βλ. Accordingly, let ¯b = max{b, 1},
κ = ¯b + 1 and ¯V = 4¯b/(√
(1+ρ1/η)/2−ρ1/η). Then, the spectral gap of the dynamics is given by
Λ∗
η =
min

λ
2µ1 , 1
2

4 log(κ( ¯V + 1)/(1 −δ))δ
(10)
where 0 < δ < 1 is a real number satisfying δ = Ω(exp(−Θ(poly(λ−1)β))). We deﬁne Λ∗
0 =
limη→0 Λ∗
η (i.e., ¯V is replaced by 4¯b/(
q
(1+exp(−λ
µ1 ))/2−exp(−λ
µ1 ))). We also deﬁne CW0 = κ[ ¯V +
1] +
√
2( ¯
R+b)
√
δ
. Then, for any 0 < a < 1/4, the following convergence bound holds for almost sure
observation Dn: for either L = L or L = bL,
|EWk[L(Wk)|Dn] −EW ∼π∞[L(W)|Dn]|
(11)
≤C1

CW0 exp(−Λ∗
ηηk) +
√β
Λ∗
0
η1/2−a

= Ξ′
k,
(12)
where C1 is a constant depending only on cµ, B, L, Cα′, a, ¯R (independent of η, k, β, λ).
Proposition 4. Assume that Assumptions 3 and 4 hold.
Let ˜α := 1/{2(α + 1)} for a given
α > 0 and θ be an arbitrary real number satisfying 0 < θ < 1 −˜α.
Assume that the
true function f o can be represented by hW ∗
= f o for W ∗∈Hθ(α+1).
Then, if M
≥
min

λ˜α/2[θ(α+1)]β1/2[θ(α+1)], λ−1/2(α+1), n1/2[θ(α+1)]	
, the expected excess risk is bounded by
EDn
h
EW (M)
k
[L(hT α/2
M
W (M)
k
)|Dn] −L(f o)
i
≤C max

(λβ)
2 ˜
α/θ
1+ ˜
α/θ n−
1
1+ ˜
α/θ , λ−˜αβ−1, λθ, 1/n
	
+ Ξ′
k,
(13)
where C is a constant independent of n, β, λ, η, k.
Proof. Repeating the same argument in Proposition 1 and using the same notation, Proposition 3
gives
|EW (M)
k
[L(W (M)
k
)|Dn] −EW ∼π(M)
∞[L(W)|Dn]| ≤Ξ′
k,
17

Published as a conference paper at ICLR 2021
for any 1
≤
M
≤
∞.
Therefore, we just need to bound the following quantity:
EDn
h
EW (M)∼π(M)
∞[L(hT α/2
M
W (M))|Dn]
i
−L(f o)
.
We deﬁne ∥W (M)∥H(M) := ∥ι∗(W (M))∥H for W (M) ∈H(M). For a > 0, we deﬁne H(M)
a
be
the projection of Ha to the ﬁrst M components, H(M)
a
= {ι(W) | W ∈Ha}, and we deﬁne
∥W (M)∥H(M)
a
:= ∥ι∗(W (M))∥Ha (note that since H(M)
a
is a ﬁnite dimensional linear space, it is
same as H as a set). Let ν(M)
β
be the Gaussian measure on H(M) with mean 0 and covariance
(βA(M))−1, and ˜ν(M)
β
be the Gaussian measure corresponding to the random variable T α/2
M W (M)
with W (M) ∼ν(M)
β
. Let the concentration function be
φ(M)
β,λ (ϵ) :=
inf
W ∈H(M)
α+1:
L(hW )−L(f o)≤ϵ2
βλ∥W∥2
H(M)
α+1 −log ˜ν(M)
β
({W ∈H(M) : ∥W∥H(M) ≤ϵ}) + log(2),
where, if there does not exist W ∈H(M)
α+1 that satisﬁes the condition inf, then we deﬁne φ(M)
β,λ (ϵ) =
∞, then Let ϵ∗> 0 be
ϵ∗:= max{inf{ϵ > 0 | φβ,λ(ϵ) ≤βϵ2}, 1/n}.
Then, Suzuki (2020) showed the following bound:
EDn

EW (M)∼π(M)
∞[L(hT α/2
(M)W (M))|Dn] −L(f o)

≤C max
n
ϵ∗2,
  β
nϵ∗2 + n−
1
1+ ˜
α/θ (λβ)
2 ˜
α/θ
1+ ˜
α/θ 
, 1
n
o
.
(14)
They also showed that, for M = ∞, it holds that
ϵ∗2 ≲max
n
(λβ)−˜αβ−(1−˜α), λθ, n−1o
= max

λ−˜αβ−1, λθ, n−1	
.
Substituting this bound of ϵ∗to Eq. (14), we obtain Eq. (13) for M = ∞. Moreover, in their proof,
if M ≥(ϵ∗)−1/[θ(α+1)], then
inf
W ∈H(M)
α+1:
L(hW )−L(f o)≤ϵ2
βλ∥W∥2
H(M)
α+1 ≲β(ϵ∗)2.
Finally, since ˜ν(M)
β
is a marginal distribution of ˜ν(∞)
β
, it holds that
−log ˜ν(M)
β
({W ∈H(M) : ∥W∥H(M) ≤ϵ}) ≤−log ˜ν(∞)
β
({W ∈H : ∥W∥H ≤ϵ}).
Therefore, as long as M ≥(ϵ∗)−1/[θ(α+1)], the rate of ϵ∗is not deteriorated from M = ∞. In
other words, if M ≥min

λ˜α/2[θ(α+1)]β1/2[θ(α+1)], λ−θ/2[θ(α+1)], n1/2[θ(α+1)]	
, the bound (13)
holds.
Remark 2. Suzuki (2020) showed Proposition 4 under a condition α > 1/2. However, this is used
only to ensure Assumption 3. In our setting, we can show Assumption 3 holds directly and thus we
may omit the condition α > 1/2.
B.2
PROOFS OF PROPOSITION 1, THEOREM 2 AND COROLLARY 1
Here, we give the proofs of Proposition 1 and Theorem 2 simultaneously.
Proof of Proposition 1 and Theorem 2. Let ¯R = (2 P∞
m=1 amR + U)2. Then, we can easily check
that (yi −fW (xi))2 ≤¯R. As stated above, we use Propositions 3 and 4 to show the statements.
First, we show Proposition 1 for the dynamics of W (M)
k
for any 1 ≤M ≤∞. However, it sufﬁces
to show the statement only for M = ∞because the ﬁnite dimensional version can be seen as a
18

Published as a conference paper at ICLR 2021
speciﬁc case of the inﬁnite dimensional one. Actually, the dynamics of W (M)
k
is same as that of
ι( ˜Wk) where ˜Wk ∈H obeys the following dynamics:
˜Wk+1 = Sη

˜Wk −η∇bL(fι( ˜
Wk)) +
r2η
β ξk

.
This is because fι( ˜
Wk) is determined by only the ﬁrst M components ι( ˜Wk), ι(∇bL(fι( ˜
Wk))) =
∇W (M) bL(fW (M))|W (M)=ι( ˜
Wk) and Sη is a diagonal operator. Since the components of ˜Wk with
indexes higher than M does not affect the objective, smoothness of the objective is not lost. The
stationary distribution π(M)
∞
of the continuous dynamics corresponding to W (M) is a probability
measure on H(M) that satisﬁes
dπ(M)
∞
dν(M)
β
(W (M)) ∝exp(−β bL(fW (M))),
where ν(M)
β
is the Gaussian measure on RM×(d+2) with mean 0 and covariance (βA(M))−1. We
can notice that this is the marginal distribution of the stationary distribution of the continuous time
counterpart of ˜Wk: d˜π∞( ˜W) ∝exp(−β bL(fι( ˜
W )))dνβ. Therefore, we just need to consider an
inﬁnite dimensional one. For this reasoning, we show the convergence for the original inﬁnite
dimensional dynamics (Wk)∞
k=1. The convergence of the ﬁnite dimensional one (W (M)
k
)∞
k=1 can be
shown by the same manner using the argument above.
To show Proposition 1, we use Propositions 3. To do so, we need to check validity of Assumptions
3. First, we check Assumption 3. Assumption 3-(i) is ensured by Assumption 1. Next, we check
Assumption 3-(ii). The boundedness of the gradient can be shown as follows:
∥∇bL(fW )∥2
H
=
∞
X
m=1
 1
n
n
X
i=1
2(fW (xi) −yi) ¯w2,mam[xi; 1]σ′
m(w⊤
1,m[xi; 1])

2
+
 1
n
n
X
i=1
2(fW (xi) −yi)am tanh′(w2,m/R)σm(w⊤
1,m[xi; 1]))∞
m=1

2
≤
∞
X
m=1
4 ¯RR2a2
m(d + 1)C2
σ + 4 ¯Ra2
m
(∵|fW (xi) −yi| ≤¯R, ∥σ′
m∥∞≤Cσ, ∥tanh′ ∥∞≤1)
≤4 ¯R[R2C2
σ(d + 1) + 1]
∞
X
m=1
a2
m < ∞.
Similarly, we can show the Lipschitz continuity of the gradient as
∥∇bL(fW ) −∇bL(fW ′)∥2
H
≤
∞
X
m=1
µ−2α1
m
µ2α1
m
n
4 ¯Ra2
m(d + 1)C2
σ[(w2,m −w′
2,m)2 + R2∥w1,m −w′
1,m∥2]
+ 4 ¯Ra2
m[(w2,m −w′
2,m)2/R2 + C2
σ(d + 1)∥w1,m −w′
1,m∥2]
o
(∵∥tanh′′ ∥∞≤1)
≤4 ¯R[(d + 1)C2
σ(1 + R2) + 1/R2 + C2
σ(d + 1)] max
m∈N{µ−2α1
m
a2
m}
×
∞
X
m=1
µ2α1
m [(w2,m −w′
2,m)2 + ∥w1,m −w′
1,m∥2]
≲∥W −W ′∥2
H−α1.
We can also verify Assumption 3-(iii) in a similar way. Then, we have veriﬁed Assumption 3.
Therefore, we may apply Proposition 3, and then we obtain Proposition 1.
19

Published as a conference paper at ICLR 2021
Next, we show Theorem 2 by using Proposition 4. For that purpose, we need to we verify Assump-
tion 4. The ﬁrst condition can be veriﬁed as
EX,Y [((Y −fW (X))2 −(Y −f o(X))2)2]
= EX,ϵ[((f o(X) + ϵ −fW (X))2 −ϵ2)2]
= EX[((f o(X) −fW (X))2 + 2ϵ(f o(X) −fW (X)))2]
= EX[(f o(X) −fW (X))4 + 2ϵ(f o(X) −fW (X))(f o(X) −fW (X))2 + ϵ2(f o(X) −fW (X))2]
= ∥f o −fW ∥2
∞EX[(f o(X) −fW (X))2] + U 2EX[(f o(X) −fW (X))2]
≤¯REX[(f o(X) −fW (X))2] = ¯R(L(fW ) −L(f o)).
The second condition can be checked as follows. Note that
EY |X=x

exp

−β
n[(Y −fW (x))2 −(Y −f o(x))2]

= Eϵ

exp

−β
n(f o(x) −fW (x))2 −2ϵ(fW (x) −f o(x))]

= exp

−β
n(f o(x) −fW (x))2

Eϵ

exp
2β
n ϵ(fW (x) −f o(x))

≤exp

−β
n(f o(x) −fW (x))2

exp
1
8
4β2
n2 4U 2(fW (x) −f o(x))2

.
Thus, under the condition β ≤n/(2U 2), the right hand side can be upper bounded by
exp

−β
n

1 −2U 2β
n

(fW (x) −f o(x))2

≤1.
Next, we check the third and fourth conditions. Noting that
∇W hW (X)
=

am(µ−α/2
m
w2,m)µ−α/2
m
[xi; 1]σ′
m(µ−α/2
m
w⊤
1,m[xi; 1]),
amµ−α/2
m
tanh′(µ−α/2
m
w2,m/R)σm(µ−α/2
m
w⊤
1,m[xi; 1]))∞
m=1
∞
m=1,
we have that
∥∇W hW (X)∥2
H
≤
∞
X
m=1
a2
mµ−α
m [(d + 1)R2C2
σ + 1]
≤[(d + 1)R2C2
σ + 1]
∞
X
m=1
µ−α+2α1
m
≤[(d + 1)R2C2
σ + 1]c−α+2α1
µ
∞
X
m=1
m−2(−α+2α1) =: C1 < ∞
(∵−α + 2α1 = α1 > 1/2),
and
∥∇W hW (X) −∇W hW ′(X)∥2
H
≤
∞
X
m=1
a2
mµ−α
m (d + 1)[µ−α
m (w2,m −w′
2,m)2 + R2µ−α
m ∥w1,m −w′
1,m∥2]
+ a2
mµ−α
m [µ−α
m (w2,m −w′
2,m)2/R2 + C2
σ(d + 1)µ−α
m ∥w1,m −w′
1,m∥2]
≤
∞
X
m=1
a2
mµ−2α
m
[(d + 1)(1 + R2) + 1/R2 + C2
σ(d + 1)][∥w1,m −w′
1,m∥2 + (w2,m −w′
2,m)2]
20

Published as a conference paper at ICLR 2021
≤c2α1
µ
max
m {µ2(α1−α)
m
}[(d + 1)(1 + R2) + 1/R2 + C2
σ(d + 1)]∥W −W ′∥2
H =: C2∥W −W ′∥2
H,
for a constant 0 < C2 < ∞. Therefore, it holds that
|hW (X) −hW ′(X)|2 ≤C1∥W −W ′∥2
H,
which yields the forth condition, and we also have
∥∇W ℓ(Y, hW (X)) −∇W ℓ(Y, hW ′(X))∥2
H
=∥2(hW (X) −Y )∇W hW (X) −2(hW ′(X) −Y )∇W hW ′(X)∥2
H
≤2∥2(hW (X) −Y )(∇W hW (X) −∇W hW (X))∥2
H
+ 2∥2(hW (X) −hW ′(X))∇W hW ′(X)∥2
H
≤8 ¯RC2∥W −W ′∥2
H + 8C2
1∥W −W ′∥2
H ≲∥W −W ′∥2
H,
which yields the third condition.
Since f o ∈Fγ, there exists W ∗∈Hγ such that f o = fW ∗. Therefore, applying Proposition 4 with
α = α1 (˜α = 1/[2(α1 + 1)]) and θ = γ/(1 + α1) (since γ < 1/2 + α1, the condition θ < 1 −˜α
is satisﬁed), we obtain that for M ≥min

λ1/4γ(α1+1)β1/2γ, λ−1/2(α1+1), n1/2γ	
, the following
excess risk bound holds:
EDn
h
EW (M)
k
[L(W (M)
k
)|Dn] −L(f ∗)
i
≲max

(λβ)
2 ˜
α/θ
1+ ˜
α/θ n−
1
1+ ˜
α/θ , λ−˜αβ−1, λθ, 1/n
	
+ Ξk.
Finally, by noting L(W (M)
k
) −L(f ∗) = ∥fW (M)
k
−f ∗∥2
L2(PX), we obtain the assertion.
Finally, we give the proof of Corollary 1.
Proof of Corollary 1. Note that
fW (x)
=
∞
X
m=1
am ¯w2,mσm(w⊤
1,m[x; 1])
=
∞
X
m=1
µα1
m ¯w2,mµqα2
m µ−qα2
m
µsα2
m σ(µ−α2
m
w⊤
1,m[x; 1]) (∵am = µα1
m , bm = µα2
m )
=
∞
X
m=1
µα1+qα2
m
¯w2,mµ−(s−q)α2
m
σ(µ−α2
m
w⊤
1,m[x; 1]).
Therefore, we may redeﬁne α′
1 ←α1 +qα2 and s′ ←s−q so that we obtain another representation
of the model Fγ:
Fγ =
(
fW (x) =
∞
X
m=1
µα′
1
m ¯w2,mˇσm(w⊤
1,m[x; 1])
 W ∈Hγ, ∥W∥Hγ ≤1
)
,
where ˇσm(·) = µ−s′α2
m
σ(µ−α2
m
·). Note that the condition 0 ≤q ≤s−3 gives s−q ≥3. Therefore,
Assumptions 3 and 4 are valid even for the redeﬁned parameters α′
1, s′ and ˇσm instead of α1, s and
σm. Therefore, we can apply Theorem 2 by simply replacing α1 by α′
1 = α1 + qα2.
21

