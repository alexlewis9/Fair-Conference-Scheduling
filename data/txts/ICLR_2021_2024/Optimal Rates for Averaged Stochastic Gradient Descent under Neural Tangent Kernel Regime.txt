Published as a conference paper at ICLR 2021
OPTIMAL RATES FOR AVERAGED STOCHASTIC GRA-
DIENT DESCENT UNDER NEURAL TANGENT KERNEL
REGIME
Atsushi Nitanda1,2,3,‚Ä†, Taiji Suzuki1,2,‚ãÜ
1Graduate School of Information Science and Technology, The University of Tokyo
2Center for Advanced Intelligence Project, RIKEN
3PRESTO, Japan Science and Technology Agency
Email: ‚Ä†nitanda@mist.i.u-tokyo.ac.jp, ‚ãÜtaiji@mist.i.u-tokyo.ac.jp
ABSTRACT
We analyze the convergence of the averaged stochastic gradient descent for over-
parameterized two-layer neural networks for regression problems. It was recently
found that a neural tangent kernel (NTK) plays an important role in showing the
global convergence of gradient-based methods under the NTK regime, where the
learning dynamics for overparameterized neural networks can be almost character-
ized by that for the associated reproducing kernel Hilbert space (RKHS). However,
there is still room for a convergence rate analysis in the NTK regime. In this study,
we show that the averaged stochastic gradient descent can achieve the minimax
optimal convergence rate, with the global convergence guarantee, by exploiting the
complexities of the target function and the RKHS associated with the NTK. More-
over, we show that the target function speciÔ¨Åed by the NTK of a ReLU network
can be learned at the optimal convergence rate through a smooth approximation of
a ReLU network under certain conditions.
1
INTRODUCTION
Recent studies have revealed why a stochastic gradient descent for neural networks converges to
a global minimum and why it generalizes well under the overparameterized setting in which the
number of parameters is larger than the number of given training examples. One prominent approach
is to map the learning dynamics for neural networks into function spaces and exploit the convexity of
the loss functions with respect to the function. The neural tangent kernel (NTK) (Jacot et al., 2018)
has provided such a connection between the learning process of a neural network and a kernel method
in a reproducing kernel Hilbert space (RKHS) associated with an NTK.
The global convergence of the gradient descent was demonstrated in Du et al. (2019b); Allen-Zhu
et al. (2019a); Du et al. (2019a); Allen-Zhu et al. (2019b) through the development of a theory of
NTK with the overparameterization. In these theories, the positivity of the NTK on the given training
examples plays a crucial role in exploiting the property of the NTK. SpeciÔ¨Åcally, the positivity
of the Gram-matrix of the NTK leads to a rapid decay of the training loss, and thus the learning
dynamics can be localized around the initial point of a neural network with the overparameterization,
resulting in the equivalence between two learning dynamics for neural networks and kernel methods
with the NTK through a linear approximation of neural networks. Moreover, Arora et al. (2019a)
provided a generalization bound of O(T ‚àí1/2), where T is the number of training examples, on a
gradient descent under the positivity assumption of the NTK. These studies provided the Ô¨Årst steps in
understanding the role of the NTK.
However, the eigenvalues of the NTK converge to zero as the number of examples increases, as shown
in Su & Yang (2019) (also see Figure 1), resulting in the degeneration of the NTK. This phenomenon
indicates that the convergence rates in previous studies in terms of generalization are generally slower
than O(T ‚àí1/2) owing to the dependence on the minimum eigenvalue. Moreover, Bietti & Mairal
(2019); Ronen et al. (2019); Cao et al. (2019) also supported this observation by providing a precise
1

Published as a conference paper at ICLR 2021
estimation of the decay of the eigenvalues, and Ronen et al. (2019); Cao et al. (2019) proved the
spectral bias (Rahaman et al., 2019) for a neural network, where lower frequencies are learned Ô¨Årst
using a gradient descent.
By contrast, several studies showed faster convergence rates of the (averaged) stochastic gradient
descent in the RKHS in terms of the generalization (Cesa-Bianchi et al., 2004; Smale & Yao, 2006;
Ying & Zhou, 2006; Neu & Rosasco, 2018; Lin et al., 2020). In particular, by extending the results
in a Ô¨Ånite-dimensional case (Bach & Moulines, 2013), Dieuleveut & Bach (2016); Dieuleveut et al.
(2017) showed convergence rates of O(T
‚àí2rŒ≤
2rŒ≤+1 ) depending on the complexity r ‚àà[1/2, 1] of the
target functions and the decay rate Œ≤ > 1 of the eigenvalues of the kernel (a.k.a. the complexity of
the hypothesis space). In addition, extensions to the random feature settings (Rahimi & Recht, 2007;
Rudi & Rosasco, 2017; Carratino et al., 2018), to the multi-pass variant (Pillaud-Vivien et al., 2018b),
and to the tail-averaging and mini-batching variant (M√ºcke et al., 2019) have been developed.
Motivation.
The convergence rate of O(T
‚àí2rŒ≤
2rŒ≤+1 ) is always faster than O(T ‚àí1/2) and is known as
the minimax optimal rate (Caponnetto & De Vito, 2007; Blanchard & M√ºcke, 2018). Hence, a gap
exists between the theories regarding NTK and kernel methods. In other words, there is still room for
an investigation into a stochastic gradient descent due to a lack of speciÔ¨Åcation of the complexities of
the target function and the hypothesis space. That is, to obtain faster convergence rates, we should
specify the eigenspaces of the NTK that mainly contain the target function (i.e., the complexity of the
target function), and specify the decay rates of the eigenvalues of the NTK (i.e., the complexity of the
hypothesis space), as studied in kernel methods (Caponnetto & De Vito, 2007; Steinwart et al., 2009;
Dieuleveut & Bach, 2016). In summary, the fundamental question in this study is
Can stochastic gradient descent for overparameterized neural networks achieve the optimal rate in
terms of the generalization by exploiting the complexities of the target function and hypothesis space?
In this study, we answer this question in the afÔ¨Årmative, thereby bridging the gap between the theories
of overparameterized neural networks and kernel methods.
0
5000
10000
k
10‚àí5
10‚àí3
10‚àí1
k¬≠th¬≠eigenvalue
d=5
0
5000
10000
k
10‚àí4
10‚àí¬†
d=10
0
5000
10000
k
10‚àí4
10‚àí¬†
d=100
Figure 1: An estimation of the eigenvalues of Œ£‚àûusing two-layer ReLU networks with a width of
M = 2 √ó 104. The number of uniformly randomly generated samples on the unit sphere is n = 104
and the dimensionality of the input space is d ‚àà{5, 10, 100}.
1.1
CONTRIBUTIONS
The connection between neural networks and kernel methods is being understood via the NTK, but it
is still unknown whether the optimal convergence rate faster than O(T ‚àí1/2) is achievable by a certain
algorithm for neural networks. This is the Ô¨Årst paper to overcome technical challenges of achieving
the optimal convergence rate under the NTK regime. We obtain the minimax optimal convergence
rates (Corollary 1), inherited from the learning dynamics in an RKHS, for an averaged stochastic
gradient descent for neural networks. That is, we show that smooth target functions efÔ¨Åciently
speciÔ¨Åed by the NTK are learned rapidly at faster convergence rates than O(1/
‚àö
T). Moreover, we
obtain an explicit optimal convergence rate of O

T
‚àí2rd
2rd+d‚àí1

for a smooth approximation of the
ReLU network (Corollary 2), where d is the dimensionality of the data space and r is the complexity
of the target function speciÔ¨Åed by the NTK of the ReLU network.
1.2
TECHNICAL CHALLENGE
The key to showing a global convergence (Theorem 1) is making the connection between kernel
methods and neural networks in some sense. Although this sort of analysis has been developed
in several studies (Du et al., 2019b; Arora et al., 2019a; Weinan et al., 2019; Arora et al., 2019b;
2

Published as a conference paper at ICLR 2021
Lee et al., 2019; 2020), we would like to emphasize that our results cannot be obtained by direct
application of their results. A naive idea is to simply combine their results with the convergence
analysis of the stochastic gradient descent for kernel methods, but it does not work. The main reason
is that we need the L2-bound weighted by a true data distribution on the gap between dynamics of
stochastic gradient descent for neural networks and kernel methods if we try to derive a convergence
rate of population risks for neural networks from that for kernel methods. However, such a bound is
not provided in related studies. Indeed, to the best of our knowledge, all related studies make this
kind of connection regarding the gap on training dataset or sample-wise high probability bound (Lee
et al., 2019; Arora et al., 2019b). That is, a statement ‚Äúfor every input data x with high probability
|g(t)
nn(x) ‚àíg(t)
ntk(x)| < œµ‚Äù cannot yield a desired statement ‚Äú‚à•g(t)
nn ‚àíg(t)
ntk‚à•L2(œÅX) < œµ‚Äù where g(t)
nn and
g(t)
ntk are t-th iterate of gradient descent for a neural network and corresponding iterate described by
NTK, and ‚à•¬∑‚à•L2(œÅX) is the L2-norm weighted by a marginal data distribution œÅX over the input space.
Moreover, we note that we cannot utilize the positivity of the Gram-matrix of NTK which plays a
crucial role in related studies because we consider the population risk with respect to ‚à•¬∑ ‚à•L2(œÅX)
rather than the empirical risk.
To overcome these difÔ¨Åculties we develop a different strategy of the proof. First, we make a bound
on the gap between two dynamics of the averaged stochastic gradient descent for a two-layer neural
network and its NTK with width M (Proposition A), and obtain a generalization bound for this
intermediate NTK (Theorem A in Appendix). Second, we remove the dependence on the width of M
from the intermediate bound. These steps are not obvious because we need a detailed investigation
to handle the misspeciÔ¨Åcation of the target function by an intermediate NTK. Based on detailed
analyses, we obtain a faster and precise bound than those in previous results (Arora et al., 2019a).
The following is an informal version of Proposition A providing a new connection between a two-layer
neural networks and corresponding NTK with width M.
Proposition 1 (Informal). Under appropriate conditions we simultaneously run averaged stochastic
gradient descent for a neural network with width of M and for its NTK. Assume they share the same
hyper-parameters and examples to compute stochastic gradients. Then, for arbitrary number of
iterations T ‚ààZ+ and œµ > 0, there exists M ‚ààZ+ depending only on T and œµ such that ‚àÄt ‚â§T,
‚à•g(t)
nn ‚àíg(t)
ntk‚à•L‚àû(œÅX) ‚â§œµ,
where g(t)
nn and g(t)
ntk are iterates obtained by averaged stochastic gradient descent.
This proposition is the key because it connects two learning dynamics for a neural network and its
NTK through overparameterization without the positivity of the NTK. Instead of the positivity, this
proposition says that overparameterization increases the time stayed in the NTK regime where the
learning dynamics for neural networks can be characterized by the NTK. As a result, the averaged
stochastic gradient descent for the overparameterized two-layer neural networks can fully inherit
preferable properties from learning dynamics in the NTK as long as the network width is sufÔ¨Åciently
large. See Appendix A for detail.
1.3
ADDITIONAL RELATED WORK
Besides the abovementioned studies, there are several works (Chizat & Bach, 2018b; Wu et al.,
2019; Zou & Gu, 2019) that have shown the global convergence of (stochastic) gradient descent for
overparameterized neural networks essentially relying on the positivity condition of NTK. Moreover,
faster convergence rates of the second-order methods such as the natural gradient descent and Gauss-
Newton method have been demonstrated (Zhang et al., 2019; Cai et al., 2019) in the similar setting,
and the further improvement of Gauss-Newton method with respect to the cost per iteration has been
conducted in Brand et al. (2020).
There have been several attempts to improve the overparameterization size in the NTK theory. For
the regression problem, Song & Yang (2019) has succeeded in reducing the network width required
in Du et al. (2019b) by utilizing matrix Chernoff bound. For the classiÔ¨Åcation problem, the positivity
condition can be relaxed to a separability condition using another reference model (Cao & Gu,
2019a;b; Nitanda et al., 2019; Ji & Telgarsky, 2019), resulting in mild overparameterization and
generalization bounds of O(T ‚àí1/2) or O(T ‚àí1/4) on classiÔ¨Åcation errors.
For an averaged stochastic gradient descent on classiÔ¨Åcation problems in RKHSs, linear convergence
rates of the expected classiÔ¨Åcation errors have been demonstrated in Pillaud-Vivien et al. (2018a);
3

Published as a conference paper at ICLR 2021
Nitanda & Suzuki (2019). Although our study focuses on regression problems, we describe how to
combine their results with our theory in the Appendix.
The mean Ô¨Åeld regime (Nitanda & Suzuki, 2017; Mei et al., 2018; Chizat & Bach, 2018a) that is a
different limit of neural networks from the NTK is also important for the global convergence analysis
of the gradient descent. In the mean Ô¨Åeld regime, the learning dynamics follows the Wasserstein
gradient Ô¨Çow which enables us to establish convergence analysis in the probability space.
Moreover, several studies (Allen-Zhu & Li, 2019; Bai & Lee, 2019; Ghorbani et al., 2019; Allen-Zhu
& Li, 2020; Li et al., 2020; Suzuki, 2020) attempt to show the superiority of neural networks over
kernel methods including the NTK. Although it is also very important to study the conditions beyond
the NTK regime, they do not affect our contribution and vice versa. Indeed, which method is better
depends on the assumption on the target function and data distribution, so it is important to investigate
the optimal convergence rate and optimal method in each regime. As shown in our study, the averaged
stochastic gradient descent for learning neural network achieves the optimal convergence rate if the
target function is included in RKHS associated with the NTK with the small norm. It means there are
no methods that outperform the averaged stochastic gradient descent under this setting.
2
PRELIMINARY
Let X ‚äÇRd and Y ‚äÇR be the measurable feature and label spaces, respectively. We denote by œÅ a
data distribution on X √ó Y, by œÅX the marginal distribution on X, and by œÅ(¬∑|X) the conditional
distribution on Y , where (X, Y ) ‚àºœÅ. Let ‚Ñì(z, y) (z ‚ààR, y ‚ààY) be the squared loss function
1
2(z ‚àíy)2, and let g : X ‚ÜíR be a hypothesis. The expected risk function is deÔ¨Åned as follows:
L(g)
def
= E(X,Y )‚àºœÅ[‚Ñì(g(X), Y )].
(1)
The Bayes rule gœÅ : X ‚ÜíR is a global minimizer of L over all measurable functions.
For the least squares regression, the Bayes rule is known to be gœÅ(X) = EY [Y |X] and the excess
risk of a hypothesis g (which is the difference between the expected risk of g and the expected risk
of the Bayes rule gœÅ) is expressed as a squared L2(œÅX)-distance between g and gœÅ (for details, see
Cucker & Smale (2002)) up to a constant:
L(g) ‚àíL(gœÅ) = 1
2‚à•g ‚àígœÅ‚à•2
L2(œÅX),
where ‚à•¬∑ ‚à•L2(œÅX) is L2-norm weighted by œÅX deÔ¨Åned as ‚à•g‚à•L2(œÅX)
def
=
 R
g2(X)dœÅX(X)
1/2
(g ‚ààL2(œÅX)). Hence, the goal of the regression problem is to approximate gœÅ in terms of the
L2(œÅX)-distance in a given hypothesis class.
Two-layer neural networks.
The hypothesis class considered in this study is the set of two-layer
neural networks, which is formalized as follows. Let M ‚ààZ+ be the network width (number of
hidden nodes). Let a = (a1, . . . , aM)‚ä§‚ààRM (ar ‚ààR) be the parameters of the output layer, B =
(b1, . . . , bM) ‚ààRd√óM (br ‚ààRd) be the parameters of the input layer, and c = (c1, . . . , cM)‚ä§‚ààRM
(cr ‚ààR) be the bias parameters. We denote by Œò the collection of all parameters (a, B, c), and
consider two-layer neural networks:
gŒò(x) =
1
‚àö
M
M
X
r=1
arœÉ(b‚ä§
r x + Œ≥cr),
(2)
where œÉ : R ‚ÜíR is an activation function and Œ≥ > 0 is a scale of the bias terms.
Symmetric initialization.
We adopt symmetric initialization for the parameters Œò. Let a(0) =
(a(0)
1 , . . . , a(0)
M )‚ä§, B(0) = (b(0)
1 , . . . , b(0)
M ), and c(0) = (c(0)
1 , . . . , c(0)
M )‚ä§denote the initial values
for a, B, and c, respectively. Assume that the number of hidden units M ‚ààZ+ is even. The
parameters for the output layer are initialized as a(0)
r
= R for r ‚àà{1, . . . , M
2 } and a(0)
r
= ‚àíR for
r ‚àà{ M
2 + 1, . . . , M}, where R > 0 is a positive constant. Let ¬µ0 be a uniform distribution on the
sphere Sd‚àí1 = {b ‚ààRd | ‚à•b‚à•2 = 1} ‚äÇRd used to initialize the parameters for the input layer. The
parameters for the input layer are initialized as b(0)
r
= b(0)
r+ M
2 for r ‚àà{1, . . . , M
2 }, where (b(0)
r )
M
2
r=1
4

Published as a conference paper at ICLR 2021
are independently drawn from the distribution ¬µ0. The bias parameters are initialized as c(0)
r
= 0 for
r ‚àà{1, . . . , M}. The aim of the symmetric initialization is to make an initial function gŒò(0) = 0,
where Œò(0) = (a(0), B(0), c(0)). This is just for theoretical simplicity. Indeed, we can relax the
symmetric initialization by considering an additional error stemming from the nonzero initialization
in the function space.
Regularized expected risk minimization.
Instead of minimizing the expected risk (1) itself, we
consider the minimization problem of the regularized expected risk around the initial values:
min
Œò

L(gŒò) + Œª
2

‚à•a ‚àía(0)‚à•2
2 + ‚à•B ‚àíB(0)‚à•2
F + ‚à•c ‚àíc(0)‚à•2
2

.
(3)
where the last term is the L2-regularization at an initial point with a regularization parameter Œª > 0.
This regularization forces iterations obtained by optimization algorithms to stay close to the initial
value, which enables us to utilize the better convergence property of regularized kernel methods.
Averaged stochastic gradient descent.
Stochastic gradient descent is the most popular method
for solving large-scale machine learning problems, and its averaged variant is also frequently used
to stabilize and accelerate the convergence. In this study, we analyze the generalization ability
of an averaged stochastic gradient descent. The update rule is presented in Algorithm 1. Let
Œò(t) = (a(t), B(t), c(t)) denote the collection of t-th iterates of parameters a ‚ààRM, B ‚ààRd√óM,
and c ‚ààRM. At t-th iterate, stochastic gradient descent using a learning rate Œ∑t for the problem
(3) with respect to a, B, c is performed on lines 4‚Äì6 for a randomly sampled example (xt, yt) ‚àºœÅ.
These updates can be rewritten in an element-wise fashion as follows. For r ‚àà{1, . . . , M},
a(t+1)
r
‚àía(0)
r
= (1 ‚àíŒ∑tŒª)(a(t)
r
‚àía(0)
r ) ‚àíŒ∑tM ‚àí1/2(gŒò(t)(xt) ‚àíyt)œÉ(b(t)‚ä§
r
xt + Œ≥c(t)
r ),
b(t+1)
r
‚àíb(0)
r
= (1 ‚àíŒ∑tŒª)(b(t)
r
‚àíb(0)
r ) ‚àíŒ∑tM ‚àí1/2(gŒò(t)(xt) ‚àíyt)a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt + Œ≥c(t)
r )xt,
c(t+1)
r
‚àíc(0)
r
= (1 ‚àíŒ∑tŒª)(c(t)
r
‚àíc(0)
r ) ‚àíŒ∑tM ‚àí1/2(gŒò(t)(xt) ‚àíyt)a(t)
r Œ≥œÉ‚Ä≤(b(t)‚ä§
r
xt + Œ≥c(t)
r ),
where a(t) = (a(t)
1 , . . . , a(t)
M )‚ä§, B(t) = (b(t)
1 , . . . , b(t)
M ), and c(t) = (c(t)
1 , . . . , c(t)
M )‚ä§. Finally, a
weighted average using weights Œ±t of the history of parameters is computed on line 9. In our theory,
we consider the constant learning rate Œ∑t = Œ∑ and uniform averaging Œ±t = 1/(T + 1).
Algorithm 1 Averaged Stochastic Gradient Descent
1: Input: number of iterations T, regularization parameter Œª, learning rates (Œ∑t)T ‚àí1
t=0 , averaging
weights (Œ±t)T
t=0, initial values Œò(0) = (a(0), B(0), c(0))
2: for t = 0 to T ‚àí1 do
3:
Randomly draw a sample (xt, yt) ‚àºœÅ
4:
a(t+1) ‚Üêa(t) ‚àíŒ∑t‚àÇa‚Ñì(gŒò(t)(xt), yt) ‚àíŒ∑tŒª(a(t) ‚àía(0))
5:
B(t+1) ‚ÜêB(t) ‚àíŒ∑t‚àÇB‚Ñì(gŒò(t)(xt), yt) ‚àíŒ∑tŒª(B(t) ‚àíB(0))
6:
c(t+1) ‚Üêc(t) ‚àíŒ∑t‚àÇc‚Ñì(gŒò(t)(xt), yt) ‚àíŒ∑tŒª(c(t) ‚àíc(0))
7:
Œò(t+1) ‚Üê(a(t+1), B(t+1), c(t+1))
8: end for
9: Œò
(T ) = (PT
t=0 Œ±ta(t), PT
t=0 Œ±tB(t), PT
t=0 Œ±tc(t))
10: Return gŒò
(T )
Integral and Covariance Operators.
The integral and covariance operators associated with the
kernels, which are the limit of the Gram-matrix as the number of examples goes to inÔ¨Ånity, play
a crucial role in determining the learning speed. For a given Hilbert space H, we denote by ‚äóH
the tensor product on H, that is, ‚àÄ(f, g) ‚ààH2, f ‚äóH g deÔ¨Ånes a linear operator; h ‚ààH 7‚Üí
(f ‚äóH g)h = ‚ü®f, h‚ü©H g ‚ààH. Note that f ‚äóH g naturally induces a bilinear function: (h, h‚Ä≤) ‚àà
H √ó H 7‚Üí‚ü®(f ‚äóH g)h, h‚Ä≤‚ü©H = ‚ü®f, h‚ü©H ‚ü®g, h‚Ä≤‚ü©H. When H is a reproducing kernel Hilbert space
(RKHS) associated with a bounded kernel k : X √ó X ‚ÜíR, the covariance operator Œ£ : H 7‚ÜíH is
deÔ¨Åned as follows: Set KX
def
= k(X, ¬∑) and
Œ£ = EX‚àºœÅX[KX ‚äóH KX].
5

Published as a conference paper at ICLR 2021
Note that the covariance operator is a restriction of the integral operator on L2(œÅX):
f ‚ààL2(œÅX) 7‚àí‚ÜíŒ£f =
Z
X
f(X)KXdœÅX ‚ààL2(œÅX).
We use the same symbol as above for convenience with a slight abuse of notation.
Be-
cause Œ£ is a compact self-adjoint operator on L2(œÅX), Œ£ has the following eigendecomposi-
tion: Œ£f = P‚àû
i=1 Œªi ‚ü®f, œÜi‚ü©L2(œÅX) œÜi for f ‚ààL2(œÅX), where {(Œªi, œÜi)}‚àû
i=1 is a pair of eigen-
values and orthogonal eigenfunctions in L2(œÅX).
For s ‚ààR, the power Œ£s is deÔ¨Åned as
Œ£sf = P‚àû
i=1 Œªs
i ‚ü®f, œÜi‚ü©L2(œÅX) œÜi.
3
MAIN RESULTS: MINIMAX OPTIMAL CONVERGENCE RATES
In this section, we present the main results regarding the fast convergence rates of the averaged
stochastic gradient descent under a certain condition on the NTK and target function gœÅ.
Neural tangent kernel.
The NTK is a recently developed kernel function and has been shown to
be extremely useful in demonstrating the global convergence of the gradient descent method for
neural networks (cf., Jacot et al. (2018); Chizat & Bach (2018b); Du et al. (2019b); Allen-Zhu et al.
(2019a;b); Arora et al. (2019a)). The NTK in our setting is deÔ¨Åned as follows: ‚àÄx, ‚àÄx‚Ä≤ ‚ààX,
k‚àû(x, x‚Ä≤)
def
= Eb(0)[œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)] + R2(x‚ä§x‚Ä≤ + Œ≥2)Eb(0)[œÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)],
(4)
where the expectation is taken with respect to b(0) ‚àº¬µ0. The NTK is the key to the global convergence
of a neural network because it makes a connection between the (averaged) stochastic gradient descent
for a neural network and the RKHS associated with k‚àû(see Proposition A). Although this type of
connection has been shown in previous studies (Arora et al., 2019b; Weinan et al., 2019; Lee et al.,
2019; 2020), note that their results are inapplicable to our theory because we consider the population
risk. Indeed, our study is the Ô¨Årst to establish this connection for an (averaged) stochastic gradient
descent in terms of the uniform distance on the support of the data distribution, enabling us to obtain
faster convergence rates. We note that an NTK k‚àûis the sum of two NTKs, that is, the Ô¨Årst and
second terms in (4) are NTKs for the output and input layers with bias, respectively.
3.1
GLOBAL CONVERGENCE ANALYSIS
Let H‚àûbe an RKHS associated with NTK k‚àû, and let Œ£‚àûbe the corresponding integral operator.
Let {Œªi}‚àû
i=1 denote the eigenvalues of Œ£‚àûsorted in decreasing order: Œª1 ‚â•Œª2 ‚â•¬∑ ¬∑ ¬∑ .
Assumption 1.
(A1) There exists C > 0 such that ‚à•œÉ‚Ä≤‚Ä≤‚à•‚àû‚â§C, ‚à•œÉ‚Ä≤‚à•‚àû‚â§2, and |œÉ(u)| ‚â§1 + |u| for ‚àÄu ‚ààR.
(A2) supp(œÅX) ‚äÇ{x ‚ààRd | ‚à•x‚à•2 ‚â§1}, Y ‚äÇ[‚àí1, 1], R = 1, and Œ≥ ‚àà[0, 1].
(A3) There exists r ‚àà[1/2, 1] such that gœÅ ‚ààŒ£r
‚àû(L2(œÅX)), i.e., ‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX) < ‚àû.
(A4) There exists Œ≤ > 1 such that Œªi = Œò(i‚àíŒ≤).
Remark.
‚Ä¢ (A1): Typical smooth activation functions, such as sigmoid and tanh functions, and smooth
approximations of the ReLU, such as swish (Ramachandran et al., 2017), which performs as
well as or even better than the ReLU, satisfy Assumption (A1). This condition is used to relate
the two learning dynamics between neural networks and kernel methods (see Proposition A).
‚Ä¢ (A2): The boundedness (A2) of the feature space and label are often assumed for stochastic
optimization and least squares regression for theoretical guarantees (see Steinwart et al. (2009)).
Note that these constants in (A2) can be relaxed to arbitrary constants.
‚Ä¢ (A3): Assumption (A3) measures the complexity of gœÅ because Œ£‚àûcan be considered as a
smoothing operator using a kernel k‚àû. A larger r indicates a faster decay of the coefÔ¨Åcients of
expansion of gœÅ based on the eigenfunctions of Œ£‚àûand smoothens gœÅ. In addition, Œ£r
‚àû(L2(œÅX))
6

Published as a conference paper at ICLR 2021
shrinks with respect to r and Œ£1/2
‚àû(L2(œÅX)) = H‚àû, resulting in gœÅ ‚ààH‚àû. This condition
is used to control the bias of the estimators through L2-regularization. The notation Œ£‚àír
‚àûgœÅ
represents any function G ‚ààL2(œÅX) such that gœÅ = Œ£r
‚àûG.
‚Ä¢ (A4): Assumption (A4) controls the complexity of the hypothesis class H‚àû. A larger Œ≤ indicates
a faster decay of the eigenvalues and makes H‚àûsmaller. This assumption is essentially needed
to bound the variance of the estimators efÔ¨Åciently and derive a fast convergence rate. Theorem
1 and Corollary 1, 2 hold even though the condition in (A4) is relaxed to Œªi = O(i‚àíŒ≤) and the
lower bound Œªi = ‚Ñ¶(i‚àíŒ≤) is necessary only for making obtained rates minimax optimal.
Under these assumptions, we derive the convergence rate of the averaged stochastic gradient descent
for an overparameterized two-layer neural network, the proof is provided in the Appendix.
Theorem 1. Suppose Assumptions (A1)-(A3) hold. Run Algorithm 1 with a constant learning rate Œ∑
satisfying 4(6 + Œª)Œ∑ ‚â§1. Then, for any œµ > 0, ‚à•Œ£‚àû‚à•op ‚â•Œª > 0, Œ¥ ‚àà(0, 1), and T ‚ààZ+, there
exists M0 ‚ààZ+ such that for any M ‚â•M0, the following holds with high probability at least 1 ‚àíŒ¥
over the random choice of features Œò(0):
E

‚à•gŒò
(T ) ‚àígœÅ‚à•2
L2(œÅX)

‚â§œµ + Œ±

Œª2r‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX) +
1
T + 1‚à•gœÅ‚à•2
H‚àû+
1
ŒªŒ∑2(T + 1)2 ‚à•gœÅ‚à•2
H‚àû

+
Œ±
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + ‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
,
where Œ± > 0 is a universal constant and gŒò
(T ) is an iterate obtained through Algorithm 1.
Remark.
The Ô¨Årst term œµ and second term Œª2r‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX) are the approximation error and
bias, which can be chosen to be arbitrarily small. The Ô¨Årst term comes from the approximation of
the NTK using Ô¨Ånite-sized neural networks, and the second term comes from the L2-regularization,
which coincides with a bias term in the theory of least squares regression (Caponnetto & De Vito,
2007). The third and fourth terms come from the convergence of the averaged semi-stochastic
gradient descent (which is considered in the proof) in terms of the optimization. The appearance of
an inverse dependence on Œª in the fourth term is common because a smaller Œª indicates a weaker
strong convexity, which slows down the convergence speed of the optimization methods (Rakhlin
et al., 2012). The term Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
is the variance from the stochastic approximation of
the gradient, and it is referred to as the degree of freedom or the effective dimension, which is known
to be unavoidable in kernel regression problems (Caponnetto & De Vito, 2007; Dieuleveut & Bach,
2016; Rudi & Rosasco, 2017).
Global convergence in NTK regime.
This theorem shows the global convergence to the Bayes
rule gœÅ, which is a minimizer over all measurable maps because the approximation term œµ can be
arbitrarily small by taking a sufÔ¨Åciently large network width M. The required value of M has an
exponential dependence on T; note, however, that reducing M is not the main focus of the present
study. The key technique is to relate two learning dynamics for two-layer neural networks and kernel
methods in an RKHS approximating H‚àûup to a small error. Unlike existing studies (Du et al.,
2019b; Arora et al., 2019a;b; Weinan et al., 2019; Lee et al., 2019; 2020) showing such connections,
we establish this connection in term of the L‚àû(œÅX)-norm, which is more useful in a generalization
analysis. Moreover, existing studies essentially rely on the strict positivity of the Gram-matrix to
localize all iterates around an initial value, which can slow down the convergence rate in terms of the
generalization because the convergence of the eigenvalues of the NTK to zero affects the Rademacher
complexity. By contrast, our theory succeeds in demonstrating the global convergence in the NTK
regime without the positivity of the NTK.
3.2
OPTIMAL CONVERGENCE RATE
We derive the fast convergence rate from Theorem 1 by utilizing Assumption (A4), which deÔ¨Ånes
the complexity of the NTK. The regularization parameter Œª mainly controls the trade-off within the
generalization bound, that is, a smaller value decreases the bias term but increases the variance term
including the degree of freedom. The degree of freedom Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
can be speciÔ¨Åed
by imposing Assumption (A4) because it determines the decay rate of the eigenvalues of Œ£‚àû. As a
result, this trade-off between bias and variance depending on the choice of Œª becomes clear, and we
7

Published as a conference paper at ICLR 2021
can determine the optimal value. Concretely, by setting Œª = T ‚àíŒ≤/(2rŒ≤+1), the sum of the bias and
variance terms is minimized, and these terms become asymptotically equivalent.
Corollary 1. Suppose Assumptions (A1)-(A4) hold. Run Algorithm 1 with the constant learning
rate Œ∑ = O(1) satisfying 4(6 + Œª)Œ∑ ‚â§1 and Œª = T ‚àíŒ≤/(2rŒ≤+1). Then, for any œµ > 0, Œ¥ ‚àà(0, 1) and
T ‚ààZ+ satisfying ‚à•Œ£‚àû‚à•op ‚â•Œª, there exists M0 ‚ààZ+ such that for any M ‚â•M0, the following
holds with high probability at least 1 ‚àíŒ¥ over the random choice of random features Œò(0):
E

‚à•gŒò
(T ) ‚àígœÅ‚à•2
L2(œÅX)

‚â§œµ + Œ±T
‚àí2rŒ≤
2rŒ≤+1

1 + ‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

,
where Œ± > 0 is a universal constant and gŒò
(T ) is an iterate obtained by Algorithm 1.
The resulting convergence rate is O(T
‚àí2rŒ≤
2rŒ≤+1 ) with respect to T by considering a sufÔ¨Åciently large
network width of M such that the error œµ stemming from the approximation of NTK can be ignored.
Because T corresponds to the number of examples used to learn a predictor gŒò
(T ), this convergence
rate is simply the generalization error bound for the averaged stochastic gradient descent. In general,
this rate is always faster than T ‚àí1/2 and is known to be the minimax optimal rate of estimation
(Caponnetto & De Vito, 2007; Blanchard & M√ºcke, 2018) in H‚àûin the following sense. Let P(Œ≤, r)
be a data distribution class satisfying Assumptions (A2)-(A4). Then,
lim
œÑ‚Üí0 lim inf
T ‚Üí‚àûinf
h(T ) sup
œÅ P
h
‚à•h(T ) ‚àígœÅ‚à•2
L2(œÅX) > œÑT
‚àí2rŒ≤
2rŒ≤+1
i
= 1,
where œÅ is taken in P(Œ≤, r) and h(T ) is taken over all mappings (xt, yt)T ‚àí1
t=0 7‚Üíh(T ) ‚ààH‚àû.
3.3
EXPLICIT OPTIMAL CONVERGENCE RATE FOR SMOOTH APPROXIMATION OF RELU
For smooth activation functions that sufÔ¨Åciently approximate the ReLU, an optimal explicit conver-
gence rate can be derived under the setting in which the target function is speciÔ¨Åed by NTK with
the ReLU, and the data are distributed uniformly on a sphere. We denote the ReLU activation by
œÉ(u) = max{0, u} and a smooth approximation of ReLU by œÉ(s), which converges to ReLU, as
s ‚Üí‚àûin the following sense. We make alternative assumptions to (A1), (A2), and (A3):
Assumption 2.
(A1‚Äô) œÉ(s) satisÔ¨Åes (A1). œÉ(s) and œÉ(s)‚Ä≤ converge pointwise almost surely to œÉ and œÉ‚Ä≤ as s ‚Üí‚àû.
(A2‚Äô) œÅX is a uniform distribution on {x ‚ààRd | ‚à•x‚à•2 = 1}. Y ‚äÇ[‚àí1, 1], R = 1, and Œ≥ ‚àà(0, 1].
(A3‚Äô) The condition (A3) is satisÔ¨Åed by the NTK associated with the ReLU activation œÉ.
(A1‚Äô) and (A2‚Äô) are special cases of (A1) and (A2). There are several activation functions that satisfy
this condition, including swish (Ramachandran et al., 2017): œÉ(s)(u) =
u
1+exp(‚àísu). Under these
conditions, we can estimate the decay rate of the eigenvalues for the ReLU as Œ≤ = 1 +
1
d‚àí1, yielding
the explicit optimal convergence rate by adapting the proof of Theorem 1 to the current setting. Note
that Algorithm 1 is run for a neural network with a smooth approximation œÉ(s) of the ReLU.
Corollary 2. Suppose Assumptions (A1‚Äô), (A2‚Äô), and (A3‚Äô) hold. Run Algorithm 1 with the constant
learning rate Œ∑ = O(1) satisfying 4(6 + Œª)Œ∑ ‚â§1, and Œª = T ‚àíd/(2rd+d‚àí1). Given any œµ > 0,
Œ¥ ‚àà(0, 1) and T ‚ààZ+ satisfying ‚à•Œ£‚àû‚à•op ‚â•2Œª, let s be an arbitrary and sufÔ¨Åciently large positive
value. Then, there exists M0 ‚ààZ+ such that for any M ‚â•M0, the following holds with high
probability at least 1 ‚àíŒ¥ over the random choice of random features Œò(0):
E

‚à•gŒò
(T ) ‚àígœÅ‚à•2
L2(œÅX)

‚â§œµ + Œ±T
‚àí2rd
2rd+d‚àí1

1 + ‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

,
where Œ± > 0 is a universal constant and gŒò
(T ) is an iterate obtained by Algorithm 1.
4
EXPERIMENTS
We verify the importance of the speciÔ¨Åcation of target functions by showing the misspeciÔ¨Åcation
signiÔ¨Åcantly slows down the convergence speed. To evaluate the misspeciÔ¨Åcation, we consider
8

Published as a conference paper at ICLR 2021
single-layer learning as well as the two-layer learning, and we see the advantage of two-layer learning.
Here, note that, with evident modiÔ¨Åcation of the proofs, the counterparts of Corollaries 1 and 2
for learning a single layer also hold by replacing Œ£‚àûwith the covariance operator Œ£a,‚àû(Œ£b,‚àû)
associated with ka,‚àû(kb,‚àû), where
ka,‚àû(x, x‚Ä≤) = Eb(0)[œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)],
kb,‚àû(x, x‚Ä≤) = R2(x‚ä§x‚Ä≤ + Œ≥2)Eb(0)[œÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)],
which are components of k‚àû= ka,‚àû+kb,‚àûcorresponding to the output and input layers, respectively.
Then, from Corollaries 1 and 2, a Bayes rule gœÅ is learned efÔ¨Åciently by optimizing the layer which
has a small norm ‚à•Œ£‚àírgœÅ‚à•L2(œÅX) for Œ£ ‚àà{Œ£a,‚àû, Œ£b,‚àû, Œ£‚àû}.
0.5
0.6
0.7
0.8
0.9
1.0
r
103
105
107
109
1011
L2¬≠norm
Œ£a, ‚àû
Œ£b, ‚àû
Œ£‚àû
0.5
0.6
0.7
0.8
0.9
1.0
r
104
108
1012
1016
Œ£a, ‚àû
Œ£b, ‚àû
Œ£‚àû
0.5
0.6
0.7
0.8
0.9
1.0
r
104
108
1012
1016
Œ£a, ‚àû
Œ£b, ‚àû
Œ£‚àû
Figure 2:
Top:
Estimation of ‚à•Œ£‚àírgœÅ‚à•L2(œÅX) (r
‚àà
[0.5, 1]) for integral operators Œ£
‚àà
{Œ£a,‚àû, Œ£b,‚àû, Œ£‚àû} of two-layer ReLU networks. Bayes rules gœÅ are set to the average eigen-
functions of Œ£a,‚àû(left), Œ£b,‚àû(middle), and Œ£‚àû(right). Bottom: Learning curves of test errors for
Algorithm 1 with two-layer swish networks.
Experimental settings.
Figure 2 (Top) depicts norms ‚à•Œ£‚àírgœÅ‚à•L2(œÅX) for Œ£ ‚àà{Œ£a,‚àû, Œ£b,‚àû, Œ£‚àû}.
Bayes rules gœÅ are averages of eigenfunctions of Œ£a,‚àû(left), Œ£b,‚àû(middle), and Œ£‚àû(right) corre-
sponding to the 10-largest eigenvalues excluding the Ô¨Årst and second, with the setting: R = 1/(20
‚àö
2),
Œ≥ = 10
‚àö
2, and œÅX is the uniform distribution on the unit sphere in R2. To estimate eigenvalues and
eigenfunctions, we draw 104-samples from œÅX and M = 2 √ó 104-hidden nodes of a two-layer ReLU.
Empirical observations.
We observe gœÅ has the smallest norm with respect to the integral operator
which speciÔ¨Åes gœÅ and has a comparably small norm with respect to Œ£‚àûeven for the cases where gœÅ
is speciÔ¨Åed by Œ£a,‚àûor Œ£b,‚àû. This observation suggests the efÔ¨Åciency of learning a corresponding
layer to gœÅ and learning both layers, and it is empirically veriÔ¨Åed. We run Algorithm 1 10-times with
respect to output (blue), input (purple), and both layers (orange) of two-layer swish networks with
s = 10. Figure 2 (Bottom) depicts the average and standard deviation of test errors. From the Ô¨Ågure,
we see that learning a corresponding layer to gœÅ and both layers exhibit faster convergence, and that
misspeciÔ¨Åcation signiÔ¨Åcantly slows down the convergence speed in all cases.
5
CONCLUSION
We analyzed the convergence of the averaged stochastic gradient descent for overparameterized two-
layer neural networks for a regression problem. Through the development of a new proof strategy that
does not rely on the positivity of the NTK, we proved that the global convergence (Theorem 1) relies
only on the overparameterization. Moreover, we demonstrated the minimax optimal convergence
rates (Corollary 1) in terms of the generalization error depending on the complexities of the target
function and the hypothesis class and showed the explicit optimal rate for the smooth approximation
of the ReLU.
9

Published as a conference paper at ICLR 2021
ACKNOWLEDGMENTS
AN was partially supported by JSPS Kakenhi (19K20337) and JST-PRESTO. TS was partially
supported by JSPS KAKENHI (18K19793, 18H03201, and 20H00576), Japan Digital Design, and
JST CREST.
REFERENCES
Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efÔ¨Åciently, going beyond kernels? In
Advances in Neural Information Processing Systems 32, pp. 9017‚Äì9028, 2019.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of International Conference on Machine Learning 36, pp.
242‚Äì252, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent neural
networks. In Advances in neural information processing systems, pp. 6676‚Äì6688, 2019b.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceedings
of International Conference on Machine Learning 36, pp. 322‚Äì332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an inÔ¨Ånitely wide neural net. In Advances in Neural Information Processing
Systems, pp. 8139‚Äì8148, 2019b.
Kendall Atkinson and Weimin Han. Spherical harmonics and approximations on the unit sphere: an
introduction. Springer, 2012.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629‚Äì681, 2017a.
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.
The Journal of Machine Learning Research, 18(1):714‚Äì751, 2017b.
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with
convergence rate O(1/n). In Advances in Neural Information Processing Systems 26, pp. 773‚Äì781,
2013.
Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2019.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classiÔ¨Åcation, and risk bounds.
Journal of the American Statistical Association, 101(473):138‚Äì156, 2006.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In Advances in
Neural Information Processing Systems, pp. 12873‚Äì12884, 2019.
Gilles Blanchard and Nicole M√ºcke. Optimal rates for regularization of statistical inverse learning
problems. Foundations of Computational Mathematics, 18(4):971‚Äì1013, 2018.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. arXiv preprint arXiv:2006.11648, 2020.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
Gram-gauss-newton method: Learning overparameterized neural networks for regression problems.
arXiv preprint arXiv:1905.11675, 2019.
Yuan Cao and Quanquan Gu.
A generalization theory of gradient descent for learning over-
parameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.
10

Published as a conference paper at ICLR 2021
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. arXiv preprint arXiv:1905.13210, 2019b.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198, 2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331‚Äì368, 2007.
Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco. Learning with sgd and random features. In
Advances in Neural Information Processing Systems 31, pp. 10192‚Äì10203, 2018.
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line
learning algorithms. IEEE Transactions on Information Theory, 50(9):2050‚Äì2057, 2004.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems 31, pp.
3040‚Äì3050, 2018a.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018b.
Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the
American mathematical society, 39(1):1‚Äì49, 2002.
Aymeric Dieuleveut and Francis Bach. Nonparametric stochastic approximation with large step-sizes.
The Annals of Statistics, 44(4):1363‚Äì1399, 2016.
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger con-
vergence rates for least-squares regression. The Journal of Machine Learning Research, 18(1):
3520‚Äì3570, 2017.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent Ô¨Ånds global
minima of deep neural networks. In Proceedings of International Conference on Machine Learning
36, pp. 1675‚Äì1685, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. International Conference on Learning Representations 7,
2019b.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural network. In Advances in Neural Information Processing Systems 32,
pp. 9111‚Äì9121, 2019.
Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8580‚Äì8589, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width sufÔ¨Åces for gradient descent to achieve
arbitrarily small test error with shallow relu networks. arXiv preprint arXiv:1909.12292, 2019.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570‚Äì8581,
2019.
Jason D Lee, Ruoqi Shen, Zhao Song, and Mengdi Wang. Generalized leverage score sampling for
neural networks. In Advances in Neural Information Processing Systems, 2020.
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Proceedings of Conference on Learning Theory 33, pp. 2613‚Äì2682, 2020.
Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational Harmonic
Analysis, 48(3):868‚Äì890, 2020.
11

Published as a conference paper at ICLR 2021
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean Ô¨Åeld view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665‚ÄìE7671,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
The MIT Press, 2012.
Nicole M√ºcke, Gergely Neu, and Lorenzo Rosasco. Beating sgd saturation with tail-averaging and
minibatching. In Advances in Neural Information Processing Systems, pp. 12568‚Äì12577, 2019.
Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic gradient descent.
In Proceedings of Conference On Learning Theory 32, pp. 3222‚Äì3242, 2018.
Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for inÔ¨Ånite ensembles. arXiv
preprint arXiv:1712.05438, 2017.
Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates of
expected classiÔ¨Åcation errors. In Proceedings of International Conference on ArtiÔ¨Åcial Intelligence
and Statistics 22, pp. 1417‚Äì1426, 2019.
Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki.
Gradient descent can learn less
over-parameterized two-layer neural networks on classiÔ¨Åcation problems.
arXiv preprint
arXiv:1905.09870, 2019.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Exponential convergence of testing error
for stochastic gradient methods. In Proceedings of Conference on Learning Theory 31, pp. 1‚Äì47,
2018a.
Loucas Pillaud-Vivien, Alessandro Rudi, and Francis Bach. Statistical optimality of stochastic
gradient descent on hard learning problems through multiple passes. In Advances in Neural
Information Processing Systems, pp. 8114‚Äì8124, 2018b.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A Hamprecht,
Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In Proceedings of
International Conference on Machine Learning 36, pp. 5301‚Äì5310, 2019.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20, pp. 1177‚Äì1184, 2007.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Proceedings of International Conference on Machine
Learning 29, pp. 1571‚Äì1578, 2012.
Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941, 2017.
Basri Ronen, David Jacobs, Yoni Kasten, and Shira Kritchman. The convergence rate of neural
networks for learned functions of different frequencies. In Advances in Neural Information
Processing Systems, pp. 4763‚Äì4772, 2019.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.
In Advances in Neural Information Processing Systems, pp. 3215‚Äì3225, 2017.
Steve Smale and Yuan Yao. Online learning algorithms. Foundations of computational mathematics,
6(2):145‚Äì170, 2006.
Zhao Song and Xin Yang. Quadratic sufÔ¨Åces for over-parametrization via matrix chernoff bound.
arXiv preprint arXiv:1906.03593, 2019.
Ingo Steinwart, Don R Hush, and Clint Scovel. Optimal rates for regularized least squares regression.
In Proceedings of Conference on Learning Theory 22, pp. 79‚Äì93, 2009.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approx-
imation perspective. In Advances in Neural Information Processing Systems, pp. 2637‚Äì2646,
2019.
12

Published as a conference paper at ICLR 2021
Taiji Suzuki. Generalization bound of globally optimal non-convex neural network training: Trans-
portation map estimation by inÔ¨Ånite dimensional langevin dynamics. In Advances in Neural
Information Processing Systems, 2020.
E Weinan, Chao Ma, and Lei Wu. A comparative analysis of optimization and generalization
properties of two-layer neural network and random feature models under gradient descent dynamics.
Science China Mathematics, pp. 1‚Äì24, 2019.
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for an
over-parameterized neural network. arXiv preprint arXiv:1902.07111, 2019.
Yiming Ying and D-X Zhou. Online regularized classiÔ¨Åcation algorithms. IEEE Transactions on
Information Theory, 52(11):4775‚Äì4788, 2006.
Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. In Advances in Neural Information Processing Systems,
pp. 8082‚Äì8093, 2019.
Tong Zhang. Statistical behavior and consistency of classiÔ¨Åcation methods based on convex ris
minimization. The Annals of Statistics, 32(1):56‚Äì134, 2004.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2053‚Äì2062, 2019.
13

Published as a conference paper at ICLR 2021
Appendix
A
PROOF SKETCH OF THE MAIN RESULTS
We provide several key results and a proof sketch of Theorem 1 and Corollary 1. We Ô¨Årst recall the
deÔ¨Ånition of stochastic gradients of L in a general RKHS (H, ‚ü®, ‚ü©H) associated with a uniformly
bounded real-valued kernel function k : X √ó X ‚ÜíR. We set KX = k(X, ¬∑). Then, it follows that
for ‚àÄg, ‚àÄh ‚ààH,
L(g + h) = L(g) + ‚ü®E[‚àÇz‚Ñì(g(X), Y )KX], h‚ü©H + o(‚à•h‚à•H),
which is conÔ¨Årmed by the following equations:
E[l((g + h)(X), Y )] = E[l(g(X), Y ) + ‚àÇŒ∂l(g(X), Y )h(X) + o(|h(X)|)],
h(X) = ‚ü®h, k(X, ¬∑)‚ü©H, and |h(X)| ‚â§‚à•h‚à•H
p
k(X, X). This means that the stochastic gradient of
L in H is given by ‚àÇŒ∂‚Ñì(g(X), Y )k(X, ¬∑) for (X, Y ) ‚àºœÅ. In addition, the stochastic gradient of the
L2-regularized risk is given by ‚àÇŒ∂‚Ñì(g(X), Y )k(X, ¬∑) + Œªg.
A. 1
REFERENCE AVERAGED STOCHASTIC GRADIENT DESCENT
We consider a random feature approximation of NTK k‚àû: for an initial value B(0) = (b(0)
r )M
r=1,
‚àÄx, ‚àÄx‚Ä≤ ‚ààX,
kM(x, x‚Ä≤)
def
=
1
M
M
X
r=1
œÉ(b(0)‚ä§
r
x)œÉ(b(0)‚ä§
r
x‚Ä≤) + (x‚ä§x‚Ä≤ + Œ≥2)
M
M
X
r=1
œÉ‚Ä≤(b(0)‚ä§
r
x)œÉ‚Ä≤(b(0)‚ä§
r
x‚Ä≤),
(5)
We can conÔ¨Årm that kM is an approximation of NTK, that is, kM converges to k‚àûuniformly
over supp(œÅX) √ó supp(œÅX) almost surely by the uniform law of large numbers. We denote by
(HM, ‚ü®, ‚ü©HM ) an RKHS associated with kM. By the assumptions, we see kM(x, x‚Ä≤) ‚â§12 for
‚àÄ(x, x‚Ä≤) ‚ààsupp(œÅX) √ó supp(œÅX).
We introduce averaged stochastic gradient descent in HM (see Algorithm 2) as a reference for
Algorithm 1. The notation G(t) represents a stochastic gradient at the t-th iterate:
G(t) def
= ‚àÇz‚Ñì(g(t)(xt), yt)kM(xt, ¬∑).
Algorithm 2 Reference ASGD in HM
1: Input: number of iterations T, regularization parameter Œª, learning rates (Œ∑t)T ‚àí1
0=1 , averaging
weights (Œ±t)T
t=0,
2: g(0) ‚Üê0
3: for t = 0 to T ‚àí1 do
4:
Randomly draw a sample (xt, yt) ‚àºœÅ
5:
g(t+1) ‚Üê(1 ‚àíŒ∑tŒª)g(t) ‚àíŒ∑tG(t)
6: end for
7: Return g(T ) = PT
t=0 Œ±tg(t)
The following proposition shows the equivalence between the averaged stochastic gradient descent
for two-layer neural networks and that in HM up to a small constant depending on M.
Proposition A. Suppose Assumptions (A1) and (A2) hold. Run Algorithms 1 and 2 with the constant
learning rate Œ∑t = Œ∑ satisfying Œ∑Œª < 1 and Œ∑ ‚â§1. Moreover, assume that they share the same
hyper-parameter settings and the same examples (xt, yt)T ‚àí1
t=0 to compute stochastic gradient. Then,
for arbitrary T ‚ààZ+ and œµ > 0, there exists M ‚ààZ+ depending only on T and œµ such that ‚àÄt ‚â§T,
‚à•gŒò
(t) ‚àíg(t)‚à•L‚àû(œÅX) ‚â§œµ,
(6)
where gŒò
(t) and g(t) are iterates obtained by Algorithm 1 and 2, respectively.
14

Published as a conference paper at ICLR 2021
Remark.
Note that this proposition holds for non-averaged SGD too because it is a special case of
averaged SGD by setting only one Œ±t to 1.
Key idea.
This proposition is the key because it connects two learning dynamics for neural networks
and RKHS HM by utilizing overparameterization without the positivity of NTK unlike existing
studies (Weinan et al., 2019; Arora et al., 2019b) that provide such a connection for continuous
gradient Ô¨Çow with the positive NTK. Instead of the positivity of NTK, Proposition A says that
overparameterization increases the time stayed in the NTK regime where the learning dynamics for
neural networks can be characterized by the NTK. As a result, because M is free from the other
hyper-parameters, the averaged stochastic gradient descent for the overparameterized two-layer neural
networks can fully inherit preferable properties from learning dynamics in HM with an appropriate
choice of learning rates and regularization parameters as long as the network width is sufÔ¨Åciently
large depending only on the number of iterations and the required accuracy.
A. 2
CONVERGENCE RATE OF THE REFERENCE ASGD
We give the convergence analysis of Algorithm 2 in HM, which will be a part of a bound in Theorem
1. Proofs essentially rely on several techniques developed in serial studies (Bach & Moulines, 2013;
Dieuleveut & Bach, 2016; Dieuleveut et al., 2017; Pillaud-Vivien et al., 2018a; Rudi & Rosasco,
2017; Carratino et al., 2018) with several adaptations to our settings.
Let M ‚ààZ+ ‚à™{‚àû} be a positive number or ‚àû. We set KM,X
def
= kM(X, ¬∑) and denote by Œ£M the
covariance operator deÔ¨Åned by kM:
Œ£M
def
= EX‚àºœÅX[KM,X ‚äóHM KM,X].
We denote by gM,Œª the minimizer of the regularized risk over HM:
gM,Œª
def
= arg min
g‚ààHM

L(g) + Œª
2 ‚à•g‚à•2
HM

.
We remark that Œ£M : L2(œÅX) ‚ÜíHM is isometric (Cucker & Smale, 2002), that is, ‚àÄ(f, g) ‚àà
L2(œÅX) √ó L2(œÅX),
D
Œ£1/2
M f, Œ£1/2
M g
E
HM
= ‚ü®f, g‚ü©L2(œÅX) ,
and we use this fact frequently. It is known that gM,Œª is represented as follows (Caponnetto & De Vito,
2007):
gM,Œª = (Œ£M + ŒªI)‚àí1E(X,Y )[Y KM,X]
= (Œ£M + ŒªI)‚àí1Œ£MgœÅ.
(7)
The following theorem provides a convergence rate of Algorithm 2 to the minimizer gM,Œª.
Theorem A. Suppose Assumptions (A1), (A2) and (A3) hold. Run Algorithm 2 with the constant
learning rate Œ∑t = Œ∑ satisfying 4(6+Œª)Œ∑ ‚â§1. Then, for ‚àÄŒª > 0 and ‚àÄŒ¥ ‚àà(0, 1) there exists M0 > 0
such that for ‚àÄM ‚â•M0 the following holds with high probability at least 1 ‚àíŒ¥:
E
g(T ) ‚àígM,Œª

2
L2(œÅX)

‚â§
4
Œ∑2(T + 1)2 ‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•2
L2(œÅX)
+ 2 ¬∑ 242
T + 1 ‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX)
+
8
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£M(Œ£M + ŒªI)‚àí1
,
where g(T ) is an iterate obtained by Algorithm 2.
Remark.
The Ô¨Årst and second terms stem from the optimization speed of a semi-stochastic part
of averaged stochastic gradient descent. The Ô¨Årst term has a better dependency on T, but it has
a worse dependency on Œª than the second one. This kind of deterioration due to the weak strong
convexity is common in Ô¨Årst-order optimization methods. However, as conÔ¨Årmed later, these two
15

Published as a conference paper at ICLR 2021
terms are dominated by the variance term corresponding to the third term by setting hyper-parameters
appropriately.
To make the bound in Theorem A free from the size of M, we introduce the following proposition.
Proposition B. Suppose gœÅ ‚ààH‚àûholds. Under Assumption (A1) and (A2), for any Œ¥ ‚àà(0, 1),
there exists M0 ‚ààZ+ such that for any M ‚â•M0, the following holds with high probability at least
1 ‚àíŒ¥:
‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•2
L2(œÅX) ‚â§2Œª‚àí1‚à•gœÅ‚à•2
H‚àû,
‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX) ‚â§2‚à•gœÅ‚à•2
H‚àû,
and if Œª ‚â§‚à•Œ£‚àû‚à•op, then
Tr
 Œ£M(Œ£M + ŒªI)‚àí1
‚â§3Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
.
Remark.
The last inequality on the degree of freedom was shown in Rudi & Rosasco (2017).
To show the convergence to gœÅ, we utilize the following decomposition:
1
3‚à•g(T ) ‚àígœÅ‚à•2
L2(œÅX) ‚â§‚à•g(T ) ‚àígM,Œª‚à•2
L2(œÅX) + ‚à•gM,Œª ‚àíg‚àû,Œª‚à•2
L2(œÅX) + ‚à•g‚àû,Œª ‚àígœÅ‚à•2
L2(œÅX),
(8)
where g‚àû,Œª
def
= arg ming‚ààH‚àû{L(g) + Œª
2 ‚à•g‚à•2
H‚àû}.
The Ô¨Årst term is the optimization speed evaluated in Theorem A, and the second and third terms are
approximation errors from a random feature approximation of NTK and imposing L2-regularization,
respectively. These approximation terms can be evaluated by the following existing results. The next
proposition is a simpliÔ¨Åed version of Lemma 8 in Carratino et al. (2018)
Proposition C (Carratino et al. (2018)). Under Assumption (A1), (A2), and (A3), for any œµ, Œª > 0
and Œ¥ ‚àà(0, 1], there exists M0 ‚ààZ+ depending on œµ, Œª, Œ¥ such that for any M ‚â•M0, the following
holds with high probability at least 1 ‚àíŒ¥:
‚à•gM,Œª ‚àíg‚àû,Œª‚à•2
L2(œÅX) ‚â§œµ.
Proposition D (Caponnetto & De Vito (2007)). Under Assumption (A3), it follows that
‚à•g‚àû,Œª ‚àígœÅ‚à•2
L2(œÅX) ‚â§Œª2r‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX).
By combining Theorem A, Proposition B, C, and D with the decomposition (8), we can establish the
convergence rate of reference ASGD to reach gœÅ, which is simply the generalization error bound.
Theorem B. Assume the same conditions as in Theorem A. Then, for ‚àÄœµ > 0, ‚à•Œ£‚àû‚à•op ‚â•‚àÄŒª > 0,
and ‚àÄŒ¥ ‚àà(0, 1), there exists M0 ‚ààZ+ such that for ‚àÄM ‚â•M0, the following holds with high
probability at least 1 ‚àíŒ¥ over the random choice of random features Œò(0):
E

‚à•g(T ) ‚àígœÅ‚à•2
L2(œÅX)

‚â§œµ + 3Œª2r‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)
+
24
T + 1

288 +
1
ŒªŒ∑2(T + 1)

‚à•gœÅ‚à•2
H‚àû
+
24
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
,
where g(T ) is an iterate obtained by Algorithm 2.
A. 3
CONVERGENCE RATES OF ASGD FOR NEURAL NETWORKS
As explained earlier, the generalization bound for the reference ASGD is inherited by that for two-
layer neural networks through Proposition A with the following decomposition: for an iterate ŒòT
obtained by Algorithm 1,
‚à•gŒò
(T ) ‚àígœÅ‚à•L2(œÅX) ‚â§‚à•gŒò
(T ) ‚àíg(T )‚à•L2(œÅX) + ‚à•g(T ) ‚àígœÅ‚à•L2(œÅX).
16

Published as a conference paper at ICLR 2021
That is, these two terms are bounded by Proposition A and Theorem B under Assumption (A1)-(A3),
resulting in Theorem 1, which exhibits comparable generalization error to Theorem B as long as the
network width M is sufÔ¨Åciently large.
Theorem 1 immediately leads to the fast convergence rate in Corollary 1 by setting Œ∑t = Œ∑ = O(1)
satisfying 4(6 + Œª)Œ∑ ‚â§1 and Œª = T ‚àíŒ≤/(2rŒ≤+1) with the bounds on ‚à•gœÅ‚à•2
H‚àû, ‚à•gœÅ‚à•2
L2(œÅX), and the
degree of freedom. Because Œ≤ in Assumption (A4) controls the complexity of the hypothesis space
H‚àû, it derives a bound on the degree of freedom, as shown in Caponnetto & De Vito (2007):
Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
= O(Œª‚àí1/Œ≤).
In addition, the boundedness of ‚à•Œ£‚àû‚à•op ‚â§O(1) gives
‚à•gœÅ‚à•H‚àû= ‚à•Œ£
r‚àí1
2
‚àû
Œ£‚àír
‚àûgœÅ‚à•L2(œÅX) ‚â§O
 ‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX)

,
‚à•gœÅ‚à•L2(œÅX) ‚â§‚à•Œ£r
‚àûŒ£‚àír
‚àûgœÅ‚à•L2(œÅX) ‚â§O
 ‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX)

.
This Ô¨Ånishes the proof of Corollary 1.
B
PROOF OF PROPOSITION A
We Ô¨Årst show the Proposition A that says the equivalence between averaged stochastic gradient
descent for two-layer neural networks and that in an RKHS associated with kM.
Proof. Proof of Proposition A
Bound the growth of ‚à•gŒò(t)‚à•L‚àû(œÅX).
We Ô¨Årst show that there exist increasing functions d(t) and
M(t) depending only on t uniformly over the choice of the history of examples (xt, yt)‚àû
t=1 used in
Algorithms such that ‚à•gŒò(s)‚à•L‚àû(œÅX) ‚â§d(t) for ‚àÄs ‚â§t when M ‚â•M(t). We show this statement
by the induction.
Without loss of generality, we assume that there is no bias term, ‚à•b(0)
r ‚à•2 = 1, and supp(œÅX) ‚äÇ{x ‚àà
Rd+1 | ‚à•x‚à•2 ‚â§2} by setting x ‚Üê(x, Œ≥) (where Œ≥ ‚àà(0, 1)). Hence, we consider the update only for
parameters a and B. The above statement clearly holds for t = 0. Thus, we assume it holds for t. We
recall the speciÔ¨Åc update rules of the stochastic gradient descent:
a(t+1)
r
‚àía(0)
r
= (1 ‚àíŒ∑Œª)(a(t)
r
‚àía(0)
r ) ‚àí
Œ∑
‚àö
M
(gŒò(t)(xt) ‚àíyt)œÉ(b(t)‚ä§
r
xt),
(9)
b(t+1)
r
‚àíb(0)
r
= (1 ‚àíŒ∑Œª)(b(t)
r
‚àíb(0)
r ) ‚àí
Œ∑
‚àö
M
(gŒò(t)(xt) ‚àíyt)a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt)xt.
(10)
Here, let us consider ‚àÄM ‚â•M(t). Set dM
b (t) = maxs‚â§t,1‚â§r‚â§M ‚à•b(s)
r ‚à•2. Then, by expanding
equation (9), we get
|a(t+1)
r
‚àía(0)
r | ‚â§|a(t)
r
‚àía(0)
r | +
Œ∑
‚àö
M
(d(t) + 1)(1 + 2dM
b (t))
‚â§
Œ∑
‚àö
M
t
X
s=0
(d(s) + 1)(1 + 2dM
b (t))
‚â§Œ∑(t + 1)
‚àö
M
(d(t) + 1)(1 + 2dM
b (t)),
(11)
where we used ‚à•œÉ(u)‚à•‚â§1 + u and |yt| ‚â§1. As for the term |a(s)
r | (s ‚â§t + 1), from the similar
augment for s and the monotonicity, we have for s ‚â§t + 1,
|a(s)
r | ‚â§1 + |a(s)
r
‚àía(0)
r |
‚â§1 + Œ∑(t + 1)
‚àö
M
(d(t) + 1)(1 + 2dM
b (t))
‚â§1 + Œ∑(t + 1)(d(t) + 1)(1 + 2dM
b (t)).
17

Published as a conference paper at ICLR 2021
We next give a bound on ‚à•b(t+1)
r
‚àíb(0)
r ‚à•2. By expanding equation (10), we get
‚à•b(t+1)
r
‚àíb(0)
r ‚à•2 ‚â§‚à•b(t)
r
‚àíb(0)
r ‚à•2 + 4Œ∑
‚àö
M
|a(t)
r |(d(t) + 1)
‚â§
4Œ∑
‚àö
M
t
X
s=0
|a(s)
r |(d(s) + 1)
‚â§4Œ∑(t + 1)
‚àö
M
(d(t) + 1)

1 + Œ∑(t + 1)
‚àö
M
(d(t) + 1)(1 + 2dM
b (t))

,
(12)
where we used ‚à•œÉ‚Ä≤‚à•‚àû‚â§2 and ‚à•xt‚à•2 ‚â§2. Here, we evaluate dM
b (t). From the similar augment for
s ‚â§t, the monotonicity, and ‚à•b(s)
r ‚à•2 ‚â§1 + ‚à•b(s)
r
‚àíb(0)
r ‚à•2, we get
dM
b (t) ‚â§1 + 4Œ∑(t + 1)
‚àö
M
(d(t) + 1)

1 + Œ∑(t + 1)
‚àö
M
(d(t) + 1)(1 + 2dM
b (t))

.
Let M ‚Ä≤(t + 1) be a positive integer depending on t and d(t) such that t+1
‚àö
M (d(t) + 1) ‚â§1
4. Let us
reconsider ‚àÄM ‚â•M ‚Ä≤(t + 1). Then, since Œ∑ ‚â§1, we have
dM
b (t) ‚â§5
2, |a(s)
r | ‚â§5
2
(‚àÄs ‚â§t + 1).
From the derivation of (11) and (12) and since Œ∑ ‚â§1, we have for 0 ‚â§‚àÄs ‚â§t + 1,
|a(s)
r
‚àía(0)
r | ‚â§d1(t + 1)
‚àö
M
,
‚à•b(s)
r
‚àíb(0)
r ‚à•2 ‚â§d2(t + 1)
‚àö
M
,
(13)
where d1(t + 1) and d2(t + 1) are set to
d1(t + 1)
def
= 6(t + 1)(d(t) + 1),
d2(t + 1)
def
= 10(t + 1)(d(t) + 1).
We next bound |gŒò(t+1)(x)| for x ‚àà‚àÄsupp(œÅX) as follows. Since gŒò(0) ‚â°0,
|gŒò(t+1)(x)| = |gŒò(t+1)(x) ‚àígŒò(0)(x)|
‚â§
1
‚àö
M
M
X
r=1
n(a(t+1)
r
‚àía(0)
r )œÉ(b(0)‚ä§
r
x)
 +
a(t+1)
r
(œÉ(b(t+1)‚ä§
r
x)) ‚àíœÉ(b(0)‚ä§
r
x)

o
‚â§
1
‚àö
M
M
X
r=1
n
2
a(t+1)
r
‚àía(0)
r
 + 4
a(t+1)
r

b(t+1)
r
‚àíb(0)
r

o
‚â§2d1(t + 1) + 10d2(t + 1).
In summary, by setting M(t + 1) = max{M(t), 16(t + 1)2(d(t) + 1)2} and d(t + 1) = 2d1(t +
1) + 10d2(t + 1), we get ‚à•gŒò(t+1)‚à•L‚àû(œÅX) ‚â§d(t + 1) when M ‚â•M(t + 1). We note that from
the above construction, d(t), d1(t), d2(t) depend only on t and inequalities (13) are always hold for
‚àÄt ‚ààZ+ when M ‚â•M(t + 1).
Linear approximation of the model.
For a given T ‚ààZ+, we consider ‚àÄM ‚â•M(T) and deÔ¨Åne
the neighborhood of Œò(0) = (a(0)
r , b(0)
r )M
r=1:
BT (Œò(0))
def
=

(ar, br)M
r=1 ‚àà(R √ó Rd+1)M | |ar| ‚â§5
2, |ar ‚àía(0)
r | ‚â§d1(T)
‚àö
M
, ‚à•br ‚àíb(0)
r ‚à•2 ‚â§d2(T)
‚àö
M

.
From Taylor‚Äôs formula |œÉ(b‚ä§
r x) ‚àíœÉ(b(0)‚ä§
r
x) ‚àíœÉ‚Ä≤(b(0)‚ä§
r
x)(br ‚àíb(0)
r )‚ä§x| ‚â§2‚à•œÉ‚Ä≤‚Ä≤‚à•‚àû‚à•br ‚àíb(0)
r ‚à•2
2
and the smoothness of œÉ, we get for Œò ‚ààBT (Œò(0)) and x ‚ààsupp(œÅX),
arœÉ(b‚ä§
r x) ‚àí(a(0)
r œÉ(b(0)‚ä§
r
x) + (ar ‚àía(0)
r )œÉ(b(0)‚ä§
r
x) + a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
x)(br ‚àíb(0)
r )‚ä§x)

‚â§4|ar ‚àía(0)
r |‚à•br ‚àíb(0)
r ‚à•2 + 2C|ar|‚à•br ‚àíb(0)
r ‚à•2
2
‚â§2d1(T)d2(T)
M
+ 5Cd2
2(T)
M
.
(14)
18

Published as a conference paper at ICLR 2021
We here deÔ¨Åne a linear model:
hŒò(x)
def
=
1
‚àö
M
M
X
r=1

(ar ‚àía(0)
r )œÉ(b(0)‚ä§
r
x) + a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
x)(br ‚àíb(0)
r )‚ä§x

.
By taking the sum of (14) over r ‚àà{1, . . . , M} and by gŒò(0) ‚â°0,
|gŒò(x) ‚àíhŒò(x)| ‚â§
1
‚àö
M
M
X
r=1
2d1(T)d2(T)
M
+ 5Cd2
2(T)
M

‚â§
1
‚àö
M
 2d1(T)d2(T) + 5Cd2
2(T)

.
We denote d3(T)
def
= 2d1(T)d2(T) + 5Cd2
2(T). Since iterates (Œò(t))T
t=0 obtained by Algorithm 1
are contained in BT (Œò(0)), weighted averages (Œò
(t))T
t=0 are also contained in BT (Œò(0)). Thus, we
get for ‚àÄt ‚àà{1, . . . , T},
|gŒò(t)(x) ‚àíhŒò(t)(x)| ‚â§d3(T)
‚àö
M
,
gŒò
(t)(x) ‚àíhŒò
(t)(x)
 ‚â§d3(T)
‚àö
M
.
(15)
Recursion of hŒò(t) using the random feature approximation of NTK.
We here derive a recur-
sion of hŒò(t) using kM. From the updates (9) and (10), we have
hŒò(t+1)(x) =
1
‚àö
M
M
X
r=1

(a(t+1)
r
‚àía(0)
r )œÉ(b(0)‚ä§
r
x) + a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
x)(b(t+1)
r
‚àíb(0)
r )‚ä§x

= (1 ‚àíŒ∑Œª)hŒò(t)(x) ‚àíŒ∑
M
M
X
r=1
(gŒò(t)(xt) ‚àíyt)œÉ(b(t)‚ä§
r
xt)œÉ(b(0)‚ä§
r
x)
‚àíŒ∑
M
M
X
r=1
a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
x)(gŒò(t)(xt) ‚àíyt)a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt)x‚ä§
t x.
(16)
Note that for t ‚àà{0, . . . , T},
|(gŒò(t)(xt) ‚àíyt)œÉ(b(t)‚ä§
r
xt) ‚àí(hŒò(t)(xt) ‚àíyt)œÉ(b(0)‚ä§
r
xt)|
‚â§|(gŒò(t)(xt) ‚àíhŒò(t)(xt))œÉ(b(0)‚ä§
r
xt)| + |(gŒò(t)(xt) ‚àíyt)(œÉ(b(t)‚ä§
r
xt) ‚àíœÉ(b(0)‚ä§
r
xt))|
‚â§2d3(T)
‚àö
M
+ 4(d(T) + 1)‚à•b(t)
r
‚àíb(0)
r ‚à•2
‚â§
2
‚àö
M
(d3(T) + 2(d(T) + 1)d2(T)) ,
and
|(gŒò(t)(xt) ‚àíyt)a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt) ‚àí(hŒò(t)(xt) ‚àíyt)a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
xt)|
‚â§|(gŒò(t)(xt) ‚àíhŒò(t)(xt))a(0)
r œÉ‚Ä≤(b(0)‚ä§
r
xt)| + |(gŒò(t)(xt) ‚àíyt)(a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt) ‚àía(0)
r œÉ‚Ä≤(b(0)‚ä§
r
xt))|
‚â§2d3(T)
‚àö
M
+ (d(T) + 1)|a(t)
r œÉ‚Ä≤(b(t)‚ä§
r
xt) ‚àía(0)
r œÉ‚Ä≤(b(0)‚ä§
r
xt))|
‚â§2d3(T)
‚àö
M
+ (d(T) + 1)
n
|a(0)
r (œÉ‚Ä≤(b(t)‚ä§
r
xt) ‚àíœÉ‚Ä≤(b(0)‚ä§
r
xt))| + |(a(t)
r
‚àía(0)
r )œÉ‚Ä≤(b(t)‚ä§
r
xt)|
o
‚â§2d3(T)
‚àö
M
+ 2(d(T) + 1)
n
C‚à•b(t)
r
‚àíb(0)
r ‚à•2 + |a(t)
r
‚àía(0)
r |
o
‚â§2d3(T)
‚àö
M
+ 2(d(T) + 1)
‚àö
M
(d1(T) + Cd2(T))
=
2
‚àö
M
(d3(T) + (d(T) + 1)(d1(T) + Cd2(T))) .
19

Published as a conference paper at ICLR 2021
Plugging these two inequalities into (16), we have ‚àÄt ‚àà{1, . . . , T ‚àí1},
hŒò(t+1)(x) ‚â§(1 ‚àíŒ∑Œª)hŒò(t)(x) ‚àíŒ∑
M
M
X
r=1
(hŒò(t)(xt) ‚àíyt)œÉ(b(0)‚ä§
r
xt)œÉ(b(0)‚ä§
r
x)
‚àíŒ∑
M
M
X
r=1
œÉ‚Ä≤(b(0)‚ä§
r
x)(hŒò(t)(xt) ‚àíyt)œÉ‚Ä≤(b(0)‚ä§
r
xt)x‚ä§
t x
+ 2Œ∑
‚àö
M
(2d3(T) + (d(T) + 1) (d1(T) + (C + 2)d2(T)))
= (1 ‚àíŒ∑Œª)hŒò(t)(x) ‚àíŒ∑(hŒò(t)(xt) ‚àíyt) 1
M
M
X
r=1

œÉ(b(0)‚ä§
r
xt)œÉ(b(0)‚ä§
r
x) + œÉ‚Ä≤(b(0)‚ä§
r
x)œÉ‚Ä≤(b(0)‚ä§
r
xt)x‚ä§
t x

+ 2Œ∑
‚àö
M
(2d3(T) + (d(T) + 1) (d1(T) + (C + 2)d2(T)))
= (1 ‚àíŒ∑Œª)hŒò(t)(x) ‚àíŒ∑(hŒò(t)(xt) ‚àíyt)kM(x, xt) +
Œ∑
‚àö
M
d4(T),
where d4(T) = 2d3(T) + (d(T) + 1) (d1(T) + (C + 2)d2(T)). Clearly, the inverse inequality also
holds:
hŒò(t+1)(x) ‚â•(1 ‚àíŒ∑Œª)hŒò(t)(x) ‚àíŒ∑(hŒò(t)(xt) ‚àíyt)kM(x, xt) ‚àí
Œ∑
‚àö
M
d4(T).
Thus, we get
|hŒò(t+1)(x) ‚àí(1 ‚àíŒ∑Œª)hŒò(t)(x) + Œ∑(hŒò(t)(xt) ‚àíyt)kM(x, xt)| ‚â§
Œ∑
‚àö
M
d4(T).
(17)
Equivalence between Algorithm 1 and 2.
We provide a bound between recursions of Algorithm
2 and (17). Noting that hŒò(0) ‚â°g(0) ‚â°0, we have for ‚àÄt ‚àà{0, . . . , T ‚àí1},
|hŒò(t+1)(x) ‚àíg(t+1)(x)| ‚â§(1 ‚àíŒ∑Œª)|hŒò(t)(x) ‚àíg(t)(x)| + Œ∑|hŒò(t)(xt) ‚àíg(t)(xt)|kM(x, xt) +
Œ∑
‚àö
M
d4(T).
Noting ‚à•kM‚à•L‚àû(œÅX) ‚â§12 and taking a supremum over x, xt ‚ààsupp(œÅX) in both sides, we have
‚à•hŒò(t+1) ‚àíg(t+1)‚à•L‚àû(œÅX) ‚â§(1 ‚àíŒ∑Œª)‚à•hŒò(t) ‚àíg(t)‚à•L‚àû(œÅX) + Œ∑‚à•hŒò(t) ‚àíg(t)‚à•L‚àû(œÅX)‚à•kM‚à•L‚àû(œÅX) +
Œ∑
‚àö
M
d4(T)
‚â§(1 ‚àíŒ∑Œª + 12Œ∑)‚à•hŒò(t) ‚àíg(t)‚à•L‚àû(œÅX) +
Œ∑
‚àö
M
d4(T)
‚â§
t
X
s=0
(1 + 12Œ∑)t‚àís
Œ∑
‚àö
M
d4(T)
‚â§
T
‚àö
M
(1 + 12Œ∑)T d4(T).
Since hŒò is a linear model, we have hŒò
(T ) = PT
t=0 Œ±thŒò(t) and
‚à•hŒò
(T ) ‚àíg(T )‚à•L‚àû(œÅX) ‚â§
T
X
t=0
Œ±t‚à•hŒò(t) ‚àíg(t)‚à•L‚àû(œÅX) ‚â§
T
‚àö
M
(1 + 12Œ∑)T d4(T).
Combining this inequality with (15), we Ô¨Ånally have
‚à•gŒò
(T ) ‚àíg(T )‚à•L‚àû(œÅX) ‚â§‚à•gŒò
(T ) ‚àíhŒò
(T )‚à•L‚àû(œÅX) + ‚à•hŒò
(T ) ‚àíg(T )‚à•L‚àû(œÅX)
‚â§
1
‚àö
M
(d3(T) + 13T Td4(T)).
Because (d3(T) + 13T Td4(T)) depends only on T and C from the construction, ‚à•gŒò(T ) ‚àí
g(T )‚à•L‚àû(œÅX) ‚Üí0 as M ‚Üí‚àû. This Ô¨Ånishes the proof of Proposition A.
20

Published as a conference paper at ICLR 2021
C
PROOF OF THEOREM A
In this section, we give the proof of the convergence theory for the reference ASGD (Algorithm 2).
We introduce an auxiliary result for proving Theorem A.
Lemma A. Suppose Assumption (A1), (A2), and (A3) hold. Set Œæ
def
= Y KM,X ‚àí(KM,X ‚äóHM
KM,X + ŒªI)gM,Œª. Then, for ‚àÄŒª > 0 and ‚àÄŒ¥ ‚àà(0, 1) there exists M0 > 0 such that for ‚àÄM ‚â•M0
the following holds with high probability at least 1 ‚àíŒ¥:
E(X,Y )‚àºœÅ[Œæ ‚äóHM Œæ] ‚âº2(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))Œ£M.
Proof. Since Œæ = (Y ‚àígM,Œª(X))KM,X ‚àíŒªgM,Œª, we get
E[Œæ ‚äóHM Œæ] = E[(Y ‚àígM,Œª(X))2KM,X ‚äóHM KM,X]
‚àíŒªE[(Y ‚àígM,Œª(X))KM,X] ‚äóHM gM,Œª
‚àíŒªgM,Œª ‚äóHM E[(Y ‚àígM,Œª(X))KM,X]
+ Œª2gM,Œª ‚äóHM gM,Œª.
We evaluate an expectation in the second and third terms in the right hand side of the above equation
as follows:
E[(Y ‚àígM,Œª(X))KM,X] = E[Y KM,X ‚àí(KM,X ‚äóHM KM,X)gM,Œª]
= E[Y KM,X] ‚àíŒ£MgM,Œª
= E[Y KM,X] ‚àíŒ£M(Œ£M + ŒªI)‚àí1E[Y KM,X]
= E[Y KM,X] ‚àí(Œ£M + ŒªI ‚àíŒªI)(Œ£M + ŒªI)‚àí1E[Y KM,X]
= Œª(Œ£M + ŒªI)‚àí1E[Y KM,X]
= ŒªgM,Œª.
Hence, we get
E[Œæ ‚äóHM Œæ] ‚âºE[(Y ‚àígM,Œª(X))2KM,X ‚äóHM KM,X].
For h ‚ààHM,
E[(Y ‚àígM,Œª(X))2KM,X ‚äóHM KM,X]h, h

HM
= E[(Y ‚àígM,Œª(X))2 ‚ü®(KM,X ‚äóHM KM,X)h, h‚ü©HM ]
‚â§‚à•Y ‚àígM,Œª(X)‚à•2
L‚àû(œÅX)E[‚ü®(KM,X ‚äóHM KM,X)h, h‚ü©HM ]
‚â§2(1 + ‚à•gM,Œª‚à•2
L‚àû(œÅX))E[‚ü®(KM,X ‚äóHM KM,X)h, h‚ü©HM ],
(18)
where we used Assumption (A2) for the last inequality.
Finally, we provide an upper-bound on ‚à•gM,Œª‚à•L‚àû(œÅX). Since S‚àí1 ‚àíT ‚àí1 = ‚àíS‚àí1(S ‚àíT)T ‚àí1 for
arbitrary operators S and T, we get
‚à•
 (Œ£‚àû+ ŒªI)‚àí1 ‚àí(Œ£M + ŒªI)‚àí1
gœÅ‚à•L2(œÅX)
= ‚à•(Œ£‚àû+ ŒªI)‚àí1(Œ£‚àû‚àíŒ£M)(Œ£M + ŒªI)‚àí1gœÅ‚à•L2(œÅX)
= ‚à•(Œ£‚àû+ ŒªI)‚àí1‚à•op‚à•Œ£‚àû‚àíŒ£M‚à•op‚à•(Œ£M + ŒªI)‚àí1‚à•op‚à•gœÅ‚à•L2(œÅX)
‚â§1
Œª2 ‚à•Œ£‚àû‚àíŒ£M‚à•op‚à•gœÅ‚à•L2(œÅX).
(19)
We denote F‚àû= (Œ£‚àû+ ŒªI)‚àí1gœÅ and FM = (Œ£M + ŒªI)‚àí1gœÅ. Noting g‚àû,Œª = Œ£‚àûF‚àûand
gM,Œª = Œ£MFM, we get for ‚àÄx ‚ààsupp(œÅX),
|g‚àû,Œª(x) ‚àígM,Œª(x)| = |Œ£‚àûF‚àû(x) ‚àíŒ£MFM(x)|
=

Z
X
K‚àû,x(X)F‚àû(X)dœÅX ‚àí
Z
X
KM,x(X)FM(X)dœÅX

=

Z
X
(K‚àû,x ‚àíKM,x)(X)F‚àû(X)dœÅX ‚àí
Z
X
KM,x(X)(FM(X) ‚àíF‚àû(X))dœÅX

‚â§‚à•K‚àû,x ‚àíKM,x‚à•L2(œÅX)‚à•F‚àû‚à•L2(œÅX) + ‚à•KM,x‚à•L2(œÅX)‚à•FM ‚àíF‚àû‚à•L2(œÅX)
‚â§1
Œª‚à•k‚àû‚àíkM‚à•L‚àû(œÅX)2‚à•gœÅ‚à•L2(œÅX) + 12
Œª2 ‚à•Œ£‚àû‚àíŒ£M‚à•op‚à•gœÅ‚à•L2(œÅX),
21

Published as a conference paper at ICLR 2021
where we used kM(x, x‚Ä≤) ‚â§12 for ‚àÄ(x, x‚Ä≤) ‚ààsupp(œÅX) √ó supp(œÅX) and inequality (19).
Moreover, we get
|g‚àû,Œª(x)| = | ‚ü®g‚àû,Œª, Kx‚ü©H‚àû|
‚â§‚à•Kx‚à•H‚àû‚à•g‚àû,Œª‚à•H‚àû
‚â§2
‚àö
3‚à•Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1gœÅ‚à•H‚àû
‚â§2
‚àö
3‚à•Œ£1+r
‚àû(Œ£‚àû+ ŒªI)‚àí1Œ£‚àír
‚àûgœÅ‚à•H‚àû
‚â§2
‚àö
3‚à•Œ£
1
2 +r
‚àû
(Œ£‚àû+ ŒªI)‚àí1Œ£‚àír
‚àûgœÅ‚à•L2(œÅX)
‚â§2
‚àö
3‚à•Œ£
1
2 +r
‚àû
(Œ£‚àû+ ŒªI)‚àí1‚à•op‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX)
‚â§2
‚àö
3‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX).
(20)
where we used Assumption (A3) and the isometric map Œ£1/2
‚àû: L2(œÅX) ‚ÜíH‚àû.
Hence, we get
‚à•gM,Œª‚à•L‚àû(œÅX) ‚â§
 1
Œª‚à•k‚àû‚àíkM‚à•L‚àû(œÅX)2 + 12
Œª2 ‚à•Œ£‚àû‚àíŒ£M‚à•op

‚à•gœÅ‚à•L2(œÅX) + 2
‚àö
3‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX).
By the uniform law of large numbers (Theorem 3.1 in Mohri et al. (2012)) and the Bernstein‚Äôs
inequality (Proposition 3 in Rudi & Rosasco (2017)) to random operators, ‚à•k‚àû‚àíkM‚à•L‚àû(œÅX√óœÅX)
and ‚à•Œ£‚àû‚àíŒ£M‚à•op converge to zero as M ‚Üí‚àûin probability. That is, for given Œª > 0 and
Œ¥ ‚àà(0, 1), there exists M0 such that for any M ‚â•M0 the following holds with high probability at
least 1 ‚àíŒ¥:
‚à•gM,Œª‚à•L‚àû(œÅX) ‚â§1
2‚à•gœÅ‚à•L2(œÅX) + 2
‚àö
3‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX).
Combining with (18), we get

E[(Y ‚àígM,Œª(X))2KM,X ‚äóHM KM,X]h, h

HM
‚â§2(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))E[‚ü®(KM,X ‚äóHM KM,X)h, h‚ü©HM ].
Proof of Theorem A. Since, stochastic gradient in HM is described as
G(t) = ‚àÇz‚Ñì(g(t)(xt), yt)kM(xt, ¬∑) =
D
KM,xt, g(t)E
HM
‚àíyt

KM,xt,
the update rule of Algorithm 2 is
g(t+1) = (1 ‚àíŒ∑Œª)g(t) ‚àíŒ∑
D
KM,xt, g(t)E
HM
‚àíyt

KM,xt
= (I ‚àíŒ∑KM,xt ‚äóHM KM,xt ‚àíŒ∑ŒªI) g(t) + Œ∑ytKM,xt.
Hence, we get
g(t+1) ‚àígM,Œª = (I ‚àíŒ∑KM,xt ‚äóHM KM,xt ‚àíŒ∑ŒªI)
|
{z
}
=Œ±t
(g(t) ‚àígM,Œª)
|
{z
}
=At
‚àíŒ∑(KM,xt ‚äóHM KM,xt + ŒªI)gM,Œª + Œ∑ytKM,xt
|
{z
}
=Œ≤t
.
(21)
This leads to the following stochastic recursion: t ‚àà{0, . . . , T ‚àí1},
At+1 = Œ±tAt + Œ≤t =
tY
s=0
Œ±sA0 +
t
X
s=0
tY
l=s+1
Œ±sŒ≤s.
22

Published as a conference paper at ICLR 2021
By taking the average, we get
AT =
1
T + 1
T
X
t=0
At
=
1
T + 1
T
X
t=0
tY
s=0
Œ±sA0
|
{z
}
Bias term
+
1
T + 1
T
X
t=0
t
X
s=0
tY
l=s+1
Œ±sŒ≤s
|
{z
}
Noise term
.
(22)
Thus, the average AT is composed of bias and noise terms. We next bound these two terms, separately.
Bound the bias term.
Note that the bias term exactly corresponds to the recursion (21) with
Œ≤t = 0. Hence, we consider the case of Œ≤t = 0 and consider the following stochastic recursion in
HM: A0 = ‚àígM,Œª,
At+1 = (I ‚àíŒ∑Ht ‚àíŒ∑ŒªI)At,
where we deÔ¨Åne Ht = KM,xt ‚äóHM KM,xt. In addition, we consider the deterministic recursion of
this recursion: A‚Ä≤
0 = A0,
A‚Ä≤
t+1 = (I ‚àíŒ∑Œ£M ‚àíŒ∑ŒªI)A‚Ä≤
t.
We set
AT =
1
T + 1
T
X
t=0
At,
A‚Ä≤T =
1
T + 1
T
X
t=0
A‚Ä≤
t.
Then, the bias term we want to evaluate is decomposed as follows: by Minkowski‚Äôs inequality,

E[‚à•AT ‚à•2
L2(œÅX)]
 1
2 ‚â§‚à•A‚Ä≤T ‚à•L2(œÅX) +

E[‚à•AT ‚àíA‚Ä≤T ‚à•2
L2(œÅX)]
 1
2 .
(23)
We here bound the Ô¨Årst term in the right hand side of (23). Note that from 4(6 + Œª)Œ∑ ‚â§1 and
‚à•kM‚à•L‚àû(œÅX)2 ‚â§12, we see 1 Œ∑(Œ£M +ŒªI) ‚âºŒ∑(12+Œª)I ‚â∫1
2I. Since A‚Ä≤
t = (I‚àíŒ∑Œ£M ‚àíŒ∑ŒªI)tgM,Œª,
its average is
A‚Ä≤T =
1
T + 1
T
X
t=0
A‚Ä≤
t =
1
Œ∑(T + 1)(Œ£M + ŒªI)‚àí1(I ‚àí(I ‚àíŒ∑Œ£M ‚àíŒ∑Œª)T +1)gM,Œª.
Therefore,
‚à•A‚Ä≤T ‚à•L2(œÅX) =
1
Œ∑(T + 1)‚à•(Œ£M + ŒªI)‚àí1(I ‚àí(I ‚àíŒ∑Œ£M ‚àíŒ∑Œª)T +1)gM,Œª‚à•L2(œÅX)
‚â§
1
Œ∑(T + 1)‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•L2(œÅX).
(24)
We bound the second term in (23), which measures the gap between AT and A‚Ä≤T . To do so, we
consider the following recursion:
At+1 ‚àíA‚Ä≤
t+1 = At ‚àíA‚Ä≤
t ‚àíŒ∑(Ht + ŒªI)(At ‚àíA‚Ä≤
t) + Œ∑(Œ£M ‚àíHt)A‚Ä≤
t.
Hence, we have
‚à•At+1 ‚àíA‚Ä≤
t+1‚à•2
HM = ‚à•At ‚àíA‚Ä≤
t‚à•2
HM
‚àíŒ∑ ‚ü®At ‚àíA‚Ä≤
t, (Ht + ŒªI)(At ‚àíA‚Ä≤
t) ‚àí(Œ£M ‚àíHt)A‚Ä≤
t‚ü©HM
‚àíŒ∑ ‚ü®(Ht + ŒªI)(At ‚àíA‚Ä≤
t) ‚àí(Œ£M ‚àíHt)A‚Ä≤
t, At ‚àíA‚Ä≤
t‚ü©HM
+ Œ∑2‚à•(Ht + ŒªI)(At ‚àíA‚Ä≤
t) ‚àí(Œ£M ‚àíHt)A‚Ä≤
t‚à•2
HM .
1In general, for any operator F : L2(œÅX) ‚ÜíL2(œÅX) that commutes with Œ£M and has a common eigen-
bases with Œ£M, it follows that F(HM) ‚äÇHM and inequality F ‚âΩ0 in L2(œÅX) is equivalent with F |HM ‚âΩ0.
Hence, we do not specify a Hilbert space we consider in such a case for the simplicity.
23

Published as a conference paper at ICLR 2021
Let (Ft)T ‚àí1
t=0 be a Ô¨Åltration. We take a conditional expectation given Ft:
E[‚à•At+1 ‚àíA‚Ä≤
t+1‚à•2
HM | Ft] ‚â§‚à•At ‚àíA‚Ä≤
t‚à•2
HM ‚àí2Œ∑ ‚ü®(Œ£M + ŒªI)(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM
+ 2Œ∑2E[‚à•(Ht + ŒªI)(At ‚àíA‚Ä≤
t)‚à•2
HM | Ft]
(25)
+ 2Œ∑2E[‚à•(Œ£M ‚àíHt)A‚Ä≤
t‚à•2
HM | Ft],
(26)
where we used ‚à•g + h‚à•2
HM ‚â§2(‚à•g‚à•2
HM + ‚à•h‚à•2
HM ).
For g ‚ààHM, we have

E[(KM,xt ‚äóHM KM,xt)2]g, g

HM = E
h
(KM,xt ‚äóHM KM,xt)2g, g

HM
i
= E
D
‚ü®KM,xt, g‚ü©HM (KM,xt ‚äóHM KM,xt)KM,xt, g
E
HM

= E
D
‚ü®KM,xt, g‚ü©HM kM(xt, xt)KM,xt, g
E
HM

= E
h
‚ü®KM,xt, g‚ü©2
HM kM(xt, xt)
i
‚â§12E
h
‚ü®KM,xt, g‚ü©2
HM
i
= 12 ‚ü®E [KM,xt ‚äóHM KM,xt] g, g‚ü©HM .
(27)
where we used kM(xt, xt) ‚â§12 which is conÔ¨Årmed from the deÔ¨Ånition of kM and Assumption (A2).
This means that E[H2
t ] = E[(KM,xt ‚äóHM KM,xt)2] ‚âº12Œ£M on HM √ó HM. Hence, we get a
bound on (25) as follows:
E[‚à•(Ht + ŒªI)(At ‚àíA‚Ä≤
t)‚à•2
HM | Ft] = E[

(Ht + ŒªI)2(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t

HM | Ft]
=

 Œª2I + 2ŒªŒ£M + E[(KM,xt ‚äóHM KM,xt)2]

(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t

HM
‚â§

 Œª2I + 2(6 + Œª)Œ£M

(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t

HM .
Next, we bound a term (26):
E[‚à•(Œ£M ‚àíHt)A‚Ä≤
t‚à•2
HM | Ft] = E[

(Œ£M ‚àíHt)2A‚Ä≤
t, A‚Ä≤
t

HM | Ft]
= E[

(Œ£2
M ‚àíŒ£MHt ‚àíHtŒ£M + H2
t )A‚Ä≤
t, A‚Ä≤
t

HM | Ft]
=

(E[H2
t ] ‚àíŒ£2
M)A‚Ä≤
t, A‚Ä≤
t

HM
‚â§

E[H2
t ]A‚Ä≤
t, A‚Ä≤
t

HM
‚â§12 ‚ü®Œ£MA‚Ä≤
t, A‚Ä≤
t‚ü©HM .
Combining these inequalities, we get
E[‚à•At+1 ‚àíA‚Ä≤
t+1‚à•2
HM | Ft] ‚â§‚à•At ‚àíA‚Ä≤
t‚à•2
HM ‚àí2Œ∑ ‚ü®(Œ£M + ŒªI)(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM
+ 2Œ∑2 
 Œª2I + 2(6 + Œª)Œ£M

(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t

HM
+ 24Œ∑2 ‚ü®Œ£MA‚Ä≤
t, A‚Ä≤
t‚ü©HM
= (1 ‚àí2ŒªŒ∑ + 2Œª2Œ∑2)‚à•At ‚àíA‚Ä≤
t‚à•2
HM ‚àí2Œ∑ ‚ü®Œ£M(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM
+ 4Œ∑2(6 + Œª) ‚ü®Œ£M(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM
+ 24Œ∑2 ‚ü®Œ£MA‚Ä≤
t, A‚Ä≤
t‚ü©HM
‚â§‚à•At ‚àíA‚Ä≤
t‚à•2
HM ‚àíŒ∑ ‚ü®Œ£M(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM
+ 24Œ∑2 ‚ü®Œ£MA‚Ä≤
t, A‚Ä≤
t‚ü©HM ,
(28)
where for the last inequality we used 4Œ∑(6 + Œª) ‚â§1.
By taking the expectation and the average of (28) over t ‚àà{0, . . . , T ‚àí1}, we get
1
T + 1
T
X
t=0
‚ü®Œ£M(At ‚àíA‚Ä≤
t), At ‚àíA‚Ä≤
t‚ü©HM ‚â§
24Œ∑
T + 1
T
X
t=0
‚ü®Œ£MA‚Ä≤
t, A‚Ä≤
t‚ü©HM .
24

Published as a conference paper at ICLR 2021
Since Œ£1/2
M : L2(œÅX) ‚ÜíHM is isometric, we see ‚à•Œ£1/2
M (At ‚àíA‚Ä≤
t)‚à•HM = ‚à•At ‚àíA‚Ä≤
t‚à•L2(œÅX). Thus,
the second term in (23) can be bounded as follows:
E[‚à•AT ‚àíA‚Ä≤T ‚à•2
L2(œÅX)] = E[‚à•Œ£1/2
M (AT ‚àíA‚Ä≤T )‚à•2
HM ]
‚â§
1
T + 1
T
X
t=0
E[‚à•Œ£1/2
M (At ‚àíA‚Ä≤
t)‚à•2
HM ]
‚â§
24Œ∑
T + 1
T
X
t=0
‚à•Œ£1/2
M A‚Ä≤
t‚à•2
HM
=
24Œ∑
T + 1
T
X
t=0
‚à•Œ£1/2
M (I ‚àíŒ∑Œ£M ‚àíŒ∑ŒªI)tgM,Œª‚à•2
HM
=
24Œ∑
T + 1
* T
X
t=0
(I ‚àíŒ∑Œ£M ‚àíŒ∑ŒªI)2tŒ£1/2
M gM,Œª, Œ£1/2
M gM,Œª
+
HM
‚â§
24
T + 1
D
(Œ£M + ŒªI)‚àí1Œ£1/2
M gM,Œª, Œ£1/2
M gM,Œª
E
HM
=
24
T + 1‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX)
(29)
where we used the convexity for the Ô¨Årst inequality and we used the following inequality for the last
inequality: since ‚à•kM‚à•L‚àû(œÅX)2 ‚â§12 and Œ∑(Œ£M + ŒªI) ‚âºŒ∑(12 + Œª)I ‚âº1
2I,
T
X
t=0
(I ‚àíŒ∑Œ£M ‚àíŒ∑ŒªI)2t ‚âº1
Œ∑ (Œ£M + ŒªI)‚àí1.
By plugging (24) and (29) into (23), we get the bound on the bias term:
E[‚à•AT ‚à•2
L2(œÅX)] ‚â§2‚à•A‚Ä≤T ‚à•2
L2(œÅX) + 2E[‚à•AT ‚àíA‚Ä≤T ‚à•2
L2(œÅX)]
‚â§
2
Œ∑2(T + 1)2 ‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•2
L2(œÅX) +
242
T + 1‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX).
(30)
Bound the noise term.
Note that the noise term in (22) exactly corresponds to the recursion (21)
with A0 = 0. Hence, it is enough to consider the case of A0 = 0 to evaluate the noise term. In this
case, the average AT can be rewritten as follows:
AT =
1
T + 1
T
X
t=0
t
X
s=0
tY
l=s+1
Œ±lŒ≤s =
Œ∑
T + 1
T
X
s=0
T
X
t=s
tY
l=s+1
Œ±l
Œ≤s
Œ∑
|
{z
}
=Zs
.
We here evaluate the noise term. We set zs = (xs, ys). Note that since Ezs[Œ≤s] = 0, we have for
s < s‚Ä≤,
E(zs,...,zT )
h
‚ü®Zs, Zs‚Ä≤‚ü©L2(œÅX)
i
=
Z
X
E(zs,...,zT )
" T
X
t=s
tY
l=s+1
Œ±l
Œ≤s
Œ∑
!  T
X
t=s‚Ä≤
tY
l=s‚Ä≤+1
Œ±l
Œ≤s‚Ä≤
Œ∑
!#
dœÅX
=
Z
X
Ezs[Œ≤s]E(zs+1,...,zT )
"
Œ≤s‚Ä≤
 T
X
t=s
tY
l=s+1
Œ±l
Œ∑
!  T
X
t=s‚Ä≤
tY
l=s‚Ä≤+1
Œ±l
Œ∑
!#
dœÅX
= 0.
25

Published as a conference paper at ICLR 2021
Therefore, we have
E[‚à•AT ‚à•2
L2(œÅX)] =
Œ∑2
(T + 1)2 E
Ô£Æ
Ô£∞

T
X
s=0
Zs

2
L2(œÅX)
Ô£π
Ô£ª
=
Œ∑2
(T + 1)2 E
Ô£Æ
Ô£∞
T
X
s,s‚Ä≤=0
‚ü®Zs, Zs‚Ä≤‚ü©L2(œÅX)
Ô£π
Ô£ª
=
Œ∑2
(T + 1)2
T
X
s=0
E
h
‚ü®Zs, Zs‚ü©L2(œÅX)
i
=
Œ∑2
(T + 1)2
T
X
s=0
E
h
‚à•Œ£1/2
M Zs‚à•2
HM
i
.
(31)
Here, we apply Lemma 21 in Pillaud-Vivien et al. (2018a) with A = Œ£M, H = Œ£M + ŒªI, C =
2(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))Œ£M. One of required conditions in this lemma is veriÔ¨Åed by
Lemma A. We verify the other required condition described below:
E

(KM,X ‚äóH KM,X + ŒªI)CH‚àí1(KM,X ‚äóH KM,X + ŒªI)

‚âº1
Œ∑ C.
(32)
Indeed, we have
E

(KM,X ‚äóH KM,X + ŒªI)CH‚àí1(KM,X ‚äóH KM,X + ŒªI)

= E

KM,X ‚äóH KM,XCH‚àí1KM,X ‚äóH KM,X

+ 2ŒªŒ£MCH‚àí1 + Œª2CH‚àí1
‚âºE

KM,X ‚äóH KM,XCH‚àí1KM,X ‚äóH KM,X

+ 6Œª(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))Œ£M,
where we used Œ£M(Œ£M + ŒªI)‚àí1 ‚âºI and Œª(Œ£M + ŒªI)‚àí1 ‚âºI. Moreover, we see
E

KM,X ‚äóH KM,XCH‚àí1KM,X ‚äóH KM,X

= 2(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))E

KM,X ‚äóH KM,XŒ£M(Œ£M + ŒªI)‚àí1KM,X ‚äóH KM,X

‚âº2(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))E

(KM,X ‚äóH KM,X)2
‚âº24(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))Œ£M,
where we used (27) for the last inequality. Hence, we get
E

(KM,X ‚äóH KM,X + ŒªI)CH‚àí1(KM,X ‚äóH KM,X + ŒªI)

‚âº(24 + 6Œª)(1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX))Œ£M.
Since, 4Œ∑(6 + Œª) ‚â§1, the condition (32) is veriÔ¨Åed. We apply Lemma 21 in Pillaud-Vivien et al.
(2018a) to (31), yielding the following inequality:
E[‚à•AT ‚à•2
L2(œÅX)] ‚â§
4
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£2
M(Œ£M + ŒªI)‚àí2
‚â§
4
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£M(Œ£M + ŒªI)‚àí1
. (33)
Convergence rate in terms of the optimization.
Finally, by combining (30) and (33) with (22),
we get the convergence rate of averaged stochastic gradient descent to gM,Œª:
E
g(T ) ‚àígM,Œª

2
L2(œÅX)

‚â§
4
Œ∑2(T + 1)2 ‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•2
L2(œÅX)
+ 2 ¬∑ 242
T + 1 ‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX)
+
8
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£M(Œ£M + ŒªI)‚àí1
,
where g(T ) def
=
1
T +1
PT
t=0 g(t). This Ô¨Ånishes the proof.
26

Published as a conference paper at ICLR 2021
D
PROOF OF PROPOSITION B
We provide Proposition B which provides the bound on Theorem A.
Proof of Proposition B. From the Bernstein‚Äôs inequality (Proposition 3 in Rudi & Rosasco (2017))
to random operators, the covariance operator Œ£M converges to Œ£‚àûas M ‚Üí‚àûin probability.
Especially, there exits M0 ‚ààZ+ such that for any M ‚â•M0, it follows that with high probability at
least 1 ‚àíŒ¥, Œ£‚àû‚àíŒ£M ‚âº1
2(Œ£‚àû+ ŒªI) in L2(œÅX). Thus, for ‚àÄf ‚ààL2(œÅX), we see
D
(Œ£‚àû+ ŒªI)‚àí1/2(Œ£‚àû‚àíŒ£M)(Œ£‚àû+ ŒªI)‚àí1/2f, f
E
L2(œÅX)
=
D
(Œ£‚àû‚àíŒ£M)(Œ£‚àû+ ŒªI)‚àí1/2f, (Œ£‚àû+ ŒªI)‚àí1/2f
E
L2(œÅX)
‚â§1
2
D
(Œ£‚àû+ ŒªI)(Œ£‚àû+ ŒªI)‚àí1/2f, (Œ£‚àû+ ŒªI)‚àí1/2f
E
L2(œÅX)
= 1
2‚à•f‚à•2
L2(œÅX).
Hence, we have
(Œ£‚àû+ ŒªI)‚àí1/2(Œ£‚àû‚àíŒ£M)(Œ£‚àû+ ŒªI)‚àí1/2 ‚âº1
2I.
Following the argument in Bach (2017b), we have for ‚àÄf ‚ààL2(œÅX),

(Œ£M + ŒªI)‚àí1f, f

L2(œÅX)
=

(Œ£‚àû+ ŒªI + Œ£M ‚àíŒ£‚àû)‚àí1f, f

L2(œÅX)
=

I + (Œ£‚àû+ ŒªI)‚àí1/2(Œ£M ‚àíŒ£‚àû)(Œ£‚àû+ ŒªI)‚àí1/2‚àí1
(Œ£‚àû+ ŒªI)‚àí1/2f, (Œ£‚àû+ ŒªI)‚àí1/2f

L2(œÅX)
= 2
D
(Œ£‚àû+ ŒªI)‚àí1/2f, (Œ£‚àû+ ŒªI)‚àí1/2f
E
L2(œÅX)
= 2

(Œ£‚àû+ ŒªI)‚àí1f, f

L2(œÅX) .
Thus, we conÔ¨Årm that with high probability at least 1 ‚àíŒ¥,
(Œ£M + ŒªI)‚àí1 ‚âº2(Œ£‚àû+ ŒªI)‚àí1
(34)
Utilizing this inequality, we show the Ô¨Årst and second inequalities in Proposition B as follows. It is
sufÔ¨Åcient to prove the second inequality because of
‚à•(Œ£M + ŒªI)‚àí1gM,Œª‚à•2
L2(œÅX) ‚â§1
Œª‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX)
Noting that gœÅ ‚ààH‚àûand gM,Œª = (Œ£M + ŒªI)‚àí1Œ£MgœÅ, we get
‚à•(Œ£M + ŒªI)‚àí1/2gM,Œª‚à•2
L2(œÅX) = ‚à•(Œ£M + ŒªI)‚àí1/2(Œ£M + ŒªI)‚àí1Œ£MgœÅ‚à•2
L2(œÅX)
‚â§‚à•(Œ£M + ŒªI)‚àí1/2gœÅ‚à•2
L2(œÅX)
‚â§2‚à•(Œ£‚àû+ ŒªI)‚àí1/2gœÅ‚à•2
L2(œÅX)
‚â§2‚à•Œ£‚àí1/2
‚àû
gœÅ‚à•2
L2(œÅX)
= 2‚à•gœÅ‚à•2
H‚àû.
The third inequality on the degree of freedom is a result obtained by Rudi & Rosasco (2017).
E
EIGENVALUE ANALYSIS OF NEURAL TANGENT KERNEL
E. 1
REVIEW OF SPHERICAL HARMONICS
We brieÔ¨Çy review the spherical harmonics which is useful in analyzing the eigenvalues of dot-product
kernels. For references, see Atkinson & Han (2012); Bach (2017a); Bietti & Mairal (2019); Cao et al.
(2019).
27

Published as a conference paper at ICLR 2021
Here, we denote by œÑd‚àí1 is the uniform distribution on the sphere Sd‚àí1 ‚äÇRd. The surface area of
Sd‚àí1 is œâd‚àí1 =
2œÄd/2
Œì(d/2) where Œì is the Gamma function. In L2(œÑd‚àí1), there is an orthonomal basis
consisting of a constant 1 and the spherical harmonics Ykj(x), k ‚ààZ‚â•1, j = 1, . . . , N(d, k), where
N(d, k) = 2k+d‚àí2
k

k + d ‚àí3
d ‚àí2

. That is, ‚ü®Yki, Ysj‚ü©L2(œÑd‚àí1) = Œ¥ksŒ¥ij and ‚ü®Yki, 1‚ü©L2(œÑd‚àí1) = 0.
The spherical harmonics Ykj are homogeneous functions of degree k, and clearly Ykj have the same
parity as k.
Legendre polynomial Pk(t) of degree k and dimension d (a.k.a. Gegenbauer polynomial) is deÔ¨Åned
as (Rodrigues‚Äô formula):
Pk(t) = (‚àí1/2)k
Œì( d‚àí1
2 )
Œì
 k + d‚àí1
2
(1 ‚àít2)(3‚àíd)/2
 d
dt
k
(1 ‚àít2)k+(d‚àí3)/2.
Legendre polynomials have the same parity as k. This polynomial is very useful in describing several
formulas regarding the spherical harmonics.
Addition formula.
We have the following addition formula:
N(d,k)
X
j=1
Ykj(x)Ykj(y) = N(d, k)Pk(x‚ä§y),
‚àÄx, ‚àÄy ‚ààSd‚àí1.
(35)
Hence, we see that Pk(x‚ä§¬∑) is spherical harmonics of degree k. Using the addition formula and the
orthogonality of spherical harmonics, we have
Z
Sd‚àí1 Pj(Z‚ä§x)Pk(Z‚ä§y)dœÑd‚àí1(Z) =
Œ¥jk
N(d, k)Pk(x‚ä§y).
(36)
Combining the following equation: for x = ted +
‚àö
1 ‚àít2x‚Ä≤, (x ‚ààSd‚àí1, x‚Ä≤ ‚ààSd‚àí2, t ‚àà[‚àí1, 1]),
œâd‚àí1
œâd‚àí2
dœÑd‚àí1(x) = (1 ‚àít2)(d‚àí3)/2dtdœÑd‚àí2(x‚Ä≤),
we see the orthogonality of Legendre polynomials in L2([‚àí1, 1], (1 ‚àít2)(d‚àí3)/2dt) and since
Pk(1) = 1 we see
Z 1
‚àí1
P 2
k (t)(1 ‚àít2)(d‚àí3)/2dt = œâd‚àí1
œâd‚àí2
1
N(d, k).
Recurrence relation.
We have the following relation:
tPk(t) =
k
2k + d ‚àí2Pk‚àí1(t) + k + d ‚àí2
2k + d ‚àí2Pk+1(t),
(37)
for k ‚â•1, and for k = 0 we have tP0(t) = P1(t).
Funk-Hecke formula.
The following formula is useful in computing Fourier coefÔ¨Åcients with
respect to spherical harmonics via Legendre polynomials. For any linear combination Yk of Ykj,
(j ‚àà{1, . . . , N(d, k)}) and any f ‚ààL2([‚àí1, 1], (1 ‚àít2)(d‚àí3)/2dt), we have for ‚àÄx,
Z
Sd‚àí1 f(x‚ä§y)Yk(y)dœÑd‚àí1(y) = œâd‚àí2
œâd‚àí1
Yk(x)
Z 1
‚àí1
f(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt.
(38)
This formula say that spherical harmonics are eigenfunctions of the integral operator deÔ¨Åned by
f(x‚ä§y) and each eigen-space is spanned by spherical harmonics of the same degree. Moreover, it
also provides a way of computing corresponding eigenvalues.
28

Published as a conference paper at ICLR 2021
E. 2
EIGENVALUES OF DOT-PRODUCT KERNELS
Let ¬µ0 be the uniform distribution on Sd‚àí1. Note that although œÑd‚àí1 and ¬µ0 are the same distribution,
we use two distributions œÑd‚àí1 and ¬µ0 depending on random variables. First, we consider any
activation function œÉ : R ‚ÜíR and a kernel function k(x, x‚Ä≤) = Eb(0)‚àº¬µ0[œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)]
on the sphere Sd‚àí1. We show this kernel function is a type of dot-product kernels, that is, there
is ÀÜk : R ‚ÜíR such that k(x, x‚Ä≤) = ÀÜk(x‚ä§x‚Ä≤). In fact, it can be conÔ¨Årmed as follows. For any
x, x‚Ä≤ ‚ààSd‚àí1, we take Œ∏ ‚àà[0, œÄ] so that x‚ä§x‚Ä≤ = cos Œ∏, and an orthogonal matrix A ‚ààRd√ód so that
Ax = (1, 0, . . . , 0)‚ä§and Ax‚Ä≤ = (cos Œ∏, sin Œ∏, 0, . . . , 0)‚ä§because A preserves the value of x‚ä§x‚Ä≤.
Then, since ¬µ0 is rotationally invariant we see
k(x, x‚Ä≤) =
Z
Sd‚àí1 œÉ(b(0)‚ä§Ax)œÉ(b(0)‚ä§Ax‚Ä≤)d¬µ0(b(0))
=
Z
Sd‚àí1 œÉ(b(0)
1 )œÉ(b(0)
1
cos(Œ∏) + b(0)
2
sin(Œ∏))d¬µ0(b(0)),
where b(0) = (b0
1, b(0)
2 , . . . , b(0)
d ). In other words, we see k is a function of Œ∏ = arccos (x‚ä§x‚Ä≤), and is
a dot-product kernel k(x, x‚Ä≤) = ÀÜk(x‚ä§x‚Ä≤). Hence, we can apply Funk-Hecke formula (38) to k(x, ¬∑).
The derivation of eigenvalues of the integral operator follows a way developed by Bach (2017a);
Bietti & Mairal (2019); Cao et al. (2019). In general, g ‚ààL2(œÑd‚àí1) can be decomposed by spherical
harmonics as follows.
g ‚àí
Z
Sd‚àí1 g(Z)dœÑd‚àí1(Z) =
‚àû
X
k=1
N(d,k)
X
j=1
‚ü®g, Ykj‚ü©L2(œÑd‚àí1) Ykj
=
‚àû
X
k=1
N(d,k)
X
j=1
Z
Sd‚àí1 g(Z)Ykj(Z)Ykj(¬∑)dœÑd‚àí1(Z)
=
‚àû
X
k=1
N(d, k)
Z
Sd‚àí1 g(Z)Pk(Z‚ä§¬∑)dœÑd‚àí1(Z),
(39)
where we used addition formula to the last equality.
Here, we apply this decomposition (39) to k(x, ¬∑) = ÀÜk(x‚ä§¬∑). Since Pk(Z‚ä§¬∑) is a linear combination
of spherical harmonics of degree k (see addition formula), we get
k(x, ¬∑) ‚àí
Z
Sd‚àí1
ÀÜk(x‚ä§Z)dœÑd‚àí1(Z) =
‚àû
X
k=1
N(d, k)
Z
Sd‚àí1
ÀÜk(x‚ä§Z)Pk(Z‚ä§¬∑)dœÑd‚àí1(Z)
=
‚àû
X
k=1
ÀÜŒªkN(d, k)Pk(x‚ä§¬∑),
(40)
where we used Funk-Hecke formula (38) and we set ÀÜŒªk = œâd‚àí2
œâd‚àí1
R 1
‚àí1 ÀÜk(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt. We
note that ÀÜŒªk is eigenvalue with multiplicity N(d, k) of the integral operator deÔ¨Åned by k.
Next, we derive another expression of k. In a similar way, we obtain the following equation:
œÉ(b(0)‚ä§x) ‚àí
Z
Sd‚àí1 œÉ(Z‚ä§x)dœÑd‚àí1(Z) =
‚àû
X
k=1
ÀÜ¬µkN(d, k)Pk(b(0)‚ä§x),
where ÀÜ¬µk = œâd‚àí2
œâd‚àí1
R 1
‚àí1 œÉ(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt. By the deÔ¨Ånition of k and the orthogonality of
spherical harmonics, we get
k(x, x‚Ä≤) = Eb(0)
h
œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)
i
=
Z
Sd‚àí1 œÉ(Z‚ä§x)dœÑd‚àí1(Z)
Z
Sd‚àí1 œÉ(Z‚ä§x‚Ä≤)dœÑd‚àí1(Z) +
‚àû
X
k=1
ÀÜ¬µ2
kN 2(d, k)Eb(0)
h
Pk(b(0)‚ä§x)Pk(b(0)‚ä§x‚Ä≤)
i
=
Z
Sd‚àí1 œÉ(Z‚ä§x)dœÑd‚àí1(Z)
Z
Sd‚àí1 œÉ(Z‚ä§x‚Ä≤)dœÑd‚àí1(Z) +
‚àû
X
k=1
ÀÜ¬µ2
kN(d, k)Pk(x‚ä§x‚Ä≤),
(41)
29

Published as a conference paper at ICLR 2021
where we used equation (36). By the rotationally invariance, we can show
Z
Sd‚àí1
ÀÜk(x‚ä§Z)dœÑd‚àí1(Z) =
Z
Sd‚àí1 œÉ(Z‚ä§x)dœÑd‚àí1(Z)
Z
Sd‚àí1 œÉ(Z‚ä§x‚Ä≤)dœÑd‚àí1(Z).
Thus, comparing (40) with (41), we get ÀÜŒªk = ÀÜ¬µ2
k.
E. 3
EIGENVALUES OF NEURAL TANGENT KERNELS
Utilizing a relation ÀÜŒªk = ÀÜ¬µ2
k, we derive a way of computing eigenvalues of the integral operator
deÔ¨Åned by the integral operators Œ£‚àûassociated with the activation œÉ. Recall the deÔ¨Ånition of the
neural tangent kernel:
k‚àû(x, x‚Ä≤)
def
= Eb(0)‚àº¬µ0[œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)] + (x‚ä§x‚Ä≤ + Œ≥2)Eb(0)‚àº¬µ0[œÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)].
A neural tangent kernel consists of three kernels:
h1(x, x‚Ä≤) = Eb(0)‚àº¬µ0
h
œÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)
i
,
h2(x, x‚Ä≤) = Eb(0)‚àº¬µ0
h
œÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)
i
,
h3(x, x‚Ä≤) = x‚ä§x‚Ä≤Eb(0)‚àº¬µ0
h
œÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)
i
.
By the argument in the previous subsection, h1 and h2 are dot-product kernel, that is, there exist ÀÜh1
and ÀÜh2 such that h1(x, x‚Ä≤) = ÀÜh1(x‚ä§x‚Ä≤) and h2(x, x‚Ä≤) = ÀÜh2(x‚ä§x‚Ä≤). Moreover, h3 is a dot-product
kernel as well because we get h3(x, x‚Ä≤) = ÀÜh3(x‚ä§x‚Ä≤) by setting ÀÜh3(t) = tÀÜh2(t). Hence, theory
explained earlier is applicable to these kernels.
Eigenvalues ÀÜ¬µk for kernels h1 and h2 are described as follows:
ÀÜ¬µ(1)
k
= œâd‚àí2
œâd‚àí1
Z 1
‚àí1
œÉ(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt,
(42)
ÀÜ¬µ(2)
k
= œâd‚àí2
œâd‚àí1
Z 1
‚àí1
œÉ‚Ä≤(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt,
(43)
yielding eigenvalues ÀÜŒª(1)
k
= (ÀÜ¬µ(1)
k )2 and ÀÜŒª(2)
k
= (ÀÜ¬µ(2)
k )2 for h1 and h2, respectively. As for eigenval-
ues ÀÜŒª(3)
k
for h3, we have
ÀÜŒª(3)
k
= œâd‚àí2
œâd‚àí1
Z 1
‚àí1
tÀÜh2(t)Pk(t)(1 ‚àít2)(d‚àí3)/2dt
=
k
2k + d ‚àí2
œâd‚àí2
œâd‚àí1
Z 1
‚àí1
ÀÜh2(t)Pk‚àí1(t)(1 ‚àít2)(d‚àí3)/2dt
+ k + d ‚àí2
2k + d ‚àí2
œâd‚àí2
œâd‚àí1
Z 1
‚àí1
ÀÜh2(t)Pk+1(t)(1 ‚àít2)(d‚àí3)/2dt
=
k
2k + d ‚àí2
ÀÜŒª(2)
k‚àí1 + k + d ‚àí2
2k + d ‚àí2
ÀÜŒª(2)
k+1,
where we used the recurrence relation (37). Since, h1, h2, and h3 have the same eigenfunctions,
eigenvalues ÀÜŒª‚àû,k of k‚àûis
ÀÜŒª‚àû,k = ÀÜŒª(1)
k
+ Œ≥2ÀÜŒª(2)
k
+
k
2k + d ‚àí2
ÀÜŒª(2)
k‚àí1 + k + d ‚àí2
2k + d ‚àí2
ÀÜŒª(2)
k+1.
(44)
Hence, calculation of {ÀÜŒª‚àû,k}‚àû
k=1 results in computing ÀÜ¬µ(1)
k
and ÀÜ¬µ(2)
k
for given activation œÉ.
30

Published as a conference paper at ICLR 2021
Eigenvalues for ReLU and smooth approximations of ReLU.
As for ReLU activation, its eigen-
values were derived in Bach (2017a). Let œÉ be ReLU. Then, ÀÜ¬µ(1)
k
= 0 and ÀÜ¬µ(2)
k
‚àºk‚àíd/2 when k is
odd and ÀÜ¬µ(1)
k
‚àºk‚àíd/2‚àí1 and ÀÜ¬µ(2)
k
= 0 when k is even. Consequently, we see ÀÜŒª‚àû,k = Œò(k‚àíd).
We note that the multiplicity of ÀÜŒª‚àû,k is N(d, k), so that we should take into account the multiplicity
to derive decay order of eigenvalues Œª‚àû,i of Œ£‚àû. Since 1+Pk
j=1 N(d‚àí1, j) = N(d, k) (for details
see Atkinson & Han (2012)), we see Œª‚àû,N(d+1,k) = Œò(k‚àíd). Moreover, N(d + 1, k) = Œò(kd‚àí1)
yields Œª‚àû,N(d+1,k) = Œò

N(d + 1, k)‚àí1‚àí
1
d‚àí1

. As a result, Assumption (A4) is veriÔ¨Åed with
Œ≤ = 1 +
1
d‚àí1 for ReLU.
For the smooth approximation œÉ(s) of ReLU satisfying Assumption (A1‚Äô), we can show that every
eigenvalue of Œ£(s)
‚àûderived from œÉ(s) converges to that for ReLU as s ‚Üí‚àûbecause of (42) and (43)
with Lebesgue‚Äôs convergence theorem.
F
EXPLICIT CONVERGENCE RATES FOR SMOOTH APPROXIMATION OF RELU
For convenience, we here list notations used in this section. In this section, let œÉ and œÉ(s) be ReLU
activation and its smooth approximation satisfying (A1‚Äô), respectively, and for M ‚ààZ+ ‚à™{‚àû} let
kM, Œ£M, gM,Œª, k(s)
M , Œ£(s)
M , g(s)
M,Œª be corresponding kernel, integral operators, and minimizers of the
regularized expected risk functions. Let g(T ) be iterates obtained by the reference ASGD (Algorithm
2) in the RKHS associated with k(s)
M .
We consider the following decomposition:
1
3‚à•g(T ) ‚àígœÅ‚à•2
L2(œÅX) ‚â§‚à•g(T ) ‚àíg(s)
M,Œª‚à•2
L2(œÅX)
(45)
+ ‚à•g(s)
M,Œª ‚àíg(s)
‚àû,Œª‚à•2
L2(œÅX)
(46)
+ ‚à•g(s)
‚àû,Œª ‚àíg‚àû,Œª‚à•2
L2(œÅX)
(47)
+ ‚à•g‚àû,Œª ‚àígœÅ‚à•2
L2(œÅX).
(48)
These terms can be made arbitrarily small by taking large M and s. As for (48) this property is a
direct consequence of Proposition D. Note that Proposition C is not applicable to (46) because this
proposition require the speciÔ¨Åcation of the target function by k(s)
‚àûwhich does not hold in general.
In the following, we treat the remaining terms.
Proposition E. Suppose (A1‚Äô) and (A2‚Äô) hold. Then, we have
1. plimM‚Üí‚àû‚à•k(s)
M ‚àík(s)
‚àû‚à•L‚àû(œÅX)2 = 0, lims‚Üí‚àû‚à•k(s)
‚àû‚àík‚àû‚à•L‚àû(œÅX)2 = 0,
2. plimM‚Üí‚àû
Tr

Œ£(s)
M ‚àíŒ£(s)
‚àû
 = 0, lims‚Üí‚àû
Tr

Œ£(s)
‚àû‚àíŒ£‚àû
 = 0,
3. plimM‚Üí‚àû‚à•g(s)
M,Œª ‚àíg(s)
‚àû,Œª‚à•L‚àû(œÅX) = 0, lims‚Üí‚àû‚à•g(s)
‚àû,Œª ‚àíg‚àû,Œª‚à•L‚àû(œÅX) = 0,
where plim denotes the convergence in probability.
Proof. We show the Ô¨Årst statement. By the uniform law of large numbers (Theorem 3.1 in Mohri
et al. (2012)), we see the convergence in probability:
‚à•k(s)
M ‚àík(s)
‚àû‚à•L‚àû(œÅX)2 ‚â§
sup
x,x‚Ä≤‚ààSd‚àí1

1
M
M
X
r=1
œÉ(s)(b(0)‚ä§
r
x)œÉ(s)(b(0)‚ä§
r
x‚Ä≤) ‚àíEb(0)
h
œÉ(s)(b(0)‚ä§x)œÉ(s)(b(0)‚ä§x‚Ä≤)
i
+ (1 + Œ≥2)
sup
x,x‚Ä≤‚ààSd‚àí1

1
M
M
X
r=1
œÉ(s)‚Ä≤(b(0)‚ä§
r
x)œÉ(s)‚Ä≤(b(0)‚ä§
r
x‚Ä≤) ‚àíEb(0)
h
œÉ(s)‚Ä≤(b(0)‚ä§x)œÉ(s)‚Ä≤(b(0)‚ä§x‚Ä≤)
i
p‚àí‚Üí0,
31

Published as a conference paper at ICLR 2021
where the limit is taken with respect to M ‚Üí‚àûand the notation
p‚àí‚Üídenotes the convergence in
probability. Next, we have the following convergence:
‚à•k(s)
‚àû‚àík‚àû‚à•L‚àû(œÅX)2 ‚â§
sup
x,x‚Ä≤‚ààSd‚àí1 Eb(0)
hœÉ(s)(b(0)‚ä§x)œÉ(s)(b(0)‚ä§x‚Ä≤) ‚àíœÉ(b(0)‚ä§x)œÉ(b(0)‚ä§x‚Ä≤)

i
+ (1 + Œ≥2)
sup
x,x‚Ä≤‚ààSd‚àí1 Eb(0)
hœÉ(s)‚Ä≤(b(0)‚ä§x)œÉ(s)‚Ä≤(b(0)‚ä§x‚Ä≤) ‚àíœÉ‚Ä≤(b(0)‚ä§x)œÉ‚Ä≤(b(0)‚ä§x‚Ä≤)

i
‚â§4 sup
x‚ààSd‚àí1 Eb(0)
hœÉ(s)(b(0)‚ä§x) ‚àíœÉ(b(0)‚ä§x)

i
+ 4(1 + Œ≥2) sup
x‚ààSd‚àí1 Eb(0)
hœÉ(s)‚Ä≤(b(0)‚ä§x) ‚àíœÉ‚Ä≤(b(0)‚ä§x)

i
= 4
hœÉ(s)(b(0)‚ä§e1) ‚àíœÉ(b(0)‚ä§e1)

i
+ 4(1 + Œ≥2)Eb(0)
œÉ(s)‚Ä≤(b(0)‚ä§e1) ‚àíœÉ‚Ä≤(b(0)‚ä§e1)
 ‚Üí0,
where for the Ô¨Årst inequality we used the boundedness of œÉ, œÉ‚Ä≤, œÉ(s), and œÉ(s)‚Ä≤ on [‚àí1, 1], for the
equality we used the rotationally invariance of the measure ¬µ0, and the limit is taken with respect
to s ‚Üí‚àû. For the Ô¨Ånal convergence in the above expression we used Assumption (A1‚Äô) and
boundedness with Lebesgue‚Äôs convergence theorem.
In general, for a kernel k and associated integral operator Œ£ with œÅX, we have Tr (Œ£) =
R
Sd‚àí1 k(X, X)dœÅX. Hence,
Tr

Œ£(s)
M ‚àíŒ£(s)
‚àû
 ‚â§‚à•k(s)
M ‚àík(s)
‚àû‚à•L‚àû(œÅX)2 and
Tr

Œ£(s)
‚àû‚àíŒ£‚àû
 ‚â§
‚à•k(s)
‚àû‚àík‚àû‚à•L‚àû(œÅX)2, and the second statement holds immediately.
Finally, we show the third statement. In the same manner as the derivation of inequality (19), we get
‚à•((Œ£(s)
M + ŒªI)‚àí1 ‚àí(Œ£(s)
‚àû+ ŒªI)‚àí1)gœÅ‚à•L2(œÅX) ‚â§1
Œª2 ‚à•Œ£(s)
M ‚àíŒ£(s)
‚àû‚à•op‚à•gœÅ‚à•L2(œÅX).
We denote F (s)
M = (Œ£(s)
M + ŒªI)‚àí1gœÅ and F (s)
‚àû= (Œ£(s)
‚àû+ ŒªI)‚àí1gœÅ. Noting g(s)
M,Œª = Œ£(s)
M F (s)
M and
g(s)
‚àû,Œª = Œ£(s)
‚àûF (s)
‚àû, we get for x ‚ààSd‚àí1,
|g(s)
M,Œª(x) ‚àíg(s)
‚àû,Œª(x)|
=
Œ£(s)
M F (s)
M (x) ‚àíŒ£(s)
‚àûF (s)
‚àû(x)

=

Z
X
K(s)
M,x(X)F (s)
M (X)dœÅX ‚àí
Z
X
K(s)
‚àû,x(X‚Ä≤)F (s)
‚àû(X‚Ä≤)dœÅX

=

Z
X
(K(s)
M,x ‚àíK(s)
‚àû,x)(X)F (s)
M (X)dœÅX ‚àí
Z
X
K(s)
‚àû,x(X)(F (s)
M ‚àíF (s)
‚àû)(X)dœÅX

‚â§‚à•K(s)
M,x ‚àíK(s)
‚àû,x‚à•L2(œÅX)‚à•F (s)
M ‚à•L2(œÅX) + ‚à•K(s)
‚àû,x‚à•L2(œÅX)‚à•F (s)
M ‚àíF (s)
‚àû‚à•L2(œÅX)
‚â§1
Œª‚à•k(s)
M ‚àík(s)
‚àû‚à•L‚àû(œÅX)2‚à•gœÅ‚à•L2(œÅX) + 12
Œª2 ‚à•Œ£(s)
M ‚àíŒ£(s)
‚àû‚à•op‚à•gœÅ‚à•L2(œÅX).
(49)
The both terms in the last expression (49) converge to 0 in probability because of the Ô¨Årst statement of
this proposition and the Bernstein‚Äôs inequality (Proposition 3 in Rudi & Rosasco (2017)) to random
operators. This Ô¨Ånishes the proof of the former of the third statement.
In the same manner, we have
‚à•g(s)
‚àû,Œª ‚àíg‚àû,Œª‚à•L‚àû(œÅX) ‚â§1
Œª‚à•k‚àû‚àík(s)
‚àû‚à•L‚àû(œÅX)2‚à•gœÅ‚à•L2(œÅX) + 12
Œª2 ‚à•Œ£‚àû‚àíŒ£(s)
‚àû‚à•op‚à•gœÅ‚à•L2(œÅX).
(50)
The Ô¨Årst term in the right hand side converges to 0 because of the Ô¨Årst statement of this proposition.
We next show the convergence ‚à•Œ£‚àû‚àíŒ£(s)
‚àû‚à•op ‚Üí0 as s ‚Üí‚àû. As seen in the previous section,
Œ£‚àûand Œ£(s)
‚àûshare the same eigenfunctions and every eigenvalue of Œ£(s)
‚àûconverges to that of Œ£‚àû
as s ‚Üí‚àû. Let {Œª(s)
‚àû,i}‚àû
i=1 and {Œª‚àû,i}‚àû
i=1 be eigenvalues of Œ£(s)
‚àûand Œ£‚àû, respectively. For an
arbitrary œµ > 0, we can take iœµ such that P‚àû
i=iœµ Œª‚àû,i < œµ. From the convergence Œª(s)
‚àû,i ‚ÜíŒª‚àû,i and
Tr

Œ£(s)
‚àû

‚ÜíTr (Œ£‚àû) as s ‚Üí‚àû, we see that for arbitrarily sufÔ¨Åciently large s, |Œª(s)
‚àû,i ‚àíŒª‚àû,i| < œµ
(i < iœµ) and P‚àû
i=iœµ Œª(s)
‚àû,i < 2œµ. Clearly, for i ‚â•iœµ, |Œª(s)
‚àû,i ‚àíŒª‚àû,i| ‚â§P‚àû
i=iœµ(Œª(s)
‚àû,i + Œª‚àû,i) < 3œµ.
32

Published as a conference paper at ICLR 2021
Therefore, we conclude the uniform convergence supi‚àà{1,...,‚àû} |Œª(s)
‚àû,i ‚àíŒª‚àû,i| ‚Üí0 as s ‚Üí‚àû. This
implies ‚à•Œ£‚àû‚àíŒ£(s)
‚àû‚à•op ‚Üí0 as s ‚Üí‚àû.
So far, we have shown that (46), (47), and (48) can be made arbitrarily small by taking large s and M
depending on Œª. The remaining problem is to show the convergence of (45). To do so, we establish
the counterpart of Theorem B by adapting Theorem A and Proposition B to the current setting.
The counterpart of Theorem B.
In Theorem A, the condition (A3) is required for NTK associated
with the smooth activation œÉ(s) and it is not satisÔ¨Åed in general. Note that (A3) is used for bounding
‚à•g(s)
M,Œª‚à•L‚àû(œÅX) uniformly as seen in the proof of Lemma A. Let us consider the decomposition:
‚à•g(s)
M,Œª‚à•L‚àû(œÅX) ‚â§‚à•g‚àû,Œª‚à•L‚àû(œÅX) + ‚à•g(s)
M,Œª ‚àíg(s)
‚àû,Œª‚à•L‚àû(œÅX) + ‚à•g(s)
‚àû,Œª ‚àíg‚àû,Œª‚à•L‚àû(œÅX)
‚â§2
‚àö
3‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX) + ‚à•g(s)
M,Œª ‚àíg(s)
‚àû,Œª‚à•L‚àû(œÅX) + ‚à•g(s)
‚àû,Œª ‚àíg‚àû,Œª‚à•L‚àû(œÅX)
‚Üí2
‚àö
3‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX).
Here, for the second inequality we used (20). Note that the inequality (20) holds for Œ£‚àûbecause
the condition (A3) is supposed for ReLU. For the last inequality we used Proposition E. Hence,
Theorem A can be applicable and the same convergence in Theorem A holds for œÉ(s). For arbitrarily
sufÔ¨Åciently large s and M with high probability, we have
E
g(T ) ‚àíg(s)
M,Œª

2
L2(œÅX)

‚â§
4
Œ∑2(T + 1)2 ‚à•(Œ£(s)
M + ŒªI)‚àí1g(s)
M,Œª‚à•2
L2(œÅX)
+ 2 ¬∑ 242
T + 1 ‚à•(Œ£(s)
M + ŒªI)‚àí1/2g(s)
M,Œª‚à•2
L2(œÅX)
+
8
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + 24‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr

Œ£(s)
M (Œ£(s)
M + ŒªI)‚àí1
.
(51)
Next, we adapt Proposition B to the current setting. By inequality (34), there exists M0 ‚ààZ+ such
that ‚àÄM ‚â•M0, with high probability,
‚à•(Œ£(s)
M + ŒªI)‚àí1/2g(s)
M,Œª‚à•2
L2(œÅX) ‚â§2‚à•(Œ£(s)
‚àû+ ŒªI)‚àí1/2gœÅ‚à•2
L2(œÅX).
If (Œ£(s)
‚àû+ ŒªI)‚àí1 ‚âº2(Œ£‚àû+ ŒªI)‚àí1 holds, then we have the counterpart of the second inequality in
Proposition B because
‚à•(Œ£‚àû+ ŒªI)‚àí1/2gœÅ‚à•2
L2(œÅX) ‚â§‚à•Œ£‚àí1/2
‚àû
gœÅ‚à•2
L2(œÅX) = ‚à•gœÅ‚à•2
H‚àû,
(52)
where we used the fact that gœÅ is contained in H‚àûbecause of (A3‚Äô). Note that the Ô¨Årst inequality
in Proposition B is a direct consequence of the second one. We consider eigenvalues {Œª(s)
‚àû,i}‚àû
i=1
and {Œª‚àû,i}‚àû
i=1 of Œ£(s)
‚àûand Œ£‚àû, respectively. Let iŒª be an index such that for ‚àÄi > iŒª, Œª‚àû,i ‚â§Œª
2 .
Since, every eigenvalue of {Œª(s)
‚àû,i}‚àû
i=1 converges to that of {Œª‚àû,i}‚àû
i=1 as s ‚Üí‚àû, for an arbitrarily
sufÔ¨Åciently large s, we have |Œª(s)
‚àû,i‚àíŒª‚àû,i| ‚â§Œª
2 for ‚àÄi < iŒª, leading to 1/(Œª+Œª(s)
‚àû,i) ‚â§2/(Œª+Œª‚àû,i).
As for the case i ‚â•iŒª, since 3
2Œª ‚â•Œª + Œª‚àû,i, we have 1/(Œª + Œª(s)
‚àû,i) ‚â§1/Œª ‚â§3/(2(Œª + Œª‚àû,i)).
Combining these, we obtain (Œ£(s)
‚àû+ ŒªI)‚àí1 ‚âº2(Œ£‚àû+ ŒªI)‚àí1 and
lim
s‚Üí‚àûplimM‚Üí‚àû‚à•(Œ£(s)
M + ŒªI)‚àí1g(s)
M,Œª‚à•2
L2(œÅX) ‚â§4Œª‚àí1‚à•gœÅ‚à•2
H‚àû,
(53)
lim
s‚Üí‚àûplimM‚Üí‚àû‚à•(Œ£(s)
M + ŒªI)‚àí1/2g(s)
M,Œª‚à•2
L2(œÅX) ‚â§4‚à•gœÅ‚à•2
H‚àû.
(54)
These are the counterpart of the Ô¨Årst and second inequalities in Proposition B.
Next, we consider the bound on the degree of freedom in this proposition. Assume Œª ‚â§1
2‚à•Œ£‚àû‚à•op.
As seen earlier, an operator Œ£(s)
‚àûconverge to Œ£‚àûin terms of the operator norm. Hence, Œª ‚â§‚à•Œ£(s)
‚àû‚à•op
33

Published as a conference paper at ICLR 2021
for an arbitrarily sufÔ¨Åciently large s and the bound on the degree of freedom in Proposition B is
applicable. We get
Tr

Œ£(s)
M (Œ£(s)
M + ŒªI)‚àí1
‚â§3Tr

Œ£(s)
‚àû(Œ£(s)
‚àû+ ŒªI)‚àí1
.
(55)
Let us consider upper bounding the right hand side:
Tr

Œ£(s)
‚àû(Œ£(s)
‚àû+ ŒªI)‚àí1
=
iŒª‚àí1
X
i=1
Œª(s)
‚àû,i
Œª + Œª(s)
‚àû,i
+
‚àû
X
i=iŒª
Œª(s)
‚àû,i
Œª + Œª(s)
‚àû,i
‚â§
iŒª‚àí1
X
i=1
Œª(s)
‚àû,i
Œª + Œª(s)
‚àû,i
+ 1
Œª
‚àû
X
i=iŒª
Œª(s)
‚àû,i
=
iŒª‚àí1
X
i=1
Œª(s)
‚àû,i
Œª + Œª(s)
‚àû,i
‚àí1
Œª
iŒª‚àí1
X
i=1
Œª(s)
‚àû,i + 1
ŒªTr

Œ£(s)
‚àû

.
On the other hand, by the deÔ¨Ånition of iŒª,
2Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
= 2
iŒª‚àí1
X
i=1
Œª‚àû,i
Œª + Œª‚àû,i
+ 2
‚àû
X
i=iŒª
Œª‚àû,i
Œª + Œª‚àû,i
‚â•2
iŒª‚àí1
X
i=1
Œª‚àû,i
Œª + Œª‚àû,i
+ 1
Œª
‚àû
X
i=iŒª
Œª‚àû,i
= 2
iŒª‚àí1
X
i=1
Œª‚àû,i
Œª + Œª‚àû,i
‚àí1
Œª
iŒª‚àí1
X
i=1
Œª‚àû,i + 1
ŒªTr (Œ£‚àû)
‚â•
iŒª‚àí1
X
i=1
Œª‚àû,i
Œª + Œª‚àû,i
‚àí1
Œª
iŒª‚àí1
X
i=1
Œª‚àû,i + 1
ŒªTr (Œ£‚àû) .
Therefore, by inequality (55), the convergence of Œª(s)
‚àû,i ‚ÜíŒª‚àû,i for i ‚àà{1, . . . , iŒª ‚àí1} as s ‚Üí‚àû,
and the second statement in Proposition E, we have
plims‚Üí‚àûlim
M‚Üí‚àûTr

Œ£(s)
M (Œ£(s)
M + ŒªI)‚àí1
‚â§9Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
.
(56)
Combining (45)-(48) with (51), (53), (54), and (56), we establish the counterpart of Theorem B. For
given œµ, Œª, and Œ¥, there exist sufÔ¨Åciently large s and M such that with high probability 1 ‚àíŒ¥,
E
g(T ) ‚àígœÅ

2
L2(œÅX)

‚â§œµ + Œ±Œª2r‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX) +
Œ±
T + 1

1 +
1
ŒªŒ∑2(T + 1)

‚à•gœÅ‚à•2
H‚àû
+
Œ±
T + 1

1 + ‚à•gœÅ‚à•2
L2(œÅX) + ‚à•Œ£‚àír
‚àûgœÅ‚à•2
L2(œÅX)

Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
,
(57)
where Œ± > 0 is a universal constant.
Proof of Corollary 2.
Since conditions (A1‚Äô) and (A2‚Äô) are special cases of (A1) and (A2), we can
apply Proposition A to Algorithm 1 for the neural network with the smooth approximation œÉ(s) of
ReLU. Hence, by setting Œ∑t = Œ∑ = O(1) satisfying 4(6 + Œª)Œ∑ ‚â§1 and Œª = T ‚àíŒ≤/(2rŒ≤+1) where
Œ≤ = 1 +
1
d‚àí1, and by applying Tr
 Œ£‚àû(Œ£‚àû+ ŒªI)‚àí1
= O(Œª‚àí1/Œ≤) (Caponnetto & De Vito, 2007)
and
‚à•gœÅ‚à•H‚àû, ‚à•gœÅ‚à•L2(œÅX) ‚â§O
 ‚à•Œ£‚àír
‚àûgœÅ‚à•L2(œÅX)

because of ‚à•Œ£‚àû‚à•op ‚â§O(1), we Ô¨Ånish the proof of Corollary 2.
34

Published as a conference paper at ICLR 2021
G
APPLICATION TO BINARY CLASSIFICATION PROBLEMS
In this paper, we mainly focused on regression problems, but our idea can be applied to other
applications. We brieÔ¨Çy discuss its application to binary classiÔ¨Åcation problems. A label space is set
to Y = {‚àí1, 1} and a loss function is set to be the squared loss: ‚Ñì(z, y) = 0.5(y ‚àíz)2. The ultimate
goal of the binary classiÔ¨Åcation problem is to obtain the Bayes classiÔ¨Åer that minimizes the expected
classiÔ¨Åcation error,
R(g)
def
= P(X,Y )‚àºœÅ[sgn(g(X)) Ã∏= Y ],
over all measurable maps. It is known that the Bayes classiÔ¨Åer is expressed as sgn(gœÅ(X)), where gœÅ
is the Bayes rule of L(g) = EœÅ[l(g(X), Y )] (see Zhang (2004); Bartlett et al. (2006)). Therefore, if
gœÅ satisÔ¨Åes a margin condition, i.e., |gœÅ(x)| ‚â•‚àÉœÑ > 0 on supp(œÅX), then this goal is achieved by
obtaining an œÑ/2-accurate solution of gœÅ in terms of the uniform norm on supp(œÅX). That is, the
required optimization accuracy on ‚à•gŒò
(T ) ‚àígœÅ‚à•L‚àû(œÅX) to obtain the Bayes classiÔ¨Åer depends only on
the margin œÑ unlike regression problems. Due to this property, averaged stochastic gradient descent
in RKHSs can achieve the linear convergence rate demonstrated in Pillaud-Vivien et al. (2018a). To
leverage this theory to our problem setting, we consider the following decomposition:
‚à•gŒò
(T ) ‚àígœÅ‚à•L‚àû(œÅX) ‚â§‚à•gŒò
(T ) ‚àíg(T )‚à•L‚àû(œÅX)
(58)
+ ‚à•g(T ) ‚àígM,Œª‚à•L‚àû(œÅX)
(59)
+ ‚à•gM,Œª ‚àíg‚àû,Œª‚à•L‚àû(œÅX)
(60)
+ ‚à•g‚àû,Œª ‚àígœÅ‚à•L‚àû(œÅX).
(61)
The last term (61) can be made arbitrary small by Œª ‚Üí0 as shown in Pillaud-Vivien et al. (2018a).
A term (60) can be bounded in the same manner as the third statement of Proposition E, yielding
the convergence to 0 as M ‚Üí‚àûwith high probability. The convergence of (59) was shown in
Pillaud-Vivien et al. (2018a) and the convergence of (58) is guaranteed by Proposition A. As a result,
we can show the following exponential convergence of the classiÔ¨Åcation error R(g) for two-layer
neural networks with a sufÔ¨Åciently small Œª as demonstrated in Pillaud-Vivien et al. (2018a).
E[R(gŒò
(T )) ‚àíR(gœÅ)] ‚â§2 exp(‚àíO(Œª2œÑ 2T)).
In Nitanda & Suzuki (2019), an exponential convergence was shown for the logistic loss ‚Ñì(z, y) =
log(1 + exp(‚àíyz)) as well. Proposition A also holds for the logistic loss with an easier proof than
the squared loss because of the boundedness of stochastic gradients of the loss. Hence, their theory is
also applicable to the reference ASGD in an RKHS. In summary, (58), (59), and (61) can be bounded
by the above argument. However, we note that bounding (60) is not obvious and is left for future
work.
35

