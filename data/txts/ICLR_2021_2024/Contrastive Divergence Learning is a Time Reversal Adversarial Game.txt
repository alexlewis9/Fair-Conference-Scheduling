Published as a conference paper at ICLR 2021
CONTRASTIVE DIVERGENCE LEARNING IS A TIME
REVERSAL ADVERSARIAL GAME
Yair Omer
Department of Electrical Engineering
Technion - Israel Institute of Technology
Haifa, Israel
omeryair@gmail.com
Tomer Michaeli
Department of Electrical Engineering
Technion - Israel Institute of Technology
Haifa, Israel
tomer.m@ee.technion.ac.il
ABSTRACT
Contrastive divergence (CD) learning is a classical method for ﬁtting unnormal-
ized statistical models to data samples. Despite its wide-spread use, the conver-
gence properties of this algorithm are still not well understood. The main source
of difﬁculty is an unjustiﬁed approximation which has been used to derive the
gradient of the loss. In this paper, we present an alternative derivation of CD that
does not require any approximation and sheds new light on the objective that is
actually being optimized by the algorithm. Speciﬁcally, we show that CD is an
adversarial learning procedure, where a discriminator attempts to classify whether
a Markov chain generated from the model has been time-reversed. Thus, although
predating generative adversarial networks (GANs) by more than a decade, CD is,
in fact, closely related to these techniques. Our derivation settles well with pre-
vious observations, which have concluded that CD’s update steps cannot be ex-
pressed as the gradients of any ﬁxed objective function. In addition, as a byprod-
uct, our derivation reveals a simple correction that can be used as an alternative
to Metropolis-Hastings rejection, which is required when the underlying Markov
chain is inexact (e.g., when using Langevin dynamics with a large step).
1
INTRODUCTION
Unnormalized probability models have drawn signiﬁcant attention over the years. These models
arise, for example, in energy based models, where the normalization constant is intractable to com-
pute, and are thus relevant to numerous settings. Particularly, they have been extensively used in the
context of restricted Boltzmann machines (Smolensky, 1986; Hinton, 2002), deep belief networks
(Hinton et al., 2006; Salakhutdinov & Hinton, 2009), Markov random ﬁelds (Carreira-Perpinan &
Hinton, 2005; Hinton & Salakhutdinov, 2006), and recently also with deep neural networks (Xie
et al., 2016; Song & Ermon, 2019; Du & Mordatch, 2019; Grathwohl et al., 2019; Nijkamp et al.,
2019).
Fitting an unnormalized density model to a dataset is challenging due to the missing normalization
constant of the distribution. A naive approach is to employ approximate maximum likelihood esti-
mation (MLE). This approach relies on the fact that the likelihood’s gradient can be approximated
using samples from the model, generated using Markov Chain Monte Carlo (MCMC) techniques.
However, a good approximation requires using very long chains and is thus impractical. This dif-
ﬁculty motivated the development of a plethora of more practical approaches, like score matching
(Hyv¨arinen, 2005), noise contrastive estimation (NCE) (Gutmann & Hyv¨arinen, 2010), and condi-
tional NCE (CNCE) (Ceylan & Gutmann, 2018), which replace the log-likelihood loss with objec-
tives that do not require the computation of the normalization constant or its gradient.
Perhaps the most popular method for learning unnormalized models is contrastive divergence (CD)
(Hinton, 2002). CD’s advantage over MLE stems from its use of short Markov chains initialized at
the data samples. CD has been successfully used in a wide range of domains, including modeling
images (Hinton et al., 2006), speech (Mohamed & Hinton, 2010), documents (Hinton & Salakhut-
dinov, 2009), and movie ratings (Salakhutdinov et al., 2007), and is continuing to attract signiﬁcant
research attention (Liu & Wang, 2017; Gao et al., 2018; Qiu et al., 2019).
1

Published as a conference paper at ICLR 2021
i) Generator update step
ii) Discriminator update step
Discriminator
Original order
Reversed
Dataset
sample
MCMC
(generator)
Learned model 
Figure 1: Contrastive divergence as an adversarial process. In the ﬁrst step, the distribution model is
used to generate an MCMC process which is used to generate a chain of samples. In the second step
the distribution model is updated using a gradient descent step, using the MCMC transition rule.
Despite CD’s popularity and empirical success, there still remain open questions regarding its theo-
retical properties. The primary source of difﬁculty is an unjustiﬁed approximation used to derive its
objective’s gradient, which biases its update steps (Carreira-Perpinan & Hinton, 2005; Bengio & De-
lalleau, 2009). The difﬁculty is exacerbated by the fact that CD’s update steps cannot be expressed
as the gradients of any ﬁxed objective (Tieleman, 2007; Sutskever & Tieleman, 2010).
In this paper, we present an alternative derivation of CD, which relies on completely different princi-
ples and requires no approximations. Speciﬁcally, we show that CD’s update steps are the gradients
of an adversarial game in which a discriminator attempts to classify whether a Markov chain gen-
erated from the model is presented to it in its original or a time-reversed order (see Fig. 1). Thus,
our derivation sheds new light on CD’s success: Similarly to modern generative adversarial meth-
ods (Goodfellow et al., 2014), CD’s discrimination task becomes more challenging as the model
approaches the true distribution. This keeps the update steps effective throughout the entire train-
ing process and prevents early saturation as often happens in non-adaptive methods like NCE and
CNCE. In fact, we derive CD as a natural extension of the CNCE method, replacing the ﬁxed distri-
bution of the contrastive examples with an adversarial adaptive distribution.
CD requires that the underlying MCMC be exact, which is not the case for popular methods like
Langevin dynamics. This commonly requires using Metropolis-Hastings (MH) rejection, which
ignores some of the generated samples. Interestingly, our derivation reveals an alternative correction
method for inexact chains, which does not require rejection.
2
BACKGROUND
2.1
THE CLASSICAL DERIVATION OF CD
Assume we have an unnormalized distribution model pθ. Given a dataset of samples {xi} inde-
pendently drawn from some unknown distribution p, CD attempts to determine the parameters θ
with which pθ best explains the dataset. Rather than using the log-likelihood loss, CD’s objective
involves distributions of samples along ﬁnite Markov chains initialized at {xi}. When based on
chains of length k, the algorithm is usually referred to as CD-k.
Concretely, let qθ(x′|x) denote the transition rule of a Markov chain with stationary distribution pθ,
and let rm
θ denote the distribution of samples after m steps of the chain. As the Markov chain is
initialized from the dataset distribution and converges to pθ, we have that r0
θ = p and r∞
θ = pθ. The
CD algorithm then attempts to minimize the loss
ℓCD-k = DKL(r0
θ||r∞
θ ) −DKL(rk
θ||r∞
θ )
= DKL(p||pθ) −DKL(rk
θ||pθ),
(1)
where DKL is the Kullback-Leibler divergence. Under mild conditions on qθ (Cover & Halliwell,
1994) this loss is guaranteed to be positive, and it vanishes when pθ = p (in which case rk
θ = pθ).
2

Published as a conference paper at ICLR 2021
To allow the minimization of (1) using gradient-based methods, one can write
∇θℓCD-k =E ˜
X∼rk
θ [∇θ log pθ( ˜X)] −EX∼p[∇θ log pθ(X)] + dDKL(rk
θ||pθ)
drk
θ
∇θrk
θ.
(2)
Here, the ﬁrst two terms can be approximated using two batches of samples, one drawn from p and
one from rk
θ. The third term is the derivative of the loss with respect only to the θ that appears in rk
θ,
ignoring the dependence of pθ on θ. This is the original notation from (Hinton, 2002); an alternative
way to write this term would be ∇˜θDKL(rk
˜θ||pθ). This term turns out to be intractable and in the
original derivation, it is argued to be small and thus neglected, leading to the approximation
∇θℓCD-k ≈1
n
X
i
(∇θ log pθ(˜xi) −∇θ log pθ(xi))
(3)
Here {xi} is a batch of n samples from the dataset and {˜xi} are n samples generated by applying k
MCMC steps to each of the samples in that batch. The intuition behind the resulting algorithm (sum-
marized in App. A) is therefore simple. In each gradient step θ ←θ −η∇θℓCD-k, the log-likelihood
of samples from the dataset is increased on the expense of the log-likelihood of the contrastive
samples {˜xi}, which are closer to the current learned distribution pθ.
Despite the simple intuition, it has been shown that without the third term, CD’s update rule (2)
generally cannot be the gradient of any ﬁxed objective (Tieleman, 2007; Sutskever & Tieleman,
2010) except for some very speciﬁc cases. For example, Hyv¨arinen (2007) has shown that when the
Markov chain is based on Langevin dynamics with a step size that approaches zero, the update rule
of CD-1 coincides with that of score-matching Hyv¨arinen (2005). Similarly, the probability ﬂow
method of Sohl-Dickstein et al. (2011) has been shown to be equivalent to CD with a very unique
Markov chain. Here, we show that regardless of the selection of the Markov chain, the update rule
is in fact the exact gradient of a particular adversarial objective, which adapts to the current learned
model in each step.
2.2
CONDITIONAL NOISE CONTRASTIVE ESTIMATION
Our derivation views CD as an extension of the CNCE method, which itself is an extension of NCE.
We therefore start by brieﬂy reviewing those two methods.
In NCE, the unsupervised density learning problem is transformed into a supervised one. This is
done by training a discriminator Dθ(x) to distinguish between samples drawn from p and samples
drawn from some preselected contrastive distribution pref. Speciﬁcally, let the random variable Y
denote the label of the class from which the variable X has been drawn, so that X|(Y = 1) ∼p and
X|(Y = 0) ∼pref. Then it is well known that the discriminator minimizing the binary cross-entropy
(BCE) loss is given by
Dopt(x) = P(Y = 1|X = x) =
p(x)
p(x) + pref(x).
(4)
Therefore, letting our parametric discriminator have the form
Dθ(x) =
pθ(x)
pθ(x) + pref(x),
(5)
and training it with the BCE loss, should in theory lead to Dθ(x) = Dopt(x) and thus to pθ(x) =
p(x). In practice, however, the convergence of NCE highly depends on the selection of pref. If
it signiﬁcantly deviates from p, then the two distributions can be easily discriminated even when
the learned distribution pθ is still very far from p. At this point, the optimization essentially stops
updating the model, which can result in a very inaccurate estimate for p. In the next section we
provide a precise mathematical explanation for this behavior.
The CNCE method attempts to alleviate this problem by drawing the contrastive samples based on
the dataset samples. Speciﬁcally, each dataset sample x is paired with a contrastive sample ˜x that is
drawn conditioned on x from some predetermined conditional distribution q(˜x|x) (e.g. N(x, σ2I)).
The pair is then concatenated in a random order, and a discriminator is trained to predict the correct
3

Published as a conference paper at ICLR 2021
 - ﬁxed
(a) CNCE
 - Based on MCMC
(b) CD-1
Figure 2: From CNCE to CD-1. (a) In CNCE, each contrastive sample is generated using a ﬁxed
conditional distribution q(·|·) (which usually corresponds to additive noise). The real and fake sam-
ples are then concatenated and presented to a discriminator in a random order, which is trained to
predict the correct order. (b) CD-1 can be viewed as CNCE with a q(·|·) that corresponds to the
transition rule of a Markov chain with stationary distribution pθ. Since q depends on pθ (hence the
subscript θ), during training the distribution of contrastive samples becomes more similar to that of
the real samples, making the discrimination task harder.
order. This is illustrated in Fig. 2a. Speciﬁcally, here the two classes are of pairs (A, B) corre-
sponding to (A, B) = (X, ˜X) for Y = 1, and (A, B) = ( ˜X, X) for Y = 0, and the discriminator
minimizing the BCE loss is given by
Dopt(a, b) = P(Y = 1|A = a, B = b) =
q(b|a)p(a)
q(b|a)p(a) + q(a|b)p(b).
(6)
Therefore, constructing a parametric discriminator of the form
Dθ(a, b) =
q(b|a)pθ(a)
q(b|a)pθ(a) + q(a|b)pθ(b) =

1 + q(a|b)pθ(b)
q(b|a)pθ(a)
−1
,
(7)
and training it with the BCE loss, should lead to pθ ∝p. Note that here Dθ is indifferent to a scaling
of pθ, which is thus determined only up to an arbitrary multiplicative constant.
CNCE improves upon NCE, as it allows working with contrastive samples whose distribution is
closer to p. However, it does not completely eliminate the problem, especially when p exhibits
different scales of variation in different directions. This is the case, for example, with natural images,
which are known to lie close to a low-dimensional manifold. Indeed if the conditional distribution
q(·|·) is chosen to have a small variance, then CNCE fails to capture the global structure of p. And
if q(·|·) is taken to have a large variance, then CNCE fails to capture the intricate features of p
(see Fig. 3). The latter case can be easily understood in the context of images (see Fig. 2a). Here,
the discriminator can easily distinguish which of its pair of input images is the noisy one, without
having learned an accurate model for the distribution of natural images (e.g., simply by comparing
their smoothness). When this point is reached, the optimization essentially stops.
In the next section we show that CD is in fact an adaptive version of CNCE, in which the contrastive
distribution is constantly updated in order to keep the discrimination task hard. This explains why
CD is less prone to early saturation than NCE and CNCE.
3
AN ALTERNATIVE DERIVATION OF CD
We now present our alternative derivation of CD. In Sec. 3.1 we identify a decomposition of the
CNCE loss, which reveals the term that is responsible for early saturation. In Sec. 3.2, we then
present a method for adapting the contrastive distribution in a way that provably keeps this term
bounded away from zero. Surprisingly, the resulting update step turns out to precisely match that
of CD-1, thus providing a new perspective on CD learning. In Sec. 3.3, we extend our derivation to
include CD-k (with k ≥1).
4

Published as a conference paper at ICLR 2021
3D view (x1­x2­x3)
The x1­x2 plane
The x1­x3 plane
(a) The toy model
0
50000
10 13
10 10
10 7
10 4
10 1
Weights (
)
CNCE large
First iteration
Last iteration
Learned model
0
50000
10 13
10 10
10 7
10 4
10 1
CNCE small
0
50000
10 13
10 10
10 7
10 4
10 1
Median
CD
GT density
Samples from data
Contrastive samples
Learned density
Samples drawn
from the model
(b) Comparing CNCE with CD
Figure 3: A toy example illustrating the importance of the adversarial nature of CD. Here, the
data lies close to a 2D spiral embedded in a 10-dimensional space. (a) The training samples in
the ﬁrst 3 dimensions. (b) Three different approaches for learning the distribution: CNCE with
large contrastive variance (top), CNCE with small contrastive variance (middle), and CD based on
Langevin dynamics MCMC with the weight adjustment described in Sec. 3.4 (bottom). As can be
seen in the ﬁrst two columns, CD adapts the contrastive samples according to the data distribution,
whereas CNCE does not. Therefore, CNCE with large variance fails to learn the distribution because
the vast majority of its contrastive samples are far from the manifold and quickly become irrelevant
(as indicated by the weights αθ in the third column). And CNCE with small variance fails to learn
the global structure of the distribution because its contrastive samples are extremely close to the
dataset samples. CD, on the other hand, adjusts the contrastive distribution during training, so as to
generate samples that are close to the manifold yet traverse large distances along it.
3.1
REINTERPRETING CNCE
Let us denote
wθ(a, b) ≜q(a|b)pθ(b)
q(b|a)pθ(a),
(8)
so that we can write CNCE’s discriminator (7) as
Dθ(a, b) = (1 + wθ(a, b))−1 .
(9)
Then we have the following observation (see proof in App. B).
Observation 1. The gradient of the CNCE loss can be expressed as
∇θℓCNCE = E X∼p
˜
X|X∼q
h
αθ(X, ˜X)

∇θ log pθ( ˜X) −∇θ log pθ(X)
i
,
(10)
where
αθ(x, ˜x) ≜(1 + wθ(x, ˜x)−1)−1.
(11)
5

Published as a conference paper at ICLR 2021
Note that (10) is similar in nature to the (approximate) gradient of the CD loss (3). Particularly, as in
CD, the term ∇θ log pθ( ˜X) −∇θ log pθ(X) causes each gradient step to increase the log-likelihood
of samples from the dataset on the expense of the log-likelihood of the contrastive samples. How-
ever, as opposed to CD, here we also have the coefﬁcient αθ(x, ˜x), which assigns a weight between
0 and 1 to each pair of samples (x, ˜x). To understand its effect, observe that
αθ(x, ˜x) = 1 −Dθ(x, ˜x) = Dθ(˜x, x).
(12)
Namely, this coefﬁcient is precisely the probability that the discriminator assigns to the incorrect
order of the pair. Therefore, this term gives a low weight to “easy” pairs (i.e., for which Dθ(x, ˜x) is
close to 1) and a high weight to “hard” ones.
This weighting coefﬁcient is of course essential for ensuring convergence to p. For example, it
prevents log pθ from diverging to ±∞when the discriminator is presented with the same samples
over and over again. The problem is that a discriminator can often correctly discriminate all training
pairs, even with a pθ that is still far from p. In such cases, αθ becomes practically zero for all pairs
and the model stops updating. This shows that a good contrastive distribution is one which keeps
the discrimination task hard throughout the training. As we show next, there is a particular choice
which provably prevents αθ from converging to zero, and that choice results in the CD method.
3.2
FROM CNCE TO CD-1
To bound αθ away from 0, and thus avoid the early stopping of the training process, we now extend
the original CNCE algorithm by allowing the conditional distribution q to depend on pθ (and thus
to change from one step to the next). Our next key observation is that in this setting there exists a
particular choice that keeps αθ constant.
Observation 2. If q is chosen to be the transition probability of a reversible Markov chain with
stationary distribution pθ, then
αθ(x, ˜x) = 1
2,
∀x, ˜x.
(13)
Proof. A reversible chain with transition q and stationary distribution pθ, satisﬁes the detailed bal-
ance property
q(˜x|x)pθ(x) = q(x|˜x)pθ(˜x),
∀x, ˜x.
(14)
Substituting (14) into (8) leads to wθ(x, ˜x) = 1, which from (11) implies αθ(x, ˜x) = 1
2.
This observation directly links CNCE to CD. First, the suggested method for generating the con-
trastive samples is precisely the one used in CD-1. Second, as this choice of q leads to αθ(x, ˜x) = 1
2,
it causes the gradient of the CNCE loss (10) to become
∇θℓCNCE = 1
2E X∼p
˜
X|X∼q
h
∇θ log pθ( ˜X) −∇θ log pθ(X)
i
,
(15)
which is exactly proportional to the CD-1 update (3). We have thus obtained an alternative derivation
of CD-1. Namely, rather than viewing CD-1 learning as an approximate gradient descent process
for the loss (1), we can view each step as the exact gradient of the CNCE discrimination loss, where
the reference distribution q is adapted to the current learned model pθ. This is illustrated in Fig. 2b.
Since q is chosen based on pθ, the overall process is in fact an adversarial game. Namely, the opti-
mization alternates between updating q, which acts as a generator, and updating pθ, which deﬁnes
the discriminator. As pθ approaches p, the distribution of samples generated from the MCMC also
becomes closer to p, which makes the discriminator’s task harder and thus prevents early saturation.
It should be noted that formally, since q depends on pθ, it also indirectly depends on θ, so that a
more appropriate notation would be qθ. However, during the update of pθ we ﬁx qθ (and vise versa),
so that the gradient in the discriminator update does not consider the dependence of qθ on θ. This is
why (15) does not involve the gradient of ˜X which depends on qθ.
The reason for ﬁxing qθ comes from the adversarial nature of the learning process. Being part of the
chain generation process, the goal of the transition rule qθ is to generate chains that appear to be time-
reversible, while the goal of the classiﬁer, which is based on the model pθ, is to correctly classify
6

Published as a conference paper at ICLR 2021
whether the chains were reversed. Therefore, we do not want the optimization of the classiﬁer to
affect qθ. This is just like in GANs, where the generator and discriminator have different objectives,
and so when updating the discriminator the generator is kept ﬁxed.
3.3
FROM CD-1 TO CD-k
To extend our derivation to CD-k with an arbitrary k ≥1, let us now view the discrimination
problem of the previous section as a special case of a more general setting. Speciﬁcally, the pairs
of samples presented to the discriminator in Sec. 3.2, can be viewed as Markov chains of length
two (comprising the initial sample from the dataset and one extra generated sample). It is therefore
natural to consider also Markov chains of arbitrary lengths. That is, assume we initialize the MCMC
at a sample xi from the dataset and run it for k steps to obtain a sequence (x(0), x(1), . . . , x(k)),
where x(0) = xi. We can then present this sequence to a discriminator either in its original order, or
time-reversed, and train the discriminator to classify the correct order. We coin this a time-reversal
classiﬁcation task. Interestingly, in this setting, we have the following.
Observation 3. When using a reversible Markov chain of length k + 1 with stationary distribution
pθ, the gradient of the BCE loss of the time-reversal classiﬁcation task is given by
∇θℓCNCE = 1
2E
h
∇θ log pθ(X(k)) −∇θ log pθ(X(0))
i
,
(16)
which is exactly identical to the CD-k update (3) up to a multiplicative factor of 1
2.
This constitutes an alternative interpretation of CD-k. That is, CD-k can be viewed as a time-reversal
adversarial game, where in each step, the model pθ is updated so as to allow the discriminator to
better distinguish MCMC chains from their time-reversed counterparts.
Two remarks are in order. First, it is interesting to note that although the discriminator’s task is to
classify the order of the whole chain, its optimal strategy is to examine only the endpoints of the
chain, x(0) and x(k). Second, it is insightful to recall that the original motivation behind the CD-k
loss (1) was that when pθ equals p, the marginal probability of each individual step in the chain is
also p. Our derivation, however, requires more than that. To make the chain indistinguishable from
its time-reversed version, the joint probability of all samples in the chain must be invariant to a ﬂip
of the order. When pθ = p, this is indeed the case, due to the detailed balance property (14).
Proof of Observation 3. We provide the outline of the proof (see full derivation in App. C). Let
(A(0), A(1), . . . , A(k)) denote the input to the discriminator and let Y indicate the order of the chain,
with Y = 1 corresponding to (A(0), A(1), . . . , A(k)) = (X(0), X(1), . . . , X(k)) and Y = 0 to
(A(0), A(1), . . . , A(k)) = (X(k), X(k−1), . . . , X(0)). The discriminator that minimizes the BCE
loss is now given by
D(a0, a1, . . . , ak) = P(Y = 1|A(0) = a0, A(1) = a1, . . . , A(k) = ak)
=

1 + q(a0|a1) · · · q(ak−1|ak)p(ak)
q(ak|ak−1) · · · q(a1|a0)p(a0)
−1
=
 
1 +
k
Y
i=1
wθ(ai−1, ai)
!−1
.
(17)
The CNCE paradigm thus deﬁnes a discriminator Dθ having the form of (17) but with p replaced
by pθ. Recall that despite the dependence of the transition probability q on the current learned
model pθ, it is regarded as ﬁxed within each discriminator update step. We therefore omit the
subscript θ from q here. Similarly to the derivation of (10), explicitly writing the gradient of the
BCE loss of our discriminatrion task, gives
∇θℓchain = E


 
1 +
k
Y
i=1
wθ(X(i−1), X(i))−1
!−1 
∇θ log pθ(X(k)) −∇θ log pθ(X(0))



= E
h
αθ(X(0), . . . , X(k))

∇θ log pθ(X(k)) −∇θ log pθ(X(0))
i
.
(18)
7

Published as a conference paper at ICLR 2021
GT
CD
CD+MH
ACD
Learned density
MCMC samples
Figure 4: Here, we use different CD conﬁgurations for learning the model of Fig. 3. All conﬁgu-
rations use Langevin dynamics as their MCMC process, but with different ways of compensating
for the lack of detailed balance. From left to right we have the ground-truth density, CD w/o any
correction, CD with Metropolis-Hastings rejection, and CD with our proposed adjustment.
where we now deﬁned
αθ(a0, . . . , ak) ≜
 
1 +
k
Y
i=1
wθ(ai−1, ai)−1
!−1
.
(19)
Note that (10) is a special case of (18) corresponding to k = 1, where X and ˜X in (10) are X(0) and
X(1) in (18). As before, when q satisﬁes the detailed balance property (14), we obtain wθ = 1 and
consequently the weighting term αθ again equals 1
2. Thus, the gradient (18) reduces to (16), which
is exactly proportional to the CD-k update (3).
3.4
MCMC PROCESSES THAT DO NOT HAVE DETAILED BALANCE
In our derivation, we assumed that the MCMC process is reversible, and thus exactly satisﬁes the
detailed balance property (14). This assumption ensured that wθ = 1 and thus αθ = 1
2. In practice,
however, commonly used MCMC methods satisfy this property only approximately. For example,
the popular discrete Langevin dynamics process obeys detailed balance only in the limit where the
step size approaches zero. The common approach to overcome this is through Metropolis-Hastings
(MH) rejection (Hastings, 1970), which guarantees detailed balance by accepting only a portion of
the proposed MCMC transitions. In this approach, the probability of accepting a transition from x
to ˜x is closely related to the weighing term wθ, and is given by
A(x, ˜x) = min (1, wθ(x, ˜x)) .
(20)
Interestingly, our derivation reveals an alternative method for accounting for lack of detailed balance.
Concretely, we saw that the general expression for the gradient of the BCE loss (before assuming
detailed balance) is given by (18). This expression differs from the original update step of CD-k only
in the weighting term αθ(x(0), . . . , x(k)). Therefore, all that is required for maintaining correctness
in the absence of detailed balance, is to weigh each chain by its “hardness” αθ(x(0), . . . , x(k)) (see
Alg. 2 in App. A). Note that in this case, the update depends not only on the end-points of the chains,
but rather also on their intermediate steps. As can be seen in Fig. 4, this method performs just as
well as MH, and signiﬁcantly better than vanilla CD without correction.
4
ILLUSTRATION THROUGH A TOY EXAMPLE
To illustrate our observations, we now conclude with a simple toy example (see Fig. 3). Our goal
here is not to draw general conclusions regarding the performance of CNCE and CD, but rather
merely to highlight the adversarial nature of CD and its importance when the data density exhibits
different scales of variation along different directions.
We take data concentrated around a 2-dimensional manifold embedded in 10-dimensional space.
Speciﬁcally, let e(1), . . . , e(10) denote the standard basis in R10. Then each data sample is generated
by adding Gaussian noise to a random point along a 2D spiral lying in the e(1)-e(2) plane. The STD
of the noise in the e(1) and e(2) directions is 5 times larger than that in the other 8 axes. Figure
3a shows the projections of the the data samples onto the the ﬁrst 3 dimensions. Here, we use a
8

Published as a conference paper at ICLR 2021
multi-layer perceptron (MLP) as our parametric model, log pθ, and train it using several different
learning conﬁgurations (for the full details see App. D).
Figure 3b visualizes the training as well as the ﬁnal result achieved by each conﬁguration. The ﬁrst
two rows show CNCE with Gaussian contrastive distributions of two different STDs. The third row
shows the adjusted CD described in Sec. 3.4 with Langevin Dynamics as its MCMC process. As
can be seen, for CNCE with a large STD, the contrastive samples are able to explore large areas
around the original samples, but this causes their majority to lie relatively far from the manifold (see
their projections onto the e(1)-e(3) plane). In this case, αθ decreases quickly, causing the learning
process to ignore most samples at a very early stage of the training. When using CNCE with a small
STD, the samples remain relevant throughout the training, but this comes at the price of inability to
capture the global structure of the distribution. CD, on the other hand, is able to enjoy the best of
both worlds as it adapts the contrastive distribution over time. Indeed, as the learning progresses,
the contrastive samples move closer to the manifold to maintain their relevance. Note that since we
use the adjusted version of CD, the weights in this conﬁguration are not precisely 1. We chose the
step size of the Langevin Dynamics so that the median of the weights is approximately 10−2.
Figure 4 shows the results achieved by different variants of CD. As can be seen, without correcting
for the lack of detailed balance, CD fails to estimate the density correctly. When using MH rejec-
tion to correct the MCMC, or our adaptive CD (ADC) to correct the update steps, the estimate is
signiﬁcantly improved.
5
CONCLUSION
The classical CD method has seen many uses and theoretical analyses over the years. The original
derivation presented the algorithm as an approximate gradient descent process for a certain loss.
However, the accuracy of the approximation has been a matter of much dispute, leaving it unclear
what objective the algorithm minimizes in practice. Here, we presented an alternative derivation of
CD’s update steps, which involves no approximations. Our analysis shows that CD is in essence
an adversarial learning procedure, where a discriminator is trained to distinguish whether a Markov
chain generated from the learned model has been time-ﬂipped or not. Therefore, although predating
GANs by more than a decade, CD in fact belongs to the same family of techniques. This provides a
possible explanation for its empirical success.
Acknowledgement
This research was supported by the Technion Ollendorff Minerva Center.
REFERENCES
Yoshua Bengio and Olivier Delalleau. Justifying and generalizing contrastive divergence. Neural
Computation, 21(6):1601–1621, 2009.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Society
for Artiﬁcial Intelligence and Statistics, volume 10, pp. 33–40, 2005.
Ciwan Ceylan and Michael U Gutmann. Conditional noise-contrastive estimation of unnormalised
models. In International Conference on Machine Learning, pp. 726–734, 2018.
Thomas M Cover and J Halliwell. Which processes satisfy the second law. Physical Origins of Time
Asymmetry, pp. 98–107, 1994.
Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. In
Advances in Neural Information Processing Systems, volume 32, pp. 3608–3618, 2019.
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative con-
vnets via multi-grid modeling and sampling. In Conference on Computer Vision and Pattern
Recognition, pp. 9155–9164, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672–2680, 2014.
9

Published as a conference paper at ICLR 2021
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should treat it like
one. In International Conference on Learning Representations, 2019.
Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Society for Artiﬁcial Intelligence and Statistics, pp. 297–
304, 2010.
W Keith Hastings. Monte Carlo sampling methods using markov chains and their applications.
Biometrika, 57(1):97–109, 1970.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14(8):1771–1800, 2002.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. Science, 313(5786):504–507, 2006.
Geoffrey E Hinton and Russ R Salakhutdinov. Replicated softmax: an undirected topic model. In
Advances in Neural Information Processing Systems, pp. 1607–1614, 2009.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527–1554, 2006.
Aapo Hyv¨arinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(Apr):695–709, 2005.
Aapo Hyv¨arinen. Connections between score matching, contrastive divergence, and pseudolikeli-
hood for continuous-valued variables. IEEE Transactions on Neural Networks, 18(5):1529–1531,
2007.
Qiang Liu and Dilin Wang. Learning deep energy models: Contrastive divergence vs. amortized
mle. arXiv preprint arXiv:1707.00797, 2017.
Abdel-rahman Mohamed and Geoffrey Hinton. Phone recognition using restricted boltzmann ma-
chines. In International Conference on Acoustics, Speech and Signal Processing, pp. 4354–4357.
IEEE, 2010.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. In Advances in Neural Information Pro-
cessing Systems, pp. 5232–5242, 2019.
Yixuan Qiu, Lingsong Zhang, and Xiao Wang. Unbiased contrastive divergence algorithm for train-
ing energy-based latent variable models. In International Conference on Learning Representa-
tions, 2019.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep Boltzmann machines. In Artiﬁcial Intelligence
and Statistics, pp. 448–455, 2009.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. Restricted Boltzmann machines for
collaborative ﬁltering. In International Conference on Machine Learning, pp. 791–798, 2007.
Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory.
Technical report, Colorado Univ at Boulder Dept of Computer Science, 1986.
Jascha Sohl-Dickstein, Peter Battaglino, and Michael R DeWeese. Minimum probability ﬂow learn-
ing. In ICML, 2011.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918–11930, 2019.
Ilya Sutskever and Tijmen Tieleman. On the convergence properties of contrastive divergence. In
International Conference on Artiﬁcial Intelligence and Statistics, pp. 789–795, 2010.
Tijmen Tieleman. Some investigations into energy-based models. PhD thesis, University of Toronto,
2007.
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
International Conference on Machine Learning, pp. 2635–2644, 2016.
10

Published as a conference paper at ICLR 2021
A
ALGORITHMS
Below we summarize the algorithms of the classical CD and the proposed adjusted version described
in Sec. 3.4.
Algorithm 1: Contrastive Divergence - k
Require: parametric model pθ, MCMC transition rule qθ(·|·) with stationary distribution pθ,
step size η, chain length k.
while not converged do
Sample a batch {xi}n
i=1 from the dataset
Initialize {˜xi}n
i=1 to be a copy of the batch
for i=1 to n do
for j=1 to k do
Draw a sample x′ from qθ(·|˜xi)
˜xi ←x′
end
gi ←∇θ log pθ(˜xi) −∇θ log pθ(xi)
end
θ ←θ −η 1
n
P
i gi
end
Algorithm 2: Adjusted Contrastive Divergence - k
Require: parametric model pθ, MCMC transition rule qθ(·|·) whose stationary distribution is
pθ, step size η, chain length k.
while not converged do
Sample a batch {xi}n
i=1 from the dataset
Initialize {˜xi}n
i=1 to be a copy of the batch
for i=1 to n do
wtot
i
←1
for j=1 to k do
Draw a sample x′ from qθ(·|˜xi)
wtot
i
←wtot
i · q(xi|x′)pθ(x′)
q(x′|xi)pθ(xi)
˜xi ←x′
end
αi ←(1 + 1/wtot
i )−1
gi ←∇θ log pθ(˜xi) −∇θ log pθ(xi)
end
θ ←θ −η 1
n
P
i αi · gi
end
B
DERIVATION OF CNCE’S GRADIENT
Proof of Observation 1. The BCE loss achieved by the CNCE discriminator (7) is given by
ℓCNCE = −1
2E A∼p
B|A∼q
[log(Dθ(A, B))] −1
2E B∼p
A|B∼q
[log(1 −Dθ(A, B))] =
= −E X∼p
˜
X|X∼q
h
log(Dθ(X, ˜X))
i
,
(21)
11

Published as a conference paper at ICLR 2021
where we used the fact that 1 −Dθ(a, b) = Dθ(b, a). Now, substituting the deﬁnition of Dθ form
(9), the gradient of (21) can be expressed as
∇θℓCNCE = E X∼p
˜
X|X∼q
h
∇θ log

1 + wθ(X, ˜X)
i
= E X∼p
˜
X|X∼q

1 + wθ(X, ˜X)
−1
∇θwθ(X, ˜X)

= E X∼p
˜
X|X∼q
"
1 + wθ(X, ˜X)
−1 wθ(X, ˜X)
wθ(X, ˜X)
∇θwθ(X, ˜X)
#
= E X∼p
˜
X|X∼q


 
1 + wθ(X, ˜X)
wθ(X, ˜X)
!−1
∇θwθ(X, ˜X)
wθ(X, ˜X)


= E X∼p
˜
X|X∼q

1 + wθ(X, ˜X)−1−1
∇θ log(wθ(X, ˜X))

= E X∼p
˜
X|X∼q
h
αθ(X, ˜X)

∇θ log pθ( ˜X) −∇θ log pθ(X)
i
,
(22)
where we used the fact that ∇θwθ = wθ∇θ log(wθ) and the deﬁnition of αθ from (11).
C
DERIVATION OF THE GRADIENT OF CNCE WITH MULTIPLE MC STEPS
We here describe the full derivation of the gradient in (18) following the same steps as in (22). The
BCE loss achieved by the discriminator in (17) is given by
ℓchain = −E
h
log(Dθ(X(0), X(1), . . . , X(k)))
i
.
(23)
where we again used the fact that 1−Dθ(a0, a1, . . . , ak) = Dθ(ak, ak−1, . . . , a0). Now, substituting
the deﬁnition of Dθ form (17), the gradient of (23) can be expressed as
∇θℓchain = E
"
∇θ log
 
1 +
k
Y
i=1
wθ(X(i−1), X(i))
!#
= E


 
1 +
k
Y
i=1
wθ(X(i−1), X(i))
!−1
∇θ
 k
Y
i=1
wθ(X(i−1), X(i))
!

= E


 
1 + Qk
i=1 wθ(X(i−1), X(i))
Qk
i=1 wθ(X(i−1), X(i))
!−1 ∇θ
Qk
i=1 wθ(X(i−1), X(i))

Qk
i=1 wθ(X(i−1), X(i))


= E


 
1 +
k
Y
i=1
wθ(X(i−1), X(i))−1
!−1
∇θ log
 k
Y
i=1
wθ(X(i−1), X(i))
!

= E
h
αθ(X(0), . . . , X(k))

∇θ log pθ(X(k)) −∇θ log pθ(X(0))
i
,
(24)
where we used the deﬁnition of αθ from (19).
D
TOY EXPERIMENT AND TRAINING DETAILS
We here describe the full details of the toy model and learning conﬁguration, which we used to
produce the results in the paper. The code for reproducing the results is available at —- (for the
blind review the code will be available in the supplementary material).
The toy model used in the paper consists of a distribution concentrated around a 2D spiral embedded
in a 10 dimensional space. Denoting the 10 orthogonal axes of the standard basis in this space by
12

Published as a conference paper at ICLR 2021
+
+
+
FC 10 x 512
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
FC 512 x 512
Leaky ReLU
Figure 5: The architecture.
e(1), . . . , e(10), the spiral lies in the e(1)-e(2) plane and is conﬁned to [−1, 1] in each of these two
axes. The samples of the model are produced by selecting random points along the spiral and adding
Gaussian noise to them. In order to keep the samples close to the e(1)-e(2) plane we used a non-
isotropic noise with an STD of 0.05 in the e(1) and e(2) directions, and an STD of 0.01 in the
directions e(3), . . . , e(10).
As a parametric model for log pθ(x), we used an 8-layer multi-layer perceptron (MLP) of width 512
with skip connections, as illustrated in Fig. 5.
Throughout the paper we referred to the results of ﬁve different learning conﬁgurations.
1. CNCE with an optimal (small) variance. This conﬁguration uses additive Gaussian noise
as its contrastive distribution. We found 0.0075 to be the STD of the Gaussian which
produces the best results.
2. CNCE with a large variance. This conﬁguration is similar to the previous one except for
the STD of the Gaussian which was set to 0.3 in order to illustrate the problems of using a
conditional distribution with a large variance.
3. CD without any MCMC correction. For the MCMC process we used 5 steps of Langevin
dynamics, where we did not employ any correction for the inaccuracy which results from
using Langavin dynamics with a ﬁnite step size. We found 0.0075 to be the step size
(multiplying the standard Gaussian noise term) which produces the best results.
4. CD with MH correction. This conﬁguration is similar to the previous one except for a
MH rejection scheme which was used during the MCMC sampling. In this case we found
the step size of 0.0125 to produce the best results.
5. Adjusted CD. This conﬁguration is similar to the previous one except that we used the
method from Sec. 3.4 instead of MH rejection. Similarly to the previous conﬁguration, we
found the step size of 0.0125 to produce the best results.
The optimization of all conﬁgurations was preformed using SGD with a momentum of 0.9 and an
exponential decaying learning rate. Except for the training of the third conﬁguration, the learning
rate ran down from 10−2 to 10−4 over 100000 optimization steps. For the third conﬁguration we
had to reduce the learning rate by a factor of 10 in order to prevent the optimization from diverging.
In order to select the best step size / variance for each of the conﬁgurations we ran a parameter sweep
around the relevant value range. The results of this sweep are shown in Fig. 6.
For the selection of the number of training steps, we have iteratively increased the number of steps
until the results stopped improving for all conﬁgurations. These results are presented in Fig. 7.
13

Published as a conference paper at ICLR 2021
CNCE
Step size = 0.005
CD w/o correction
CD + MH
ACD
Step size = 0.0075
Step size = 0.01
Step size = 0.0125
Step size = 0.015
Step size = 0.0175
Figure 6: The step size sweep. In the case of CNCE, the step size is in fact the STD of the conditional
distribution. For the case of CD, the training has diverged for large step sizes even after the learning
rate has been signiﬁcantly reduced. The highlighted ﬁgures indicate the selected step sizes.
14

Published as a conference paper at ICLR 2021
CNCE
# of Steps = 10000
CD w/o correction
CD + MH
ACD
# of Steps = 20000
# of Steps = 50000
# of Steps = 100000
# of Steps = 200000
Figure 7: Selecting the number of training steps.
15

