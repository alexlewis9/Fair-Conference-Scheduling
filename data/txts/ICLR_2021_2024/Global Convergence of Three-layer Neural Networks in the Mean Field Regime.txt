Published as a conference paper at ICLR 2021
GLOBAL CONVERGENCE OF THREE-LAYER NEURAL
NETWORKS IN THE MEAN FIELD REGIME∗
Huy Tuan Pham† and Phan-Minh Nguyen‡ §
ABSTRACT
In the mean ﬁeld regime, neural networks are appropriately scaled so that as the
width tends to inﬁnity, the learning dynamics tends to a nonlinear and nontriv-
ial dynamical limit, known as the mean ﬁeld limit. This lends a way to study
large-width neural networks via analyzing the mean ﬁeld limit. Recent works have
successfully applied such analysis to two-layer networks and provided global con-
vergence guarantees. The extension to multilayer ones however has been a highly
challenging puzzle, and little is known about the optimization efﬁciency in the
mean ﬁeld regime when there are more than two layers.
In this work, we prove a global convergence result for unregularized feedforward
three-layer networks in the mean ﬁeld regime. We ﬁrst develop a rigorous frame-
work to establish the mean ﬁeld limit of three-layer networks under stochastic gra-
dient descent training. To that end, we propose the idea of a neuronal embedding,
which comprises of a ﬁxed probability space that encapsulates neural networks of
arbitrary sizes. The identiﬁed mean ﬁeld limit is then used to prove a global con-
vergence guarantee under suitable regularity and convergence mode assumptions,
which – unlike previous works on two-layer networks – does not rely critically
on convexity. Underlying the result is a universal approximation property, natural
of neural networks, which importantly is shown to hold at any ﬁnite training time
(not necessarily at convergence) via an algebraic topology argument.
1
INTRODUCTION
Interests in the theoretical understanding of the training of neural networks have led to the recent
discovery of a new operating regime: the neural network and its learning rates are scaled appro-
priately, such that as the width tends to inﬁnity, the network admits a limiting learning dynamics in
which all parameters evolve nonlinearly with time1. This is known as the mean ﬁeld (MF) limit (Mei
et al. (2018); Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos
(2018); Nguyen (2019); Araújo et al. (2019); Sirignano & Spiliopoulos (2019)). The four works Mei
et al. (2018); Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos
(2018) led the ﬁrst wave of efforts in 2018 and analyzed two-layer neural networks. They estab-
lished a connection between the network under training and its MF limit. They then used the MF
limit to prove that two-layer networks could be trained to ﬁnd (near) global optima using variants
of gradient descent, despite non-convexity (Mei et al. (2018); Chizat & Bach (2018)). The MF limit
identiﬁed by these works assumes the form of gradient ﬂows in the measure space, which factors
out the invariance from the action of a symmetry group on the model. Interestingly, by lifting to
the measure space, with a convex loss function (e.g. squared loss), one obtains a limiting optimiza-
tion problem that is convex (Bengio et al. (2006); Bach (2017)). The analyses of Mei et al. (2018);
∗This paper is a conference submission. We refer to the work Nguyen & Pham (2020) and its companion
note Pham & Nguyen (2020) for generalizations as well as other conditions for global convergence in the case
of multilayer neural networks.
†Department of Mathematics, Stanford University. This work was done in parts while H. T. Pham was at
the University of Cambridge.
‡The Voleon Group. This work was done while P.-M. Nguyen was at Stanford University.
§The author ordering is randomized.
1This is to be contrasted with another major operating regime (the NTK regime) where parameters essen-
tially do not evolve and the model behaves like a kernel method (Jacot et al. (2018); Chizat et al. (2019); Du
et al. (2019); Allen-Zhu et al. (2019); Zou et al. (2018); Lee et al. (2019)).
1

Published as a conference paper at ICLR 2021
Chizat & Bach (2018) utilize convexity, although the mechanisms to attain global convergence in
these works are more sophisticated than the usual convex optimization setup in Euclidean spaces.
The extension to multilayer networks has enjoyed much less progresses. The works Nguyen (2019);
Araújo et al. (2019); Sirignano & Spiliopoulos (2019) argued, heuristically or rigorously, for the
existence of a MF limiting behavior under gradient descent training with different assumptions. In
fact, it has been argued that the difﬁculty is not simply technical, but rather conceptual (Nguyen
(2019)): for instance, the presence of intermediate layers exhibits multiple symmetry groups with
intertwined actions on the model. Convergence to the global optimum of the model under gradient-
based optimization has not been established when there are more than two layers.
In this work, we prove a global convergence guarantee for feedforward three-layer networks trained
with unregularized stochastic gradient descent (SGD) in the MF regime. After an introduction of
the three-layer setup and its MF limit in Section 2, our development proceeds in two main steps:
Step 1 (Theorem 3 in Section 3):
We ﬁrst develop a rigorous framework that describes the MF
limit and establishes its connection with a large-width SGD-trained three-layer network. Here we
propose the new idea of a neuronal embedding, which comprises of an appropriate non-evolving
probability space that encapsulates neural networks of arbitrary sizes. This probability space is in
general abstract and is constructed according to the (not necessarily i.i.d.) initialization scheme of
the neural network. This idea addresses directly the intertwined action of multiple symmetry groups,
which is the aforementioned conceptual obstacle (Nguyen (2019)), thereby covering setups that
cannot be handled by formulations in Araújo et al. (2019); Sirignano & Spiliopoulos (2019) (see also
Section 5 for a comparison). Our analysis follows the technique from Sznitman (1991); Mei et al.
(2018) and gives a quantitative statement: in particular, the MF limit yields a good approximation
of the neural network as long as n−1
min log nmax ≪1 independent of the data dimension, where nmin
and nmax are the minimum and maximum of the widths.
Step 2 (Theorem 8 in Section 4):
We prove that the MF limit, given by our framework, con-
verges to the global optimum under suitable regularity and convergence mode assumptions. Several
elements of our proof are inspired by Chizat & Bach (2018); the technique in their work however
does not generalize to our three-layer setup. Unlike previous two-layer analyses, we do not exploit
convexity; instead we make use of a new element: a universal approximation property. The result
turns out to be conceptually new: global convergence can be achieved even when the loss function
is non-convex. An important crux of the proof is to show that the universal approximation property
holds at any ﬁnite training time (but not necessarily at convergence, i.e. at inﬁnite time, since the
property may not realistically hold at convergence).
Together these two results imply a positive statement on the optimization efﬁciency of SGD-trained
unregularized feedforward three-layer networks (Corollary 10). Our results can be extended to the
general multilayer case – with new ideas on top and signiﬁcantly more technical works – or used to
obtain new global convergence guarantees in the two-layer case (Nguyen & Pham (2020); Pham &
Nguyen (2020)). We choose to keep the current paper concise with the three-layer case being a pro-
totypical setup that conveys several of the basic ideas. Complete proofs are presented in appendices.
Notations.
K denotes a generic constant that may change from line to line. |·| denotes the absolute
value for a scalar and the Euclidean norm for a vector. For an integer n, we let [n] = {1, ..., n}.
2
THREE-LAYER NEURAL NETWORKS AND THE MEAN FIELD LIMIT
2.1
THREE-LAYER NEURAL NETWORK
We consider the following three-layer network at time k ∈N≥0 that takes as input x ∈Rd:
ˆy (x; W (k)) = ϕ3 (H3 (x; W (k))) ,
(1)
H3 (x; W (k)) = 1
n2
n2
X
j2=1
w3 (k, j2) ϕ2 (H2 (x, j2; W (k))) ,
2

Published as a conference paper at ICLR 2021
H2 (x, j2; W (k)) = 1
n1
n1
X
j1=1
w2 (k, j1, j2) ϕ1 (⟨w1 (k, j1) , x⟩) ,
in which W (k) = (w1 (k, ·) , w2 (k, ·, ·) , w3 (k, ·)) consists of the weights2 w1 (k, j1) ∈Rd,
w2 (k, j1, j2) ∈R and w3 (k, j2) ∈R. Here ϕ1 : R →R, ϕ2 : R →R and ϕ3 : R →R are the
activation functions, and the network has widths {n1, n2}.
We train the network with SGD w.r.t. the loss L : R × R →R≥0. We assume that at each time k,
we draw independently a fresh sample z (k) = (x (k) , y (k)) ∈Rd × R from a training distribution
P. Given an initialization W (0), we update W (k) according to
w3 (k + 1, j2) = w3 (k, j2) −ϵξ3 (kϵ) Grad3 (z (k) , j2; W (k)) ,
w2 (k + 1, j1, j2) = w2 (k, j1, j2) −ϵξ2 (kϵ) Grad2 (z (k) , j1, j2; W (k)) ,
w1 (k + 1, j1) = w1 (k, j1) −ϵξ1 (kϵ) Grad1 (z (k) , j1; W (k)) ,
in which j1 = 1, ..., n1, j2 = 1, ..., n2, ϵ ∈R>0 is the learning rate, ξi : R≥0 7→R≥0 is the learning
rate schedule for wi, and for z = (x, y), we deﬁne
Grad3 (z, j2; W (k)) = ∂2L (y, ˆy (x; W (k))) ϕ′
3 (H3 (x; W (k))) ϕ2 (H2 (x, j2; W (k))) ,
Grad2 (z, j1, j2; W (k)) = ∆H
2 (z, j2; W (k)) ϕ1 (⟨w1 (k, j1) , x⟩) ,
Grad1 (z, j1; W (k)) =
 1
n2
n2
X
j2=1
∆H
2 (z, j2; W (k)) w2 (k, j1, j2)

ϕ′
1 (⟨w1 (k, j1) , x⟩) x,
∆H
2 (z, j2; W (k)) = ∂2L (y, ˆy (x; W (k))) ϕ′
3 (H3 (x; W (k))) w3 (k, j2) ϕ′
2 (H2 (x, j2; W (k))) .
We note that this setup follows the same scaling w.r.t. n1 and n2, which is applied to both the
forward pass and the learning rates in the backward pass, as Nguyen (2019).
2.2
MEAN FIELD LIMIT
The MF limit is a continuous-time inﬁnite-width analog of the neural network under training. To
describe it, we ﬁrst introduce the concept of a neuronal ensemble. Given a product probability space
(Ω, F, P) = (Ω1 × Ω2, F1 × F1, P1 × P2), we independently sample Ci ∼Pi, i = 1, 2. In the
following, we use ECi to denote the expectation w.r.t. the random variable Ci ∼Pi and ci to denote
an arbitrary point ci ∈Ωi. The space (Ω, F, P) is referred to as a neuronal ensemble.
Given a neuronal ensemble (Ω, F, P), the MF limit is described by a time-evolving system with
state/parameter W (t), where the time t ∈R≥0 and W (t) = (w1 (t, ·) , w2 (t, ·, ·) , w3 (t, ·)) with
w1 : R≥0 × Ω1 →Rd, w2 : R≥0 × Ω1 × Ω2 →R and w3 : R≥0 × Ω2 →R. It entails the
quantities:
ˆy (x; W (t)) = ϕ3 (H3 (x; W (t))) ,
H3 (x; W (t)) = EC2 [w3 (t, C2) ϕ2 (H2 (x, C2; W (t)))] ,
H2 (x, c2; W (t)) = EC1 [w2 (t, C1, c2) ϕ1 (⟨w1 (t, C1) , x⟩)] .
Here for each t ∈R≥0, w1 (t, ·) is (Ω1, F1)-measurable, and similar for w2 (t, ·, ·), w3 (t, ·).
The MF limit evolves according to a continuous-time dynamics, described by a system of
ODEs, which we refer to as the MF ODEs.
Speciﬁcally, given an initialization W (0) =
(w1 (0, ·) , w2 (0, ·, ·) , w3 (0, ·)), the dynamics solves:
∂tw3 (t, c2) = −ξ3 (t) ∆3 (c2; W (t)) ,
∂tw2 (t, c1, c2) = −ξ2 (t) ∆2 (c1, c2; W (t)) ,
∂tw1 (t, c1) = −ξ1 (t) ∆1 (c1; W (t)) .
Here c1 ∈Ω1, c2 ∈Ω2, EZ denotes the expectation w.r.t. the data Z = (X, Y ) ∼P, and for
z = (x, y), we deﬁne
∆3 (c2; W (t)) = EZ [∂2L (Y, ˆy (X; W (t))) ϕ′
3 (H3 (X; W (t))) ϕ2 (H2 (X, c2; W (t)))] ,
2To absorb ﬁrst layer’s bias term to w1, we assume the input x to have 1 appended to the last entry.
3

Published as a conference paper at ICLR 2021
∆2 (c1, c2; W (t)) = EZ

∆H
2 (Z, c2; W (t)) ϕ1 (⟨w1 (t, c1) , X⟩)

,
∆1 (c1; W (t)) = EZ

EC2

∆H
2 (Z, C2; W (t)) w2 (t, c1, C2)

ϕ′
1 (⟨w1 (t, c1) , X⟩) X

,
∆H
2 (z, c2; W (t)) = ∂2L (y, ˆy (x; W (t))) ϕ′
3 (H3 (x; W (t))) w3 (t, c2) ϕ′
2 (H2 (x, c2; W (t))) .
In Appendix B, we show well-posedness of MF ODEs under the following regularity conditions.
Assumption 1 (Regularity). We assume that ϕ1 and ϕ2 are K-bounded, ϕ′
1, ϕ′
2 and ϕ′
3 are K-
bounded and K-Lipschitz, ϕ′
2 and ϕ′
3 are non-zero everywhere, ∂2L (·, ·) is K-Lipschitz in the
second variable and K-bounded, and |X| ≤K with probability 1. Furthermore ξ1, ξ2 and ξ3 are
K-bounded and K-Lipschitz.
Theorem 1. Under Assumption 1, given any neuronal ensemble and an initialization W (0) such
that3 ess-sup |w2 (0, C1, C2)| , ess-sup |w3 (0, C2)| ≤K, there exists a unique solution W to the
MF ODEs on t ∈[0, ∞).
An example of a suitable setup is ϕ1 = ϕ2 = tanh, ϕ3 is the identity, L is the Huber loss, although a
non-convex sufﬁciently smooth loss function sufﬁces. In fact, all of our developments can be easily
modiﬁed to treat the squared loss with an additional assumption |Y | ≤K with probability 1.
So far, given an arbitrary neuronal ensemble (Ω, F, P), for each initialization W (0), we have de-
ﬁned a MF limit W (t). The connection with the neural network’s dynamics W (k) is established in
the next section.
3
CONNECTION BETWEEN NEURAL NETWORK AND ITS MEAN FIELD LIMIT
3.1
NEURONAL EMBEDDING AND THE COUPLING PROCEDURE
To formalize a connection between the neural network and its MF limit, we consider their initializa-
tions. In practical scenarios, to set the initial parameters W (0) of the neural network, one typically
randomizes W (0) according to some distributional law ρ. We note that since the neural network
is deﬁned w.r.t. a set of ﬁnite integers {n1, n2}, so is ρ. We consider a family Init of initialization
laws, each of which is indexed by the set {n1, n2}:
Init = {ρ : ρ is the initialization law of a neural network of size {n1, n2} , n1, n2 ∈N>0}.
This is helpful when one is to take a limit that sends n1, n2 →∞, in which case the size of this
family |Init| is inﬁnite. More generally we allow |Init| < ∞(for example, Init contains a single law
ρ of a network of size {n1, n2} and hence |Init| = 1). We make the following crucial deﬁnition.
Deﬁnition 2. Given a family of initialization laws Init, we call (Ω, F, P,

w0
i
	
i=1,2,3) a neuronal
embedding of Init if the following holds:
1. (Ω, F, P) = (Ω1 × Ω2, F1 × F2, P1 × P2) a product measurable space. As a reminder,
we call it a neuronal ensemble.
2. The deterministic functions w0
1 : Ω1 →Rd, w0
2 : Ω1 × Ω2 →R and w0
3 : Ω2 →R are
such that, for each index {n1, n2} of Init and the law ρ of this index, if — with an abuse of
notations — we independently sample {Ci (ji)}ji∈[ni] ∼Pi i.i.d. for each i = 1, 2, then
Law
 w0
1 (C1 (j1)) , w0
2 (C1(j1), C2 (j2)) , w0
3 (C2(j2)) , ji ∈[ni] , i = 1, 2

= ρ.
To proceed, given Init and {n1, n2} in its index set, we perform the following coupling procedure:
1. Let (Ω, F, P,

w0
i
	
i=1,2,3) be a neuronal embedding of Init.
2. We form the MF limit W (t) (for t ∈R≥0) associated with the neuronal ensemble
(Ω, F, P) by setting the initialization W (0) to w1 (0, ·) = w0
1 (·), w2 (0, ·, ·) = w0
2 (·, ·)
and w3 (0, ·) = w0
3 (·) and running the MF ODEs described in Section 2.2.
3We recall the deﬁnition of ess-sup in Appendix A.
4

Published as a conference paper at ICLR 2021
3. We independently sample Ci (ji) ∼Pi for i = 1, 2 and ji = 1, ..., ni. We then form
the neural network initialization W (0) with w1 (0, j1) = w0
1 (C1 (j1)), w2 (0, j1, j2) =
w0
2 (C1 (j1) , C2 (j2)) and w3 (0, j2) = w0
3 (C2 (j2)) for j1 ∈[n1], j2 ∈[n2]. We obtain
the network’s trajectory W (k) for k ∈N≥0 as in Section 2.1, with the data z (k) generated
independently of {Ci (ji)}i=1,2 and hence W (0).
We can then deﬁne a measure of closeness between W (⌊t/ϵ⌋) and W (t) for t ∈[0, T]:
DT (W, W) = sup

|w1 (⌊t/ϵ⌋, j1) −w1 (t, C1 (j1))| , |w2 (⌊t/ϵ⌋, j1, j2) −w2 (t, C1 (j1) , C2 (j2))| ,
|w3 (⌊t/ϵ⌋, j2) −w3 (t, C2 (j2))| : t ≤T, j1 ≤n1, j2 ≤n2
	
.
(2)
Note that W (t) is a deterministic trajectory independent of {n1, n2}, whereas W (k) is random for
all k ∈N≥0 due to the randomness of {Ci (ji)}i=1,2 and the generation of the training data z (k).
Similarly DT (W, W) is a random quantity.
The idea of the coupling procedure is closely related to the coupling argument in Sznitman (1991);
Mei et al. (2018). Here, instead of playing the role of a proof technique, the coupling serves as a
vehicle to establish the connection between W and W on the basis of the neuronal embedding. This
connection is shown in Theorem 3 below, which gives an upper bound on DT (W, W).
We note that the coupling procedure can be carried out to provide a connection between W and
W as long as there exists a neuronal embedding for Init. Later in Section 4.1, we show that for
a common initialization scheme (in particular, i.i.d. initialization) for Init, there exists a neuronal
embedding. Theorem 3 applies to, but is not restricted to, this initialization scheme.
3.2
MAIN RESULT: APPROXIMATION BY THE MF LIMIT
Assumption 2 (Initialization of second and third layers). We assume that ess-sup
w0
2 (C1, C2)
,
ess-sup
w0
3 (C2)
 ≤K, where w0
2 and w0
3 are as described in Deﬁnition 2.
Theorem 3. Given a family Init of initialization laws and a tuple {n1, n2} that is in the index set
of Init, perform the coupling procedure as described in Section 3.1. Fix a terminal time T ∈ϵN≥0.
Under Assumptions 1 and 2, for ϵ ≤1, we have with probability at least 1 −2δ,
DT (W, W) ≤eKT

1
√nmin
+ √ϵ

log1/2
3 (T + 1) n2
max
δ
+ e

≡errδ,T (ϵ, n1, n2) ,
in which nmin = min {n1, n2}, nmax = max {n1, n2}, and KT = K
 1 + T K
.
The theorem gives a connection between W (⌊t/ϵ⌋), which is deﬁned upon ﬁnite widths n1 and n2,
and the MF limit W (t), whose description is independent of n1 and n2. It lends a way to extract
properties of the neural network in the large-width regime.
Corollary 4. Under the same setting as Theorem 3, consider any test function ψ : R × R →R
which is K-Lipschitz in the second variable uniformly in the ﬁrst variable (an example of ψ is the
loss L). For any δ > 0, with probability at least 1 −3δ,
sup
t≤T
|EZ [ψ (Y, ˆy (X; W (⌊t/ϵ⌋)))] −EZ [ψ (Y, ˆy (X; W (t)))]| ≤eKT errδ,T (ϵ, n1, n2) .
These bounds hold for any n1 and n2, similar to Mei et al. (2018); Araújo et al. (2019), in contrast
with non-quantitative results in Chizat & Bach (2018); Sirignano & Spiliopoulos (2019). These
bounds suggest that n1 and n2 can be chosen independent of the data dimension d. This agrees with
the experiments in Nguyen (2019), which found width ≈1000 to be typically sufﬁcient to observe
MF behaviors in networks trained with real-life high-dimensional data.
We observe that the MF trajectory W (t) is deﬁned as per the choice of the neuronal embedding
(Ω, F, P,

w0
i
	
i=1,2,3), which may not be unique. On the other hand, the neural network’s trajectory
W (k) depends on the randomization of the initial parameters W (0) according to an initialization
law from the family Init (as well as the data z (k)) and hence is independent of this choice. Another
corollary of Theorem 3 is that given the same family Init, the law of the MF trajectory is insensitive
to the choice of the neuronal embedding of Init.
5

Published as a conference paper at ICLR 2021
Corollary 5. Consider a family Init of initialization laws, indexed by a set of tuples {m1, m2}
that contains a sequence of indices {m1 (m) , m2 (m) : m ∈N} in which as m
→
∞,
min {m1 (m) , m2 (m)}−1 log (max {m1 (m) , m2 (m)}) →0. Let W (t) and ˆW (t) be two MF
trajectories associated with two choices of neuronal embeddings of Init, (Ω, F, P,

w0
i
	
i=1,2,3) and
(ˆΩ, ˆF, ˆP,

ˆw0
i
	
i=1,2,3). The following statement holds for any T ≥0 and any two positive integers
n1 and n2: if we independently sample Ci (ji) ∼Pi and ˆCi (ji) ∼ˆPi for ji ∈[ni], i = 1, 2,
then Law (W (n1, n2, T)) = Law( ˆ
W (n1, n2, T)), where we deﬁne W (n1, n2, T) as the below
collection w.r.t. W (t), and similarly deﬁne ˆ
W (n1, n2, T) w.r.t. ˆW (t):
W (n1, n2, T) =

w1 (t, C1 (j1)) , w2 (t, C1 (j1) , C2 (j2)) , w3 (t, C2 (j2)) :
j1 ∈[n1] , j2 ∈[n2] , t ∈[0, T]
	
.
The proofs are deferred to Appendix C.
4
CONVERGENCE TO GLOBAL OPTIMA
In this section, we prove a global convergence guarantee for three-layer neural networks via the MF
limit. We consider a common class of initialization: i.i.d. initialization.
4.1
I.I.D. INITIALIZATION
Deﬁnition 6. An initialization law ρ for a neural network of size {n1, n2} is called
 ρ1, ρ2, ρ3
-
i.i.d. initialization (or i.i.d. initialization, for brevity), where ρ1, ρ2 and ρ3 are probability mea-
sures over Rd, R and R respectively, if {w1 (0, j1)}j1∈[n1] are generated i.i.d. according to ρ1,
{w2 (0, j1, j2)}j1∈[n1], j2∈[n2] are generated i.i.d. according to ρ2 and {w3 (0, j2)}j2∈[n2] are gen-
erated i.i.d. according to ρ3, and w1, w2 and w3 are independent.
Observe that given
 ρ1, ρ2, ρ3
, one can build a family Init of i.i.d. initialization laws that contains
any index set {n1, n2}. Furthermore i.i.d. initializations are supported by our framework, as stated
in the following proposition and proven in Appendix D.
Proposition 7. There exists a neuronal embedding

Ω, F, P,

w0
i
	
i=1,2,3

for any family Init of
initialization laws, which are
 ρ1, ρ2, ρ3
-i.i.d.
4.2
MAIN RESULT: GLOBAL CONVERGENCE
To measure the learning quality, we consider the loss averaged over the data Z ∼P:
L (V ) = EZ [L (Y, ˆy (X; V ))] ,
where V = (v1, v2, v3) is a set of three measurable functions v1 : Ω1 →Rd, v2 : Ω1 × Ω2 →R,
v3 : Ω2 →R.
Assumption 3. Consider a neuronal embedding

Ω, F, P,

w0
i
	
i=1,2,3

of the
 ρ1, ρ2, ρ3
-i.i.d.
initialization, and the associated MF limit with initialization W (0) such that w1 (0, ·) = w0
1 (·),
w2 (0, ·, ·) = w0
2 (·, ·) and w3 (0, ·) = w0
3 (·). Assume:
1. Support: The support of ρ1 is Rd.
2. Convergence mode: There exist limits ¯w1, ¯w2 and ¯w3 such that as t →∞,
E [(1 + | ¯w3(C2)|) | ¯w3(C2)| | ¯w2(C1, C2)| |w1(t, C1) −¯w1(C1)|] →0,
(3)
E [(1 + | ¯w3(C2)|) | ¯w3(C2)| |w2(t, C1, C2) −¯w2(C1, C2)|] →0,
(4)
E [(1 + | ¯w3(C2)|) |w3(t, C2) −¯w3(C2)|] →0,
(5)
ess-supEC2 [|∂tw2 (t, C1, C2)|] →0.
(6)
6

Published as a conference paper at ICLR 2021
3. Universal approximation:

ϕ1 (⟨u, ·⟩) : u ∈Rd	
has dense span in L2 (PX) (the space
of square integrable functions w.r.t. PX the distribution of the input X).
Assumption 3 is inspired by the work Chizat & Bach (2018) on two-layer networks, with certain
differences. Assumptions 3.1 and 3.3 are natural in neural network learning (Cybenko (1989); Chen
& Chen (1995)), while we note Chizat & Bach (2018) does not utilize universal approximation.
Similar to Chizat & Bach (2018), Assumption 3.2 is technical and does not seem removable. Note
that this assumption speciﬁes the mode of convergence and is not an assumption on the limits ¯w1, ¯w2
and ¯w3. Speciﬁcally conditions (3)-(5) are similar to the convergence assumption in Chizat & Bach
(2018). We differ from Chizat & Bach (2018) fundamentally in the essential supremum condition
(6). On one hand, this condition helps avoid the Morse-Sard type condition in Chizat & Bach (2018),
which is difﬁcult to verify in general and not simple to generalize to the three-layer case. On the
other hand, it turns out to be a natural assumption to make, in light of Remark 9 below.
We now state the main result of the section. The proof is in Appendix D.
Theorem 8. Consider a neuronal embedding

Ω, F, P,

w0
i
	
i=1,2,3

of
 ρ1, ρ2, ρ3
-i.i.d. initial-
ization. Consider the MF limit corresponding to the network (1), such that they are coupled together
by the coupling procedure in Section 3.1, under Assumptions 1, 2 and 3. For simplicity, assume
ξ1 (·) = ξ2 (·) = 1. Further assume either:
• (untrained third layer) ξ3 (·) = 0 and w0
3 (C2) ̸= 0 with a positive probability, or
• (trained third layer) ξ3 (·) = 1 and L
 w0
1, w0
2, w0
3

< EZ [L (Y, ϕ3 (0))].
Then the following hold:
• Case 1 (convex loss): If L is convex in the second variable, then
lim
t→∞L (W (t)) = inf
V L (V ) =
inf
˜y: Rd→R EZ [L (Y, ˜y (X))] .
• Case 2 (generic non-negative loss): Suppose that ∂2L (y, ˆy) = 0 implies L (y, ˆy) = 0. If
y = y(x) is a function of x, then L (W (t)) →0 as t →∞.
Remarkably here the theorem allows for non-convex losses. A further inspection of the proof shows
that no convexity-based property is used in Case 2 (see, for instance, the high-level proof sketch
in Section 4.3); in Case 1, the key steps in the proof are the same, and the convexity of the loss
function serves as a convenient technical assumption to handle the arbitrary extra randomness of Y
conditional on X. We also remark that the same proof of global convergence should extend beyond
the speciﬁc fully-connected architecture considered here. Similar to previous results on SGD-trained
two-layer networks Mei et al. (2018); Chizat & Bach (2018), our current result in the three-layer case
is non-quantitative.
Remark 9. Interestingly there is a converse relation between global convergence and the essential
supremum condition (6): under the same setting, global convergence is unattainable if condition
(6) does not hold. A similar observation was made in Wojtowytsch (2020) for two-layer ReLU
networks. A precise statement and its proof can be found in Appendix E.
The following result is straightforward from Theorem 8 and Corollary 4, establishing the optimiza-
tion efﬁciency of the neural network with SGD.
Corollary 10. Consider the neural network (1). Under the same setting as Theorem 8, in Case 1,
lim
t→∞lim
n1,n2 lim
ϵ→0 EZ [L (Y, ˆy (X; W (⌊t/ϵ⌋)))] =
inf
f1,f2,f3 L (f1, f2, f3) = inf
˜y EZ [L (Y, ˜y (X))]
in probability, where the limit of the widths is such that min {n1, n2}−1 log (max {n1, n2}) →0.
In Case 2, the same holds with the right-hand side being 0.
4.3
HIGH-LEVEL IDEA OF THE PROOF
We give a high-level discussion of the proof. This is meant to provide intuitions and explain the
technical crux, so our discussion may simplify and deviate from the actual proof.
7

Published as a conference paper at ICLR 2021
Our ﬁrst insight is to look at the second layer’s weight w2. At convergence time t = ∞, we expect
to have zero movement and hence, denoting W (∞) = ( ¯w1, ¯w2, ¯w3):
∆2 (c1, c2; W (∞)) = EZ

∆H
2 (Z, c2; W (∞)) ϕ1 (⟨¯w1 (c1) , X⟩)

= 0,
for P-almost every c1, c2. Suppose for the moment that we are allowed to make an additional
(strong) assumption on the limit ¯w1: supp ( ¯w1 (C1)) = Rd.
It implies that the universal ap-
proximation property, described in Assumption 3, holds at t = ∞; more speciﬁcally, it implies
{ϕ1 (⟨¯w1 (c1) , ·⟩) : c1 ∈Ω1} has dense span in L2 (PX). This thus yields
EZ

∆H
2 (Z, c2; W (∞))
X = x

= 0,
for P-almost every x. Recalling the deﬁnition of ∆H
2 , one can then easily show that
EZ [∂2L (Y, ˆy (X; W (∞)))|X = x] = 0.
Global convergence follows immediately; for example, in Case 2 of Theorem 8, this is equivalent to
that ∂2L (y (x) , ˆy (x; W (∞))) = 0 and hence L (y (x) , ˆy (x; W (∞))) = 0 for P-almost every x.
In short, the gradient ﬂow structure of the dynamics of w2 provides a seamless way to obtain global
convergence. Furthermore there is no critical reliance on convexity.
However this plan of attack has a potential ﬂaw in the strong assumption that supp ( ¯w1 (C1)) =
Rd, i.e. the universal approximation property holds at convergence time. Indeed there are setups
where it is desirable that supp ( ¯w1 (C1)) ̸= Rd (Mei et al. (2018); Chizat (2019)); for instance,
it is the case where the neural network is to learn some “sparse and spiky” solution, and hence
the weight distribution at convergence time, if successfully trained, cannot have full support. On
the other hand, one can entirely expect that if supp (w1 (0, C1)) = Rd initially at t = 0, then
supp (w1 (t, C1)) = Rd at any ﬁnite t ≥0. The crux of our proof is to show the latter without
assuming supp ( ¯w1 (C1)) = Rd.
This task is the more major technical step of the proof. To that end, we ﬁrst show that there exists
a mapping (t, u) 7→M (t, u) that maps from (t, w1 (0, c1)) = (t, u) to w1 (t, c1) via a careful
measurability argument. This argument rests on a scheme that exploits the symmetry in the network
evolution. Furthermore the map M is shown to be continuous. The desired conclusion then follows
from an algebraic topology argument that the map M preserves a homotopic structure through time.
5
DISCUSSION
The MF literature is fairly recent. A long line of works (Nitanda & Suzuki (2017); Mei et al. (2018);
Chizat & Bach (2018); Rotskoff & Vanden-Eijnden (2018); Sirignano & Spiliopoulos (2018); Wei
et al. (2019); Javanmard et al. (2019); Mei et al. (2019); Shevchenko & Mondelli (2019); Woj-
towytsch (2020)) have focused mainly on two-layer neural networks, taking an interacting particle
system approach to describe the MF limiting dynamics as Wasserstein gradient ﬂows. The three
works Nguyen (2019); Araújo et al. (2019); Sirignano & Spiliopoulos (2019) independently develop
different formulations for the MF limit in multilayer neural networks, under different assumptions.
These works take perspectives that are different from ours. In particular, while the central object
in Nguyen (2019) is a new abstract representation of each individual neuron, our neuronal embed-
ding idea instead takes a keen view on a whole ensemble of neurons. Likewise our idea is also
distant from Araújo et al. (2019); Sirignano & Spiliopoulos (2019): the central objects in Araújo
et al. (2019) are paths over the weights across layers; those in Sirignano & Spiliopoulos (2019) are
time-dependent functions of the initialization, which are simpliﬁed upon i.i.d. initializations.
The result of our perspective is a neuronal embedding framework that allows one to describe the MF
limit in a clean and rigorous manner. In particular, it avoids extra assumptions made in Araújo et al.
(2019); Sirignano & Spiliopoulos (2019): unlike our work, Araújo et al. (2019) assumes untrained
ﬁrst and last layers and requires non-trivial technical tools; Sirignano & Spiliopoulos (2019) takes
an unnatural sequential limit n1 →∞before n2 →∞and proves a non-quantitative result, unlike
Theorem 3 which only requires sufﬁciently large min {n1, n2}. We note that Theorem 3 can be
extended to general multilayer networks using the neuronal embedding idea. The advantages of
our framework come from the fact that while MF formulations in Araújo et al. (2019); Sirignano
& Spiliopoulos (2019) are speciﬁc to and exploit i.i.d. initializations, our formulation does not.
Remarkably as shown in Araújo et al. (2019), when there are more than three layers and no biases,
8

Published as a conference paper at ICLR 2021
i.i.d. initializations lead to a certain simplifying effect on the MF limit. On the other hand, our
framework supports non-i.i.d. initializations which avoid the simplifying effect, as long as there
exist suitable neuronal embeddings (Nguyen & Pham (2020)). Although our global convergence
result in Theorem 8 is proven in the context of i.i.d. initializations for three-layer networks, in the
general multilayer case, it turns out that the use of a special type of non-i.i.d. initialization allows
one to prove a global convergence guarantee (Pham & Nguyen (2020)).
In this aspect, our framework follows closely the spirit of the work Nguyen (2019), whose MF
formulation is also not speciﬁc to i.i.d. initializations. Yet though similar in the spirit, Nguyen
(2019) develops a heuristic formalism and does not prove global convergence.
Global convergence in the two-layer case with convex losses has enjoyed multiple efforts with a lot
of new and interesting results (Mei et al. (2018); Chizat & Bach (2018); Javanmard et al. (2019);
Rotskoff et al. (2019); Wei et al. (2019)). Our work is the ﬁrst to establish a global convergence guar-
antee for SGD-trained three-layer networks in the MF regime. Our proof sends a new message that
the crucial factor is not necessarily convexity, but rather that the whole learning trajectory maintains
the universal approximation property of the function class represented by the ﬁrst layer’s neurons,
together with the gradient ﬂow structure of the second layer’s weights. As a remark, our approach
can also be applied to prove a similar global convergence guarantee for two-layer networks, remov-
ing the convex loss assumption in previous works (Nguyen & Pham (2020)). The recent work Lu
et al. (2020) on a MF resnet model (a composition of many two-layer MF networks) and a recent
update of Sirignano & Spiliopoulos (2019) essentially establish conditions of stationary points to be
global optima. They however require strong assumptions on the support of the limit point. As ex-
plained in Section 4.3, we analyze the training dynamics without such assumption and in fact allow
it to be violated.
Our global convergence result is non-quantitative. An important, highly challenging future direction
is to develop a quantitative version of global convergence; previous works on two-layer networks
Javanmard et al. (2019); Wei et al. (2019); Rotskoff et al. (2019); Chizat (2019) have done so under
sophisticated modiﬁcations of the architecture and training algorithms.
Finally we remark that our insights here can be applied to prove similar global convergence guaran-
tees and derive other sufﬁcient conditions for global convergence of two-layer or multilayer networks
(Nguyen & Pham (2020); Pham & Nguyen (2020)).
ACKNOWLEDGEMENT
H. T. Pham would like to thank Jan Vondrak for many helpful discussions and in particular for the
shorter proof of Lemma 19. We would like to thank Andrea Montanari for the succinct description
of the difﬁculty in extending the mean ﬁeld formulation to the multilayer case, in that there are
multiple symmetry group actions in a multilayer network.
REFERENCES
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 242–252, 2019. 1
Dyego Araújo, Roberto I Oliveira, and Daniel Yukimura. A mean-ﬁeld limit for certain deep neural
networks. arXiv preprint arXiv:1906.00193, 2019. 1, 1, 3.2, 5
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629–681, 2017. 1
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in neural information processing systems, pp. 123–130, 2006. 1
Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks
with arbitrary activation functions and its application to dynamical systems. IEEE Transactions
on Neural Networks, 6(4):911–917, 1995. 4.2
9

Published as a conference paper at ICLR 2021
Lenaic Chizat. Sparse optimization on measures with over-parameterized gradient descent. arXiv
preprint arXiv:1907.10300, 2019. 4.3, 5
Lénaïc Chizat and Francis Bach.
On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in Neural Information Processing
Systems, pp. 3040–3050. 2018. 1, 1, 3.2, 4.2, 4.2, 5, D.2
Lénaïc Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems 32, pp. 2937–2947. 2019. 1
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
signals and systems, 2(4):303–314, 1989. 4.2
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=S1eK3i09YQ. 1
Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad-
vances in Neural Information Processing Systems, pp. 9747–9757, 2018. F
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8580–8589. 2018. 1
Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network
via displacement convexity. arXiv preprint arXiv:1901.01375, 2019. 5
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems 32, pp. 8572–
8583. 2019. 1
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean-ﬁeld analysis of deep
resnet and beyond: Towards provable optimization via overparameterization from depth. arXiv
preprint arXiv:2003.05508, 2020. 5
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean ﬁeld view of the landscape of two-
layers neural networks. In Proceedings of the National Academy of Sciences, volume 115, pp.
7665–7671, 2018. 1, 1, 3.1, 3.2, 4.2, 4.3, 5
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-ﬁeld theory of two-layers neural
networks: dimension-free bounds and kernel limit. arXiv preprint arXiv:1902.06015, 2019. 5
Phan-Minh Nguyen. Mean ﬁeld limit of the learning dynamics of multilayer neural networks. arXiv
preprint arXiv:1902.02880, 2019. 1, 1, 2.1, 3.2, 5
Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean ﬁeld limit of multi-
layer neural networks. arXiv preprint arXiv:2001.11443, 2020. ∗, 1, 5
Atsushi Nitanda and Taiji Suzuki. Stochastic particle gradient descent for inﬁnite ensembles. arXiv
preprint arXiv:1712.05438, 2017. 5
Huy Tuan Pham and Phan-Minh Nguyen. A note on the global convergence of multilayer neural
networks in the mean ﬁeld regime. arXiv preprint arXiv:2006.09355, 2020. ∗, 1, 5
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, 22(4):1679–1706, 1994. F
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in Neural Information Processing
Systems 31, pp. 7146–7155. 2018. 1, 5
Grant Rotskoff, Samy Jelassi, Joan Bruna, and Eric Vanden-Eijnden. Neuron birth-death dynamics
accelerates gradient descent and converges asymptotically. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, volume 97, pp. 5508–5517, 2019. 5
10

Published as a conference paper at ICLR 2021
Alexander Shevchenko and Marco Mondelli. Landscape connectivity and dropout stability of sgd
solutions for over-parameterized neural networks. arXiv preprint arXiv:1912.10095, 2019. 5
Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of neural networks. arXiv
preprint arXiv:1805.01053, 2018. 1, 5
Justin Sirignano and Konstantinos Spiliopoulos. Mean ﬁeld analysis of deep neural networks. arXiv
preprint arXiv:1903.04440, 2019. 1, 1, 3.2, 5
Alain-Sol Sznitman. Topics in propagation of chaos. In Ecole d’été de probabilités de Saint-Flour
XIX—1989, pp. 165–251. Springer, 1991. 1, 3.1
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and op-
timization of neural nets v.s. their induced kernel. In Advances in Neural Information Processing
Systems 32, pp. 9712–9724. 2019. 5
Stephan Wojtowytsch. On the convergence of gradient descent training for two-layer relu-networks
in the mean ﬁeld regime. arXiv preprint arXiv:2005.13530, 2020. 9, 5
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018. 1
11

Published as a conference paper at ICLR 2021
A
NOTATIONAL PRELIMINARIES
For a real-valued random variable Z deﬁned on a probability space (Ω, F, P), we recall
ess-supZ = inf {z ∈R : P (Z > z) = 0} .
We also introduce some convenient deﬁnitions which we use throughout the appendices. For a set
of neural network’s parameter W, we deﬁne
9W9T = max
n
max
j1≤n1, j2≤n2 sup
t≤T
|w2 (⌊t/ϵ⌋, j1, j2)| , max
j2≤n2 sup
t≤T
|w3 (⌊t/ϵ⌋, j2)|
o
.
Similarly for a set of MF parameters W, we deﬁne:
9W9T = max
n
ess-sup sup
t≤T
|w2 (t, C1, C2)| , ess-sup sup
t≤T
|w3 (t, C2)|
o
.
For two sets of neural network’s parameters W′, W′′, we deﬁne their distance:
∥W′ −W′′∥T = sup

|w′
1 (⌊t/ϵ⌋, j1) −w′′
1 (⌊t/ϵ⌋, j1)| , |w′
2 (⌊t/ϵ⌋, j1, j2) −w′′
2 (⌊t/ϵ⌋, j1, j2)| ,
|w′
3 (⌊t/ϵ⌋, j2) −w′′
3 (⌊t/ϵ⌋, j2)| : t ∈[0, T] , j1 ∈[n1] , j2 ∈[n2]
	
.
Similarly for two sets of MF parameters W ′, W ′′, we deﬁne their distance:
∥W ′ −W ′′∥T = ess-sup sup
t∈[0,T ]
n
|w′
1 (t, C1) −w′′
1 (t, C1)| , |w′
2 (t, C1, C2) −w′′
2 (t, C1, C2)| ,
|w′
3 (t, C2) −w′′
3 (t, C2)|
o
.
B
EXISTENCE AND UNIQUENESS OF THE SOLUTION TO MF ODES
We ﬁrst collect some a priori estimates.
Lemma 11. Under Assumption 1, consider a solution W to the MF ODEs with initialization W (0)
such that 9W90 < ∞. If this solution exists, it satisﬁes the following a priori bounds, for any
T ≥0:
ess-sup sup
t≤T
|w3 (t, C2)| ≤9W 90 +KT ≡9W 90 +K0,3 (T) ,
ess-sup sup
t≤T
|w2 (t, C1, C2)| ≤9W 90 +KTK0,3 (T) ≡9W 90 +K0,2 (T) ,
and consequently, 9W9T ≤1 + max {K0,2 (T) , K0,3 (T)} .
Proof. The bounds can be obtained easily by bounding the respective initializations and update
quantities separately. In particular,
ess-sup sup
t≤T
|w3 (t, C2)| ≤ess-sup |w3 (0, C2)| + Tess-sup sup
t≤T

∂
∂tw3 (t, C2)
 ≤9W 90 +KT,
ess-sup sup
t≤T
|w2 (t, C1, C2)| ≤ess-sup |w2 (0, C1, C2)| + Tess-sup sup
t≤T

∂
∂tw2 (t, C1, C2)

≤ess-sup |w2 (0, C1, C2)| + KTess-sup sup
t≤T
|w3 (t, C2)|
≤9W 90 +KTK0,3 (T) .
Inspired by the a priori bounds in Lemma 11, given an arbitrary terminal time T and the initialization
W (0), let us consider:
12

Published as a conference paper at ICLR 2021
• for a tuple (a, b)
∈
R2
≥0,
a space WT (a, b) of W ′
=
(W ′ (t))t≤T
=
(w′
1 (t, ·) , w′
2 (t, ·, ·) , w′
3 (t, ·))t≤T such that
ess-sup sup
t≤T
|w′
3 (t, C2)| ≤b,
ess-sup sup
t≤T
|w′
2 (t, C1, C2)| ≤a,
where w′
1 : R≥0 × Ω1 →Rd, w′
2 : R≥0 × Ω1 × Ω2 7→R, w′
3 : R≥0 × Ω3 7→R,
• for a tuple (a, b) ∈R2
≥0 and W (0), a space W+
T (a, b, W (0)) of W ′ ∈WT (a, b) such
that W ′ (0) = W (0) additionally (and hence every W ′ in this space shares the same
initialization W (0)).
We equip the spaces with the metric ∥W ′ −W ′′∥T . It is easy to see that both spaces are complete.
Note that Lemma 11 implies, under Assumption 1 and 9W90 < ∞, we have any MF solution W,
if exists, is in WT (9W 90 +K0,2 (T) , 9W 90 +K0,3 (T)). For the proof of Theorem 1, we work
mainly with W+
T (9W 90 +K0,2 (T) , 9W 90 +K0,3 (T) , W (0)), although several intermediate
lemmas are proven in more generality for other uses.
Lemma 12. Under Assumption 1, for T ≥0, any W ′, W ′′ ∈WT (a, b) and almost every z ∼P:
ess-sup sup
t≤T
∆H
2 (z, C2; W ′ (t))
 ≤Ka,b,
ess-sup sup
t≤T
|H2 (x, C2; W ′ (t)) −H2 (x, C2; W ′′ (t))| ≤Ka,b ∥W ′ −W ′′∥T ,
sup
t≤T
|H3 (x; W ′ (t)) −H3 (x; W ′′ (t))| ≤Ka,b ∥W ′ −W ′′∥T ,
sup
t≤T
|∂2L (y, ˆy (x; W ′ (t))) −∂2L (y, ˆy (x; W ′′ (t)))| ≤Ka,b ∥W ′ −W ′′∥T ,
ess-sup sup
t≤T
∆H
2 (z, C2; W ′ (t)) −∆H
2 (z, C2; W ′′ (t))
 ≤Ka,b ∥W ′ −W ′′∥T ,
where Ka,b ≥1 is a generic constant that grows polynomially with a and b.
Proof. The ﬁrst bound is easy to see:
ess-sup sup
t≤T
∆H
2 (z, C2; W ′ (t))
 ≤ess-sup sup
t≤T
|w′
3 (t, C2)| ≤b.
We prove the second bound, invoking Assumption 1:
|H2 (x, C2; W ′ (t)) −H2 (x, C2; W ′′ (t))|
≤K |w′
2 (t, C1, C2)| |ϕ1 (⟨w′
1 (t, C1) , x⟩) −ϕ1 (⟨w′′
1 (t, C1) , x⟩)|
+ K |w′
2 (t, C1, C2) −w′′
2 (t, C1, C2)|
≤K (|w′
2 (t, C1, C2)| + 1) ∥W ′ −W ′′∥T ,
which yields by the fact W ′ ∈WT (a, b):
ess-sup sup
t≤T
|H2 (x, C2; W ′ (t)) −H2 (x, C2; W ′′ (t))| ≤K (a + 1) ∥W ′ −W ′′∥T .
Consequently, we have:
|H3 (x; W ′ (t)) −H3 (x; W ′′ (t))| ≤K |w′
3 (t, C2)| |ϕ2 (H2 (x, C2; W ′ (t))) −ϕ2 (H2 (x, C2; W ′′ (t)))|
+ K |w′
3 (t, C2) −w′′
3 (t, C2)|
≤K |w′
3 (t, C2)| |H2 (x, C2; W ′ (t)) −H2 (x, C2; W ′′ (t))|
+ K ∥W ′ −W ′′∥T ,
|∂2L (y, ˆy (x; W ′ (t))) −∂2L (y, ˆy (x; W ′′ (t)))| ≤K |ˆy (x; W ′ (t)) −ˆy (x; W ′′ (t))|
≤K |H3 (x; W ′ (t)) −H3 (x; W ′′ (t))| ,
13

Published as a conference paper at ICLR 2021
which then yield the third and fourth bounds by the fact W ′, W ′′ ∈WT (a, b). Using these bounds,
we obtain the last bound:
∆H
2 (z, C2; W ′ (t)) −∆H
2 (z, C2; W ′′ (t))

≤K |w′
3 (t, C2)|

|∂2L (y, ˆy (x; W ′ (t))) −∂2L (y, ˆy (x; W ′′ (t)))|
+ |H3 (x; W ′ (t)) −H3 (x; W ′′ (t))| + |H2 (x, C2; W ′ (t)) −H2 (x, C2; W ′′ (t))|

+ K |w′
3 (t, C2) −w′′
3 (t, C2)| ,
from which the last bound follows.
To prove Theorem 1, for a given W (0), we deﬁne a mapping FW (0) that maps from W ′ =
(w′
1, w′
2, w′
3) ∈WT (a, b) to FW (0) (W ′) = ¯W ′ = ( ¯w′
1, ¯w′
2, ¯w′
3), deﬁned by ¯W ′ (0) = W (0)
and
∂
∂t ¯w′
3 (t, c2) = −ξ3 (t) ∆3 (c2; W ′ (t)) ,
∂
∂t ¯w′
2 (t, c1, c2) = −ξ2 (t) ∆2 (c1, c2; W ′ (t)) ,
∂
∂t ¯w′
1 (t, c1) = −ξ1 (t) ∆1 (c1; W ′ (t)) .
Notice that the right-hand sides do not involve ¯W ′. Note that the MF ODEs’ solution, initialized at
W (0), is a ﬁxed point of this mapping.
We establish the following estimates for this mapping.
Lemma 13. Under Assumption 1, for T ≥0, any initialization W (0) and any W ′, W ′′ ∈
WT (a, b),
ess-sup sup
s≤t
|∆3 (C2; W ′ (s)) −∆3 (C2; W ′′ (s))| ≤Ka,b ∥W ′ −W ′′∥t ,
ess-sup sup
s≤t
|∆2 (C1, C2; W ′ (s)) −∆2 (C1, C2; W ′′ (s))| ≤Ka,b ∥W ′ −W ′′∥t ,
ess-sup sup
s≤t
|∆1 (C1; W ′ (s)) −∆1 (C1; W ′′ (s))| ≤Ka,b ∥W ′ −W ′′∥t ,
and consequently, if in addition W ′ (0) = W ′′ (0) (not necessarily equal W (0)), then
ess-sup sup
t≤T
| ¯w′
3 (t, C2) −¯w′′
3 (t, C2)| ≤Ka,b
Z T
0
∥W ′ −W ′′∥s ds,
ess-sup sup
t≤T
| ¯w′
2 (t, C1, C2) −¯w′′
2 (t, C1, C2)| ≤Ka,b
Z T
0
∥W ′ −W ′′∥s ds,
ess-sup sup
t≤T
| ¯w′
1 (t, C1) −¯w′′
1 (t, C1)| ≤Ka,b
Z T
0
∥W ′ −W ′′∥s ds,
in which ¯W ′ = ( ¯w′
1, ¯w′
2, ¯w′
3) = FW (0) (W ′), ¯W ′′ = ( ¯w′′
1, ¯w′′
2, ¯w′′
3) = FW (0) (W ′′) and Ka,b ≥1 is
a generic constant that grows polynomially with a and b.
Proof. From Assumption 1 and the fact W ′, W ′′ ∈WT (a, b), we get:
|∆3 (C2; W ′ (s)) −∆3 (C2; W ′′ (s))| ≤KEZ [|∂2L (Y, ˆy (X; W ′ (s))) −∂2L (Y, ˆy (X; W ′′ (s)))|]
+ KEZ [|H3 (X; W ′ (s)) −H3 (X; W ′′ (s))|]
+ KEZ [|H2 (X, C2; W ′ (s)) −H2 (X, C2; W ′′ (s))|] ,
|∆2 (C1, C2; W ′ (s)) −∆2 (C1, C2; W ′′ (s))| ≤Ka,b |w′
1 (s, C1) −w′′
1 (s, C1)|
+ K
EZ

∆H
2 (Z, C2; W ′ (s)) −∆H
2 (Z, C2; W ′′ (s))
 ,
|∆1 (C1; W ′ (s)) −∆1 (C1; W ′′ (s))| ≤Ka,bEZ
∆H
2 (Z, C2; W ′ (s)) −∆H
2 (Z, C2; W ′′ (s))

14

Published as a conference paper at ICLR 2021
+ Ka,b |w′
2 (s, C1, C2) −w′′
2 (s, C1, C2)|
+ Ka,b |w′
1 (s, C1) −w′′
1 (s, C1)| ,
from which the ﬁrst three estimates then follow, in light of Lemma 12. The last three estimates then
follow from the fact that ¯W ′ (0) = ¯W ′′ (0) and Assumption 1; for instance,
ess-sup sup
t≤T
| ¯w′
3 (t, C2) −¯w′′
3 (t, C2)| ≤
Z T
0
ess-sup

∂
∂t ¯w′
3 (s, C2) −∂
∂t ¯w′′
3 (s, C2)
 ds
≤K
Z T
0
ess-sup |∆3 (C2; W ′ (s)) −∆3 (C2; W ′′ (s))| ds.
We are now ready to prove Theorem 1.
Proof of Theorem 1. We will use a Picard-type iteration. To lighten notations:
W+
T ≡W+
T (9W 90 +K0,2 (T) , 9W 90 +K0,3 (T) , W (0)) ,
F ≡FW (0).
Since 9W90 ≤K by assumption, we have 9W 90 +K0,2 (T) + K0,3 (T) ≤KT . Recall that W+
T
is complete. For an arbitrary T > 0, consider W ′, W ′′ ∈W+
T . Lemma 13 yields:
∥F (W ′) −F (W ′′)∥T ≤KT
Z T
0
∥W ′ −W ′′∥s ds.
Note that F maps to W+
T under Assumption 1 by the same argument as Lemma 11. Hence we are
allowed to iterating this inequality and get, for an arbitrary T > 0,
F (k) (W ′) −F (k) (W ′′)

T ≤KT
Z T
0
F (k−1) (W ′) −F (k−1) (W ′′)

T2 dT2
≤K2
T
Z T
0
Z T2
0
F (k−2) (W ′) −F (k−2) (W ′′)

T3 I (T2 ≤T) dT3dT2
...
≤Kk
T
Z T
0
Z T2
0
...
Z Tk
0
∥W ′ −W ′′∥Tk+1 I (Tk ≤... ≤T2 ≤T) dTk+1...dT2
≤1
k!Kk
T ∥W ′ −W ′′∥T .
By substituting W ′′ = F (W ′), we have:
∞
X
k=1
F (k+1) (W ′) −F (k) (W ′)

T =
∞
X
k=1
F (k) (W ′′) −F (k) (W ′)

T
≤
∞
X
k=1
1
k!Kk
T ∥W ′ −W ′′∥T
< ∞.
Hence as k →∞, F (k) (W ′) converges to a limit in W+
T , which is a ﬁxed point of F. The unique-
ness of a ﬁxed point follows from the above estimate, since if W ′ and W ′′ are ﬁxed points then
∥W ′ −W ′′∥T =
F (k) (W ′) −F (k) (W ′′)

T ≤1
k!Kk
T ∥W ′ −W ′′∥T ,
while one can take k arbitrarily large. This proves that the solution exists and is unique on t ∈
[0, T]. Since T is arbitrary, we have existence and uniqueness of the solution on the time interval
[0, ∞).
15

Published as a conference paper at ICLR 2021
C
CONNECTION BETWEEN THE NEURAL NET AND ITS MF LIMIT: PROOFS
FOR SECTION 3
C.1
PROOF OF THEOREM 3
We construct an auxiliary trajectory, which we call the particle ODEs:
∂
∂t ˜w3 (t, j2) = −ξ3 (t) EZ
h
∂2L

Y, ˆy

X; ˜W (t)

ϕ′
3

H3

X; ˜W (t)

ϕ2

H2

X, j2; ˜W (t)
i
,
∂
∂t ˜w2 (t, j1, j2) = −ξ2 (t) EZ
h
∆H
2

Z, j2; ˜W (t)

ϕ1 (⟨˜w1 (t, j1) , X⟩)
i
,
∂
∂t ˜w1 (t, j1) = −ξ1 (t) EZ

1
n2
n2
X
j2=1
∆H
2

Z, j2; ˜W (t)

˜w2 (t, j1, j2) ϕ′
1 (⟨˜w1 (t, j1) , X⟩) X

,
in which j1 = 1, ..., n1, j2 = 1, ..., n2, ˜W (t) = ( ˜w1 (t, ·) , ˜w2 (t, ·, ·) , ˜w3 (t, ·)), and t ∈R≥0. We
specify the initialization ˜W (0): ˜w1 (0, j1) = w0
1 (C1 (j1)), ˜w2 (0, j1, j2) = w0
2 (C1 (j1) , C2 (j2))
and ˜w3 (0, j3) = w0
3 (C2 (j2)). That is, it shares the same initialization with the neural network
one W (0), and hence is coupled with the neural network and the MF ODEs. Roughly speaking,
the particle ODEs are continuous-time trajectories of ﬁnitely many neurons, averaged over the data
distribution. We note that ˜W (t) is random for all t ∈R≥0 due to the randomness of {Ci (ji)}i=1,2.
The existence and uniqueness of the solution to the particle ODEs follows from the same proof as
in Theorem 1, which we shall not repeat here. We equip ˜W (t) with the norm
9 ˜W9T = max

max
j1≤n1, j2≤n2 sup
t≤T
| ˜w2 (t, j1, j2)| , max
j2≤n2 sup
t≤T
| ˜w3 (t, j2)|

.
One can also deﬁne the measures DT

W, ˜W

and DT

˜W, W

similar to Eq. (2):
DT

W, ˜W

= sup

|w1 (t, C1 (j1)) −˜w1 (t, C1 (j1))| , |w2 (t, C1 (j1) , C2 (j2)) −˜w2 (t, C1 (j1) , C2 (j2))| ,
|w3 (t, C2 (j2)) −˜w3 (t, C2 (j2))| : t ≤T, j1 ≤n1, j2 ≤n2
	
,
DT

˜W, W

= sup

|w1 (⌊t/ϵ⌋, j1) −˜w1 (t, C1 (j1))| , |w2 (⌊t/ϵ⌋, j1, j2) −˜w2 (t, C1 (j1) , C2 (j2))| ,
|w3 (⌊t/ϵ⌋, j2) −˜w3 (t, C2 (j2))| : t ≤T, j1 ≤n1, j2 ≤n2
	
.
We have the following results:
Theorem 14. Under the same setting as Theorem 3, for any δ > 0, with probability at least 1 −δ,
DT

W, ˜W

≤
1
√nmin
log1/2
3 (T + 1) n2
max
δ
+ e

eKT ,
in which nmin = min {n1, n2}, nmax = max {n1, n2}, and KT = K
 1 + T K
.
Theorem 15. Under the same setting as Theorem 3, for any δ > 0 and ϵ ≤1, with probability at
least 1 −δ,
DT

˜W, W

≤
s
ϵ log
2n1n2
δ
+ e

eKT ,
in which KT = K
 1 + T K
.
Proof of Theorem 3. Using the fact
DT (W, W) ≤DT

W, ˜W

+ DT

˜W, W

,
the thesis is immediate from Theorems 14 and 15.
16

Published as a conference paper at ICLR 2021
C.2
PROOF OF THEOREMS 14 AND 15
Proof of Theorem 14. In the following, let Kt denote an generic positive constant that may change
from line to line and takes the form
Kt = K
 1 + tK
,
such that Kt ≥1 and Kt ≤KT for all t ≤T. We ﬁrst note that at initialization, D0

W, ˜W

= 0.
Since 9W90 ≤K, 9W9T ≤KT by Lemma 11. Furthermore it is easy to see that 9 ˜W90 ≤
9W90 ≤K almost surely. By the same argument as in Lemma 11, 9 ˜W9T ≤KT almost surely.
We shall use all above bounds repeatedly in the proof. We decompose the proof into several steps.
Step 1 - Main proof.
Let us deﬁne, for brevity
q3 (t, x) = H3

x; ˜W (t)

−H3 (x; W (t)) ,
q2 (t, x, j2, c2) = H2

x, j2; ˜W (t)

−H2 (x, c2; W (t)) ,
q∆(t, z, j1, j2, c1, c2) = ∆H
2

Z, j2; ˜W (t)

˜w2 (t, j1, j2) −∆H
2 (z, c2; W (t)) w2 (t, c1, c2) .
Consider t ≥0. We ﬁrst bound the difference in the updates between W and ˜W. Let us start with
w3 and ˜w3. By Assumption 1, we have:

∂
∂t ˜w3 (t, j2) −∂
∂tw3 (t, C2 (j2))
 ≤KEZ [|q3 (t, X)| + |q2 (t, X, j2, C2 (j2))|] .
Similarly, for w2 and ˜w2,

∂
∂t ˜w2 (t, j1, j2) −∂
∂tw2 (t, C1 (j1) , C2 (j2))

≤KEZ
h∆H
2

Z, j2; ˜W (t)

−∆H
2 (Z, C2 (j2) ; W (t))

i
+ K |w3 (t, C2 (j2))| | ˜w1 (t, j1) −w1 (t, C1 (j1))|
≤KtEZ [|q3 (t, X)| + |q2 (t, X, j2, C2 (j2))|]
+ Kt (| ˜w1 (t, j1) −w1 (t, C1 (j1))| + | ˜w3 (t, j2) −w3 (t, C2 (j2))|)
≤KtEZ [|q3 (t, X)| + |q2 (t, X, j2, C2 (j2))|] + KtDt

W, ˜W

,
and for w1 and ˜w1, by Lemma 12,

∂
∂t ˜w1 (t, j1) −∂
∂tw1 (t, C1 (j1))

≤KEZ



1
n2
n2
X
j2=1
EC2 [q∆(t, Z, j1, j2, C1 (j1) , C2)]



+ EC2
∆H
2 (Z, C2; W (t))
 |w2 (t, C1 (j1) , C2)|

| ˜w1 (t, j1) −w1 (t, C1 (j1))|
≤KEZ



1
n2
n2
X
j2=1
EC2 [q∆(t, Z, j1, j2, C1 (j1) , C2)]



+ KtDt

W, ˜W

.
To further the bounding, we now make the following two claims:
• Claim 1: For any ξ > 0,
max
j2≤n2

∂
∂tw3 (t + ξ, C2 (j2)) −∂
∂tw3 (t, C2 (j2))
 ≤Kt+ξξ,
17

Published as a conference paper at ICLR 2021
max
j1≤n1, j2≤n2

∂
∂tw2 (t + ξ, C1 (j1) , C2 (j2)) −∂
∂tw2 (t, C1 (j1) , C2 (j2))
 ≤Kt+ξξ,
max
j1≤n1

∂
∂tw1 (t + ξ, C1 (j1)) −∂
∂tw1 (t, C1 (j1))
 ≤Kt+ξξ,
and similarly,
max
j2≤n2

∂
∂t ˜w3 (t + ξ, j2) −∂
∂t ˜w3 (t, j2)
 ≤Kt+ξξ,
max
j1≤n1, j2≤n2

∂
∂t ˜w2 (t + ξ, j1, j2) −∂
∂t ˜w2 (t, j1, j2)
 ≤Kt+ξξ,
max
j1≤n1

∂
∂t ˜w1 (t + ξ, j1) −∂
∂t ˜w1 (t, j1)
 ≤Kt+ξξ.
• Claim 2: For any γ1, γ2, γ3 > 0 and t ≥0,
max
(
max
j2≤n2 EZ [|q2 (t, X, j2, C2 (j2))|] ,
EZ [|q3 (t, X)|] ,
max
j1≤n1 EZ



1
n2
n2
X
j2=1
EC2 [q∆(t, Z, j1, j2, C1 (j1) , C2)]



)
≥Kt

Dt

W, ˜W

+ γ1 + γ2 + γ3

,
with probability at most
n1
γ1
exp

−n2γ2
1
Kt

+ n2
γ2
exp

−n1γ2
2
Kt

+ 1
γ3
exp

−n2γ2
3
Kt

.
Combining these claims with the previous bounds,
taking a union bound over t
∈
{0, ξ, 2ξ, ..., ⌊T/ξ⌋ξ} for some ξ ∈(0, 1), we obtain that
max

max
j2≤n2

∂
∂t ˜w3 (t, j2) −∂
∂tw3 (t, C2 (j2))
 ,
max
j1≤n1, j2≤n2

∂
∂t ˜w2 (t, j1, j2) −∂
∂tw2 (t, C1 (j1) , C2 (j2))
 ,
max
j1≤n1

∂
∂t ˜w1 (t, j1) −∂
∂tw1 (t, C1 (j1))


≤KT

Dt

W, ˜W

+ γ1 + γ2 + γ3 + ξ

,
∀t ∈[0, T] ,
with probability at least
1 −T + 1
ξ
n1
γ1
exp

−n2γ2
1
KT

+ n2
γ2
exp

−n1γ2
2
KT

+ 1
γ3
exp

−n2γ2
3
KT

.
The above event in turn implies
Dt

W, ˜W

≤KT
Z t
0

Ds

W, ˜W

+ γ1 + γ2 + γ3 + ξ

ds,
and hence by Gronwall’s lemma and the fact D0

W, ˜W

= 0, we get
DT

W, ˜W

≤(γ1 + γ2 + γ3 + ξ) eKT .
The theorem then follows from the choice
ξ =
1
√nmax
,
γ2 = KT
√n1
log1/2
3 (T + 1) n2
max
δ
+ e

,
γ1 = γ3 = KT
√n2
log1/2
3 (T + 1) n2
max
δ
+ e

.
We are left with proving the claims.
18

Published as a conference paper at ICLR 2021
Step 2 - Proof of Claim 1.
We have from Assumption 1,
ess-sup |w3 (t + ξ, C2) −w3 (t, C2)| ≤K
Z t+ξ
t
ess-sup

∂
∂tw3 (s, C2)
 ds
≤Kξ,
ess-sup |w2 (t + ξ, C1, C2) −w2 (t, C1, C2)| ≤K
Z t+ξ
t
ess-sup

∂
∂tw2 (s, C1, C2)
 ds
≤K
Z t+ξ
t
ess-sup |w3 (s, C2)| ds
≤Kt+ξξ,
ess-sup |w1 (t + ξ, C1) −w1 (t, C1)| ≤K
Z t+ξ
t
ess-sup

∂
∂tw1 (s, C1)
 ds
≤K
Z t+ξ
t
ess-sup |w3 (s, C2) w2 (s, C1, C2)| ds
≤Kt+ξξ.
By Lemma 12, we then obtain that
ess-supEZ [|H2 (X, C2; W (t + ξ)) −H2 (X, C2; W (t))|] ≤Kt+ξξ,
EZ [|H3 (X; W (t + ξ)) −H3 (X; W (t))|] ≤Kt+ξξ,
ess-supEZ
∆H
2 (Z, C2; W (t + ξ)) −∆H
2 (Z, C2; W (t))

≤Kt+ξξ.
Using these estimates, we thus have, by Assumption 1,
max
j2≤n2

∂
∂tw3 (t + ξ, C2 (j2)) −∂
∂tw3 (t, C2 (j2))

≤Kt+ξξ + KEZ [|H3 (X; W (t + ξ)) −H3 (X; W (t))|]
+ Kess-supEZ [|H2 (X, C2; W (t + ξ)) −H2 (X, C2; W (t))|]
≤Kt+ξξ,
max
j1≤n1, j2≤n2

∂
∂tw2 (t + ξ, C1 (j1) , C2 (j2)) −∂
∂tw2 (t, C1 (j1) , C2 (j2))

≤Kt+ξξ + Kess-supEZ
∆H
2 (Z, C2; W (t + ξ)) −∆H
2 (Z, C2; W (t))

+ Kess-sup |w3 (t, C2)| |w1 (t + ξ, C1) −w1 (t, C1)|
≤Kt+ξξ,
max
j1≤n1

∂
∂tw1 (t + ξ, C1 (j1)) −∂
∂tw1 (t, C1 (j1))

≤Kt+ξξ + Kess-supEZ

EC2
∆H
2 (Z, C2; W (t + ξ)) −∆H
2 (Z, C2; W (t))
 |w2 (t, C1, C2)|

+ Kess-supEC2 [|w3 (t, C2)| |w2 (t + ξ, C1, C2) −w2 (t, C1, C2)|]
+ Kess-supEC2 [|w3 (t, C2) w2 (t, C1, C2)|] |w1 (t + ξ, C1) −w1 (t, C1)|
≤Kt+ξξ.
The proof of the rest of the claim is similar.
Step 3 - Proof of Claim 2.
We recall the deﬁnitions of q∆, q2 and q3. Let us decompose them as
follows. We start with q2:
|q2 (t, x, j2, C2 (j2))|
=

1
n1
n1
X
j1=1
˜w2 (t, j1, j2) ϕ1 (⟨˜w1 (t, j1) , x⟩) −EC1 [w2 (t, C1, C2 (j2)) ϕ1 (⟨w1 (t, C1) , x⟩)]

≤max
j1≤n1 | ˜w2 (t, j1, j2) ϕ1 (⟨˜w1 (t, j1) , x⟩) −w2 (t, C1 (j1) , C2 (j2)) ϕ1 (⟨w1 (t, C1 (j1)) , x⟩)|
19

Published as a conference paper at ICLR 2021
+

1
n1
n1
X
j1=1
w2 (t, C1 (j1) , C2 (j2)) ϕ1 (⟨w1 (t, C1 (j1)) , x⟩) −EC1 [w2 (t, C1, C2 (j2)) ϕ1 (⟨w1 (t, C1) , x⟩)]

≡Q2,1 (x, j2) + Q2,2 (x, j2) .
Similarly, we have for q3:
|q3 (t, x)|
=

1
n2
n2
X
j2=1
˜w3 (t, j2) ϕ2

H2

x, j2; ˜W (t)

−EC2 [w3 (t, C2) ϕ2 (H2 (x, C2; W (t)))]

≤max
j2≤n2
 ˜w3 (t, j2) ϕ2

H2

x, j2; ˜W (t)

−w3 (t, C2 (j2)) ϕ2 (H2 (x, C2 (j2) ; W (t)))

+

1
n2
n2
X
j2=1
w3 (t, C2 (j2)) ϕ2 (H2 (x, C2 (j2) ; W (t))) −EC2 [w3 (t, C2) ϕ2 (H2 (x, C2; W (t)))]

≡Q3,1 (x) + Q3,2 (x) .
Finally we have for q∆:

1
n2
n2
X
j2=1
EC2 [q∆(t, z, j1, j2, C1 (j1) , C2)]

≤max
j2≤n2
∆H
2

z, j2; ˜W (t)

˜w2 (t, j1, j2) −∆H
2 (z, C2 (j2) ; W (t)) w2 (t, C1 (j1) , C2 (j2))

+

1
n2
n2
X
j2=1
∆H
2 (z, C2 (j2) ; W (t)) w2 (t, C1 (j1) , C2 (j2)) −EC2

∆H
2 (z, C2; W (t)) w2 (t, C1 (j1) , C2)


≡Q1,1 (z, j1) + Q1,2 (z, j1) .
Now let us analyze each of the terms.
• We start with Q2,1. We have from Assumption 1,
max
j2≤n2 EZ [Q2,1 (X, j2)]
≤K
max
j1≤n1, j2≤n2 | ˜w2 (t, j1, j2) −w2 (t, C1 (j1) , C2 (j2))|
+ K
max
j1≤n1, j2≤n2 |w2 (t, C1 (j1) , C2 (j2))| | ˜w1 (t, j1) −w1 (t, C1 (j1))|
≤KtDt

W, ˜W

.
• To bound Q2,2, let us write:
Z2 (x, c1, c2) = w2 (t, c1, c2) ϕ1 (⟨w1 (t, c1) , x⟩) .
Recall that C1 (j1) and C2 (j2) are independent. We thus have:
E [Z2 (X, C1 (j1) , C2 (j2))|X, C2 (j2)] = EC1 [Z2 (X, C1, C2 (j2))] .
Furthermore {Z2 (C1 (j1) , C2 (j2))}j1∈[n1] are independent, conditional on C2 (j2). We
also have, almost surely
|Z2 (X, C1 (j1) , C2 (j2))| ≤Kt,
by Assumption 1. Then by Lemma 19,
P (EZ [Q2,2 (X, j2)] ≥Ktγ2) ≤(1/γ2) exp
 −n1γ2
2/Kt

.
20

Published as a conference paper at ICLR 2021
• To bound Q3,1, we have from Assumption 1,
EZ [Q3,1 (X)] ≤max
j2≤n2 (K | ˜w3 (t, j2) −w3 (t, C2 (j2))| + KtEZ [|q2 (t, X, j2, C2 (j2))|])
≤KDt

W, ˜W

+ Kt max
j2≤n2 EZ [|q2 (t, X, j2, C2 (j2))|] .
• To bound Q3,2, noticing that almost surely
|w3 (t, C2 (j2)) ϕ2 (H2 (x, C2 (j2) ; W (t)))| ≤Kt
by Assumption 1, we obtain
P (EZ [Q3,2 (X)] ≥Ktγ3) ≤(1/γ3) exp
 −n2γ2
3/Kt

,
similar to the treatment of Q2,2.
• To bound Q1,1, using Assumption 1,
EZ [Q1,1 (Z, j1)] ≤K max
j2≤n2 |w2 (t, C1 (j1) , C2 (j2))| EZ
h∆H
2

Z, j2; ˜W (t)

−∆H
2 (Z, C2 (j2) ; W (t))

i
+ K max
j2≤n2 | ˜w2 (t, j1, j2) −w2 (t, C1 (j1) , C2 (j2))| EZ
h∆H
2

Z, j2; ˜W (t)

i
≤K max
j2≤n2 |w2 (t, C1 (j1) , C2 (j2))|

| ˜w3 (t, j2) −w3 (t, C2 (j2))|
+ |w3 (t, C2 (j2))| EZ [|q3 (t, X)| + |q2 (t, X, j2, C2 (j2))|]

+ K max
j2≤n2 | ˜w2 (t, j1, j2) −w2 (t, C1 (j1) , C2 (j2))| | ˜w3 (t, j2)|
≤Kt

Dt

W, ˜W

+ EZ

|q3 (t, X)| + max
j2≤n2 |q2 (t, X, j2, C2 (j2))|

.
• To bound Q1,2, we note that almost surely
∆H
2 (Z, C2 (j2) ; W (t)) w2 (t, C1 (j1) , C2 (j2))
 ≤K |w3 (t, C2 (j2))| |w2 (t, C1 (j1) , C2 (j2))|
≤Kt.
Then similar to the bounding of Q2,2, we get:
P (EZ [Q1,2 (Z, j1)] ≥Ktγ1) ≤(1/γ1) exp
 −n2γ2
1/Kt

.
Finally, combining all of these bounds together, applying suitably the union bound over j1 ∈[n1]
and j2 ∈[n2], we obtain the claim.
Proof of Theorem 15. We consider t ≤T, for a given terminal time T ∈ϵN≥0. We again reuse the
notation Kt from the proof of Theorem 3. Note that Kt ≤KT for all t ≤T. We also note that at
initialization, D0

W, ˜W

= 0. We also recall from the proof of Theorem 3 that 9 ˜W9T ≤KT
almost surely.
For brevity, let us deﬁne several quantities that relate to the difference in the gradient updates be-
tween W and ˜W:
q3 (k, z, ˜z, j2) = ∂2L (y, ˆy (x; W (k))) ϕ′
3 (H3 (x; W (k))) ϕ2 (H2 (x, j2; W (k)))
−∂2L

˜y, ˆy

˜x; ˜W (kϵ)

ϕ′
3

H3

˜x; ˜W (kϵ)

ϕ2

H2

˜x, j2; ˜W (kϵ)

,
r3 (k, z, j2) = ξ3 (kϵ) ∂2L

y, ˆy

x; ˜W (kϵ)

ϕ′
3

H3

x; ˜W (kϵ)

ϕ2

H2

x, j2; ˜W (kϵ)

−ξ3 (kϵ) EZ
h
∂2L

Y, ˆy

X; ˜W (kϵ)

ϕ′
3

H3

X; ˜W (kϵ)

ϕ2

H2

X, j2; ˜W (kϵ)
i
,
q2 (k, z, ˜z, j1, j2) = ∆H
2 (z, j2; W (k)) ϕ1 (⟨w1 (k, j1) , x⟩)
21

Published as a conference paper at ICLR 2021
−∆H
2

˜z, j2; ˜W (kϵ)

ϕ1 (⟨˜w1 (kϵ, j1) , ˜x⟩) ,
r2 (k, z, j1, j2) = ξ2 (kϵ) ∆H
2

z, j2; ˜W (kϵ)

ϕ1 (⟨˜w1 (kϵ, j1) , x⟩)
−ξ2 (kϵ) EZ
h
∆H
2

Z, j2; ˜W (kϵ)

ϕ1 (⟨˜w1 (kϵ, j1) , X⟩)
i
,
q1 (k, z, ˜z, j1) = 1
n2
n2
X
j2=1
∆H
2 (z, j2; W (k)) w2 (k, j1, j2) ϕ′
1 (⟨w1 (k, j1) , x⟩) x
−1
n2
n2
X
j2=1
∆H
2

˜z, j2; ˜W (kϵ)

˜w2 (kϵ, j1, j2) ϕ′
1 (⟨˜w1 (kϵ, j1) , ˜x⟩) ˜x,
r1 (k, z, j1) = ξ1 (kϵ) 1
n2
n2
X
j2=1
∆H
2

z, j2; ˜W (kϵ)

˜w2 (kϵ, j1, j2) ϕ′
1 (⟨˜w1 (kϵ, j1) , x⟩) x
−ξ1 (kϵ) EZ

1
n2
n2
X
j2=1
∆H
2

Z, j2; ˜W (kϵ)

˜w2 (kϵ, j1, j2) ϕ′
1 (⟨˜w1 (kϵ, j1) , X⟩) X

.
Let us also deﬁne:
qH
3 (k, x) = H3 (x; W (k)) −H3

x; ˜W (kϵ)

,
qH
2 (k, x, j2) = H2 (x, j2; W (k)) −H2

x, j2; ˜W (kϵ)

.
We proceed in several steps.
Step 1: Decomposition.
As shown in the proof of Theorem 3:
max
j2≤n2

∂
∂t ˜w3 (t + ξ, j2) −∂
∂t ˜w3 (t, j2)
 ≤Kt+ξξ,
max
j1≤n1, j2≤n2

∂
∂t ˜w2 (t + ξ, j1, j2) −∂
∂t ˜w2 (t, j1, j2)
 ≤Kt+ξξ,
max
j1≤n1

∂
∂t ˜w1 (t + ξ, j1) −∂
∂t ˜w1 (t, j1)
 ≤Kt+ξξ.
for any t ≥0 and ξ ≥0. These time-interpolation estimates, along with Assumption 1, allow to
derive the following. We ﬁrst have:
max
j2≤n2 |w3 (⌊t/ϵ⌋, j2) −˜w3 (t, j2)|
≤K max
j2≤n2

ϵ
⌊t/ϵ⌋−1
X
k=0
ξ3 (kϵ) EZ [q3 (k, z (k) , Z, j2)]

+ tKtϵ
≤K max
j2≤n2 [Q3,1 (⌊t/ϵ⌋, j2) + Q3,2 (⌊t/ϵ⌋, j2)] + tKtϵ,
where we deﬁne
Q3,1 (⌊t/ϵ⌋, j2) = ϵ
⌊t/ϵ⌋−1
X
k=0
|q3 (k, z (k) , z (k) , j2)| ,
Q3,2 (⌊t/ϵ⌋, j2) =

ϵ
⌊t/ϵ⌋−1
X
k=0
r3 (k, z (k) , j2)

.
(Here P⌊t/ϵ⌋−1
k=0
= 0 if ⌊t/ϵ⌋= 0.) We have similarly:
max
j1≤n1 |w1 (⌊t/ϵ⌋, j1) −˜w1 (t, j1)| ≤K max
j1≤n1 [Q1,1 (⌊t/ϵ⌋, j1) + Q1,2 (⌊t/ϵ⌋, j1)] + tKtϵ,
22

Published as a conference paper at ICLR 2021
max
j1≤n1, j2≤n2 |w2 (⌊t/ϵ⌋, j1, j2) −˜w2 (t, j1, j2)| ≤K
max
j1≤n1, j2≤n2 [Q2,1 (⌊t/ϵ⌋, j1, j2) + Q2,2 (⌊t/ϵ⌋, j1, j2)] + tKtϵ,
in which
Q1,1 (⌊t/ϵ⌋, j1) = ϵ
⌊t/ϵ⌋−1
X
k=0
|q1 (k, z (k) , z (k) , j1)| ,
Q1,2 (⌊t/ϵ⌋, j1) =

ϵ
⌊t/ϵ⌋−1
X
k=0
r1 (k, z (k) , j1)

,
Q2,1 (⌊t/ϵ⌋, j1, j2) = ϵ
⌊t/ϵ⌋−1
X
k=0
|q2 (k, z (k) , z (k) , j1, j2)| ,
Q2,2 (⌊t/ϵ⌋, j1, j2) =

ϵ
⌊t/ϵ⌋−1
X
k=0
r2 (k, z (k) , j1, j2)

.
The task is now to bound Q1,1, Q1,2, Q2,1, Q2,2, Q3,1 and Q3,2.
Step 2: Bounding the terms.
Before we proceed, let us give some bounds for qH
3 and qH
2 , which
hold for any x ∈Rd:
qH
2 (k, x, j2)
 ≤K max
j1≤n1 (| ˜w2 (kϵ, j1, j2)| |w1 (k, j1) −˜w1 (kϵ, j1)| + |w2 (k, j1, j2) −˜w2 (kϵ, j1, j2)|)
≤KkϵDkϵ

˜W, W

,
qH
3 (k, x)
 ≤K max
j2≤n2
 |w3 (k, j2) −˜w3 (kϵ, j2)| + | ˜w3 (kϵ, j2)|
qH
2 (k, x, j2)

≤KkϵDkϵ

˜W, W

.
With these, we have the following:
• Let us bound Q3,1. By Assumption 1,
|q3 (k, z (k) , z (k) , j2)| ≤K
 qH
2 (k, x (k) , j2) +
qH
3 (k, x (k))

.
We then get:
max
j2≤n2 Q3,1 (⌊t/ϵ⌋, j2) ≤Ktϵ
⌊t/ϵ⌋−1
X
k=0
Dkϵ

˜W, W

.
• Similarly to Q3,1, we consider Q2,1:
|q2 (k, z (k) , z (k) , j1, j2)| ≤K
∆H
2 (z (k) , j2; W (k)) −∆H
2

z (k) , j2; ˜W (kϵ)

+ K
∆H
2

z (k) , j2; ˜W (kϵ)
 |w1 (k, j1) −˜w1 (kϵ, j1)|
≤Kkϵ
 qH
2 (k, x (k) , j2)
 +
qH
3 (k, x (k))

+ K |w3 (k, j2) −˜w3 (kϵ, j2)| + Kkϵ |w1 (k, j1) −˜w1 (kϵ, j1)| ,
which yields
max
j1≤n1, j2≤n2 Q2,1 (⌊t/ϵ⌋, j1, j2) ≤Ktϵ
⌊t/ϵ⌋−1
X
k=0
Dkϵ

˜W, W

.
• Again we get a similar bound for Q1,1:
|q1 (k, z (k) , z (k) , j1)| ≤K
n2
n2
X
j2=1
| ˜w2 (kϵ, j1, j2)|
∆H
2 (z (k) , j2; W (k)) −∆H
2

z (k) , j2; ˜W (kϵ)

23

Published as a conference paper at ICLR 2021
+ K
n2
n2
X
j2=1
∆H
2

z (k) , j2; ˜W (kϵ)
 |w2 (k, j1, j2) −˜w2 (kϵ, j1, j2)|
+ K
n2
n2
X
j2=1
| ˜w2 (kϵ, j1, j2)|
∆H
2

z (k) , j2; ˜W (kϵ)
 |w1 (k, j1) −˜w1 (kϵ, j1)|
≤Kkϵ

max
j2≤n2
qH
2 (k, x (k) , j2)
 +
qH
3 (k, x (k))


+ KkϵDkϵ

˜W, W

,
which yields
max
j1≤n1 Q1,1 (⌊t/ϵ⌋, j1) ≤Ktϵ
⌊t/ϵ⌋−1
X
k=0
Dkϵ

˜W, W

.
• Let us bound Q3,2. Let us deﬁne:
r3 (k, j2) =
k−1
X
ℓ=0
r3 (k, z (k) , j2) ,
r3 (0, j2) = 0.
Let Fk be the sigma-algebra generated by {z (ℓ) : ℓ∈{0, ..., k −1}}.
Note that
{r3 (k, j2)}k∈N is a martingale adapted to {Fk}k∈N. Furthermore, for k ≤T/ϵ, the mar-
tingale difference is bounded: |r3 (k, z (k) , j2)| ≤K by Assumption 1. Therefore, by
Theorem 20 and the union bound, we have:
P

max
j2≤n2
max
ℓ∈{0,1,...,T/ϵ} Q3,2 (ℓ, j2) ≥ξ

≤2n2 exp

−
ξ2
K (T + 1) ϵ

.
• The bounding of Q2,2 is similar: |r2 (k, z (k) , j1, j2)| ≤Kkϵ almost surely by Assumption
1, and thus
P

max
j1≤n1, j2≤n2
max
ℓ∈{0,1,...,T/ϵ} Q2,2 (ℓ, j1, j2) ≥ξ

≤2n1n2 exp

−
ξ2
KT (T + 1) ϵ

.
• Again the bounding of Q1,2 is also similar: |r1 (k, z (k) , j1)| ≤Kkϵ almost surely by
Assumption 1, and thus
P

max
j1≤n1
max
ℓ∈{0,1,...,T/ϵ} Q1,2 (ℓ, j1) ≥ξ

≤2n1 exp

−
ξ2
KT (T + 1) ϵ

.
Step 3: Putting everything together.
All the above results give us
D⌊t/ϵ⌋ϵ

˜W, W

≤KT ϵ
⌊t/ϵ⌋−1
X
k=0
Dkϵ

˜W, W

+ ξ + TKT ϵ
∀t ≤T,
which hold with probability at least
1 −2n1n2 exp

−
ξ2
KT (T + 1) ϵ

.
The above event implies, by Gronwall’s lemma,
DT

˜W, W

≤(ξ + ϵ) eKT .
Choosing ξ = KT
p
(T + 1) ϵ log (2n1n2/δ) completes the proof.
C.3
PROOFS OF COROLLARIES 4 AND 5
Proof of Corollary 4. By the assumption on ψ and Assumption 1, we have:
|EZ [ψ (Y, ˆy (X; W (⌊t/ϵ⌋))) −ψ (Y, ˆy (X; W (t)))]|
24

Published as a conference paper at ICLR 2021
≤KEZ [|H3 (X; W (⌊t/ϵ⌋)) −H3 (X; W (t))|]
≤KEZ [|H3 (X; W (t)) −H3 (X; W (⌊t/ϵ⌋ϵ))|] + KEZ [|H3 (X; W (⌊t/ϵ⌋)) −H3 (X; W (⌊t/ϵ⌋ϵ))|] .
An inspection of the proof of Theorem 3 (in particular, the proofs of Theorems 14 and 15) reveals
that ﬁrstly,
sup
t≤T
EZ [|H3 (X; W (t)) −H3 (X; W (⌊t/ϵ⌋ϵ))|] ≤KT ϵ,
and secondly,
sup
t≤T
EZ [|H3 (X; W (⌊t/ϵ⌋ϵ)) −H3 (X; W (⌊t/ϵ⌋))|]
≤KT DT (W, W) +
1
√nmin
log1/2
3Tn2
max
δ
+ e

eKT
with probability at least 1 −δ. Together with Theorem 3, we obtain the claim.
Proof of Corollary 5. Observe that for each index {N1, N2} of Init, one obtains a neural network
initialization W(0) with law ρ by setting
w1(0, j1) = w1(0, C1(j1)), w2(0, j1, j2) = w2 (0, C1 (j1) , C2 (j2)) ,
w3(0, j2) = w3 (0, C2 (j2)) , j1 ∈[N1] , j2 ∈[N2] .
We consider the evolution W (k) starting from W (0), which is independent of W. Note that
W (k) is a deterministic function of its initialization W (0) and the data {z (j)}j≤k. Similarly, we
consider the counterpart for ˆW: the evolution ˆ
W (k) as a function of the initialization ˆ
W (0) and
the data {ˆz (j)}j≤k. Due to sharing the same distribution for both the initialization and the data,
these evolutions have the same law; to be speciﬁc, W (n1, n2, T) and ˆ
W (n1, n2, T) has the same
distribution for any n1, n2 and T, where we deﬁne
W (n1, n2, T) =

w1 (k, j1) , w2 (k, j1, j2) , w3 (k, j2) :
j1 ∈[n1] , j2 ∈[n2] , k ≤⌊T/ϵ⌋
	
,
and a similar deﬁnition for ˆ
W (n1, n2, T). In other words,
W

W, ˆ
W

≡
inf
coupling of (W, ˆ
W)
E

max
k≤⌊T/ϵ⌋, j1≤n1, j2≤n2
n
|w1 (k, j1) −ˆw1 (k, j1)| ,
|w2 (k, j1, j2) −ˆw2 (k, j1, j2)| , |w3 (k, j2) −ˆw3 (k, j2)|
o
= 0.
Theorem 3 implies that for any tuple {n1, n2} such that n1 ≤N1 and n2 ≤N2, with probability at
least 1 −2δ,
D(n1,n2)
T
(W, W) ≡max

sup
t≤T, j1≤n1, j2≤n2
|w2 (t, j1, j2) −w2 (t, C1 (j1) , C2 (j2))| ,
sup
t≤T, j2≤n2
|w3 (t, j2) −w3 (t, C2 (j2))| ,
sup
t≤T, j1≤n1
|w1 (t, j1) −w1 (t, C1 (j1))|

≤˜Oδ,T (ϵ, N1, N2) ,
where ˜Oδ,T (ϵ, N1, N2) →0 as ϵ →0 and N −1
min log Nmax →0 with Nmin = min {N1, N2} and
Nmax = max {N1, N2}. We also have a similar result for ˆ
W and ˆW. As such, with probability at
least 1 −4δ,
W

W, ˆ
W

≡
inf
coupling of (W, ˆ
W)
E

sup
t≤T, j1≤n1, j2≤n2
n w1 (t, C1 (j1)) −ˆw1

t, ˆC1 (j1)
 ,
w2 (t, C1 (j1) , C2 (j2)) −ˆw2

t, ˆC1 (j1) , ˆC2 (j2)
 ,
w3 (t, C2 (j2)) −ˆw3

t, ˆC2 (j2)

o
≤D(n1,n2)
T
(W, W) + D(n1,n2)
T

ˆW, ˆ
W

+ W

W, ˆ
W

≤2 ˜Oδ,T (ϵ, N1, N2) .
By ﬁxing the tuple {n1, n2} while letting ϵ →0, N −1
min log Nmax →0 and δ →0, we obtain the
claim.
25

Published as a conference paper at ICLR 2021
D
GLOBAL CONVERGENCE: PROOFS FOR SECTION 4
D.1
PROOF OF PROPOSITION 7
Proof of Proposition 7. Consider a probability space (Λ, G, P0) with random processes Rd-valued
p1 (θ1), R-valued p2 (θ1, θ2) and R-valued p3 (θ2), which are indexed by (θ1, θ2) ∈[0, 1] × [0, 1],
such that the following holds.
Let m1 and m2 be two arbitrary ﬁnite positive integers and,
with these integers, let
n
θ(ki)
i
∈[0, 1] : ki ∈[mi] , i = 1, 2
o
be an arbitrary collection. For each
i = 1, 2, let Si be the set of unique elements in
n
θ(ki)
i
: ki ∈[mi]
o
. Similarly, let R2 be the set
of unique pairs in
n
θ(k1)
1
, θ(k2)
2

: k1 ∈[m1] , k2 ∈[m2]
o
. We have that {p1 (θ1) : θ1 ∈S1},
{p3 (θ2) : θ2 ∈S2}and {p2 (θ1, θ2) : (θ1, θ2) ∈R2} are all mutually independent. In addition, we
also have Law (p1 (θ1)) = ρ1, Law (p3 (θ2)) = ρ3 and Law (p2 (θ′
1, θ′
2)) = ρ2 for any θ1 ∈S1,
θ2 ∈S2 and (θ′
1, θ′
2) ∈R2. Such a space exists by Kolmogorov’s extension theorem.
We now construct the desired neuronal embedding. For i = 1, 2, consider Ωi = Λ × [0, 1] and
Fi = G × B ([0, 1]), equipped with the product measure P0 × Unif ([0, 1]) in which Unif ([0, 1])
is the uniform measure over [0, 1] equipped with the Borel sigma-algebra B ([0, 1]). We construct
Ω= Ω1 × Ω2 and F = F1 × F2, equipped with the product measure P = (P0 × Unif ([0, 1]))2.
Deﬁne the deterministic functions w0
1 : Ω1 →Rd, w0
2 : Ω1 × Ω2 →R and w0
3 : Ω2 →R:
w0
1 ((λ1, θ1)) = p1 (θ1) (λ1) ,
w0
2 ((λ1, θ1) , (λ2, θ2)) = p2 (θ1, θ2) (λ2) ,
w0
3 ((λ2, θ2)) = p3 (θ2) (λ2) .
It is easy to check that this construction yields the desired neuronal embedding.
D.2
PROOF OF THEOREM 8
We ﬁrst present a measurability argument, which is crucial to showing that a certain universal ap-
proximation property holds throughout the course of training.
Lemma 16 (Measurability argument). Consider a family Init of initialization laws, which are
 ρ1, ρ2, ρ3
-i.i.d., such that ρ2-almost surely |w2| ≤K and ρ3-almost surely |w3| ≤K. There
exists a neuronal embedding

Ω, F, P,

w0
i
	
i=1,2,3

of Init such that there exist Borel functions w∗
1
and ∆H∗
2
for which P-almost surely, for all t ≥0,
w1 (t, C1) = w∗
1
 t, w0
1 (C1)

,
∆H
2 (z, C2; W (t)) = ∆H∗
2
 t, z, w0
3 (C2)

,
where W (t) is the MF dynamics formed under the coupling procedure with this neuronal embedding
as described in Section 3.1. Furthermore,
∂
∂tw∗
1 (t, u1) = −ξ1 (t)
Z
EZ

∆H∗
2
(t, Z, u3) u2ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ρ2 (du2) ρ3 (du3)
+ ξ1 (t)
Z t
0
ξ2 (s) EZ,Z′
 Z
∆H∗
2
(t, Z, u3) ∆H∗
2
(s, Z′, u3) ρ3 (du3)
× ϕ1 (⟨w∗
1 (s, u1) , X′⟩) ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ds,
with initialization w∗
1 (0, u1) = u1 for all u1 ∈supp
 ρ1
and t ≥0, where Z′ is an independent
copy of Z.
Proof. We denote by Kt a constant that may depend on t and is ﬁnite with ﬁnite t. By Proposition
7, there exists a neuronal embedding that accommodates Init. We recall its construction and reuse
the notations from the proof of Proposition 7; in particular:
w0
1 ((λ1, θ1)) = p1 (θ1) (λ1) ,
26

Published as a conference paper at ICLR 2021
w0
2 ((λ1, θ1) , (λ2, θ2)) = p2 (θ1, θ2) (λ2) ,
w0
3 ((λ2, θ2)) = p3 (θ2) (λ2) .
Let
S1,
S3
and
S2
denote
the
sigma-algebras
generated
by
w0
1 (C1),
w0
3 (C2)
and
 w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)

respectively. Let S13 denote the sigma-algebra generated by S1
and S3. We also let SZ
1 to denote the sigma-algebra generated by S1 and the sigma-algebra of the
data Z. We deﬁne similarly for SZ
2 and SZ
3 .
Step 1: Reduced dynamics.
Given the MF dynamics W (t), let us deﬁne
¯∆3 (t, c2) = E [∆3 (C2; W (t))|S3] (c2) ,
¯∆2 (t, c1, c2) = E [∆2 (C1, C2; W (t))|S2] (c1, c2) ,
¯∆1 (t, c1) = E [∆1 (C1; W (t))|S1] (c1) .
We recall from the proof of Theorem 3 that for any t, s ≥0,
ess-sup |w3 (t, C2) −w3 (s, C2)| ≤K |t −s| ,
ess-sup |w2 (t, C1, C2) −w2 (s, C1, C2)| ≤Kt∨s |t −s| ,
ess-sup |w1 (t, C1) −w1 (s, C1)| ≤Kt∨s |t −s| .
Then by Lemma 13,
E
h ¯∆3 (t, C2) −¯∆3 (s, C2)
2i
≤Kt∨s |t −s|2 ,
E
h ¯∆2 (t, C1, C2) −¯∆2 (s, C1, C2)
2i
≤Kt∨s |t −s|2 ,
E
h ¯∆1 (t, C1) −¯∆1 (s, C1)
2i
≤Kt∨s |t −s|2 .
Therefore, by Kolmogorov continuity theorem, there exist continuous modiﬁcations of the (time-
indexed) processes ¯∆1, ¯∆2 and ¯∆3. We thus replace them with their continuous modiﬁcations,
written by the same notations.
Given these continuous modiﬁcations, we consider the following reduced dynamics:
∂
∂t ¯w3 (t, c2) = −ξ3 (t) ¯∆3 (t, c2) ,
∂
∂t ¯w2 (t, c1, c2) = −ξ2 (t) ¯∆2 (t, c1, c2) ,
∂
∂t ¯w1 (t, c1) = −ξ1 (t) ¯∆1 (t, c1) ,
in which:
• ¯w1 : R≥0 × Ω1 →Rd, ¯w2 : R≥0 × Ω1 × Ω2 7→R, ¯w3 : R≥0 × Ω3 7→R.
• ¯W (t) = { ¯w1 (t, ·) , ¯w2 (t, ·, ·) , ¯w3 (t, ·)} is the collection of reduced parameters at time t,
• the initialization is ¯w1 (0, ·) = w0
1 (·), ¯w2 (0, ·, ·) = w0
2 (·, ·) and ¯w3 (0, ·) = w0
3 (·), i.e.
¯W (0) = W (0).
Step 2: Measurability of the reduced dynamics.
It is easy to see that ¯w3 (t, C2) is S3-measurable
by its construction and the fact ¯w3 (0, C2) = w0
3 (C2) is S3-measurable. Similarly, ¯w2 (t, C1, C2) is
S2-measurable and ¯w1 (t, C1) is S1-measurable.
Notice that there exist Borel functions ¯w∗
1, ¯w∗
2 and ¯w∗
3 for which P-almost surely,
¯w1 (t, C1) = ¯w∗
1
 t, w0
1 (C1)

,
¯w2 (t, C1, C2) = ¯w∗
2
 t, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)

,
¯w3 (t, C2) = ¯w∗
3
 t, w0
3 (C2)

.
Indeed, since ¯w2 (t, C1, C2) is S2-measurable, there exists a function ¯w∗
2 (t, ·) for each rational t
such that the desired identity holds for P-almost every (C1, C2) and for all rational t ≥0. Since ¯w2
is continuous in time, there is a unique continuous (in time) function ¯w∗
2 (t, ·) such that the identity
holds for all t ≥0 and for P-almost every (C1, C2). The same argument yields the construction of
¯w∗
1 and ¯w∗
3.
27

Published as a conference paper at ICLR 2021
Step 3: Measurability of constituent quantities.
We show that H2
 X, C2; ¯W (t)

is SZ
3 -
measurable. Recall that
H2
 X, C2; ¯W (t)

= EC1 [ ¯w2 (t, C1, C2) ϕ1 (⟨¯w1 (t, C1) , X⟩)] .
By the existence of ¯w∗
1 and ¯w∗
2, for each t ≥0, there exists a Borel function ft such that almost
surely
¯w2 (t, C1, C2) ϕ1 (⟨¯w1 (t, C1) , X⟩) = ft
 X, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)

.
We recall that ρ1 and ρ2 are the laws of w0
1 (C1) and w0
2 (C1, C2). We analyze the following:
E
"H2
 X, C2; ¯W (t)

−
Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

2#
= E
hH2
 X, C2; ¯W (t)
2i
+ E
"
Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

2#
−2E

H2
 X, C2; ¯W (t)
 Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

.
Let us evaluate the ﬁrst term:
E
hH2
 X, C2; ¯W (t)
2i
= E
hEC1

ft
 X, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)
2i
(a)
= E

ft
 X, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)

ft
 X, w0
1 (C′
1) , w0
2 (C′
1, C2) , w0
3 (C2)

(b)
= E
h
ft (X, p1 (θ1) (λ1) , p2 (θ1, θ2) (λ2) , p3 (θ2) (λ2))
× ft (X, p1 (θ′
1) (λ′
1) , p2 (θ′
1, θ2) (λ2) , p3 (θ2) (λ2))
i
(c)
= E
h
ft (X, p1 (θ1) (λ1) , p2 (θ1, θ2) (λ2) , p3 (θ2) (λ2))
× ft (X, p1 (θ′
1) (λ′
1) , p2 (θ′
1, θ2) (λ2) , p3 (θ2) (λ2)) I (θ1 ̸= θ′
1)
i
(d)
= EZ
Z
ft (X, u1, u2, u3) ft (X, u′
1, u′
2, u3) ρ1 (du1) ρ1 (du′
1) ρ2 (du2) ρ2 (du′
2) ρ3 (du3)

,
where in step (a), we deﬁne C′
1 to be an independent copy of C1; in step (b), we recall C1 =
(λ1, θ1); in step (c), we recall θ1, θ′
1 ∼Unif ([0, 1]) and since C1 is independent of C′
1, we have
θ1 ̸= θ′
1 almost surely; step (d) is owing to the independence property of the construction of the
functions p1, p2 and p3. We calculate the second term:
E
"
Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

2#
=
Z
E

ft
 X, u1, u2, w0
3 (C2)

ft
 X, u′
1, u′
2, w0
3 (C2)

ρ1 (du1) ρ2 (du2) ρ1 (du′
1) ρ2 (du′
2)
=
Z
EZ [ft (X, u1, u2, u3) ft (X, u′
1, u′
2, u3)] ρ1 (du1) ρ2 (du2) ρ1 (du′
1) ρ2 (du′
2) ρ3 (du3) ,
as well as the last term:
E

H2
 X, C2; ¯W (t)
 Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

= E

ft
 X, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)
 Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

=
Z
EZ [ft (X, u1, u2, u3) ft (X, u′
1, u′
2, u3)] ρ1 (du1) ρ2 (du2) ρ1 (du′
1) ρ2 (du′
2) ρ3 (du3) .
28

Published as a conference paper at ICLR 2021
It is then easy to see that
E
"H2
 X, C2; ¯W (t)

−
Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2)

2#
= 0.
That is, we have almost surely
H2
 X, C2; ¯W (t)

=
Z
ft
 X, u1, u2, w0
3 (C2)

ρ1 (du1) ρ2 (du2) .
Note that the right-hand side is SZ
3 -measurable, and hence so is H2
 X, C2; ¯W (t)

.
Next we consider ∆H
2
 Z, C2; ¯W (t)

. Recall that
∆H
2
 z, c2; ¯W (t)

= ∂2L
 y, ˆy
 x; ¯W (t)

ϕ′
3
 H3
 x; ¯W (t)

¯w3 (t, c2) ϕ′
2
 H2
 x, c2; ¯W (t)

.
Then together with the existence of ¯w∗
3, we have ∆H
2
 Z, C2; ¯W (t)

is SZ
3 -measurable.
Now we consider EC2

∆H
2
 Z, C2; ¯W (t)

¯w2 (t, C1, C2)

. With the existence of ¯w∗
2, there exists a
Borel function gt such that
∆H
2
 Z, C2; ¯W (t)

¯w2 (t, C1, C2) = gt
 Z, w0
1 (C1) , w0
2 (C1, C2) , w0
3 (C2)

.
Then with the same argument as the treatment of H2
 X, C2; ¯W (t)

, one can show that
EC2

∆H
2
 Z, C2; ¯W (t)

¯w2 (t, C1, C2)

=
Z
gt
 Z, w0
1 (C1) , u2, u3

ρ2 (du2) ρ3 (du3) ,
which is SZ
1 -measurable.
Using these facts together with the existence of ¯w∗
1, ¯w∗
2 and ¯w∗
3, we have ∆3
 C2; ¯W (t)

is S3-
measurable, ∆2
 C1, C2; ¯W (t)

is S13-measurable and ∆1
 C1; ¯W (t)

is S1-measurable.
Step 4:
Closeness between the MF dynamics and the reduced dynamics.
We shall use
W −¯W

t with the same meaning as the distance between two sets of MF parameters. Recall
by Lemma 11 that 9W9T ≤KT since 9W90 ≤K. By the same argument,
 ¯W

T ≤KT . Then
by Lemma 13, we have for any t ≤T,
ess-sup sup
s≤t
∆3 (C2; W (s)) −∆3
 C2; ¯W (s)
 ≤KT
W −¯W

t ,
ess-sup sup
s≤t
∆2 (C1, C2; W (s)) −∆2
 C1, C2; ¯W (s)
 ≤KT
W −¯W

t ,
ess-sup sup
s≤t
∆1 (C1; W (s)) −∆1
 C1; ¯W (s)
 ≤KT
W −¯W

t .
We have:
ess-sup
 ¯∆2 (t, C1, C2) −∆2
 C1, C2; ¯W (t)

= ess-sup
E [∆2 (C1, C2; W (t))|S2] −∆2
 C1, C2; ¯W (t)

≤ess-sup
E

∆2
 C1, C2; ¯W (t)
S2

−∆2
 C1, C2; ¯W (t)

+ ess-sup
E

∆2
 C1, C2; ¯W (t)

−∆2 (C1, C2; W (t))
S2

(a)
= ess-sup
E

∆2
 C1, C2; ¯W (t)

−∆2 (C1, C2; W (t))
S2

≤ess-sup
∆2
 C1, C2; ¯W (t)

−∆2 (C1, C2; W (t))
 ,
where step (a) is because ∆2
 C1, C2; ¯W (t)

is S13-measurable from Step 3 and S13 ⊆S2. As
such,
ess-sup
 ¯∆2 (t, C1, C2) −∆2 (C1, C2; W (t))
 ≤2KT
W −¯W

t
almost surely for all rational t ≤T. By continuity in t of both sides, the same holds for all t ≤T.
Hence by Assumption 1,

∂
∂t ¯w2 (t, C1, C2) −∂
∂tw2 (t, C1, C2)
 ≤K
 ¯∆2 (t, C1, C2) −∆2 (C1, C2; W (t))

29

Published as a conference paper at ICLR 2021
≤2KT
W −¯W

t ,
for all t ≤T almost surely, which leads to
| ¯w2 (t, C1, C2) −w2 (t, C1, C2)| ≤2KT
Z t
0
W −¯W

s ds
almost surely. One can obtain similar results for ¯w1 versus w1 and ¯w3 versus w3. Therefore,
W −¯W

t ≤KT
Z t
0
W −¯W

s ds.
Since W (0) = ¯W (0), by Gronwall’s inequality,
W −¯W

t = 0 for all t ≤T. In other words,
since T is arbitrary,
¯w1 (t, C1) = w1 (t, C1) ,
¯w2 (t, C1, C2) = w2 (t, C1, C2) ,
¯w3 (t, C2) = w3 (t, C2) ,
for all t ≥0 almost surely.
Step 5: Concluding.
The ﬁrst claim of the lemma is proven by the conclusion of Step 4 and by
choosing w∗
1 = ¯w∗
1, w∗
2 = ¯w∗
2 and w∗
3 = ¯w∗
3, as well as the measurability facts from Step 3. To
prove the second claim, since ∆H
2
 Z, C2; ¯W (t)

is SZ
3 -measurable and
W −¯W

t = 0 for all
t ≥0, there exists a Borel function ∆H∗
2
such that
∆H
2
 Z, C2; ¯W (t)

= ∆H
2 (Z, C2; W (t)) = ∆H∗
2
 t, Z, w0
3 (C2)

for all t ≥0 almost surely, by the same argument in Step 2. These facts, together with the dynamics
of w1 and w2, imply that almost surely, for all t ≥0,
∂
∂tw2 (t, C1, C2)
= −ξ2 (t) EZ

∆H∗
2
 t, Z, w0
3 (C2)

ϕ1
 
w∗
1
 t, w0
1 (C1)

, X

,
∂
∂tw∗
1
 t, w0
1 (C1)

= −ξ1 (t) EZ

EC2

∆H∗
2
 t, Z, w0
3 (C2)

w2 (t, C1, C2)

ϕ′
1
 
w∗
1
 t, w0
1 (C1)

, X

X

,
with initialization w∗
1
 0, w0
1 (C1)

= w0
1 (C1). Substituting the ﬁrst equation into the second one,
we get:
∂
∂tw∗
1
 t, w0
1 (C1)

= −ξ1 (t) EZ

EC2

∆H∗
2
 t, Z, w0
3 (C2)

w0
2 (C1, C2)

ϕ′
1
 
w∗
1
 t, w0
1 (C1)

, X

X

+ ξ1 (t)
Z t
0
ξ2 (s) EZ,Z′

EC2

∆H∗
2
 t, Z, w0
3 (C2)

∆H∗
2
 s, Z′, w0
3 (C2)

× ϕ1
 
w∗
1
 s, w0
1 (C1)

, X′
ϕ′
1
 
w∗
1
 t, w0
1 (C1)

, X

X

ds.
Note that by an argument similar to Step 3,
EC2

∆H∗
2
 t, Z, w0
3 (C2)

w0
2 (C1, C2)

=
Z
∆H∗
2
(t, Z, u3) u2ρ2 (du2) ρ3 (du3) ,
which holds for all t ≥0 almost surely by the same argument in Step 2. We thus obtain:
∂
∂tw∗
1 (t, u1) = −ξ1 (t)
Z
EZ

∆H∗
2
(t, Z, u3) u2ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ρ2 (du2) ρ3 (du3)
+ ξ1 (t)
Z t
0
ξ2 (s) EZ,Z′
 Z
∆H∗
2
(t, Z, u3) ∆H∗
2
(s, Z′, u3) ρ3 (du3)
× ϕ1 (⟨w∗
1 (s, u1) , X′⟩) ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ds,
with initialization w∗
1 (t, u1) = u1 for all u1 ∈supp
 ρ1
and t ≥0.
30

Published as a conference paper at ICLR 2021
An important ingredient of the proof is that the distribution of w1 (t, C1) has full support at all time
t ≥0, even though we only need to assume this property at initialization t = 0. This key property is
proven by a topology argument, supported by the measurability result of Lemma 16. We remark that
a similar property for two-layer networks is established in Chizat & Bach (2018) using a different
topology argument.
Lemma 17. Consider the same setting as Theorem 8. For all ﬁnite time t ≥0, the support of
Law (w1 (t, C1)) is Rd.
Proof. By Lemma 16, one can choose a neural embedding such that there exists Borel functions w∗
1
and ∆H∗
2
for which almost surely, for all t ≥0,
w1 (t, C1) = w∗
1
 t, w0
1 (C1)

,
∆H
2 (z, C2; W (t)) = ∆H∗
2
 t, z, w0
3 (C2)

,
where W (t) is the MF dynamics formed under the coupling procedure with this neuronal embedding
as described in Section 3.1. Furthermore,
∂
∂tw∗
1 (t, u1) = −
Z
EZ

∆H∗
2
(t, Z, u3) u2ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ρ2 (du2) ρ3 (du3)
+
Z t
0
EZ,Z′
 Z
∆H∗
2
(t, Z, u3) ∆H∗
2
(s, Z′, u3) ρ3 (du3)
× ϕ1 (⟨w∗
1 (s, u1) , X′⟩) ϕ′
1 (⟨w∗
1 (t, u1) , X⟩) X

ds,
with initialization w∗
1 (0, u1) = u1 for all u1 ∈supp
 ρ1
and t ≥0, where Z′ is an independent
copy of Z. We recall from Lemma 12 that
ess-sup∆H∗
2
 t, Z, w0
3 (C2)

= ess-sup∆H
2 (Z, C2; W (t)) ≤Kt,
where Kt denotes a generic constant that depends on t and is ﬁnite with ﬁnite t. Therefore, by
Assumption 1, for t ≤T and u1, u′
1 ∈supp
 ρ1
,

∂
∂tw∗
1 (t, u1) −∂
∂tw∗
1 (t, u′
1)
 ≤Kt |w∗
1 (t, u1) −w∗
1 (t, u′
1)| + Kt
Z t
0
|w∗
1 (s, u1) −w∗
1 (s, u′
1)| ds
≤KT sup
s≤t
|w∗
1 (s, u1) −w∗
1 (s, u′
1)| ,

∂
∂tw∗
1 (t, u1)
 ≤KT .
Applying Gronwall’s lemma to the ﬁrst bound:
sup
t≤T
|w∗
1 (t, u1) −w∗
1 (t, u′
1)| ≤eKT |w∗
1 (0, u1) −w∗
1 (0, u′
1)|
= eKT |u1 −u′
1| .
Furthermore the second bound implies
sup
t,t′≤T
|w∗
1 (t, u1) −w∗
1 (t′, u1)| ≤KT |t −t′| .
Therefore (t, u1) 7→w∗
1 (t, u1) is a continuous mapping on [0, T] × Rd for an arbitrary T ≥0.
Given this continuity, we show the thesis by a topology argument. Consider the sphere Sd which is
a compactiﬁcation of Rd. We can extend w∗
1 to a function M : [0, T] × Sd →Sd ﬁxing the point at
inﬁnity, which remains a continuous map since |M (t, u1) −u1| = |M (t, u1) −M (0, u1)| ≤KT t.
Let Mt : Rd →Rd be deﬁned by Mt (u1) = M (t, u1). We claim that Mt is surjective for all ﬁnite
t. Indeed, if Mt fails to be surjective for some t, then for some p ∈Sd, Mt : Sd →Sd\ {p} →Sd
is homotopic to the constant map, but M then gives a homotopy from the identity map M0 on
the sphere to a constant map, which is a contradiction as the sphere Sd is not contractible. Hence
w∗
1 (t, ·) is surjective for all ﬁnite t. Recall that w1 (t, C1) = w∗
1
 t, w0
1 (C1)

almost surely and
w0
1 (C1) has full support. Now let us assume that w1 (t, C1) does not have full support at some
31

Published as a conference paper at ICLR 2021
time t, which implies there is an open ball B in Rd for which P (w1 (t, C1) ∈B) = 0. Then
P
 w∗
1
 t, w0
1 (C1)

∈B

= 0. Since w∗
1 (t, ·) has full support, there is an open set U such that
w∗
1 (t, u1) ∈B for all u1 ∈U. Then P
 w0
1 (C1) ∈U

= 0, contradicting the assumption that
w0
1 (C1) has full support. Therefore w1 (t, C1) must have full support at all t ≥0.
With this, we are ready to prove Theorem 8.
Proof of Theorem 8. Recall, by Theorem 1, the solution to the MF ODEs exists uniquely, and by
Lemma 17, the support of Law (w1 (t, C1)) is Rd at all t. By the convergence assumption, we have
that for any ϵ > 0, there exists T (ϵ) such that for all t ≥T (ϵ) and P-almost every c1:
EC2
EZ

∆H
2 (Z, C2; W (t)) ϕ1 (⟨w1 (t, c1) , X⟩)

≤ϵ.
Since Law (w1 (t, C1)) has full support, we obtain that for u in a dense subset of Rd,
EC2
EZ

∆H
2 (Z, C2; W (t)) ϕ1 (⟨u, X⟩)

≤ϵ.
By continuity of u 7→ϕ1(⟨u, x⟩), we extend the above to all u ∈Rd. Since ϕ1 is bounded,
EC2
EZ
 ∆H
2 (Z, C2; W (t)) −∆H
2 (Z, C2; ¯w1, ¯w2, ¯w3)

ϕ1 (⟨u, X⟩)

≤KE
∆H
2 (Z, C2; W (t)) −∆H
2 (Z, C2; ¯w1, ¯w2, ¯w3)

≤KE
h
(1 + | ¯w3(C2)|)

|w3(t, C2) −¯w3(C2)| + | ¯w3(C2)| |w2(t, C1, C2) −¯w2(C1, C2)|
+ | ¯w3(C2)| | ¯w2(C1, C2)| |w1(t, C1) −¯w1(C1)|
i
,
where the last step is by Assumption 1. Recall that the right-hand side converges to 0 as t →∞.
We thus obtain that for all u ∈Rd,
EC2
h

EZ

∆H
2 (Z, C2; ¯w1, ¯w2, ¯w3) |X = x

, ϕ1 (⟨u, x⟩)

L2(PX)

i
= EC2
EZ

∆H
2 (Z, C2; ¯w1, ¯w2, ¯w3) ϕ1 (⟨u, X⟩)

= 0,
which yields that for all u ∈Rd and P-almost every c2,


EZ

∆H
2 (Z, c2; ¯w1, ¯w2, ¯w3) |X = x

, ϕ1 (⟨u, x⟩)

L2(PX)
 = 0.
Here we note that by Assumption 1,
EZ

∆H
2 (Z, c2; ¯w1, ¯w2, ¯w3) |X = x
 ≤K | ¯w3 (c2)| ,
and
so
EZ

∆H
2 (Z, c2; ¯w1, ¯w2, ¯w3) |X = x

is
in
L2 (PX)
for
P-almost
ev-
ery
c2.
Since

ϕ1 (⟨u, ·⟩) : u ∈Rd	
has
dense
span
in
L2 (PX),
we
have
EZ

∆H
2 (Z, c2; ¯w1, ¯w2, ¯w3) |X = x

=
0 for PX-almost every x and P-almost every c2,
and hence
EZ [∂2L (Y, ˆy (X; ¯w1, ¯w2, ¯w3))|X = x] ϕ′
3 (H3 (x; ¯w1, ¯w2, ¯w3)) ¯w3 (c2) ϕ′
2 (H2 (x, c2; ¯w1, ¯w2)) = 0.
We note that our assumptions guarantee that P ( ¯w3 (C2) ̸= 0) is positive. Indeed:
• In the case w0
3 (C2) ̸= 0 with positive probability and ξ3 (·) = 0, the conclusion is obvious.
• In the case L
 w0
1, w0
2, w0
3

< EZ [L (Y, ϕ3 (0))], we recall the following standard prop-
erty of gradient ﬂows:
L (w1 (t, ·) , w2 (t, ·, ·) , w3 (t, ·)) ≤L (w1 (t′, ·) , w2 (t′, ·, ·) , w3 (t′, ·)) ,
for t ≥t′. In particular, setting t′ = 0 and taking t →∞, it is easy to see that
L ( ¯w1, ¯w2, ¯w3) ≤L
 w0
1, w0
2, w0
3

< EZ [L (Y, ϕ3 (0))] .
If P ( ¯w3 (C2) = 0) = 1 then L ( ¯w1, ¯w2, ¯w3) = EZ [L (Y, ϕ3 (0))], a contradiction.
32

Published as a conference paper at ICLR 2021
Then since ϕ′
2 and ϕ′
3 are strictly non-zero, we have EZ [∂2L (Y, ˆy (X; ¯w1, ¯w2, ¯w3))|X = x] = 0
for PX-almost every x.
In Case 1, since L convex in the second variable, for any measurable function ˜y(x),
L (y, ˜y (x)) −L (y, ˆy (x; ¯w1, ¯w2, ¯w3)) ≥∂2L (y, ˆy (x; ¯w1, ¯w2, ¯w3)) (˜y (x) −ˆy (x; ¯w1, ¯w2, ¯w3)) .
Taking expectation, we get EZ [L (Y, ˜y (X))] ≥L ( ¯w1, ¯w2, ¯w3), i.e. ( ¯w1, ¯w2, ¯w3) is a global mini-
mizer of L .
In Case 2, since y is a function of x, we obtain ∂2L (y, ˆy (x; ¯w1, ¯w2, ¯w3)) = 0 and hence
L (y, ˆy (x; ¯w1, ¯w2, ¯w3)) = 0 for PX-almost every x.
Finally we have from Assumptions 1, 3:
|L (W (t)) −L ( ¯w1, ¯w2, ¯w3)| = |EZ [L (Y, ˆy (X; W (t))) −L (Y, ˆy (X; ¯w1, ¯w2, ¯w3))]|
≤KEZ [|ˆy (X; W (t)) −ˆy (X; ¯w1, ¯w2, ¯w3)|]
≤KE
h
|w3 (t, C2) −¯w3 (C2)| + | ¯w3 (C2)| |w2 (t, C1, C2) −¯w2 (C1, C2)|
+ | ¯w3 (C2)| | ¯w2 (C1, C2)| |w1 (t, C1) −¯w1 (C1)|
i
which tends to 0 as t →∞. This completes the proof.
E
CONVERSE FOR GLOBAL CONVERGENCE: REMARK 9
We prove a converse statement for global convergence in relation with the essential supremum con-
dition (6).
Proposition 18. Consider a neuronal embedding

Ω, F, P,

w0
i
	
i=1,2,3

of
 ρ1, ρ2, ρ3
-i.i.d. ini-
tialization. Consider the MF limit corresponding to the network (1), such that they are coupled
together by the coupling procedure in Section 3.1, under Assumptions 1, 2, ξ1 (·) = ξ2 (·) = 1.
Assume that L(y, ˆy) →∞as |ˆy| →∞for each y. Further assume that there exists ¯w3 such that as
t →∞,
EC2 [|w3(t, C2) −¯w3(C2)|] →0.
Then the following hold:
• Case 1 (convex loss): If L is convex in the second variable and
lim
t→∞L (W (t)) = inf
V L (V ) ,
then it must be that
sup
c1∈Ω1
EC2

∂
∂tw2 (t, c1, C2)


→0
as t →∞.
• Case 2 (generic non-negative loss): Suppose that ∂2L (y, ˆy) = 0 implies L (y, ˆy) = 0, and
y = y(x) is a function of x. If L (W (t)) →0 as t →∞, then the same conclusion also
holds.
Proof. We recall
∂
∂tw2 (t, c1, c2) = −EZ
h
∂2L (Y, ˆy (X; W (t))) w3 (t, c2)
× ϕ′
3 (H3 (X; W (t))) ϕ′
2 (H2 (X, c2; W (t))) ϕ1 (⟨w1 (t, c1) , X⟩)
i
,
for c1 ∈Ω1, c2 ∈Ω2. By Assumption 1,

∂
∂tw2 (t, c1, c2)
 ≤KEZ [|∂2L (Y, ˆy (X; W (t)))|] |w3 (t, c2)| .
33

Published as a conference paper at ICLR 2021
Note that the right-hand side is independent of c1. Since EC2 [|w3(t, C2) −¯w3(C2)|] →0 as t →
∞, we have for some ﬁnite t0 ≤K,
EC2 [| ¯w3(C2)|] ≤EC2 [|w3(t0, C2)|] + K ≤K,
where the last step is by Lemma 11 and Assumption 2. As such, for all t sufﬁciently large, we have:
sup
c1∈Ω1
EC2

∂
∂tw2 (t, c1, C2)


≤KEZ [|∂2L (Y, ˆy (X; W (t)))|] EC2 [|w3 (t, C2)|]
≤KEZ [|∂2L (Y, ˆy (X; W (t)))|] (K + EC2 [| ¯w3 (C2)|])
≤KEZ [|∂2L (Y, ˆy (X; W (t)))|] .
The proof concludes once we show that EZ [|∂2L (Y, ˆy (X; W (t)))|] →0 as t →∞.
For a ﬁxed z = (x, y), let us write L (t, z) = L(y, ˆy(x; W(t))) and ∂2L(t, z) = ∂2L(y, ˆy(x; W(t)))
for brevity.
Consider Case 1.
We claim that if there is an increasing sequence of time ti so
that limi→∞[L(ti, z) −inf ˆy L(y, ˆy)] = 0, then limi→∞|∂2L(ti, z)| = 0.
Indeed, it sufﬁces
to show that for any subsequence tij of ti, there exists a further subsequence tijk such that
limk→∞
∂2L(tijk , z)
 = 0. In any subsequence tij of ti, using that L(tij, z) is convergent and
the fact L(y, ˆy) →∞as |ˆy| →∞, we have ˆy(x; W(tij)) is bounded.
Hence, we obtain a
subsequence tijk for which ˆy(x; W(tijk )) converges to some limit ˆy∗. By continuity, we have
L(y, ˆy∗) = limk→∞L(tijk , z) = inf ˆy L(y, ˆy). Thus, since L is convex in the second variable, we
have ∂2L(y, ˆy∗) = 0. Thus, limk→∞
∂2L(tijk , z)
 = |∂2L(y, ˆy∗)| = 0, as claimed. Similarly, we
obtain in Case 2 that if there is an increasing sequence of time ti so that limi→∞[L(ti, z)] = 0, then
limi→∞|∂2L(ti, z)| = 0.
To show that EZ [|∂2L (t, Z)|] →0 as t →∞, it sufﬁces to show that for any increasing sequence of
times ti tending to inﬁnity, there exists a subsequence tij of ti such that EZ
∂2L
 tij, Z

→0. In
Case 1, we have limi→∞L (W (ti)) = infV L (V ), so limi→∞EZ
h
L (ti, Z) −inf ˆY L(Y, ˆY )
i
=
0.
Since L (ti, Z) −inf ˆY L(Y, ˆY ) is nonnegative, this implies that L (ti, Z) −inf ˆY L(Y, ˆY )
converges to 0 in probability. Thus, there is a further subsequence tij for which L
 tij, Z

−
inf ˆY L(Y, ˆY ) converges to 0 P-almost surely.
By the previous claim,
∂2L
 tij, Z
 con-
verges to 0 P-almost surely.
Since
∂2L
 tij, Z
 is bounded P-almost surely, we obtain that
EZ
∂2L
 tij, Z

→0 from the bounded convergence theorem. The result in Case 2 can be
established similarly.
F
USEFUL TOOLS
We ﬁrst present a useful concentration result. In fact, the tail bound can be improved using the
argument in Feldman & Vondrak (2018), but the following simpler version is sufﬁcient for our
purposes.
Lemma 19. Consider an integer n ≥1 and let x, c1, ..., cn be mutually independent random
variables. Let Ex and Ec denote the expectations w.r.t. x only and {ci}i∈[n] only, respectively.
Consider a collection of mappings {fi}i∈[n], which map to a separable Hilbert space F. Let fi (x) =
Ec [fi (ci, x)]. Assume that for some R > 0, |fi (ci, x) −fi (x)| ≤R almost surely, then for any
δ > 0,
P
 
Ex
"
1
n
n
X
i=1
fi (ci, x) −fi (x)

#
≥δ
!
≤8R
√nδ exp

−nδ2
8R2

≤8R
δ exp

−nδ2
8R2

.
Proof. For brevity, let us deﬁne
Zn (x) =
n
X
i=1
(fi (ci, x) −fi (x)) .
34

Published as a conference paper at ICLR 2021
By Theorem 21,
P (|Zn (x)| ≥nδ|x) ≤2 exp
 −nδ2/
 4R2
,
and therefore,
P (|Zn (x)| ≥nδ) ≤2 exp
 −nδ2/
 4R2
,
since the right-hand side is uniform in x. Next note that, w.r.t. the randomness of x only,
Ex [|Zn (x)|] = Ex [|Zn (x)| I (|Zn (x)| ≥nδ/2)] + Ex [|Zn (x)| I (|Zn (x)| < nδ/2)]
≤Ex [|Zn (x)| I (|Zn (x)| ≥nδ/2)] + nδ/2.
As such, by Markov’s inequality and Cauchy-Schwarz’s inequality,
P (Ex [|Zn (x)|] ≥nδ) ≤P (Ex [|Zn (x)| I (|Zn (x)| ≥nδ/2)] ≥nδ/2)
≤2
nδ E [|Zn (x)| I (|Zn (x)| ≥nδ/2)]
≤2
nδ E
h
|Zn (x)|2i1/2
P (|Zn (x)| ≥nδ/2)1/2
≤4
nδ E
h
|Zn (x)|2i1/2
exp

−nδ2
8R2

.
Notice that since c1, ..., cn are independent and fi (x) = Ec [fi (ci, x)],
E
h
|Zn (x)|2i
=
n
X
i=1
E
h
|fi (ci, x) −fi (x)|2i
≤2nR2.
We thus get:
P (Ex [|Zn (x)|] ≥nδ) ≤8R
√nδ exp

−nδ2
8R2

.
This proves the claim.
We state a martingale concentration result, which is a special case of (Pinelis, 1994, Theorem 3.5)
which applies to a more general Banach space.
Theorem 20 (Concentration of martingales in Hilbert spaces.). Consider a martingale Zn ∈Z a
separable Hilbert space such that |Zn −Zn−1| ≤R and Z0 = 0. Then for any t > 0,
P

max
k≤n |Zk| ≥t

≤2 inf
λ>0 exp
 
−λt + ess-sup
n
X
k=1
E
h
eλ|Zk−Zk−1| −1 −λ|Zk −Zk−1| | Fk−1
i!
.
In particular, for any δ > 0,
P

max
k≤n |Zk| ≥nδ

≤2 exp

−nδ2
2R2

.
The following concentration result for i.i.d. random variables in Hilbert spaces is a corollary.
Theorem 21 (Concentration of i.i.d. sum in Hilbert spaces.). Consider n i.i.d. random variables
X1, ..., Xn in a separable Hilbert space. Suppose that there exists a constant R > 0 such that
|Xi −E [Xi]| ≤R almost surely. Then for any δ > 0,
P
 
1
n

n
X
i=1
Xi −E [Xi]
 ≥δ
!
≤2 exp

−nδ2
2R2

.
35

