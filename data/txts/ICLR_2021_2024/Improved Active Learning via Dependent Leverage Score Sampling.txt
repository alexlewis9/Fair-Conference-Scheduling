IMPROVED
ACTIVE
LEARNING
VIA
DEPENDENT
LEVERAGE SCORE SAMPLING
Atsushi Shimizu, Xiaoou Cheng, Christopher Musco, Jonathan Weare
New York University
{as15106,chengxo,cmusco,weare}@nyu.edu
ABSTRACT
We show how to obtain improved active learning methods in the agnostic (ad-
versarial noise) setting by combining marginal leverage score sampling with non-
independent sampling strategies that promote spatial coverage. In particular, we
propose an easily implemented method based on the pivotal sampling algorithm,
which we test on problems motivated by learning-based methods for parametric
PDEs and uncertainty quantification. In comparison to independent sampling, our
method reduces the number of samples needed to reach a given target accuracy by
up to 50%. We support our findings with two theoretical results. First, we show
that any non-independent leverage score sampling method that obeys a weak one-
sided ℓ∞independence condition (which includes pivotal sampling) can actively
learn d dimensional linear functions with O(d log d) samples, matching indepen-
dent sampling. This result extends recent work on matrix Chernoff bounds under
ℓ∞independence, and may be of interest for analyzing other sampling strategies
beyond pivotal sampling. Second, we show that, for the important case of polyno-
mial regression, our pivotal method obtains an improved bound on O(d) samples.
1
INTRODUCTION
In the active linear regression problem, we are given a data matrix A ∈Rn×d with n ≫d rows and
query access to a target vector b ∈Rn. The goal is to learn parameters x ∈Rd such that Ax ≈b
while observing as few entries in b as possible. We study this problem in the challenging agnostic
learning or “adversarial noise” setting, where we do not assume any underlying relationship between
A and b. Instead, our goal is to find parameters competitive with the best possible fit, good or bad.
Specifically, considering ℓ2 loss, let x∗= arg minx∥Ax −b∥2
2 be optimal model parameters. We
want to find ˜x∗using a small number of queried target values in b such that
∥A˜x∗−b∥2
2 ≤(1 + ϵ)∥Ax∗−b∥2
2,
(1.1)
for some error parameter ϵ > 0. Beyond being a fundamental learning problem, active regression
has emerged as a fundamental tool in learning based methods for the solution and uncertainty anal-
ysis of parametric partial differential equations (PDEs) (Chkifa et al., 2015; Guo et al., 2020). For
such applications, the agnostic setting is crucial, as a potentially complex quantity of interest is ap-
proximated by a simple surrogate model (e.g. polynomials, sparse polynomials, single layer neural
networks, etc.) (L¨uthen et al., 2021; Hokanson and Constantine, 2018). Additionally, reducing the
number of labels used for learning is crucial, as each label usually requires the computationally
expensive numerical solution of a PDE for a new set of parameters (Cohen and DeVore, 2015).
1.1
LEVERAGE SCORE SAMPLING
Active linear regression has been studied for decades in the statistical model where b is assumed to
equal Ax∗plus i.i.d. random noise. In this case, the problem can be addressed using tools from opti-
mal experimental design (Pukelsheim, 2006). In the agnostic case, near-optimal sample complexity
results were only obtained relatively recently using tools from non-asymptotic matrix concentration
(Tropp, 2012). In particular, it was shown independently in several papers that collecting entries
from b randomly with probability proportional to the statistical leverage scores of rows in A can
achieve (1.1) with O(d log d + d/ϵ) samples (Sarl´os, 2006; Rauhut and Ward, 2012; Hampton and
Doostan, 2015; Cohen and Migliorati, 2017). The leverage scores are defined as follows:
1

(a) Target Function
(b) Bernoulli Sampling
(c) Pivotal Sampling (our method)
Figure 1: Polynomial approximations to the maximum displacement of a damped harmonic oscilla-
tor, as a function of driving frequency and spring constant. (a) is the target value, and samples can
be obtained through the numerical solution of a differential equation governing the oscillator. Both
(b) and (c) draw 250 samples using leverage score sampling and perform polynomial regression of
degree 20. (b) uses Bernoulli sampling while (c) uses our pivotal sampling method. Our method
gives a better approximation, avoiding artifacts that result from gaps between the Bernoulli samples.
Definition 1.1 (Leverage Score). Let U ∈Rn×r be any orthogonal basis for the column span of a
matrix A ∈Rn×d. Let ai and ui be the i-th rows of A and U, respectively. The leverage score τi
of the i-th row in A can be equivalently written as:
τi = ∥ui∥2
2 = aT
i (AT A)−1ai = max
x∈Rd (aT
i x)2/∥Ax∥2
2.
(1.2)
Notice that τi = ∥ui∥2
2, Pn
i=1 τi = d when A is full-rank and thus U has r = d columns.
See (Avron et al., 2017) for a short proof of the final equality in (1.2). This last definition, based on a
maximization problem, gives an intuitive understanding of the leverage scores. The score of row ai
is higher if it is more “exceptional”, meaning that we can find a vector x that has large inner product
with ai relative to its average inner product (captured by ∥Ax∥2
2) with all other rows in the matrix.
Based on leverage score, rows that are more exceptional are sampled with higher probability.
Prior work considers independent leverage score sampling, either with or without replacement. The
typical approach for sampling without replacement, which we call “Bernoulli sampling” is as fol-
lows: Each row ai is assigned a probability pi = min(1, c·τi) for an oversampling parameter c ≥1.
Then each row is sampled independently with probability pi. We construct a subsampled data matrix
˜A and subsampled target vector ˜b by adding ai/√pi to ˜A and bi/√pi to ˜b for any index i that is
sampled. To solve the active regression problem, we return ˜x∗= arg minx ∥˜Ax −˜b∥2.
1.2
OUR CONTRIBUTIONS
In applications to PDEs, the goal is often to approximate a function over a low dimensional distribu-
tion X. E.g. X might be uniform over an interval [−1, 1] ⊂R or over a box [−1, 1]×. . .×[−1, 1] ⊂
Rq. In this setting, the length d rows of A correspond to feature transformations of samples from
X. For example, in the ubiquitous task of polynomial regression, we start with x ∼X and add to
A a row containing all combinations of entries in x with total degree p, i.e., xℓ1
1 xℓ2
2 . . . xℓq
q for all
non-negative integers ℓ1, . . . , ℓq such that Pq
i=1 ℓi ≤p. For such problems, “grid” based interpola-
tion is often used in place of randomized methods like leverage scores sampling, i.e., the target b is
queried on a deterministic grid tailored to X. For example, when X is uniform on a box, the stan-
dard approach is to use a grid based on the Chebyshev nodes (Xiu, 2016). Pictured in Figure 2, the
Cheybshev grid concentrates samples near the boundaries of the box, avoiding the well known issue
of Runge’s phenomenon for uniform grids. Leverage score sampling does the same. In fact, the
methods are closely related: in the high degree limit, the leverage scores for polynomial regression
over the box match the asymptotic density of the Chebyshev nodes (L¨uthen et al., 2021).
So how do the deterministic and randomized methods compare? The advantage of randomized
methods based on leverage score sampling is that they yield strong provable approximation guar-
antees, and easily generalize to any distribution X.1 Deterministic methods are less flexible on the
1As discussed in (Chen and Price, 2019), no deterministic method can provably solve the agnostic regression
problem with few samples. Since we make no assumptions on b, all error in Ax∗−b could be concentrated
only at the deterministic indices to be selected. Randomization is needed to avoid high error outliers in b.
2

(a) Bernoulli Sampling
(b) Pivotal Sampling (our method)
(c) Chebyshev Grid
Figure 2: The results of three different active learning methods used to collect samples to fit a
polynomial over [−1, 1] × [−1, 1]. The image on the left was obtained by collecting points inde-
pendently at random with probability according to their statistical leverage scores. The image on
the right was obtained by collecting samples at the 2-dimensional Chebyshev nodes. The image
in the middle shows our method, which collects samples according to leverage scores, but using a
non-independent pivotal sampling strategy that ensures samples are evenly spread in spatially.
other hand, and do not yield provable guarantees. However, the advantage of grid based methods is
that they more “evenly” distribute samples over the original data domain, which can lead to better
performance in practice. Randomized methods are prone to “missing” larger regions of X’s support,
as shown in Figure 2. The driving question behind our work is:
Is it possible to obtain the “best of both worlds” for fitting functions over low-dimensional
domains? I.e., can we match or improve on the strong theoretical guarantees of leverage
score sampling with a method that produces spatially well-distributed samples?
We answer this question in the affirmative. Instead of sampling rows from A independently with
probability proportional to the leverage scores, we adopt a tool from survey sampling known as
pivotal sampling (Deville and Tille, 1998). Our specific version of pivotal sampling is spatially-
aware, meaning that it covers the domain in a well-balanced manner, while the marginal probabilities
remain proportional to the leverage score. At a high-level, the pivotal method is a “competition”
based sampling approach, where candidate rows compete in a binary tree tournament. By structuring
the tournament so that spatially close points compete at lower levels (we use a novel recursive PCA
procedure to build the tree), we ensure better spatial spread than Bernoulli leverage score sampling.
We show that our pivotal sampling method matches or beats the complexity of independent leverage
score sampling in theory, while performing significantly better in practice. On the practice side, we
offer Figure 1 as an example from a PDE test problem. In comparison to independent sampling,
our spatially-aware method obtains a much better approximation to the target for a fixed number of
samples (more details in Section 4). On the theory side, we prove two results. The first is general:
we show that, as long as it samples rows from A with marginal probabilities proportional to the
leverage scores, any sampling strategy that obeys a weak “one-sided ℓ∞independence” condition
(which includes pivotal sampling) matches the complexity of independent leverage score sampling:
Theorem 1.1. Let A ∈Rn×d be a data matrix and b ∈Rn be a target vector. Consider any
algorithm which samples exactly k rows from A (and observes the corresponding entries in b) from
a distribution that 1) satisfies one-sided ℓ∞independence (Defn. 3.1) with parameter Dinf and 2)
the marginal probability of sampling any row ai is proportional to τi. 2 Let ˜A and ˜b be the scaled
sampled data and target, as defined in Section 1.1, and let ˜x∗= arg minx∈Rd∥˜Ax −˜b∥2
2. As long
as k ≥c ·

d log d · D2
inf + d
ϵ · Dinf

for a fixed positive constant c, then with probability 99/100,
∥A˜x∗−b∥2
2 ≤(1 + ϵ)∥Ax∗−b∥2
2
(1.3)
One-sided ℓ∞independence was introduced in a recent paper by Kaufman et al. (2022) on matrix
Chernoff bounds (we provide a formal definition in Sec. 3). It is the weakest condition under which
a tight matrix Chernoff bound is known to hold. For example, the condition is implied with con-
stant Dinf = O(1) by almost all existing notions of negative dependence between random variables,
including conditional negative association (CNA) and the strongly Rayleigh property (Pemantle,
2000). As discussed in Sec. 3, a tight matrix Chernoff bound is a prerequisite for proving relative
2Formally, we assume that the marginal probability of sampling ai equals min(1, cτi) for a fixed constant
c ≥1. Our proof easily generalizes to the case when some probabilities exceed this bound (since sampling
more never hurts) although the total sample complexity will depend on the sum of the marginal probabilities.
3

error active learning results like Equation (1.3), however does not imply such a result alone. Our
proof of Theorem 1.1 requires adapting an approximate matrix-multiplication method from (Drineas
et al., 2006) to non-independent sampling. It can be viewed as extending the work of Kaufman
et al. (2022) to show that essentially all sampling distributions known to yield tight matrix Chernoff
bounds also yield near optimal active regression bounds in the agnostic setting. Importantly, this
includes binary-tree-based pivotal sampling methods like those introduced in this work. Such meth-
ods are known to satisfy the strongly Rayleigh property (Br¨and´en and Jonasson, 2012), and thus
one-sided ℓ∞independence with Dinf = O(1). So, as a corollary of Theorem 1.1, we obtain:
Corollary 1.1. The spatially-aware pivotal sampling methods introduced in Section 2 (which use
a fixed binary tree) return with probability 99/100 a vector ˜x∗satisfying ∥A˜x∗−b∥2
2 ≤(1 +
ϵ)∥Ax∗−b∥2
2 while only observing O
 d log d + d
ϵ

entries in b.
We hope that Theorem 1.1 will be valuable in obtaining similar results for other sampling methods
beyond our own. However, the result falls short of justifying why pivotal sampling performs better
than independent leverage score sampling in experiments. Towards that end, we prove a second
result specific to pivotal sampling, which shows that the method actually improves on the complexity
of independent sampling by a log factor in the important special case of polynomial regression:
Theorem 1.2. Consider any function b : [ℓ, u] →R defined on an interval [ℓ, u] ⊂R, and consider
fitting b with a degree d polynomial based on evaluations of the function at x1, . . . , xk ∈[ℓ, u]. If
x1, . . . , xk are collected via pivotal sampling with leverage score marginals (see Appendix C for
details), then as long as k ≥c · (d + d
ϵ ) for a fixed positive constant c, there is a procedure that uses
these samples to construct a degree d polynomial ˜p which, with probability 99/100, satisfies:
∥˜p −b∥2
2 ≤(1 + ϵ)
min
degree d polynomial p ∥p −b∥2
2.
Here ∥f∥2
2 denotes the average squared magnitude
R u
ℓf(x)2dx of a function f.
The problem of finding a polynomial approximation to a real-valued function that minimizes the
average square error ∥p −b∥2
2 can be modeled as an active regression problem involving a matrix
A with d + 1 columns and an infinite number of rows. In fact, polynomial approximation is one
of the primary applications of prior work on leverage score-based active learning methods (Cohen
and Migliorati, 2017; Avron et al., 2019). Such methods require O
 d log d + d
ϵ

samples, so The-
orem 1.2 is better by a log d factor. Since polynomial approximation is a representative problem
where spatially-distributed samples are important, Theorem 1.2 provides theoretical justification for
the strong performance of pivotal sampling in experiments. Our proof is inspired by a result of Kane
et al. (2017), and relies on showing a tight relation between the leverage scores of the polynomial
regression problem and the orthogonality measure of the Chebyshev polynomials on [ℓ, u].
1.3
RELATED WORK
The application of leverage score sampling to the agnostic active regression problem has received
significant recent attention. Beyond the results discussed above, extensions of leverage score sam-
pling have been studied for norms beyond ℓ2 (Chen and Derezinski, 2021; Musco et al.; Meyer et al.,
2023; Parulekar et al., 2021), in the context where the sample space is infinite (i.e. A is an operator
with infinite rows) (Erd´elyi et al., 2020; Avron et al., 2019), and for functions that involve non-linear
transformations (Gajjar et al., 2023; Mai et al., 2021; Munteanu et al., 2018).
Theoretical improvements on leverage score sampling have also been studied. Notable is a recent
result that improves on the O(d log d + d/ϵ) bound by a log d factor, showing that the active least
squares regression problem can be solved with O(d/ϵ) samples (Chen and Price, 2019). This is
provably optimal. However, the algorithm in (Chen and Price, 2019) is complex, and appears to
involve large constant factors: in our initial experiments, it did not empirically improve on indepen-
dent leverage score sampling. In contrast, by Theorem 1.2, our pivotal sampling method matches
the theoretical sample complexity of (Chen and Price, 2019) for the special case of polynomial
regression, but performs well in experiments (significantly better than independent leverage score
sampling). There have been a few other efforts to develop practical improvements on leverage score
sampling. Similar to our work, Derezi´nski et al. (2018) study a variant of volume sampling that
matches the theoretical guarantees of leverage score sampling, but performs better experimentally.
However, this method does not explicitly take into account spatial-structure in the underlying regres-
sion problem. While the method from (Derezi´nski et al., 2018) does not quite fit our Theorem 1.1
4

(e.g., it samples indices with replacement) we expect similar methods could be analyzed as a special
case of our result, as volume sampling induces a strongly Rayleigh distribution.
While pivotal sampling has not been studied in the context of agnostic active regression, it is widely
used in other applications, and its negative dependence properties have been studied extensively.
(Dubhashi et al., 2007) proves that pivotal sampling satisfies the negative association (NA) property.
(Borcea et al., 2009) introduced the notion of a strongly Rayleigh distribution and proved that it
implies a stronger notion of conditional negative association (CNA), and (Br¨and´en and Jonasson,
2012) showed that pivotal sampling run with an arbitrary binary tree is strongly Rayleigh. It follows
that the method satisfies CNA. (Greene et al., 2022) discusses an efficient algorithm for pivotal
sampling by parallelization and careful manipulation of inclusion probabilities. Another variant
of pivotal sampling that is spatially-aware is proposed in (Grafstr¨om et al., 2011). Though their
approach is out of the scope of our analysis as it involves randomness in the competition order used
during sampling, our method is inspired by their work.
1.4
NOTATION AND PRELIMINARIES
Notation. We let [n] denote {1, · · · , n}. E[X] denotes the expectation of a random variable X. We
use bold lower-case letters for vectors and bold upper-case letters for matrices. For a vector z ∈Rn
with entries z1, · · · , zn, ∥z∥2 = (Pn
i=1 z2
i )1/2 denotes the Euclidean norm of z. Given a matrix
A ∈Rn×d, we let ai denote the i-th row, and aij denote the entry in the i-th row and j-th column.
Importance sampling. All of the methods studied in this paper solve the active regression problem
by collecting a single random sample of rows in A and corresponding entries in b. We introduce
a vector of binary random variables ξ = {ξ1, · · · , ξn}, where ξi is 1 if ai (and thus bi) is selected,
and 0 otherwise. ξ1, · · · , ξn will not necessarily be independent depending on our sampling method.
Given a sampling method, let pi = E[ξi] denote the marginal probability that row i is selected. We
return an approximate regression solution as follows: let ˜A ∈Rk×d contain ai/√pi for all i such
that ξi = 1, and similarly let ˜b ∈Rk contain bi/√pi for the same values of i. This scaling ensures
that, for any fixed x, E∥˜Ax −˜b∥2
2 = ∥Ax −b∥2
2. To solve the active regression problem, we return
˜x∗= arg minx∈Rd∥˜Ax −˜b∥2
2. Computing ˜x∗only requires querying k target values in b.
Leverage Score Sampling. We consider methods that choose the marginal probabilities propor-
tional to A’s leverage scores. Specifically, our methods sample row ai with marginal probability
˜pi = min(1, ck ·τi), where ck is chosen so that Pn
i=1 ˜pi = k. Details of how to find ck are discussed
in Appendix A. We note that we always have ck ≥k/d since Pn
i=1 pi ≤Pn
i=1
k
d · τi ≤k
d · d = k.
2
OUR METHODS
In this section, we present our sampling scheme which consists of two steps; deterministically con-
structing a binary tree, and choosing samples by running the pivotal method on this tree. The pivotal
method is described in Algorithm 1. It takes as input a binary tree with n leaf nodes, each corre-
sponding to a single index i to be sampled. For each index, we also have an associated probability
˜pi. The algorithm collects a set of exactly k samples S where k = Pn
i=1 ˜pi. It does so by perco-
lating up the tree and performing repeated head-to-head comparisons of the indices at sibling nodes
in the tree. After each comparison, one node promotes to the parent node with updated inclusion
probability, and the other node is determined to be sampled or not to be sampled.
It can be checked that, after running Algorithm 1, index i is always sampled with probability ˜pi,
regardless of the choice of T. However, the samples collected by the pivotal method are not in-
dependent, but rather negatively correlated: siblings in T are unlikely to both be sampled, and in
general, the events that close neighbors in the tree are both sampled are negatively correlated. In
particular, if index i could at some point compete with an index j in the pivotal process, the chance
of selecting j decreases if we condition on i being selected. We take advantage of this property
to generate spatially distributed samples by constructing a binary tree that matches the underlying
geometry of our data. In particular, assume we are given a set of points X ∈Rn×d′. X will eventu-
ally be used to construct a regression matrix A ∈Rn×d via feature transformation (e.g. by adding
polynomial features). However, we construct the sampling tree using X alone.
5

Algorithm 1 Binary Tree Based Pivotal Sampling (Deville and Tille, 1998)
Input: Depth t full binary tree T with n leaves, inclusion probabilities {˜p1, · · · , ˜pn} for each leaf.
Output: Set of k sampled indices S.
1: Initialize S = ∅.
2: while T has at least two remaining children nodes do
3:
Select any pair of sibling nodes S1, S2 with parent P. Let i, j be the indices stored at S1, S2.
4:
if ˜pi + ˜pj ≤1 then
5:
With probability
˜pi
˜pi+˜pj , set ˜pi ←˜pi + ˜pj, ˜pj ←0. Store i at P.
6:
Otherwise, set ˜pj ←˜pi + ˜pj, ˜pi ←0. Store j at P.
7:
else if ˜pi + ˜pj > 1 then
8:
With probability
1−˜pi
2−˜pi−˜pj , set ˜pi ←˜pi+˜pj−1, ˜pj = 1. Store i at P and set S ←S∪{j}.
9:
Otherwise, set ˜pj ←˜pi + ˜pj −1, ˜pi ←1. Store j at P and set S ←S ∪{i}.
10:
Remove S1, S2 from T.
11: return S
Algorithm 2 Binary Tree Construction by Coordinate or PCA Splitting
Input: Matrix X ∈Rn×d′, split method ∈{PCA, coordinate}, inclusion probabilities ˜p1, · · · , ˜pn.
Output: Binary tree T where each leaf corresponds to a row in X.
1: Create a tree T with a single root node. Assign set R to the root where R = {i ∈[n]; ˜pi < 1}.
2: while There exists a node in T that holds set K such that |K| > 1 do
3:
Select any such node N and let t be its level in the tree. Construct X(K) ∈R|K|×d′.
4:
if split method = PCA then
5:
Sort X(K) according to the direction of the maximum variance.
6:
else if split method = coordinate then
7:
Sort X(K) according to values in its ((t mod d′) + 1)-th column.
8:
Create a left child of N. Assign to it all indices associated with the first ⌊|K|
2 ⌋rows of X(K).
9:
Create a right child of N. Assign to it the all remaining indices in X(K). Delete K from N.
10: return T
Our tree construction method is given as Algorithm 2. XK denotes the subset of rows of X with
indices in the set K. First, the algorithm eliminates all data points with inclusion probability ˜pi = 1.
Next, it recursively partitions the remaining data points into two subgroups of the same size until all
the subgroups have only one data point. Our two methods, PCA-based and coordinate-wise, only
differ in how to partition. The PCA-based method performs principal component analysis to find the
direction of the maximum variance and splits the space by a hyperplane orthogonal to the direction
so that the numbers of data points on both sides are equal. The coordinate-wise version takes a
coordinate (corresponding to a column in X) in cyclic order and divides the space by a hyperplane
orthogonal to the chosen coordinate. An illustration of the PCA-based binary tree construction run
on a fine uniform grid of data points in R2 is shown in Figure 3. Note that our tree construction
method ensures the the number of indices assigned to each subgroup (color) at each level is equal to
with ±1 point. As such, we end of with an even partition of data points into spatially correlated sets.
Two indices will be more negatively correlated if they lie in the same set at a higher depth number.
3
THEORETICAL ANALYSIS
As will be shown experimentally, when using probabilities ˜p1, . . . , ˜pn proportional to the statistical
leverage scores of A, our tree-based pivotal methods significantly outperform its Bernoulli coun-
terpart for active regression. We provide two results theoretically justifying this operation. We first
show that, no matter what our original data matrix X is, and what feature transformation is used to
construct A, our methods never perform worse than Bernoulli sampling. In particular, they match
the O(d log d+d/ϵ) sample complexity of independent leverage score sampling. This result is stated
as Theorem 1.1. Its proof is given to Appendix B, but we outline our main approach here.
Following existing proofs for independent random sampling (e.g. (Woodruff, 2014) Theorem 1.1
requires two main ingredients: a subspace embedding result, and an approximate matrix-vector
multiplication result. In particular, let U ∈Rn×d be any orthogonal span for the columns of A. Let
S ∈Rk×n be a subsampling matrix that contains a row for every index i selected by our sampling
6

Figure 3: Visualization of a binary tree constructed via our Algorithm 2 using the PCA method for
a matrix X ∈Rn×2 containing points on a uniform square grid. For each depth, data points are
given the same color if they compose a subtree with root at that depth. As we can see, the method
produces uniform recursive spatial partitions, which encourage spatially separated samples.
scheme, which has value 1/√˜pi at entry i, and is 0 everywhere else. So, in the notation of Theorem
1.1, ˜A = SA and ˜b = Sb. To prove the theorem, it suffices to show that with high probability,
1. Subspace Embedding: For all x ∈Rd, 1
2∥x∥2 ≤∥SUx∥2 ≤1.5∥x∥2.
2. Approximate Matrix-Vector Multiplication: ∥UT ST S(b −Ax∗)∥2
2 ≤ϵ∥b −Ax∗∥2
2.
The first property is equivalent to ∥UT ST SU−I∥2 ≤1/2. I.e., after subsampling, U should remain
nearly orthogonal. The second property requires that after subsampling with S, the optimal residual,
b−Ax∗, should have small product with U. Note that without subsampling, ∥UT (b−Ax∗)∥2
2 = 0.
We show that both of the above bounds can be established for any sampling method that 1) samples
index i with marginal probability proportional to its leverage score 2) is homogeneous, meaning that
it takes a fixed number of samples k, and 3) produces a distribution over binary vectors satisfying
the following property:
Definition 3.1 (One-sided ℓ∞-independence). Let ξ1, · · · , ξn ∈{0, 1}n be random variables with
joint distribution µ. Let S ⊆[n] and let i, j ∈[n]\S. Define the one-sided influence matrix IS
µ as:
IS
µ (i, j) = Pr
µ [ξj = 1|ξi = 1 ∧ξℓ= 1∀ℓ∈S] −Pr
µ [ξj = 1|ξℓ= 1∀ℓ∈S]
Let ∥IS
µ ∥∞= maxi∈[n]
P
j∈[n] |IS
µ (i, j)|. µ is one-sided ℓ∞-independent with param. Dinf if, for
all S ⊂[n], ∥IS
µ ∥∞≤Dinf. Note that if ξ1, . . . , ξn are truly independent, we have Dinf = 1.
To prove Theorem 1.1, our required subspace embedding property follows immediately from re-
cently established matrix Chernoff-type bounds for sums of random matrices involving one-sided
ℓ∞-independent random variables in (Kaufman et al., 2022). In fact, this work is what inspired us
to consider this property, as it is the minimal condition under which such bounds are known to hold.
Our main theoretical contribution is thus to prove the matrix-vector multiplication property. We do
this by generalizing the approach of (Drineas et al., 2006) (which holds for independent random
samples) to any distribution that satisfies one-sided ℓ∞-independence.
3.1
IMPROVED BOUNDS FOR POLYNOMIAL REGRESSION
We do not believe that Theorem 1.1 can be strengthened in general, as spatially well-spread samples
may not be valuable in all settings. For example, in the case when A = X, the points in X live in d
dimensional space, and we are only collecting O(d log d) samples, so intuitively any set of points is
well-spread. However, we can show that better spatial distribution does offer provably better bounds
in some settings where X is low-dimensional and A is a high dimensional feature transformation. In
particular, consider the case when X is a (quasi) matrix containing just one column, with an infinite
number of rows corresponding to every point t in an interval [ℓ, u]. Every row of A is a polynomial
feature transformation, meaning of the form at = [1, t, t2, . . . , td] for degree d. Now, consider
a target vector b that can be indexed by real numbers in [ℓ, u]. Solving the infinite dimensional
7

regression problem minx ∥Ax −b∥2
2 = minx
R u
ℓ(aT
t x −bt)2dt is equivalent to finding the best
polynomial approximation to b in the ℓ2 norm, a well studied application of leverage score sampling
Cohen and Migliorati (2017). For this problem, we show that pivotal sampling obtains a sample
complexity of O(d/ϵ), improving on independent leverage score sampling by a log d factor.
This result is stated as Theorem 1.2 and proven in Appendix C. Importantly, we note that the de-
pendence on d log d in the general active regression analysis comes from the proof of the subspace
embedding guarantee – the required approximate matrix-multiplication guarantee already follows
with O(d/ϵ) samples. Our proof eliminates the log d by avoiding the use of a matrix Chernoff
bound entirely. Instead, by taking advantage of connections between leverage scores of the polyno-
mial regression problem and the orthogonality measure of the Chebyshev polynomials, we directly
use tools from polynomial approximation theory to prove a subspace embedding bound that holds
deterministically with just O(d) samples. Our approach is similar to a recently result of Kane et al.
(2017), which also obtains O(d/ϵ) sample complexity for active degree-d polynomial regression,
albeit with a sampling method not based on leverage scores.
4
EXPERIMENTS
We experimentally evaluate our pivotal sampling methods on active regression problems with low-
dimensional structure. The benefits of leverage score sampling over uniform sampling for such
problems has already been established (see e.g. (Hampton and Doostan, 2015) or (Gajjar et al.,
2023)) and we provide additional evidence in Appendix D. So, we focus on comparing our pivotal
methods to the widely used baseline of independent Bernoulli leverage score sampling.
Test Problems. We consider several test problems inspired by applications to parametric PDEs. In
these problems, we are given a differential equation with parameters, and seek to compute some
observable quantity of interest (QoI) for different choices of parameters. This can be done directly
by solving the PDE, but doing so is computationally expensive. Instead, the goal is to use a small
number of solutions to the PDE to fit a surrogate model (in our case, a low-degree polynomial) that
approximates the QoI well, either over a given parameter range, or on average for parameters drawn
from a distribution (e.g., Gaussian). The problem is naturally an active regression problem because
we can choose exactly what parameters to solve the PDE for. The first equation we consider models
the displacement x of a damped harmonic oscillator with a sinusoidal force over time t.
d2x
dt2 (t) + cdx
dt (t) + kx(t) = f cos(ωt),
x(0) = x0,
dx
dt (0) = x1.
The equation has four parameters; damping coefficient c, spring constant k, forcing amplitude f, and
frequency ω. As a QoI, we consider the maximum oscillator displacement after 20 seconds. We fix
c, f = 0.5, and seek to approximate this displacement over the range domain k × ω = [1, 3] × [0, 2].
We also consider the heat equation for values of x ∈[0, 1] with a time-dependent boundary equation
and sinusoidal initial condition parameterized by a frequency ω. The heat equation that we consider
describes the temperature f(x, t) by the partial differential equation.
π ∂f
∂t = ∂2f
∂x2 ,
f(0, t) = 0,
f(x, 0) = sin(ωπx),
πe−t + ∂f(1, t)
∂t
= 0.
(a) Damped Harmonic Os-
cillator, degree p = 12.
(b) Heat Equation, degree
p = 12.
(c) Surface Reaction, de-
gree p = 15.
(d) Surface Reaction, de-
gree p = 20.
Figure 4: Results for active polynomial regression for the damped harmonic oscillator QoI, the
heat equation QoI, and the surface reaction model with polynomials varying degree. Our leverage-
score based pivotal method outperforms standard Bernoulli leverage score sampling, suggesting the
benefits of spatially-aware sampling.
8

(a) Target Function.
(b) Bernoulli Sampling.
(c) Pivotal Sampling (our method).
Figure 5: Polynomial approximation to the maximum temperature of a heat diffusion problem, as
a function of time and starting condition. (a) is the target value and both (b) and (c) draw 240
samples using the leverage score and perform polynomial regression of degree 20. However, (b)
uses Bernoulli sampling while (c) employs our PCA-based pivotal sampling.
(a) Leverage Score.
(b) Target Function.
(c) Bernoulli Sampling.
(d) Pivotal Sampling.
Figure 6: Approximation of the surface reaction model when 425 samples are used to fit a degree
25 polynomial. To help readers focus on the area near the origin, we draw [−10, 10]2 box.
As a QoI, we estimate the maximum temperature over all values of x for t ∈[0, 3] and ω ∈[0, 5].
Data Matrix. For both problems, we construct A by uniformly selecting n = 105 data points in the
2-dimensional parameter range of interest. We then add all polynomial features of degree p = 12
as discussed in Section 1.2. We compute sampled entries from the target vector b using standard
MATLAB routines. Results comparing our PCA-based pivotal method and Bernoulli leverage score
sampling are show in Figure 4. We report median normalized error ∥A˜x∗−b∥2
2/∥b∥2
2 after 1000
trials. By drawing more samples from b, the errors of all methods eventually converge to the optimal
error ∥Ax∗−b∥2
2/∥b∥2
2, but clearly the pivotal method requires less samples to achieve a given
level of accuracy, confirming the benefits of spatially-aware sampling. We also visualize results for
the damped harmonic oscillator in Figure 1, showing approximations obtained with 250 samples.
Visualizations for the heat equation are given in Figure 5. For both targets, one can directly see that
pivotal sampling improves the performance over Bernoulli sampling.
We also consider a chemical surface coverage problem from (Hampton and Doostan, 2015). Details
are relegated to Appendix D. Again, we vary two parameters and seek to fit a surrogate model using
a small number of example pairs of parameters. Instead of a uniform distribution, for this problem,
the rows of X are drawn from a Gaussian distribution with 0-mean and 7.5 standard deviation. We
construct A using polynomial features of varying degrees. Convergence results are shown in Figure
4 and the fit visualized for data near the origin in Figure 6. This is a challenging problem since the
target function has sharp threshold behavior that is difficult to approximate with a polynomial. How-
ever, our pivotal based leverage score sampling performs well, providing a better fit than Bernoulli
sampling. Additional experiments, including on 3D problems are reported in Appendix D.
5
CONCLUSION AND FUTURE WORK
In this paper, we introduce a spatially-aware pivotal sampling method and empirically demonstrate
its effectiveness for active linear regression for spatially-structured problems. We prove a general
theorem that can be used to analyze the number of samples required in the agnostic setting for any
similar method that samples with marginal probabilities proportional to the leverage scores using a
distribution that satisfies one-sided ℓ∞independence. We provide a stronger bound for the important
special case of polynomial regression, showing that our method can obtain a sample complexity of
O(d/ϵ), removing a log(d) factor from independent leverage score sampling. This result provides
initial theoretical evidence for the strong performance of pivotal sampling in practice. Extending it
to other spatially structured function classes would be an interesting direction for future work.
9

ACKNOWLEDGEMENTS
Xiaoou Cheng and Jonathan Weare were supported by NSF award 2054306. Christopher Musco
was supported by DOE award DE-SC0022266 and NSF award 2045590. We would like to thank
NYU IT for the use of the Greene computing cluster.
REFERENCES
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. Random Fourier features for kernel ridge regression: Approximation bounds and sta-
tistical guarantees. In Proceedings of the 34th International Conference on Machine Learning
(ICML), 2017.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker, and Amir
Zandieh. A universal sampling method for reconstructing signals with simple fourier transforms.
In Proceedings of the 51st Annual ACM Symposium on Theory of Computing (STOC), 2019.
Julius Borcea, Petter Br¨and´en, and Thomas M. Liggett. Negative dependence and the geometry of
polynomials. Journal of the American Mathematical Society, 22(2):521–567, 2009.
Peter Borwein and Tam´as Erd´elyi. Polynomials and Polynomial Inequalities. Springer New York,
NY, 1995.
Petter Br¨and´en and Johan Jonasson. Negative dependence in sampling. Scandinavian Journal of
Statistics, 39(4):830–838, 2012.
Xue Chen and Michal Derezinski. Query complexity of least absolute deviation regression via robust
uniform convergence. In Proceedings of the 34th Annual Conference on Computational Learning
Theory (COLT), 2021.
Xue Chen and Eric Price. Active regression via linear-sample sparsification. In Proceedings of the
32nd Annual Conference on Computational Learning Theory (COLT), 2019.
Abdellah Chkifa, Albert Cohen, Giovanni Migliorati, Fabio Nobile, and Raul Tempone. Discrete
least squares polynomial approximation with random evaluations - application to parametric and
stochastic elliptic PDEs. ESAIM: M2AN, 49(3):815–837, 2015.
Albert Cohen and Ronald DeVore.
Approximation of high-dimensional parametric pdes.
Acta
Numerica, 24:1–159, 2015.
Albert Cohen and Giovanni Migliorati. Optimal weighted least-squares methods. SMAI Journal of
Computational Mathematics, 3:181–203, 2017.
Michal Derezi´nski, Manfred K. Warmuth, and Daniel Hsu. Leveraged volume sampling for linear
regression. In Advances in Neural Information Processing Systems 31 (NeurIPS), pages 2510–
2519, 2018.
Jean-Claude Deville and Yves Tille. Unequal probability sampling without replacement through a
splitting method. Biometrika, 85(1):89–101, 1998.
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo algorithms for matrices
I: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132–157, 2006.
Devdatt Dubhashi, Johan Jonasson, and Desh Ranjan. Positive influence and negative dependence.
Combinatorics, Probability and Computing, 16(1):29–41, 2007.
Tam´as Erd´elyi and Paul Nevai. Generalized Jacobi weights, Christoffel functions, and zeros of
orthogonal polynomials. Journal of Approximation Theory, 69(2):111–132, 1992.
Tam´as Erd´elyi, Cameron Musco, and Christopher Musco. Fourier sparse leverage scores and ap-
proximate kernel learning. In Advances in Neural Information Processing Systems 33 (NeurIPS),
2020.
10

Aarshvi Gajjar, Chinmay Hegde, and Christopher Musco. Active learning for single neuron models
with Lipschitz non-linearities. In Proceedings of the 26th International Conference on Artificial
Intelligence and Statistics (AISTATS), 2023.
Anton Grafstr¨om, Niklas L P Lundstr¨om, and Lina Schelin. Spatially balanced sampling through
the pivotal method. Biometrics, 68(2):514–520, 2011.
Samuel M. Greene, Robert J. Webber, Timothy C. Berkelbach, and Jonathan Weare. Approximating
matrix eigenvalues by subspace iteration with repeated random sparsification. SIAM Journal on
Scientific Computing, 44(5):A3067–A3097, 2022.
Ling Guo, Akil Narayan, and Tao Zhou. Constructing least-squares polynomial approximations.
SIAM Review, 62(2):483–508, 2020.
Jerrad Hampton and Alireza Doostan. Coherence motivated sampling and convergence analysis
of least squares polynomial chaos regression.
Computer Methods in Applied Mechanics and
Engineering, 290:73–97, 2015.
Jeffrey M. Hokanson and Paul G. Constantine. Data-driven polynomial ridge approximation using
variable projection. SIAM Journal on Scientific Computing, 40(3):A1566–A1589, 2018.
Daniel Kane, Sushrut Karmalkar, and Eric Price. Robust polynomial regression up to the information
theoretic limit. In Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer
Science (FOCS), 2017.
Tali Kaufman, Rasmus Kyng, and Federico Sold´a. Scalar and matrix Chernoff bounds from ℓ∞-
independence. In Proceedings of the 33rd Annual ACM-SIAM Symposium on Discrete Algorithms
(SODA), 2022.
Nora L¨uthen, Stefano Marelli, and Bruno Sudret. Sparse polynomial chaos expansions: Literature
survey and benchmark. SIAM/ASA Journal on Uncertainty Quantification, 9(2):593–649, 2021.
Tung Mai, Anup B. Rao, and Cameron Musco. Coresets for classification – simplified and strength-
ened. In Advances in Neural Information Processing Systems 34 (NeurIPS), 2021.
Raphael A. Meyer, Cameron Musco, Christopher Musco, David P. Woodruff, and Samson Zhou.
Near-linear sample complexity for Lp polynomial regression. In Proceedings of the 34th Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA), 2023.
Alexander Munteanu, Chris Schwiegelshohn, Christian Sohler, and David Woodruff. On coresets
for logistic regression. In Advances in Neural Information Processing Systems 31 (NeurIPS),
2018.
Cameron Musco, Christopher Musco, David P. Woodruff, and Taisuke Yasuda. Active linear re-
gression for ℓp norms and beyond.
In Proceedings of the 63rd Annual IEEE Symposium on
Foundations of Computer Science (FOCS).
Paul Nevai. Bernstein’s inequality in lp for 0 < p < 1. Journal of Approximation Theory, 27(3):
239–243, 1979.
Aditya Parulekar, Advait Parulekar, and Eric Price. L1 Regression with Lewis Weights Subsampling.
In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques
(APPROX/RANDOM 2021), pages 49:1–49:21, 2021.
Robin Pemantle. Towards a theory of negative dependence. Journal of Mathematical Physics, 41,
06 2000.
Robin Pemantle and Yuval Peres. Concentration of Lipschitz functionals of determinantal and other
strong rayleigh measures. Combinatorics, Probability and Computing, 23(1):140–160, 2014.
Friedrich Pukelsheim. Optimal Design of Experiments. Society for Industrial and Applied Mathe-
matics, 2006.
Holger Rauhut and Rachel Ward. Sparse Legendre expansions via ℓ1-minimization. Journal of
Approximation Theory, 164(5):517 – 533, 2012.
11

Tam´as Sarl´os. Improved approximation algorithms for large matrices via random projections. In
Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),
2006.
Keeve M. Siegel. Bounds of the Legendre function. Journal of Mathematics and Physics, 34(1-4):
43–49, 1955.
Lloyd N. Trefethen. Householder triangularization of a quasimatrix. IMA Journal of Numerical
Analysis, 30(4):887–897, 2009.
Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-
tional Mathematics, 12(4):389–434, 2012.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in
Theoretical Computer Science, 10(1–2):1–157, 2014.
Dongbin Xiu. Stochastic Collocation Methods: A Survey, pages 1–18. Springer International Pub-
lishing, 2016.
A
PROBABILITY PRE-PROCESSING
As discussed in Section 1.4, our sampling methods require computing probabilities ˜p1, . . . , ˜pn where
˜pi = min(1, ck · τi) for some fixed constant ck chosen so that Pn
i=1 ˜pi = k. We can find such
probabilities using a simple iterative method, which takes as input initial “probabilities” pi = k
dτi
that are proportional to the leverage scores τ1, . . . , τn of A. Note that pi could be larger than 1 if
k > d. Pseudocode for how to adjust these probabilities is included in Algorithm 3.
Algorithm 3 Probability Ceiling
Input: Number of samples to choose k, inclusion probabilities {p1, · · · , pn}.
1: Set ˜pi = pi for all i ∈[n].
2: while S = {i ∈[n]; ˜pi > 1} is not empty do
3:
˜pi ←1 for all i ∈S.
4:
D = {i ∈[n]; ˜pi = 1}, R = {i ∈[n]; ˜pi < 1}.
5:
˜pi ←
k−|D|
P
i∈R ˜pi ˜pi for all i ∈R.
6: return {˜p1, · · · , ˜pn}
To see that the method returns ˜pi with the desired properties, first note that
k−|D|
P
i∈R ˜pi is always
greater than 1. In particular, at the beginning of the while loop, we always have the invariant that
Pn
i=1 ˜pi = k. Accordingly, P
i∈R ˜pi = k −P
i∈D ˜pi < k −|D|. As a result, at the end of the
algorithm, ˜pi certainly equals min(1, c · τi) for some constant c ≥1. And in fact, it must be that
c = ck by the invariant that Pn
i=1 ˜pi = k.
The main loop in Algorithm 3 terminates as soon as S is empty, which happens after at most k steps.
In practice however, the method usually converges in 2 or 3 iterations. Nevertheless, our primary
objective is to minimize the number of samples required for an accurate regression solution – we do
not consider runtime costs in detail.
B
PROOF OF THEOREM 1.1
In this section, we prove our first theoretical result, which holds for any row sampling distribution
whose marginal probabilities are proportional to the leverage scores of A, and which satisfies a
few additional conditions. In particular, we require that the distribution is both k-homogeneous and
is ℓ∞-independent with constant parameter D. We define the first requirement below, and recall
the definition of ℓ∞-independence from Section 3. For both definitions, we view our subsampling
process as a method that randomly selects a binary vector ξ = {ξ1, · · · , ξn} from {0, 1}n. Entries
of 1 in the vector correspond to rows that are sampled from A (and thus labels that are observed
12

in b). Under this notation, our requirement on marginal probabilities is equivalent to requiring that
E[ξi] = min(1, c · τi) = ˜pi for some fixed constant c, where τi is the i-th leverage score of A.
Definition B.1 (Homogeneity). A distribution over binary vectors ξ
=
{ξ1, · · · , ξn} is k-
homogeneous if all possible realizations of ξ contain exactly k ones.
Definition 3.1 (One-sided ℓ∞-independence). Let ξ1, · · · , ξn ∈{0, 1}n be random variables with
joint distribution µ. Let S ⊆[n] and let i, j ∈[n]\S. Define the one-sided influence matrix IS
µ as:
IS
µ (i, j) = Pr
µ [ξj = 1|ξi = 1 ∧ξℓ= 1∀ℓ∈S] −Pr
µ [ξj = 1|ξℓ= 1∀ℓ∈S]
Let ∥IS
µ ∥∞= maxi∈[n]
P
j∈[n] |IS
µ (i, j)|. µ is one-sided ℓ∞-independent with param. Dinf if, for
all S ⊂[n], ∥IS
µ ∥∞≤Dinf. Note that if ξ1, . . . , ξn are truly independent, we have Dinf = 1.
With these definitions in place, we are ready for our proof:
Proof of Theorem 1.1. As outlined in Section 3, we need to establish two results: a subspace embed-
ding guarantee and an approximate matrix-vector multiplication result. We prove these separately
below, then combine the results to complete the proof of the theorem.
Subspace Embedding. Let S be a subsampling matrix corresponding to a vector ξ selected from
a distribution with the properties above. That is, S ∈Rk×n contains a row for every index i where
ξi = 1. That row has value 1/√˜pi at entry i, and is 0 everywhere else. Let U ∈Rn×d be an
orthonormal span for A’s columns. We will show that, if k = O

d log(d/δ)
α2
· D2
inf

, with prob.
1 −δ,
∥UT ST SU −I∥2 ≤α.
(B.1)
Note that, since
∥SUx∥2
2 −∥Ux∥2
2
 =
xT  UT ST SU −I

x
 ≤∥UT ST SU −I∥2∥x∥2
2, this is
equivalent to showing that, for all x ∈Rd,
(1 −α)∥Ux∥2
2 ≤∥SUx∥2
2 ≤(1 + α)∥Ux∥2
2.
We establish (B.1) using the following matrix Chernoff bound from (Kaufman et al., 2022):
Lemma B.1 (Matrix Chernoff for ℓ∞-independent Distributions). Let ξ1, · · · , ξn be binary ran-
dom variables with joint distribution µ that is z-homogeneous for any positive integer z and ℓ∞-
independent with parameter Dinf. Let Y1, . . . , Yn ∈Rd×d be positive semidefinite (PSD) matrices
with spectral norm ∥Yi∥2 ≤R for some R > 0. Let µmax = λmax(Eξ∼µ[Pn
i=1 ξiYi]) and
µmin = λmin(Eξ∼µ[Pn
i=1 ξiYi]). Then, for any α ∈(0, 1) and a fixed constant c,
Pr
"
λmax
 n
X
i=1
ξiYi
!
≥(1 + α)µmax
#
≤d exp
 
−α2µmax
cRD2
inf
!
,
Pr
"
λmin
 n
X
i=1
ξiYi
!
≤(1 −α)µmin
#
≤d exp
 
−α2µmin
cRD2
inf
!
.
Let D = {i ∈[n] : ˜pi = 1} and R = {i ∈[n] : ˜pi < 1}. Letting ui denote the i-th row of U,
UT ST SU can be decomposed as:
UT ST SU =
X
i∈R
ξi
uiuT
i
˜pi
+
X
i∈D
uiuT
i .
Recall that ˜pi = min(1, c·τi) for some constant c, and in fact, since Pn
i=1 ˜pi = k and Pn
i=1 τi ≤d,
it must be that c ≥k
d. Define pi = k
dτi. For each i ∈D, let mi be an integer such that mi ≥pi.
For i ∈D, pi =
k
dτi ≥1, so mi ≥1. Additionally let ξ1
i , . . . , ξmi
i
= ξi be variables that are
deterministically 1. Then we can trivially split up the second sum above so that:
UT ST SU =
X
i∈R
ξi
uiuT
i
˜pi
+
X
i∈D
mi
X
j=1
ξj
i
uiuT
i
mi
.
(B.2)
13

It is easy to see that {ξi : i ∈R} ∪{ξj
i : i ∈D, j ∈[mi]} are ℓ∞-independent random variables
with parameter Dinf and form a distribution that is ¯k homogeneous for ¯k = k + P
i∈D(mi −1). So,
noting that uiuT
i is PSD, we can apply Lemma B.1 to (B.2). We are left to bound R and µmax.
To bound R, for i ∈R, let Yi = uiuT
i
˜pi . For such i, we have that ˜pi ≥pi. So,
∥Yi∥2 = ∥uiuT
i ∥2
˜pi
= ∥ui∥2
2
˜pi
= τi
˜pi
≤τi
pi
= d
k .
I.e., for all i ∈R, ∥Yi∥2 ≤R for R = d
k. Additionally, for i ∈D, let Yi = uiuT
i
mi . Since we chose
mi ≥pi, by the same argument, we have that ∥Yi∥2 ≤R for R = d
k.
Next, we bound µmax by simply noting that
E

UT ST SU

=
X
i∈R
E[ξi]uiuT
i
˜pi
+
X
i∈D
uiuT
i =
X
i∈R
uiuT
i +
X
i∈D
uiuT
i = I,
where I denotes the d × d identity matrix. It follows that µmax = µmin = 1.
Plugging into Lemma B.1 and applying a union bound, we obtain
Pr

∥UT ST SU −I∥2 ≥α

≤d exp

−kα2
c′D2
infd

,
where c′ is a fixed constant. Setting k = O( d log(d/δ)
α2
· D2
inf), we have ∥UT ST SU −I∥2 ≤α with
probability at least 1 −δ. This establishes (B.1).
Approximate Matrix-Vector Multiplication. With our subspace embedding result in place, we
move onto proving the necessary approximate matrix-vector multiplication result introduced in
Section 3. In particular, we wish to show that, with good probability, ∥UT ST S(b −Ax∗)∥2
2 ≤
ϵ∥b −Ax∗∥2
2 where x∗= arg minx ∥Ax −b∥2
2. Since the residual of the optimal solution,
b −Ax∗, is the same under any column transformation of A, we have b −Ax∗= b −Uy∗where
y∗= arg miny ∥Uy −b∥2
2. We can then equivalently show that, if we take k = O
  d
ϵδ · Dinf

samples, with probability 1 −δ,
∥UT ST S(b −Uy∗)∥2
2 ≤ϵ∥b −Uy∗∥2
2.
(B.3)
As in the proof for independent random samples (Drineas et al., 2006), we will prove (B.3) by
bounding the expected squared error and applying Markov’s inequality. In particular, we have
Pr

∥UT ST S(b −Uy∗)∥2
2 ≥ϵ∥b −Uy∗∥2
2

≤E

∥UT ST S(b −Uy∗)∥2
2

ϵ∥b −Uy∗∥2
2
.
(B.4)
Let z = b −Uy∗and note that UT z = 0. The numerator on the right side can be transformed as
E

∥UT ST Sz∥2
2

= E

∥UT ST Sz −UT z∥2
2

= E

∥UT (ST S −I)z∥2
2

.
Note that above ST S −I is a diagonal matrix with i-th diagonal entry equal to 1
˜pi −1 if ξi = 1 and
−1 if ξi = 0. Expanding the ℓ2-norm and using that ξi = 1 and ˜pi = 1 for i /∈R, we have
E

∥UT ST Sz∥2
2

=
d
X
j=1
E


 n
X
i=1
 ξi
˜pi
−1

uijzi
!2
=
d
X
j=1
E


 X
i∈R
 ξi
˜pi
−1

uijzi
!2

=
d
X
j=1
X
i∈R
X
l∈R
cil
˜pi˜pl
uijziuljzl,
where cil = Cov(ξi, ξl) = E[(ξi −˜pi)(ξl −˜pl)] = E[ξiξl] −˜pi˜pl. We will show that
d
X
j=1
X
i∈R
X
l∈R
cil
˜pi˜pl
uijziuljzl ≤Dinf
d
X
j=1
X
i∈R
u2
ijz2
i
˜pi
,
14

where Dinf is the ℓ∞-independence parameter. It suffices to show that for every j, we have
X
i∈R
X
l∈R
cil
˜pi˜pl
uijziuljzl ≤Dinf
X
i∈R
u2
ijz2
i
˜pi
.
(B.5)
Define a symmetric matrix M ∈R|R|×|R| with entries mil =
cil
√˜pi
√˜pl and a vector v ∈R|R| with
entries vi = uijzi
√˜pi . The desired result in (B.5) can be expressed as
vT Mv ≤Dinf∥v∥2
2.
With S = ∅, the one-sided ℓ∞-independence condition implies that, for all i ∈[n],
X
l∈R

E[ξiξl]
˜pi
−˜pl
 =
X
l∈R
√˜pl
√˜pi
|mil| ≤Dinf.
Equivalently, if we define a diagonal matrix Λ ∈R|R|×|R| such that Λii = √˜pi, we have shown:
∥Λ−1MΛ∥∞≤Dinf,
where for a matrix B, ∥B∥∞denotes maxi
P
l |Bil| = maxx
∥Bx∥∞
∥x∥∞. It follows that the largest
eigenvalue of Λ−1MΛ is at most Dinf and thus, the largest eigenvalue of M is also at most Dinf.
Therefore, we have vT Mv ≤Dinf∥v∥2
2. Considering that ˜pi ≥pi = k
dτi for i ∈R, we have
d
X
j=1
E


 X
i∈R
 ξi
˜pi
−1

uijzi
!2
≤Dinf
d
X
j=1
X
i∈R
u2
ijz2
i
˜pi
= Dinf
X
i∈R
z2
i
˜pi
d
X
j=1
u2
ij = Dinf
X
i∈R
z2
i
˜pi
τi
≤Dinf
d
k ∥z∥2
2.
Recalling that z = Uy∗−b, we can plug into (B.4) with k = O
  d
ϵδ · Dinf

samples, which proves
that (B.3) holds with probability at least 1 −δ.
Putting it all together. With (B.1) and (B.3) in place, we can prove our main result, (1.3) of
Theorem 1.1. We follow a similar approach to (Woodruff, 2014). By reparameterization, proving
this inequality is equivalent to showing that
∥U˜y∗−b∥2
2 ≤(1 + ϵ)∥Uy∗−b∥2
2,
(B.6)
where ˜y∗= arg miny ∥SUy −Sb∥2
2 and y∗= arg miny ∥Uy −b∥2
2. Since y∗is the minimizer of
∥Uy −b∥2
2, we have ∇y∥Uy −b∥2
2 = 2UT (Uy −b) = 0 at y∗. This indicates that Uy∗−b is
orthogonal to any vector in the column span of U. Particularly, Uy∗−b is orthogonal to U˜y∗−Uy∗.
Therefore, by the Pythagorean theorem, we have
∥U˜y∗−b∥2
2 = ∥Uy∗−b∥2
2 + ∥U˜y∗−Uy∗∥2
2 = ∥Uy∗−b∥2
2 + ∥˜y∗−y∗∥2
2.
(B.7)
So to prove (B.6), it suffices to show that
∥˜y∗−y∗∥2
2 ≤ϵ∥Uy∗−b∥2
2.
Applying (B.1) with k = O(d log d·D2
inf +d·Dinf/ϵ) and δ = 1/200, we have that, with probability
99.5/100, ∥UT ST SU −I∥2 ≤1
2. Then, by triangle inequality,
∥˜y∗−y∗∥2 ≤∥UT ST SU(˜y∗−y∗)∥2 + ∥UT ST SU(˜y∗−y∗) −(˜y∗−y∗)∥2
≤∥UT ST SU(˜y∗−y∗)∥2 + ∥UT ST SU −I∥2∥˜y∗−y∗∥2
≤∥UT ST SU(˜y∗−y∗)∥2 + 1
2∥˜y∗−y∗∥2.
(B.8)
Rearranging, we conclude that ∥˜y∗−y∗∥2
2 ≤4∥UT ST SU(˜y∗−y∗)∥2
2. Since ˜y∗is the minimizer
of ∥SUy −Sb∥2
2, we have ∇y∥SUy −Sb∥2
2 = 2(SU)T (SU˜y∗−Sb) = 0. Thus,
∥UT ST SU(˜y∗−y∗)∥2
2 = ∥UT ST (SU˜y∗−Sb + Sb −SUy∗)∥2
2
= ∥UT ST S(b −Uy∗)∥2
2.
Applying (B.3) with δ = 1/200 and combining with (B.8) using union bound, we thus have that
with probability 99/100,
∥˜y∗−y∗∥2
2 ≤4∥UT ST SU(˜y∗−y∗)∥2
2 = 4∥UT ST S(b −Uy∗)∥2
2 ≤4ϵ∥Uy∗−b∥2
2.
(B.9)
Plugging into (B.7) and adjusting ϵ by a constant factor completes the proof of Theorem 1.1.
15

B.1
PROOF OF COROLLARY 1.1
We briefly comment on how to derive Corollary 1.1 from our main result. By definition of the
method, we immediately have that our binary-tree-based pivotal sampling is k-homogeneous, so we
just need to show that it produces a distribution over samples that is ℓ∞-independent with constant
parameter Dinf. This fact can be derived directly from a line of prior work. In particular, (Br¨and´en
and Jonasson, 2012) proves that binary-tree-based pivotal sampling satisfies negative association,
(Pemantle and Peres, 2014) proves that negative association implies a stochastic covering property,
and (Kaufman et al., 2022) shows that any distribution satisfying the stochastic covering property
has ℓ∞-independence parameter at most D = 2. We also give an arguably more direct alternative
proof below based on a natural conditional variant of negative correlation.
Proof of Corollary 1.1. (Br¨and´en and Jonasson, 2012) proves that binary-tree-based pivotal sam-
pling is conditionally negatively associated (CNA). Given a set C ⊆[n] and a vector c ∈{0, 1}|C|,
we denote the condition ξc = ci for all i ∈C by C. Conditional negative association asserts that, for
all C, any disjoint subsets S and T of {ξ1, · · · , ξn}, and any non-decreasing functions f and g,
E[f(S)|C] · E[g(T )|C] ≥E[f(S)g(T )|C].
When S and T are singletons and f and g are the identity functions, we have
E[ξi|C] · E[ξj|C] ≥E[ξiξj|C].
(B.10)
Since E[ξi|C] = Pr[ξi = 1|C], we also have
Pr[ξi = 1|C] Pr[ξj = 1|C] ≥Pr[ξi = 1 ∧ξj = 1|C]
Pr[ξi = 1|C] ≥Pr[ξi = 1 ∧ξj = 1|C]
Pr[ξj = 1|C]
= Pr[ξi = 1|ξj = 1 ∧C].
(B.11)
In words, the entries of our vector ξ are negatively correlated, even conditioned on fixing any subset
of entries in the vector. We will use this fact to show that P
j∈[n] |IS
µ (i, j)| ≤2 for all i ∈[n], where
IS
µ is as defined in Definition 3.1. For a fixed i, let qi = Prξ∼µ[ξi = 1|ξℓ= 1∀ℓ∈S]. Then, we
have |IS
µ (i, j)| = 0 for j ∈S, |IS
µ (i, j)| = 1 −qi for j = i, and P
j∈[n]\S∪{i} |IS
µ (i, j)| = 1 −qi.
The last fact follows from k-homogeneity, i.e. that Pn
i=1 qi = k, and (B.11), which implies that
IS
µ (i, j) ≤0 for all j in [n]\S ∪{i}, so P
j∈[n]\S∪{i} |IS
µ (i, j)| =
P
j∈[n]\S∪{i} IS
µ (i, j)
. Thus,
we have P
j∈[n] |IS
µ (i, j)| = 2 −2qi ≤2.
C
PROOF OF THEOREM 1.2
In this section we prove Theorem 1.2, which shows that pivotal obtains a better sample complexity
for polynomial regression on an interval than independent leverage score sampling. Since we can
always shift and scale our target function, without loss of generality we can take [ℓ, u] to be the
interval [−1, 1]. We will sample from the infinite set of points on this interval. Each point t ∈[−1, 1]
will correspond to a row in a regression matrix A with d+1 columns and an infinite number of rows.
Such an object is sometimes referred to as a quasimatrix (Trefethen, 2009).3 The row with index t
equals at = [1, t, t2, . . . , td]. The leverage score for the row with index t is defined analogously to
Equation (1.2) as:
τ(t) = max
x∈Rd+1
(xT at)2
R 1
−1(xT as)2ds
.
(C.1)
Note that
R 1
−1 τ(t)dt = (d + 1) since A has d + 1 columns. We will sample k points from [−1, 1]
interval with the probability of sampling point t to be proportional to τ(t). To do so, we renormalize
τ(t) and consider sampling point t with probability proportional to kτ(t)
d+1 . This is the analog of
sampling with probability ˜pi in the discrete case, since
R 1
−1
kτ(t)
d+1 dt = k. Then we apply pivotal
3We refer the reader to Avron et al. (2019), Erd´elyi et al. (2020), or Chen and Price (2019) for a more in
depth treatment of leverage score sampling and active linear regression for quasimatrices.
16

sampling in the infinite point limit, where we choose the pivotal competition order so that points in
[−1, 1] compete with each other from left to right across the interval. In this limit, the probability
carried along in the competition will, at one point, accumulate to exactly 1. This defines the interval
I1 whose left endpoint is −1 and its right endpoint is defined so that
R
I1
kτ(t)
d+1 dt = 1. This means
that the winner in the last competition in I1 will be sampled. A new competition then starts for the
next point, until the renormalized leverage score accumulates to 1 again and the winner is sampled.
We can define Ii(i = 1, 2, . . . , k) to be adjacent intervals with
R
Ii
kτ(t)
d+1 dt = 1. It can be seen
that pivotal sampling in the infinite point limit will always sample exactly one point from each of
I1, . . . , Ik. This is actually the only property of pivotal sampling we will need to prove Theorem 1.2,
although we note that, within an interval Ii, point t is selected with probability proportional to
τ(t)/
R
Ii τ(s)ds.
To prove a sample complexity bound on O(d/ϵ) for polynomial regression, it suffices to prove that
a constant factor subspace embedding guarantee holds when selecting O(d) samples from [−1, 1]
using the pivotal sampling process above. Observe that the required ϵ-error approximate matrix
multiplication guarantee from Section 3 already holds with O(d/ϵ) samples collected via the pivotal
method with leverage score marginals, and this analysis extends to the infinite quasi-matrix setting
(see Appendix C.1). It is only the subspace embedding guarantee that adds an extra log d factor to the
sample complexity. Since this factor is inherent to our use of a matrix Chernoff bound in analyzing
the general case, to prove a tighter bound for polynomial regression we use a direct analysis that
avoids matrix Chernoff entirely. In particular, our main result of this section is as follows:
Theorem C.1. Let k = O
  d
α

points t1, . . . , tk be selected from [−1, 1] via pivotal sampling.
Specifically, exactly one point ti is sampled from each interval I1, . . . , Ik with probability propor-
tional to its leverage score. We have (deterministically) that for any degree d polynomial p, with
w(t) = τ(t)
d+1,

Z 1
−1
p(t)2dt −1
k
k
X
i=1
p(ti)2
w(ti)
 ≤α
Z 1
−1
p(t)2dt.
(C.2)
To translate from the notation above to the notation used in (B.1), note that
R 1
−1 p(t)2dt = ∥Ux∥2
2
where U is an orthogonal span for the quasi-matrix A defined above, and x is a d + 1 dimensional
vector containing coefficients of the polynomial p in the basis U. Correspondingly, 1
k
Pk
i=1
p(ti)2
w(ti) =
∥SUx∥2
2.
Proof of Theorem 1.2. With Theorem C.1 providing the required subspace embedding guarantee
from (B.1), the proof of Theorem 1.2 is essentially identical to the proof for the discrete case (Theo-
rem 1.1). The only remaining requirement is the required approximate matrix-vector multiplication
guarantee from (B.3). For completeness, we provide a direct proof in the polynomial regression case
in Appendix C.1.
We build up to the proof of Theorem C.1 by introducing several intermediate results. Our approach
is inspired by a result of Kane et al. (2017), which also obtains O(d/ϵ) sample complexity for active
degree-d polynomial regression, albeit with a sampling distribution not based on leverage scores. In
particular, they prove the same guarantees as Theorem C.1, but where points are selected uniformly
from k intervals that evenly partition the Chebyshev polynomial weight function 1/
√
1 −t2. As
we will see, the leverage scores for polynomials closely approximate this weight function. This
connection is well known, as the leverage score function from (C.1) is exactly proportional to the
inverse of the polynomial “Christoffel function” under the uniform measure, a well-studied function
in approximation theory.
For simplicity, from now on we denote f(t) = p(t)2 and describe the main structure of the proof
of Theorem C.1. The left hand side of (C.2) can be decomposed into a sum of errors in individual
intervals as

Z 1
−1
f(t)dt −
k
X
i=1
1
k
f(ti)
w(ti)
 ≤
k
X
i=1
Z
Ii

f(t)
w(t) −f(ti)
w(ti)
 w(t)dt.
(C.3)
17

In each interval Ii, noting that by the definition of Ii,
R
Ii w(t)dt = 1
k. So we have
Z
Ii

f(t)
w(t) −f(ti)
w(ti)
 w(t)dt =
Z
Ii

Z t
ti
f ′(s)w(s) −f(s)w′(s)
w2(s)
ds
 w(t)dt
≤
Z
Ii
Z
Ii
|f ′(s)|w(s) + f(s)|w′(s)|
w2(s)
ds

w(t)dt
= 1
k
Z
Ii
|f ′(t)|
w(t) dt + 1
k
Z
Ii
f(t)|w′(t)|
w2(t)
dt.
(C.4)
Therefore,

Z 1
−1
f(t)dt −
k
X
i=1
1
k
f(ti)
w(ti)
 ≤1
k
Z 1
−1
|f ′(t)|
w(t) dt + 1
k
Z 1
−1
f(t)|w′(t)|
w2(t)
dt.
(C.5)
Then, proving the upper bound in Theorem C.1 boils down to establishing a lower bound for w(t), an
upper bound for |w′(t)|, and the connection between
R 1
−1
|f ′(t)|
w(t) dt and
R 1
−1 f(t)dt, which is related
to the lower bound for w(t).
Both f(t) and w(t) are polynomials. This is key to providing both pointwise and (weighted) integral
upper bounds of their derivatives by their function values and integrals. In polynomial approximation
theory, Markov-Bernstein inequalities address exactly this point.
We begin with the integral form of the Markov-Bernstein inequalities from Borwein and Erd´elyi
(1995) and Nevai (1979).
Proposition C.2 (L1 Bernstein’s inequality). For any degree d polynomial p(t), with a universal
constant C0,
Z 1
−1

p
1 −t2 p′(t)
 dt ≤C0 d
Z 1
−1
|p(t)|dt.
(C.6)
Proposition C.3 (L1 Markov’s inequality). For any degree d polynomial p(t), with a universal
constant C1,
Z 1
−1
|p′(t)| dt ≤C1 d2
Z 1
−1
|p(t)|dt.
(C.7)
The integral on the left hand side of (C.6) is weighted by
√
1 −t2. This weight is negligible near
the boundary of the interval [−1, 1], which, to some extent, explains the milder O(d) dependence on
the right hand side of (C.6) compared to the O(d2) dependence in (C.7).
(C.6) and (C.7) also indicate that a lower bound on w(t) of the form
1
√
1−t2 or a constant can
cooperate with |f ′(t)| to yield an integral bound. This is indeed realizable, and we will prove that
the lower bound on w(t) is of the two different forms in different parts of the interval [−1, 1]. It
is roughly the minimum of these two, as
1
√
1−t2 blows up near the boundary. We define the middle
region to be a centered subinterval in [−1, 1] that is ∼
1
d2 away from the boundary, and the boundary
region to be the two subintervals of length ∼
1
d2 near the boundary. The exact expression of these
two regions will be clear when we state different lower bounds of w(t). There we note that these
two regions are overlapping so that we get a lower bound on w(t) on the whole interval. Matching
this pattern of lower bounds, we will also apply different upper bounds of w′(t) in these two regions.
Lower bound on w(t) in the middle region. Erd´elyi and Nevai (1992) states the following bound
on τ(t), which shows the relation between the leverage score and the Chebyshev polynomial weight
function. See Section 4.3 in Meyer et al. (2023) for further discussion of this result.
Proposition C.4. With a constant C2 > 0,
τ(t) ≥C2(d + 1)
π
√
1 −t2 ,
w(t) ≥
C2
π
√
1 −t2 ,
for |t| ≤
s
1 −
9
(d −1)2 .
(C.8)
18

Lower bound on w(t) in the boundary region. This requires a bit more effort than the case of the
middle region. Inspired by (C.7), we aim for the following bound on τ(t) and w(t).
Proposition C.5. With constants C3 > 0, c > 9
2,
τ(t) ≥C3d(d + 1),
w(t) ≥C3d,
for |t| > 1 −c
d2 .
(C.9)
We specify c > 9
2 because this implies that, at least for d large enough, the middle region, defined as
h
−
q
1 −
9
(d−1)2 ,
q
1 −
9
(d−1)2
i
, and the boundary region, defined as

−1, −1 +
c
d2

∪

1 −
c
d2 , 1

,
have overlap.
To prove Proposition C.5, we need to employ the explicit expression for τ(t) in terms of Legendre
polynomials and apply properties of Legendre polynomials. We denote the unnormalized degree
d Legendre polynomial by Pd(t) and we fix Pd(1) = 1. We use Ld(t) to denote the normalized
Legendre polynomials satisfying
R 1
−1 L2
d(t)dt = 1. There are classical, explicit bounds for Ld(t)
and its derivative on the interval [−1, 1].
Lemma C.6 (See e.g. Siegel (1955)). For |t| ≤1, the normalized degree d Legendre polynomials
Ld, satisfy
|Ld(t)| ≤Ld(1) =
r
d + 1
2,
|L′
d(t)| ≤d(d + 1)
2
r
d + 1
2.
(C.10)
Proof of Proposition C.5: Since τ(t) and w(t) are related as w(t) = τ(t)
d+1, we only need to prove the
bound for τ(t). The proof follows from the fact that, τ(t) can be equivalently written as the squared
norm of the t-th row in any orthonormal basis for A,
τ(t) =
d
X
i=0
L2
i (t).
(C.11)
See e.g. Equation (4) in Meyer et al. (2023) for this equivalency. We will then lower bound τ(t)
by deriving lower bounds for the individual summands, ultimately arguing that for |t| > 1 −
c
d2 ,
τ(t) ≳d2. Lower bounds on individual L2
i (t) can be derived with an upper bound on the derivative,
since we know at the boundary Li(1) =
q
i + 1
2 exactly and L2
i (t) is an even function. In particular,
by Lemma C.6,
|(L2
i )′(t)| = |2Li(t)L′
i(t)| ≤i(i + 1
2)(i + 1).
Therefore, for any c > 0 and |t| > 1 −
c
d2 ,
L2
i (t) ≥(i + 1
2) −i(i + 1
2)(i + 1) c
d2 .
For a given c, L2
i (t) is thus positive and of order ≳i as long as i ≤d/c′ for a sufficiently large
constant c′ > 1. A careful algebraic manipulation will show that we can choose c > 9
2. With the
classical formula for summing a linear growth sequence, we have for some constant C3,
τ(t) =
d
X
i=0
L2
i (t) ≥
d/c′
X
i=0
L2
i (t) ≥C3d(d + 1).
Now we move on to upper bounds on w′(t) in the two regions matching the form of w(t) so that
some cancellation can occur to leave a clean integral
R
Ii f(t)dt in (C.5). The classical Markov-
Bernstein inequalities in Borwein and Erd´elyi (1995) bound the pointwise value of the derivative of
a polynomial by its maximum function value.
Lemma C.7 (Bernstein’s inequality). For any degree d real polynomial p(t),
|p′(t)| ≤
d
√
1 −t2
sup
t∈[−1,1]
|p(t)|,
for −1 < t < 1.
(C.12)
19

Lemma C.8 (Markov’s inequality). For any degree d real polynomial p(t),
|p′(t)| ≤d2
sup
t∈[−1,1]
|p(t)|,
for −1 < t < 1.
(C.13)
Both Lemma C.7 and Lemma C.8 are valid on the whole interval [−1, 1], but one of them is tighter
than the other depending on where t sits. Although w′(t) itself is a degree 2d −1 polynomial, it
turns out that the bounds directly given by Lemma C.7 and Lemma C.8 are too crude. However, we
can unwrap more structure in τ ′(t) by applying the recurrence relation of Legendre polynomials.
Proposition C.9 (Neat expression of τ ′(t)).
τ ′(t) = P ′
d+1(t)P ′
d(t).
(C.14)
Proof. We write τ(t) in terms of the unnormalized Legendre polynomials Pn as
τ(t) =
d
X
i=0
L2
i (t) =
d
X
i=0
(i + 1
2)P 2
i .
Pn and P ′
n are related by the recurrence relation (2n + 1)Pn = P ′
n+1 −P ′
n−1 with P ′
0 = 0, so
τ ′(t) =
d
X
i=0
(2i + 1)PiP ′
i =
d
X
i=1
P ′
i(P ′
i+1 −P ′
i−1) = P ′
d+1P ′
d.
Upper bound on w′(t) in the middle region.
Proposition C.10.
|τ ′(t)| ≤d(d + 1)
1 −t2 ,
|w′(t)| ≤
d
1 −t2 ,
for −1 < t < 1.
(C.15)
Proof. This follows from Proposition C.9 and Bernstein’s inequality in Lemma C.7 for Legendre
polynomials Pd and Pd+1 by noting that the maximum of these polynomials are Pi(1) = 1, ∀i.
Upper bound on w′(t) in the boundary region. The bound in Proposition C.10 will blow up near
the boundary. This is caused by the blowing up of Bernstein’s inequality in Lemma C.7. We should
use the tighter upper bound offered by Markov’s inequality in Lemma C.8 and a similar proof will
give the following bound.
Proposition C.11.
|τ ′(t)| ≤d2(d + 1)2,
|w′(t)| ≤d2(d + 1),
for −1 ≤t ≤1.
(C.16)
As mentioned, Proposition C.10 and Proposition C.11 cannot be obtained by treating τ(t) as a
general 2d −1 polynomial. In fact, directly applying Lemma C.7 and Lemma C.8 to τ(t), we will
get instead
|τ ′(t)| ≤(2d −1)(d + 1)2
2
√
1 −t2
and
|τ ′(t)| ≤1
2(2d −1)2(d + 1)2,
which is significantly worse in the middle region.
With the previous intermediate results in place, we are now ready to prove our main claim, following
the proof of Lemma 2.1 in Kane et al. (2017) for intervals defined by the Chebyshev measure.
Proof of Theorem C.1. Recall we start with (C.5) as

Z 1
−1
f(t)dt −
k
X
i=1
1
k
f(ti)
w(ti)
 ≤1
k
Z 1
−1
|f ′(t)|
w(t) dt + 1
k
Z 1
−1
f(t)|w′(t)|
w2(t)
dt.
(C.17)
20

We will use different pointwise bounds of w(t) and w′(t) in different parts of the interval. For the
middle region, by Proposition C.4 and Proposition C.2 we have
1
k
Z q
1−
9
(d−1)2
−
q
1−
9
(d−1)2
|f ′(t)|
w(t) dt ≤
π
C2k
Z 1
−1
p
1 −t2|f ′(t)|dt ≤2πC0d
C2k
Z 1
−1
f(t)dt.
(C.18)
Also, by Proposition C.4 and Proposition C.10, |w′(t)|
w2(t) ≤
d
1−t2
C2
2
π2(1−t2)
= π2d
C2
2 . Therefore,
1
k
Z q
1−
9
(d−1)2
−
q
1−
9
(d−1)2
f(t)|w′(t)|
w2(t)
dt ≤π2d
C2
2k
Z 1
−1
f(t)dt.
(C.19)
For the boundary region, by Proposition C.5 and Proposition C.3 we have
1
k
Z
[−1,−1+ c
d2 ]∪[1−c
d2 ,1]
|f ′(t)|
w(t) dt ≤
1
C3kd
Z 1
−1
|f ′(t)|dt ≤4C1d
C3k
Z 1
−1
|f(t)|dt.
(C.20)
Also, by Proposition C.5 and Proposition C.11, we have |w′(t)|
w2(t) ≤(d+1)d2
C2
3d2
≤2d
C2
3 . Therefore,
1
k
Z
[−1,−1+ c
d2 ]∪[1−c
d2 ,1]
f(t)|w′(t)|
w2(t)
dt ≤2d
C2
3k
Z 1
−1
f(t)dt.
(C.21)
Finally, noticing that the middle region
h
−
q
1 −
9
(d−1)2 ,
q
1 −
9
(d−1)2
i
and the boundary region

−1, −1 +
c
d2

∪

1 −
c
d2 , 1

are overlapping, we can further upper bound the right hand side of
(C.17) by adding up (C.18), (C.19), (C.20), and (C.21), so that

Z 1
−1
f(t)dt −
k
X
i=1
1
k
f(ti)
w(ti)
 ≤
2πC0
C2
+ π2
C2
2
+ 4C1
C3
+ 2
C2
3
 d
k
Z 1
−1
f(t)dt.
(C.22)
Then, by taking k = O( d
α), we prove the statement.
C.1
DIRECT PROOF OF APPROXIMATE MATRIX-VECTOR MULTIPLICATION BOUND
Approximate matrix-vector multiplication bound for Theorem 1.2. Let U be an orthogonal basis of
the column span of the polynomial regression quasi-matrix A, as defined at the beginning of Ap-
pendix C. As in the finite dimensional setting, our goal is to prove that, if we take k = O
  d
ϵδ

samples via pivotal sampling, using those samples to construct a sampling matrix S, then with prob-
ability 1 −δ,
∥UT ST S(b −Uy∗)∥2
2 ≤ϵ∥b −Uy∗∥2
2.
(C.23)
We start with the key step (B.4) in the matrix case as
Pr

∥UT ST S(b −Uy∗)∥2
2 ≥ϵ∥b −Uy∗∥2
2

≤E

∥UT ST S(b −Uy∗)∥2
2

ϵ∥b −Uy∗∥2
2
.
(C.24)
Our goal is to prove E

∥UT ST S(b −Uy∗)∥2
2

= O( d
k)∥b −Uy∗∥2
2. We have that U’s i-th
column is the normalized Legendre polynomial Li. Let p∗= arg mindegree d polynomial p ∥p −b∥2
2.
Then Uy∗= p∗, where y∗is a length d + 1 vector holding the coefficients of p∗when expanded
in the basis of Legendre polynomial Li’s. So the expectation can be written as
E

∥UT ST S(b −Uy∗)∥2
2

= E


d
X
j=0
 k
X
i=1
Lj(ti)(p∗(ti) −b(ti))
kw(ti)
!2
.
(C.25)
21

Note that each ti is chosen independently from Ii with probability density function in Ii as
τ(t)/
R
Ii τ(t)dt = kw(t). So we can calculate the expectation in (C.25) to get
E

∥UT ST S(b −Uy∗)∥2
2

= E


d
X
j=0
k
X
i=1
Lj(ti)(p∗(ti) −b(ti))
kw(ti)
2


=
d
X
j=0
k
X
i=1
Z
Ii
(Lj(t)(p∗(t) −b(t)))2
kw(t)
dt
=
d
X
i=1
Z
Ii
τ(t)(p∗(t) −b(t))2
kw(t)
dt
= d + 1
k
Z 1
−1
(p∗(t) −b(t))2dt,
(C.26)
where we use (C.11) in the third equality. The integral in the last line is exactly ∥b −Uy∗∥2
2, so we
prove that E

∥UT ST S(b −Uy∗)∥2
2

= d+1
k ∥b −Uy∗∥2
2. Plugging this back into (C.24), we get
Pr

∥UT ST S(b −Uy∗)∥2
2 ≥ϵ∥b −Uy∗∥2
2

≤d + 1
ϵk .
(C.27)
This shows that if we take k = O( d
ϵδ) samples via pivotal sampling, then with probability 1 −δ,
∥UT ST S(b −Uy∗)∥2
2 ≤ϵ∥b −Uy∗∥2
2.
D
COMPLEMENTARY EXPERIMENTS
In Section 4, we conduct experiments using three targets; a damped harmonic oscillator, a heat
equation, and a chemical surface reaction. Here, we show the results of additional simulations.
Thus far, the original domain of the experiments is 2D for visualization purposes. Here, we consider
the 3D original domain by freeing one more parameter in the damped harmonic oscillator model.
We also summarize the number of samples required to achieve a given target error for all four test
problems. To corroborate the discussion in Section 1.1, we provide a simulation result showing that
leverage score sampling outperforms uniform sampling. Lastly, the performance of the randomized
BSS algorithm from Chen and Price (2019) on the 2D damped harmonic oscillator target is displayed
which supports our discussion in Section 1.3. Before showing the simulation results, we begin this
section with a deferred detail of the chemical surface coverage target in Section 4.
Chemical Surface Coverage. As explained in (Hampton and Doostan, 2015), the target function
models the surface coverage of certain chemical species and considers the uncertainty ρ which is pa-
rameterized by absorption α, desorption γ, the reaction rate constant κ, and time t. Given a position
(x, y) in 2D space, our quantity of interest ρ is modeled by the non-linear evolution equation:
dρ
dt = α(1 −ρ) −γρ −κ(1 −ρ)2ρ,
ρ(t = 0) = 0.9,
α = 0.1 + exp(0.05x),
γ = 0.001 + 0.01 exp(0.05y)
(D.1)
In this experiment, we set κ = 10 and focus on ρ after t = 4 seconds.
3D Damped Harmonic Oscillator. The damped harmonic oscillator model is given as (restated),
d2x
dt2 (t) + cdx
dt (t) + kx(t) = f cos(ωt),
x(0) = x0,
dx
dt (0) = x1
(D.2)
In Section 4, we define the domain as k × ω = [1, 3] × [0, 2] while f was fixed as f = 0.5. This
time, we extend it to 3D space by setting the domain as k × f × ω = [1, 3] × [0, 2] × [0, 2]. 3D plot
of the target is given in Figure 7 (a). Note that if we slice the cube at f ′ = 0.5, we obtain the target
in Section 4. This time, we create the base data matrix A′ ∈Rn×d′ by constructing a n = 513 fine
grid, and the polynomial degree is set to 12. Figure 7 (b) and (c) give examples of the leverage score
of this data matrix.
22

(a) Target Function.
(b) Leverage Score at f ′ = 0.0.
(c) Leverage Score at f ′ = 1.0.
Figure 7: (a): The target value of the 3D damped harmonic oscillator model. (b) and (c): Its leverage
score.We have f ∈[0, 2], and the grid is sliced at f = 0.5 in (b) and at f = 1.0 in (c).
(a) 3D Damped Harmonic Oscilla-
tor.
(b) Leverage Score v.s.
Uniform
Sampling.
(c) Randomized BSS.
Figure 8: (a): Results for degree 12 active polynomial regression for the damped harmonic oscil-
lator QoI in 3D space. (b), (c): Degree 12 active polynomial regression for the damped harmonic
oscillator QoI in 2D space. (b) includes the performance of the Bernoulli sampling and our pivotal
sampling both using uniform inclusion probability which is given with the thin lines. (c) demon-
strates the approximation power of the randomized BSS algorithm. We run the sampling 2000 times
with different parameters, round the number of samples to the tens place, and take the median error.
Figure 8 (a) shows the relative error of Bernoulli sampling and our pivotal sampling both using the
leverage score. As expected, our method shows a better fit than Bernoulli sampling again.
Samples Needed for a Certain Error. Table 1 and 2 summarize the number of samples required to
achieve 2×OPT error and 1.1×OPT error where OPT is the error we could obtain when all the data
are labeled. In all four test problems, pivotal sampling achieves the target error with fewer samples
than the existing method (independent leverlage scores sampling), which is denoted by Bernoulli
in the tables. Our method is especially efficient when we aim at the target error close to OPT. For
instance, to achieve the 1.1 × OPT error in the 2D damped harmonic oscillator model, our method
requires less than half samples that Bernoulli sampling requires, showing a significant reduction in
terms of the number of samples needed.
Leverage Score Sampling vs. Uniform Sampling. We also conduct a complementary experiment
to show empirically that leverage score sampling is much more powerful than uniform sampling.
We extend the simulation in Figure 1 to a uniform inclusion probability setting. This time, we draw
350 samples, repeat the simulation 100 times, and report the approximation with a median error
in Figure 9. The result shows poor performance of the uniform sampling. As they draw fewer
samples near the boundaries compared to leverage score sampling, they are not able to pin down the
Oscillator 2D
Heat Eq.
Surface Reaction
Oscillator 3D
n
10000
10000
10000
513
poly. deg.
20
20
20
10
Bernoulli (a)
574
554
545
671
Pivotal (b)
398
395
390
533
Efficiency (b / a)
0.693
0.713
0.716
0.794
Table 1: Number of samples needed to achieve 2 × OPT error.
23

Oscillator 2D
Heat Eq.
Surface Reaction
Oscillator 3D
n
10000
10000
10000
513
poly. deg.
12
12
12
10
Bernoulli (a)
924
814
903
3121
Pivotal (b)
450
442
492
1943
Efficiency (b / a)
0.487
0.523
0.545
0.623
Table 2: Number of samples needed to achieve 1.1 × OPT error.
polynomial function near the edges, resulting in suffering large errors in these areas. The relative
error plot is given in Figure 8 (b). We point to three observations. 1) Comparing the thick lines
and the thin lines, one can tell that the use of leverage score significantly improves performance. 2)
Comparing the orange lines and the blue lines, one can see that our spatially-aware pivotal sampling
outperforms the Bernoulli sampling. 3) By combining the leverage score and spatially aware pivotal
sampling (our method), one can attain the best approximation among the four sampling strategies.
(a) Bernoulli Uniform.
(b) Bernoulli Leverage.
(c) Pivotal Uniform.
(d) Pivotal Leverage.
Figure 9: Visualizations of a polynomial approximation to the maximum displacement of a damped
harmonic oscillator, as a function of driving frequency and spring constant. X is a uniform dis-
tribution over a box. All four draw 350 samples but by different sampling methods; (a) and (b)
use Bernoulli sampling but (c) and (d) use pivotal sampling. Also, (a) and (c) select samples with
uniform probability while (b) and (d) employs the leverage score. Clearly, the leverage score suc-
cessfully pins down the polynomial function near the boundaries, resulting in better approximation.
Randomized BSS Algorithm. Finally, we run the randomized BSS algorithm (Chen and Price,
2019) on the 2D damped harmonic oscillator target function, and report the relative error together
with Bernoulli leverage score sampling and our pivotal sampling in Figure 8 (c). The setting is the
same as the Section 4 that the initial data points are drawn uniformly at random from [1, 3] × [0, 2],
and the polynomial degree is set to 12. Even though this algorithm also has a theoretical sample
complexity of O(d/ϵ) for the regression guarantee in (1.3), our sampling method achieves a certain
relative error with fewer samples.
24

