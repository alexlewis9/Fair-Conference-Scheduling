Published as a conference paper at ICLR 2021
GRADIENT PROJECTION MEMORY FOR CONTINUAL
LEARNING
Gobinda Saha, Isha Garg & Kaushik Roy
School of Electrical and Computer Engineering, Purdue University
gsaha@purdue.edu, gargi@purdue.edu, kaushik@purdue.edu
ABSTRACT
The ability to learn continually without forgetting the past tasks is a desired at-
tribute for artiﬁcial learning systems. Existing approaches to enable such learning
in artiﬁcial neural networks usually rely on network growth, importance based
weight update or replay of old data from the memory. In contrast, we propose a
novel approach where a neural network learns new tasks by taking gradient steps
in the orthogonal direction to the gradient subspaces deemed important for the
past tasks. We ﬁnd the bases of these subspaces by analyzing network represen-
tations (activations) after learning each task with Singular Value Decomposition
(SVD) in a single shot manner and store them in the memory as Gradient Pro-
jection Memory (GPM). With qualitative and quantitative analyses, we show that
such orthogonal gradient descent induces minimum to no interference with the
past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse im-
age classiﬁcation datasets with short and long sequences of tasks and report better
or on-par performance compared to the state-of-the-art approaches1.
1
INTRODUCTION
Humans exhibit remarkable ability in continual adaptation and learning new tasks throughout their
lifetime while maintaining the knowledge gained from past experiences. In stark contrast, Artiﬁ-
cial Neural Networks (ANNs) under such Continual Learning (CL) paradigm (Ring, 1998; Thrun &
Mitchell, 1995; Lange et al., 2021) forget the information learned in the past tasks upon learning new
ones. This phenomenon is known as ‘Catastrophic Forgetting’ or ‘Catastrophic Interference’ (Mc-
closkey & Cohen, 1989; Ratcliff, 1990). The problem is rooted in the general optimization meth-
ods (Goodfellow et al., 2016) that are being used to encode input data distribution into the parametric
representation of the network during training. Upon exposure to a new task, gradient-based opti-
mization methods, without any constraint, change the learned encoding to minimize the objective
function with respect to the current data distribution. Such parametric updates lead to forgetting.
Given a ﬁxed capacity network, one way to address this problem is to put constraints on the gra-
dient updates so that task speciﬁc knowledge can be preserved. To this end, Kirkpatrick et al.
(2017), Zenke et al. (2017), Aljundi et al. (2018), Serr`a et al. (2018) add a penalty term to the objec-
tive function while optimizing for new task. Such term acts as a structural regularizer and dictates
the degree of stability-plasticity of individual weights. Though these methods provide resource efﬁ-
cient solution to the catastrophic forgetting problem, their performance suffer while learning longer
task sequence and when task identity is unavailable during inference.
Approaches (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019a) that store episodic memories
of old data essentially solve an optimization problem with ‘explicit’ constraints on the new gradient
directions so that losses for the old task do not increase. In Chaudhry et al. (2019b) the performance
of old task is retained by taking gradient steps in the average gradient direction obtained from the
new data and memory samples. To minimize interference, Farajtabar et al. (2020) store gradient
directions (instead of data) of the old tasks and optimize the network in the orthogonal directions
to these gradients for the new task, whereas Zeng et al. (2018) update gradients orthogonal to the
old input directions using projector matrices calculated iteratively during training. However, these
methods either compromise data privacy by storing raw data or utilize resources poorly, which limits
their scalability.
1Our code is available at https://github.com/sahagobinda/GPM
1

Published as a conference paper at ICLR 2021
In this paper, we address the problem of catastrophic forgetting in a ﬁxed capacity network when data
from the old tasks are not available. To mitigate forgetting, our approach puts explicit constraints
on the gradient directions that the optimizer can take. However, unlike contemporary methods, we
neither store old gradient directions nor store old examples for generating reference directions. In-
stead we propose an approach that, after learning each task, partitions the entire gradient space of the
weights into two orthogonal subspaces: Core Gradient Space (CGS) and Residual Gradient Space
(RGS) (Saha et al., 2020). Leveraging the relationship between the input and the gradient spaces, we
show how learned representations (activations) form the bases of these gradient subspaces in both
fully-connected and convolutional networks. Using Singular Value Decomposition (SVD) on these
activations, we show how to obtain the minimum set of bases of the CGS by which past knowledge
is preserved and learnability for the new tasks is ensured. We store these bases in the memory which
we deﬁne as Gradient Projection Memory (GPM). In our method, we propose to learn any new
task by taking gradient steps in the orthogonal direction to the space (CGS) spanned by the GPM.
Our analysis shows that such orthogonal gradient descent induces minimum to no interference with
the old learning, and thus effective in alleviating catastrophic forgetting. We evaluate our approach
in the context of image classiﬁcation with miniImageNet, CIFAR-100, PMNIST and sequence of
5-Datasets on a variety of network architectures including ResNet. We compare our method with
related state-of-the-art approaches and report comparable or better classiﬁcation performance. Over-
all, we show that our method is memory efﬁcient and scalable to complex dataset with longer task
sequence while preserving data privacy.
2
RELATED WORKS
Approaches to continual learning for ANNs can be broadly divided into three categories. In this
section we present a detailed discussion on the representative works from each category, highlighting
their contributions and differences with our approach.
Expansion-based methods: Methods in this category overcome catastrophic forgetting by dedicat-
ing different subsets of network parameters to each task. With no constraint on network architec-
ture, Progressive Neural Network (PGN) (Rusu et al., 2016) preserves old knowledge by freezing
the base model and adding new sub-networks with lateral connections for each new task. Dynam-
ically Expandable Networks (DEN) (Yoon et al., 2018) either retrains or expands the network by
splitting/duplicating important units on new tasks, whereas Sarwar et al. (2020) grow the network
to learn new tasks while sharing part of the base network. Li et al. (2019) with neural architecture
search (NAS) ﬁnd optimal network structures for each sequential task. RCL (Xu & Zhu, 2018) adap-
tively expands the network at each layer using reinforcement learning, whereas APD (Yoon et al.,
2020) additively decomposes the parameters into shared and task speciﬁc parameters to minimize
the increase in the network complexity. In contrast, our method avoids network growth or expensive
NAS operations and performs sequential learning within a ﬁxed network architecture.
Regularization-based methods: These methods attempt to overcome forgetting in ﬁxed capacity
model through structural regularization which penalizes major changes in the parameters that were
important for the previous tasks. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017)
computes such importance from diagonal of Fisher information matrix after training, whereas Zenke
et al. (2017) compute them during training based on loss sensitivity with respect to the parameters.
Additionally, Aljundi et al. (2018) compute importance from sensitivity of model outputs to the
inputs. Other methods, such as PackNet (Mallya & Lazebnik, 2018) uses iterative pruning to fully
restrict gradient updates on important weights via binary mask, whereas HAT (Serr`a et al., 2018)
identiﬁes important neurons by learning attention masks that control gradient propagation in the
individual parameters. Saha et al. (2020) using a PCA based pruning on activations (Garg et al.,
2020) partition the parametric space of the weights (ﬁlters) into core and residual (ﬁlter) spaces
after learning each task. The past knowledge is preserved in the frozen core space, whereas the
residual space is updated when learning the next task. In contrast to these methods, we do not
ascribe importance to or restrict the gradients of any individual parameters or ﬁlters. Rather we put
constraints on the ‘direction’ of gradient descent.
Memory-based methods: Methods under this class mitigate forgetting by either storing a subset
of (raw) examples from the past tasks in the memory for rehearsal (Robins, 1995; Rebufﬁet al.,
2017; Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2019a;b; Riemer et al., 2019) or synthesiz-
ing old data from generative models to perform pseudo-rehearsal (Shin et al., 2017). For instance,
2

Published as a conference paper at ICLR 2021
Gradient Episodic Memory (GEM) (Lopez-Paz & Ranzato, 2017) avoids interference with previous
task by projecting the new gradients in the feasible region outlined by previous task gradients cal-
culated from the samples of episodic memory. Averaged-GEM (A-GEM) (Chaudhry et al., 2019a)
simpliﬁed this optimization problem to projection in one direction estimated by randomly selected
samples from the memory. Guo et al. (2020) propose a uniﬁed view of episodic memory-based CL
methods, that include GEM and A-GEM and improves performance over these methods utilizing
loss-balancing update rule. Additionally, Experience Replay (ER) (Chaudhry et al., 2019b) and
Meta-Experience Replay (MER) (Riemer et al., 2019) mitigate forgetting in online CL setup by
jointly training on the samples from new tasks and episodic memory. All these methods, however,
rely on the access to old data which might not be possible when users have concern over data pri-
vacy. Like all the memory-based methods we also use a storage unit which we call GPM. However,
we do not save any raw data in GPM, thus satisfy data privacy criterion.
Our method is closely related to recently proposed Orthogonal Gradient Descent (OGD) (Farajtabar
et al., 2020) and Orthogonal Weight Modulation (OWM) (Zeng et al., 2018). OGD stores a set of
gradient directions in the memory for each task and minimizes catastrophic forgetting by taking
gradient steps in the orthogonal directions for new tasks. In contrast to OGD, we compute and
store the bases of core gradient space from network representations (activations) which reduces
the memory requirement by orders of magnitude. Moreover, OGD is shown to work under locality
assumption for small learning rates which limits its scalability in learning longer task sequences with
complex dataset. Since our method does not use gradient directions (like OGD) to describe the core
gradient spaces, we do not need to obey such assumptions, thus can use higher learning rates. On the
other hand, OWM reduces forgetting by modifying the weights of the network in the orthogonal to
the input directions of the past tasks. This is achieved by multiplying new gradients with projector
matrices. These matrices are computed from the stored past projectors and the inputs with recursive
least square (RLS) method at each training step. However, such an iterative method not only slows
down the training process but also shows limited scalability in end-to-end task learning with modern
network architectures. Like OWM, we aim to encode new learning in the orthogonal to the old input
directions. In contrast to iterative projector computation in OWM, we identify a low-dimensional
subspace in the gradient space analyzing the learned representations with SVD in one-shot manner
at the end of each task. We store the bases of these subspaces in GPM and learn new tasks in the
orthogonal to these spaces to protect old knowledge. We quantitatively show that our method is
memory efﬁcient, fast and scalable to deeper networks for complex long sequence of tasks.
3
NOTATIONS AND BACKGROUND
In this section, we introduce the notations used throughout the paper and give a brief overview of
SVD for matrix approximation. In section 4, we establish the relationship between input and gradient
spaces. In section 5 we show the steps of our algorithm that leverage such relationship.
Continual Learning: We consider supervised learning setup where T tasks are learned sequen-
tially.
Each task has a task descriptor, τ ∈{1, 2...., T} with a corresponding dataset, Dτ =
{(xi,τ, yi,τ)nτ
i=1} having nτ example pairs. Let’s consider an L layer neural network where at each
layer network computes the following function for task τ :
xl+1
i,τ = σ(f(W l
τ, xl
i,τ)).
(1)
Here, l = 1, ...L, σ(.) is a non-linear function and f(., .) is a linear function. We will use vector
notation for input (xi,τ) in fully connected layers and matrix notation for input (Xi,τ) in convolu-
tional layers. At the ﬁrst layer, x1
i,τ = xi,τ represents the raw input data from task τ, whereas in the
subsequent layers we deﬁne xl
i,τ as the representation of input xi,τ at layer l. Set of parameters of
the network is deﬁned by, Wτ = {(W l
τ)L
l=1}, where W0 denotes set of parameters at initialization.
Matrix approximation with SVD: SVD can be used to factorize a rectangular matrix, A =
UΣV T ∈Rm×n into the product of three matrices, where U ∈Rm×m and V ∈Rn×n are
orthogonal, and Σ contains the sorted singular values along its main diagonal (Deisenroth et al.,
2020). If the rank of the matrix is r (r ≤min(m, n)), A can be expressed as A = Pr
i=1 σiuivT
i ,
where ui ∈U and vi ∈V are left and right singular vectors and σi ∈diag(Σ) are singular values.
Also, k-rank approximation to this matrix can be expressed as, Ak = Pk
i=1 σiuivT
i , where k ≤r
and its value can be chosen by the smallest k that satisﬁes ||Ak||2
F ≥ϵth||A||2
F . Here, ||.||F is the
Frobenius norm of the matrix and ϵth (0 < ϵth ≤1) is the threshold hyperparameter.
3

Published as a conference paper at ICLR 2021
Ci × k × k
ho × wo 
ho × wo 
Co
Ci × k × k
Co
×
=
p1 p2
pn
Ci × k × k
Ci × k × k
ho × wo 
ho × wo 
Co
Co
×
p1
T
p2
T
pn
T
=
X
W
O
XT
Δ
Δ
WL
(a) Forward Pass
(b) Backward Pass
Input
Weight
Output
Input
Error
Weight Gradient
Figure 1: Illustration of convolution operation in matrix multiplication format during (a) Forward
Pass and (b) Backward Pass.
4
INPUT AND GRADIENT SPACES
Our algorithm leverages the fact that stochastic gradient descent (SGD) updates lie in the span of
input data points (Zhang et al., 2017). In the following subsections we will establish this relationship
for both fully connected and convolutional layers. The analysis presented in this section is generally
applicable to any layer of the network for any task, and hence we drop the task and layer identiﬁers.
4.1
FULLY CONNECTED LAYER
Let’s consider a single layer linear neural network in supervised learning setup where each (input,
label) training data pair comes from a training dataset, D. Let, x ∈Rn is the input vector, y ∈Rm
is the label vector in the dataset and W ∈Rm×n are the parameters (weights) of the network. The
network is trained by minimizing the following mean-squared error loss function
L = 1
2||W x −y||2
2.
(2)
We can express gradient of this loss with respect to weights as
∇W L = (W x −y)xT = δxT ,
(3)
where δ ∈Rm is the error vector. Thus, the gradient update will lie in the span of input (x),
where elements in δ scale the magnitude of x by different factors. Here, we have considered per-
example loss (batch size of 1) for simplicity. However, this relation also holds for mini-batch setting
(see appendix B.1). The input-gradient relation in equation 3 is generically applicable to any fully
connected layer of a neural network where x is the input to that layer and δ is the error coming from
the next layer. Moreover, this equation also holds for network with non-linear units (e.g. ReLU) and
cross-entropy losses except the calculation of δ will be different.
4.2
CONVOLUTIONAL LAYER
Filters in a convolutional (Conv) layer operate in a different way on the inputs than the weights in
a fully connected (FC) layer. Let’s consider a Conv layer with the input tensor X ∈RCi×hi×wi
and ﬁlters W ∈RCo×Ci×k×k. Their convolution ⟨X, W, ∗⟩produces output feature map, O ∈
RCo×ho×wo (Liu et al., 2018). Here, Ci (Co) denotes the number of input (output) channels of the
Conv layer, hi, wi (ho, wo) denote the height and width of the input (output) feature maps and k is
the kernel size of the ﬁlters. As shown in Figure 1(a), if X is reshaped into a (ho×wo)×(Ci×k×k)
matrix, X and W is reshaped into a (Ci × k × k) × Co matrix, W , then the convolution can be
expressed as matrix multiplication between X and W as O=XW , where O ∈R(h0×w0)×Co. Each
row of X contains an input patch vector, pj ∈R(Ci×k×k)×1, where j = 1, 2..., n (n = ho ∗wo).
Formulation of convolution in terms of matrix multiplication provides an intuitive picture of the
gradient computation during backpropagation. Similar to the FC layer case, in Conv layer, during
backward pass an error matrix ∆of size (h0 × w0) × Co (same size as O) is obtained from the next
layer. As shown in Figure 1(b), the gradient of loss with respect to ﬁlter weights is calculated by
∇W L = XT ∆,
(4)
where, ∇W L is of shape (Ci × k × k) × Co (same size as W ). Since, columns of XT are the input
patch vectors (p), the gradient updates of the convolutional ﬁlters will lie in the space spanned
by these patch vectors.
4

Published as a conference paper at ICLR 2021
5
CONTINUAL LEARNING WITH GRADIENT PROJECTION MEMORY (GPM)
In this section, we describe our continual learning algorithm which leverages the relationship be-
tween gradient and input spaces to identify the core gradient spaces of the past tasks. We show how
gradient descent orthogonal to these spaces enable us to learn continually without forgetting.
Learning Task 1: We learn the ﬁrst task (τ = 1) using dataset, D1 without imposing any constraint
on parameter updates. At the end of Task 1, we obtain a learned set of parameters W1. To preserve
the knowledge of the learned task, we impose constraints on the direction of gradient updates for the
next tasks. To do so, we partition the entire gradient space into two (orthogonal) subspaces: Core
Gradient Space (CGS) and Residual Gradient Space (RGS), such that gradient steps along CGS
induce high interference on the learned tasks whereas gradient steps along RGS have minimum to
no interference. We aim to ﬁnd and store the bases of the CGS and take gradient steps orthogonal to
the CGS for the next task. In our formulation, each layer has its own CGS.
To ﬁnd the bases, after learning Task 1 , for each layer we construct a representation matrix, Rl
1 =
[xl
1,1, xl
2,1, ..., xl
ns,1] (for Conv layers Rl
1 = [(Xl
1,1)T , (Xl
2,1)T , ..., (Xl
ns,1)T ] ) concatenating ns
representations along the column obtained from forward pass of ns random samples from the current
training dataset through the network. Next, we perform SVD on Rl
1 = U l
1Σl
1(V l
1 )T followed by its
k-rank approximation (Rl
1)k according to the following criteria for the given threshold, ϵl
th :
||(Rl
1)k||2
F ≥ϵl
th||Rl
1||2
F .
(5)
We deﬁne the space, Sl = span{ul
1,1, ul
2,1, ..., ul
k,1}, spanned by the ﬁrst k vectors in U l
1 as the
space of signiﬁcant representation for task 1 at layer l since it contains all the directions with
highest singular values in the representation. For the next task, we aim to take gradient steps in a
way that the correlation between this task speciﬁc signiﬁcant representation and the weights in each
layer is preserved. Since, inputs span the space of gradient descent (section 4), the bases of Sl will
span a subspace in the gradient space which we deﬁne as the Core Gradient space (CGS). Thus
gradient descent along CGS will cause maximum change in the input-weight correlation whereas
gradient steps in the orthogonal directions to CGS (space of low representational signiﬁcance) will
induce very small to no interference to the old tasks. We deﬁne this subspace orthogonal to CGS as
Residual Gradient space (RGS). We save the bases of the CGS in the memory, M = {(M l)L
l=1},
where M l = [ul
1,1, ul
2,1, ..., ul
k,1]. We deﬁne this memory as Gradient Projection Memory (GPM).
Learning Task 2 to T: We learn task 2 with the examples from dataset D2 only. Before taking
gradient step, bases of the CGS are retrieved from GPM. New gradients (∇W l
2L2) are ﬁrst projected
onto the CGS and then projected components are subtracted out from the new gradient so that
remaining gradient components lie in the space orthogonal to CGS. Gradients are updated as
FC Layer:
∇W l
2L2 = ∇W l
2L2 −(∇W l
2L2)M l(M l)T ,
(6)
Conv Layer: ∇W l
2L2 = ∇W l
2L2 −M l(M l)T (∇W l
2L2).
(7)
At the end of the task 2 training, we update the GPM with new task-speciﬁc bases (of CGS). To
obtain such bases, we construct Rl
2 = [xl
1,2, xl
2,2, ..., xl
ns,2] using data from task 2 only. However,
before performing SVD and subsequent k-rank approximation, from Rl
2 we eliminate the common
directions (bases) that are already present in the GPM so that newly added bases are unique and
orthogonal to the existing bases in the memory. To do so, we perform the following step :
ˆRl
2 = Rl
2 −M l(M l)T (Rl
2) = Rl
2 −Rl
2,P roj.
(8)
Afterwards, SVD is performed on ˆRl
2 (= ˆU l
2 ˆΣl
2( ˆV l
2 )T ) and k new orthogonal bases are chosen for
minimum value of k satisfying the following criteria for the given threshold, ϵl
th:
||Rl
2,proj||2
F + ||( ˆRl
2)k||2
F ≥ϵl
th||Rl
2||2
F .
(9)
GPM is updated by adding new bases as M l = [M l, ˆul
1,2, ..., ˆul
k,2]. Thus after learning each new
task, CGS grows and RGS becomes smaller, where maximum size of M l (hence the dimension of
the gradient bases) is ﬁxed by the choice of initial network architecture. Once the GPM update is
complete we move on to the next task and repeat the same procedure that we followed for task 2.
The pseudo-code of the algorithm is given in Algorithm 1 in the appendix.
5

Published as a conference paper at ICLR 2021
6
EXPERIMENTAL SETUP
Datasets: We evaluate our continual learning algorithm on Permuted MNIST (PMNIST) (Le-
cun et al., 1998), 10-Split CIFAR-100 (Krizhevsky, 2009), 20-Spilt miniImageNet (Vinyals et al.,
2016) and sequence of 5-Datasets (Ebrahimi et al., 2020b). The PMNIST dataset is a variant of
MNIST dataset where each task is considered as a random permutation of the original MNIST
pixels. For PMNIST, we create 10 sequential tasks using different permutations where each task
has 10 classes (Ebrahimi et al., 2020a). The 10-Split CIFAR-100 is constructed by splitting 100
classes of CIFAR-100 into 10 tasks with 10 classes per task. Whereas, 20-Spilt miniImageNet, used
in (Chaudhry et al., 2019a), is constructed by splitting 100 classes of miniImageNet into 20 sequen-
tial tasks where each task has 5 classes. Finally, we use a sequence of 5-Datasets including CIFAR-
10, MNIST, SVHN (Netzer et al., 2011), notMNIST (Bulatov, 2011) and Fashion MNIST (Xiao
et al., 2017), where classiﬁcation on each dataset is considered as a task. In our experiments we do
not use any data augmentation. The dataset statistics are given in Table 4 & 5 in the appendix.
Network Architecture: We use fully-connected network with two hidden layer of 100 units each
for PMNIST following Lopez-Paz & Ranzato (2017). For experiments with split CIFAR-100 we
use a 5-layer AlexNet similar to Serr`a et al. (2018). For split miniImageNet and 5-Datasets, similar
to Chaudhry et al. (2019b), we use a reduced ResNet18 architecture. No bias units are used and batch
normalization parameters are learned for the ﬁrst task and shared with all the other tasks (follow-
ing Mallya & Lazebnik (2018)). Details on architectures are given in the appendix section C.2. For
permuted MNIST, we evaluate and compare our algorithm in ‘single-head’ setting (Hsu et al., 2018;
Farquhar & Gal, 2018) where all tasks share the ﬁnal classiﬁer layer and inference is performed
without task hint. For all other experiments, we evaluate our algorithm in ‘muti-head’ setting, where
each task has a separate classiﬁer on which no gradient constraint is imposed during learning.
Baselines: We compare our method with state-of-the art approaches from both memory based
and regularization based methods that consider sequential task learning in ﬁxed network archi-
tecture. From memory based approach, we compare with Experience Replay with reservoir sam-
pling (ER Res) (Chaudhry et al., 2019b), Gradient Episodic Memory (GEM) (Lopez-Paz & Ran-
zato, 2017), Averaged GEM (A-GEM) (Chaudhry et al., 2019a), Orthogonal Gradient Descent
(OGD) (Farajtabar et al., 2020) and Orthogonal Weight Modulation (OWM) (Zeng et al., 2018).
Moreover, we compare with sate-of-the-art HAT (Serr`a et al., 2018) baseline and Elastic Weight
Consolidation (EWC) (Kirkpatrick et al., 2017) from regularization based methods. Additionally,
we add ‘multitask’ baseline where all the tasks are learned jointly using the entire dataset at once
in a single network. Multitask is not a continual learning strategy but will serve as upper bound
on average accuracy on all tasks. Details on the implementation along with the hyperparameters
considered for each of these baselines are provided in section C.4 and Table 6 in the appendix.
Training Details: We train all the models with plain stochastic gradient descent (SGD). For each
task in PMNIST and split miniImageNet we train the network for 5 and 10 epochs respectively with
batch size of 10. In Split CIFAR-100 and 5-Datasets experiments, we train each task for maximum
of 200 and 100 epochs respectively with the early termination strategy based on the validation loss
as proposed in Serr`a et al. (2018). For both datasets, batch size is set to 64. For GEM, A-GEM
and ER Res the episodic memory size is chosen to be approximately the same size as the maximum
GPM size (GPM Max). Calculation of GPM size is given in Table 7 in the appendix. Moreover,
selection of the threshold values (ϵth) in our method is discussed in section C.5 in the appendix.
Performance Metrics: To evaluate the classiﬁcation performance, we use the ACC metric, which
is the average test classiﬁcation accuracy of all tasks. To measure the forgetting we report backward
transfer, BWT which indicates the inﬂuence of new learning on the past knowledge. For instance,
negative BWT indicates (catastrophic) forgetting. Formally, ACC and BWT are deﬁned as:
ACC = 1
T
T
X
i=1
RT,i,
BWT =
1
T −1
T −1
X
i=1
RT,i −Ri,i.
(10)
Here, T is the total number of sequential tasks and RT,i is the accuracy of the model on ith task
after learning the T th task sequentially (Lopez-Paz & Ranzato, 2017).
7
RESULTS AND DISCUSSIONS
Single-head inference with PMNIST: First, we evaluate our algorithm in single-head setup for 10
sequential PMNIST tasks. In this setup task hint is not necessary. As HAT cannot perform infer-
6

Published as a conference paper at ICLR 2021
Memory
100
101
102
(a)
Time
0.00
0.25
0.50
0.75
1.00
(b)
Memory
0.00
0.25
0.50
0.75
1.00
(c)
Memory
0.0
0.5
1.0
(d)
Memory
0.0
0.5
1.0
(e)
OGD
OWM
GEM
A-GEM
ER_Res
GPM_Max
GPM
Figure 2: (a) Memory utilization and (b) per epoch training time for PMNIST tasks for different
methods. Memory utilization for different approaches for (c) CIFAR-100, (d) miniImageNet and (e)
5-Datasets tasks. For memory, size of GPM Max and for time, method with highest complexity is
used as references (value of 1). All the other methods are reported relative to these references.
Table 1: Continual learning on different datasets. Methods that do not adhere to CL setup is indicated
by (*). All the results are (re) produced by us and averaged over 5 runs. Standard deviations are
reported in Table 8 and 9 in the appendix.
(a)
PMNIST
Methods
ACC (%)
BWT
OGD
82.56
- 0.14
OWM
90.71
- 0.01
GEM
83.38
- 0.15
A-GEM
83.56
- 0.14
ER Res
87.24
- 0.11
EWC
89.97
- 0.04
GPM (ours)
93.91
-0.03
Multitask*
96.70
-
(b)
CIFAR-100
miniImageNet
5-Datasets
Methods
ACC (%)
BWT
ACC (%)
BWT
ACC (%)
BWT
OWM
50.94
- 0.30
-
-
-
-
EWC
68.80
- 0.02
52.01
- 0.12
88.64
- 0.04
HAT
72.06
- 0.00
59.78
- 0.03
91.32
- 0.01
A-GEM
63.98
- 0.15
57.24
- 0.12
84.04
-0.12
ER Res
71.73
- 0.06
58.94
- 0.07
88.31
- 0.04
GPM (ours)
72.48
- 0.00
60.41
- 0.00
91.22
- 0.01
Multitask*
79.58
-
69.46
-
91.54
-
ence without task hint, it is not included in the comparison. Since network size is very small (0.1M
parameters) with 87% parameters in the ﬁrst layer, we choose threshold value (ϵth) of 0.95 for that
layer and 0.99 for the other layers to ensure better learnability. From the results, shown in Table 1(a),
we observe that our method (GPM) achieves best average accuracy (93.91 ± 0.16%). In addition,
we achieve least amount of forgetting, except OWM, which essentially trades off accuracy to mini-
mize forgetting. Figure 2(a) compares the memory utilization of all the memory-based approaches.
While OWM, GEM, A-GEM and ER Res use memory of size of GPM Max, we obtain better per-
formance by using only 69% of the GPM Max. Moreover, compared to OGD, we use about 400
times lower memory and achieve ∼10% better accuracy. In Figure 2(b), we compare the per epoch
training time of different memory based methods and found our method to be the fastest primarily
due to the precomputation of the reference gradient bases (of CGS). Additionally, in single-epoch
setting (Lopez-Paz & Ranzato, 2017), as shown in Table 8 in the appendix, we obtain best average
accuracy (91.74 ± 0.15%), which demonstrates the potential for our algorithm in online CL setup.
Split CIFAR-100: Next, we switch to multi-head setup which enables us to compare with strong
baselines such as HAT. For ten split CIFAR-100 tasks, as shown in Table 1(b), we outperform all the
memory based approaches while using 45% less memory (Figure 2(c)). We also outperform EWC
and our accuracy is marginally better than HAT while achieving zero forgetting. Also, we obtain
∼20% better accuracy than OWM, which have high forgetting (BWT=−0.30) thus demonstrating
its limited scalability to convolutional architectures.
Split miniImageNet: With this experiment, we test the scalability of our algorithm to deeper net-
work (ResNet18) for long task sequence from miniImageNet dataset. The average accuracies for
different methods after learning 20 sequential tasks are given in Table 1(b). Again, in this case
we outperform A-GEM, ER Res and EWC using 76% of the GPM Max (Figure 2(d)). Also, we
achieve marginally better accuracy than HAT, however unlike HAT (and other methods) we com-
pletely avoid forgetting (BWT=0.00). Moreover, compared other methods sequential learning in our
method is more stable, which means accuracy of the past tasks have minimum to no degradation
over the course of learning (shown for task 1 accuracy in Figure 4 in the appendix).
5-Datasets: Next, we validate our approach on learning across diverse datasets, where classiﬁcation
on each dataset is treated as one task. Even in this challenging setting, as shown in in Table 1(b),
7

Published as a conference paper at ICLR 2021
Table 2: Total wall-clock training time measured on a single GPU after learning all the tasks.
(a)
Training Time [s]
Methods
PMNIST
OGD
1658
OWM
396
GEM
1639
A-GEM
445
ER Res
259
EWC
645
GPM (ours)
245
(b)
Training Time [s]
Methods
CIFAR-100
miniImageNet
5-Datasets
OWM
1856
-
-
EWC
1352
4138
7613
HAT
1248
3077
7246
A-GEM
2678
6069
12077
ER Res
1147
2775
7015
GPM (ours)
770
3387
5008
Table 3: Continual learning of 20-task from CIFAR-100 Superclass dataset. (†) denotes the result
reported from APD. (*) indicates the methods that do not adhere to CL setup. Single-task learning
(STL), where a separate network in trained for each task, serves as an upper bound on accuracy.
Methods
Metric
STL†*
PGN†
DEN†
RCL†
APD†
GPM (ours)
ACC (%)
61.00
50.76
51.10
51.99
56.81
57.72
Capacity (%)
2000
271
191
184
130
100
we achieve better accuracy (91.22 ± 0.20%) then A-GEM, ER Res and EWC utilizing 78% of the
GPM Max (Figure 2(e)). Though, HAT performs marginally better than our method, both HAT and
we achieve the lowest BWT (-0.01). In this experiment, we have used tasks that are less related to
each other. After learning 5 such tasks 78% of the gradient space is already constrained. Which
implies, if the tasks are less or non-similar, GPM will get populated faster and reach to its maximum
capacity after which no new learning will be possible. Since, we use a ﬁxed capacity network and
the size of GPM is determined by the network architecture, the ability of learning sequences of
hundreds of such tasks with our method will be limited by the chosen network capacity.
Training Time. Table 2 shows the total training time for all the sequential tasks for different al-
gorithms. This includes time spent for memory management for the memory-based methods, ﬁsher
importance calculation for EWC and learning activation masks for HAT. Details of time measure-
ment are given in appendix section C.6. For PMNIST, CIFAR-100, and 5-dataset tasks our algorithm
trains faster than all the other baselines while spending only 0.2%, 3% and 6% of its total training
time in GPM update (using SVD) respectively. Since each miniImageNet tasks are trained for only
10 epochs, our method have relatively higher overhead (30% of the total time) due to GPM update,
thus runs a bit slower than the fastest ER Res. Overall, our formulation uses GPM bases and projec-
tion matrices of reasonable dimensions (see appendix section C.8); precomputation of which at the
start of each task leads to fast per-epoch training. This gain in time essentially compensates for the
extra time required for the GPM update, which is done only once per task, enabling fast training.
Comparison with Expansion-based methods. To compare our method with the state-of-the-art ex-
pansion based methods we perform experiment with 20-task CIFAR-100 Superclass dataset (Yoon
et al., 2020). In this experiment, each task contains 5 different but semantically related classes from
CIFAR-100 dataset. Similar to APD, here we use the LeNet-5 architecture. Details of architec-
ture and training setup are given in appendix section C.3. Results are shown in Table 3 where ACC
represents average accuracy over 5 different task sequences (used in APD) and Capacity denotes
percentage of network capacity used with respect to the original network. We outperform all the
CL methods achieving best average accuracy (57.72 ± 0.37%) with BWT of -0.01 using the small-
est network. For instance, we outperform RCL and APD utilizing 84% and 30% fewer network
parameters respectively, which shows that our method induces more sharing between tasks.
Overall, we outperform the memory-based methods with less memory utilization, achieve better
accuracy than expansion-based methods using smaller network, and obtain better or on-par per-
formance compared to HAT in the given experimental setups. However, in the class-incremental
learning setup (Rebufﬁet al., 2017), method (Kamra et al., 2017) that uses data replay achieves
better performance than GPM (see experiment in appendix section D.3). In this setup, we believe a
subset of old data replay either from storage or via generation is inevitable for attaining better per-
8

Published as a conference paper at ICLR 2021
1.0
0.5
0.0
0.5
1.0
activation
0
2
4
6
count
(a)
=0.5
=0.7
=0.8
=0.9
=0.95
1.0
0.5
0.0
0.5
1.0
activation
0
1
2
3
4
count
(b)
=0.5
=0.7
=0.8
=0.9
=0.95
ACC
BWT
15
0
15
30
45
60
75
Accuracy (%)
(c)
=0.5
=0.7
=0.8
=0.9
=0.95
Figure 3: Histograms of interference activations as a function of threshold, (ϵth) at (a) Conv layer 2
(b) FC layer 2 for split CIFAR-100 tasks. (c) Impact of ϵth on ACC (%) and BWT(%). With increas-
ing value of ϵth, spread of interference reduces, which improves accuracy and reduces forgetting.
formance with minimal forgetting (Rajasegaran et al., 2019). In that quest, a hybrid approach such
as combining GPM with small data replay would be an interesting direction for future exploration.
Controlling Forgetting: Finally, we discuss the factors that implicitly or explicitly control the
amount of forgetting in our algorithm. As discussed in section 5, we propose to minimize interfer-
ence by taking gradient steps orthogonal to the CGS, where CGS bases are computed such that space
of signiﬁcant representations of the past tasks can be well approximated by these bases. The degree
of this approximation is controlled by the threshold hyperparameter, ϵth (through equation 5, 9). For
instance, a low value of ϵth (closer to 0) would allow the optimizer to change the weights along the
directions where past data has higher representational signiﬁcance, thereby signiﬁcantly altering the
past input-weight correlation inducing (catastrophic) interference. On the other hand, a high value
of ϵth (closer to 1) would preserve such correlation, however learnability of the new task might
suffer due to high volume of constraints in the gradient space. Therefore, in our continual learning
algorithm, ϵth mediates the stability-plasticity dilemma. To show this analytically, let’s consider a
network after learning T sequential tasks with weights of the network at any layer, l expressed as :
W l
T = W l
1 +
T −1
X
i=1
∆W l
i→i+1 = W l
1 + ∆W l
1→T .
(11)
Here, W l
1 is the weights after task 1 and ∆W l
1→T is the change of weights from task 1 to T.
Weight update with our method ensures that ∆W l
1→T lie in the orthogonal space of the data (rep-
resentations) of task 1. Linear operation at layer l with data from task 1 (x1) would produce:
W l
T xl
1 = W l
1xl
1 + ∆W l
1→T xl
1 . If ∆W l
1→T xl
1 = 0, then the output of the network for task 1 data
after learning task T will be the same as the output after learning task 1 (i.e. W l
T xl
1 = W l
1xl
1), that
means no interference for task 1. We deﬁne ∆W l
1→T xl
1 as the interference activation for task 1
at layer l (for any task, τ < T: ∆W l
τ→T xl
τ). As discussed above, degree of such interference is
dictated by ϵth. Figure 3(a)-(b) (and Figure 5 in appendix) show histograms (distributions) of inter-
ference activations at each layer of the network for split CIFAR-100 experiment. For lower value
of ϵth, these distributions have higher variance (spread) implying high interference, whereas with
increasing value of ϵth, the variance reduces around the (zero) mean value. As a direct consequence,
as shown in Figure 3(c), backward transfer reduces for increasing ϵth with improvement in accuracy.
8
CONCLUSION
In this paper we propose a novel continual learning algorithm that ﬁnds important gradient sub-
spaces for the past tasks and minimizes catastrophic forgetting by taking gradient steps orthogonal
to these subspaces when learning a new task. We show how to analyse the network representations
to obtain minimum number of bases of these subspaces by which past information is preserved and
learnability for the new tasks is ensured. Evaluation on diverse image classiﬁcation tasks with differ-
ent network architectures and comparisons with state-of-the-art algorithms show the effectiveness
of our approach in achieving high classiﬁcation performance while mitigating forgetting. We also
show our algorithm is fast, makes efﬁcient use of memory and is capable of learning long sequence
of tasks in deeper networks preserving data privacy.
ACKNOWLEDGMENTS
This work was supported in part by the National Science Foundation, Vannevar Bush Faculty Fel-
lowship, Army Research Ofﬁce, MURI, and by Center for Brain Inspired Computing (C-BRIC), one
of six centers in JUMP, a Semiconductor Research Corporation program sponsored by DARPA.
9

Published as a conference paper at ICLR 2021
REFERENCES
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In The European Conference on Com-
puter Vision (ECCV), pp. 139–154, 2018.
Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for con-
tinual learning with orthogonal gradient descent. ArXiv, abs/2006.11942, 2020.
Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online], 2011. URL http:
//yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient
lifelong learning with A-GEM. In International Conference on Learning Representations, 2019a.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K.
Dokania, Philip H. S. Torr, and Marc’Aurelio Ranzato. Continual learning with tiny episodic
memories. ArXiv, abs/1902.10486, 2019b.
Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for Machine Learning.
Cambridge University Press, 2020.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In International Conference on Learning Rep-
resentations, 2020a.
Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell, and Marcus Rohrbach. Adver-
sarial continual learning. In The European Conference on Computer Vision (ECCV), 2020b.
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for contin-
ual learning. ArXiv, abs/1910.07104, 2020.
Sebastian Farquhar and Yarin Gal.
Towards robust evaluations of continual learning.
ArXiv,
abs/1805.09733, 2018.
Isha Garg, Priyadarshini Panda, and Kaushik Roy. A low effort approach to structured cnn design
using pca. IEEE Access, 8:1347–1360, 2020.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Yunhui Guo, Mingrui Liu, Tianbao Yang, and Tajana Rosing.
Improved schemes for episodic
memory-based lifelong learning.
In Advances in Neural Information Processing Systems 33,
2020.
Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and Zsolt Kira. Re-evaluating continual learn-
ing scenarios: A categorization and case for strong baselines. In NeurIPS Continual learning
Workshop, 2018. URL https://arxiv.org/abs/1810.12488.
Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative dual memory network for continual
learning. ArXiv, abs/1710.10368, 2017.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for-
getting in neural networks. Proceedings of the National Academy of Sciences, 114:3521 – 3526,
2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021.
10

Published as a conference paper at ICLR 2021
Yann Lecun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278–2324, 1998.
Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: A continual
structure learning framework for overcoming catastrophic forgetting. In Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 3925–3934. PMLR, 09–15 Jun 2019.
Zhenhua Liu, Jizheng Xu, Xiulian Peng, and Ruiqin Xiong. Frequency-domain dynamic pruning
for convolutional neural networks. In Advances in Neural Information Processing Systems 31, pp.
1043–1053. 2018.
David Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, volume 30, 2017.
Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7765–
7773, 2018.
Michael Mccloskey and Neil J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. The Psychology of Learning and Motivation, 24:104–169, 1989.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Jathushan Rajasegaran, Munawar Hayat, Salman H Khan, Fahad Shahbaz Khan, and Ling Shao.
Random path selection for continual learning. In Advances in Neural Information Processing
Systems, volume 32, pp. 12669–12679, 2019.
Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychological review, 97 2:285–308, 1990.
Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. iCaRL:
Incremental classiﬁer and representation learning. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5533–5542, 2017.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. In International Conference on Learning Representations, 2019.
Mark B. Ring. Child: A ﬁrst step towards continual learning. In Learning to Learn, 1998.
Anthony V. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connect. Sci., 7:123–
146, 1995.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. ArXiv,
abs/1606.04671, 2016.
Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy. Space: Structured compression and
sharing of representational space for continual learning. ArXiv, abs/2001.08650, 2020.
Syed Shakib Sarwar, Aayush Ankit, and Kaushik Roy. Incremental learning in deep convolutional
neural networks using partial network sharing. IEEE Access, 8:4615–4628, 2020.
Joan Serr`a, D´ıdac Sur´ıs, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4548–4557.
PMLR, 10–15 Jul 2018.
Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative
replay. In Advances in Neural Information Processing Systems, volume 30, 2017.
Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics Auton. Syst., 15:25–46,
1995.
11

Published as a conference paper at ICLR 2021
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In Advances in Neural Information Processing Systems, vol-
ume 29, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. ArXiv, abs/1708.07747, 2017.
Ju Xu and Zhanxing Zhu. Reinforced continual learning. In Advances in Neural Information Pro-
cessing Systems, volume 31, pp. 899–908, 2018.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. In 6th International Conference on Learning Representations, 2018.
Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust con-
tinual learning with additive parameter decomposition. In International Conference on Learning
Representations, 2020.
Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continuous learning of context-dependent
processing in neural networks. ArXiv, abs/1810.01256, 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelli-
gence. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pp. 3987–3995. PMLR, 06–11 Aug 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
12

Published as a conference paper at ICLR 2021
A
APPENDIX
B
ALGORITHM
B.1
INPUT AND GRADIENT SPACES (CONT.)
Since, the batch loss is the summation of the losses due to individual examples, the total batch loss
for n samples can be expressed as
Lbatch =
n
X
i=1
Li =
n
X
i=1
1
2||W xi −yi||2
2.
(12)
The gradient of this loss with respect to weights can be expressed as
∇W Lbatch = δ1xT
1 + δ2xT
2 + ... + δnxT
n.
(13)
The gradient update will remain in the subspace spanned by the n input examples.
B.2
ALGORITHM PSEUDO CODE
Algorithm 1 Algorithm for Continual Learning with GPM
1: function TRAIN (fW , Dtrain, α, ϵth )
2: Initialize, M l ←[ ], for all l = 1, 2, ....L // till L-1 if multi-head setting
3: M ←{(M l)L
l=1}
4: W ←W0
5: for τ ∈1, 2, ....., T do
6:
repeat
7:
Bn ∼Dtrain
τ
// sample a mini-batch of size n from task τ
8:
gradient, ∇W Lτ ←SGD(Bn, fW )
9:
∇W Lτ ←PROJECT(∇W Lτ, M) // see equation (6, 7)
10:
W ←W −α∇W Lτ
11:
until convergence
12:
13:
// Update Memory (GPM)
14:
Bns ∼Dtrain
τ
// sample a mini-batch of size ns from task τ
15:
// construct representation matrices for each layer by forward pass (section 5)
16:
Rτ ←forward(Bns, fW ), where Rτ = {(Rl
τ)L
l=1}
17:
for layer, l = 1, 2, ...L do
18:
ˆRl
τ ←PROJECT(Rl
τ, M l) // see equation (8)
19:
ˆU l
τ ←SVD( ˆRl
τ)
20:
k ←criteria( ˆRl
τ, Rl
τ, ϵl
th) // see equation (9)
21:
M l ←[M l, ˆU l
τ[0 : k]]
22:
end for
23: end for
24: return fW , M
25: end function
C
EXPERIMENTAL DETAILS
C.1
DATASET STATISTICS
Table 4 and Table 5 show the summary of the datasets used in the experiments.
13

Published as a conference paper at ICLR 2021
Table 4: Dataset Statistics.
PMNIST
Split CIFAR-100
Split-miniImageNet
num. of tasks
10
10
20
input size
1 × 28 × 28
3 × 32 × 32
3 × 84 × 84
# Classes/task
10
10
5
# Training samples/tasks
54,000
4,750
2,375
# Validation Samples/tasks
6,000
250
125
# Test samples/tasks
10,000
1,000
500
Table 5: 5-Datasets Statistics. For the datasets with monochromatic images, we replicate the image
across all RGB channels so that size of each image becomes 3 × 32 × 32.
CIFAR-10
MNIST
SVHN
Fashion MNIST
notMNIST
# Classes
10
10
10
10
10
# Training samples
47,500
57,000
69,595
57,000
16,011
# Validation Samples
2,500
3,000
3,662
3,000
842
# Test samples
10,000
10,000
26,032
10,000
1,873
C.2
ARCHITECTURE DETAILS
AlexNet-like architecture: This is the same architecture used by Serr`a et al. (2018) with batch
normalization added in each layer except the classiﬁer layer. The network consists of 3 convolutional
layers of 64, 128, and 256 ﬁlters with 4 × 4, 3 × 3, and 2 × 2 kernel sizes, respectively, plus two
fully connected layers of 2048 units each. Rectiﬁed linear units is used as activations, and 2 × 2
max-pooling after the convolutional layers. Dropout of 0.2 is used for the ﬁrst two layers and 0.5
for the rest.
Reduced ResNet18 architecture: This is the similar architecture used by Lopez-Paz & Ranzato
(2017). For miniImageNet experiment, we use convolution with stride 2 in the ﬁrst layer. For both
miniImageNet and 5-Datasets experiments we replace the 4 × 4 average-pooling before classiﬁer
layer with 2 × 2 average-pooling.
All the networks use ReLU in the hidden units and softmax with cross entropy loss in the ﬁnal layer.
C.3
CIFAR-100 SUPERCLASS EXPERIMENT
For this experiment, similar to APD (Yoon et al., 2020), we use a modiﬁed LeNet-5 architecture
with 20-50-800-500 neurons. All the baseline results are reported from APD. Like APD, we do
not use any data augmentation or preprocessing. We keep 5% of training data from each task for
validation. We train the network with our algorithm with batch size of 64 and initial learning rate of
0.01. We Train each task for a maximum of 50 epochs with decay schedule and early termination
strategy similar to Serr`a et al. (2018). We use ϵth = 0.98 for all the layers and increasing the value
of ϵth by 0.001 for each new tasks.
C.4
BASELINE IMPLEMENTATIONS
GEM (Lopez-Paz & Ranzato, 2017), A-GEM (Chaudhry et al., 2019a), ER Res (Chaudhry et al.,
2019b) and OWM (Zeng et al., 2018) are implemented from their respective ofﬁcial implementa-
tions. EWC and HAT are implemented from the ofﬁcial implementation provided by Serr`a et al.
(2018). While, OGD is implemented from adapting the code provided by Bennani et al. (2020).
14

Published as a conference paper at ICLR 2021
C.5
THRESHOLD HYPERPARAMETER
As discussed in section 5 and section 7, the threshold hyperparameter, ϵth controls the degree of
interference through the approximation of space of signiﬁcant representations of the past tasks.
Since in neural network, characteristics of learned representations vary for different architectures
and different dataset, using the same value of ϵth may not be useful in capturing the similar space
of signiﬁcance. In our experiments we use ϵth in the range of 0.95 to 1. For PMNIST experiment,
as discussed in section 7, we use ϵth = 0.95 in the ﬁrst layer and 0.99 in the other layers. For split
CIFAR-100 experiment, we use ϵth = 0.97 for all the layers and increasing the value of ϵth by 0.003
for each new tasks. For split miniImageNet experiment, we use ϵth = 0.985 for all the layers and
increasing the value of ϵth by 0.0003 for each new tasks. For experiment with 5-Datasets, we use
ϵth = 0.965 for all the layers across all the tasks.
C.6
TRAINING TIME MEASUREMENT
We measured per epoch training times (in Figure 2(b)) for computation in NVIDIA GeForce GTX
1060 GPU. For ten sequential tasks in PMNIST experiment, we computed per epoch training time
for each task and reported the average value over all the tasks.
Training time for different algorithms reported in Table 2(a) for PMNIST tasks were measured on
a Single NVIDIA GeForce GTX 1060 GPU. For all the other datasets, training time for different
algorithms reported in Table 2(b) were measured on a Single NVIDIA GeForce GTX 1080 Ti GPU.
C.7
LIST OF HYPERPARAMETERS
Table 6: List of hyperparameters for the baselines and our approach. Here, ‘lr’ represents (initial)
learning rate. In the table we represent PMNIST as ‘perm’, 10-Split CIFAR-100 as ‘cifar’, Split
miniImageNet as ‘minImg’ and 5-Datasets as ‘5data’.
Methods
Hyperparameters
OGD
lr : 0.001 (perm)
# stored gradients : 200/task (perm)
OWM
lr : 0.01 (cifar), 0.3 (perm)
GEM
lr : 0.1 (perm)
memory size (samples) : 1000 (perm)
memory strength, γ : 0.5 (perm)
A-GEM
lr : 0.05 (cifar), 0.1 (perm, minImg, 5data)
memory size (samples) : 1000 (perm), 2000 (cifar), 500 (minImg), 3000 (5data)
ER Res
lr : 0.05 (cifar), 0.1 (perm, minImg, 5data)
memory size (samples) : 1000 (perm), 2000 (cifar), 500 (minImg), 3000 (5data)
EWC
lr : 0.03 (perm, minImg, 5data), 0.05 (cifar)
regularization coefﬁcient : 1000 (perm), 5000 (cifar, minImg, 5data)
HAT
lr : 0.03 (minImg), 0.05 (cifar), 0.1 (5data)
smax : 400 (cifar, minImg, 5data)
c : 0.75 (cifar, minImg, 5data)
Multitask
lr : 0.05 (cifar), 0.1 (perm, minImg, 5data)
GPM (ours)
lr : 0.01 (perm, cifar), 0.1 (minImg, 5data)
ns : 100 (minImg, 5data), 125 (cifar), 300 (perm)
15

Published as a conference paper at ICLR 2021
C.8
GPM SIZE
As discussed in section 4, the gradient update will lie in the span of input vectors (x) in fully con-
nected layers, whereas the gradient updates of the convolutional ﬁlters will lie in the space spanned
by the input patch vectors (p). Therefore, each basis stored in the GPM for a particular layer, l will
have the same dimension as xl or pl. Thus, for any particular layer, GPM matrix, M l can have a
maximum size of : size(xl) × size(xl) or size(pl) × size(pl). Maximum size of the GPM, which
we refer as GPM Max, is computed by including GPM matrices (M l) from all the layers. Thus the
size of GPM Max is ﬁxed by the choice of network architecture. In Table 7, we show the maximum
size of GPM matrix (M l) for each layers for the architectures that we have used in our experiments
along with the size of GPM Max.
Table 7: Size of GPM matrices for each layer for the architectures used in our experiments. Maxi-
mum sizes of the GPM in terms of number of parameters are also given.
Network
Size of maximum M l
GPM Max
(parameters)
MLP
784 × 784, 100 × 100, 100 × 100
0.63M
(3 layers)
AlexNet
48 × 48, 576 × 576, 512 × 512,
5.84M
(5 layers)
1024 × 1024, 2048 × 2048
ResNet18
27 × 27, 180 × 180, 180 × 180, 180 × 180, 180 × 180,
(17 layers+
180 × 180, 360 × 360, 20 × 20, 360 × 360, 360 × 360,
8.98M
3 short-cut
360 × 360, 720 × 720, 40 × 40, 720 × 720, 720 × 720,
connections)
720 × 720, 1440 × 1440, 80 × 80, 1440 × 1440,
1440 × 1440
D
ADDITIONAL RESULTS
D.1
RESULT TABLES
Table 8 contains the additional results for PMNIST experiment in single-epoch setting along with
the standard deviation values for the results shown in Table 1(a) for multi-epoch (5 epoch) setting.
Method that does not adhere to CL setup is indicated by (*) in the table. Results are reported from
5 different runs.
Table 8: Continual learning on PMNIST in single-epoch and multi-epoch setting.
1 Epoch
5 Epochs
Methods
ACC (%)
BWT
ACC (%)
BWT
OGD
85.18 ± 0.29
- 0.06 ± 0.00
82.56 ± 0.66
- 0.14 ± 0.01
OWM
90.55 ± 0.15
- 0.01 ± 0.00
90.71 ± 0.11
- 0.01 ± 0.00
GEM
88.65 ± 0.27
- 0.07 ± 0.00
83.38 ± 0.56
- 0.15 ± 0.01
A-GEM
87.80 ± 0.16
- 0.08 ± 0.00
83.56 ± 0.16
- 0.14 ± 0.00
ER Res
90.63 ± 0.27
- 0.05 ± 0.00
87.24 ± 0.53
- 0.11 ± 0.01
EWC
88.27 ± 0.39
- 0.04 ± 0.01
89.97 ± 0.57
- 0.04 ± 0.01
GPM (ours)
91.74 ± 0.15
- 0.03 ± 0.00
93.91 ± 0.16
-0.03 ± 0.00
Multitask*
95.21 ± 0.01
-
96.70 ± 0.02
-
16

Published as a conference paper at ICLR 2021
Table 9: Continual learning on different datasets along with the standard deviation values for the
results shown in Table 1(b).
CIFAR-100
miniImageNet
5-Datasets
Methods
ACC (%)
BWT
ACC (%)
BWT
ACC (%)
BWT
OWM
50.94 ± 0.60
- 0.30 ± 0.01
-
-
-
-
EWC
68.80 ± 0.88
- 0.02 ± 0.01
52.01 ± 2.53
- 0.12 ± 0.03
88.64 ± 0.26
- 0.04 ± 0.01
HAT
72.06 ± 0.50
- 0.00 ± 0.00
59.78 ± 0.57
- 0.03 ± 0.00
91.32 ± 0.18
- 0.01 ± 0.00
A-GEM
63.98 ± 1.22
- 0.15 ± 0.02
57.24 ± 0.72
- 0.12 ± 0.01
84.04 ± 0.33
-0.12 ± 0.01
ER Res
71.73 ± 0.63
- 0.06 ± 0.01
58.94 ± 0.85
- 0.07 ± 0.01
88.31 ± 0.22
- 0.04 ± 0.00
GPM (ours)
72.48 ± 0.40
- 0.00 ± 0.00
60.41 ± 0.61
- 0.00 ± 0.00
91.22 ± 0.20
- 0.01 ± 0.00
Multitask*
79.58± 0.54
-
69.46 ± 0.62
-
91.54 ± 0.28
-
D.2
k VALUES
Table 10 (a) and (b) show the number of new bases added at each layer per PMNIST and 10-split
CIFAR-100 task respectively. Total number of bases in the GPM after learning all the tasks is also
given.
Table 10: Number of new bases (k) added to the GPM at different layers after each (a) PMNIST
task and (b) 10-split CIFAR-100 task (for a random seed conﬁguration).
(a)
k
Task ID
FC1
FC2
FC3
1
81
60
41
2
71
22
19
3
70
11
11
4
61
4
7
5
57
2
4
6
50
1
2
7
46
0
2
8
41
0
1
9
35
0
1
10
31
0
0
Total
543
100
88
(b)
k
Task ID
ϵth
Conv1
Conv2
Conv3
FC1
FC2
1
0.970
7
125
197
80
98
2
0.973
3
44
74
81
101
3
0.976
0
20
29
71
93
4
0.979
1
21
26
76
99
5
0.982
2
33
28
73
98
6
0.985
0
16
19
71
98
7
0.988
0
16
14
71
99
8
0.991
2
41
26
76
104
9
0.994
6
56
34
84
109
10
0.997
1
45
26
86
113
Total
22
417
473
769
1012
Table 11: Continual learning of Digit dataset tasks in class-incremental learning setup (Kamra et al.,
2017). (†) denotes the result reported from DGDMN.
Methods
Metric
EWC†
DGR†
DGDMN†
GPM (ours)
ACC (%)
10.00
59.60
81.80
70.67
BWT
- 1.00
- 0.43
- 0.15
- 0.26
D.3
CLASS-INCREMENTAL LEARNING
In this section, we evaluate our algorithm in a class-incremental learning setup (Rebufﬁet al., 2017),
where disjoint classes are learned one by one and classiﬁcation is performed within all the learned
classes without task hint (Kamra et al., 2017). This setup is different (Hsu et al., 2018) from the
(single-head/multi-head) evaluation setups used throughout this paper. Also, this scenario is very
challenging and often infeasible for the regularization (HAT, EWC etc.) and expansion-based (DEN,
APD etc.) methods which do not use old data replay. Using the experimental setting similar to
DGDMN (Kamra et al., 2017), we implemented the ‘Digit dataset’ experiment where a single class
17

Published as a conference paper at ICLR 2021
of MNIST digit is learned per task. Results are listed in Table 11, where the baselines are reported
from DGDMN. While EWC forgets catastrophically, we perform better than DGR (Shin et al.,
2017), which employs data replay through old data generation. DGDMN, an improved data replay
method, outperforms all. In this setup, we believe a subset of old data replay either from storage or
via generation is inevitable for attaining better performance with minimal forgetting (Rebufﬁet al.,
2017; Rajasegaran et al., 2019).
D.4
ADDITIONAL PLOTS
0
2
4
6
8
10
12
14
16
18
20
Tasks
40
50
60
70
Accuracy (%)
EWC
HAT
A-GEM
ER_Res
GPM
Figure 4: Evolution of task 1 accuracy over the course of incremental learning of 20 sequential tasks
from miniImageNet dataset. Learned accuracy in our method remains stable throughout learning.
1.0
0.5
0.0
0.5
1.0
activation
0
2
4
6
8
count
(a)
=0.5
=0.7
=0.8
=0.9
=0.95
1.0
0.5
0.0
0.5
1.0
activation
0
2
4
6
8
(b)
=0.5
=0.7
=0.8
=0.9
=0.95
1.0
0.5
0.0
0.5
1.0
activation
0
2
4
(c)
=0.5
=0.7
=0.8
=0.9
=0.95
Figure 5: Illustration of how threshold hyperparameter controls the degree of interference at (a)
Conv layer 1 (b) Conv layer 3 (c) FC layer 1 with the histogram plots of interference activations
from Split CIFAR-100 experiment. With increasing ϵth, spread of the inference activation decreases
resulting in minimization of forgetting.
18

