Published as a conference paper at ICLR 2021
INFORMATION LAUNDERING FOR MODEL PRIVACY
Xinran Wang
School of Statistics
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
wang8740@umn.edu
Yu Xiang
Electrical and Computer Engineering
University of Utah
Salt Lake City, UT 84112, USA
yu.xiang@utah.edu
Jun Gao
Department of Mathematics
Stanford University
Stanford, CA 94305, USA
jung2@stanford.edu
Jie Ding
School of Statistics
University of Minnesota-Twin Cities
Minneapolis, MN 55455, USA
dingj@umn.edu
ABSTRACT
In this work, we propose information laundering, a novel framework for enhanc-
ing model privacy. Unlike data privacy that concerns the protection of raw data
information, model privacy aims to protect an already-learned model that is to be
deployed for public use. The private model can be obtained from general learning
methods, and its deployment means that it will return a deterministic or random
response for a given input query. An information-laundered model consists of
probabilistic components that deliberately maneuver the intended input and out-
put for queries of the model, so the modelâ€™s adversarial acquisition is less likely.
Under the proposed framework, we develop an information-theoretic principle to
quantify the fundamental tradeoffs between model utility and privacy leakage, and
derive the optimal design.
1
INTRODUCTION
An emerging number of applications involve the following user-scenario. Alice developed a model
that takes a speciï¬c query as input and calculates a response as output. The model is a stochastic
black-box that may represent a novel type of ensemble models, a known deep neural network ar-
chitecture with sophisticated parameter tuning, or a physical law described by stochastic differential
equations. Bob is a user that sends a query to Alice and obtains the corresponding response for his
speciï¬c purposes, whether benign or adversarial. Examples of the above scenario include many re-
cent Machine-Learning-as-a-Service (MLaaS) services (Alabbadi, 2011; Ribeiro et al., 2015; Xian
et al., 2020) and artiï¬cial intelligence chips, where Alice represents a learning service provider, and
Bob represents users.
Suppose that Bob obtains sufï¬cient paired input-output data as generated from Aliceâ€™s black-box
model, it is conceivable that Bob could treat it as supervised data and reconstruct Aliceâ€™s model to
some extent. From the view of Alice, her model may be treated as valuable and private. As Bob
who queries the model may be benign or adversarial, Alice may intend to offer limited utility for the
return of enhanced privacy. The above concern naturally motivates the following problem.
(Q1) How to enhance the privacy for an already-learned model? Note that the above problem is
not about data privacy, where the typical goal is to prevent adversarial inference of the data infor-
mation during data transmission or model training. In contrast, model privacy concerns an already-
established model. We propose to study a general approach to jointly maneuver the original queryâ€™s
input and output so that Bob ï¬nds it challenging to guess Aliceâ€™s core model. As illustrated in
Figure 1a, Aliceâ€™s model is treated as a transition kernel (or communication channel) that produces
ËœY conditional on any given ËœX. Compared with an honest service Alice would have provided (Fig-
ure 1b), the input ËœX is a maneuvered version of Bobâ€™s original input X; Moreover, Alice may choose
1

Published as a conference paper at ICLR 2021
!"
!#
#
$
Pseudo-response
Response
"
Query
#
"
Query
Pseudo-query
Response
(a)
(b)
$%
$âˆ—
$âˆ—
$'
Figure 1: Illustration of (a) Aliceâ€™s effective system for public use, and (b) Aliceâ€™s idealistic system
not for public use. In the ï¬gure, Kâˆ—denotes the already-learned model/API, K1 denotes the kernel
that perturbs the input data query by potential adversaries, and K2 denotes the kernel that perturbs
the output response from Kâˆ—to publish the ï¬nal response Y .
to return a perturbed outcome Y instead of ËœY to Bob. Consequently, the apparent kernel from Bobâ€™s
input query X to the output response Y is a cascade of three kernels, denoted by K in Figure 1a. The
above perspective provides a natural and general framework to study model privacy. Admittedly, if
Alice produces a (nearly) random response, adversaries will ï¬nd it difï¬cult to steal the model, while
benign users will ï¬nd it useless. Consequently, we raise another problem.
(Q2) How to formulate the model privacy-utility tradeoff, and what is the optimal way of imposing
privacy? To address this question, we formulate a model privacy framework from an information-
theoretic perspective, named information laundering. We brieï¬‚y describe the idea below. The gen-
eral goal is to jointly design the input and output kernels (K1 and K2 in Figure 1a) that deliberately
maneuver the intended input and output for queries of the model so that 1) the effective kernel (K
in Figure 1a) for Bob is not too far away from the original kernel (Kâˆ—in Figure 1a), and 2) adver-
sarial acquisition of the model becomes difï¬cult. Alternatively, Alice â€˜laundersâ€™ the input-output
information maximally given a ï¬xed utility loss. To ï¬nd the optimal way of information launder-
ing, we propose an objective function that involves two components: the ï¬rst being the information
shared between X, ËœX and between ËœY , Y , and the second being the average Kullback-Leibler (KL)
divergence between the conditional distribution describing K and Kâˆ—. Intuitively, the ï¬rst compo-
nent controls the difï¬culty of guessing Kâˆ—sandwiched between two artiï¬cial kernels K1 and K2,
while the second component ensures that overall utility is maximized under the same privacy con-
straints. By optimizing the objective for varying weights between the components, we can quantify
the fundamental tradeoffs between model utility and privacy.
1.1
RELATED WORK
We introduce some closely related literature below. Section 3.3 will incorporate more technical
discussions on some related but different frameworks, including information bottleneck, local data
privacy, information privacy, and adversarial model attack.
A closely related subject of study is data privacy, which has received extensive attention in re-
cent years due to societal concerns (Voigt & Von dem Bussche, 2017; Evans et al., 2015; Cross
& Cavallaro, 2020; Google, 2019; Facebook, 2020). Data privacy concerns the protection of (usu-
ally personal) data information from different perspectives, including lossless cryptography (Yao,
1982; Chaum et al., 1988), randomized data collection (Evï¬mievski et al., 2003; Kasiviswanathan
et al., 2011; Ding & Ding, 2020), statistical database query (Dwork & Nissim, 2004; Dwork, 2011),
membership inference (Shokri et al., 2017), and Federated learning (Shokri & Shmatikov, 2015;
Konevcny et al., 2016; McMahan et al., 2017; Yang et al., 2019; Diao et al., 2020).
A common
goal in data privacy is to obfuscate individual-level data values while still enabling population-wide
learning. In contrast, the subject of model privacy focuses on protecting a single learned model
ready to deploy. For example, we want to privatize a classiï¬er to deploy on the cloud for public use,
whether the model is previously trained from raw image data or a data-private procedure.
Another closely related subject is the model extraction in (Tram`er et al., 2016; Papernot et al.,
2016b), where Bobâ€™s goal is to reconstruct Aliceâ€™s model from several queriesâ€™ inputs and outputs,
knowing what speciï¬c model Alice uses. For example, suppose that Aliceâ€™s model is a generalized
linear regression with p features. In that case, it is likely to be reconstructed using p queries of the ex-
pected mean (a known function of XÎ²) by solving equations (Tram`er et al., 2016). In the supervised
classiï¬cation scenario, when only labels are returned to any given input, model extraction could be
2

Published as a conference paper at ICLR 2021
cast as an active learning problem where the goal is to query most efï¬ciently (Chandrasekaran et al.,
2018). Model extraction was also studied in contexts beyond the prediction API, e.g., when an ad-
versary utilizes the gradient information (Milli et al., 2019). From Aliceâ€™s perspective, there exist
several solutions to safeguard against model leakage. A warning-based method was developed in
(Kesarwani et al., 2018), where Alice continuously monitors the information gain and raise alarms
when they become unusual. Another warning method was developed in (Juuti et al., 2019), where
the detection of an adversary is based on testing whether the pairwise distances among queried data
approximately follow the Gaussian distribution. The work in (Lee et al., 2018) developed a defense
strategy for the setting where the target is a classiï¬er, and the adversary queries each classâ€™s prob-
ability. The probabilities are maximally perturbed under the constraint that the most-likely class
label) remains the same. The work in (Orekondy et al., 2019) studied a similar setting but from a
different perspective. The main idea is to perturb the probabilities within an â„“2-distance constraint
to poison the adversaryâ€™s gradient signals.
1.2
CONTRIBUTIONS AND OUTLINE
The main contributions of this work are three folds. First, we develop a novel concept, theory,
and method, generally referred to as information laundering, to study model privacy. Unlike data
privacy that concerns the protection of raw data information, model privacy aims to privatize an
already-learned model for public use. To the best of the authorsâ€™ knowledge, this work is the ï¬rst
framework to study model privacy in a principled manner. Second, under the developed information-
theoretic framework, we cast the tradeoffs between model privacy and utility as a general optimiza-
tion problem. We derive the optimal solution using the calculus of variations and provide extensive
discussions on the solutionâ€™s insights from different angles. Third, we develop a concrete algorithm,
prove its convergence, and elaborate on some speciï¬c cases. We also provide some experimental
studies to illustrate the concepts, and discuss several future problems at the end.
The paper is organized as follows. In Section 2, we describe the problem formulation and a gen-
eral approach to protect the model. In Section 3, we propose the information laundering method that
casts the model privacy-utility tradeoff as an optimization problem and derives a general solution. In
Section 3.3, we provide some additional discussions of the related frameworks, including informa-
tion bottleneck, local data privacy, information privacy, and adversarial model attack. In Section 5,
we conclude the paper with some potential future work. In the Appendix, we provide the proofs of
the main results and experimental studies.
2
FORMULATION
2.1
BACKGROUND
The private model can be obtained from general learning methods, and its deployment means that
it will return a response for a given input query. Suppose that X and Y are the input and output
alphabets (data space), respectively.
Deï¬nition 1 (Learned model) A learned model is a kernel p : X Ã— Y â†’[0, 1], which induces a
class of conditional distributions {p(Â· | x) : x âˆˆX}.
A model in the above deï¬nition is also referred to as a communication channel in information theory.
A model can be regarded as the input-output (or Aliceâ€™s application programming interface, API)
offered to Bob. Examples include a regression/classiï¬cation model that outputs predicted labels,
a clustering model that outputs the probabilities of belonging to speciï¬c groups, and a stochastic
differential equation system that outputs the likely paths for various inputs variables. It does not
matter where the model comes from since we are only concerned about the privacy of a ï¬xed given
model. The (authentic) model of Alice is denoted by pKâˆ—.
An adversary Bob is a user that can access the above modelâ€™s API, providing an arbitrary input, X,
and obtaining an output, Y . Bob aims to use as few queries as possible to construct a model that
closely matches Aliceâ€™s model pKâˆ—. We will formalize the â€˜closenessâ€™ using the KL divergence.
What is model privacy? Our perspective is that privacy is not an intrinsic quantity associated with
a model; instead, it is a measure of information that arises from interactions between the model and
3

Published as a conference paper at ICLR 2021
its queries. In our context, the interactions are through X (offered by Bob) and Y (offered by Alice).
The key idea of enhancing Aliceâ€™s model privacy is to let Alice output noisy predictions ËœY for any
input X so that Bob cannot easily infer Aliceâ€™s original model. Similarly, Alice may choose to
manipulate X as well before passing it through Kâˆ—. Alternatively, Alice intends to 1) impose some
ambiguity between X, ËœX, and between Y, ËœY , which conceivably will produce response deviating
from the original one, and 2) seek the K closest to Kâˆ—under the same amount of ambiguity imposed.
Motivated by the above concepts, we introduce the following notion.
Deï¬nition 2 (Information-laundered model) A information-laundered model with respect to a
given model Kâˆ—is a model K that consists of three internal kernels K = K1 â—¦Kâˆ—â—¦K2 (illus-
trated in Figure 1).
Naturally, the information-laundered model of Alice is denoted by pK.
2.2
NOTATION
We let pKâˆ—(Â· | Â·), pK1(Â· | Â·), pK2(Â· | Â·), pK(Â· | Â·) denote the kernels that represent the authentic
model, input kernel, output kernel, and the information-laundered model, respectively. We let pX(Â·)
denote the marginal distribution of X. Similar notation is for p Ëœ
X(Â·), p ËœY (Â·), and pY (Â·). Note that the
pY implicitly depends on the above conditional distributions. We use pK1â—¦Kâˆ—(Â· | Â·) and pKâˆ—â—¦K2(Â· | Â·)
to denote cascade conditional distributions of ËœY | X and Y | ËœX, respectively.
Throughout the paper, random variables are denoted by capital letters. Suppose that X âˆˆX, ËœX âˆˆËœ
X,
ËœY âˆˆËœY, and Y âˆˆY. For technical convenience, we will assume that X, Ëœ
X, ËœY, Y are ï¬nite alphabets
unless otherwise stated. We will discuss some special cases when some of them are the same. Our
theoretical results apply to continuous alphabets as well under suitable conditions. For notational
convenience, we write the sum P
xâˆˆX u(x) as P
x u(x) for any function u.
With a slight abuse of notation, we will use p to denote a distribution, density function, or transition
kernel, depending on the context.
3
INFORMATION LAUNDERING
3.1
THE INFORMATION LAUNDERING PRINCIPLE
The information laundering method is an optimization problem formulated from the concept of
KL-divergence between the (designed) effective kernel and the original kernel, with constraints of
the privacy leakage during the model-data interaction. In particular, we propose to minimize the
following objective function over (pK1, pK2),
L(pK1, pK2)
âˆ†= EXâˆ¼pXDKL(pKâˆ—(Â· | X), pK(Â· | X)) + Î²1I(X; ËœX) + Î²2I(Y ; ËœY ).
(1)
In the above, K1 and K2 are implicitly involved in each additive term of L, and Î²1 â‰¥0, Î²2 â‰¥0
are constants that determine the utility-privacy tradeoffs. Small values of Î²1 and Î²2 (e.g., zeros)
pushes the K to be the same as Kâˆ—, while large values of Î²1 pushes ËœX to be nearly-independent
with X (similarly for Î²2). It is worth mentioning that the principle presumes a given alphabet (or
representation) for ËœX and ËœY . The variables to optimize over is the transition laws X â†’ËœX and
ËœY â†’Y .
The objective in (1) may be interpreted in the following way. On the one hand, Alice aims to develop
an effective system of K that resembles the authentic one Kâˆ—for the utility of benign users. This
goal is realized through the ï¬rst term in (1), which is the average divergence between two system
dynamics. On the other hand, Aliceâ€™s model privacy leakage is through interactions with Bob, which
in turn is through the input X (publicly offered by Bob) and output Y (publicly offered by Alice).
Thus, we control the information propagated through both the input-interfaces and out-interfaces,
leading to the second and third terms in (1).
We note that the above objective function may also be formulated in alternative ways from different
perspectives. For example, we may change the third term to be Î²2I(Y ; ËœY | X, ËœX), interpreted
4

Published as a conference paper at ICLR 2021
in the way that Alice will design K1 ï¬rst, and then design K2 conditional on K1. Likewise, we
may change the second term to be Î²1I(X; ËœX | ËœY , Y ), meaning that K2 is designed ï¬rst. From
Bobâ€™s perspective, we may also change the third term to Î²2I(Y ; ËœY | X), interpreted for the scenario
where Bob conditions on the input information X during model extraction. Additionally, from
the perspective of adaptive interactions between Alice and Bob, we may consider pX as part of
the optimization and solve the max-min problem maxpX minpK1,pK2 L(pK1, pK2). We leave these
alternative views for future work.
3.2
THE OPTIMAL SOLUTION
We derive the solution that corresponds to the optimal tradeoffs and point out some nice interpre-
tations of the results. The derivation is nontrivial as the functional involves several nonlinear terms
of the variables to optimize over. Note that for the notation deï¬ned in Subsection 2.2, only pX and
pKâˆ—are known and others are (implicitly) determined by pK1, pK2.
Theorem 1 The optimal solution of (1) satisï¬es the following equations.
pK1(Ëœx | x) = Îºxp Ëœ
X(Ëœx) exp
 1
Î²1
EY |X=xâˆ¼pKâˆ—
pKâˆ—â—¦K2(Y | Ëœx)
pK(Y | x)
âˆ’Î²2
Î²1
E ËœY ,Y | Ëœ
X=Ëœx log pK2(Y | ËœY )
pY (Y )

,
(2)
pK2(y | Ëœy) = Ï„ËœypY (y) exp

1
Î²2p ËœY (Ëœy)EXâˆ¼pX
pKâˆ—(y | X) Â· pK1â—¦Kâˆ—(Ëœy | X)
pK(y | X)

,
(3)
where Îºx and Ï„Ëœy are normalizing constants implicitly deï¬ned so that the conditional density function
integrates to one.
Note that the distributions of ËœX, ËœY , Y , and ËœY , Y | ËœX, implicitly depend on pK1 and pK2. The above
theorem naturally leads to an iterative algorithm to estimate the unknown conditional distributions
pK1 and pK2. In particular, we may alternate Equations (2) and (3) to obtain p(â„“)
K1(Ëœx | x), p(â„“)
K2(y | Ëœy)
from p(â„“âˆ’1)
K1
(Ëœx | x), p(â„“âˆ’1)
K2
(y | Ëœy) at step â„“= 1, 2, . . . with random initial values at â„“= 0. The
pseudocode is summarized in Algorithm 1.
In the next theorem, we show that the convergence of the algorithm. The sketch of the proof is
described below. First, we treat the original objective L as another functional J of four independent
variables, pK1, pK2, h1, h2, evaluated at h1 = p Ëœ
X and h2 = pY . Using a technique historically used
to prove the convergence of the Blahut-Arimoto algorithm for calculating rate-distortion functions
in information theory, we show that J â‰¥L. We also show that L is convex in each variable so that
the objective function is non-increasing in each alternation between four equations. Since L â‰¥0,
the convergence is implied by the monotone convergence theorem.
Theorem 2 Algorithm 1 converges to a minimum that satisï¬es equations (2) and (3).
Note that the minimum is possibly a local minimum. We will later show the convergence to a global
minimum in a particular case. Next, we provide interpretations of the parameters and how they
affect the ï¬nal solution.
A large Î²1 in the optimization of (1) indicates a higher weight on the term I(X; ËœX). In the extreme
case when Î²1 = âˆ, minimizing I(X; ËœX) is attained when ËœX is independent with X. Consequently,
the effective model of Alice produces a ï¬xed distribution of responses for whatever Bob queries.
The above observation is in line with the derived equation (2), which will become pK1(Ëœx | x) â‰ˆ
Îºxp Ëœ
X(Ëœx) (and thus Îºx â‰ˆ1) for a large Î²1 > 0.
Similar to the effect of Î²1, a larger Î²2 imposes more independence between ËœY and Y . In the case
Î²2 = âˆ, Alice may pass the input to her internal model Kâˆ—but output random results. This can be
seen from either the Formulation (1) or Equation (3).
For the ï¬rst expectation in equation (2), the term may be interpreted as the average likelihood ratio
of y conditional on Ëœx against x. From Equation (2), it is more likely to transit from x to Ëœx in the
presence of a larger likelihood ratio. This result is intuitively appealing because a large likelihood
ratio indicates that x may be replaced with Ëœx without harming the overall likelihood of observing Y .
5

Published as a conference paper at ICLR 2021
Algorithm 1 Optimized Information Laundering (OIL)
input Input distribution pX, private model pKâˆ—, alphabets X, Ëœ
X, ËœY, Y for X, Ëœ
X, ËœY , Y , respectively.
output Transition kernels pK1 and pK2
1: Let p(0)
Ëœ
X and p(0)
Y
denote the uniform distribution on Ëœ
X and Y, respectively.
2: for t = 0 â†’T âˆ’1 do
3:
Calculate
p(t+1)
K1
(Ëœx | x) = Îºxp(t)
Ëœ
X (Ëœx) exp
 1
Î²1 EY |xâˆ¼pKâˆ—
p(t)
Kâˆ—â—¦K2(Y | Ëœx)
p(t)
K (Y | x)
âˆ’Î²2
Î²1 E ËœY ,Y |Ëœxâˆ¼p(t)
Kâˆ—â—¦K2
log
p(t)
K2(Y | ËœY )
p(t)
Y (Y )

,
p(t+1)
K2
(y | Ëœy) = Ï„Ëœyp(t)
Y (y) exp

1
Î²2p(t)
ËœY (Ëœy)
EXâˆ¼pX
pKâˆ—(y | X) Â· p(t+1)
K1â—¦Kâˆ—(Ëœy | X)
p(t+1,t)
K
(y | X)

,
p(t+1)
Ëœ
X
(Ëœx) =
X
x
p(t+1)
K1
(Ëœx | x)pX(x),
p(t+1)
Y
(y) =
X
Ëœy
p(t+1)
K2
(y | Ëœy)p(t+1)
ËœY
(Ëœy),
where p(t+1)
K1â—¦Kâˆ—, p(t)
Kâˆ—â—¦K2, and p(t+1,t)
K
denote the kernels cascaded from (p(t+1)
K1
, pKâˆ—), (pKâˆ—, p(t)
K2), and
(p(t+1)
K1
, pKâˆ—, p(t)
K2), respectively, and p(t+1)
ËœY
is the marginal from (p(t+1)
Ëœ
X
, pKâˆ—, p(t+1)
K2
).
4: end for
5: Return pK1 = p(T )
K1 , pK2 = p(T )
K2 .
3.3
FURTHER DISCUSSIONS ON RELATED WORK
Information Bottleneck: extracting instead of privatizing information. The information bottle-
neck method (Tishby et al., 2000) is an information-theoretic approach that aims to ï¬nd a parsi-
monious representation of raw data X, denoted by ËœX, that contains the maximal information of a
variable Y of interest. The method has been applied to various learning problems such as clustering,
dimension reduction, and theoretical interpretations for deep neural networks (Tishby & Zaslavsky,
2015). Formally, the information bottleneck method assumes the Markov chain
ËœX â†’X â†’Y,
(4)
and seeks the the optimal transition law from X to ËœX by minimizing the functional
L(p Ëœ
X|X) = I(X; ËœX) âˆ’Î²I( ËœX; Y ),
with Î² being is a tuning parameter that controls the tradeoffs between compression rate (the ï¬rst
term) and amount of meaningful information (second term). The alphabet of the above ËœX needs to
be pre-selected and often much smaller in size compared to the alphabet of X to meet the purpose
of compression. In other words, the information that X provides about Y is passed through a
â€˜bottleneckâ€™ formed by the parsimonious alphabet of ËœX.
A similarity between the information bottleneck method and the particular case of information laun-
dering in Subsection A.2 is that they both optimize a functional of the transition law of X â†’ËœX.
Nevertheless, their objective and formulation are fundamentally different. First, the objective of
information bottleneck is to compress the representation while preserving meaningful information,
under the assumption of (4); Our goal is to distort X while minimizing the gap between the (random)
functionality of X â†’Y , under a different Markov chain X â†’ËœX â†’Y .
Data Privacy and Information Privacy: protecting data instead of a model. The tradeoffs be-
tween individual-level data privacy and population-level learning utility have motivated active re-
search on what is generally referred to as â€˜local data privacyâ€™ across multiple ï¬elds such as data
mining (Evï¬mievski et al., 2003), security (Kasiviswanathan et al., 2011), statistics (Duchi et al.,
2018), and information theory (du Pin Calmon & Fawaz, 2012; Sun et al., 2016). For example, a
popular framework is the local differential privacy (Evï¬mievski et al., 2003; Dwork et al., 2006;
6

Published as a conference paper at ICLR 2021
Kasiviswanathan et al., 2011), where raw data X is suitably randomized (often by adding Laplace
noises) into Y so that the ratio of conditional densities
eâˆ’Î± â‰¤pY |X(y | x1)
pY |X(y | x2) â‰¤eÎ±
(5)
for any y, x1, x2 âˆˆX, where Î± > 0 is a pre-determined value that quantities the level of privacy.
In the above, X and Y represent the private data and the processed data to be collected or publicly
distributed. The requirement (5) guarantees that the KL-divergence between pY |x1 and pY |x2 is
universally upper-bounded by a known function of Î± (see, e.g., Duchi et al., 2018), meaning that
x1 and x2 are barely distinguishable from the observed y. Note that the above comparison is made
between two conditional distributions, while the comparison in information laundering (recall the
ï¬rst term in (1)) is made between two transition kernels.
The local differential privacy framework does not need to specify a probability space for X, since
the notion of data privacy is only built on conditional distributions. Another related framework is
the information privacy (du Pin Calmon & Fawaz, 2012), which assumes a probabilistic structure
on X and a Markov chain X â†’ËœY â†’Y . In the above chain, X is the private raw data, ËœY is
a set of measurement points to transmit or publicize, and Y is a distortion of ËœY that is eventually
collected or publicized. We deliberately chose the above notation of X, ËœY , Y , so that the Markov
chain appears similar to the special case of information laundering in Subsection 4. Nevertheless,
the objective of information privacy is to minimize I(X; Y ) over pY | ËœY subject to utility constraints,
assuming that the joint distribution of X, ËœY is known. In other words, the goal is to maximally hide
the information of X. In the context of information laundering, the system input X is provided by
users and is known.
Adversarial Model Attack: rendering harm instead of utility to a model. The adversarial model
attack literature concerns the adversarial use of specially crafted input data to cause a machine
learning model, often a deep neural network, to malfunction (Papernot et al., 2016a; Narodytska
& Kasiviswanathan, 2017; Papernot et al., 2017). For example, an adversarial may inject noise
into an image so that a well-trained classiï¬er produces an unexpected output, even if the noise is
perceptually close to the original one. A standard attack is the so-called (Adaptive) Black-Box
Attack against classiï¬ers hosted by a model owner, e.g., Amazon and Google (Rosenberg et al.,
2017; Chakraborty et al., 2018). For a target model Kâˆ—, a black-box adversary has no information
about the training process of Kâˆ—but can access the target model through query-response interfaces.
The adversary issues (adaptive) queries and record the returned labels to train a local surrogate
model. The surrogate model is then used to craft adversarial samples to maximize the target modelâ€™s
prediction error.
If we let X, ËœX, Y denote the model input, adversarially perturbed input, and output, respectively,
then we may draw a similarity between adversarial model attack and the particular case of informa-
tion laundering in Subsection A.2 since they both look for the law X â†’ËœX. The main difference
is in the objective. While the model attack aims to ï¬nd an input domain that maximally distorts the
model, information laundering aims to maintain a small model discrepancy. Under our notation, a
possible formulation for the model attack is to seek maxp Ëœ
X|X EXâˆ¼pXDKL(pKâˆ—(Â· | X), pKâˆ—(Â· | ËœX))
under a constraint on p Ëœ
X|X.
4
SPECIAL CASE: INFORMATION LAUNDERING OF THE OUTPUT (Y ) ONLY
Two special cases of an information-laundered system are illustrated in Figure 2. Here, we elaborate
on one case and include the other special case in the Appendix. Suppose that K1 is an identity map
and let Î²1 = 0. In other words, we alter the output data only (Figure 2b). Furthermore, suppose that
for each given Ëœx, the conditional distribution pK(Â· | Ëœx) assigns all the mass at Ëœy. In other words,
Kâˆ—reduces to a deterministic function mapping from each Ëœx âˆˆX to a unique Ëœy âˆˆY, which is
denoted by Ëœy = f(Ëœx). For example, Aliceâ€™s model is a classiï¬er that takes input features and returns
hard-thresholded classiï¬cation labels. Then the optimization problem (1) reduces to minimizing
L(pK2)
âˆ†= EXâˆ¼pXDKL(pKâˆ—(Â· | X), pK(Â· | X)) + Î²2I(Y ; ËœY ).
(6)
7

Published as a conference paper at ICLR 2021
!"
#
$
Response
"
Query
Pseudo-query
(a)
$%
$âˆ—
!#
#
$
Pseudo-response
Response
"
Query
(b)
$âˆ—
$'
Figure 2: Illustration of Aliceâ€™s information-laundered system for public use, by (a) alternating input
only, and (b) alternating output only. The notations are similar to those in Figure 1.
Corollary 1 The solution to the optimization problem (6) satisï¬es
pK2(y | Ëœy) = Ï„ËœypY (y) exp

1
Î²2p ËœY (Ëœy)EXâˆ¼pX
pKâˆ—(y | X) Â· pKâˆ—(Ëœy | X)
pK(y | X)

,
(7)
where Ï„Ëœy is a normalizing constant. In particular, if Kâˆ—is deterministic, equation (7) becomes
pK2(y | Ëœy) = Ï„ËœypY (y) exp

1
Î²2p ËœY (Ëœy)
X
x:f(x)=y
pX(x)
1y=Ëœy
pK(y | x)

= Ï„ËœypY (y) exp

1y=Ëœy
Î²2 pK2(y | y)

(8)
To exemplify the proposed methodology, we study a speciï¬c case with the following conditions.
1) X may be large or continuously-valued, ËœY = Y is a moderately-large alphabet,
2) ËœY = Y so that ËœY and Y are in the same space,
3) Kâˆ—is deterministic.
Under the above scenario, we can apply Algorithm 1 and Corollary 1 to obtain a simpliï¬ed procedure
below (denoted by OIL-Y). At each time step t = 1, 2, . . . ,, for each Ëœy, y âˆˆY, we calculate
p(t+1)
K2
(y | Ëœy) = Ï„Ëœyp(t)
Y (y) exp

1y=Ëœy
Î²2 p(t)
K2(y | y)

, where Ï„ âˆ’1
Ëœy
=
X
y
p(t)
Y (y) exp

1y=Ëœy
Î²2 p(t)
K2(y | y)

,
p(t+1)
Y
(y) = rËœyp(t+1)
K2
(y | Ëœy), where rËœy =
X
x:f(x)=Ëœy
pX(x).
(9)
Note that the above rËœy is the probability that Alice observes Ëœy as an output of Kâˆ—âˆ—if Bob inputs
X âˆˆpX. Therefore, rËœy can be easily estimated to be the empirical frequency of observing Ëœy at the
end of Alice.
Note that since Y is a ï¬nite alphabet, we can use a matrix representation for easy implementation.
In particular, we represent the elements of Y by 1, . . . , a, where a = card(Y). We then represent
pK2 by P âˆˆRaÃ—a, and pY by q âˆˆRa, where Py,Ëœy = pK2(y | Ëœy). Such a representation will lead to
a matrix form of the above procedure, summarized in Algorithm 2.
Algorithm 2 OIL-Y (a special case of Algorithm 1, in the matrix form)
input Input distribution pX, private model pKâˆ—
output Transition kernels pK2 : Y Ã— Y â†’[0, 1] represented by P âˆˆRaÃ—a, where a = card(Y)
1: Estimate r = [r1, . . . , ra] from pX and pKâˆ—as in equation (9)
2: Initialize the entries of P (0) and q(0) (respectively representing pK2, pY ) to be 1/a
3: for t = 0 â†’T âˆ’1 do
4:
Calculate P (t+1) = q(t) Ã— 1T , diag(P ), where 1 = [1, . . . , 1] denote the a Ã— 1 vector.
5:
Update diag(P (t+1)) â†diag(P (t+1))Â·exp{1/(Î²2 diag(P (t))}, where the operations are element-wise
6:
Scale each column (conditional distribution) of P (t+1) so that it sums to one
7:
Calculate q(t+1) = P (t+1) Ã— r
8: end for
9: Return p(T )
K2 that is represented by P (T ).
Moreover, we proved the convergence to the global minimum for the alternating equations in the
above scenario. The same technique can be emulated to show a similar result when we employ K1
(instead of K2) only. The result is summarized in Theorem 3.
8

Published as a conference paper at ICLR 2021
Theorem 3 Suppose that Kâˆ—is deterministic. The alternating equation (9), or its matrix form in
Algorithm 1, converges to a global minimum of the problem (6).
5
CONCLUSION AND FURTHER REMARKS
Despite extensive studies on data privacy, little has been studied for enhancing model privacy. Mo-
tivated by the emerging concern of model privacy from the perspective of machine learning service
providers, we develop a plug-and-play methodology â€œinformation launderingâ€ to enhance the pri-
vacy of any given model of interest. The information laundering is model-agnostic as it applies to
general API models, including classiï¬cation and regression models that output labels/probabilities,
and black-box models generating probabilistic outputs. We believe that the developed principles,
theories, and insights can lead to new resilient machine learning algorithms and services.
An interesting problem is to integrate information laundering with various application scenarios on
a case-by-case basis. Another problem is to adapt the developed principle to speciï¬c API models
to incorporate side information (also mentioned in Section 3.1). Taking into account the potential
side information available to an adversary can be essential in some pathetic situations. Consider
an example where the adversary knows that the output associated with extremely large input is a
ï¬xed constant. Then, the adversary may strategically send the same input with extreme values to
accurately identify that constant. An information-laundered model may be vulnerable in the above
scenario since the current information laundering concerns the average over data distributions.
Theoretically, there are three open problems left from the work that deserves further research. First,
how does the imposed constraint of mutual information affect the rate of convergence from the
adversary perspective for speciï¬c models (e.g., generalized linear models, decision trees, neural
networks)? Second, we focused on ï¬nite alphabets for technical convenience. How to emulate the
current methods for continuously-valued alphabets (especially with large dimensions)? Third, what
would be the relative importance of laundering X versus Y , and will this depend on speciï¬c learning
problems?
Appendix. In Appendices A.1 and A.2, we ï¬rst include two particular cases of information laun-
dering that were not included in the main part of the paper. We then include the proofs of the
theorems in Appendix A.3. Experimental results are included in Appendices A.4, A.5, A.6, and A.7
to demonstrate the algorithm convergence, model privacy-utility tradeoffs, how tradeoff parameters
and unbalanced samples may inï¬‚uence the optimized information laundering, and how the informa-
tion laundering effectively mitigates adversarial attacks.
ACKNOWLEDGMENTS
The last author was supported by the Army Research Ofï¬ce (ARO) under grant number W911NF-
20-1-0222.
REFERENCES
Mohssen M Alabbadi. Mobile learning (mlearning) based on cloud computing: mlearning as a
service (mlaas). In UbiComp, 2011.
Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep Mukhopad-
hyay. Adversarial attacks and defences: A survey. arXiv preprint arXiv:1810.00069, 2018.
Varun Chandrasekaran, Kamalika Chaudhuri, Irene Giacomelli, Somesh Jha, and Songbai Yan.
Model extraction and active learning. arXiv preprint arXiv:1811.02054, 2018.
David Chaum, Claude CrÂ´epeau, and Ivan Damgard. Multiparty unconditionally secure protocols. In
Proc. STOC, pp. 11â€“19, 1988.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Maria S Cross and Andrea Cavallaro. Privacy as a feature for body-worn cameras [in the spotlight].
IEEE Signal Processing Magazine, 37(4):145â€“148, 2020.
9

Published as a conference paper at ICLR 2021
Enmao Diao, Jie Ding, and Vahid Tarokh. HeteroFL: Computation and communication efï¬cient
federated learning for heterogeneous clients. arxiv preprint arxiv:2010.01264, 2020.
Jie Ding and Bangjun Ding. â€˜To tell you the truthâ€™ by interval-private data. In Proc. IEEE BigData,
2020.
Flavio du Pin Calmon and Nadia Fawaz. Privacy against statistical inference. In Proc. Allerton Conf.
on Commun., Control and Computing, pp. 1401â€“1408, 2012.
John C Duchi, Michael I Jordan, and Martin J Wainwright. Minimax optimal procedures for locally
private estimation. J. Am. Stat. Assoc., 113(521):182â€“201, 2018.
Cynthia Dwork. Differential privacy. Encyclopedia of Cryptography and Security, pp. 338â€“340,
2011.
Cynthia Dwork and Kobbi Nissim.
Privacy-preserving datamining on vertically partitioned
databases. In Proc. CRYPTO, pp. 528â€“544. Springer, 2004.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265â€“284. Springer, 2006.
Nicholas Evans, Sebastien Marcel, Arun Ross, and Andrew Beng Jin Teoh. Biometrics security and
privacy protection. IEEE Signal Processing Magazine, 32(5):17â€“18, 2015.
Alexandre Evï¬mievski, Johannes Gehrke, and Ramakrishnan Srikant. Limiting privacy breaches in
privacy preserving data mining. In Proc. SIGMOD/PODS03, pp. 211â€“222, 2003.
Facebook.
Communicating
about
privacy:
Towards
people-centered
and
account-
able
design.
https://about.fb.com/wp-content/uploads/2020/07/
Privacy-Transparency-White-Paper.pdf, July 2020.
Google.
Google security whitepaper.
https://services.google.com/fh/files/
misc/google_security_wp.pdf, Jan 2019.
Mika Juuti, Sebastian Szyller, Samuel Marchal, and N Asokan. Prada: protecting against dnn model
stealing attacks. In Proc. IEEE EuroS&P, pp. 512â€“527. IEEE, 2019.
Kaggle. Life expectancy dataset. https://tinyurl.com/yxgaa4go, 2020.
Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam
Smith. What can we learn privately? SIAM J. Comput., 40(3):793â€“826, 2011.
Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, and Sameep Mehta. Model extraction warning
in mlaas paradigm. In Proc. ACSAC, pp. 371â€“380, 2018.
Jakub Konevcny, H Brendan McMahan, Felix X Yu, Peter RichtÂ´arik, Ananda Theertha Suresh, and
Dave Bacon.
Federated learning: Strategies for improving communication efï¬ciency.
arXiv
preprint arXiv:1610.05492, 2016.
Taesung Lee, Benjamin Edwards, Ian Molloy, and Dong Su. Defending against machine learning
model stealing attacks using deceptive perturbations. arXiv preprint arXiv:1806.00054, 2018.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efï¬cient learning of deep networks from decentralized data. In Proc. AISTATS,
pp. 1273â€“1282. PMLR, 2017.
Smitha Milli, Ludwig Schmidt, Anca D Dragan, and Moritz Hardt. Model reconstruction from
model explanations. In Proc. ACM FAccT, pp. 1â€“9, 2019.
Nina Narodytska and Shiva Kasiviswanathan. Simple black-box adversarial attacks on deep neural
networks. In Proc. CVPRW, pp. 1310â€“1318. IEEE, 2017.
Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses
against dnn model stealing attacks. In Proc. ICLR, 2019.
10

Published as a conference paper at ICLR 2021
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016a.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against deep learning systems using adversarial examples.
arXiv preprint arXiv:1602.02697, 1(2):3, 2016b.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proc. ASIA CCS, pp. 506â€“519,
2017.
Anand Rajaraman and Jeffrey David Ullman. Mining of massive datasets. Cambridge University
Press, 2011.
Mauro Ribeiro, Katarina Grolinger, and Miriam AM Capretz. Mlaas: Machine learning as a service.
In ICMLA, pp. 896â€“902. IEEE, 2015.
Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic black-box end-to-end attack
against rnns and other API calls based malware classiï¬ers. arXiv preprint arXiv:1707.05970,
2017.
Scikit-learn. The R2 score. https://tinyurl.com/yy8m3u3d, 2020a.
Scikit-learn.
Breast cancer wisconsin dataset.
https://scikit-learn.org/stable/
modules/generated/sklearn.datasets.load_breast_cancer.html, 2020b.
Scikit-learn.
Moons dataset.
https://scikit-learn.org/stable/modules/
generated/sklearn.datasets.make_moons.html, 2020c.
Scikit-learn. The 20 newsgroups text dataset. https://tinyurl.com/y26m6dvw, 2020d.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proc. CCS, pp. 1310â€“1321.
ACM, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In IEEE Symp. Sec. Priv., pp. 3â€“18. IEEE, 2017.
Meng Sun, Wee Peng Tay, and Xin He. Towards information privacy for the internet of things. arXiv
preprint arXiv:1611.04254, 2016.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
Proc. ITW, pp. 1â€“5. IEEE, 2015.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Florian Tram`er, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine
learning models via prediction apis. In USENIX, pp. 601â€“618, 2016.
Paul Voigt and Axel Von dem Bussche. The EU general data protection regulation (GDPR). A
Practical Guide, 1st Ed., Cham: Springer International Publishing, 2017.
Xun Xian, Xinran Wang, Jie Ding, and Reza Ghanadan. Assisted learning: a framework for multi-
organization learning. NeurIPS 2020 (spotlight), 2020.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Trans. Intell. Syst. Technol., 10(2):1â€“19, 2019.
Andrew C Yao. Protocols for secure computations. In Proc. SFCS, pp. 160â€“164. IEEE, 1982.
11

Published as a conference paper at ICLR 2021
A
APPENDIX
A.1
SPECIAL CASES: DETERMINISTIC MODEL Kâˆ—
In this case, Theorem 1 implies the following corollary. We will use this result in later sections.
Corollary 2 The optimal solution of (1) satisï¬es the following equations.
pK1(Ëœx | x) = Îºxp Ëœ
X(Ëœx) exp
 1
Î²1
pK2(f(x) | f(Ëœx))
P
Ëœxâ€² pK2(f(x) | f(Ëœxâ€²))pK1(Ëœxâ€² | x)
âˆ’Î²2
Î²1
EY | ËœY =f(Ëœx) log pK2(Y | f(Ëœx))
pY (Y )

,
pK2(y | Ëœy) = Ï„ËœypY (y) exp

1
Î²2p ËœY (Ëœy)
X
x:f(x)=y
pX(x)pK1â—¦Kâˆ—(Ëœy | x)
pK(y | x)

,
where Îºx and Ï„Ëœy are normalizing constants implicitly deï¬ned so that the conditional density function
integrates to one.
A.2
INFORMATION LAUNDERING OF THE INPUT (X) ONLY
Suppose that K2 is an identity map and let Î²2 = 0 so that we only maneuver the input data (Fig-
ure 2a). Then the optimization problem (1) reduces to minimizing
L(pK1)
âˆ†= EXâˆ¼pXDKL(pKâˆ—(Â· | X), pK(Â· | X)) + Î²1I(X; ËœX).
(10)
Corollary 3 The optimal solution of (10) satisï¬es the following equations.
pK1(Ëœx | x) = Îºxp Ëœ
X(Ëœx) exp
 1
Î²1
EY |X=xâˆ¼pKâˆ—
pKâˆ—(Y | ËœX = Ëœx)
pK(Y | X = x)

,
(11)
where Îºx is an implicitly deï¬ned normalizing constant. In particular, if Kâˆ—is deterministic, equation
(11) becomes
pK1(Ëœx | x) = Îºxp Ëœ
X(Ëœx) exp

1f(x)=f(Ëœx)
Î²1
P
Ëœxâ€²:f(x)=f(Ëœxâ€²) pK1(Ëœxâ€² | x)

.
(12)
As we can see from Corollaries 1 and 3, for a deterministic Kâˆ—(represented by f), the simpliï¬ed
equation of (8) is similar to that of (12). The subtle difference that one has a sum while the other
does not is because f may not be a one-to-one mapping.
A.3
PROOFS
Proof 1 (Proof of Theorem 1) Introducing Lagrange multipliers, Î»1(x) for the normalization of
the conditional distributions pK1(Â· | x) at each x, Î»2(Ëœy) for the normalization of the conditional
distributions pK2(Â· | Ëœy) at each Ëœy. The Lagrangian of (1) can be written as
L = âˆ’
X
x,y
pX(x)pKâˆ—(y | x) log pK(y | x) + Î²1
X
x,Ëœx
pX(x)pK1(Ëœx | x) log pK1(Ëœx | x)
p Ëœ
X(Ëœx)
+ Î²2
X
Ëœy,y
p ËœY (Ëœy)pK2(y | Ëœy) log pK2(y | Ëœy)
pY (y)
+
X
x
Î»1(x)pK1(Ëœx | x) +
X
Ëœy
Î»2(Ëœy)pK2(y | Ëœy) + c
= A1 + A2 + A3 + A4 + A5 + c
(13)
up to an additive constant c that is determined by the known pX and pKâˆ—.
12

Published as a conference paper at ICLR 2021
It can be veriï¬ed that
âˆ‚pK(y | x)
pK1(Ëœx | x) = pKâˆ—â—¦K2(y | Ëœx)
(14)
âˆ‚p Ëœ
X(Ëœx)
pK1(Ëœx | x) = pX(x)
(15)
âˆ‚p ËœY (Ëœy)
pK1(Ëœx | x) = pX(x)pKâˆ—(Ëœy | Ëœx)
(16)
âˆ‚pY (y)
pK1(Ëœx | x) = pX(x)pKâˆ—â—¦K2(y | Ëœx).
(17)
Using (14)-(17), for a given x and Ëœx, we calculate the derivatives of each term in (13) with respect
to pK1(Ëœx | x) to be
âˆ‚A1
pK1(Ëœx | x) = âˆ’pX(x)
X
y
pKâˆ—(y | x)pKâˆ—â—¦K2(y | Ëœx)
pK(y | x)
(18)
âˆ‚A2
pK1(Ëœx | x) = Î²1pX(x) log pK1(Ëœx | x)
p Ëœ
X(Ëœx)
(19)
âˆ‚A3
pK1(Ëœx | x) = Î²2pX(x)
X
Ëœy,y
pKâˆ—â—¦K2(Ëœy, y | ËœX = Ëœx) log pK2(y | Ëœy)
pY (y)
âˆ’Î²2pX(x)
X
Ëœy,y
p ËœY (Ëœy)pK2(y | Ëœy)pKâˆ—â—¦K2(y | Ëœx)
pY (y)
= Î²2pX(x)
X
Ëœy,y
pKâˆ—â—¦K2(Ëœy, y | ËœX = Ëœx) log pK2(y | Ëœy)
pY (y)
âˆ’Î²2pX(x)
(20)
âˆ‚A4
pK1(Ëœx | x) = Î»1(x)
(21)
âˆ‚A5
pK1(Ëœx | x) = 0
(22)
Taking equations (18)-(22) into (13), we obtain the ï¬rst-order equation
âˆ‚L
âˆ‚pK1(Ëœx | x) = pX(x)

âˆ’EY |X=xâˆ¼pKâˆ—
pKâˆ—â—¦K2(Y | ËœX = Ëœx)
pK(Y | X = x)
+ Î²1 log pK1(Ëœx | x)
p Ëœ
X(Ëœx)
+ Î²2E ËœY ,Y | Ëœ
X=Ëœx log pK2(Y | ËœY )
pY (Y )
+ ËœÎ»1(x)

= 0,
(23)
where ËœÎ»(x) = Î»1(x)/pX(x) âˆ’Î²2. Rearranging the terms in Equation (23), we obtain
log pK1(Ëœx | x)
p Ëœ
X(Ëœx)
= 1
Î²1

âˆ’ËœÎ»1(x) + EY |X=xâˆ¼pKâˆ—
pKâˆ—â—¦K2(Y | ËœX = Ëœx)
pK(Y | X = x)
âˆ’Î²2E ËœY ,Y | Ëœ
X=Ëœx log pK2(Y | ËœY )
pY (Y )

which implies Equation (2).
Similarly, taking derivatives with respect to pK2(y | Ëœy) for given Ëœy and y, it can be veriï¬ed that
âˆ‚pK(y | x)
âˆ‚pK2(y | Ëœy) = pK1â—¦Kâˆ—(Ëœy | x)
âˆ‚L
âˆ‚pK2(y | Ëœy) = âˆ’
X
x
pX(x)pKâˆ—(y | x)pK1â—¦Kâˆ—(Ëœy | x)
pK(y | x)
+ Î²2p ËœY (Ëœy) log pK2(y | Ëœy)
pY (y)
+ Î»2(Ëœy)
= âˆ’EXâˆ¼pX
pKâˆ—(y | X) Â· pK1â—¦Kâˆ—(Ëœy | X)
pK(y | X)
+ Î²2p ËœY (Ëœy) log pK2(y | Ëœy)
pY (y)
+ Î»2(Ëœy). (24)
Letting Equation (24) be zero and rearranging it, we obtain Equation (3).
13

Published as a conference paper at ICLR 2021
Proof 2 (Proof of Theorem 2) We
deï¬ne
the
following
functional
of
four
variables:
pK1, pK2, h1, h2,
J(pK1, pK2, h1, h2) = âˆ’
X
x,y
pX(x)pKâˆ—(y | x) log pK(y | x)
(25)
+ Î²1
X
x,Ëœx
pX(x)pK1(Ëœx | x) log pK1(Ëœx | x)
h1(Ëœx)
+ Î²2
X
Ëœy,y
p ËœY (Ëœy)pK2(y | Ëœy) log pK2(y | Ëœy)
h2(y)
.
(26)
We will use the following known result (Cover, 1999, Lemma 10.8.1). Suppose that X and Y have a
joint distribution with density pXY , and the marginal densities are pX, pY , respectively. Then a den-
sity function rY of y that minimizes the KL-divergence D(pXY , pXrY ) is the marginal distribution
pY . This result implies that minimizing the objective function in (1) can be written as a quadruple
minimization
min
pK1,pK2,h1,h2 J(pK1, pK2, h1, h2).
(27)
It can be veriï¬ed from (23) and its preceding identities that
âˆ‚2J
âˆ‚pK1(Ëœx | x)2 = pX(x)EY |X=xâˆ¼pKâˆ—
pKâˆ—â—¦K2(Y | Ëœx)
pK(Y | x)2
âˆ‚pK(Y | x)
âˆ‚pK1(Ëœx | x) + Î²1
pX(x)
pK1(Ëœx | x)
= pX(x)EY |X=xâˆ¼pKâˆ—
pKâˆ—â—¦K2(Y | Ëœx)2
pK(Y | x)2
+ Î²1
pX(x)
pK1(Ëœx | x)
(28)
âˆ‚2J
âˆ‚pK2(y | Ëœy)2 = EXâˆ¼pX
pKâˆ—(y | X) Â· pK1â—¦Kâˆ—(Ëœy | X)
pK(y | X)2
âˆ‚pK(y | X)
âˆ‚pK2(y | Ëœy) + Î²2
p ËœY (Ëœy)
pK2(y | Ëœy)
(29)
= EXâˆ¼pX
pKâˆ—(y | X) Â· pK1â—¦Kâˆ—(Ëœy | X)2
pK(y | X)2
+ Î²2
p ËœY (Ëœy)
pK2(y | Ëœy)
(30)
âˆ‚2J
âˆ‚h1(Ëœx)2 = Î²1
X
x
pX(x)pK1(Ëœx | x)
h1(Ëœx)2
(31)
âˆ‚2J
âˆ‚h2(y)2 = Î²2
X
Ëœy
p ËœY (Ëœy)pK2(y | Ëœy)
h2(y)2
(32)
Thus, J(pK1, pK2, h1, h2) is convex in each of the variables.
We begin with a choice of initial pK2, h1, h2, and calculate the pK1 that minimizes the objective.
Using the method of Lagrange multipliers for this minimization (in a way similar to (13)), we obtain
the solution of pK1 shown in the ï¬rst equation of Line 3, Algorithm 1. Similarly, we obtain the
second equation in Algorithm 1. For the conditional distributions pK1 and pK2, we then calculate
the marginal distributions h1 (of Ëœx) that minimizes (26). Note that the terms of (26) involving h1
may be rewritten as
Î²1
X
x,Ëœx
p(x, Ëœx) log
p(x, Ëœx)
p(x)h1(Ëœx)
which, by the aforementioned lemma, is minimized by the third equation of Line 3, Algorithm 1.
Similar arguments apply for h2. Consequently, each iteration step in Algorithm 1 reduces J. By
the non-negativeness of KL-divergence, J + c â‰¥L â‰¥0, where L is in (1) and c is introduced
in (13). Therefore, J has a lower bound, and the algorithm will converge to a minimum. Note
that J(pK1, pK2, h1, h2) is convex in each of the variables independently but not in the variablesâ€™
product space. The current proof does not imply the convergence to a global minimum.
Proof 3 (Proof of Theorem 3) Similar to the technique used in the above proof of Theorem 2, we
cast the optimization problem in (6) as a double minimization with respect to (pK2, h2),
J(pK2, h2)
âˆ†= âˆ’
X
x,y
pX(x)pKâˆ—(y | x) log pK(y | x) + Î²2
X
Ëœy,y
p ËœY (Ëœy)pK2(y | Ëœy) log pK2(y | Ëœy)
h2(y)
.
14

Published as a conference paper at ICLR 2021
We only need to check that J is strongly convex in its arguments. Direct calculations show that
âˆ‚2J
âˆ‚pK2(y | Ëœy)2 =
X
x
pX(x)pKâˆ—(y | x)p2
Kâˆ—(Ëœy | x)
p2
K(y | x) + Î²2
p ËœY (Ëœy)
pK2(y | Ëœy)
=
X
x:f(x)=y,y=Ëœy
pX(x)
1
p2
K(y | x) + Î²2
p ËœY (Ëœy)
pK2(y | Ëœy)
=
p ËœY (Ëœy)
p2
K2(y | Ëœy)1y=Ëœy + Î²2
p ËœY (Ëœy)
pK2(y | Ëœy)
âˆ‚2J
âˆ‚h2(y)2 = Î²2
pY (y)
h2(y)2
âˆ‚2L
âˆ‚pK2(y | Ëœy)âˆ‚h2(y) = âˆ’Î²2
p ËœY (Ëœy)
h2(y) .
The above equations indicate that the determinant of the Hessian satisï¬es
âˆ‚2J
âˆ‚pK2(y | Ëœy)2 Â·
âˆ‚2J
âˆ‚h2(y)2 âˆ’

âˆ‚2L
âˆ‚pK2(y | Ëœy)âˆ‚h2(y)
2
= Î²2
p ËœY (Ëœy)pY (y)
pK2(y | Ëœy)h2(y)2 1y=Ëœy + Î²2
2
p ËœY (Ëœy)pY (y)
pK2(y | Ëœy)h2(y)2

1 âˆ’
p ËœY ,Y (Ëœy, y)
pY (y)

,
which further implies the convexity of J in the product space of pK2 and h2.
A.4
VISUALIZATION OF ALGORITHM 2
We provide a toy example to visualize Algorithm 2. In the simulation, we choose an alphabet of
size 100, and p ËœY as described by r âˆˆ[0, 1]a is uniform-randomly generated from the probability
simplex. We independently replicate the experiment 50 times, each time running Algorithm 2 for 30
iterations, and calculate the average of the following results. First, we record âˆ¥P (t+1) âˆ’P (t)âˆ¥1/a
at each iteration t, which traces the convergence of the estimated transition probabilities. Second,
we record the ï¬nal transition probability matrix into a heat-map where Py,Ëœy means the estimated
pK2(y | Ëœy). The experiments are performed for Î² = 100, 10, 1, corresponding to columns 1-3. The
plots indicate the convergence of the algorithm, though the rate of convergence depends on Î². They
also imply the expected result that a small Î² induces an identity transition while a large Î² induces
ËœY that is nearly independent with Y .
A.5
DATA STUDY: NEWS TEXT CLASSIFICATION
In this experimental study, we use the â€˜20-newsgroupsâ€™ dataset provided by scikit-learn open-
source library (Scikit-learn, 2020d), which comprises news texts on various topics.
The ex-
periment is intended to illustrate the utility-privacy tradeoff and the optimality of our proposed
solution compared with other methods.
For better visualization we pick up the ï¬rst four top-
ics (in alphabetic order), which are â€˜alt.atheismâ€™, â€˜comp.graphicsâ€™, â€˜comp.os.ms-windows.miscâ€™,
â€˜comp.sys.ibm.pc.hardwareâ€™. Suppose that the service Alice provides is to perform text-based clus-
tering, which takes text data as input and returns one of the four categories (denoted by 0, 1, 2, 3)
as output. The texts are transformed into vectors of numerical values using the technique of term
frequency-inverse document frequency (TF-IDF) (Rajaraman & Ullman, 2011). In the transforma-
tion, metadata such as headers, signature blocks, and quotation blocks are removed. To evaluate the
out-sample utility, we split the data into two parts using the default option provided in (Scikit-learn,
2020d), which results in a training part (2245 samples, 49914 features) and a testing part (1494
samples, 49914 features). The above split between the training and testing is based upon messages
posted before and after a speciï¬c date.
Alice trains a classiï¬er using the Naive Bayes method and records the frequency of observing each
category [0.220.270.210.30] (r in Algorithm 2). Then, Alice runs the OIL-Y Algorithm (under a
given Î²2) to obtain the transition probability matrix P âˆˆ[0, 1]4Ã—4. In other words, the effective
system provided by Alice is the cascade of the learned classiï¬er, and P determines the Markov
15

Published as a conference paper at ICLR 2021
10
20
30
Iteration
2.3
2.2
2.1
2.0
1.9
1.8
1.7
1.6
1.5
Error (in log scale)
(a1)
0
20
40
60
80
Y
0
20
40
60
80
Y
(a2)
0.0
0.2
0.4
0.6
0.8
1.0
10
20
30
Iteration
7
6
5
4
3
2
1
0
Error (in log scale)
(b1)
0
20
40
60
80
Y
0
20
40
60
80
Y
(b2)
0.0
0.2
0.4
0.6
0.8
1.0
10
20
30
Iteration
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
Error (in log scale)
(c1)
0
20
40
60
80
Y
0
20
40
60
80
Y
(c2)
0.0
0.2
0.4
0.6
0.8
1.0
Figure 3: Visualization of Algorithm 2 in terms of the convergence (row 1) and the ï¬nal transition
probabilities (row 2), for Î² = 100, 10, 1 (corresponding to three columns).
transition. Aliceâ€™s resulting out-sample performance from the testing data is recorded in Figure 4a,
where we considered different Î²â€™s summarized in Table 1. As we expected, a larger value of Î²2 cuts
off more information propagated from ËœY to Y , resulting in a degraded out-sample performance of
Aliceâ€™s effective system.
We also visualize the model privacy-utility tradeoff by the following procedure. First, we approxi-
mate the utility that quantiï¬es the useful information conveyed by Alice. With Aliceâ€™s trained model
and the optimally laundered Y (from training data), we retrain another Naive Bayes classiï¬er and
generate predictions on the testing data, denoted by ypred
K . Meanwhile, we apply Aliceâ€™s authentic
model to generate predictions on the testing data, denoted by ypred
Kâˆ—. We approximate the model util-
ity as the accuracy measure between ypred
K
and ypred
Kâˆ—. The model utility can be approximated by other
measures. We also considered retraining methods such as tree-based classiï¬ers and average F1-
score in computing the model utility, and the results are consistent in the data experiments. Second,
we approximate the privacy leakage as Aliceâ€™s prediction accuracy on the testing data. Intuitively
speaking, for a given utility, larger out-sample prediction accuracy indicates less information laun-
dered, indicating a higher privacy leakage of Aliceâ€™s internal model. We plot the model leakage
against utility obtained from our proposed solution in Figure 4b.
For comparison, we considered a benchmark method described below. The conditional probability
mass function pK2(Â· | Ëœy) given each Ëœy is independently drawn from a Dirichlet distribution with
parameters [b, . . . , b, a, b, . . . , b], where a is the Ëœyth entry. An interpretation of the parameter is that
a larger a/b favors a larger probability mass at y = Ëœy (and thus less noise). We consider different
pairs of (a, b) so that the tradeoff curve matches the counterpart curve from our proposed method.
The curve is averaged over 50 independent replications. As shown in Figure 4b, the results indicate
that our proposed solution produces less leakage (and thus better privacy) for a given utility.
We also plot heatmaps illustrating the transition laws pK2(y | Ëœy) obtained from the proposed infor-
mation laundering in Figure 5. We considered two cases, where there are 20% class-0 labels, and
where there are 1% class-0 labels (by removing related samples from the original dataset). Intu-
itively, once we reduce the size of class-0 data in (b), the transition probabilities pK2(0 | Ëœy) for each
Ëœy should be smaller compared with those in (a) as class-0 is no longer â€˜importantâ€™. Our expectation
16

Published as a conference paper at ICLR 2021
Table 1: Summary of the tradeoff parameters used for the OIL-Y algorithm and random bench-
mark from Dirichlet distributions (averaged over 50 independent replications), and the correspond-
ing model utility (as evaluated by the closeness of Aliceâ€™s authentic and effective systems), as well
as the model privacy leakage (as evaluated by Aliceâ€™s out-sample accuracy).
Proposed
Î²
0
1
2
5
20
50
Utility
1.00
0.86
0.78
0.68
0.46
0.30
Leakage
0.79
0.64
0.53
0.45
0.35
0.30
Random
Benchmark
a, b
100, 1
20, 1
10, 1
5, 2
5, 3
10, 10
Utility
0.96
0.88
0.79
0.49
0.39
0.23
Leakage
0.77
0.70
0.62
0.40
0.34
0.27
0
10
20
30
40
50
2
0.3
0.4
0.5
0.6
0.7
0.8
Accuracy
(a) Performance curve (of Alice)
0.2
0.4
0.6
0.8
1.0
Utility
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Model leakage
(b) Tradeoff curve
Proposed
Random
Figure 4: Visualization of (a) Aliceâ€™s out-sample performance against the tradeoff parameter Î²2
in Information Laundering, and (b) Aliceâ€™s model utility-privacy tradeoffs under the information
laundering technique and the random benchmark using Dirichlet-generated transition laws. Detailed
parameters are summarized in Table 1.
is aligned with Figure 5, where the ï¬rst row in (b) are indicated by darker colors compared with that
in (a), meaning that the class-0 is less likely to be observed.
A.6
DATA STUDY: LIFE EXPECTANCY REGRESSION
In this experimental study, we use the â€˜life expectancyâ€™ dataset provided by kaggle open-source
data (Kaggle, 2020), originally collected from the World Health Organization (WHO). The data
was collected from 193 countries from 2000 to 2015, and Aliceâ€™s model is a linear regression that
predicts life expectancy using potential factors such as demographic variables, immunization factors,
and mortality rates. This experiment is intended to illustrate the utility-privacy tradeoff and our
proposed solution in regression contexts.
In the regression model, we quantize the output alphabet Y by 30 points equally-spaced in between
Âµ Â± 3Ïƒ, where Âµ, Ïƒ represent the mean and the standard deviation of Y in the training data. We
then applied a similar procedure as in Subsection A.6, except that we use the empirical R2 score
as the underlying measure of utility and leakage. The empirical R2 score has been commonly
used for evaluating regression performance, and it can be negative, meaning that the predictive
performance is worse than sample mean-based prediction (Scikit-learn, 2020a). In particular, we
obtain tradeoff curves in Figure 6, where we compared the information laundering results based
on the proposed technique and Dirichlet-based technique (similar to that in Subsection A.6). The
different Î²â€™s and Dirichlet parameters are summarized in Table 2. The detailed performance values
are also summarized in Table 2.
17

Published as a conference paper at ICLR 2021
Y
Y
(a) 20% class-0
0.0
0.2
0.4
0.6
0.8
1.0
Y
Y
(b) 1% class-0
0.0
0.2
0.4
0.6
0.8
1.0
Figure 5: Heatmap showing the transition law pK2(y | Ëœy) for information laundering, under (a) 20%
of class-0 labels, and (b) 1% of class-0 labels. In contrast with the case (a), the class-0 is negligible
in (b) and thus the transition probabilities pK2(0 | Ëœy) for each Ëœy becomes smaller (as indicated by
darker colors).
Table 2: Summary of the tradeoff parameters used for the OIL-Y algorithm and random bench-
mark from Dirichlet distributions (averaged over 50 independent replications), and the correspond-
ing model utility (as evaluated by the closeness of Aliceâ€™s authentic and effective systems), as well
as the model privacy leakage (as evaluated by Aliceâ€™s out-sample accuracy). The underlying metric
used is the empirical R2, which can be less than zero.
Proposed
Î²
0
1
2
5
8
20
Utility
0.99
0.92
0.84
0.62
0.48
0.35
Leakage
0.79
0.42
0.09
âˆ’0.26
âˆ’0.45
âˆ’0.51
Random
Benchmark
(a, b)
10000, 1
200, 5
100, 5
100, 8
100, 10
100, 20
Utility
0.99
0.77
0.58
0.42
0.36
0.15
Leakage
0.78
0.10
âˆ’0.07
âˆ’0.15
âˆ’0.17
âˆ’0.22
To illustrate the impact of tradeoffs, we considered two cases corresponding to Î²2 = 1 and Î²2 = 20.
We compute the transition laws pK2(y | Ëœy) obtained from Algorithm 2 and illustrate them in the
ï¬rst row of Figure 5. We also take the snapshot at the year ËœY = 69 and plot the conditional density
function pK2(Â· | ËœY = 69) (as approximated by the quantizers) in the second row of Figure 5. The
visualized results are aligned with our expectation that a larger penalty of model leakage will cause
a more dispersed transition law.
A.7
DATA STUDY: MITIGATION OF ADVERSARIAL ATTACKS
In this experimental study, we demonstrate the use of information laundering in mitigating two
model extraction attacks studied in (Tram`er et al., 2016). We consider settings where the target
model is a classiï¬er. The ï¬rst attack is the Retraining attack. The adversary Bob sends random
queries and receives class labels to train a local model. Bob does not need to know Aliceâ€™s model
architecture. The second attack is the equation-solving attack. Aliceâ€™s model is assumed to be a
Logistic classiï¬er, and Bob knows about it. Bob sends random queries in this attack and receives
class probabilities (a vector on a simplex). Bob then solves a linear equation to obtain the coefï¬cients
of Aliceâ€™s Logistic model. We note that there exist more types of attacks suitable for various speciï¬c
settings (see., e.g., Tram`er et al., 2016; Juuti et al., 2019).
The experiments in this section indicate the following two points. First, an adversary needs more
samples to achieve the same utility under the information laundering (Figure 8). Second, the ef-
fects of attack and laundering depend on the particular models speciï¬ed by the adversary and target
(Figure 9).
18

Published as a conference paper at ICLR 2021
0
5
10
15
20
2
0.4
0.2
0.0
0.2
0.4
0.6
0.8
R2
(a) Performance curve (of Alice)
0.2
0.4
0.6
0.8
1.0
Utility
0.4
0.2
0.0
0.2
0.4
0.6
0.8
Model leakage
(b) Tradeoff curve
Proposed
Random
Figure 6: Visualization of (a) Aliceâ€™s out-sample performance against the tradeoff parameter Î²2
in Information Laundering, and (b) Aliceâ€™s model utility-privacy tradeoffs under the information
laundering technique and the random benchmark using Dirichlet-generated transition laws. Detailed
parameters are summarized in Table 2.
40
60
80
Y
40
60
80
Y
(a) 
2 = 1
0.0
0.2
0.4
0.6
0.8
1.0
40
60
80
Y
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
P(Y
Y = 69)
40
60
80
Y
40
60
80
Y
(b) 
2 = 20
0.0
0.2
0.4
0.6
0.8
1.0
40
60
80
Y
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
P(Y
Y = 69)
Figure 7: Heatmap (row 1) showing the transition laws optimized from information laundering,
under (a) Î²2 = 1, and (b) Î²2 = 20. The snapshots of probability mass functions of Y conditional
on ËœY = 69 are also visualized (row 2).
The experimental details are given below. In Figure 8(a), Alice uses half of the Breast Cancer
dataset (Scikit-learn, 2020b) (standardized) to train a Logistic classiï¬cation model. Bob queries
the class labels with standard Gaussian random input and locally trains another Logistic classiï¬er.
In responding to Bob, Alice employs different levels of laundering. The utility is deï¬ned as the
19

Published as a conference paper at ICLR 2021
0
10
20
30
40
50
2
0.60
0.65
0.70
0.75
0.80
0.85
0.90
Utility
(a) Retraining attacks
n = 100
n = 300
n = 1000
0
10
20
30
40
50
2
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Utility
(b) Equation-Solving attacks
n = 100
n = 300
n = 1000
Figure 8: An adversaryâ€™s utility against the laundering parameter Î²2 in the contexts of (a) Retraining
attack with random queries, and (b) Equation-Solving attack. In each plot, the three curves represent
query sizes n = 100, 300, 1000, respectively.
0
10
20
30
40
50
2
0.5
0.6
0.7
0.8
0.9
1.0
Utility
(a) RandomForest against RandomForest
n = 100
n = 300
n = 1000
0
10
20
30
40
50
2
0.5
0.6
0.7
0.8
0.9
1.0
Utility
(b) Logistic against RandomForest
n = 100
n = 300
n = 1000
Figure 9: An adversaryâ€™s utility against the laundering parameter Î²2 in the contexts of Retraining
attacks, where (a) both the adversary and the target use the Random Forest model, and (b) the
adversary uses the Logistic model while the target uses the Random Forest. In each plot, the three
curves represent query sizes n = 100, 300, 1000, respectively.
percentage of agreement of Aliceâ€™s and Bobâ€™s models tested on the other half of the dataset. In
Figure 8(b), Bob sends the same random queries and solves a linear equation to estimate Aliceâ€™s
Logistic coefï¬cients. From the results, we can see that Bobâ€™s performance is better than that in
Figure 8(a). It is mainly due to the substantial side information and the least-squares estimate of
Bob.
In Figure 9, Alice used half of the simulated Moons dataset (Scikit-learn, 2020c) (with 1000 sam-
ples, 0.1 standard deviation for the noise) to train a Random Forest model. The other half of the
data are reserved to evaluate Bobâ€™s utility. Suppose that Bob uses the above Retraining attack, but
with two different models. In Figure 9(a), Bob uses the Random Forest classiï¬er, which has the
expressive power to extract Aliceâ€™s model. In Figure 9(b), Bob uses the Logistic classiï¬er, which
has a linear decision boundary. Bobâ€™s model in (b) is inadequate because the Moons data is not
linearly separable. Figure 9 shows that the inï¬‚uence of information laundering on the adversarial
query size needed to maintain the same utility largely depends on Bobâ€™s model choices. Also, Bobâ€™s
inadequate model may show more robustness against laundered information even though it does not
perform satisfactorily on cleaned data.
20

