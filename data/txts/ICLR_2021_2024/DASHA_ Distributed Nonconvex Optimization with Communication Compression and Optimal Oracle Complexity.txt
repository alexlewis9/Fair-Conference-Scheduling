Published as a conference paper at ICLR 2023
DASHA: DISTRIBUTED NONCONVEX OPTIMIZATION
WITH COMMUNICATION COMPRESSION AND OPTIMAL
ORACLE COMPLEXITY
Alexander Tyurin
KAUST
Saudi Arabia
alexandertiurin@gmail.com
Peter RichtÃ¡rik
KAUST
Saudi Arabia
richtarik@gmail.com
ABSTRACT
We develop and analyze DASHA: a new family of methods for nonconvex dis-
tributed optimization problems. When the local functions at the nodes have a
ï¬nite-sum or an expectation form, our new methods, DASHA-PAGE, DASHA-MVR
and DASHA-SYNC-MVR, improve the theoretical oracle and communication com-
plexity of the previous state-of-the-art method MARINA by Gorbunov et al. (2020).
In particular, to achieve an Îµ-stationary point, and considering the random sparsiï¬er
RandK as an example, our methods compute the optimal number of gradients
O (
âˆšm/Îµâˆšn) and O (Ïƒ/Îµ
3/2n) in ï¬nite-sum and expectation form cases, respectively,
while maintaining the SOTA communication complexity O (d/Îµâˆšn). Furthermore,
unlike MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send
compressed vectors only, which makes them more practical for federated learning.
We extend our results to the case when the functions satisfy the Polyak-Åojasiewicz
condition. Finally, our theory is corroborated in practice: we see a signiï¬cant
improvement in experiments with nonconvex classiï¬cation and training of deep
learning models.
1
INTRODUCTION
Nonconvex optimization problems are widespread in modern machine learning tasks, especially with
the rise of the popularity of deep neural networks (Goodfellow et al., 2016). In the past years, the
dimensionality of such problems has increased because this leads to better quality (Brown et al.,
2020) and robustness (Bubeck & Sellke, 2021) of the deep neural networks trained this way. Such
huge-dimensional nonconvex problems need special treatment and efï¬cient optimization methods
(Danilova et al., 2020).
Because of their high dimensionality, training such models is a computationally intensive undertaking
that requires massive training datasets (Hestness et al., 2017), and parallelization among several
compute nodes1 (Ramesh et al., 2021). Also, the distributed learning paradigm is a necessity in
federated learning (KoneË‡cnÃ½ et al., 2016), where, among other things, there is an explicit desire to
secure the private data of each client.
Unlike in the case of classical optimization problems, where the performance of algorithms is deï¬ned
by their computational complexity (Nesterov, 2018), distributed optimization algorithms are typically
measured in terms of the communication overhead between the nodes since such communication
is often the bottleneck in practice (KoneË‡cnÃ½ et al., 2016; Wang et al., 2021). Many approaches
tackle the problem, including managing communication delays (Vogels et al., 2021), ï¬ghting with
stragglers (Li et al., 2020a), and optimization over time-varying directed graphs (NediÂ´c & Olshevsky,
2014). Another popular way to alleviate the communication bottleneck is to use lossy compression
of communicated messages (Alistarh et al., 2017; Mishchenko et al., 2019; Gorbunov et al., 2021;
Szlendak et al., 2021). In this paper, we focus on this last approach.
1Alternatively, we sometimes use the terms: machines, workers and clients.
1

Published as a conference paper at ICLR 2023
1.1
PROBLEM FORMULATION
In this work, we consider the optimization problem
min
xâˆˆRd
(
f(x) := 1
n
n
X
i=1
fi(x)
)
,
(1)
where fi : Rd â†’R is a smooth nonconvex function for all i âˆˆ[n] := {1, . . . , n}. Moreover, we
assume that the problem is solved by n compute nodes, with the ith node having access to function fi
only, via an oracle. Communication is facilitated by an orchestrating server able to communicate
with all nodes. Our goal is to ï¬nd an Îµ-solution (Îµ-stationary point) of (1): a (possibly random) point
bx âˆˆRd, such that E
h
âˆ¥âˆ‡f(bx)âˆ¥2i
â‰¤Îµ.
1.2
GRADIENT ORACLES
We consider all of the following structural assumptions about the functions {fi}n
i=1, each with its
own natural gradient oracle:
1. Gradient Setting. The ith node has access to the gradient âˆ‡fi : Rd â†’Rd of function fi.
2. Finite-Sum Setting. The functions {fi}n
i=1 have the ï¬nite-sum form
fi(x) = 1
m
m
X
j=1
fij(x),
âˆ€i âˆˆ[n],
(2)
where fij : Rd â†’R is a smooth nonconvex function for all j âˆˆ[m]. For all i âˆˆ[n], the ith node has
access to a mini-batch of B gradients, 1
B
P
jâˆˆIi âˆ‡fij(Â·), where Ii is a multi-set of i.i.d. samples of
the set [m], and |Ii| = B.
3. Stochastic Setting. The function fi is an expectation of a stochastic function,
fi(x) = EÎ¾ [fi(x; Î¾)] ,
âˆ€i âˆˆ[n],
(3)
where fi : Rd Ã— â„¦Î¾ â†’R. For a ï¬xed x âˆˆR, fi(x; Î¾) is a random variable over some distribution
Di, and, for a ï¬xed Î¾ âˆˆâ„¦Î¾, fi(x; Î¾) is a smooth nonconvex function. The ith node has access to a
mini-batch of B stochastic gradients 1
B
PB
j=1 âˆ‡fi(Â·; Î¾ij) of the function fi through the distribution
Di, where {Î¾ij}B
j=1 is a collection of i.i.d. samples from Di.
1.3
ORACLE COMPLEXITY
In this paper, the oracle complexity of a method is the number of (stochastic) gradient calculations
per node to achieve an Îµ-solution. Every considered method performs some number T of communi-
cations rounds to get an Îµ-solution; thus, if every node (on average) calculates B gradients in each
communication round, then the oracle complexity equals O (Binit + BT) , where Binit is the number
of gradient calculations in the initialization phase of a method.
1.4
UNBIASED COMPRESSORS
The method proposed in this paper is based on unbiased compressors â€“ a family of stochastic
mappings with special properties that we deï¬ne now.
Deï¬nition 1.1. A stochastic mapping C : Rd â†’Rd is an unbiased compressor if there exists Ï‰ âˆˆR
such that
E [C(x)] = x,
E
h
âˆ¥C(x) âˆ’xâˆ¥2i
â‰¤Ï‰ âˆ¥xâˆ¥2 ,
âˆ€x âˆˆRd.
(4)
We denote this class of unbiased compressors as U(Ï‰).
One can ï¬nd more information about unbiased compressors in (Beznosikov et al., 2020; HorvÃ¡th
et al., 2019). The purpose of such compressors is to quantize or sparsify the communicated vectors in
order to increase the communication speed between the nodes and the server. Our methods will work
collection of stochastic mappings {Ci}n
i=1 satisfying the following assumption.
Assumption 1.2. Ci âˆˆU(Ï‰) for all i âˆˆ[n], and the compressors are independent.
2

Published as a conference paper at ICLR 2023
Table 1: General Nonconvex Case. The number of communication rounds (iterations) and the oracle
complexity of algorithms to get an Îµ-solution (E
h
âˆ¥âˆ‡f(bx)âˆ¥2i
â‰¤Îµ), and the necessity (or not) of
algorithms to send non-compressed vectors periodically (see Section 3).
Setting
Method
T := # Communication Rounds(a) Oracle Complexity
Full?(b)
Gradient
MARINA
1+Ï‰/âˆšn
Îµ
T
Yes
DASHA (Cor. 6.2)
1+Ï‰/âˆšn
Îµ
T
No
Finite-Sum
(2)
VR-MARINA
1+Ï‰/âˆšn
Îµ
+
âˆš
(1+Ï‰)m
ÎµâˆšnB
m + BT
Yes
DASHA-PAGE (Cor. 6.5)
1+Ï‰/âˆšn
Îµ
+
âˆšm
ÎµâˆšnB
m + BT
No
Stochastic
(3)
VR-MARINA (online)
1+Ï‰/âˆšn
Îµ
+
Ïƒ2
ÎµnB +
âˆš1+Ï‰Ïƒ
Îµ3/2nB
BÏ‰ + BT
Yes
DASHA-MVR (Cor. 6.8)
1+Ï‰/âˆšn
Îµ
+
Ïƒ2
ÎµnB +
Ïƒ
Îµ3/2nB
BÏ‰
q
Ïƒ2
ÎµnB
(c)
+ BT
No
DASHA-SYNC-MVR (Cor. 6.10)
1+Ï‰/âˆšn
Îµ
+
Ïƒ2
ÎµnB +
Ïƒ
Îµ3/2nB
BÏ‰ + BT
Yes
(a) Only dependencies w.r.t. the following variables are shown: Ï‰ = quantization parameter, n = # of nodes,
m = # of local functions (only in ï¬nite-sum case (2)), Ïƒ2 = variance of stochastic gradients (only in stochastic case
(3)), B = batch size (only in ï¬nite-sum and stochastic case). To simplify bounds, we assume that Ï‰ +1 = Î˜ (d/Î¶C) ,
where d is dimension of x in (1) and Î¶C is the expected number of nonzero coordinates that each compressor Ci
returns (see Deï¬nition 1.3).
(b) Does the algorithm periodically send full (non-compressed) vectors? (see Section 3)
(c) One can always choose the parameter of RandK such that this term does not dominate (see Section 6.5).
1.5
COMMUNICATION COMPLEXITY
The quantity below characterizes the number of nonzero coordinates that a compressor C returns.
This notion is useful in case of sparsiï¬cation compressors.
Deï¬nition 1.3. The expected density of the compressor Ci is Î¶Ci := supxâˆˆRd E [âˆ¥Ci(x)âˆ¥0], where
âˆ¥xâˆ¥0 is the number of nonzero components of x âˆˆRd. Let Î¶C = maxiâˆˆ[n] Î¶Ci.
In this paper, the communication complexity of a method is the number of coordinates sent to the
server per node to achieve an Îµ-solution. If every node (on average) sends Î¶ coordinates in each
communication round, then the communication complexity equals O (Î¶init + Î¶T) , where T is the
number of communication rounds, and Î¶init is the number of coordinates sent in the initialization
phase.
We would like to notice that the established communication complexities are compared to previous
upper bounds from (Gorbunov et al., 2021; Szlendak et al., 2021; Mishchenko et al., 2019; Alistarh
et al., 2017), and in this line of work, the comparisons of the communication complexities are made
with respect to the number of send coordinates. As far as we know, in this sense, no lower bounds
are proved, and it deserves a separate piece of work. However, Korhonen & Alistarh (2021) proved
the lower bounds of the communication complexity with respect to the number of send bits in the
constraint optimization setting that x âˆˆ[0, 1]d, so our upper bounds can not be directly compared to
their result because we operate on a different level of abstraction.
2
RELATED WORK
â€¢ Uncompressed communication. This line of work is characterized by methods in which the nodes
send messages (vectors) to the server without any compression. In the ï¬nite-sum setting, the current
state-of-the-art methods were proposed by Sharma et al. (2019); Li et al. (2021b), showing that after
O (1/Îµ) communication rounds and
O

m +
âˆšm
Îµâˆšn

(5)
3

Published as a conference paper at ICLR 2023
Table 2: Polyak-Åojasiewicz Case. The number of communications rounds (iterations) and oracle
complexity of algorithms to get an Îµ-solution (E [f(bx)] âˆ’f âˆ—â‰¤Îµ), and the necessity (or not) of
algorithms to send non-compressed vectors periodically.
Setting
Method
T := # Communication Rounds (a)
Oracle Complexity
Full?(b)
Gradient
MARINA
Ï‰ + L(1+Ï‰/âˆšn)
Âµ
T
Yes
DASHA (Cor. I.10)
Ï‰ + L(1+Ï‰/âˆšn)
Âµ
T
No
Finite-Sum
(2)
VR-MARINA
Ï‰ + m
B + L(1+Ï‰/âˆšn)
Âµ
+
Lâˆš
(1+Ï‰)m
ÂµâˆšnB
BT
Yes
DASHA-PAGE (Cor. I.13)
Ï‰ + m
B + L(1+Ï‰/âˆšn)
Âµ
+
Lâˆšm
ÂµâˆšnB
BT
No
Stochastic
(3)
VR-MARINA (online)
Ï‰ + L(1+Ï‰/âˆšn)
Âµ
+
Ïƒ2
ÂµÎµnB +
âˆš1+Ï‰LÏƒ
Âµ3/2âˆšÎµnB
BT
Yes
DASHA-MVR (Cor. I.16)
Ï‰ + Ï‰
q
Ïƒ2
ÂµÎµnB
(c)
+ L(1+Ï‰/âˆšn)
Âµ
+
Ïƒ2
ÂµÎµnB +
LÏƒ
Âµ3/2âˆšÎµnB
BT
No
DASHA-SYNC-MVR (Cor. I.21)
Ï‰ + L(1+Ï‰/âˆšn)
Âµ
+
Ïƒ2
ÂµÎµnB +
LÏƒ
Âµ3/2âˆšÎµnB
BT
Yes
(a) Logarithmic factors are omitted and only dependencies w.r.t. the following variables are shown: L = the worst case smoothness
constant, Âµ = PÅ constant, Ï‰ = quantization parameter, n = # of nodes, m = # of local functions (only in ï¬nite-sum case (2)),
Ïƒ2 = variance of stochastic gradients (only in stochastic case (3)), B = batch size (only in ï¬nite-sum and stochastic case). To
simplify bounds, we assume that Ï‰ + 1 = Î˜ (d/Î¶C) , where d is dimension of x in (1) and Î¶C is the expected number of nonzero
coordinates that each compressor Ci returns (see Deï¬nition 1.3).
(b) Does the algorithm periodically send full (non-compressed) vectors? (see Section 3)
(c) One can always choose the parameter of RandK such that this term does not dominate (see Section 6.5).
calculations of âˆ‡fij per node, these methods can return an Îµ-solution. Moreover, Sharma et al. (2019)
show that the same can be done in the stochastic setting after
O
Ïƒ2
Îµn +
Ïƒ
Îµ
3/2n

(6)
stochastic gradient calculations per node. Note that complexities (5) and (6) are optimal (Arjevani
et al., 2019; Fang et al., 2018; Li et al., 2021a). An adaptive variant was proposed by Khanduri et al.
(2020) based on the work of Cutkosky & Orabona (2019). See also (Khanduri et al., 2021; Murata &
Suzuki, 2021).
â€¢ Compressed communication. In practice, it is rarely affordable to send uncompressed messages
(vectors) from the nodes to the server due to limited communication bandwidth. Because of this,
researchers started to develop methods keeping in mind the communication complexity: the total
number of coordinates/ï¬‚oats/bits that the nodes send to the server to ï¬nd an Îµ-solution. Two important
families of compressors are investigated in the literature to reduce communication bottleneck: biased
and unbiased compressors. While unbiased compressors are superior in theory (Mishchenko et al.,
2019; Li et al., 2020b; Gorbunov et al., 2021), biased compressors often enjoy better performance
in practice (Beznosikov et al., 2020; Xu et al., 2020). Recently, RichtÃ¡rik et al. (2021) developed
EF21, which is the ï¬rst method capable of working with biased compressors an having the theoretical
iteration complexity of gradient descent (GD), up to constant factors.
â€¢ Unbiased compressors. The theory around unbiased compressors is much more optimistic.
Alistarh et al. (2017) developed the QSGD method providing convergence rates of stochastic gradient
method with quantized vectors. However, the nonstrongly convex case was analyzed under the
strong assumption that all nodes have identical functions, and the stochastic gradients have bounded
second moment. Next, Mishchenko et al. (2019); HorvÃ¡th et al. (2019) proposed the DIANA method
and proved convergence rates without these restrictive assumptions. Also, distributed nonconvex
optimization methods with compression were developed by Haddadpour et al. (2021); Das et al.
(2020). Finally, Gorbunov et al. (2021) proposed MARINA â€“ the current state-of-the-art distributed
method in terms of theoretical communication complexity, inspired by the PAGE method of Li et al.
(2021a).
3
CONTRIBUTIONS
We develop a new family of distributed optimization methods DASHA for nonconvex optimization
problems with unbiased compressors. Compared to MARINA, our methods make more practical and
4

Published as a conference paper at ICLR 2023
simpler optimization steps. In particular, in MARINA, all nodes simultaneously send either compressed
vectors, with some probability p, or the gradients of functions {fi}n
i=1 (uncompressed vectors), with
probability 1âˆ’p. In other words, the server periodically synchronizes all nodes. In federated learning,
where some nodes can be inaccessible for a long time, such periodic synchronization is intractable.
Our method DASHA solves both problems: i) the nodes always send compressed vectors, and ii) the
server never synchronizes all nodes in the gradient setting.
Further, a simple tweak in the compressors (see Appendix D) results in support for partial participa-
tion in the gradient setting , which makes DASHA more practical for federated learning tasks. Let us
summarize our most important theoretical and practical contributions:
â€¢ New theoretical SOTA complexity in the ï¬nite-sum setting. Using our novel approach to
compress gradients, we improve the theoretical complexities of VR-MARINA (see Tables 1 and 2) in
the ï¬nite-sum setting. Indeed, if the number of functions m is large, our algorithm DASHA-PAGE
needs âˆšÏ‰ + 1 times fewer communications rounds, while communicating compressed vectors only.
â€¢ New theoretical SOTA complexity in the stochastic setting. We develop a new method, DASHA-
SYNC-MVR, improving upon the previous state of the art (see Table 1). When Îµ is small, the number
of communication rounds is reduced by a factor of âˆšÏ‰ + 1. Indeed, we improve the dominant term
which depends on Îµ
3/2 (the other terms depend on Îµ only). However, DASHA-SYNC-MVR needs to
periodically send uncompressed vectors with the same rate as VR-MARINA (online). Nevertheless,
we show that DASHA-MVR also improves the dominant term when Îµ is small, and this method
sends compressed vectors only. Moreover, we provide detailed experiments on practical machine
learning tasks: training nonconvex generalized linear models and deep neural networks, showing
improvements predicted by our theory. See Appendix A.
â€¢ Closing the gap between uncompressed and compressed methods. In Section 2, we mentioned
that the optimal oracle complexities of methods without compression in the ï¬nite-sum and stochastic
settings are (5) and (6), respectively. Considering the RandK compressor (see Deï¬nition F.1),
we show that DASHA-PAGE, DASHA-MVR and DASHA-SYNC-MVR attain these optimal oracle
complexities while attainting the state-of-the-art communication complexity as MARINA, which needs
to use the stronger gradient oracle! Therefore, our new methods close the gap between results from
(Gorbunov et al., 2021) and (Sharma et al., 2019; Li et al., 2021b).
4
ALGORITHM DESCRIPTION
We now describe our proposed family of optimization methods, DASHA (see Algorithm 1). DASHA
is inspired by MARINA and momentum variance reduction methods (MVR) (Cutkosky & Orabona,
2019; Tran-Dinh et al., 2021; Liu et al., 2020): the general structure repeats MARINA except for the
variance reduction strategy, which we borrow from MVR. Unlike MARINA, our algorithm never sends
uncompressed vectors, and the number of bits that every node sends is always the same. Moreover,
we reduce the variance from the oracle and the compressor separately, which helps us to improve the
theoretical convergence rates in the stochastic and ï¬nite-sum cases.
First, using the gradient estimator gt, the server in each communication round calculates the next
point xt+1 and broadcasts it to the nodes. Subsequently, all nodes in parallel calculate vectors ht+1
i
in one of three ways, depending on the available oracle. For the the gradient, ï¬nite-sum, and the
stochastic settings, we use GD-like, PAGE-like, and MVR-like strategies, respectively. Next, each
node compresses their message and uploads it to the server. Finally, the server aggregates all received
messages and calculates the next vector gt+1. We refer to Section H to get a better intuition about
DASHA.
We note that in the stochastic setting, our analysis of DASHA-MVR (Algorithm 1) provides a subopti-
mal oracle complexity w.r.t. Ï‰ (see Tables 1 and 2). In Appendix J we provide experimental evidence
that our analysis is tight. For this reason, we developed DASHA-SYNC-MVR (see Algorithm 2 in
Appendix C) that improves the previous state-of-the-art results and sends non-compressed vectors
with the same rate as VR-MARINA (online). Note that DASHA-MVR still enjoys the optimal oracle and
SOTA communication complexity (see Section 6.5); and this can be seen it in experiments.
5

Published as a conference paper at ICLR 2023
Algorithm 1 DASHA
1: Input: starting point x0 âˆˆRd, stepsize Î³ > 0, momentum a âˆˆ(0, 1], momentum b âˆˆ(0, 1]
(only in DASHA-MVR), probability p âˆˆ(0, 1] (only in DASHA-PAGE), batch size B (only in
DASHA-PAGE and DASHA-MVR), number of iterations T â‰¥1
2: Initialize g0
i âˆˆRd, h0
i âˆˆRd on the nodes and g0 = 1
n
Pn
i=1 g0
i on the server
3: for t = 0, 1, . . . , T âˆ’1 do
4:
xt+1 = xt âˆ’Î³gt
5:
Flip a coin ct+1 =
1,
with probability p
0,
with probability 1 âˆ’p (only in DASHA-PAGE)
6:
Broadcast xt+1 to all nodes
7:
for i = 1, . . . , n in parallel do
8:
ht+1
i
=
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
âˆ‡fi(xt+1)
(DASHA)
(
âˆ‡fi(xt+1)
if ct+1 = 1
ht
i + 1
B
P
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

if ct+1 = 0
(DASHA-PAGE)
1
B
PB
j=1 âˆ‡fi(xt+1; Î¾t+1
ij ) + (1 âˆ’b)

ht
i âˆ’1
B
PB
j=1 âˆ‡fi(xt; Î¾t+1
ij )

(DASHA-MVR)
9:
mt+1
i
= Ci
 ht+1
i
âˆ’ht
i âˆ’a (gt
i âˆ’ht
i)

10:
gt+1
i
= gt
i + mt+1
i
11:
Send mt+1
i
to the server
12:
end for
13:
gt+1 = gt + 1
n
Pn
i=1 mt+1
i
14: end for
15: Output: Ë†xT chosen uniformly at random from {xt}T âˆ’1
k=0 (or xT under the PÅ-condition)
5
ASSUMPTIONS
We now provide the assumptions used throughout our paper.
Assumption 5.1. There exists f âˆ—âˆˆR such that f(x) â‰¥f âˆ—for all x âˆˆR.
Assumption 5.2. The function f is Lâ€“smooth, i.e.,
âˆ¥âˆ‡f(x) âˆ’âˆ‡f(y)âˆ¥â‰¤L âˆ¥x âˆ’yâˆ¥
for all x, y âˆˆRd.
Assumption 5.3. For all i âˆˆ[n], the function fi is Liâ€“smooth.2 We deï¬ne bL2 := 1
n
Pn
i=1 L2
i .
The next assumption is used in the ï¬nite-sum setting (2).
Assumption 5.4. For all i âˆˆ[n], j âˆˆ[m], the function fij is Lij-smooth.
Let Lmax :=
maxiâˆˆ[n],jâˆˆ[m] Lij.
The two assumptions below are provided for the stochastic setting (3).
Assumption 5.5. For all i âˆˆ[n] and for all x âˆˆRd, the stochastic gradient âˆ‡fi(x; Î¾) is unbiased
and has bounded variance, i.e.,
EÎ¾ [âˆ‡fi(x; Î¾)] = âˆ‡fi(x),
and
EÎ¾
h
âˆ¥âˆ‡fi(x; Î¾) âˆ’âˆ‡fi(x)âˆ¥2i
â‰¤Ïƒ2,
where Ïƒ2 â‰¥0.
Assumption 5.6. For all i âˆˆ[n] and for all x, y âˆˆR, the stochastic gradient âˆ‡fi(x; Î¾) satisï¬es the
mean-squared smoothness property, i.e.,
EÎ¾
h
âˆ¥âˆ‡fi(x; Î¾) âˆ’âˆ‡fi(y; Î¾) âˆ’(âˆ‡fi(x) âˆ’âˆ‡fi(y))âˆ¥2i
â‰¤L2
Ïƒ âˆ¥x âˆ’yâˆ¥2 .
2Note that one can always take L2 = bL2 := 1
n
Pn
i=1 L2
i . However, the optimal constant L can be much
better because L2 â‰¤
  1
n
Pn
i=1 Li
2 â‰¤1
n
Pn
i=1 L2
i .
6

Published as a conference paper at ICLR 2023
6
THEORETICAL CONVERGENCE RATES
Now, we provide convergence rate theorems for DASHA, DASHA-PAGE and DASHA-MVR. All three
methods are listed in Algorithm 1 and differ in Line 8 only. At the end of the section, we provide a
theorem for DASHA-SYNC-MVR.
6.1
GRADIENT SETTING (DASHA)
Theorem 6.1. Suppose that Assumptions 5.1, 5.2, 5.3 and 1.2 hold. Let us take a = 1/ (2Ï‰ + 1)
, Î³ â‰¤

L +
q
16Ï‰(2Ï‰+1)
n
bL
âˆ’1
, and g0
i = h0
i = âˆ‡fi(x0) for all i âˆˆ[n] in Algorithm 1 (DASHA),
then E
hâˆ‡f(bxT )
2i
â‰¤
2(f(x0)âˆ’f âˆ—)
Î³T
.
The corollary below simpliï¬es the previous theorem and reveals the communication complexity of
DASHA.
Corollary 6.2. Suppose that assumptions from Theorem 6.1 hold, and g0
i = h0
i = âˆ‡fi(x0) for all
i âˆˆ[n], then DASHA needs T := O
 
1
Îµ
"
 f(x0) âˆ’f âˆ— 
L +
Ï‰
âˆšn bL
 #!
communication rounds
to get an Îµ-solution and the communication complexity is equal to O (d + Î¶CT) , where Î¶C is the
expected density from Deï¬nition 1.3.
In the previous corollary, we have free parameters Ï‰ and Î¶C. Now, we consider the RandK compressor
(see Deï¬nition F.1) and choose its parameters to get the communication complexity w.r.t. only d and
n.
Corollary 6.3. Suppose that assumptions of Corollary 6.2 hold. We take the unbiased compressor
RandK with K = Î¶C â‰¤d/âˆšn, then the communication complexity equals O

d +
bL(f(x0)âˆ’f âˆ—)d
Îµâˆšn

.
6.2
FINITE-SUM SETTING (DASHA-PAGE)
Next, we provide the complexity bounds for DASHA-PAGE.
Theorem 6.4. Suppose that Assumptions 5.1, 5.2, 5.3, 5.4, and 1.2 hold. Let us take a = 1/ (2Ï‰ + 1),
probability p âˆˆ(0, 1],
Î³ â‰¤
 
L +
s
48Ï‰ (2Ï‰ + 1)
n
(1 âˆ’p)L2max
B
+ bL2

+ 2 (1 âˆ’p) L2max
pnB
!âˆ’1
and g0
i = h0
i = âˆ‡fi(x0) for all i âˆˆ[n] in Algorithm 1 (DASHA-PAGE) then E
hâˆ‡f(bxT )
2i
â‰¤
2(f(x0)âˆ’f âˆ—)
Î³T
.
Let us simplify the statement of Theorem 6.4 by choosing particular parameters.
Corollary 6.5. Let the assumptions from Theorem 6.4 hold, p = B/(m+B), and g0
i = h0
i = âˆ‡fi(x0)
for all i âˆˆ[n]. Then DASHA-PAGE needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— 
L + Ï‰
âˆšn
bL +
 Ï‰
âˆšn +
r m
nB
 Lmax
âˆš
B

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the expected # of gradient calculations per node equals O (m + BT) , where Î¶C is the expected
density from Deï¬nition 1.3.
The corollary below reveals the communication and oracle complexities of Algorithm 1 (DASHA-
PAGE) with RandK.
7

Published as a conference paper at ICLR 2023
Corollary 6.6. Suppose that assumptions of Corollary 6.5 hold, B â‰¤
p
m/n, and we use the unbiased
compressor RandK with K = Î¶C = Î˜ (Bd/âˆšm) . Then the communication complexity of Algorithm 1
is
O
 
d + Lmax
 f(x0) âˆ’f âˆ—
d
Îµâˆšn
!
,
(7)
and the expected # of gradient calculations per node equals
O
 
m + Lmax
 f(x0) âˆ’f âˆ— âˆšm
Îµâˆšn
!
.
(8)
Up to Lipschitz constants factors, bound (8) is optimal (Fang et al., 2018; Li et al., 2021a), and unlike
VR-MARINA, we recover the optimal bound with compression! At the same time, the communication
complexity (7) is the same as in DASHA (see Corollary 6.3) or MARINA.
6.3
STOCHASTIC SETTING (DASHA-MVR)
Let ht := 1
n
Pn
i=1 ht
i. This vector is not used in Algorithm 1, but appears in the theoretical results.
Theorem 6.7. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2Ï‰+1,
b âˆˆ(0, 1], Î³ â‰¤

L +
r
96Ï‰(2Ï‰+1)
n

(1âˆ’b)2L2Ïƒ
B
+ bL2

+ 4(1âˆ’b)2L2Ïƒ
bnB
âˆ’1
, and g0
i = h0
i for all i âˆˆ[n]
in Algorithm 1 (DASHA-MVR). Then
E
hâˆ‡f(bxT )
2i
â‰¤1
T
ï£®
ï£¯ï£¯ï£°
2
 f(x0) âˆ’f âˆ—
Î³
+ 2
b
h0 âˆ’âˆ‡f(x0)
2
+ 32bÏ‰ (2Ï‰ + 1)
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£»+
96Ï‰ (2Ï‰ + 1)
nB
+
4
bnB

b2Ïƒ2.
Corollary 6.8.
Suppose that assumptions from Theorem 6.7 hold,
momentum b
=
Î˜

min
n
1
Ï‰
q
nÎµB
Ïƒ2 , nÎµB
Ïƒ2
o
, and g0
i = h0
i =
1
Binit
PBinit
k=1 âˆ‡fi(x0; Î¾0
ik) for all i âˆˆ[n], and batch
size Binit = Î˜ (B/b) , then Algorithm 1 (DASHA-MVR) needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn
bL +
 
Ï‰
âˆšn +
r
Ïƒ2
Îµn2B
!
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where Î¶C is the
expected density from Deï¬nition 1.3.
The following corollary reveals the communication and oracle complexity of DASHA-MVR.
Corollary 6.9. Suppose that assumptions of Corollary 6.8 hold, batch size B â‰¤
Ïƒ
âˆšÎµn, we take RandK
with K = Î¶C = Î˜

BdâˆšÎµn
Ïƒ

, and eL := max{L, LÏƒ, bL}. Then the communication complexity equals
O
 
dÏƒ
âˆšnÎµ +
eL
 f(x0) âˆ’f âˆ—
d
âˆšnÎµ
!
,
(9)
and the expected # of stochastic gradient calculations per node equals
O
 
Ïƒ2
nÎµ +
eL
 f(x0) âˆ’f âˆ—
Ïƒ
Îµ
3/2n
!
.
(10)
8

Published as a conference paper at ICLR 2023
Up to Lipschitz constant factors, the bound (10) is optimal (Arjevani et al., 2019; Sharma et al., 2019),
and unlike VR-MARINA (online), we recover the optimal bound with compression! At the same time,
the communication complexity (9) is the same as in DASHA (see Corollary 6.3) or MARINA for small
enough Îµ.
6.4
STOCHASTIC SETTING (DASHA-SYNC-MVR)
We now provide the complexities of Algorithm 2 (DASHA-SYNC-MVR) presented in Appendix C.
The main convergence rate Theorem I.19 is in the appendix.
Corollary 6.10.
Suppose that assumptions from Theorem I.19 hold,
probability p
=
min
n
Î¶C
d , nÎµB
Ïƒ2
o
, batch size Bâ€² = Î˜

Ïƒ2
nÎµ

and h0
i = g0
i =
1
Binit
PBinit
k=1 âˆ‡fi(x0; Î¾0
ik) for all i âˆˆ[n],
initial batch size Binit = Î˜

max
n
Ïƒ2
nÎµ, B d
Î¶C
o
, then DASHA-SYNC-MVR needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn
bL +
 
Ï‰
âˆšn +
s
d
Î¶Cn +
r
Ïƒ2
Îµn2B
!
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where Î¶C is the
expected density from Deï¬nition 1.3.
Corollary 6.11. Suppose that assumptions of Corollary 6.10 hold, batch size B â‰¤
Ïƒ
âˆšÎµn, we take
RandK with K = Î¶C = Î˜

BdâˆšÎµn
Ïƒ

, and eL := max{L, LÏƒ, bL}. Then the communication complex-
ity equals
O
 
dÏƒ
âˆšnÎµ +
eL
 f(x0) âˆ’f âˆ—
d
âˆšnÎµ
!
,
(11)
and the expected # of stochastic gradient calculations per node equals
O
 
Ïƒ2
nÎµ +
eL
 f(x0) âˆ’f âˆ—
Ïƒ
Îµ
3/2n
!
.
(12)
Up to Lipschitz constant factors, the bound (12) is optimal (Arjevani et al., 2019; Sharma et al., 2019),
and unlike VR-MARINA (online), we recover the optimal bound with compression! At the same time,
the communication complexity (11) is the same as in DASHA (see Corollary 6.3) or MARINA for
small enough Îµ.
6.5
COMPARISON OF DASHA-MVR AND DASHA-SYNC-MVR
Let us consider RandK (note that Ï‰ + 1 = d/Î¶C). Comparing Corollary 6.8 to Corollary 6.10
(see Table 1), we see that DASHA-SYNC-MVR improved the size of the initial batch from
Î˜

max

Ïƒ2
nÎµ, BÏ‰
q
Ïƒ2
nÎµB

to Î˜

max
n
Ïƒ2
nÎµ, BÏ‰
o
. Fortunately, we can control the parameter K
in RandK, and Corollary 6.9 reveals that we can take K = Î˜

BdâˆšÎµn
Ïƒ

, and get BÏ‰
q
Ïƒ2
nÎµB â‰¤Ïƒ2
nÎµ.
As a result, the â€œbad termâ€ does not dominate the oracle complexity, and DASHA-MVR attains the
optimal oracle and SOTA communication complexity. The same reasoning applies to optimization
problems under PÅ-condition with K = Î˜

BdâˆšÂµÎµn
Ïƒ

.
ACKNOWLEDGEMENTS
The work of P. RichtÃ¡rik was partially supported by the KAUST Baseline Research Fund Scheme
and by the SDAIA-KAUST Center of Excellence in Data Science and Artiï¬cial Intelligence. The
work of A. Tyurin was supported by the Extreme Computing Research Center (ECRC) at KAUST.
9

Published as a conference paper at ICLR 2023
REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efï¬cient SGD via gradient quantization and encoding. In Advances in Neural Information Process-
ing Systems (NIPS), pp. 1709â€“1720, 2017.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Aleksandr Beznosikov, Samuel HorvÃ¡th, Peter RichtÃ¡rik, and Mher Safaryan. On biased compression
for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
SÃ©bastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. arXiv preprint
arXiv:2105.12806, 2021.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology (TIST), 2(3):1â€“27, 2011.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD.
arXiv preprint arXiv:1905.10018, 2019.
Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov, Sergey Guminov,
Dmitry Kamzolov, and Innokentiy Shibaev. Recent theoretical advances in non-convex optimiza-
tion. arXiv preprint arXiv:2012.06188, 2020.
Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S Dhillon. Improved convergence
rates for non-convex federated learning with compression. arXiv e-prints, pp. arXivâ€“2012, 2020.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex op-
timization via stochastic path integrated differential estimator. In NeurIPS Information Processing
Systems, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter RichtÃ¡rik. MARINA: Faster non-
convex distributed learning with compression. In 38th International Conference on Machine
Learning, 2021.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated
learning with compression: Uniï¬ed analysis and sharp guarantees. In International Conference on
Artiï¬cial Intelligence and Statistics, pp. 2350â€“2358. PMLR, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770â€“778, 2016.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.
Samuel HorvÃ¡th, Chen-Yu Ho, Lâ€™udovÃ­t HorvÃ¡th, Atal Narayan Sahu, Marco Canini, and Peter
RichtÃ¡rik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988,
2019.
Samuel HorvÃ¡th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter RichtÃ¡rik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019.
10

Published as a conference paper at ICLR 2023
Prashant Khanduri, Pranay Sharma, Swatantra Kaï¬‚e, Saikiran Bulusu, Ketan Rajawat, and Pramod K
Varshney. Distributed stochastic non-convex optimization: Momentum-based variance reduction.
arXiv preprint arXiv:2005.00224, 2020.
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan Rajawat, and Pramod
Varshney. STEM: A stochastic two-sided momentum algorithm achieving near-optimal sample and
communication complexities for federated learning. Advances in Neural Information Processing
Systems, 34, 2021.
Jakub KoneË‡cnÃ½, H Brendan McMahan, Felix X Yu, Peter RichtÃ¡rik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efï¬ciency. arXiv
preprint arXiv:1610.05492, 2016.
Janne H Korhonen and Dan Alistarh. Towards tight communication lower bounds for distributed
optimisation. Advances in Neural Information Processing Systems, 34:7254â€“7266, 2021.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, Toronto, 2009.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems,
2:429â€“450, 2020a.
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter RichtÃ¡rik. Acceleration for compressed gradient
descent in distributed and federated optimization.
In International Conference on Machine
Learning, 2020b.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter RichtÃ¡rik. PAGE: A simple and optimal
probabilistic gradient estimator for nonconvex optimization. In International Conference on
Machine Learning, pp. 6286â€“6295. PMLR, 2021a.
Zhize Li, SlavomÃ­r Hanzely, and Peter RichtÃ¡rik. ZeroSARAH: Efï¬cient nonconvex ï¬nite-sum
optimization with zero full gradient computation. arXiv preprint arXiv:2103.01447, 2021b.
Deyi Liu, Lam M Nguyen, and Quoc Tran-Dinh. An optimal hybrid variance-reduced algorithm for
stochastic composite nonconvex optimization. arXiv preprint arXiv:2008.09055, 2020.
Konstantin Mishchenko, Eduard Gorbunov, Martin TakÃ¡Ë‡c, and Peter RichtÃ¡rik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Tomoya Murata and Taiji Suzuki. Bias-variance reduced local SGD for less heterogeneous federated
learning. arXiv preprint arXiv:2102.03198, 2021.
Angelia NediÂ´c and Alex Olshevsky. Distributed optimization over time-varying directed graphs.
IEEE Transactions on Automatic Control, 60(3):601â€“615, 2014.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.
Peter RichtÃ¡rik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and
practically faster error feedback. arXiv preprint arXiv:2106.05203, 2021.
Pranay Sharma, Swatantra Kaï¬‚e, Prashant Khanduri, Saikiran Bulusu, Ketan Rajawat, and Pramod K
Varshney. Parallel restarted SPIDERâ€“communication efï¬cient distributed nonconvex optimization
with optimal computation complexity. arXiv preprint arXiv:1912.06036, 2019.
RafaÅ‚ Szlendak, Alexander Tyurin, and Peter RichtÃ¡rik. Permutation compressors for provably faster
distributed nonconvex optimization. arXiv preprint arXiv:2110.03300, 2021.
11

Published as a conference paper at ICLR 2023
Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. A hybrid stochastic optimization
framework for composite nonconvex optimization. Mathematical Programming, pp. 1â€“67, 2021.
Thijs Vogels, Lie He, Anastasiia Koloskova, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich,
and Martin Jaggi. RelaySum for decentralized deep learning on heterogeneous data. Advances in
Neural Information Processing Systems, 34, 2021.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ï¬eld guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021.
Hang Xu, Chen-Yu Ho, Ahmed M Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos
Karatsenidis, Marco Canini, and Panos Kalnis. Compressed communication for distributed deep
learning: Survey and quantitative evaluation. Technical report, 2020.
12

Published as a conference paper at ICLR 2023
CONTENTS
1
Introduction
1
1.1
Problem formulation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Gradient oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Oracle complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.4
Unbiased compressors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.5
Communication complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Related Work
3
3
Contributions
4
4
Algorithm Description
5
5
Assumptions
6
6
Theoretical Convergence Rates
7
6.1
Gradient Setting (DASHA)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
6.2
Finite-Sum Setting (DASHA-PAGE)
. . . . . . . . . . . . . . . . . . . . . . . . .
7
6.3
Stochastic Setting (DASHA-MVR)
. . . . . . . . . . . . . . . . . . . . . . . . . .
8
6.4
Stochastic Setting (DASHA-SYNC-MVR) . . . . . . . . . . . . . . . . . . . . . . .
9
6.5
Comparison of DASHA-MVR and DASHA-SYNC-MVR . . . . . . . . . . . . . . . .
9
A Experiments
15
A.1
Gradient setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.2
Finite-sum setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.3
Stochastic setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.4
Deep neural network training . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
B
Experiments Details
18
C Description of DASHA-SYNC-MVR
19
D Partial Participation
19
E
Auxiliary Facts
19
F
Compressors Facts
20
G Polyak-Åojasiewicz Condition
20
H Intuition Behind DASHA
21
H.1
Different Sources of Contractions
. . . . . . . . . . . . . . . . . . . . . . . . . .
21
13

Published as a conference paper at ICLR 2023
H.2
The Source of Improvements in the Convergence Rates . . . . . . . . . . . . . . .
21
I
Theorems with Proofs
22
I.1
Case of DASHA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
I.2
Case of DASHA under PÅ-condition
. . . . . . . . . . . . . . . . . . . . . . . . .
28
I.3
Case of DASHA-PAGE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
I.4
Case of DASHA-PAGE under PÅ-condition . . . . . . . . . . . . . . . . . . . . . .
35
I.5
Case of DASHA-MVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
I.6
Case of DASHA-MVR under PÅ-condition . . . . . . . . . . . . . . . . . . . . . .
45
I.7
Case of DASHA-SYNC-MVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
I.8
Case of DASHA-SYNC-MVR under PÅ-condition
. . . . . . . . . . . . . . . . . .
55
J
Extra Experiments
58
14

Published as a conference paper at ICLR 2023
A
EXPERIMENTS
We have tested all developed algorithms on practical machine learnings problems3. Note that the goal
of our experiments is to justify the theoretical convergence rates from our paper. We compare the
new methods with MARINA on LIBSVM datasets (Chang & Lin, 2011) (under the 3-clause BSD
license) because MARINA is the only previous state-of-the-art method for the problem (1). Moreover,
we show the advantage of our method on an image recognition task with CIFAR10 (Krizhevsky et al.,
2009) and a deep neural network. In all experiments, we take parameters of algorithms predicted by
the theory (stated in the convergence rate theorems our paper and in (Gorbunov et al., 2021)), except
for the step sizes â€“ we ï¬ne-tune them using a set of powers of two {2i | i âˆˆ[âˆ’10, 10]} â€“ and use the
RandK compressor. We evaluate communication complexity; thus, each plot represents the relation
between the norm of a gradient or function value (vertical axis), and the total number of transmitted
bits per node (horizontal axis).
A.1
GRADIENT SETTING
We consider nonconvex functions
fi(x) := 1
m
m
X
j=1
 
1 âˆ’
1
1 + exp(yijaâŠ¤
ijx)
!2
to solve a classiï¬cation problem. Here, aij âˆˆRd is the feature vector of a sample on the ith node,
yij âˆˆ{âˆ’1, 1} is the corresponding label, and m is the number of samples on the ith node. All nodes
calculate full gradients. We take the mushrooms dataset (dimension d = 112, number of samples
equals 8124) from LIBSVM, randomly split the dataset between 5 nodes and take K = 10 in RandK.
One can see in Figure 1 that DASHA converges approximately 2 times faster.
0.0
0.2
0.4
0.6
0.8
1.0
1.2
#bit# / n
1e7
10âˆ’9
10âˆ’7
10âˆ’5
10âˆ’3
10âˆ’1
||âˆ‡f(xk)||2
Number of nodes: 5
MARINA: Step size: 0.25
MARINA: Step size: 0.5
MARINA: Step size: 1.0
DASHA: Step size: 0.25
DASHA: Step size: 0.5
DASHA: Step size: 1.0
Figure 1: Classiï¬cation task with the mushrooms dataset and gradient oracle.
3Code: https://github.com/mysteryresearcher/dasha
15

Published as a conference paper at ICLR 2023
A.2
FINITE-SUM SETTING
Now, we conduct the same experiments as in Section A.1 with real-sim dataset (dimension d =
20,958, number of samples equals 72,309) from LIBSVM in the ï¬nite-sum setting; moreover, we
compare VR-MARINA versus DASHA-PAGE with batch size B = 1 in both algorithms. Results in
Figure 2 coincide with Table 1 â€“ our new method DASHA-PAGE converges faster than MARINA.
When K = 100, the improvement is not signiï¬cant because 1+Ï‰/âˆšn
Îµ
dominates
âˆšm
ÎµâˆšnB (see Table 1),
and both algorithms get the same theoretical convergence complexity.
0.0
0.5
1.0
1.5
#bits / n
1e8
10
6
10
5
10
4
|| f(xk)||2
K = 100
Number of nodes: 5
VR-MARINA: Step size: 0.03125
VR-MARINA: Step size: 0.0625
VR-MARINA: Step size: 0.125
DASHA-PAGE: Step size: 0.03125
DASHA-PAGE: Step size: 0.0625
DASHA-PAGE: Step size: 0.125
0
2
4
6
8
#bits / n
1e8
10
7
10
6
10
5
10
4
K = 500
VR-MARINA: Step size: 0.0625
VR-MARINA: Step size: 0.125
VR-MARINA: Step size: 0.25
DASHA-PAGE: Step size: 0.25
DASHA-PAGE: Step size: 0.5
DASHA-PAGE: Step size: 1.0
0
1
2
3
#bits / n
1e9
10
7
10
6
10
5
10
4
K = 2000
VR-MARINA: Step size: 0.25
VR-MARINA: Step size: 0.5
VR-MARINA: Step size: 1.0
DASHA-PAGE: Step size: 0.5
DASHA-PAGE: Step size: 1.0
DASHA-PAGE: Step size: 2.0
Figure 2: Classiï¬cation task with the real-sim dataset and K âˆˆ{100; 500; 2, 000} in RandK in the
ï¬nite-sum setting.
16

Published as a conference paper at ICLR 2023
A.3
STOCHASTIC SETTING
In this experiment, we consider the following logistic regression functions with nonconvex regularizer
{fi}n
i=1 to solve a classiï¬cation problem:
fi(x1, x2) := Ejâˆ¼[m]
ï£®
ï£¯ï£¯ï£°âˆ’log
 
exp
 aâŠ¤
ijxyij

P
yâˆˆ{1,2} exp
 aâŠ¤
ijxy

!
+ Î»
X
yâˆˆ{1,2}
d
X
k=1
{xy}2
k
1 + {xy}2
k
ï£¹
ï£ºï£ºï£»,
where x1, x2 âˆˆRd, {Â·}k is an indexing operation, aij âˆˆRd is a feature of a sample on the ith
node, yij âˆˆ{1, 2} is a corresponding label, m is the number of samples located on the ith node,
constant Î» = 0.001. We take batch size B = 1 and compare VR-MARINA (online), DASHA-MVR,
and DASHA-SYNC-MVR that depend on a common ratio Ïƒ2/nÎµB 4. We ï¬x Ïƒ2/nÎµB âˆˆ{104, 105} and
K âˆˆ{200, 2000} in RandK compressors. We consider real-sim dataset from LIBSVM splitted
between 5 nodes. When we increase Ïƒ2/nÎµB from 104 to 105, we implicitly decrease Îµ because other
parameters are ï¬xed. In Figure 3, when Îµ is small, DASHA-MVR and DASHA-SYNC-MVR converge
faster than VR-MARINA (online).
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e9
10
6
10
5
10
4
|| f(xk)||2
K = 200
2
n  = 10000
VR-MARINA (online): Step size: 0.0625
DASHA-MVR: Step size: 0.0625
DASHA-SYNC-MVR: Step size: 0.125
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e10
10
6
10
5
10
4
K = 2000
VR-MARINA (online): Step size: 0.25
DASHA-MVR: Step size: 0.25
DASHA-SYNC-MVR: Step size: 0.5
0.00
0.25
0.50
0.75
1.00
1.25
1.50
#bits / n
1e10
10
7
10
6
10
5
10
4
|| f(xk)||2
2
n  = 100000
VR-MARINA (online): Step size: 0.015625
DASHA-MVR: Step size: 0.0625
DASHA-SYNC-MVR: Step size: 0.0625
0.00
0.25
0.50
0.75
1.00
1.25
1.50
#bits / n
1e11
10
7
10
6
10
5
10
4
VR-MARINA (online): Step size: 0.03125
DASHA-MVR: Step size: 0.125
DASHA-SYNC-MVR: Step size: 0.125
Figure 3: Classiï¬cation task with the real-sim dataset, Ïƒ2/nÎµB âˆˆ{104, 105}, and K âˆˆ{200, 2000}
in RandK in the stochastic setting.
4Indeed, in DASHA-SYNC-MVR and MARINA, the probability p = min{K/d, nÎµB/Ïƒ2}. In DASHA-MVR,
the momentum b = min{K/d
p
nÎµB/Ïƒ2, nÎµB/Ïƒ2}.
17

Published as a conference paper at ICLR 2023
A.4
DEEP NEURAL NETWORK TRAINING
Finally, we test our algorithms on an image recognition task, CIFAR10 (Krizhevsky et al., 2009),
with the ResNet-18 (He et al., 2016) deep neural network (the number of parameters d â‰ˆ107).
We split CIFAR10 among 5 nodes, and take K â‰ˆ2 Â· 106 in RandK. In all methods we ï¬ne-
tune two parameters: step size Î³ âˆˆ{0.05, 0.01, 0.005, 0.001} and ratio Ïƒ2/nÎµB âˆˆ{2, 10, 20, 100}.
Moreover, we trained the neural network with SGD without compression as a baseline, with step size
Î³ âˆˆ{1.0, 0.5, 0.1, 0.05, 0.01, 0.001}. All nodes have batch size B = 25.
Results are provided in Figure 4. We see that DASHA-MVR converges signiï¬cantly faster than other
algorithms in the terms of communication complexity. Moreover, DASHA-SYNC-MVR works better
than VR-MARINA (online) and SGD.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
#bits / n
1e12
10âˆ’3
1002
1001
100
f(xk) 0 f(x * )
Number )f ()des: 5
Va(illa SGD: Sâˆ’e* size: 0.05
VR-MARINA (online): Step size: 0.01; Batch Size B': 10
DASHA-MVR: Step size: 0.01; Momentum b: 0.1
DASHA-SYNC-MVR: Step size: 0.01; Batch Size B': 10
Figure 4: Classiï¬cation task with CIFAR10 dataset and ResNet-18 deep neural network. Dimension
d â‰ˆ107 and K â‰ˆ2 Â· 106 in RandK.
B
EXPERIMENTS DETAILS
The code was written in Python 3.6.8 using PyTorch 1.9 (Paszke et al., 2019). A distributed
environment was emulated on a machine with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz and
64 cores. Deep learning experiments were conducted with NVIDIA A100 GPU with 40GB memory
(each deep learning experiment uses at most 5GB of this memory).
When the number of nodes n does not divide the number of samples N in a dataset, we randomly
ignore N mod n samples from a dataset (up to 4 when n = 5).
18

Published as a conference paper at ICLR 2023
C
DESCRIPTION OF DASHA-SYNC-MVR
In this section, we provide a description of DASHA-SYNC-MVR (see Algorithm 2). This algorithm
is closely related to DASHA-MVR (Algorithm 1), but DASHA-SYNC-MVR synchronizes all nodes
with some probability p. This synchronization procedure enabled us to ï¬x the convergence rate
suboptimality of DASHA-MVR w.r.t. Ï‰.
Algorithm 2 DASHA-SYNC-MVR
1: Input: starting point x0 âˆˆRd, stepsize Î³ > 0, momentum a âˆˆ(0, 1], probability p âˆˆ(0, 1],
batch size Bâ€², number of iterations T â‰¥1.
2: Initialize g0
i , h0
i on the nodes and g0 = 1
n
Pn
i=1 g0
i on the server
3: for t = 0, 1, . . . , T âˆ’1 do
4:
xt+1 = xt âˆ’Î³gt
5:
ct+1 =
1, with probability p,
0, with probability 1 âˆ’p
6:
Broadcast xt+1 to all nodes
7:
for i = 1, . . . , n in parallel do
8:
if ct+1 = 1 then
9:
ht+1
i
=
1
Bâ€²
PBâ€²
k=1 âˆ‡fi(xt+1; Î¾t+1
ik )
10:
mt+1
i
= gt+1
i
= ht+1
i
11:
else
12:
ht+1
i
= 1
B
PB
j=1 âˆ‡fi(xt+1; Î¾t+1
ij ) + ht
i âˆ’1
B
PB
j=1 âˆ‡fi(xt; Î¾t+1
ij )
13:
mt+1
i
= Ci
 ht+1
i
âˆ’ht
i âˆ’a (gt
i âˆ’ht
i)

14:
gt+1
i
= gt
i + mt+1
i
15:
end if
16:
Send mt+1
i
to the server
17:
end for
18:
if ct+1 = 1 then
19:
gt+1 = 1
n
Pn
i=1 mt+1
i
20:
else
21:
gt+1 = gt + 1
n
Pn
i=1 mt+1
i
22:
end if
23: end for
24: Output: Ë†xT chosen uniformly at random from {xt}T âˆ’1
k=0
D
PARTIAL PARTICIPATION
A partial participation mechanism, important for federated learning applications, can be easily
implemented in DASHA. Let us assume that the ith node either participates in a communication round
with probability pâ€², or sends nothing. From the view of unbiased compressors, it can mean that
instead of using a compressor C, we have use the following new stochastic mapping Cpâ€² :
Cpâ€²(x) =
(
1
pâ€² C(x),
with probability pâ€²,
0,
with probability 1 âˆ’pâ€².
(13)
The following simple result states that the new mapping Cpâ€² is also an unbiased compressor, which
means that our theory applies to this choice as well.
Theorem D.1. If C âˆˆU(Ï‰), then Cpâ€² âˆˆU

Ï‰+1
pâ€²
âˆ’1

.
In the case of partial participation, all theorems from Section 6 will hold with Ï‰ replaced by
(Ï‰+1)/pâ€² âˆ’1.
E
AUXILIARY FACTS
In this section, we recall wellâ€“known auxiliary facts that we use in the proofs.
19

Published as a conference paper at ICLR 2023
1. For all x, y âˆˆRd, we have
âˆ¥x + yâˆ¥2 â‰¤2 âˆ¥xâˆ¥2 + 2 âˆ¥yâˆ¥2
(14)
2. Let us take a random vector Î¾ âˆˆRd, then
E
h
âˆ¥Î¾âˆ¥2i
= E
h
âˆ¥Î¾ âˆ’E [Î¾]âˆ¥2i
+ âˆ¥E [Î¾]âˆ¥2 .
(15)
F
COMPRESSORS FACTS
Deï¬nition F.1. Let us take a random subset S from [d], |S| = K, K âˆˆ[d]. We say that a stochastic
mapping C : Rd â†’Rd is RandK if
C(x) = d
K
X
jâˆˆS
xjej,
where {ei}d
i=1 is the standard unit basis.
Informally, RandK randomly keeps K coordinates and zeroes out the other.
Theorem F.2. If C is RandK, then C âˆˆU
  d
k âˆ’1

.
See the proof in (Beznosikov et al., 2020).
In the next theorem, we show that Cpâ€²(x) from (13) is an unbiased compressor.
Theorem D.1. If C âˆˆU(Ï‰), then Cpâ€² âˆˆU

Ï‰+1
pâ€²
âˆ’1

.
Proof. First, we proof the unbiasedness:
E [Cpâ€²(x)] = pâ€²
 1
pâ€² C(x)

+ (1 âˆ’pâ€²)0 = C(x),
âˆ€x âˆˆRd.
Next, we get a bound for the variance:
E
h
âˆ¥Cpâ€²(x) âˆ’xâˆ¥2i
=
pâ€²E
"
1
pâ€² C(x) âˆ’x

2#
+ (1 âˆ’pâ€²) âˆ¥xâˆ¥2
=
pâ€²E
 1
pâ€²2 âˆ¥C(x)âˆ¥2 âˆ’2
 1
pâ€² C(x), x

+ âˆ¥xâˆ¥2

+ (1 âˆ’pâ€²) âˆ¥xâˆ¥2
=
1
pâ€² E
h
âˆ¥C(x)âˆ¥2i
âˆ’(2 âˆ’pâ€²) âˆ¥xâˆ¥2 + (1 âˆ’pâ€²) âˆ¥xâˆ¥2
=
1
pâ€² E
h
âˆ¥C(x)âˆ¥2i
âˆ’âˆ¥xâˆ¥2 .
From C âˆˆU(Ï‰), we have
E
h
âˆ¥Cpâ€²(x) âˆ’xâˆ¥2i
â‰¤
Ï‰ + 1
pâ€²
âˆ¥xâˆ¥2 âˆ’âˆ¥xâˆ¥2 =
Ï‰ + 1
pâ€²
âˆ’1

âˆ¥xâˆ¥2 .
G
POLYAK-ÅOJASIEWICZ CONDITION
In this section, we discuss our convergence rates under the (Polyak-Åojasiewicz) PÅ-condition:
Assumption G.1. A functions f satisfy (Polyak-Åojasiewicz) PÅ-condition:
âˆ¥âˆ‡f(x)âˆ¥2 â‰¥2Âµ(f(x) âˆ’f âˆ—),
âˆ€x âˆˆR,
(16)
where f âˆ—= infxâˆˆRd f(x) > âˆ’âˆž.
20

Published as a conference paper at ICLR 2023
Here we use a different notion of an Îµ-solution: it is a (random) point bx, such that E [f(bx)] âˆ’f âˆ—â‰¤Îµ.
Under this assumption, Algorithm 1 achieves a linear convergence rate O (ln (1/Îµ)) instead of a
sublinear convergence rate O (1/Îµ) in the gradient and ï¬nite-sum settings. Moreover, in the stochastic
setting, Algorithms 1 and 2 also improve dependence on Îµ. Related Theorems I.9, I.12, I.15 and I.20
are stated in Appendix I. Note that in the ï¬nite-sum and stochastic settings, Theorems I.12 and I.20
provide new SOTA theoretical convergence rates (see Table 2).
H
INTUITION BEHIND DASHA
In this section, we want to outline an intuition of differences between the proofs of DASHA and
MARINA that helps us to improve the convergence rates.
H.1
DIFFERENT SOURCES OF CONTRACTIONS
In both algorithms the proofs analyze EC
hgt+1 âˆ’âˆ‡f(xk+1)
2i
, a norm of a difference between a
gradient âˆ‡f(xk+1) and a gradient estimator gt+1. For simplicity, we assume that n = 1, then for
MARINA, we have
EC
hgt+1 âˆ’âˆ‡f(xk+1)
2i
= p
âˆ‡f(xk+1) âˆ’âˆ‡f(xk+1)
2 + (1 âˆ’p)EC
hgt + C
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk)

âˆ’âˆ‡f(xk+1)
2i
= (1 âˆ’p)EC
hgt + C
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk)

âˆ’âˆ‡f(xk+1)
2i
(4),(15)
=
(1 âˆ’p)
gt âˆ’âˆ‡f(xk)
2 + (1 âˆ’p)EC
hC
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk)

âˆ’
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk)
2i
(4)
â‰¤(1 âˆ’p)
gt âˆ’âˆ‡f(xk)
2 + (1 âˆ’p)Ï‰
âˆ‡f(xk+1) âˆ’âˆ‡f(xk)
2 .
In order to get a contraction, i.e., EC
hgt+1 âˆ’âˆ‡f(xk+1)
2i
â‰¤(1 âˆ’p)
gt âˆ’âˆ‡f(xk)
2 + Â· Â· Â· ,
MARINA has to send a full gradient âˆ‡f(xk+1) with the probability p > 0.
Now, let us look how we get a contraction in DASHA:
EC
hgt+1 âˆ’âˆ‡f(xk+1)
2i
= EC
hgt + C
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk) âˆ’a
 gt âˆ’âˆ‡f(xk)

âˆ’âˆ‡f(xk+1)
2i
= EC
hgt + C
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk) âˆ’a
 gt âˆ’âˆ‡f(xk)

âˆ’âˆ‡f(xk+1)
2i
(4),(15)
=
(1 âˆ’a)2 gt âˆ’âˆ‡f(xk)
2
+ EC
hC
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk) âˆ’a
 gt âˆ’âˆ‡f(xk)

âˆ’
 âˆ‡f(xk+1) âˆ’âˆ‡f(xk) âˆ’a
 gt âˆ’âˆ‡f(xk)
2i
(4)
â‰¤(1 âˆ’a)2 gt âˆ’âˆ‡f(xk)
2 + Ï‰
âˆ‡f(xk+1) âˆ’âˆ‡f(xk) âˆ’a
 gt âˆ’âˆ‡f(xk)
2
(14)
â‰¤
 (1 âˆ’a)2 + 2Ï‰a2 gt âˆ’âˆ‡f(xk)
2 + 2Ï‰
âˆ‡f(xk+1) âˆ’âˆ‡f(xk)
2
â‰¤(1 âˆ’a)
gt âˆ’âˆ‡f(xk)
2 + 2Ï‰
âˆ‡f(xk+1) âˆ’âˆ‡f(xk)
2 .
In the last inequality we use that a â‰¤1/2Ï‰+1. On can see that we get exactly the same recursion and
contraction. The source of contraction is a correction âˆ’a(gt âˆ’âˆ‡f(xk)) inside the compressor C.
H.2
THE SOURCE OF IMPROVEMENTS IN THE CONVERGENCE RATES
Let us brieï¬‚y explain why we get the improvements in the convergence rates of DASHA in the
ï¬nite-sum setting. The same intuitions implies to the stochastic setting.
In DASHA, we reduce variances from the compressors C and the random sampling It
j separately:
we have two different control variables ht
i and gt
i, two different parameters the probability p and
21

Published as a conference paper at ICLR 2023
the momentum a. For simplicity, let us assume that the number of nodes n = 1. Let us consider a
Lyapunov function from our proofs:
E

f(xt) âˆ’f âˆ—
+ Î³ (4Ï‰ + 1) E
hgt âˆ’ht2i
+ Î³
1
p + 16Ï‰ (2Ï‰ + 1)

E
hht âˆ’âˆ‡f(xt)
2i
.
In contrast, MARINA (VR-MARINA) has only one control variable gt
i and on parameter p. A Lyapunov
function of MARINA is
E

f(xt) âˆ’f âˆ—
+ Î³
2pE
hgt âˆ’âˆ‡f(xt)
2i
.
MARINA has a simpler Lyapunov function that leads to a suboptimal convergence rate. Intu-
itively, having one control variable and one parameter is not enough to reduce variances from
two different sources of randomness. So in DASHA, the parameter p =
B
m+B , while in MARINA
p = min
n
B
m+B , Î¶C
d
o
, because the parameter p of MARINA helps to reduce the variance from the
compressors C.
I
THEOREMS WITH PROOFS
Lemma I.1. Suppose that Assumption 5.2 holds and let xt+1 = xt âˆ’Î³gt. Then for any gt âˆˆRd and
Î³ > 0, we have
f(xt+1) â‰¤f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
2
gt âˆ’âˆ‡f(xt)
2 .
(17)
The proof of Lemma I.1 is provided in (Li et al., 2021a).
There are two different sources of randomness in Algorithm 1: the ï¬rst one from vectors {ht+1
i
}n
i=1
and the second one from compressors {Ci}n
i=1. In this section, we deï¬ne Eh [Â·] and EC [Â·] to be
conditional expectations w.r.t. {ht+1
i
}n
i=1 and {Ci}n
i=1, accordingly, conditioned on all previous
randomness.
Lemma I.2. Suppose that Assumption 1.2 holds and let us consider sequences gt+1
i
and ht+1
i
from
Algorithm 1, then
EC
hgt+1 âˆ’ht+12i
â‰¤2Ï‰
n2
n
X
i=1
ht+1
i
âˆ’ht
i
2 + 2a2Ï‰
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt âˆ’ht2 ,
(18)
and
EC
hgt+1
i
âˆ’ht+1
i
2i
â‰¤2Ï‰
ht+1
i
âˆ’ht
i
2 +

2a2Ï‰ + (1 âˆ’a)2 gt
i âˆ’ht
i
2 ,
âˆ€i âˆˆ[n].
(19)
Proof. First, we estimate EC
hgt+1 âˆ’ht+12i
:
EC
hgt+1 âˆ’ht+12i
= EC
ï£®
ï£°
gt + 1
n
n
X
i=1
Ci
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’ht+1

2ï£¹
ï£»
(4),(15)
=
EC
ï£®
ï£°

1
n
n
X
i=1
Ci
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’1
n
n
X
i=1
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i


2ï£¹
ï£»
+ (1 âˆ’a)2 gt âˆ’ht2 .
Using the independence of compressors and (4), we get
EC
hgt+1 âˆ’ht+12i
22

Published as a conference paper at ICLR 2023
= 1
n2
n
X
i=1
EC
hCi
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2i
+ (1 âˆ’a)2 gt âˆ’ht2
â‰¤Ï‰
n2
n
X
i=1
ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt âˆ’ht2
â‰¤2Ï‰
n2
n
X
i=1
ht+1
i
âˆ’ht
i
2 + 2a2Ï‰
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt âˆ’ht2 .
Analogously, we can get the bound for EC
hgt+1
i
âˆ’ht+1
i
2i
:
EC
hgt+1
i
âˆ’ht+1
i
2i
= EC
hgt
i + Ci
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’ht+1
i
2i
= EC
hCi
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’
 ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2i
+ (1 âˆ’a)2 gt
i âˆ’ht
i
2
â‰¤Ï‰
ht+1
i
âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt
i âˆ’ht
i
2
â‰¤2Ï‰
ht+1
i
âˆ’ht
i
2 + 2a2Ï‰
gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt
i âˆ’ht
i
2
= 2Ï‰
ht+1
i
âˆ’ht
i
2 +

2a2Ï‰ + (1 âˆ’a)2 gt
i âˆ’ht
i
2 .
Lemma I.3. Suppose that Assumptions 5.2 and 1.2 hold and let us take a = 1/ (2Ï‰ + 1) , then
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 8Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Proof. Due to Lemma I.1 and the update step from Line 4 in Algorithm 1, we have
E

f(xt+1)

â‰¤
E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
2
gt âˆ’âˆ‡f(xt)
2

=
E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
2
gt âˆ’ht + ht âˆ’âˆ‡f(xt)
2

(20)
â‰¤
E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
gt âˆ’ht2 +
ht âˆ’âˆ‡f(xt)
2i
.
In the last inequality we use Jensenâ€™s inequality (14). Let us ï¬x some constants Îº, Î· âˆˆ[0, âˆž) that we
will deï¬ne later. Combining bounds (20), (18), (19) and using the law of total expectation, we get
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
gt âˆ’ht2 +
ht âˆ’âˆ‡f(xt)
2
23

Published as a conference paper at ICLR 2023
+ ÎºE
"
2Ï‰
n2
n
X
i=1
ht+1
i
âˆ’ht
i
2 + 2a2Ï‰
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’a)2 gt âˆ’ht2
#
+ Î·E
"
2Ï‰
n
n
X
i=1
ht+1
i
âˆ’ht
i
2 +

2a2Ï‰ + (1 âˆ’a)2 1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
= E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+

Î³ + Îº (1 âˆ’a)2
E
hgt âˆ’ht2i
+
2Îºa2Ï‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2ÎºÏ‰
n
+ 2Î·Ï‰

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
(21)
Now, by taking Îº = Î³
a, we can see that Î³ + Îº (1 âˆ’a)2 â‰¤Îº, and thus
E

f(xt+1)

+ Î³
aE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³
aE
hgt âˆ’ht2i
+
2Î³aÏ‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2Î³Ï‰
an + 2Î·Ï‰

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Next,
by taking Î·
=
2Î³Ï‰
n
and considering the choice of a,
one can show that

2Î³aÏ‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
â‰¤Î·. Thus
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2Î³Ï‰ (2Ï‰ + 1)
n
+ 4Î³Ï‰2
n

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 8Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
24

Published as a conference paper at ICLR 2023
The following lemma almost repeats the previous one. We will use it in the theorems with Assumption
G.1.
Lemma I.4. Suppose that Assumptions 5.2, 1.2 and G.1 hold and let us take a = 1/ (2Ï‰ + 1) and
Î³ â‰¤
a
2Âµ, then
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 20Î³Ï‰(2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Proof. Up to (21) we can follow the proof of Lemma I.3 to get
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+

Î³ + Îº (1 âˆ’a)2
E
hgt âˆ’ht2i
+
2Îºa2Ï‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2ÎºÏ‰
n
+ 2Î·Ï‰

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Now, by taking Îº = 2Î³
a , we can see that Î³ + Îº (1 âˆ’a)2 â‰¤
 1 âˆ’a
2

Îº, and thus
E

f(xt+1)

+ 2Î³
a E
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+

1 âˆ’a
2
 2Î³
a E
hgt âˆ’ht2i
+
4Î³aÏ‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
4Î³Ï‰
an + 2Î·Ï‰

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Next,
by taking Î·
=
8Î³Ï‰
n
and considering the choice of a,
one can show that

4Î³aÏ‰
n
+ Î·

2a2Ï‰ + (1 âˆ’a)2
â‰¤
 1 âˆ’a
2

Î·. Thus
E

f(xt+1)

25

Published as a conference paper at ICLR 2023
+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+

1 âˆ’a
2

2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+

1 âˆ’a
2
 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
4Î³Ï‰(2Ï‰ + 1)
n
+ 16Î³Ï‰2
n

E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+

1 âˆ’a
2

2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+

1 âˆ’a
2
 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 20Î³Ï‰(2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’ht
i
2
#
.
Finally, the assumption Î³ â‰¤
a
2Âµ implies an inequality 1 âˆ’a
2 â‰¤1 âˆ’Î³Âµ.
Lemma I.5. Suppose that Assumption 5.1 holds and
E

f(xt+1)

+ Î³Î¨t+1 â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³Î¨t + Î³C,
(22)
where Î¨t is a sequence of numbers, Î¨t â‰¥0 for all t âˆˆ[T], constant C â‰¥0, and constant Î³ > 0.
Then
E
hâˆ‡f(bxT )
2i
â‰¤2
 f(x0) âˆ’f âˆ—
Î³T
+ 2Î¨0
T
+ 2C,
(23)
where a point bxT is chosen uniformly from a set of points {xt}T âˆ’1
t=0 .
Proof. By unrolling (22) for t from 0 to T âˆ’1, we obtain
Î³
2
T âˆ’1
X
t=0
E
hâˆ‡f(xt)
2i
+ E

f(xT )

+ Î³Î¨T â‰¤f(x0) + Î³Î¨0 + Î³TC.
We subtract f âˆ—, divide inequality by Î³T
2 , and take into account that f(x) â‰¥f âˆ—for all x âˆˆR, and
Î¨t â‰¥0 for all t âˆˆ[T], to get the following inequality:
1
T
T âˆ’1
X
t=0
E
hâˆ‡f(xt)
2i
â‰¤2
 f(x0) âˆ’f âˆ—
Î³T
+ 2Î¨0
T
+ 2C.
It is left to consider the choice of a point bxT to complete the proof of the lemma.
Lemma I.6. Suppose that Assumptions 5.1 and G.1 hold and
E

f(xt+1)

+ Î³Î¨t+1 â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ)Î³Î¨t + Î³C,
where Î¨t is a sequence of numbers, Î¨t â‰¥0 for all t âˆˆ[T], constant C â‰¥0, constant Âµ > 0, and
constant Î³ âˆˆ(0, 1/Âµ). Then
E

f(xT ) âˆ’f âˆ—
â‰¤(1 âˆ’Î³Âµ)T   f(x0) âˆ’f âˆ—
+ Î³Î¨0
+ C
Âµ .
(24)
26

Published as a conference paper at ICLR 2023
Proof. We subtract f âˆ—and use PÅ-condition (16) to get
E

f(xt+1) âˆ’f âˆ—
+ Î³Î¨t+1
â‰¤
E

f(xt) âˆ’f âˆ—
âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³Î¨t + Î³C
â‰¤
(1 âˆ’Î³Âµ)E

f(xt) âˆ’f âˆ—
+ (1 âˆ’Î³Âµ)Î³Î¨t + Î³C
=
(1 âˆ’Î³Âµ)
 E

f(xt) âˆ’f âˆ—
+ Î³Î¨t
+ Î³C.
Unrolling the inequality, we have
E

f(xt+1) âˆ’f âˆ—
+ Î³Î¨t+1
â‰¤
(1 âˆ’Î³Âµ)t+1   f(x0) âˆ’f âˆ—
+ Î³Î¨0
+ Î³C
t
X
i=0
(1 âˆ’Î³Âµ)i
â‰¤
(1 âˆ’Î³Âµ)t+1   f(x0) âˆ’f âˆ—
+ Î³Î¨0
+ C
Âµ .
It is left to note that Î¨t â‰¥0 for all t âˆˆ[T].
Lemma I.7. If 0 < Î³ â‰¤(L +
âˆš
A)âˆ’1, L > 0, and A â‰¥0, then
1
2Î³ âˆ’L
2 âˆ’Î³A
2
â‰¥0.
It is easy to verify with a direct calculation.
I.1
CASE OF DASHA
Despite the triviality of the following lemma, we provide it for consistency with Lemma I.14 and
Lemma I.11.
Lemma I.8. Suppose that Assumption 5.3 holds. Assuming that h0
i = âˆ‡fi(x0) for all i âˆˆ[n], for
ht+1
i
from Algorithm 1 (DASHA) we have
1.
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
= 0.
2.
Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
= 0,
âˆ€i âˆˆ[n].
3.
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤L2
i
xt+1 âˆ’xt2 ,
âˆ€i âˆˆ[n].
Theorem 6.1. Suppose that Assumptions 5.1, 5.2, 5.3 and 1.2 hold. Let us take a = 1/ (2Ï‰ + 1) and
Î³ â‰¤

L +
q
16Ï‰(2Ï‰+1)
n
bL
âˆ’1
, and h0
i = âˆ‡fi(x0) for all i âˆˆ[n] in Algorithm 1 (DASHA), then
E
hâˆ‡f(bxT )
2i
â‰¤1
T
"
2
 f(x0) âˆ’f âˆ—
 
L +
r
16Ï‰ (2Ï‰ + 1)
n
bL
!
+ 2 (2Ï‰ + 1)
g0 âˆ’âˆ‡f(x0)
2 + 4Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’âˆ‡fi(x0)
2
! #
.
Proof. Considering Lemma I.3, Lemma I.8, and the law of total expectation, we obtain
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2
27

Published as a conference paper at ICLR 2023
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 8Î³Ï‰ (2Ï‰ + 1)
n
bL2 xt+1 âˆ’xt2
= E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
 1
2Î³ âˆ’L
2 âˆ’8Î³Ï‰ (2Ï‰ + 1)
n
bL2

E
hxt+1 âˆ’xt2i
.
Using assumption about Î³, we can show that
1
2Î³ âˆ’L
2 âˆ’8Î³Ï‰(2Ï‰+1)
n
bL2 â‰¥0 (see Lemma I.7), thus
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
.
In the view of Lemma I.5 with Î¨t = (2Ï‰ + 1) E
h
âˆ¥gt âˆ’htâˆ¥2i
+ 2Ï‰
n E
h
1
n
Pn
i=1 âˆ¥gt
i âˆ’ht
iâˆ¥2i
we can
conclude the proof.
Corollary 6.2. Suppose that assumptions from Theorem 6.1 hold, and g0
i = h0
i = âˆ‡fi(x0) for all
i âˆˆ[n], then DASHA needs T := O
 
1
Îµ
"
 f(x0) âˆ’f âˆ— 
L +
Ï‰
âˆšn bL
 #!
communication rounds
to get an Îµ-solution and the communication complexity is equal to O (d + Î¶CT) , where Î¶C is the
expected density from Deï¬nition 1.3.
Proof. The communication complexities can be easily derived using Theorem 6.1. At each commu-
nication round of Algorithm 1, each node sends Î¶C coordinates. In the view of g0
i = âˆ‡fi(x0) for
all i âˆˆ[n], we additionally have to send d coordinates from the nodes to the server, thus the total
communication complexity would be O (d + Î¶CT) .
Corollary 6.3. Suppose that assumptions of Corollary 6.2 hold. We take the unbiased compressor
RandK with K = Î¶C â‰¤d/âˆšn, then the communication complexity equals O

d +
bL(f(x0)âˆ’f âˆ—)d
Îµâˆšn

.
Proof. In the view of Theorem F.2, we have Ï‰ + 1 = d/K. Combining this and an inequality L â‰¤bL,
the communication complexity equals
O (d + Î¶CT)
=
O
 
d + 1
Îµ
"
 f(x0) âˆ’f âˆ— 
KL + K Ï‰
âˆšn
bL
 #!
=
O
 
d + 1
Îµ
"
 f(x0) âˆ’f âˆ—  d
âˆšnL + d
âˆšn
bL
 #!
=
O
 
d + 1
Îµ
"
 f(x0) âˆ’f âˆ—  d
âˆšn
bL
 #!
.
I.2
CASE OF DASHA UNDER PÅ-CONDITION
Theorem I.9. Suppose that Assumption 5.1, 5.2, 5.3, 1.2 and G.1 hold. Let us take a = 1/ (2Ï‰ + 1) ,
Î³ â‰¤min
(
L +
q
40Ï‰(2Ï‰+1)
n
bL
âˆ’1
, a
2Âµ
)
, and h0
i = âˆ‡fi(x0) for all i âˆˆ[n] in Algorithm 1
28

Published as a conference paper at ICLR 2023
(DASHA), then
E

f(xT ) âˆ’f âˆ—
â‰¤(1 âˆ’Î³Âµ)T
 
 f(x0) âˆ’f âˆ—
+ 2Î³(2Ï‰ + 1)
g0 âˆ’âˆ‡f(x0)
2 + 8Î³Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’âˆ‡fi(x0)
2
!!
.
Proof. Considering Lemma I.4, Lemma I.8, and the law of total expectation, we obtain
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 20Î³Ï‰(2Ï‰ + 1)
n
bL2 xt+1 âˆ’xt2
= E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
 1
2Î³ âˆ’L
2 âˆ’20Î³Ï‰(2Ï‰ + 1)
n
bL2
 xt+1 âˆ’xt2 .
Using the assumption about Î³, we can show that
1
2Î³ âˆ’L
2 âˆ’20Î³Ï‰(2Ï‰+1)
n
bL2 â‰¥0 (see Lemma I.7), thus
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
.
In the view of Lemma I.6 with Î¨t = 2(2Ï‰ + 1)E
h
âˆ¥gt âˆ’htâˆ¥2i
+ 8Ï‰
n E
h
1
n
Pn
i=1 âˆ¥gt
i âˆ’ht
iâˆ¥2i
we
can conclude the proof.
We use e
O (Â·), when we provide a bound up to logarithmic factors.
Corollary I.10. Suppose that assumptions from Theorem I.9 hold, and g0
i = 0 for all i âˆˆ[n], then
DASHA needs
T := e
O
 
Ï‰ + L
Âµ + Ï‰bL
Âµâˆšn
!
.
(25)
communication rounds to get an Îµ-solution and the communication complexity is equal to O (Î¶CT) ,
where Î¶C is the expected density from Deï¬nition 1.3.
Proof. Clearly, using Theorem I.9, one can show that Algorithm 1 returns an Îµ-solution after
(25) communication rounds. At each communication round of Algorithm 1, each node sends
Î¶C coordinates, thus the total communication complexity would be O (Î¶CT) per node. Unlike
Corollary 6.2, in this corollary, we can initialize g0
i , for instance, with zeros because the corresponding
initialization error Î¨0 from the proof of Theorem I.9 would be under the logarithm.
29

Published as a conference paper at ICLR 2023
I.3
CASE OF DASHA-PAGE
Lemma I.11. Suppose that Assumptions 5.3 and 5.4 hold. For ht+1
i
from Algorithm 1 (DASHA-PAGE)
we have
1.
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤(1 âˆ’p) L2
max
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2 .
2.
Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
â‰¤(1 âˆ’p) L2
max
B
xt+1 âˆ’xt2 + (1 âˆ’p)
ht
i âˆ’âˆ‡fi(xt)
2 ,
âˆ€i âˆˆ[n].
3.
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤
(1 âˆ’p)L2
max
B
+ 2L2
i
 xt+1 âˆ’xt2 + 2p
ht
i âˆ’âˆ‡fi(xt)
2 ,
âˆ€i âˆˆ[n].
Proof. Using the deï¬nition of ht+1, we obtain
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
= (1 âˆ’p) Eh
ï£®
ï£¯ï£°

ht + 1
n
n
X
i=1
1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’âˆ‡f(xt+1)

2ï£¹
ï£ºï£»
(15)
= (1 âˆ’p) Eh
ï£®
ï£¯ï£°

1
n
n
X
i=1
1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡f(xt+1) âˆ’âˆ‡f(xt)


2ï£¹
ï£ºï£»
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2 .
From the unbiasedness and independence of mini-batch samples, we get
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤(1 âˆ’p)
n2B2
n
X
i=1
Eh
ï£®
ï£°X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
ï£¹
ï£»
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
= (1 âˆ’p)
n2B
n
X
i=1
ï£«
ï£­1
m
m
X
j=1
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
ï£¶
ï£¸
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
â‰¤(1 âˆ’p)
n2B
n
X
i=1
ï£«
ï£­1
m
m
X
j=1
âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)
2
ï£¶
ï£¸
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
â‰¤(1 âˆ’p) L2
max
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2 .
In the last inequality, we use Assumption 5.4. Using the same reasoning, we have
Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
30

Published as a conference paper at ICLR 2023
= (1 âˆ’p) Eh
ï£®
ï£¯ï£°

ht
i + 1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’âˆ‡fi(xt+1)

2ï£¹
ï£ºï£»
= (1 âˆ’p) Eh
ï£®
ï£¯ï£°

1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡f(xt+1) âˆ’âˆ‡f(xt)


2ï£¹
ï£ºï£»
+ (1 âˆ’p)
ht
i âˆ’âˆ‡fi(xt)
2
â‰¤(1 âˆ’p) L2
max
B
xt+1 âˆ’xt2 + (1 âˆ’p)
ht
i âˆ’âˆ‡fi(xt)
2 .
Finally, we consider the last ineqaulity of the lemma:
Eh
hht+1
i
âˆ’ht
i
2i
= p
âˆ‡fi(xt+1) âˆ’ht
i
2 + (1 âˆ’p)Eh
ï£®
ï£¯ï£°

ht
i + 1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’ht
i

2ï£¹
ï£ºï£»
(15)
= p
âˆ‡fi(xt+1) âˆ’ht
i
2
+ (1 âˆ’p)Eh
ï£®
ï£¯ï£°

1
B
X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)


2ï£¹
ï£ºï£»
+ (1 âˆ’p)
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2 .
Using the unbiasedness and independence of the gradients, we obtain
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤p
âˆ‡fi(xt+1) âˆ’ht
i
2
+ (1 âˆ’p)
B2
Eh
ï£®
ï£°X
jâˆˆIt
i
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
ï£¹
ï£»
+ (1 âˆ’p)
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
= p
âˆ‡fi(xt+1) âˆ’ht
i
2
+ (1 âˆ’p)
B
ï£«
ï£­1
m
m
X
j=1
 âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
ï£¶
ï£¸
+ (1 âˆ’p)
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
â‰¤p
âˆ‡fi(xt+1) âˆ’ht
i
2
+ (1 âˆ’p)
B
ï£«
ï£­1
m
m
X
j=1
âˆ‡fij(xt+1) âˆ’âˆ‡fij(xt)
2
ï£¶
ï£¸
+ (1 âˆ’p)
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2 .
From Assumptions 5.3 and 5.4, we can conclude that
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤p
âˆ‡fi(xt+1) âˆ’ht
i
2 + (1 âˆ’p)
L2
max
B
+ L2
i
 xt+1 âˆ’xt2
31

Published as a conference paper at ICLR 2023
= p
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt) + âˆ‡fi(xt) âˆ’ht
i
2 + (1 âˆ’p)
L2
max
B
+ L2
i
 xt+1 âˆ’xt2
(14)
â‰¤2p
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2 + 2p
ht
i âˆ’âˆ‡fi(xt)
2 + (1 âˆ’p)
L2
max
B
+ L2
i
 xt+1 âˆ’xt2
â‰¤2pL2
i
xt+1 âˆ’xt2 + 2p
ht
i âˆ’âˆ‡fi(xt)
2 + (1 âˆ’p)
L2
max
B
+ L2
i
 xt+1 âˆ’xt2
â‰¤
(1 âˆ’p)L2
max
B
+ 2L2
i
 xt+1 âˆ’xt2 + 2p
ht
i âˆ’âˆ‡fi(xt)
2 .
Theorem 6.4. Suppose that Assumptions 5.1, 5.2, 5.3, 5.4, and 1.2 hold. Let us take a = 1/ (2Ï‰ + 1),
probability p âˆˆ(0, 1], and
Î³ â‰¤
 
L +
s
48Ï‰ (2Ï‰ + 1)
n
(1 âˆ’p)L2max
B
+ bL2

+ 2 (1 âˆ’p) L2max
pnB
!âˆ’1
in Algorithm 1 (DASHA-PAGE) then
E
hâˆ‡f(bxT )
2i
â‰¤1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
Ã—
 
L +
s
48Ï‰ (2Ï‰ + 1)
n
(1 âˆ’p)L2max
B
+ bL2

+ 2 (1 âˆ’p) L2max
pnB
!
+ 2 (2Ï‰ + 1)
g0 âˆ’h02 + 4Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’h0
i
2
!
+ 2
p
h0 âˆ’âˆ‡f(x0)
2 + 32Ï‰ (2Ï‰ + 1)
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£».
Proof. Let us ï¬x constants Î½, Ï âˆˆ[0, âˆž) that we will deï¬ne later.
Considering Lemma I.3,
Lemma I.11, and the law of total expectation, we obtain
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 8Î³Ï‰ (2Ï‰ + 1)
n
E
"(1 âˆ’p)L2
max
B
+ 2bL2
 xt+1 âˆ’xt2 + 2p 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ Î½E
(1 âˆ’p) L2
max
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2

+ ÏE
"
(1 âˆ’p) L2
max
B
xt+1 âˆ’xt2 + (1 âˆ’p) 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
32

Published as a conference paper at ICLR 2023
After rearranging the terms, we get
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
8Î³Ï‰ (2Ï‰ + 1)

(1âˆ’p)L2
max
B
+ 2bL2
n
âˆ’Î½ (1 âˆ’p) L2
max
nB
âˆ’Ï(1 âˆ’p) L2
max
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+ (Î³ + Î½(1 âˆ’p)) E
hht âˆ’âˆ‡f(xt)
2i
+
16Î³pÏ‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’p)

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
Next, let us ï¬x Î½ = Î³
p, to get
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
8Î³Ï‰ (2Ï‰ + 1)

(1âˆ’p)L2
max
B
+ 2bL2
n
âˆ’Î³ (1 âˆ’p) L2
max
pnB
âˆ’Ï(1 âˆ’p) L2
max
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
16Î³pÏ‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’p)

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
By taking Ï = 16Î³Ï‰(2Ï‰+1)
n
, we obtain
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 16Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ 16Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
33

Published as a conference paper at ICLR 2023
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
8Î³Ï‰ (2Ï‰ + 1)

(1âˆ’p)L2
max
B
+ 2bL2
n
âˆ’Î³ (1 âˆ’p) L2
max
pnB
âˆ’16Î³Ï‰ (2Ï‰ + 1) (1 âˆ’p) L2
max
nB

E
hxt+1 âˆ’xt2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ 16Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
24Î³Ï‰ (2Ï‰ + 1)

(1âˆ’p)L2
max
B
+ bL2
n
âˆ’Î³ (1 âˆ’p) L2
max
pnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
.
Next, considering the choice of Î³ and Lemma I.7, we get
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 16Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ 16Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
Finally, in the view of Lemma I.5 with
Î¨t
=
(2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
1
pE
hht âˆ’âˆ‡f(xt)
2i
+ 16Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
,
we can conclude the proof.
Corollary 6.5. Let the assumptions from Theorem 6.4 hold, p = B/(m+B), and g0
i = h0
i = âˆ‡fi(x0)
for all i âˆˆ[n]. Then DASHA-PAGE needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— 
L + Ï‰
âˆšn
bL +
 Ï‰
âˆšn +
r m
nB
 Lmax
âˆš
B

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the expected # of gradient calculations per node equals O (m + BT) , where Î¶C is the expected
density from Deï¬nition 1.3.
Proof. Corollary 6.5 can be proved in the same way as Corollary 6.2. One only should note that the
expected number of gradients calculations at each communication round equals pm + (1 âˆ’p)B =
2mB
m+B â‰¤2B.
34

Published as a conference paper at ICLR 2023
Corollary 6.6. Suppose that assumptions of Corollary 6.5 hold, B â‰¤
p
m/n, and we use the unbiased
compressor RandK with K = Î¶C = Î˜ (Bd/âˆšm) . Then the communication complexity of Algorithm 1
is
O
 
d + Lmax
 f(x0) âˆ’f âˆ—
d
Îµâˆšn
!
,
(7)
and the expected # of gradient calculations per node equals
O
 
m + Lmax
 f(x0) âˆ’f âˆ— âˆšm
Îµâˆšn
!
.
(8)
Proof. In the view of Theorem F.2, we have Ï‰ + 1 = d/K. Combining this, inequalities L â‰¤bL â‰¤
Lmax, and K = Î˜

Bd
âˆšm

= O

d
âˆšn

, we can show that the communication complexity equals
O (d + Î¶CT)
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— 
KL + K Ï‰
âˆšn
bL + K
 Ï‰
âˆšn +
r m
nB
 Lmax
âˆš
B

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšnL + d
âˆšn
bL + d
âˆšnLmax

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšnLmax

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
And the expected number of gradient calculations per node equals
O (m + BT)
=
O
ï£«
ï£¬
ï£¬
ï£­m + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— 
BL + B Ï‰
âˆšn
bL + B
 Ï‰
âˆšn +
r m
nB
 Lmax
âˆš
B

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­m + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— rm
n L +
rm
n
bL +
rm
n Lmax

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­m + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ— rm
n Lmax

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
I.4
CASE OF DASHA-PAGE UNDER PÅ-CONDITION
Theorem I.12. Suppose that Assumption 5.1, 5.2, 5.3, 1.2, 5.4, and G.1 hold.
Let
us take a
=
1/ (2Ï‰ + 1) , probability p
âˆˆ
(0, 1], batch size B
âˆˆ
[m], and
Î³ â‰¤min
(
L +
r
200Ï‰(2Ï‰+1)
n

(1âˆ’p)L2
max
B
+ 2bL2

+ 4(1âˆ’p)L2max
pnB
âˆ’1
, a
2Âµ, p
2Âµ
)
in Algorithm 1
(DASHA-PAGE), then
E

f(xT ) âˆ’f âˆ—
â‰¤
(1 âˆ’Î³Âµ)T
ï£«
ï£¬
ï£¬
ï£­(f(x0) âˆ’f âˆ—) + 2Î³(2Ï‰ + 1)
g0 âˆ’h02 + 8Î³Ï‰
n
1
n
n
X
i=1
g0
i âˆ’h0
i
2
35

Published as a conference paper at ICLR 2023
+
2Î³
p
h0 âˆ’âˆ‡f(x0)
2 + 80Î³Ï‰ (2Ï‰ + 1)
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¶
ï£·
ï£·
ï£¸.
Proof. Let us ï¬x constants Î½, Ï âˆˆ[0, âˆž) that we will deï¬ne later.
Considering Lemma I.4,
Lemma I.11, and the law of total expectation, we obtain
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 20Î³Ï‰(2Ï‰ + 1)
n
E
"(1 âˆ’p)L2
max
B
+ 2bL2
 xt+1 âˆ’xt2 + 2p 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ Î½E
(1 âˆ’p) L2
max
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2

+ ÏE
"
(1 âˆ’p) L2
max
B
xt+1 âˆ’xt2 + (1 âˆ’p) 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
After rearranging the terms, we get
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
 1
2Î³ âˆ’L
2 âˆ’20Î³Ï‰(2Ï‰ + 1)
n
(1 âˆ’p)L2
max
B
+ 2bL2

âˆ’Î½ (1 âˆ’p) L2
max
nB
âˆ’Ï(1 âˆ’p) L2
max
B

E
hxt+1 âˆ’xt2i
+ (Î³ + Î½(1 âˆ’p)) E
hht âˆ’âˆ‡f(xt)
2i
+
40pÎ³Ï‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’p)

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
By taking Î½ = 2Î³
p and Ï = 80Î³Ï‰(2Ï‰+1)
n
, one can see that Î³+Î½(1âˆ’p) â‰¤
 1 âˆ’p
2

Î½ and 40pÎ³Ï‰(2Ï‰+1)
n
+
Ï(1 âˆ’p) â‰¤
 1 âˆ’p
2

Ï, thus
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
36

Published as a conference paper at ICLR 2023
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’p
2
 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+

1 âˆ’p
2
 80Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
 1
2Î³ âˆ’L
2 âˆ’20Î³Ï‰(2Ï‰ + 1)
n
(1 âˆ’p)L2
max
B
+ 2bL2

âˆ’2Î³ (1 âˆ’p) L2
max
pnB
âˆ’80Î³Ï‰ (2Ï‰ + 1) (1 âˆ’p) L2
max
nB

E
hxt+1 âˆ’xt2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’p
2
 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+

1 âˆ’p
2
 80Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
 1
2Î³ âˆ’L
2 âˆ’100Î³Ï‰(2Ï‰ + 1)
n
(1 âˆ’p)L2
max
B
+ 2bL2

âˆ’2Î³ (1 âˆ’p) L2
max
pnB

E
hxt+1 âˆ’xt2i
.
Next, considering the choice of Î³ and Lemma I.7, we get
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ (1 âˆ’Î³Âµ) 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 80Î³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
In the view of Lemma I.6 with
Î¨t
=
2(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 8Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2
pE
hht âˆ’âˆ‡f(xt)
2i
+ 80Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
,
we can conclude the proof of the theorem.
Corollary I.13. Suppose that assumptions from Theorem I.12 hold, probability p = B/(m + B),
and h0
i = g0
i = 0 for all i âˆˆ[n], then DASHA-PAGE needs
T := e
O
 
Ï‰ + m
B + L
Âµ + Ï‰bL
Âµâˆšn +
 Ï‰
âˆšn +
âˆšm
âˆš
nB
 Lmax
Âµ
âˆš
B
!
(26)
communication rounds to get an Îµ-solution, the communication complexity is equal to O (Î¶CT) , and
the expected number of gradient calculations per node equals O (BT) , where Î¶C is the expected
density from Deï¬nition 1.3.
37

Published as a conference paper at ICLR 2023
Proof. Clearly, using Theorem I.12, one can show that Algorithm 1 returns an Îµ-solution after
(26) communication rounds. At each communication round of Algorithm 1, each node sends Î¶C
coordinates, thus the total communication complexity would be O (Î¶CT) . Moreover, the expected
number of gradients calculations at each communication round equals pm+(1âˆ’p)B = 2mB
m+B â‰¤2B,
thus the total expected number of gradients that each node calculates is O (BT) . Unlike Corollary 6.5,
in this corollary, we can initialize h0
i and g0
i , for instance, with zeros because the corresponding
initialization error Î¨0 from the proof of Theorem I.12 would be under the logarithm.
I.5
CASE OF DASHA-MVR
We introduce new notations: âˆ‡fi(xt+1; Î¾t+1
i
) = 1
B
PB
j=1 âˆ‡fi(xt+1; Î¾t+1
ij ) and âˆ‡f(xt+1; Î¾t+1) =
1
n
Pn
i=1 âˆ‡fi(xt+1; Î¾t+1
i
).
Lemma I.14. Suppose that Assumptions 5.3, 5.5 and 5.6 hold. For ht+1
i
from Algorithm 1 (DASHA-
MVR) we have
1.
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤2b2Ïƒ2
nB
+ 2 (1 âˆ’b)2 L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2 .
2.
Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
â‰¤2b2Ïƒ2
B
+ 2 (1 âˆ’b)2 L2
Ïƒ
B
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht
i âˆ’âˆ‡fi(xt)
2 ,
âˆ€i âˆˆ[n].
3.
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤2b2Ïƒ2
B
+ 2
 
(1 âˆ’b)2 L2
Ïƒ
B
+ L2
i
!
xt+1 âˆ’xt2 + 2b2 ht
i âˆ’âˆ‡fi(xt)
2 ,
âˆ€i âˆˆ[n].
Proof. First, let us proof the bound for Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
:
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
= Eh
hâˆ‡f(xt+1; Î¾t+1) + (1 âˆ’b)
 ht âˆ’âˆ‡f(xt; Î¾t+1)

âˆ’âˆ‡f(xt+1)
2i
(15)
= Eh
hb
 âˆ‡f(xt+1; Î¾t+1) âˆ’âˆ‡f(xt+1)

+ (1 âˆ’b)
 âˆ‡f(xt+1; Î¾t+1) âˆ’âˆ‡f(xt+1) + âˆ‡f(xt) âˆ’âˆ‡f(xt; Î¾t+1)
2i
+ (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2
(14)
â‰¤2b2Eh
hâˆ‡f(xt+1; Î¾t+1) âˆ’âˆ‡f(xt+1)
2i
+ 2 (1 âˆ’b)2 Eh
hâˆ‡f(xt+1; Î¾t+1) âˆ’âˆ‡f(xt+1) + âˆ‡f(xt) âˆ’âˆ‡f(xt; Î¾t+1)
2i
+ (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2
= 2b2
n2
n
X
i=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1)
2i
+ 2 (1 âˆ’b)2
n2
n
X
i=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt; Î¾t+1
i
) âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2i
+ (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2
=
2b2
n2B2
n
X
i=1
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt+1)
2i
+ 2 (1 âˆ’b)2
n2B2
n
X
i=1
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt; Î¾t+1
ij ) âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2i
38

Published as a conference paper at ICLR 2023
+ (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2 .
Using Assumptions 5.5 and 5.6, we obtain
Eh
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤2b2Ïƒ2
nB
+ 2 (1 âˆ’b)2 L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2 .
Similarly, we can get the bound for Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
:
Eh
hht+1
i
âˆ’âˆ‡fi(xt+1)
2i
= Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) + (1 âˆ’b)
 ht
i âˆ’âˆ‡fi(xt; Î¾t+1
i
)

âˆ’âˆ‡fi(xt+1)
2i
= E
hb
 âˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1)

+ (1 âˆ’b)
 âˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1) + âˆ‡f(xt) âˆ’âˆ‡fi(xt; Î¾t+1
i
)
2i
+ (1 âˆ’b)2 ht
i âˆ’âˆ‡fi(xt)
2
â‰¤2b2Ïƒ2
B
+ 2 (1 âˆ’b)2 L2
Ïƒ
B
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht
i âˆ’âˆ‡fi(xt)
2 .
Now, we proof the last inequality of the lemma:
Eh
hht+1
i
âˆ’ht
i
2i
= Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) + (1 âˆ’b)
 ht
i âˆ’âˆ‡fi(xt; Î¾t+1
i
)

âˆ’ht
i
2i
(15)
= Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1) + (1 âˆ’b)
 âˆ‡fi(xt) âˆ’âˆ‡fi(xt; Î¾t+1
i
)
2i
+
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt) âˆ’b
 ht
i âˆ’âˆ‡fi(xt)
2
= Eh
hb
 âˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1)

+ (1 âˆ’b)
 âˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt; Î¾t+1
i
) âˆ’(âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt))
2i
+
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt) âˆ’b
 ht
i âˆ’âˆ‡fi(xt)
2
(14)
â‰¤2b2Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt+1)
2i
+ 2 (1 âˆ’b)2 Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt; Î¾t+1
i
) âˆ’(âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt))
2i
+ 2
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2 + 2b2 ht
i âˆ’âˆ‡fi(xt)
2
= 2b2
B2
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt+1)
2i
+ 2 (1 âˆ’b)2
B2
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt; Î¾t+1
ij ) âˆ’(âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt))
2i
+ 2
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2 + 2b2 ht
i âˆ’âˆ‡fi(xt)
2
In the view of Assumptions 5.3, 5.5 and 5.6, we obtain
Eh
hht+1
i
âˆ’ht
i
2i
â‰¤2b2Ïƒ2
B
+ 2 (1 âˆ’b)2 L2
Ïƒ
B
xt+1 âˆ’xt2 + 2L2
i
xt+1 âˆ’xt2 + 2b2 ht
i âˆ’âˆ‡fi(xt)
2 .
Theorem 6.7. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2Ï‰+1, b âˆˆ(0, 1], and Î³ â‰¤

L +
r
96Ï‰(2Ï‰+1)
n

(1âˆ’b)2L2Ïƒ
B
+ bL2

+ 4(1âˆ’b)2L2Ïƒ
bnB
âˆ’1
, in Algorithm 1
(DASHA-MVR). Then
E
hâˆ‡f(bxT )
2i
â‰¤1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
39

Published as a conference paper at ICLR 2023
Ã—
ï£«
ï£­L +
v
u
u
t96Ï‰ (2Ï‰ + 1)
n
 
(1 âˆ’b)2 L2Ïƒ
B
+ bL2
!
+ 4 (1 âˆ’b)2 L2Ïƒ
bnB
ï£¶
ï£¸
+ 2 (2Ï‰ + 1)
g0 âˆ’h02 + 4Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’h0
i
2
!
+ 2
b
h0 âˆ’âˆ‡f(x0)
2 + 32bÏ‰ (2Ï‰ + 1)
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£»
+
96Ï‰ (2Ï‰ + 1)
nB
+
4
bnB

b2Ïƒ2.
Proof. Let us ï¬x constants Î½, Ï âˆˆ[0, âˆž) that we will deï¬ne later.
Considering Lemma I.3,
Lemma I.14, and the law of total expectation, we obtain
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 8Î³Ï‰ (2Ï‰ + 1)
n
E
"
2b2Ïƒ2
B
+ 2
 
(1 âˆ’b)2 L2
Ïƒ
B
+ bL2
!
xt+1 âˆ’xt2 + 2b2 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ Î½E
"
2b2Ïƒ2
nB
+ 2 (1 âˆ’b)2 L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2
#
+ ÏE
"
2b2Ïƒ2
B
+ 2 (1 âˆ’b)2 L2
Ïƒ
B
xt+1 âˆ’xt2 + (1 âˆ’b)2 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
After rearranging the terms, we get
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
16Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’2Î½ (1 âˆ’b)2 L2
Ïƒ
nB
âˆ’2Ï (1 âˆ’b)2 L2
Ïƒ
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
 Î³ + Î½(1 âˆ’b)2
E
hht âˆ’âˆ‡f(xt)
2i
+
16b2Î³Ï‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’b)2

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
40

Published as a conference paper at ICLR 2023
+ 2
8Î³Ï‰ (2Ï‰ + 1)
nB
+ Î½
nB + Ï
B

b2Ïƒ2.
By taking Î½ = Î³
b , one can see that Î³ + Î½(1 âˆ’b)2 â‰¤Î½, and
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
b E
hht âˆ’âˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
16Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’2Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
âˆ’2Ï (1 âˆ’b)2 L2
Ïƒ
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
16b2Î³Ï‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’b)2

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ 2
8Î³Ï‰ (2Ï‰ + 1)
nB
+
Î³
bnB + Ï
B

b2Ïƒ2.
Next, we ï¬x Ï =
16bÎ³Ï‰(2Ï‰+1)
n
. With this choice of Ï and for all b âˆˆ[0, 1], we can show that
16b2Î³Ï‰(2Ï‰+1)
n
+ Ï(1 âˆ’b)2 â‰¤Ï, thus
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 16bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+ 16bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
16Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’2Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
âˆ’32bÎ³Ï‰ (2Ï‰ + 1) (1 âˆ’b)2 L2
Ïƒ
nB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+ 2
8Î³Ï‰ (2Ï‰ + 1)
nB
+
Î³
bnB + 16bÎ³Ï‰ (2Ï‰ + 1)
nB

b2Ïƒ2
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+ 16bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
41

Published as a conference paper at ICLR 2023
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
48Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’2Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
48Î³Ï‰ (2Ï‰ + 1)
nB
+ 2Î³
bnB

b2Ïƒ2.
In the last inequality we use b âˆˆ(0, 1]. Next, considering the choice of Î³ and Lemma I.7, we get
E

f(xt+1)

+ Î³ (2Ï‰ + 1) E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 16bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³ (2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+ 16bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+
48Î³Ï‰ (2Ï‰ + 1)
nB
+ 2Î³
bnB

b2Ïƒ2.
In the view of Lemma I.5 with
Î¨t
=
(2Ï‰ + 1) E
hgt âˆ’ht2i
+ 2Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
1
b E
hht âˆ’âˆ‡f(xt)
2i
+ 16bÏ‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
and C =

48Ï‰(2Ï‰+1)
nB
+
2
bnB

b2Ïƒ2, we can conclude the proof.
Corollary 6.8.
Suppose that assumptions from Theorem 6.7 hold,
momentum b
=
Î˜

min
n
1
Ï‰
q
nÎµB
Ïƒ2 , nÎµB
Ïƒ2
o
, and g0
i = h0
i =
1
Binit
PBinit
k=1 âˆ‡fi(x0; Î¾0
ik) for all i âˆˆ[n], and batch
size Binit = Î˜ (B/b) , then Algorithm 1 (DASHA-MVR) needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn
bL +
 
Ï‰
âˆšn +
r
Ïƒ2
Îµn2B
!
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where Î¶C is the
expected density from Deï¬nition 1.3.
Proof. In the view of Theorem 6.7, we have
E
hâˆ‡f(bxT )
2i
= O
ï£«
ï£¬
ï£¬
ï£­
1
T
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
ï£«
ï£­L + Ï‰
âˆšn
r
(1 âˆ’b)2 L2Ïƒ
B + bL2 +
s
(1 âˆ’b)2
bn
LÏƒ
âˆš
B
ï£¶
ï£¸
42

Published as a conference paper at ICLR 2023
+ 1
b
h0 âˆ’âˆ‡f(x0)
2 + bÏ‰2
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£»
+
Ï‰2
n + 1
bn

b2 Ïƒ2
B
ï£¶
ï£·
ï£·
ï£¸.
Note, that 1
b = Î˜

max

Ï‰
q
Ïƒ2
nÎµB ,
Ïƒ2
nÎµB

â‰¤Î˜

max
n
Ï‰2,
Ïƒ2
nÎµB
o
, thus
E
hâˆ‡f(bxT )
2i
= O
ï£«
ï£¬
ï£¬
ï£­
1
T
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+ 1
b
h0 âˆ’âˆ‡f(x0)
2 + bÏ‰2
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£»+ Îµ
ï£¶
ï£·
ï£·
ï£¸.
Thus we can take
T = O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+ 1
b
h0 âˆ’âˆ‡f(x0)
2 + bÏ‰2
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
Note, that h0
i = g0
i =
1
Binit
PBinit
k=1 âˆ‡fi(x0; Î¾0
ik) for all i âˆˆ[n]. Let us bound E
hh0 âˆ’âˆ‡f(x0)
2i
:
E
hh0 âˆ’âˆ‡f(x0)
2i
=
E
ï£®
ï£°

1
n
n
X
i=1
1
Binit
Binit
X
k=1
âˆ‡fi(x0; Î¾0
ik) âˆ’âˆ‡f(x0)

2ï£¹
ï£»
=
1
n2B2
init
n
X
i=1
Binit
X
k=1
E
hâˆ‡fi(x0; Î¾0
ik) âˆ’âˆ‡fi(x0)
2i
â‰¤
Ïƒ2
nBinit
.
Likewise, 1
n
Pn
i=1 E
hh0
i âˆ’âˆ‡fi(x0)
2i
â‰¤
Ïƒ2
Binit . All in all, we have
T = O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+
Ïƒ2
bnBinit
+ bÏ‰2Ïƒ2
nBinit
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
43

Published as a conference paper at ICLR 2023
= O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+ Ïƒ2
nB + b2Ï‰2Ïƒ2
nB
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
= O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸.
In the view of Algorithm 1 and the fact that we use a mini-batch of stochastic gradients, the number
of stochastic gradients that each node calculates equals Binit + 2BT = O(Binit + BT).
Corollary 6.9. Suppose that assumptions of Corollary 6.8 hold, batch size B â‰¤
Ïƒ
âˆšÎµn, we take RandK
with K = Î¶C = Î˜

BdâˆšÎµn
Ïƒ

, and eL := max{L, LÏƒ, bL}. Then the communication complexity equals
O
 
dÏƒ
âˆšnÎµ +
eL
 f(x0) âˆ’f âˆ—
d
âˆšnÎµ
!
,
(9)
and the expected # of stochastic gradient calculations per node equals
O
 
Ïƒ2
nÎµ +
eL
 f(x0) âˆ’f âˆ—
Ïƒ
Îµ
3/2n
!
.
(10)
Proof. In the view of Theorem F.2, we have Ï‰+1 = d/K. Moreover, K = Î˜

BdâˆšÎµn
Ïƒ

= O

d
âˆšn

,
thus the communication complexity equals
O (d + Î¶CT)
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
KL + K Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+ K
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ K Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšnL + d
âˆšn

bL + LÏƒ
âˆš
B

+ d
âˆšnLÏƒ

ï£¹
ï£ºï£ºï£»+ dÏƒ
âˆšnÎµ
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + dÏƒ
âˆšnÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­
dÏƒ
âˆšnÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
And the expected number of stochastic gradient calculations per node equals
O (Binit + BT)
= O
ï£«
ï£¬
ï£¬
ï£­B Ïƒ2
BnÎµ + BÏ‰
r
Ïƒ2
nÎµB + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
BL + B Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+ B
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
44

Published as a conference paper at ICLR 2023
= O
ï£«
ï£¬
ï£¬
ï£­
Ïƒ2
nÎµ +
Ïƒ2
nÎµ
âˆš
B
+ 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  Ïƒ
âˆšÎµnL +
Ïƒ
âˆšÎµn

bL + LÏƒ
âˆš
B

+
Ïƒ
âˆšÎµnLÏƒ

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
= O
ï£«
ï£¬
ï£¬
ï£­
Ïƒ2
nÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  Ïƒ
âˆšÎµn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
I.6
CASE OF DASHA-MVR UNDER PÅ-CONDITION
Theorem
I.15.
Suppose
that
Assumption
5.1,
5.2,
5.3,
1.2,
5.5,
5.6
and
G.1
hold.
Let
us
take
a
=
1/ (2Ï‰ + 1) ,
b
âˆˆ
(0, 1]
and
Î³
â‰¤
min
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
ï£«
ï£¬
ï£­L +
s
400Ï‰(2Ï‰+1)

(1âˆ’b)2L2Ïƒ
B
+bL2

n
+ 8(1âˆ’b)2L2Ïƒ
bnB
ï£¶
ï£·
ï£¸
âˆ’1
, a
2Âµ, b
2Âµ
ï£¼
ï£´
ï£´
ï£½
ï£´
ï£´
ï£¾
in Algorithm 1 (DASHA-
MVR), then
E

f(xT ) âˆ’f âˆ—
â‰¤
(1 âˆ’Î³Âµ)T
ï£«
ï£¬
ï£¬
ï£­
 f(x0) âˆ’f âˆ—
+ 2Î³(2Ï‰ + 1)
g0 âˆ’h02 + 8Î³Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’h0
i
2
!
+
2Î³
b
h0 âˆ’âˆ‡f(x0)
2 + 80bÎ³Ï‰ (2Ï‰ + 1)
n
 
1
n
n
X
i=1
h0
i âˆ’âˆ‡fi(x0)
2
!
ï£¶
ï£·
ï£·
ï£¸
+
1
Âµ
200Ï‰ (2Ï‰ + 1)
nB
+
4
bnB

b2Ïƒ2.
Proof. Let us ï¬x constants Î½, Ï âˆˆ[0, âˆž) that we will deï¬ne later.
Considering Lemma I.4,
Lemma I.14, and the law of total expectation, we obtain
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ 20Î³Ï‰(2Ï‰ + 1)
n
E
"
2b2Ïƒ2
B
+ 2
 
(1 âˆ’b)2 L2
Ïƒ
B
+ bL2
!
xt+1 âˆ’xt2 + 2b2 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ Î½E
"
2b2Ïƒ2
nB
+ 2 (1 âˆ’b)2 L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’b)2 ht âˆ’âˆ‡f(xt)
2
#
+ ÏE
"
2b2Ïƒ2
B
+ 2 (1 âˆ’b)2 L2
Ïƒ
B
xt+1 âˆ’xt2 + (1 âˆ’b)2 1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
.
45

Published as a conference paper at ICLR 2023
After rearranging the terms, we get
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
40Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’2Î½ (1 âˆ’b)2 L2
Ïƒ
nB
âˆ’2Ï (1 âˆ’b)2 L2
Ïƒ
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
 Î³ + Î½(1 âˆ’b)2
E
hht âˆ’âˆ‡f(xt)
2i
+
40b2Î³Ï‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’b)2

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ 2
20Î³Ï‰ (2Ï‰ + 1)
nB
+ Î½
nB + Ï
B

b2Ïƒ2.
By taking Î½ = 2Î³
b , one can see that Î³ + Î½(1 âˆ’b)2 â‰¤
 1 âˆ’b
2

Î½, and
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ 2Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ ÏE
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’b
2
 2Î³
b E
hht âˆ’âˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
40Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’4Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
âˆ’2Ï (1 âˆ’b)2 L2
Ïƒ
B
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
40b2Î³Ï‰ (2Ï‰ + 1)
n
+ Ï(1 âˆ’b)2

E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+ 2
20Î³Ï‰ (2Ï‰ + 1)
nB
+ 2Î³
bnB + Ï
B

b2Ïƒ2.
Next, we ï¬x Ï =
80bÎ³Ï‰(2Ï‰+1)
n
. With this choice of Ï and for all b âˆˆ(0, 1], we can show that
40b2Î³Ï‰(2Ï‰+1)
n
+ Ï(1 âˆ’b)2 â‰¤
 1 âˆ’b
2

Ï, thus
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ 2Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 80bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
46

Published as a conference paper at ICLR 2023
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’b
2
 2Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+

1 âˆ’b
2
 80bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
40Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’4Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
âˆ’160bÎ³Ï‰ (2Ï‰ + 1) (1 âˆ’b)2 L2
Ïƒ
nB
!
E
hxt+1 âˆ’xt2i
+ 2
20Î³Ï‰ (2Ï‰ + 1)
nB
+ 2Î³
bnB + 80bÎ³Ï‰ (2Ï‰ + 1)
nB

b2Ïƒ2
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’b
2
 2Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+

1 âˆ’b
2
 80bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
200Î³Ï‰ (2Ï‰ + 1)

(1âˆ’b)2L2
Ïƒ
B
+ bL2
n
âˆ’4Î³ (1 âˆ’b)2 L2
Ïƒ
bnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
200Î³Ï‰ (2Ï‰ + 1)
nB
+ 4Î³
bnB

b2Ïƒ2.
In the last inequality we use b âˆˆ(0, 1]. Next, considering the choice of Î³ and Lemma I.7, we get
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ 2Î³
b E
hht+1 âˆ’âˆ‡f(xt+1)
2i
+ 80bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
âˆ’âˆ‡fi(xt+1)
2
#
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ (1 âˆ’Î³Âµ) 2Î³
b E
hht âˆ’âˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 80bÎ³Ï‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
+
200Î³Ï‰ (2Ï‰ + 1)
nB
+ 4Î³
bnB

b2Ïƒ2.
In the view of Lemma I.6 with
Î¨t
=
2(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 8Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2
b E
hht âˆ’âˆ‡f(xt)
2i
+ 80bÏ‰ (2Ï‰ + 1)
n
E
"
1
n
n
X
i=1
ht
i âˆ’âˆ‡fi(xt)
2
#
47

Published as a conference paper at ICLR 2023
and C =

200Ï‰(2Ï‰+1)
nB
+
4
bnB

b2Ïƒ2, we can conclude the proof.
Corollary I.16.
Suppose that assumptions from Theorem I.15 hold,
momentum b
=
Î˜

min

1
Ï‰
q
ÂµnÎµB
Ïƒ2 , ÂµnÎµB
Ïƒ2

, and h0
i = g0
i = 0 for all i âˆˆ[n], then Algorithm 1 needs
T := e
O
 
Ï‰ + Ï‰
s
Ïƒ2
ÂµnÎµB +
Ïƒ2
ÂµnÎµB + L
Âµ + Ï‰bL
Âµâˆšn +
 Ï‰
âˆšn +
Ïƒ
nâˆšBÂµÎµ
 LÏƒ
Âµ
âˆš
B
!
(27)
communication rounds to get an Îµ-solution, the communication complexity is equal to O (Î¶CT) , and
the number of stochastic gradient calculations per node equals O(BT), where Î¶C is the expected
density from Deï¬nition 1.3.
Proof. Considering the choice of b, we have 1
Âµ

200Ï‰(2Ï‰+1)
nB
+
4
bnB

b2Ïƒ2 = O (Îµ) . Therefore, is it
enough to take the number of communication rounds equals (27) to get an Îµ-solution. In the view
of Algorithm 1 and the fact that we use a mini-batch of stochastic gradients, the communication
complexity is equal to O (Î¶CT) and the number of stochastic gradients that each node calculates
equals O(BT). Unlike Corollary 6.8, in this corollary, we can initialize h0
i and g0
i , for instance, with
zeros because the corresponding initialization error Î¨0 from the proof of Theorem I.15 would be
under the logarithm.
I.7
CASE OF DASHA-SYNC-MVR
Comparing Algorithm 1 and Algorithm 2, one can see that Algorithm 2 has the third source of
randomness from ct+1. In this section, we deï¬ne Ep [Â·] to be a conditional expectation w.r.t. ct+1
conditioned on all previous randomness. And we deï¬ne Et+1 [Â·] to be a conditional expectation
w.r.t. ct+1, {Ci}n
i=1, {ht+1
i
}n
i=1 conditioned on all previous randomness. Note, that Et+1 [Â·] =
Eh [EC [Ep [Â·]]] .
Lemma I.17. Suppose that Assumptions 5.3, 5.5 and 1.2 hold and let us consider sequences
{gt+1
i
}n
i=1 and {ht+1
i
}n
i=1 from Algorithm 2, then
Et+1
hgt+1 âˆ’ht+12i
â‰¤
2Ï‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
xt+1 âˆ’xt2 + 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2 ,
and
Et+1
hgt+1
i
âˆ’ht+1
i
2i
â‰¤2Ï‰(1 âˆ’p)
L2
Ïƒ
B + L2
i
 xt+1 âˆ’xt2 + (1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2 gt
i âˆ’ht
i
2 ,
âˆ€i âˆˆ[n].
Proof. First,
we
estimate
Et+1
hgt+1 âˆ’ht+12i
.
Let
us
denote
ht+1
i,0
=
1
B
PB
j=1 âˆ‡fi(xt+1; Î¾t+1
ij ) + ht
i âˆ’1
B
PB
j=1 âˆ‡fi(xt; Î¾t+1
ij ).
Et+1
hgt+1 âˆ’ht+12i
= Et+1
h
Ep
hgt+1 âˆ’ht+12ii
= (1 âˆ’p)Et+1
ï£®
ï£°
gt + 1
n
n
X
i=1
Ci
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’1
n
n
X
i=1
ht+1
i,0

2ï£¹
ï£»
(4),(15)
=
(1 âˆ’p)Eh
ï£®
ï£°EC
ï£®
ï£°

1
n
n
X
i=1
Ci
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’1
n
n
X
i=1
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i


2ï£¹
ï£»
ï£¹
ï£»
48

Published as a conference paper at ICLR 2023
+ (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2 .
Using the independence of compressors and (4), we get
Et+1
hgt+1 âˆ’ht+12i
= (1 âˆ’p)
n2
n
X
i=1
Eh
h
EC
hCi
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2ii
+ (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
â‰¤Ï‰(1 âˆ’p)
n2
n
X
i=1
Eh
hht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i
2i
+ (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
â‰¤2Ï‰(1 âˆ’p)
n2
n
X
i=1
Eh
hht+1
i,0 âˆ’ht
i
2i
+ 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
= 2Ï‰(1 âˆ’p)
n2
n
X
i=1
Eh
ï£®
ï£¯ï£°

1
B
B
X
j=1
âˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’1
B
B
X
j=1
âˆ‡fi(xt; Î¾t+1
ij )

2ï£¹
ï£ºï£»
+ 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
(15)
= 2Ï‰(1 âˆ’p)
n2
n
X
i=1
 
Eh
ï£®
ï£¯ï£°

1
B
B
X
j=1
 âˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt; Î¾t+1
ij )

âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)


2ï£¹
ï£ºï£»
+
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
!
+ 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
= 2Ï‰(1 âˆ’p)
n2
n
X
i=1
 
1
B2
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt; Î¾t+1
ij ) âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2i
+
âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2
!
+ 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
â‰¤
2Ï‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
xt+1 âˆ’xt2 + 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2 ,
where in the inequalities we use Assumptions 1.2, 5.5 and 5.3, and (14). Analogously, we can get the
bound for Et+1
hgt+1
i
âˆ’ht+1
i
2i
for all i âˆˆ[n]:
Et+1
hgt+1
i
âˆ’ht+1
i
2i
= Et+1
h
Ep
hgt+1
i
âˆ’ht+1
i
2ii
= (1 âˆ’p)Et+1
hgt
i + Ci
 ht+1
i,0 âˆ’ht
i âˆ’a
 gt
i âˆ’ht
i

âˆ’ht+1
i,0
2i
â‰¤2Ï‰(1 âˆ’p)
L2
Ïƒ
B + L2
i
 xt+1 âˆ’xt2 + 2a2Ï‰(1 âˆ’p)
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt
i âˆ’ht
i
2 .
49

Published as a conference paper at ICLR 2023
We introduce new notations: âˆ‡fi(xt+1; Î¾t+1
i
) = 1
B
PB
j=1 âˆ‡fi(xt+1; Î¾t+1
ij ) and âˆ‡f(xt+1; Î¾t+1) =
1
n
Pn
i=1 âˆ‡fi(xt+1; Î¾t+1
i
).
Lemma I.18. Suppose that Assumptions 5.5 and 5.6 hold and let us consider sequence {ht+1
i
}n
i=1
from Algorithm 2, then
Et+1
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤pÏƒ2
nBâ€² + (1 âˆ’p)L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2 .
Proof.
Et+1
hht+1 âˆ’âˆ‡f(xt+1)
2i
= pEh
ï£®
ï£¯ï£°

1
n
n
X
i=1
1
Bâ€²
Bâ€²
X
k=1
âˆ‡fi(xt+1; Î¾t+1
ik ) âˆ’âˆ‡f(xt+1)

2ï£¹
ï£ºï£»
+ (1 âˆ’p)Eh
hâˆ‡f(xt+1; Î¾t+1) + ht âˆ’âˆ‡f(xt; Î¾t+1) âˆ’âˆ‡f(xt+1)
2i
â‰¤pÏƒ2
nBâ€² + (1 âˆ’p)Eh
hâˆ‡f(xt+1; Î¾t+1) + ht âˆ’âˆ‡f(xt; Î¾t+1) âˆ’âˆ‡f(xt+1)
2i
,
where we use Assumption 5.5. Next, using Assumption 5.6 and (15), we have
Et+1
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤pÏƒ2
nBâ€² + (1 âˆ’p)Eh
hâˆ‡f(xt+1; Î¾t+1) + ht âˆ’âˆ‡f(xt; Î¾t+1) âˆ’âˆ‡f(xt+1)
2i
= pÏƒ2
nBâ€² + (1 âˆ’p)Eh
hâˆ‡f(xt+1; Î¾t+1) âˆ’âˆ‡f(xt; Î¾t+1) âˆ’
 âˆ‡f(xt+1) âˆ’âˆ‡f(xt)
2i
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
= pÏƒ2
nBâ€² + (1 âˆ’p)
n2
n
X
i=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
i
) âˆ’âˆ‡fi(xt; Î¾t+1
i
) âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2i
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
= pÏƒ2
nBâ€² + (1 âˆ’p)
n2B2
n
X
i=1
B
X
j=1
Eh
hâˆ‡fi(xt+1; Î¾t+1
ij ) âˆ’âˆ‡fi(xt; Î¾t+1
ij ) âˆ’
 âˆ‡fi(xt+1) âˆ’âˆ‡fi(xt)
2i
+ (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2
â‰¤pÏƒ2
nBâ€² + (1 âˆ’p)L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2 .
Theorem I.19. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2Ï‰+1,
probability p âˆˆ(0, 1], batch size Bâ€² â‰¥1 and
Î³ â‰¤
 
L +
s
12Ï‰(2Ï‰ + 1)(1 âˆ’p)
n
L2Ïƒ
B + bL2

+ 2(1 âˆ’p)L2Ïƒ
pnB
!âˆ’1
,
in Algorithm 2. Then
E
hâˆ‡f(bxT )
2i
â‰¤1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
Ã—
 
L +
s
12Ï‰(2Ï‰ + 1)(1 âˆ’p)
n
L2Ïƒ
B + bL2

+ 2(1 âˆ’p)L2Ïƒ
pnB
!
+ 2 (2Ï‰ + 1)
g0 âˆ’h02 + 4Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’h0
i
2
!
50

Published as a conference paper at ICLR 2023
+ 2
p
h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»+ 2Ïƒ2
nBâ€² .
Proof. Let us ï¬x constants Îº, Î·, Î½ âˆˆ[0, âˆž) that we will deï¬ne later. Using Lemma I.1, we can get
(20). Considering (20), Lemma I.17, Lemma I.18, and the law of total expectation, we obtain
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
gt âˆ’ht2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ ÎºE
ï£®
ï£°
2Ï‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
xt+1 âˆ’xt2 + 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
ï£¹
ï£»
+ Î·E
"
2Ï‰(1 âˆ’p)
L2
Ïƒ
B + bL2
 xt+1 âˆ’xt2 + (1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2 1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î½E
 pÏƒ2
nBâ€² + (1 âˆ’p)L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2

.
After rearranging the terms, we get
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
2ÎºÏ‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’2Î·Ï‰(1 âˆ’p)
L2
Ïƒ
B + bL2

âˆ’Î½(1 âˆ’p)L2
Ïƒ
nB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
 Î³ + Îº(1 âˆ’p)(1 âˆ’a)2
E
hgt âˆ’ht2i
+
2Îºa2Ï‰(1 âˆ’p)
n
+ Î·(1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ (Î³ + Î½(1 âˆ’p)) E
hht âˆ’âˆ‡f(xt)
2i
+ Î½pÏƒ2
nBâ€² .
Let us take Î½ = Î³
p, Îº = Î³
a, a =
1
2Ï‰+1, and Î· = 2Î³Ï‰
n . Thus Î³+Îº(1âˆ’p)(1âˆ’a)2 â‰¤Îº, Î³+Î½(1âˆ’p) = Î½,
2Îºa2Ï‰(1âˆ’p)
n
+ Î·(1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2
â‰¤Î·, and
E

f(xt+1)

+ Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
51

Published as a conference paper at ICLR 2023
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
2Î³Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’
4Î³Ï‰2(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’Î³(1 âˆ’p)L2
Ïƒ
pnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+ Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ Î³Ïƒ2
nBâ€²
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
6Î³Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’Î³(1 âˆ’p)L2
Ïƒ
pnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+ Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ Î³Ïƒ2
nBâ€² .
In the view of the choice of Î³, we obtain
E

f(xt+1)

+ Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 2Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ Î³Ïƒ2
nBâ€² .
Finally, using Lemma I.5 with
Î¨t
=
(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 2Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
1
pE
hht âˆ’âˆ‡f(xt)
2i
and C =
Ïƒ2
nBâ€² , we can conclude the proof.
Corollary 6.10.
Suppose that assumptions from Theorem I.19 hold,
probability p
=
min
n
Î¶C
d , nÎµB
Ïƒ2
o
, batch size Bâ€² = Î˜

Ïƒ2
nÎµ

and h0
i = g0
i =
1
Binit
PBinit
k=1 âˆ‡fi(x0; Î¾0
ik) for all i âˆˆ[n],
initial batch size Binit = Î˜

max
n
Ïƒ2
nÎµ, B d
Î¶C
o
, then DASHA-SYNC-MVR needs
T := O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn
bL +
 
Ï‰
âˆšn +
s
d
Î¶Cn +
r
Ïƒ2
Îµn2B
!
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
52

Published as a conference paper at ICLR 2023
communication rounds to get an Îµ-solution, the communication complexity is equal to O (d + Î¶CT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where Î¶C is the
expected density from Deï¬nition 1.3.
Proof. Considering Theorem I.19 and the choice of Bâ€², we have
E
hâˆ‡f(bxT )
2i
â‰¤1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
ï£«
ï£¬
ï£¬
ï£­L +
v
u
u
t12Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2Ïƒ
B + bL2

n
+ 2(1 âˆ’p)L2Ïƒ
pnB
ï£¶
ï£·
ï£·
ï£¸
+ 2
p
h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»+ 2Ïƒ2
nBâ€²
â‰¤1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
ï£«
ï£¬
ï£¬
ï£­L +
v
u
u
t12Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2Ïƒ
B + bL2

n
+ 2(1 âˆ’p)L2Ïƒ
pnB
ï£¶
ï£·
ï£·
ï£¸
+ 2
p
h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»+ 2
3Îµ.
Due to p = min
n
Î¶C
d , nÎµB
Ïƒ2
o
, we have
E
hâˆ‡f(bxT )
2i
â‰¤O
ï£«
ï£¬
ï£¬
ï£­
1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
ï£«
ï£¬
ï£¬
ï£­L +
v
u
u
t12Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2Ïƒ
B + bL2

n
+ 2d(1 âˆ’p)L2Ïƒ
Î¶CnB
+ 2Ïƒ2(1 âˆ’p)L2Ïƒ
Îµn2B2
ï£¶
ï£·
ï£·
ï£¸
+ 2
 d
Î¶C
+ Ïƒ2
nÎµB
 h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸+ 2
3Îµ
â‰¤O
ï£«
ï£¬
ï£¬
ï£­
1
T
ï£®
ï£¯ï£¯ï£°2
 f(x0) âˆ’f âˆ—
ï£«
ï£¬
ï£¬
ï£­L +
v
u
u
tÏ‰2(1 âˆ’p)

L2Ïƒ
B + bL2

n
+ d(1 âˆ’p)L2Ïƒ
Î¶CnB
+ 2Ïƒ2(1 âˆ’p)L2Ïƒ
Îµn2B2
ï£¶
ï£·
ï£·
ï£¸
+ 2
 d
Î¶C
+ Ïƒ2
nÎµB
 h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸+ 2
3Îµ.
Therefore, we can take
T = O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
s
d
Î¶Cn
LÏƒ
âˆš
B
+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+
 d
Î¶C
+ Ïƒ2
nÎµB
 h0 âˆ’âˆ‡f(x0)
2
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
Note, that
E
hh0 âˆ’âˆ‡f(x0)
2i
=
E
ï£®
ï£°

1
n
n
X
i=1
1
Binit
Binit
X
k=1
âˆ‡fi(x0; Î¾0
ik) âˆ’âˆ‡f(x0)

2ï£¹
ï£»
53

Published as a conference paper at ICLR 2023
=
1
n2B2
init
n
X
i=1
Binit
X
k=1
E
hâˆ‡fi(x0; Î¾0
ik) âˆ’âˆ‡fi(x0)
2i
â‰¤
Ïƒ2
nBinit
.
Next, by taking Binit = max
n
Ïƒ2
nÎµ, B d
Î¶C
o
and using the last ineqaulity, we have
T = O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
s
d
Î¶Cn
LÏƒ
âˆš
B
+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
+
 d
Î¶C
+ Ïƒ2
nÎµB

min
Ïƒ2Î¶C
ndB , Îµ

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
= O
ï£«
ï£¬
ï£¬
ï£­
1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
L + Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+
s
d
Î¶Cn
LÏƒ
âˆš
B
+
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸.
Finally, it is left to estimate the communication and oracle complexity. On average, the number
of coordinates that each node in Algorithm 2 sends at each communication round equals pd +
(1 âˆ’p)Î¶C â‰¤
Î¶C
d d +

1 âˆ’Î¶C
d

Î¶C â‰¤2Î¶C. Therefore, the communication complexity is equal to
O (d + Î¶CT) . Considering the fact that we use a mini-batch of stochastic gradients, on average,
the number of stochastic gradients that each node calculates at each communication round equals
pBâ€² + (1 âˆ’p)2B â‰¤O

nÎµB
Ïƒ2 Â· Ïƒ2
nÎµ

+ 2B = O (B) . Considering the initial batch size Binit, the
number of stochastic gradients that each node calculates equals O(Binit + BT).
Corollary 6.11. Suppose that assumptions of Corollary 6.10 hold, batch size B â‰¤
Ïƒ
âˆšÎµn, we take
RandK with K = Î¶C = Î˜

BdâˆšÎµn
Ïƒ

, and eL := max{L, LÏƒ, bL}. Then the communication complex-
ity equals
O
 
dÏƒ
âˆšnÎµ +
eL
 f(x0) âˆ’f âˆ—
d
âˆšnÎµ
!
,
(11)
and the expected # of stochastic gradient calculations per node equals
O
 
Ïƒ2
nÎµ +
eL
 f(x0) âˆ’f âˆ—
Ïƒ
Îµ
3/2n
!
.
(12)
Proof. In the view of Theorem F.2, we have Ï‰+1 = d/K. Moreover, K = Î˜

BdâˆšÎµn
Ïƒ

= O

d
âˆšn

,
thus the communication complexity equals
O (d + Î¶CT)
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
KL + K Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+ K
rÏ‰
n
LÏƒ
âˆš
B
+ K
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»+ K Ïƒ2
nÎµB
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšnL + d
âˆšn

bL + LÏƒ
âˆš
B

+ d
âˆšnLÏƒ

ï£¹
ï£ºï£ºï£»+ dÏƒ
âˆšnÎµ
ï£¶
ï£·
ï£·
ï£¸
=
O
ï£«
ï£¬
ï£¬
ï£­d + dÏƒ
âˆšnÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
54

Published as a conference paper at ICLR 2023
=
O
ï£«
ï£¬
ï£¬
ï£­
dÏƒ
âˆšnÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  d
âˆšn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
And the expected number of stochastic gradient calculations per node equals
O (Binit + BT)
= O
ï£«
ï£¬
ï£¬
ï£­
Ïƒ2
nÎµ + B d
Î¶C
+ 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—
 
BL + B Ï‰
âˆšn

bL + LÏƒ
âˆš
B

+ B
rÏ‰
n
LÏƒ
âˆš
B
+ B
r
Ïƒ2
Îµn2B
LÏƒ
âˆš
B
!
ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
= O
ï£«
ï£¬
ï£¬
ï£­
Ïƒ2
nÎµ +
Ïƒ
âˆšnÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  Ïƒ
âˆšÎµnL +
Ïƒ
âˆšÎµn

bL + LÏƒ
âˆš
B

+
Ïƒ
âˆšÎµnLÏƒ

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸
= O
ï£«
ï£¬
ï£¬
ï£­
Ïƒ2
nÎµ + 1
Îµ
ï£®
ï£¯ï£¯ï£°
 f(x0) âˆ’f âˆ—  Ïƒ
âˆšÎµn
eL

ï£¹
ï£ºï£ºï£»
ï£¶
ï£·
ï£·
ï£¸.
I.8
CASE OF DASHA-SYNC-MVR UNDER PÅ-CONDITION
Theorem
I.20.
Suppose
that
Assumption
5.1,
5.2,
1.2,
5.5,
5.6
and
G.1
hold.
Let
us
take
a
=
1/ (2Ï‰ + 1) ,
probability
p
âˆˆ
(0, 1]
and
Î³
â‰¤
min
ï£±
ï£´
ï£²
ï£´
ï£³
ï£«
ï£¬
ï£­L +
s
40Ï‰(2Ï‰+1)(1âˆ’p)

L2Ïƒ
B +bL2

n
+ 4(1âˆ’p)L2Ïƒ
pnB
ï£¶
ï£·
ï£¸
âˆ’1
, a
2Âµ, p
2Âµ
ï£¼
ï£´
ï£½
ï£´
ï£¾
in Algorithm 1, then
E

f(xT ) âˆ’f âˆ—
â‰¤
(1 âˆ’Î³Âµ)T
ï£«
ï£¬
ï£¬
ï£­
 f(x0) âˆ’f âˆ—
+ 2Î³(2Ï‰ + 1)
g0 âˆ’h02 + 8Î³Ï‰
n
 
1
n
n
X
i=1
g0
i âˆ’h0
i
2
!
+
2Î³
p
h0 âˆ’âˆ‡f(x0)
2
ï£¶
ï£·
ï£·
ï£¸+ 2Ïƒ2
nÂµBâ€² .
Proof. Let us ï¬x constants Îº, Î·, Î½ âˆˆ[0, âˆž) that we will deï¬ne later. Using Lemma I.1, we can get
(20). Considering (20), Lemma I.17, Lemma I.18, and the law of total expectation, we obtain
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt) âˆ’Î³
2
âˆ‡f(xt)
2 âˆ’
 1
2Î³ âˆ’L
2
 xt+1 âˆ’xt2 + Î³
gt âˆ’ht2 + Î³
ht âˆ’âˆ‡f(xt)
2

+ ÎºE
ï£®
ï£°
2Ï‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
xt+1 âˆ’xt2 + 2a2Ï‰(1 âˆ’p)
n2
n
X
i=1
gt
i âˆ’ht
i
2 + (1 âˆ’p) (1 âˆ’a)2 gt âˆ’ht2
ï£¹
ï£»
+ Î·E
"
2Ï‰(1 âˆ’p)
L2
Ïƒ
B + bL2
 xt+1 âˆ’xt2 + (1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2 1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
55

Published as a conference paper at ICLR 2023
+ Î½E
 pÏƒ2
nBâ€² + (1 âˆ’p)L2
Ïƒ
nB
xt+1 âˆ’xt2 + (1 âˆ’p)
ht âˆ’âˆ‡f(xt)
2

After rearranging the terms, we get
E

f(xt+1)

+ ÎºE
hgt+1 âˆ’ht+12i
+ Î·E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ Î½E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
2ÎºÏ‰(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’2Î·Ï‰(1 âˆ’p)
L2
Ïƒ
B + bL2

âˆ’Î½(1 âˆ’p)L2
Ïƒ
nB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+
 Î³ + Îº(1 âˆ’p)(1 âˆ’a)2
E
hgt âˆ’ht2i
+
2Îºa2Ï‰(1 âˆ’p)
n
+ Î·(1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2
E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ (Î³ + Î½(1 âˆ’p)) E
hht âˆ’âˆ‡f(xt)
2i
+ Î½pÏƒ2
nBâ€² .
Let us take Î½ = 2Î³
p , Îº = 2Î³
a , a =
1
2Ï‰+1, and Î· = 8Î³Ï‰
n . Thus Î³ + Îº(1 âˆ’p)(1 âˆ’a)2 â‰¤
 1 âˆ’a
2

Îº,
Î³ + Î½(1 âˆ’p) =
 1 âˆ’p
2

Î½, 2Îºa2Ï‰(1âˆ’p)
n
+ Î·(1 âˆ’p)

2a2Ï‰ + (1 âˆ’a)2
â‰¤
 1 âˆ’a
2

Î·, and
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ 2Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
4Î³Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’
16Î³Ï‰2(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’2Î³(1 âˆ’p)L2
Ïƒ
pnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+

1 âˆ’a
2

2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+

1 âˆ’a
2
 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’p
2
 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ 2Î³Ïƒ2
nBâ€²
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
âˆ’
ï£«
ï£­1
2Î³ âˆ’L
2 âˆ’
20Î³Ï‰(2Ï‰ + 1)(1 âˆ’p)

L2
Ïƒ
B + bL2
n
âˆ’2Î³(1 âˆ’p)L2
Ïƒ
pnB
ï£¶
ï£¸E
hxt+1 âˆ’xt2i
+

1 âˆ’a
2

2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+

1 âˆ’a
2
 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+

1 âˆ’p
2
 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
56

Published as a conference paper at ICLR 2023
+ 2Î³Ïƒ2
nBâ€² .
In the view of the choice of Î³ and Lemma I.7, one can show that 1
2Î³ âˆ’L
2 âˆ’
40Î³Ï‰(2Ï‰+1)(1âˆ’p)

L2
Ïƒ
B +bL2

n
âˆ’
4Î³(1âˆ’p)L2
Ïƒ
pnB
â‰¥0, 1 âˆ’a
2 â‰¤1 âˆ’Î³Âµ, and 1 âˆ’p
2 â‰¤1 âˆ’Î³Âµ, thus
E

f(xt+1)

+ 2Î³(2Ï‰ + 1)E
hgt+1 âˆ’ht+12i
+ 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt+1
i
âˆ’ht+1
i
2
#
+ 2Î³
p E
hht+1 âˆ’âˆ‡f(xt+1)
2i
â‰¤E

f(xt)

âˆ’Î³
2 E
hâˆ‡f(xt)
2i
+ (1 âˆ’Î³Âµ) 2Î³(2Ï‰ + 1)E
hgt âˆ’ht2i
+ (1 âˆ’Î³Âµ) 8Î³Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+ (1 âˆ’Î³Âµ) 2Î³
p E
hht âˆ’âˆ‡f(xt)
2i
+ 2Î³Ïƒ2
nBâ€² .
In the view of Lemma I.6 with
Î¨t
=
2(2Ï‰ + 1)E
hgt âˆ’ht2i
+ 8Ï‰
n E
"
1
n
n
X
i=1
gt
i âˆ’ht
i
2
#
+
2
pE
hht âˆ’âˆ‡f(xt)
2i
and C = 2Ïƒ2
nBâ€² , we can conclude the proof.
Corollary I.21.
Suppose that assumptions from Theorem I.20 hold,
probability p
=
min
n
Î¶C
d , ÂµnÎµB
Ïƒ2
o
, batch size Bâ€² = Î˜

Ïƒ2
ÂµnÎµ

, and h0
i = g0
i = 0 for all i âˆˆ[n], then DASHA-
SYNC-MVR needs
T := e
O
 
Ï‰ + d
Î¶C
+
Ïƒ2
ÂµnÎµB + L
Âµ + Ï‰bL
Âµâˆšn +
 
Ï‰
âˆšn +
s
d
Î¶Cn +
Ïƒ
nâˆšBÂµÎµ
!
LÏƒ
Âµ
âˆš
B
!
(28)
communication rounds to get an Îµ-solution, the communication complexity is equal to O (Î¶CT) , and
the number of stochastic gradient calculations per node equals O(BT), where Î¶C is the expected
density from Deï¬nition 1.3.
Proof. Considering the choice of Bâ€², we have
2Ïƒ2
nÂµBâ€² = O (Îµ) . Therefore, is it enough to take the
number of communication rounds equals (28) to get an Îµ-solution.
It is left to estimate the communication and oracle complexity. On average, in Algorithm 2, at
each communication round the number of coordinates that each node sends equals pd + (1 âˆ’
p)Î¶C â‰¤Î¶C
d d +

1 âˆ’Î¶C
d

Î¶C â‰¤2Î¶C. Therefore, the communication complexity is equal to O (Î¶CT) .
Considering the fact that we use a mini-batch of stochastic gradients, on average, the number of
stochastic gradients that each node calculates at each communication round equals pBâ€²+(1âˆ’p)2B =
O

ÂµnÎµB
Ïƒ2
Â·
Ïƒ2
ÂµnÎµ

+ 2B = O (B) , thus the number of stochastic gradients that each node calculates
equals O(BT). Unlike Corollary 6.10, in this corollary, we can initialize h0
i and g0
i , for instance,
with zeros because the corresponding initialization error Î¨0 from the proof of Theorem I.20 would
be under the logarithm.
57

Published as a conference paper at ICLR 2023
J
EXTRA EXPERIMENTS
DASHA-MVR improves VR-MARINA (online) when Îµ is small (see Tables 1 and 2 and experiments
in Section A). However, our analysis shows that DASHA-MVR gets a term BÏ‰
q
Ïƒ2
ÎµnB in the oracle
complexity and a term Ï‰
q
Ïƒ2
ÂµÎµnB in the number of communication rounds in general nonconvex
and PÅ settings accordingly. Both terms can be a bottleneck in some regimes; now, we verify this
dependence in the PÅ setting.
We take a synthetically generated stochastic quadratic optimization problem with one node (n = 1):
min
xâˆˆRd

f(x; Î¾) := xâŠ¤(A + Î¾I) x âˆ’bâŠ¤x
	
,
where A âˆˆRdÃ—d, b âˆˆRd, A = AâŠ¤â‰»0, and Î¾ âˆ¼Normal
 0, Ïƒ2
.
We generate A in such way, that Âµ â‰ˆ1.0 â‰¤L â‰ˆ2.0, take d = 104, Ïƒ2 = 1.0, RandK with K = 1
(Ï‰ â‰ˆd), batch size B = 1, and
Ïƒ2
ÂµÎµnB = 104. With this particular choice of parameters, Ï‰
q
Ïƒ2
ÂµÎµnB
would dominate in the number of communication rounds T = Ï‰ + Ï‰
q
Ïƒ2
ÂµÎµnB + L(1+Ï‰/âˆšn)
Âµ
+
Ïƒ2
ÂµÎµnB +
LÏƒ
Âµ3/2âˆšÎµnB .
Results are provided in Figure 5. We consider DASHA-MVR with a momentum b from Corollary I.16
and b = min
n
1
Ï‰, ÂµnÎµB
Ïƒ2
o
. With the latter choice of momentum b, DASHA-MVR converges at the
same rate as DASHA-SYNC-MVR or VR-MARINA (online) but to an Îµ-solution with a smaller Îµ. On the
other hand, the former choice of momentum b guarantees the convergence to the correct Îµ-solution,
but with a slower rate. Overall, the experiment provides the pieces of evidence that our choice of b is
correct and that our analysis in Theorem I.15 is tight.
If we decrease Ï‰ from 104 to 103 (see Figure 6), or Ïƒ2 from 1.0 to 0.1 (see Figure 7), or Âµ from 1.0
to 0.1 (see Figure 8), then the gap between algorithms closes.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-MVR b = min[1 ,
n B
2 ]
DASHA-SYNC-MVR
Figure 5: Comparison of algorithms on a synthetic stochastic quadratic optimization task
58

Published as a conference paper at ICLR 2023
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
#bits / n
1e9
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 10
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 6: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
K = 10
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 7: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
Ïƒ2 = 0.1
59

Published as a conference paper at ICLR 2023
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 8: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
Âµ = 0.1
60

