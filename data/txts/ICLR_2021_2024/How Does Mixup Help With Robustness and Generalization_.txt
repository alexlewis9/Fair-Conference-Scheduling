Published as a conference paper at ICLR 2021
HOW DOES MIXUP HELP WITH ROBUSTNESS AND
GENERALIZATION?
Linjun Zhang∗
Rutgers University
linjun.zhang@rutgers.edu
Zhun Deng∗
Harvard University
zhundeng@g.harvard.edu
Kenji Kawaguchi∗
Harvard University
kkawaguchi@fas.harvard.edu
Amirata Ghorbani
Stanford University
amiratag@stanford.edu
James Zou
Stanford University
jamesz@stanford.edu
ABSTRACT
Mixup is a popular data augmentation technique based on taking convex combina-
tions of pairs of examples and their labels. This simple technique has been shown
to substantially improve both the robustness and the generalization of the trained
model. However, it is not well-understood why such improvement occurs. In this
paper, we provide theoretical analysis to demonstrate how using Mixup in training
helps model robustness and generalization. For robustness, we show that minimiz-
ing the Mixup loss corresponds to approximately minimizing an upper bound of
the adversarial loss. This explains why models obtained by Mixup training ex-
hibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign
Method (FGSM). For generalization, we prove that Mixup augmentation corre-
sponds to a speciﬁc type of data-adaptive regularization which reduces overﬁtting.
Our analysis provides new insights and a framework to understand Mixup.
1
INTRODUCTION
Mixup was introduced by Zhang et al. (2018) as a data augmentation technique. It has been em-
pirically shown to substantially improve test performance and robustness to adversarial noise of
state-of-the-art neural network architectures (Zhang et al., 2018; Lamb et al., 2019; Thulasidasan
et al., 2019; Zhang et al., 2018; Arazo et al., 2019). Despite the impressive empirical performance,
it is still not fully understood why Mixup leads to such improvement across the different aspects
mentioned above. We ﬁrst provide more background about robustness and generalization properties
of deep networks and Mixup. Then we give an overview of our main contributions.
Adversarial robustness. Although neural networks have achieved remarkable success in many areas
such as natural language processing (Devlin et al., 2018) and image recognition (He et al., 2016a),
it has been observed that neural networks are very sensitive to adversarial examples — prediction
can be easily ﬂipped by human imperceptible perturbations (Goodfellow et al., 2014; Szegedy et al.,
2013). Speciﬁcally, in Goodfellow et al. (2014), the authors use fast gradient sign method (FGSM)
to generate adversarial examples, which makes an image of panda to be classiﬁed as gibbon with
high conﬁdence. Although various defense mechanisms have been proposed against adversarial
attacks, those mechanisms typically sacriﬁce test accuracy in turn for robustness (Tsipras et al.,
2018) and many of them require a signiﬁcant amount of additional computation time. In contrast,
Mixup training tends to improve test accuracy and at the same time also exhibits a certain degree of
resistance to adversarial examples, such as those generated by FGSM (Lamb et al., 2019). Moreover,
the corresponding training time is relatively modest. As an illustration, we compare the robust test
∗Equal contribution.
1

Published as a conference paper at ICLR 2021
(a) Robustness
0
100
200
300
400
epoch
100
0.3
0.4
0.5
0.6
0.7
0.8
0.9
2.0
test loss
mixup
ERM
0
100
200
300
400
epoch
10
1
0.1
0.1
0.1
0.1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Generalization gap
mixup
ERM
(b) Generalization
Figure 1: Illustrative examples of the impact of Mixup on robustness and generalization. (a) Adver-
sarial robustness on the SVHN data under FGSM attacks. (b) Generalization gap between test and
train loss. More details regarding the experimental setup are included in Appendix C.1, C.2.
accuracy between a model trained with Mixup and a model trained with standard empirical risk
minimization (ERM) under adversarial attacks generated by FGSM (Fig. 1a). The model trained
with Mixup loss has much better robust accuracy. Robustness of Mixup under other attacks have
also been empirically studied in Lamb et al. (2019).
Generalization. Generalization theory has been a central focus of learning theory (Vapnik, 1979;
2013; Bartlett et al., 2002; Bartlett & Mendelson, 2002; Bousquet & Elisseeff, 2002; Xu & Mannor,
2012), but it still remains a mystery for many modern deep learning algorithms (Zhang et al., 2016;
Kawaguchi et al., 2017). For Mixup, from Fig. (1b), we observe that Mixup training results in
better test performance than the standard empirical risk minimization. That is mainly due to its good
generalization property since the training errors are small for both Mixup training and empirical risk
minimization (experiments with training error results are included in the appendix). While there
have been many enlightening studies trying to establish generalization theory for modern machine
learning algorithms (Sun et al., 2015; Neyshabur et al., 2015; Hardt et al., 2016; Bartlett et al.,
2017; Kawaguchi et al., 2017; Arora et al., 2018; Neyshabur & Li, 2019), few existing studies have
illustrated the generalization behavior of Mixup training in theory.
Our contributions.
In this paper, we theoretically investigate how Mixup improves both adver-
sarial robustness and generalization. We begin by relating the loss function induced by Mixup to
the standard loss with additional adaptive regularization terms. Based on the derived regulariza-
tion terms, we show that Mixup training minimizes an upper bound on the adversarial loss,which
leads to the robustness against single-step adversarial attacks. For generalization, we show how the
regularization terms can reduce over-ﬁtting and lead to better generalization behaviors than those of
standard training. Our analyses provides insights and framework to understand the impact of Mixup.
Outline of the paper.
Section 2 introduces the notations and problem setup. In Section 3, we
present our main theoretical results, including the regularization effect of Mixup and the subse-
quent analysis to show that such regularization improves adversarial robustness and generalization.
Section 4 concludes with a discussion of future work. Proofs are deferred to the Appendix.
1.1
RELATED WORK
Since its advent, Mixup training (Zhang et al., 2018) has been shown to substantially improve gen-
eralization and single-step adversarial robustness among a wide rage of tasks, on both supervised
(Lamb et al., 2019; Verma et al., 2019a; Guo et al., 2019), and semi-supervised settings (Berth-
elot et al., 2019; Verma et al., 2019b). This has motivated a recent line of work for developing a
number of variants of Mixup, including Manifold Mixup (Verma et al., 2019a), Puzzle Mix (Kim
et al., 2020), CutMix (Yun et al., 2019), Adversarial Mixup Resynthesis (Beckham et al., 2019),
and PatchUp (Faramarzi et al., 2020). However, theoretical understanding of the underlying mech-
anism of why Mixup and its variants perform well on generalization and adversarial robustness is
still limited.
2

Published as a conference paper at ICLR 2021
Some of the theoretical tools we use in this paper are related to Wang & Manning (2013) and Wager
et al. (2013), where the authors use second-order Taylor approximation to derive a regularized loss
function for Dropout training. This technique is then extended to drive more properties of Dropout,
including the inductive bias of Dropout (Helmbold & Long, 2015), the regularization effect in ma-
trix factorization (Mianjy et al., 2018), and the implicit regularization in neural networks (Wei et al.,
2020). This technique has been recently applied to Mixup in a parallel and independent work (Car-
ratino et al., 2020) to derive regularization terms. Compared with the results in Carratino et al.
(2020), our derived regularization enjoys a simpler form and therefore enables the subsequent anal-
ysis of adversarial robustness and generalization. We clarify the detailed differences in Section 3.
To the best of our knowledge, our paper is the ﬁrst to provide a theoretical treatment to connect the
regularization, adversarial robustness, and generalization for Mixup training.
2
PRELIMINARIES
In this section, we state our notations and brieﬂy recap the deﬁnition of Mixup.
Notations.
We denote the general parameterized loss as l(θ, z), where θ ∈Θ ⊆Rd and z =
(x, y) is the input and output pair. We consider a training dataset S = {(x1, y1), · · · , (xn, yn)},
where xi ∈X ⊆Rp and yi ∈Y ⊆Rm are i.i.d. drawn from a joint distribution Px,y. We
further denote ˜xi,j(λ) = λxi + (1 −λ)xj, ˜yi,j(λ) = λyi + (1 −λ)yj for λ ∈[0, 1] and let
˜zi,j(λ) = (˜xi,j(λ), ˜yi,j(λ)). Let L(θ) = Ez∼Px,yl(θ, z) denote the standard population loss and
Lstd
n (θ, S) = Pn
i=1 l(θ, zi)/n denote the standard empirical loss. For the two distributions D1 and
D2, we use pD1 + (1 −p)D2 for p ∈(0, 1) to denote the mixture distribution such that a sample is
drawn with probabilities p and (1 −p) from D1 and D2 respectively. For a parameterized function
fθ(x), we use ∇fθ(x) and ∇θfθ(x) to respectively denote the gradient with respect to x and θ. For
two vectors a and b, we use cos(x, y) to denote ⟨x, y⟩/(∥x∥· ∥y∥).
Mixup.
Generally, for classiﬁcation cases, the output yi is the embedding of the class of xi, i.e.
the one-hot encoding by taking m as the total number of classes and letting yi ∈{0, 1}m be the
binary vector with all entries equal to zero except for the one corresponding to the class of xi. In
particular, if we take m = 1, it degenerates to the binary classiﬁcation. For regression cases, yi can
be any real number/vector. The Mixup loss is deﬁned in the following form:
Lmix
n (θ, S) = 1
n2
n
X
i,j=1
Eλ∼Dλl(θ, ˜zij(λ)),
(1)
where Dλ is a distribution supported on [0, 1]. Throughout the paper, we consider the most com-
monly used Dλ – Beta distribution Beta(α, β) for α, β > 0.
3
MAIN RESULTS
In this section, we ﬁrst introduce a lemma that characterizes the regularization effect of Mixup.
Based on this lemma, we then derive our main theoretical results on adversarial robustness and
generalization error bound in Sections 3.2 and 3.3 respectively.
3.1
THE REGULARIZATION EFFECT OF MIXUP
As a starting point, we demonstrate how Mixup training is approximately equivalent to optimizing
a regularized version of standard empirical loss Lstd
n (θ, S). Throughout the paper, we consider the
following class of loss functions for the prediction function fθ(x) and target y:
L = {l(θ, (x, y))|l(θ, (x, y)) = h(fθ(x)) −yfθ(x) for some function h}.
(2)
This function class L includes many commonly used losses, including the loss function induced by
Generalized Linear Models (GLMs), such as linear regression and logistic regression, and also cross-
entropy for neural networks. In the following, we introduce a lemma stating that the Mixup training
with λ ∼Dλ = Beta(α, β) induces a regularized loss function with the weights of each regulariza-
tion speciﬁed by a mixture of Beta distributions ˜Dλ =
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α).
3

Published as a conference paper at ICLR 2021
Lemma 3.1. Consider the loss function l(θ, (x, y)) = h(fθ(x)) −yfθ(x), where h(·) and fθ(·)
for all θ ∈Θ are twice differentiable. We further denote ˜Dλ as a uniform mixture of two Beta
distributions, i.e.,
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α), and DX as the empirical distribution
of the training dataset S = (x1, · · · , xn), the corresponding Mixup loss Lmix
n (θ, S), as deﬁned in
Eq. (1) with λ ∼Dλ = Beta(α, β), can be rewritten as
Lmix
n (θ, S) = Lstd
n (θ, S) +
3
X
i=1
Ri(θ, S) + Eλ∼˜
Dλ[(1 −λ)2ϕ(1 −λ)],
where lima→0 ϕ(a) = 0 and
R1(θ, S) = Eλ∼˜
Dλ[1 −λ]
n
n
X
i=1
(h′(fθ(xi)) −yi)∇fθ(xi)⊤Erx∼DX[rx −xi],
R2(θ, S) = Eλ∼˜
Dλ[(1 −λ)2]
2n
n
X
i=1
h′′(fθ(xi))∇fθ(xi)⊤Erx∼DX[(rx −xi)(rx −xi)⊤]∇fθ(xi),
R3(θ, S) = Eλ∼˜
Dλ[(1 −λ)2]
2n
n
X
i=1
(h′(fθ(xi)) −yi)Erx∼DX[(rx −xi)∇2fθ(xi)(rx −xi)⊤].
By putting the higher order terms of approximation in ϕ(·), this result shows that Mixup is related
to regularizing ∇fθ(xi) and ∇2fθ(xi), which are the ﬁrst and second directional derivatives with
respect to xi. Throughout the paper, our theory is mainly built upon analysis of the quadratic ap-
proximation of Lmix
n (θ, S), which we further denote as
˜Lmix
n (θ, S) := Lstd
n (θ, S) +
3
X
i=1
Ri(θ, S).
(3)
Comparison with related work.
The result in Lemma 3.1 relies on the second-order Taylor ex-
pansion of the loss function Eq. (1). Similar approximations have been proposed before to study the
regularization effect of Dropout training, see Wang & Manning (2013); Wager et al. (2013); Mianjy
et al. (2018); Wei et al. (2020). Recently, Carratino et al. (2020) independently used similar approx-
imation to study the regularization effect of Mixup. However, the regularization terms derived in
Carratino et al. (2020) is much more complicated than those in Lemma 3.1. For example, in GLM,
our technique yields the regularization term as shown in Lemma 3.3, which is much simpler than
those in Corollaries 2 and 3 in Carratino et al. (2020). One technical step we use here to simplify the
regularization expression is to equalize Mixup with input perturbation, see more details in the proof
in the Appendix. This simpler expression enables us to study the robustness and generalization of
Mixup in the subsequent sections.
Validity of the approximation.
In the following, we present numerical experiments to support
the approximation in Eq. (3). Following the setup of numerical validations in Wager et al. (2013);
Carratino et al. (2020), we experimentally show that the quadratic approximation is generally very
accurate. Speciﬁcally, we train a Logistic Regression model (as one example of a GLM model,
which we study later) and a two layer neural network with ReLU activations. We use the two-moons
dataset (Buitinck et al., 2013). Fig. 2 shows the training and test data’s loss functions for training
two models with different loss functions: the original Mixup loss and the approximate Mixup loss.
Both models had the same random initialization scheme. Throughout training, we compute the
test and training loss of each model using its own loss function. The empirical results shows the
approximation of Mixup loss is quite close to the original Mixup loss.
3.2
MIXUP AND ADVERSARIAL ROBUSTNESS
Having introduced ˜Lmix
n (θ, S) in Eq. (3), we are now ready to state our main theoretical results. In
this subsection, we illustrate how Mixup helps adversarial robustness. We prove that minimizing
˜Lmix
n (θ, S) is equivalent to minimizing an upper bound of the second order Taylor expansion of an
adversarial loss.
4

Published as a conference paper at ICLR 2021
Logistic Regression
Two Layer ReLU Neural Network
Figure 2: Comparison of the original Mixup loss with the approximate Mixup loss function.
Throughout this subsection, we study the logistic loss function
l(θ, z) = log(1 + exp(fθ(x))) −yfθ(x),
where y ∈Y = {0, 1}. In addition, let g be the logistic function such that g(s) = es/(1 + es) and
consider the case where θ is in the data-dependent space Θ, deﬁned as
Θ = {θ ∈Rd : yifθ(xi) + (yi −1)fθ(xi) ≥0 for all i = 1, . . . , n}.
Notice that Θ contains the set of all θ with zero training errors:
Θ ⊇{θ ∈Rq : the label prediction ˆyi = 1{fθ(xi) ≥0} is equal to yi for all i = 1, . . . , n }.
(4)
In many practical cases, the training error (0-1 loss) becomes zero in ﬁnite time although the training
loss does not. Equation (4) shows that the condition of θ ∈Θ is satisﬁed in ﬁnite time in such
practical cases with zero training errors.
Logistic regression.
As a starting point, we study the logistic regression with fθ(x) = θ⊤x,
in which case the number of parameters coincides with the data dimension, i.e. p = d. For a
given ε > 0, we consider the adversarial loss with ℓ2-attack of size ε
√
d, that is, Ladv
n
(θ, S) =
1/n Pn
i=1 max∥δi∥2≤ε
√
d l(θ, (xi + δi, yi)). We ﬁrst present the following second order Taylor ap-
proximation of Ladv
n
(θ, S).
Lemma 3.2. The second order Taylor approximation of Ladv
n
(θ, S) is Pn
i=1 ˜ladv(ε
√
d, (xi, yi))/n,
where for any η > 0, x ∈Rp and y ∈{0, 1},
˜ladv(η, (x, y)) = l(θ, (x, y)) + η|g(x⊤θ) −y| · ∥θ∥2 + η2
2 · g(x⊤θ)(1 −g(x⊤θ)) · ∥θ∥2
2.
(5)
By comparing ˜ladv(δ, (x, y)) and ˜Lmix
n (θ, S) applied to logistic regression, we prove the following.
Theorem 3.1. Suppose that fθ(x) = x⊤θ and there exists a constant cx > 0 such that ∥xi∥2 ≥
cx
√
d for all i ∈{1, . . . , n}. Then, for any θ ∈Θ, we have
˜Lmix
n (θ, S) ≥1
n
n
X
i=1
˜ladv(εi
√
d, (xi, yi)) ≥1
n
n
X
i=1
˜ladv(εmix
√
d, (xi, yi))
where εi = RicxEλ∼˜
Dλ[1 −λ] with Ri = | cos(θ, xi)|, and εmix = R · cxEλ∼˜
Dλ[1 −λ] with
R = mini∈{1,...,n} | cos(θ, xi)|.
Theorem 3.1 suggests that ˜Lmix
n (θ, S) is an upper bound of the second order Taylor expansion of the
adversarial loss with ℓ2-attack of size εmix
√
d. Note that εmix depends on θ; one can think the ﬁnal
radius is taken at the minimizer of ˜Lmix
n (θ, S). Therefore, minimizing the Mixup loss would result in
a small adversarial loss. Our analysis suggests that Mixup by itself can improve robustness against
small attacks, which tend to be single-step attacks (Lamb et al., 2019). An interesting direction
for future work is to explore whether combining Mixup with adversarial training is able to provide
robustness against larger and more sophisticated attacks such as iterative projected gradient descent
and other multiple-step attacks.
5

Published as a conference paper at ICLR 2021
60
80
100
Train accuracy
0
100
200
300
400
epoch
10 3
10 2
R
(a) Linear
40
60
80
100
Train accuracy
0
100
200
300
400
epoch
10 4
10 3
10 2
10 1
R
(b) ANN
0.0
0.2
0.4
0.6
0.8
1.0
Ri
0
1
2
3
4
Density
(c) ANN: epoch = 0
0.0
0.2
0.4
0.6
0.8
1.0
Ri
0
1
2
3
4
5
Density
(d) ANN: epoch = 400
Figure 3: The behaviors of the values of R and Ri during training for linear models and artiﬁcial
neural network with ReLU (ANN). The subplots (c) and (d) show the histogram of (R1, R2, . . . , Rn)
for ANN before and after training. R and Ri control the radii of adversarial attacks that Mixup
training protects for.
Remark 3.1. Note that Theorem 3.1 also implies adversarial robustness against ℓ∞attacks with size
ε since for any attack δ, ∥δ∥∞≤ε implies ∥δ∥2 ≤ε
√
d, and therefore max∥δ∥∞≤ϵ l(θ, (x+δ, y)) ≤
max∥δ∥2⩽
√
d·ϵ l(θ, (x + δ, y)).
In the following we provide more discussion about the range of R = mini∈{1,...,n} | cos(θ, xi)|. We
ﬁrst show that under additional regularity conditions, we can obtain a high probability lower bound
that does not depend on sample size. We then numerically demonstrate that R tends to increase
during training for both cases of linear models and neural networks at the end of this subsection.
A constant lower bound for logistic regression. Now, we show how to obtain a constant lower bound
by adding some additional conditions.
Assumption 3.1. Let us denote ˆΘn ⊆Θ as the set of minimizers of ˜Lmix
n (θ, S). We assume there
exists a set Θ∗1, such that for all n ≥N, where N is a positive integer, ˆΘn ⊆Θ∗with probability
at least 1 −δn where δn →0 as n →0. Moreover, there exists a τ ∈(0, 1) such that
pτ = P ({x ∈X : | cos(x, θ)| ≥τ for all θ ∈Θ∗}) ∈(0, 1].
Such condition generally holds for regular optimization problems, where the minimizers are not lo-
cated too dispersedly in the sense of solid angle (instead of Euclidean distance). More speciﬁcally,
if we normalize all the minimizers’ ℓ2 norm to 1, this assumption requires that the set of minimizers
should not be located all over the sphere. In addition, Assumption 3.1 only requires that the proba-
bility pτ and the threshold τ to be non-zero. In particular, if the distribution of x has positive mass
in all solid angles, then when the set of minimizers is discrete, this assumption holds. For more
complicated cases in which the set of minimizers consists of sub-manifolds, as long as there exists
a solid angle in X that is disjoint with the set of minimizers, the assumption still holds.
Theorem 3.2. Under Assumption 3.1, for fθ(x) = x⊤θ, if there exists constants bx, cx > 0 such
that cx
√
d ≤∥xi∥2 ≤bx
√
d for all i ∈{1, . . . , n}. Then, with probability at least 1 −δn −
2 exp(−np2
τ/2), there exists constants κ > 0, κ2 > κ1 > 0, such that for any θ ∈ˆΘn, we have
˜Lmix
n (θ, S) ≥1
n
n
X
i=1
˜ladv(˜εmix
√
d, (xi, yi))
where ˜εmix = ˜RcxEλ∼˜
Dλ[1 −λ] and ˜R = min
n
pτ κ1
2κ2−pτ (κ2−κ1),
q
4κpτ
2−pτ +4κpτ
o
τ.
Neural networks with ReLU / Max-pooling.
The results in the above subsection can be extended
to the case of neural networks with ReLU activation functions and max-pooling. Speciﬁcally, we
1Under some well-separation and smoothness conditions, we would expect all elements in ˆΘn will fall into
a neighborhood Nn of minimizers of ES ˜Lmix
n (θ, S), and Nn will shrink as n increases, i.e. Nn+1 ⊂Nn. One
can think Θ∗is a set containing all Nn for n ≥N.
6

Published as a conference paper at ICLR 2021
consider the logistic loss, l(θ, z) = log(1 + exp(fθ(x))) −yfθ(x) with y ∈{0, 1}, where fθ(x)
represents a fully connected neural network with ReLU activation function or max-pooling:
fθ(x) = β⊤σ
 WN−1 · · · (W2σ(W1x)

.
Here, σ represents nonlinearity via ReLU and max pooling, each Wi is a matrix, and β is a column
vector: i.e., θ consists of {Wi}N−1
i=1
and β. With the nonlinearity σ for ReLU and max-pooling,
the function fθ satisﬁes that fθ(x) = ∇fθ(x)⊤x and ∇2fθ(x) = 0 almost everywhere, where
the gradient is taken with respect to input x. Under such conditions, similar to Lemma 3.2, the
adversarial loss function Pn
i=1 max∥δi∥2≤ε
√
d l(θ, (xi + δi, yi))/n can be written as
Lstd
n (θ, S)+εmix
√
d( 1
n
n
X
i=1
|g(fθ(xi))−yi|∥∇fθ(xi)∥2)+ ε2
mixd
2
( 1
n
n
X
i=1
|h′′(fθ(xi))|∥∇fθ(xi)∥2
2).
(6)
With a little abuse of notations, we also denote
˜ladv(δ, (x, y)) = l(θ, (x, y)) + δ|g(fθ(x)) −y|∥∇fθ(x)∥2 + δ2d
2 |h′′(fθ(x))|∥∇fθ(x)∥2
2.
The following theorem suggests that minimizing the Mixup loss in neural nets also lead to a small
adversarial loss.
Theorem 3.3. Assume that fθ(xi) = ∇fθ(xi)⊤xi, ∇2fθ(xi) = 0 (which are satisﬁed by the ReLU
and max-pooling activation functions) and there exists a constant cx > 0 such that ∥xi∥2 ≥cx
√
d
for all i ∈{1, . . . , n}. Then, for any θ ∈Θ, we have
˜Lmix
n (θ, S) ≥1
n
n
X
i=1
˜ladv(εi
√
d, (xi, yi)) ≥1
n
n
X
i=1
˜ladv(εmix
√
d, (xi, yi))
where εi = RicxEλ∼˜
Dλ[1 −λ], εmix = R · cxEλ∼˜
Dλ[1 −λ] and Ri = | cos(∇fθ(xi), xi)|, R =
mini∈{1,...,n} | cos(∇fθ(xi), xi)|.
Similar constant lower bound can be derived to the setting of neural networsk. Due to limited space,
please see the detailed discussion in the appendix.
On the value of R = mini Ri via experiments. For both linear models and neural networks, after
training accuracy reaches 100%, the logistic loss is further minimized when ∥fθ(xi)∥2 increases.
Since ∥fθ(xi)∥2 = ∥∇fθ(xi)⊤xi∥2 = ∥∇fθ(xi)∥2∥xi∥2Ri, this suggests that Ri and R tend to
increase after training accuracy reaches 100% (e.g., ∇fθ(xi) = θ in the case of linear models). We
conﬁrm this phenomenon in Fig. 3. In the ﬁgure, R is initially small but tends to increase after
training accuracy reaches 100%, as expected. For example, for ANN, the value of R was initially
2.27×10−5 but increased to 6.11×10−2 after training. Fig. 3 (c) and (d) also show that Ri for each
i-th data point tends to increase during training and that the values of Ri for many points are much
larger than the pessimistic lower bound R: e.g., whereas R = 6.11 × 10−2, we have Ri > 0.8 for
several data points in Fig. 3 (d). For this experiment, we generated 100 data points as xi ∼N(0, I)
and yi = 1{x⊤
i θ∗> 0} where xi ∈R10 and θ∗∼N(0, I). We used SGD to train linear models
and ANNs with ReLU activations and 50 neurons per each of two hidden layers. We set the learning
rate to be 0.1 and the momentum coefﬁcient to be 0.9. We turned off weight decay so that R is not
maximized as a result of bounding ∥∇fθ(xi)∥, which is a trivial case from the discussion above.
3.3
MIXUP AND GENERALIZATION
In this section, we show that the data-dependent regularization induced by Mixup directly controls
the Rademacher complexity of the underlying function classes, and therefore yields concrete gener-
alization error bounds. We study two models – the Generalized Linear Model (GLM) and two-layer
ReLU nets with squared loss.
Generalized linear model.
A Generalized Linear Model is a ﬂexible generalization of ordinary
linear regression, where the corresponding loss takes the following form:
l(θ, (x, y)) = A(θ⊤x) −yθ⊤x,
7

Published as a conference paper at ICLR 2021
where A(·) is the log-partition function, x ∈Rp and y ∈R. For instance, if we take A(θ⊤x) =
log(1 + eθ⊤x) and y ∈{0, 1}, then the model corresponds to the logistic regression. In this para-
graph, we consider the case where Θ, X and Y are all bounded.
By further taking advantage of the property of shift and scaling invariance of GLM, we can further
simplify the regularization terms in Lemma 3.1 and obtain the following results.
Lemma 3.3. Consider the centralized dataset S, that is, 1/n Pn
i=1 xi = 0. and denote ˆΣX =
1
nxix⊤
i . For a GLM, if A(·) is twice differentiable, then the regularization term obtained by the
second-order approximation of ˜Lmix
n (θ, S) is given by
1
2n[
n
X
i=1
A′′(θ⊤xi)] · Eλ∼˜
Dλ[(1 −λ)2
λ2
]θ⊤ˆΣXθ,
(7)
where ˜Dλ =
α
α+β Beta(α + 1, β) +
α
α+β Beta(β + 1, α).
Given the above regularization term, we are ready to investigate the corresponding generalization
gap. Following similar approaches in Arora et al. (2020), we shed light upon the generalization
problem by investigating the following function class that is closely related to the dual problem of
Eq. (7):
Wγ := {x →θ⊤x, such that θ satisfying ExA′′(θ⊤x) · θ⊤ΣXθ ⩽γ},
where α > 0 and ΣX = E[xix⊤
i ]. Further, we assume that the distribution of x is ρ-retentive
for some ρ ∈(0, 1/2], that is, if for any non-zero vector v ∈Rd,

Ex[A′′(x⊤v)]
2 ≥ρ ·
min{1, Ex(v⊤x)2}. Such an assumption has been similarly assumed in Arora et al. (2020) and
is satisﬁed by general GLMs when θ has bounded ℓ2 norm. We then have the following theorem.
Theorem 3.4. Assume that the distribution of xi is ρ-retentive, and let ΣX = E[xx⊤]. Then the
empirical Rademacher complexity of Wγ satisﬁes
Rad(Wγ, S) ≤max{(γ
ρ)1/4, (γ
ρ)1/2} ·
r
rank(ΣX)
n
.
The above bound on Rademacher complexity directly implies the following generalization gap of
Mixup training.
Corollary 3.1. Suppose A(·) is LA-Lipchitz continuous, X, Y and Θ are all bounded, then there
exists constants L, B > 0, such that for all θ satisfying ExA′′(θ⊤x)·θ⊤ΣXθ ⩽γ (the regularization
induced by Mixup), we have
L(θ) ⩽Lstd
n (θ, S) + 2L · LA ·
 
max{(γ
ρ)1/4, (γ
ρ)1/2} ·
r
rank(ΣX)
n
!
+ B
r
log(1/δ)
2n
,
with probability at least 1 −δ.
Remark 3.2. This result shows that the Mixup training would adapt to the intrinsic dimension of
x and therefore has a smaller generalization error. Speciﬁcally, if we consider the general ridge
penalty and consider the function class Wridge
γ
:= {x →θ⊤x, ∥θ∥2 ⩽γ}, then the similar tech-
nique would yield a Rademacher complexity bound Rad(Wγ, S) ≤max{(γ/ρ)1/4, (γ/ρ)1/2} ·
p
p/n, where p is the dimension of x. This bound is much larger than the result in Theorem 3.4
when the intrinsic dimension rank(ΣX) is small.
Non-linear cases.
The above results on GLM can be extended to the non-linear neural network
case with Manifold Mixup (Verma et al., 2019a). In this section, we consider the two-layer ReLU
neural networks with the squared loss L(θ, S) = 1
n
Pn
i=1(yi −fθ(xi))2, where y ∈R and fθ(x) is
a two-layer ReLU neural network, with the form of
fθ(x) = θ⊤
1 σ
 Wx

+ θ0.
where W ∈Rp×d, θ1 ∈Rd, and θ0 denotes the bias term. Here, θ consists of W, θ0 and θ1.
If we perform Mixup on the second layer (i.e., mix neurons on the hidden layer as proposed by
Verma et al. (2019a)), we then have the following result on the induced regularization.
8

Published as a conference paper at ICLR 2021
Lemma 3.4. Denote ˆΣσ
X as the sample covariance matrix of {σ(Wxi)}n
i=1, then the regularization
term obtained by the second-order approximation of ˜Lmix
n (θ, S) is given by
Eλ∼˜
Dλ[(1 −λ)2
λ2
]θ⊤
1 ˆΣσ
Xθ1,
(8)
where ˜Dλ ∼
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α).
To show the generalization property of this regularizer, similar to the last section, we consider the
following distribution-dependent class of functions indexed by θ:
WNN
γ
:= {x →fθ(x), such that θ satisfying θ⊤
1 Σσ
Xθ1 ⩽γ},
where Σσ
X = E[ˆΣσ
X] and α > 0. We then have the following result.
Theorem 3.5. Let µσ = E[σ(Wx)] and denote the generalized inverse of Σσ
X by Σσ†
X . Suppose X,
Y and Θ are all bounded, then there exists constants L, B > 0, such that for all fθ in WNN
γ
(the
regularization induced by Manifold Mixup), we have, with probability at least 1 −δ,
L(θ) ⩽Lstd
n (θ, S) + 4L ·
s
γ · (rank(Σσ
X) + ∥Σσ†/2
X
µσ∥2)
n
+ B
r
log(1/δ)
2n
.
4
CONCLUSION AND FUTURE WORK
Mixup is a data augmentation technique that generates new samples by linear interpolation of multi-
ple samples and their labels. The Mixup training method has been empirically shown to have better
generalization and robustness against attacks with adversarial examples than the traditional training
method, but there is a lack of rigorous theoretical understanding. In this paper, we prove that the
Mixup training is approximately a regularized loss minimization. The derived regularization terms
are then used to demonstrate why Mixup has improved generalization and robustness against one-
step adversarial examples. One interesting future direction is to extend our analysis to other Mixup
variants, for example, Puzzle Mix (Kim et al., 2020) and Adversarial Mixup Resynthesis (Beckham
et al., 2019), and investigate if the generalization performance and adversarial robustness can be
further improved by these newly developed Mixup methods.
ACKNOWLEDGMENTS
The research of Linjun Zhang is supported by NSF DMS-2015378. The research of James Zou is
supported by NSF CCF 1763191, NSF CAREER 1942926 and grants from the Silicon Valley Foun-
dation and the Chan-Zuckerberg Initiative. The research of Kenji Kawaguchi is partially supported
by the Center of Mathematical Sciences and Applications at Harvard University. This work is also
in part supported by NSF award 1763665.
REFERENCES
Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor, and Kevin McGuinness. Unsupervised
label noise modeling and loss correction. arXiv preprint arXiv:1904.11238, 2019.
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit forms and
capacity control. arXiv preprint arXiv:2003.03397, 2020.
Sanjeev Arora, R Ge, B Neyshabur, and Y Zhang. Stronger generalization bounds for deep nets via
a compression approach. In 35th International Conference on Machine Learning, ICML 2018,
2018.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.
Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In
International Conference on Computational Learning Theory, pp. 44–58. Springer, 2002.
9

Published as a conference paper at ICLR 2021
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240–6249, 2017.
Christopher Beckham, Sina Honari, Vikas Verma, Alex M Lamb, Farnoosh Ghadiri, R Devon Hjelm,
Yoshua Bengio, and Chris Pal. On adversarial mixup resynthesis. In Advances in neural informa-
tion processing systems, pp. 4346–4357, 2019.
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A
Raffel.
Mixmatch: A holistic approach to semi-supervised learning.
In Advances in Neural
Information Processing Systems, pp. 5049–5059, 2019.
Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499–526, 2002.
Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel,
Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake Van-
derPlas, Arnaud Joly, Brian Holt, and Ga¨el Varoquaux. API design for machine learning software:
experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Min-
ing and Machine Learning, pp. 108–122, 2013.
Luigi Carratino, Moustapha Ciss´e, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regular-
ization. arXiv preprint arXiv:2006.06049, 2020.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. In NeurIPS Creativity Workshop 2019, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Mojtaba Faramarzi, Mohammad Amini, Akilesh Badrinaaraayanan, Vikas Verma, and Sarath Chan-
dar.
Patchup: A regularization technique for convolutional neural networks.
arXiv preprint
arXiv:2006.07794, 2020.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regulariza-
tion. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pp. 3714–3722,
2019.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234. PMLR,
2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630–645. Springer, 2016b.
David P Helmbold and Philip M Long. On the inductive bias of dropout. The Journal of Machine
Learning Research, 16(1):3403–3454, 2015.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
arXiv preprint arXiv:1710.05468, 2017.
Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puzzle mix: Exploiting saliency and local
statistics for optimal mixup. International Conference on Machine Learning, 2020.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, Citeseer, 2009.
10

Published as a conference paper at ICLR 2021
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training:
Achieving robust neural networks without sacriﬁcing too much accuracy. In Proceedings of the
12th ACM Workshop on Artiﬁcial Intelligence and Security, pp. 95–103, 2019.
Poorya Mianjy, Raman Arora, and Rene Vidal. On the implicit bias of dropout. In International
Conference on Machine Learning, pp. 3540–3548, 2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Behnam Neyshabur and Zhiyuan Li. Towards understanding the role of over-parametrization in gen-
eralization of neural networks. In International Conference on Learning Representations (ICLR),
2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376–1401, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026–8037, 2019.
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu. On the depth of deep neural
networks: A theoretical view. arXiv preprint arXiv:1506.05232, 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Micha-
lak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks.
In Advances in Neural Information Processing Systems, pp. 13888–13899, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. arXiv preprint arXiv:1805.12152, 2018.
V Vapnik. Estimation of dependences based on empirical data nauka, 1979.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najaﬁ, Ioannis Mitliagkas, David Lopez-
Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states.
In International Conference on Machine Learning, pp. 6438–6447. PMLR, 2019a.
Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation con-
sistency training for semi-supervised learning. In Proceedings of the 28th International Joint
Conference on Artiﬁcial Intelligence, pp. 3635–3641. AAAI Press, 2019b.
Stefan Wager, Sida Wang, and Percy S Liang.
Dropout training as adaptive regularization.
In
Advances in neural information processing systems, pp. 351–359, 2013.
Sida Wang and Christopher Manning. Fast dropout training. In international conference on machine
learning, pp. 118–126, 2013.
Colin Wei, Sham Kakade, and Tengyu Ma.
The implicit and explicit regularization effects of
dropout. arXiv preprint arXiv:2002.12915, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391–423,
2012.
11

Published as a conference paper at ICLR 2021
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In Proceed-
ings of the IEEE International Conference on Computer Vision, pp. 6023–6032, 2019.
Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
12

Published as a conference paper at ICLR 2021
Appendix
In this appendix, we provide proofs of the main theorems and the corresponding technical lemmas.
Additional discussion on the range of R in the case of neural nets, and some further numerical
experiments are also provided.
A
TECHNIQUE PROOFS
A.1
PROOF OF LEMMA 3.1
Consider the following problem with loss function lx,y(θ) := l(θ, (x, y)) = h(fθ(x))−yfθ(x), that
is
Lstd
n (θ, S) = 1
n
n
X
i=1
[h(fθ(xi)) −yifθ(xi)].
The corresponding Mixup version, as deﬁned in Eq.(1), is
Lmix
n
(θ, S) = 1
n2 Eλ∼Beta(α,β)
n
X
i,j=1
[h(fθ(˜xi,j(λ))) −(λyi + (1 −λ)yj)fθ(˜xi,j(λ))],
where ˜xi,j(λ) = λxi + (1 −λ)xj. Further transformation leads to
Lmix
n
(θ, S) = 1
n2 Eλ∼Beta(α,β)
n
X
i,j=1
n
λh(fθ(˜xi,j(λ)))) −λyifθ(˜xi,j(λ))
+ (1 −λ)h(fθ(˜xi,j(λ))) −(1 −λ)yjfθ(˜xi,j(λ))
o
= 1
n2 Eλ∼Beta(α,β)EB∼Bern(λ)
n
X
i,j=1
n
B[h(fθ(˜xi,j(λ))) −yifθ(˜xi,j(λ))]
+ (1 −B)[h(fθ(˜xi,j(λ))) −yjfθ(˜xi,j(λ))]
o
Note that λ ∼Beta(α, β), B|λ ∼Bern(λ), by conjugacy, we can exchange them in order and
have
B ∼Bern(
α
α + β ), λ | B ∼Beta(α + B, β + 1 −B).
As a result,
Lmix
n
(θ, S) = 1
n2
n
X
i,j=1
n
α
α + β Eλ∼Beta(α+1,β)[h(fθ(˜xi,j(λ))) −yifθ(˜xi,j(λ))]
+
β
α + β Eλ∼Beta(α,β+1)[h(fθ(˜xi,j(λ))) −yjfθ(˜xi,j(λ))]
o
.
Using the fact 1−Beta(α, β +1) and Beta(β +1, α) are of the same distribution and ˜xij(1−λ) =
˜xji(λ), we have
X
i,j
Eλ∼Beta(α,β+1)[h(fθ(˜xi,j(λ))) −yjfθ(˜xi,j(λ))]
=
X
i,j
Eλ∼Beta(β+1,α)[h(fθ(˜xi,j(λ))) −yifθ(˜xi,j(λ))].
Thus, let ˜Dλ =
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α)
Lmix
n (θ, S) = 1
n
n
X
i=1
Eλ∼˜
DλErx∼Dxh(f(θ, λxi + (1 −λ)rx))) −yif(θ, λxi + (1 −λ)rx)
= 1
n
n
X
i=1
Eλ∼˜
DλErx∼Dxlˇxi,yi(θ)
(9)
13

Published as a conference paper at ICLR 2021
where Dx is the empirical distribution induced by training samples, and ˇxi = λxi + (1 −λ)rx.
In the following, denote ˇS = {(ˇxi, yi)}n
i=1, and let us analyze Lstd
n (θ, ˇS) = 1
n
Pn
i=1 lˇxi,yi(θ), and
compare it with Lstd
n (θ, S). Let α = 1 −λ and ψi(α) = lˇxi,yi(θ). Then, using the deﬁnition of the
twice-differentiability of function ψi,
lˇxi,yi(θ) = ψi(α) = ψi(0) + ψ′
i(0)α + 1
2ψ′′
i (0)α2 + α2ϕi(α),
(10)
where limz→0 ϕi(z) = 0. By linearity and chain rule,
ψ′
i(α) = h′(fθ(ˇxi))∂fθ(ˇxi)
∂ˇxi
∂ˇxi
∂α −yi
∂fθ(ˇxi)
∂ˇxi
∂ˇxi
∂α
= h′(fθ(ˇxi))∂fθ(ˇxi)
∂ˇxi
(rx −xi) −yi
∂fθ(ˇxi)
∂ˇxi
(rx −xi)
where we used ∂ˇxi
∂α = (rx −xi). Since
∂
∂α
∂fθ(ˇxi)
∂ˇxi
(rx−xi) = ∂
∂α(rx−xi)⊤[∂fθ(ˇxi)
∂ˇxi
]⊤= (rx−xi)⊤∇2fθ(ˇxi)∂ˇxi
∂α = (rx−xi)⊤∇2fθ(ˇxi)(rx−xi),
we have
ψ′′
i (α) =h′(fθ(ˇxi))(rx −xi)⊤∇2fθ(ˇxi)(rx −xi)
+ h′′(fθ(ˇxi))[∂fθ(ˇxi)
∂ˇxi
(rx −xi)]2 −yi(rx −xi)⊤∇2fθ(ˇxi)(rx −xi).
Thus,
ψ′
i(0) = h′(fθ(xi))∇fθ(xi)⊤(rx−xi)−yi∇fθ(xi)⊤(rx−xi) = (h′(fθ(xi))−yi)∇fθ(xi)⊤(rx−xi)
ψ′′
i (0) =h′(fθ(xi))(rx −xi)⊤∇2fθ(xi)(rx −xi) + h′′(fθ(xi))[∇fθ(xi)⊤(rx −xi)]2
−yi(rx −xi)⊤∇2fθ(xi)(rx −xi).
=h′′(fθ(xi))∇fθ(xi)⊤(rx −xi)(rx −xi)⊤∇fθ(xi) + (h′(fθ(xi)) −yi)(rx −xi)⊤∇2fθ(xi)(rx −xi)
By substituting these into equation 10 with ϕ(α) = 1
n
Pn
i=1 ϕi(α), we obtain the desired statement.
A.2
PROOFS RELATED TO ADVERSARIAL ROBUSTNESS
A.2.1
PROOF OF LEMMA 3.2
Recall that Ladv
n
(θ, S) = 1
n
Pn
i=1 max∥δi∥2≤ε
√
d l(θ, (xi + δi, yi)) and g(u) = 1/(1 + e−u). Then
the second-order Taylor expansion of l(θ, (x + δ, t)) is given by
l(θ, (x + δ, y)) = l(θ, (x, y)) + (g(θ⊤x) −y) · δ⊤θ + 1
2g(x⊤θ)(1 −g(x⊤θ)) · (δ⊤θ)2.
Consequently, for any given η > 0,
max
∥δ∥2≤η l(θ, (x + δ, y)) = max
∥δ∥2≤η l(θ, (x, y)) + (g(θ⊤x) −y) · δ⊤θ + 1
2g(x⊤θ)(1 −g(x⊤θ)) · (δ⊤θ)2
=l(θ, (x, y)) + η|g(x⊤θ) −y| · ∥θ∥2 + η2
2 (g(x⊤θ)(1 −g(x⊤θ))) · ∥θ∥2,
where the maximum is taken when δ = sgn(g(x⊤θ) −y) ·
θ
∥θ∥· η.
A.2.2
PROOF OF THEOREM 3.1
Since fθ(x) = x⊤θ, we have ∇fθ(xi) = θ and ∇2fθ(xi) = 0. Since h(z) = log(1 + ez), we have
h′(z) =
ez
1+ez = g(z) ≥0 and h′′(z) =
ez
(1+ez)2 = g(z)(1 −g(z)) ≥0. By substituting these into
the equation of Lemma 3.1 with Erx[rx] = 0,
˜Lmix
n (θ, S) = ˜Lmix
n (θ, S) + R1(θ, S) + R2(θ, S),
(11)
14

Published as a conference paper at ICLR 2021
where
R1(θ, S) = Eλ[(1 −λ)]
n
n
X
i=1
(yi −g(x⊤
i θ))θ⊤xi
R2(θ, S) = Eλ[(1 −λ)2]
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|θ⊤Erx[(rx −xi)(rx −xi)⊤]θ
≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|θ⊤Erx[(rx −xi)(rx −xi)⊤]θ
where we used E[z2] = E[z]2 + Var(z) ≥E[z]2 and θ⊤Erx[(rx −xi)(rx −xi)⊤]θ ≥0. Since
Erx[(rx −xi)(rx −xi)⊤] = Erx[rxr⊤
x −rxx⊤
i −xir⊤
x + xix⊤
i ] = Erx[rxr⊤
x ] + xix⊤
i where
Erx[rxr⊤
x ] is positive semideﬁnite,
R2(θ, S) ≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|θ⊤(Erx[rxr⊤
x ] + xix⊤
i )θ.
≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|(θ⊤xi)2
= Eλ[(1 −λ)]2
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|∥θ∥2
2∥xi∥2
2(cos(θ, xi))2
≥R2c2
xEλ[(1 −λ)]2d
2n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|∥θ∥2
2
Now we bound E = Eλ[(1−λ)]
n
Pn
i=1(yi −g(x⊤
i θ))(θ⊤xi) by using θ ∈Θ. Since θ ∈Θ, we have
yifθ(xi) + (yi −1)fθ(xi) ≥0, which implies that (θ⊤xi) ≥0 if yi = 1 and (θ⊤xi) ≤0 if yi = 0.
Thus, if yi = 1,
(yi −g(x⊤
i θ))(θ⊤xi) = (1 −g(x⊤
i θ))(θ⊤xi) ≥0,
since (θ⊤xi) ≥0 and (1 −g(x⊤
i θ)) ≥0 due to g(x⊤
i θ) ∈(0, 1). If yi = 0,
(yi −g(x⊤
i θ))(θ⊤xi) = −g(x⊤
i θ)(θ⊤xi) ≥0,
since (θ⊤xi) ≤0 and −g(x⊤
i θ) < 0. Therefore, for all i = 1, . . . , n,
(yi −g(x⊤
i θ))(θ⊤xi) ≥0,
which implies that, since Eλ[(1 −λ)] ≥0,
R1(θ, S) = Eλ[(1 −λ)]
n
n
X
i=1
|yi −g(x⊤
i θ)||θ⊤xi|
= Eλ[(1 −λ)]
n
n
X
i=1
|g(x⊤
i θ) −yi|∥θ∥2∥xi∥2| cos(θ, xi)|
≥RcxEλ[(1 −λ)]
√
d
n
n
X
i=1
|g(x⊤
i θ) −yi|∥θ∥2
By substituting these lower bounds of R1(θ, S) and R2(θ, S) into equation 11, we obtain the desired
statement.
A.2.3
PROOF OF THEOREM 3.2
Recall we assume that ˆθn will fall into the set Θ∗with probability at least 1 −δn, and δn →0 as
n →∞. In addition, deﬁne the set
XΘ∗(τ) = {x ∈X : | cos(x, θ)| ⩾τ for all θ ∈Θ∗},
15

Published as a conference paper at ICLR 2021
there is τ ∈(0, 1) such that XΘ∗(τ) ̸= ∅, and
pτ := P(x ∈XΘ∗(τ)) ∈(0, 1).
Let us ﬁrst study
1
n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|(cos(θ, xi))2
Since we assume Θ∗is bounded and cx
√
d ⩽∥xi∥2 ⩽bx
√
d for all i, there exists κ > 0, such that
|g(x⊤
i θ)(1 −g(x⊤
i θ))| ⩾κ.
If we denote ˆp = {number of x′
is such that xi ∈XΘ∗(τ)}/n. Then, it is easy to see
1
n
P
xi∈X c
Θ∗(τ) |g(x⊤
i θ)(1 −g(x⊤
i θ))|
1
n
P
xi∈XΘ∗(τ) |g(x⊤
i θ)(1 −g(x⊤
i θ))| ⩽(1 −ˆp)/4
ˆpκ
For η2 satisfying
η2(1 + (1 −ˆp)/4
ˆpκ
) ⩽τ 2
we have
1
n
n
X
i=1
|g(x⊤
i θ)(1 −g(x⊤
i θ))|(cos(θ, xi))2 ⩾1
n
X
xi∈XΘ∗(τ)
|g(x⊤
i θ)(1 −g(x⊤
i θ))|τ 2
⩾1
n
X
xi∈XΘ∗(τ)
|g(x⊤
i θ)(1 −g(x⊤
i θ))|η2 + 1
n
X
xi∈X c
Θ∗(τ)
|g(x⊤
i θ)(1 −g(x⊤
i θ))|η2.
Lastly by Hoeffding’s inequality, if we take ε = pτ/2
(1 + (1 −ˆp)/4
ˆpκ
) ⩽(1 + (1 −pτ/2)/4
(pτ/2)κ
)
with probability at least 1 −2 exp(−2nε2)
η ⩽τ
r
4κpτ
2 −pτ + 4κpτ
.
Similarly, if we study
n
X
i=1
|g(x⊤
i θ) −yi|| cos(θ, x)|
By boundedness of θ, x and y ∈{0, 1}, we know there are constants κ1, κ2 > 0, such that
κ1 ⩽|g(fθ(xi)) −yi| ⩽κ2
Similarly, we know
η ⩽
pτκ1
2κ2 −pτ(κ2 −κ1)τ.
Combined together, we can obtain the result:
η ⩽min{
pτκ1
2κ2 −pτ(κ2 −κ1),
r
4κpτ
2 −pτ + 4κpτ
}τ
16

Published as a conference paper at ICLR 2021
A.2.4
PROOF OF THEOREM 3.3
From the assumption, we have fθ(xi) = ∇fθ(xi)⊤xi and ∇2fθ(xi) = 0. Since h(z) = log(1+ez),
we have h′(z) =
ez
1+ez = g(z) ≥0 and h′′(z) =
ez
(1+ez)2 = g(z)(1 −g(z)) ≥0. By substituting
these into the equation of Lemma 3.1 with Erx[rx] = 0,
˜Lmix
n (θ, S) = ˜Lmix
n (θ, S) + R1(θ, S) + R2(θ, S),
(12)
where
R1(θ, S) = Eλ[(1 −λ)]
n
n
X
i=1
(yi −g(fθ(xi)))fθ(xi)
R2(θ, S) = Eλ[(1 −λ)2]
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∇fθ(xi)⊤Erx[(rx −xi)(rx −xi)⊤]∇fθ(xi)
≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∇fθ(xi)⊤Erx[(rx −xi)(rx −xi)⊤]∇fθ(xi)
where we used E[z2] = E[z]2+Var(z) ≥E[z]2 and ∇fθ(xi)⊤Erx[(rx−xi)(rx−xi)⊤]∇fθ(xi) ≥
0. Since Erx[(rx −xi)(rx −xi)⊤] = Erx[rxr⊤
x −rxx⊤
i −xir⊤
x + xix⊤
i ] = Erx[rxr⊤
x ] + xix⊤
i
where Erx[rxr⊤
x ] is positive semideﬁnite,
R2(θ, S) ≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∇fθ(xi)⊤(Erx[rxr⊤
x ] + xix⊤
i )∇fθ(xi).
≥Eλ[(1 −λ)]2
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|(∇fθ(xi)⊤xi)2
= Eλ[(1 −λ)]2
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∥∇fθ(xi)∥2
2∥xi∥2
2(cos(∇fθ(xi), xi))2
≥R2c2
xEλ[(1 −λ)]2d
2n
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∥∇fθ(xi)∥2
2
Now we bound E = Eλ[(1−λ)]
n
Pn
i=1(yi −g(fθ(xi)))fθ(xi) by using θ ∈Θ. Since θ ∈Θ, we have
yifθ(xi) + (yi −1)fθ(xi) ≥0, which implies that fθ(xi) ≥0 if yi = 1 and fθ(xi) ≤0 if yi = 0.
Thus, if yi = 1,
(yi −g(fθ(xi)))(fθ(xi)) = (1 −g(fθ(xi)))(fθ(xi)) ≥0,
since (fθ(xi)) ≥0 and (1 −g(fθ(xi))) ≥0 due to g(fθ(xi)) ∈(0, 1). If yi = 0,
(yi −g(fθ(xi)))(fθ(xi)) = −g(fθ(xi))(fθ(xi)) ≥0,
since (fθ(xi)) ≤0 and −g(fθ(xi)) < 0. Therefore, for all i = 1, . . . , n,
(yi −g(fθ(xi)))(fθ(xi)) ≥0,
which implies that, since Eλ[(1 −λ)] ≥0,
R1(θ, S) = Eλ[(1 −λ)]
n
n
X
i=1
|yi −g(fθ(xi))||fθ(xi)|
= Eλ[(1 −λ)]
n
n
X
i=1
|g(fθ(xi)) −yi|∥∇fθ(xi)∥2∥xi∥2| cos(∇fθ(xi), xi)|
≥RcxEλ[(1 −λ)]
√
d
n
n
X
i=1
|g(fθ(xi)) −yi|∥∇fθ(xi)∥2
By substituting these lower bounds of E and F into equation 12, we obtain the desired statement.
17

Published as a conference paper at ICLR 2021
A.3
PROOFS RELATED TO GENERALIZATION
A.3.1
PROOF OF LEMMA 3.3 AND LEMMA 3.4
We ﬁrst prove Lemma 3.3. The proof of Lemma 3.4 is similar.
By Eq. (9), we have Lmix
n (θ, S) = Lstd
n (θ, ˇS), where ˇS = {(ˇxi, yi)}n
i=1 with ˇxi = λxi + (1 −λ)rx
and λ ∼˜Dλ =
α
α+β Beta(α + 1, β) +
β
α+β Beta(β + 1, α). Since for Generalized Linear Model
(GLM), the prediction is invariant to the scaling of the training data, so it sufﬁces to consider ˜S =
{(˜xi, yi)}n
i=1 with ˜xi = 1
¯λ(λxi + (1 −λ)rx).
In the following, we analyze Lstd
n (θ, ˜S). For GLM the loss function is
Lstd
n (θ, ˜S) = 1
n
n
X
i=1
l˜xi,yi(θ) = 1
n
n
X
i=1
−(yi˜x⊤
i θ −A(˜x⊤
i θ)),
where A(·) is the log-partition function in GLMs.
Denote the randomness (of λ and rx) by ξ, then the second order Taylor expansion yields
Eξ[A(˜x⊤
i θ) −A(x⊤
i θ)]
2nd−order approx.
=
Eξ[A′(x⊤
i θ)(˜xi −xi)⊤θ + A′′(x⊤
i θ)V ar(˜x⊤
i θ)]
Notice Eξ[˜xi −xi] = 0 and V arξ(˜xi) = 1
n
Pn
i=1 xix⊤
i = ˆΣX, then we have the RHS of the last
equation equal to
A′′(x⊤
i θ)(E(1 −λ)2
¯λ2
)θ⊤ˆΣXθ.
As a result, the second-order Taylor approximation of the Mixup loss Lstd
n (θ, ˜S) is
n
X
i=1
−(yix⊤
i θ −A(x⊤
i θ)) + 1
2n[
n
X
i=1
A′′(x⊤
i θ)]E((1 −λ)2
λ2
)θ⊤ˆΣXθ
=Lstd
n (θ, S) + 1
2n[
n
X
i=1
A′′(x⊤
i θ)]E((1 −λ)2
λ2
)θ⊤ˆΣXθ.
This completes the proof of Lemma 3.3. For Lemma 3.4, since the Mixup is performed on the ﬁnal
layer of the neural nets, the setting is the same as the least square with covariates σ(w⊤
j x). Moreover,
since we include both the linear coefﬁcients vector θ1 and bias term θ0, the prediction is invariant to
the shifting and scaling of σ(w⊤
j x). Therefore, we can consider training θ1 and θ0 on the covariates
{(σ(Wxi) −¯σW ) + 1−λ
λ (σ(Wrx) −¯σW )}n
i=1, where ¯σW = 1
n
Pn
i=1 σ(Wxi). Moreover, since
we consider the least square loss, which is a special case of GLM loss with A(u) = 1
2u2, we have
A′′ = 1. Plugging these quantities into Lemma 3.3, we get the desired result of Lemma 3.4.
A.3.2
PROOF OF THEOREM 3.4 AND COROLLARY 3.1
By deﬁnition, given n i.i.d. Rademacher rv. ξ1, ..., ξn, the empirical Rademacher complexity is
Rad(Wγ, S) = Eξ
sup
a(θ)·θ⊤ΣXθ≤γ
1
n
n
X
i=1
ξiθ⊤xi
Let ˜xi = Σ†/2
X xi, a(θ) = Ex[A′′(x⊤θ)] and v = Σ1/2
X θ, then ρ-retentiveness condition implies
a(θ)2 ≥ρ · min{1, Ex(θ⊤x)2} ≥ρ · min{1, θ⊤ΣXθ} and therefore a(θ) · θ⊤ΣXθ ≤γ implies that
∥v∥2 = θ⊤ΣXθ ≤max{( γ
ρ)1/2, γ
ρ}.
18

Published as a conference paper at ICLR 2021
As a result,
Rad(Wγ, S) =Eξ
sup
a(θ)·θ⊤ΣXθ≤γ
1
n
n
X
i=1
ξiθ⊤xi
=Eξ
sup
a(θ)·θ⊤ΣXθ≤γ
1
n
n
X
i=1
ξiv⊤˜xi
≤Eξ
sup
∥v∥2≤( γ
ρ )1/2∨γ
ρ
1
n
n
X
i=1
ξiv⊤˜xi
≤1
n · (γ
ρ)1/4 ∨(γ
ρ)1/2 · Eξ∥
n
X
i=1
ξi˜xi ∥
≤1
n · (γ
ρ)1/4 ∨(γ
ρ)1/2 ·
v
u
u
tEξ∥
n
X
i=1
ξi˜xi ∥2
≤1
n · (γ
ρ)1/4 ∨(γ
ρ)1/2 ·
v
u
u
t
n
X
i=1
˜x⊤
i ˜xi .
Consequently,
Rad(Wγ, S) = ES[Rad(Wγ, S)] ≤1
n · (γ
ρ)1/4 ∨(γ
ρ)1/2 ·
v
u
u
t
n
X
i=1
Exi[˜x⊤
i ˜xi]
≤1
√n · (γ
ρ)1/4 ∨(γ
ρ)1/2 · rank(ΣX).
Based on this bound on Rademacher complexity, Corollary 3.1 can be proved by directly applying
the following theorem.
Lemma A.1 (Result from Bartlett & Mendelson (2002)). For any B-uniformly bounded and L-
Lipchitz function ζ, for all φ ∈Φ, with probability at least 1 −δ,
Eζ(φ(xi)) ≤1
n
n
X
i=1
ζ(φ(xi)) + 2LRad(Φ, S) + B
r
log(1/δ)
2n
.
A.3.3
PROOF OF THEOREM 3.5
To prove Theorem 3.5, by Lemma A.1, it sufﬁces to show the following bound on Rademacher
complexity.
Theorem A.1. The empirical Rademacher complexity of WNN
γ
satisﬁes
Rad(WNN
γ
, S) ≤2
s
γ · (rank(Σσ
X) + ∥Σσ†/2
X
µσ∥2)
n
.
By deﬁnition, given n i.i.d. Rademacher rv. ξ1, ..., ξn, the empirical Rademacher complexity is
Rad(Wγ, S) =Eξ sup
Wγ
1
n
n
X
i=1
ξiθ⊤
1 σ(Wxi).
19

Published as a conference paper at ICLR 2021
Let ˜θ1 = Σσ1/2
X
θ1 and µσ = E[σ(Wx)], then
RS(WNN
γ
) =Eξ sup
WNN
γ
1
n
n
X
i=1
ξi˜θ⊤
1 Σσ†/2
X
(σ(Wxi) −µσ) + Eξ sup
WNN
γ
1
n
n
X
i=1
ξi˜θ⊤
1 Σσ†/2
X
µσ
≤∥˜θ1∥2 · ∥Eξ[ 1
n
n
X
i=1
ξiΣσ†/2
X
σ(Wxi)]∥+ ∥˜θ1∥·
1
√n∥Σσ†/2
X
µσ∥
≤2
s
γ · (rank(Σσ
X) + ∥Σσ†/2
X
µσ∥2)
n
,
where the last inequality is obtained by using the same technique as in the proof of Lemma 3.4.
Combining all the pieces, we get
Rad(Wγ, S) ≤
r
γ · rank(Σσ
X)
n
.
B
DISCUSSION OF R IN THE NEURAL NETWORK CASE
(B.1). On the value of R = mini Ri via experiments for neural networks. After training accuracy
reaches 100%, the loss is further minimized when ∥fθ(xi)∥2 increases. Since
∥fθ(xi)∥2 = ∥∇fθ(xi)⊤xi∥2 = ∥∇fθ(xi)∥2∥xi∥2Ri,
this suggests that Ri and R tend to increase after training accuracy reaches 100%. We conﬁrm this
phenomenon in Figure 3. In the ﬁgure, R is initially small but tend to increase after training accuracy
reaches 100%, as expected. For example, for ANN, the values of R were initially 2.27 × 10−5 but
increased to 6.11 × 10−2 after training. Figure 3 (c) and (d) also show that Ri for each i-th data
point tends to increase during training and that the values of Ri for many points are much larger
than the pessimistic lower bound R: e.g., whereas R = 6.11 × 10−2, we have Ri > 0.8 for several
data points in Figure 3 (d). For this experiment, we generated 100 data points as xi ∼N(0, I) and
yi = 1{x⊤
i θ∗> 0} where xi ∈R10 and θ∗∼N(0, I). We used SGD to train linear models and
ANNs with ReLU activations and 50 neurons per each of two hidden layers. We set the learning
rate to be 0.1 and the momentum coefﬁcient to be 0.9. We turned off weight decay so that R is not
maximized as a result of bounding ∥∇fθ(xi)∥, which is a trivial case from the above discussion.
(B.2). A constant lower bound for neural networks. Similarly, we can obtain a constant lower bound
by adding some additional conditions.
Assumption B.1. Let us denote ˆΘn ⊆Θ as the set of minimizers of ˜Lmix
n (θ, S). We assume there
exists a set Θ∗, such that for all n ≥N, where N is a positive integer, ˆΘn ⊆Θ∗with probability at
least 1 −δn and δn →0 as n →0. Moreover, there exists τ, τ ′ ∈(0, 1) such that
XΘ∗(τ, τ ′) = {x ∈X : | cos(x, ∇fθ(x))| ⩾τ, ∥∇fθ(x)∥⩾τ ′, for all θ ∈Θ∗},
has probability pτ,τ ′ ∈(0, 1).
Theorem B.1. Deﬁne
FΘ := {fθ|fθ(xi) = ∇fθ(xi)⊤xi, ∇2fθ(xi) = 0 almost everywhere, θ ∈Θ}.
Under Assumption B.1, for any fθ(x) ∈FΘ, if there exists constants bx, cx > 0 such that cx
√
d ≤
∥xi∥2 ≤bx
√
d for all i ∈{1, . . . , n}. Then, with probability at least 1 −δn −2 exp(−np2
τ,τ ′/2),
there exist constants κ > 0, κ2 > κ1 > 0, if we further have θ ∈ˆΘn, then
˜Lmix
n (θ, S) ≥1
n
n
X
i=1
˜ladv(˜εmix
√
d, (xi, yi))
where ˜εmix = ˜RcxEλ∼˜
Dλ[1−λ] and ˜R = min{
r
pτ,τ′κτ ′2
(2−pτ,τ′)/4τ ′′2+pτ,τ′κτ ′2 ,
pτ,τ′κ1τ ′
pτ,τ′κ1τ ′+(2−pτ,τ′)κ2τ ′′ }τ.
20

Published as a conference paper at ICLR 2021
B.1
PROOF OF THEOREM B.1
Notice if we assume for
XΘ∗(τ, τ ′) = {x ∈X : | cos(x, ∇fθ(x))| ⩾τ, ∥∇fθ(x)∥⩾τ ′, for all θ ∈Θ∗},
there is τ, τ ′ ∈(0, 1) such that XΘ∗(τ, τ ′) ̸= ∅, and
pτ,τ ′ := P(x ∈XΘ∗(τ, τ ′)) ∈(0, 1).
Let us ﬁrst study
1
n
n
X
i=1
|g(fθ(xi)) −yi|∥∇fθ(xi)∥2| cos(∇fθ(xi), xi)|
By boundedness of θ, x and y ∈{0, 1}, we know there is κ1, κ2 > 0, such that
κ1 ⩽|g(fθ(xi)) −yi| ⩽κ2
If we denote ˆp = {number of x′
is such that xi ∈XΘ∗(τ, τ ′)}/n. Then, it is easy to see
1
n
P
xi∈X c
Θ∗(τ,τ ′) ||g(fθ(xi)) −yi|∥∇fθ(xi)∥2
1
n
P
xi∈XΘ∗(τ,τ ′) |g(fθ(xi)) −yi|∥∇fθ(xi)∥2
⩽(1 −ˆp)κ2τ ′′
ˆpκ1τ ′
For η2 satisfying
η(1 + (1 −ˆp)κ2τ ′′
ˆpκ1τ ′
) ⩽τ
we have
1
n
n
X
i=1
|g(fθ(xi)) −yi|∥∇fθ(xi)∥2 cos(∇fθ(xi), xi)| ⩾1
n
n
X
i=1
|g(fθ(xi)) −yi|∥∇fθ(xi)∥2η
Besides, if we consider
n
X
i=1
|g(fθ(xi))(1 −g(fθ(xi)))|∥∇fθ(xi)∥2
2(cos(∇fθ(xi), xi))2
Thus, we have
η2(1 + (1 −ˆp)/4τ ′′2
ˆpκτ ′2
) ⩽τ 2
With probability at least 1 −2 exp(−2nε2), for ε = pτ,τ ′/2, we have
η ⩽min{
s
pτ,τ ′κτ ′2
(2 −pτ,τ ′)/4τ ′′2 + pτ,τ ′κτ ′2 ,
pτ,τ ′κ1τ ′
pτ,τ ′κ1τ ′ + (2 −pτ,τ ′)κ2τ ′′ }τ
B.2
PROOFS OF THE CLAIM fθ(x) = ∇fθ(x)⊤x AND ∇2fθ(x) = 0 FOR NN WITH
RELU/MAX-POOLING
Consider the neural networks with ReLU and max-pooling:
fθ(x) = W [L]σ[L−1] z[L−1]
,
z[l](x, θ) = W [l]σ(l−1) 
z[l−1](x, θ)

, l = 1, 2, . . . , L −1,
with σ(0)  z[0](x, θ)

= x, where σ represents nonlinear function due to ReLU and/or max-pooling,
and W [l] ∈RNl×Nl−1 is a matrix of weight parameters connecting the (l −1)-th layer to the l-
th layer. For the nonlinear function σ due to ReLU and/or max-pooling, we can deﬁne ˙σ[l](x, θ)
21

Published as a conference paper at ICLR 2021
such that ˙σ[l](x, θ) is a diagonal matrix with each element being 0 or 1, and σ[l]  z[l](x, θ)

=
˙σ[l](x, θ)z[l](x, θ). Using this, we can rewrite the model as:
fθ(x) = W [L] ˙σ[L−1](x, θ)W [L−1] ˙σ[L−2](x, θ) · · · W [2] ˙σ[1](x, θ)W [1]x.
Since ∂˙σ[l](x,θ)
∂x
= 0 almost everywhere for all l, which will cancel all derivatives except for
d
dxW [1]x, we then have that
∂fθ(x)
∂x
= W [L] ˙σ[L−1](x, θ)W [L−1] ˙σ[L−2](x, θ) · · · W [2] ˙σ[1](x, θ)W [1].
(13)
Therefore,
∂fθ(x)
∂x
x = W [L] ˙σ[L−1](x, θ)W [L−1] ˙σ[L−2](x, θ) · · · W [2] ˙σ[1](x, θ)W [1]x = fθ(x).
This proves that fθ(x) = ∇fθ(x)⊤x for deep neural networks with ReLU/Max-pooling.
Moreover, from equation 13, we have that
∇2fθ(x) = ∇x(W [L] ˙σ[L−1](x, θ)W [L−1] ˙σ[L−2](x, θ) · · · W [2] ˙σ[1](x, θ)W [1]) = 0,
since ∂˙σ[l](x,θ)
∂x
= 0 almost everywhere for all l. This proves that ∇2fθ(x) = 0 for deep neural
networks with ReLU/Max-pooling.
C
MORE ABOUT EXPERIMENTS
C.1
ADVERSARIAL ATTACK AND MIXUP
We demonstrate the comparison between Mixup and standard training against adversarial attacks
created by FGSM. We train two WideResNet-16-8 (Zagoruyko & Komodakis, 2016) architectures
on the Street View House Numbers SVHN (Netzer et al., 2011)) dataset; one model with regular
empirical risk minimization and the other one with Mixup loss (α = 5, β = 0.5). We create FGSM
adversarial attacks (Goodfellow et al., 2014) for 1000 randomly selected test images. Fig. (1a)
describes the results for the two models. It can be observed that the model trained with Mixup loss
has better robustness.
C.2
VALIDITY OF THE APPROXIMATION OF ADVERSARIAL LOSS
In this subsection, we present numerical experiments to support the approximation in Eq. (5) and (6).
Under the same setup of our numerical experiments of Figure 2, we experimentally show that the
quadratic approximation of the adversarial loss is valid. Speciﬁcally, we train a Logistic Regression
model (as one example of a GLM model, which we study later) and a two layer neural network with
ReLU activations. We use the two-moons dataset (Buitinck et al., 2013). Fig. 4, and compare the
approximated adversarial loss and the original one along the iterations of computing the original
adversarial loss against ℓ2 attacks. The attack size is chosen such that ϵ
√
d = 0.5, and both models
had the same random initialization scheme. This experiment shows that using second order Taylor
expansion yields a good approximation of the original adversarial loss.
Logistc Regression
Two Layer ReLu Neural Network
Figure 4: Comparison of the original adversarial loss with the approximate adversarial loss function.
22

Published as a conference paper at ICLR 2021
C.3
GENERALIZATION AND MIXUP
Figures 5–8 show the results of experiments for generalization with various datasets that moti-
vated us to mathematically study Mixup. We followed the standard experimental setups without
any modiﬁcation as follows. We adopted the standard image datasets, CIFAR-10 (Krizhevsky &
Hinton, 2009), CIFAR-100 (Krizhevsky & Hinton, 2009), Fashion-MNIST (Xiao et al., 2017), and
Kuzushiji-MNIST (Clanuwat et al., 2019). For each dataset, we consider two cases: with and with-
out standard additional data augmentation for each dataset. We used the standard pre-activation
ResNet with 18 layers (He et al., 2016b). Stochastic gradient descent (SGD) was used to train the
models with mini-batch size = 64, the momentum coefﬁcient = 0.9, and the learning rate = 0.1. All
experiments were implemented in PyTorch (Paszke et al., 2019).
0
50
100
150
200
epoch
2 × 101
3 × 101
4 × 101
6 × 101
test error
mixup
ERM
0
50
100
150
200
epoch
100
6 × 10
1
2 × 100
test loss
0
50
100
150
200
epoch
0
20
40
60
80
train error
0
50
100
150
200
epoch
10
4
10
3
10
2
10
1
100
train loss
0
50
100
150
200
epoch
0.0
2.5
5.0
7.5
10.0
12.5
15.0
generaliztion gap (error)
0
50
100
150
200
epoch
100
3 × 10
1
4 × 10
1
6 × 10
1
generaliztion gap (loss)
(a) without extra data augmentation
0
100
200
300
400
epoch
101
102
test error
mixup
ERM
0
100
200
300
400
epoch
100
test loss
0
100
200
300
400
epoch
0
20
40
60
80
train error
0
100
200
300
400
epoch
10
2
10
1
100
train loss
0
100
200
300
400
epoch
2
4
6
generaliztion gap (error)
0
100
200
300
400
epoch
10
1
generaliztion gap (loss)
(b) With extra data augmentation
Figure 5: Generalization: CIFAR-10
0
50
100
150
200
epoch
102
5 × 101
6 × 101
7 × 101
8 × 101
9 × 101
test error
mixup
ERM
0
50
100
150
200
epoch
101
102
103
test loss
0
50
100
150
200
epoch
0
20
40
60
80
100
train error
0
50
100
150
200
epoch
10
1
100
101
train loss
0
50
100
150
200
epoch
0
10
20
30
40
50
generaliztion gap (error)
0
50
100
150
200
epoch
100
101
102
103
generaliztion gap (loss)
(a) Without extra data augmentation
0
100
200
300
400
epoch
102
3 × 101
4 × 101
6 × 101
test error
mixup
ERM
0
100
200
300
400
epoch
100
101
test loss
0
100
200
300
400
epoch
0
20
40
60
80
100
train error
0
100
200
300
400
epoch
10
1
100
train loss
0
100
200
300
400
epoch
0
5
10
15
20
25
generaliztion gap (error)
0
100
200
300
400
epoch
100
101
generaliztion gap (loss)
(b) With extra data augmentation
Figure 6: Generalization: CIFAR-100
23

Published as a conference paper at ICLR 2021
0
100
200
300
400
epoch
101
102
test error
mixup
ERM
0
100
200
300
400
epoch
100
test loss
0
100
200
300
400
epoch
0
20
40
60
80
train error
0
100
200
300
400
epoch
10
5
10
3
10
1
train loss
0
100
200
300
400
epoch
0
2
4
6
generaliztion gap (error)
0
100
200
300
400
epoch
2 × 10
1
3 × 10
1
4 × 10
1
6 × 10
1
generaliztion gap (loss)
(a) Without extra data augmentation
0
100
200
300
400
epoch
101
102
test error
mixup
ERM
0
100
200
300
400
epoch
100
test loss
0
100
200
300
400
epoch
0
20
40
60
80
train error
0
100
200
300
400
epoch
10
2
10
1
100
train loss
0
100
200
300
400
epoch
0
1
2
3
4
5
generaliztion gap (error)
0
100
200
300
400
epoch
10
1
generaliztion gap (loss)
(b) With extra data augmentation
Figure 7: Generalization: Fashion-MNIST
0
50
100
150
200
epoch
100
101
102
test error
mixup
ERM
0
50
100
150
200
epoch
10
1
100
test loss
0
50
100
150
200
epoch
0
20
40
60
80
train error
0
50
100
150
200
epoch
10
5
10
3
10
1
train loss
0
50
100
150
200
epoch
0.0
0.5
1.0
1.5
2.0
generaliztion gap (error)
0
50
100
150
200
epoch
10
3
10
2
10
1
generaliztion gap (loss)
(a) Without extra data augmentation
0
100
200
300
400
epoch
100
101
102
test error
mixup
ERM
0
100
200
300
400
epoch
10
1
100
test loss
0
100
200
300
400
epoch
0
20
40
60
80
train error
0
100
200
300
400
epoch
10
5
10
3
10
1
train loss
0
100
200
300
400
epoch
0.5
1.0
1.5
2.0
generaliztion gap (error)
0
100
200
300
400
epoch
10
3
10
2
10
1
generaliztion gap (loss)
(b) With extra data augmentation
Figure 8: Generalization: Kuzushiji-MNIST
24

