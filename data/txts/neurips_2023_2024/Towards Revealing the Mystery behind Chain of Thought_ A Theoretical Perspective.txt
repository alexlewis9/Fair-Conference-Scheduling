Towards Revealing the Mystery behind
Chain of Thought: A Theoretical Perspective
Guhao Feng1,5,∗
Bohang Zhang2,∗
Yuntian Gu3,∗
Haotian Ye3,∗
Di He2,B
Liwei Wang2,4,B
1School of EECS, Peking University
2National Key Laboratory of General Artificial Intelligence,
School of Intelligence Science and Technology, Peking University
3Yuanpei College,
Peking University
4Center for Machine Learning Research, Peking University
5Pazhou Lab
fenguhao@stu.pku.edu.cn
zhangbohang@pku.edu.cn
guyuntian@stu.pku.edu.cn
haotianye@pku.edu.cn
dihe@pku.edu.cn
wanglw@pku.edu.cn
Abstract
Recent studies have discovered that Chain-of-Thought prompting (CoT) can dra-
matically improve the performance of Large Language Models (LLMs), particularly
when dealing with complex tasks involving mathematics or reasoning. Despite
the enormous empirical success, the underlying mechanisms behind CoT and how
it unlocks the potential of LLMs remain elusive. In this paper, we take a first
step towards theoretically answering these questions. Specifically, we examine the
expressivity of LLMs with CoT in solving fundamental mathematical and decision-
making problems. By using circuit complexity theory, we first give impossibility
results showing that bounded-depth Transformers are unable to directly produce
correct answers for basic arithmetic/equation tasks unless the model size grows
super-polynomially with respect to the input length. In contrast, we then prove by
construction that autoregressive Transformers of constant size suffice to solve both
tasks by generating CoT derivations using a commonly used math language format.
Moreover, we show LLMs with CoT can handle a general class of decision-making
problems known as Dynamic Programming, thus justifying their power in tackling
complex real-world tasks. Finally, an extensive set of experiments show that, while
Transformers always fail to directly predict the answers, they can consistently learn
to generate correct solutions step-by-step given sufficient CoT demonstrations.
1
Introduction
Transformer-based Large Language Models (LLMs) have emerged as a foundation model in natural
language processing. Among them, the autoregressive paradigm has gained arguably the most
popularity [51, 9, 46, 69, 57, 16, 52, 54], based on the philosophy that all different tasks can be
uniformly treated as sequence generation problems. Specifically, given any task, the input along with
the task description can be together encoded as a sequence of tokens (called the prompt); the answer is
then generated by predicting subsequent tokens conditioned on the prompt in an autoregressive way.
Previous studies highlighted that a carefully designed prompt greatly matters LLMs’ performance
[32, 38]. In particular, the so-called Chain-of-Thought prompting (CoT) [61] has been found crucial
for tasks involving arithmetic or reasoning, where the correctness of generated answers can be dramat-
ically improved via a modified prompt that triggers LLMs to output intermediate derivations. Practi-
cally, this can be achieved by either adding special phrases such as “let’s think step by step” or by giv-
ing few-shot CoT demonstrations [34, 61, 56, 44, 70, 63]. However, despite the striking performance,
the underlying mechanism behind CoT remains largely unclear and mysterious. On one hand, are there
∗Equal contributions.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

indeed inherent limitations of LLMs in directly answering math/reasoning questions? On the other
hand, what is the essential reason behind the success of CoT2 in boosting the performance of LLMs?
This paper takes a step towards theoretically answering the above questions. We begin with studying
the capability of LLMs on two basic mathematical tasks: evaluating arithmetic expressions and solving
linear equations. Both tasks are extensively employed and serve as elementary building blocks in
solving complex real-world math problems [10]. We first provide fundamental impossibility results,
showing that none of these tasks can be solved using bounded-depth Transformer models without
CoT unless the model size grows super-polynomially with respect to the input length (Theorems 3.1
and 3.2). Remarkably, our proofs provide insights into why this happens: the reason is not due to the
(serialized) computational cost of these problems but rather to their parallel complexity [2]. We next
show that the community may largely undervalue the strength of autoregressive generation: we prove
by construction that autoregressive Transformers of constant size can already perfectly solve both
tasks by generating intermediate derivations in a step-by-step manner using a commonly-used math
language format (Theorems 3.3 and 3.4). Intuitively, this result hinges on the recursive nature of CoT,
which increases the “effective depth” of the Transformer to be proportional to the generation steps.
Besides mathematics, CoT also exhibits remarkable performance across a wide range of reasoning
tasks. To gain a systematic understanding of why CoT is beneficial, we next turn to a fundamental
class of problems known as Dynamic Programming (DP) [6]. DP represents a golden framework
for solving sequential decision-making tasks: it decomposes a complex problem into a sequence (or
chain) of subproblems, and by following the reasoning chain step by step, each subproblem can be
solved based on the results of previous subproblems. Our main finding demonstrates that, for general
DP problems of the form (5), LLMs with CoT can generate the complete chain and output the correct
answer (Theorem 4.7). However, it is impossible to directly generate the answer in general: as a
counterexample, we prove that bounded-depth Transformers of polynomial size cannot solve a classic
DP problem known as Context-Free Grammar Membership Testing (Theorem 4.8).
Our theoretical findings are complemented by an extensive set of experiments. We consider the two
aforementioned math tasks plus two celebrated DP problems listed in the “Introduction to Algorithms”
book [17], known as longest increasing subsequence (LIS) and edit distance (ED). For all these tasks,
our experimental results show that directly predicting the answers without CoT always fails (accuracy
mostly below 60%). In contrast, autoregressive Transformers equipped with CoT can learn entire
solutions given sufficient training demonstrations. Moreover, they even generalize well to longer
input sequences, suggesting that the models have learned the underlying reasoning process rather
than statistically memorizing input-output distributions. These results verify our theory and reveal
the strength of autoregressive LLMs and the importance of CoT in practical scenarios.
2
Preliminary
An (autoregressive) Transformer [58, 50] is a neural network architecture designed to process a
sequence of input tokens and generate tokens for subsequent positions. Given an input sequence s
of length n, a Transformer operates the sequence as follows. First, each input token si (i ∈[n]) is
converted to a d-dimensional vector vi = Embed(si) ∈Rd using an embedding layer. To identify
the sequence order, there is also a positional embedding pi ∈Rd applied to token si. The embedded
input can be compactly written into a matrix X(0) = [v1 + p1, · · · , vn + pn]⊤∈Rn×d. Then, L
Transformer blocks follow, each of which transforms the input based on the formula below:
X(l) = X(l−1) + Attn(l)(X(l−1)) + FFN(l) 
X(l−1) + Attn(l)(X(l−1))

,
l ∈[L],
(1)
where Attn(l) and FFN(l) denote the multi-head self-attention layer and the feed-forward network
for the l-th Transformer block, respectively:
Attn(l)(X) =
H
X
h=1
softmax

XW (l,h)
Q
(XW (l,h)
K
)⊤+ M

XW (l,h)
V
W (l,h)
O
,
(2)
FFN(l)(X) = σ(XW (l)
1 )W (l)
2 .
(3)
2Throughout this paper, we use the term CoT to refer to the general framework of the step-by-step generation
process rather than a specific prompting technique. In other words, this paper studies why an LLM equipped
with CoT can succeed in math/reasoning tasks rather than which prompt can trigger this process.
2

Here, we focus on the standard setting adopted in Vaswani et al. [58], namely, an H-head softmax
attention followed by a two-layer pointwise FFN, both with residual connections. The size of the
Transformer is determined by three key quantities: its depth L, width d, and the number of heads
H. The parameters W (l,h)
Q
, W (l,h)
K
, W (l,h)
V
, W (l,h)
O
are query, key, value, output matrices of the
h-th head, respectively; and W (l)
1 , W (l)
2
are two weight matrices in the FFN. The activation σ is
chosen as GeLU [28], following [51, 21]. The matrix M ∈{−∞, 0}n×n is a causal mask defined as
Mij = −∞iff i < j. This ensures that each position i can only attend to preceding positions j ≤i
and is the core design for autoregressive generation.
After obtaining X(L) ∈Rn×d, its last entry X(L)
n,: ∈Rd will be used to predict the next token sn+1
(e.g., via a softmax classifier). By concatenating sn+1 to the end of the input sequence s, the above
process can be repeated to generate the subsequent token sn+2. The process continues iteratively
until a designated End-of-Sentence token is generated, signifying the completion of the process.
Chain-of-Thought prompting. Autoregressive Transformers possess the ability to tackle a wide
range of tasks by encoding the task description into a partial sentence, with the answer being derived
by complementing the subsequent sentence [9]. However, for some challenging tasks involving math
or general reasoning, a direct generation often struggles to yield a correct answer. To address this
shortcoming, researchers proposed the CoT prompting that induces LLMs to generate intermediate
reasoning steps before reaching the answer [61, 34, 56, 44, 70, 11]. In this paper, our primary focus
lies in understanding the mechanism behind CoT, while disregarding the aspect of how prompting
facilitates its triggering. Specifically, we examine CoT from an expressivity perspective: for both
mathematical problems and general decision-making tasks studied in Sections 3 and 4, we will
investigate whether autoregressive Transformers are expressive for (i) directly generating the answer,
and (ii) generating a CoT solution for the tasks.
3
CoT is the Key to Solving Mathematical Problems
Previous studies have observed that Transformer-based LLMs exhibit surprising math abilities in
various aspects [46, 10]. In this section, we begin to explore this intriguing phenomenon via two
well-chosen tasks: arithmetic and equation. We will give concrete evidence that LLMs are capable of
solving both tasks when equipped with CoT, while LLMs without CoT are provably incapable.
3.1
Problem formulation
Input: 
(7 + 5) ÷ (6 + 4 × 3 −2 × 7) =
Output:
12 ÷ (6 + 4 × 3 −2 × 7)
= 12 ÷ (6 + 12 −2 × 7)
= 12 ÷ (18 −2 × 7)
= 12 ÷ (18 −14)
= 12 ÷ 4
= 3
Input:
3𝑥𝑥+ 3𝑦𝑦+ 12𝑧𝑧= 6;
2𝑥𝑥+ 5𝑦𝑦+ 14𝑧𝑧= 7;
2𝑥𝑥+ 4𝑦𝑦+ 15𝑧𝑧= 6;
Output:
𝑥𝑥+ 1𝑦𝑦+ 4𝑧𝑧= 2;
3𝑦𝑦+ 6𝑧𝑧= 3;
2𝑦𝑦+ 7𝑧𝑧= 2;
𝑥𝑥+ 2𝑧𝑧= 1;
𝑦𝑦+ 2𝑧𝑧= 1;
3𝑧𝑧= 0;
𝑥𝑥= 1;
𝑦𝑦= 1;
𝑧𝑧= 0;
Arithmetic Expression
Linear Equations 
⟹
⟹
⟹
Figure 1: Illustrations of CoT on two math tasks.
Arithmetic. The first task focuses on evaluat-
ing arithmetic expressions. As shown in Fig-
ure 1 (left), the input of this task is a sequence
consisting of numbers, addition (+), subtrac-
tion (−), multiplication (×), division (÷), and
brackets, followed by an equal sign. The goal
is to calculate the arithmetic expression and
generate the correct result. This task has a nat-
ural CoT solution, where each step performs
an intermediate computation, gradually reduc-
ing one atomic operation at a time while copy-
ing down other unrelated items. Figure 1 (left)
gives an illustration, and the formal definition
of the CoT format is deferred to Appendix B.
Equation. The second task considers solving linear equations. As shown in Figure 1 (right), the
input of this task is a sequence consisting of m linear equations, each of which involves m variables.
The input ends with a special symbol =⇒. The goal is to output the value of these variables that
satisfies the set of equations (assuming the answer exists and is unique). A natural CoT solution is the
Gaussian elimination algorithm: at each step, it eliminates a certain variable in all but one equations.
After m −1 steps, all equations will have only one variable and the problem is solved. Figure 1
(right) gives an illustration, and we defer the formal definition of the CoT format to Appendix B.
Number field. Ideally, for both tasks, the input sequences involve not only symbol tokens but
also floating-point numbers. This complicates the definitions of the model’s input/output format
3

and further entails intricate precision considerations when dealing with floating-point divisions. To
simplify our subsequent analysis, here we turn to a more convenient setting by transitioning to
the finite field generated by integers modulo p for a prime number p. Importantly, the finite field
contains only p numbers (ranging from 0 to p −1) and thus can be uniformly treated as tokens in a
pre-defined dictionary (like other operators or brackets), making the problem setting much cleaner.
Moreover, arithmetic operations (+, −, ×, ÷) are well-defined and parallel the real number field (see
Appendix A.1 for details). Therefore, this setting does not lose generalities.
In subsequent sections, we denote by Arithmetic(n, p) the arithmetic evaluation task defined on the fi-
nite field modulo p, where the input length does not exceed n. Similarly, we denote by Equation(m, p)
the linear equation task defined on the finite field modulo p with no more than m variables.
3.2
Theoretical results
We begin by investigating whether Transformers can directly produce answers to the aforemen-
tioned problems. This corresponds to generating, for instance, the number “3” or the solution
“x = 1; y = 1; z = 0” in Figure 1 immediately after the input sequence (without outputting intermedi-
ate steps). This question can be examined via different theoretical perspectives. One natural approach
is to employ the classic representation theory, which states that multi-layer perceptrons with suffi-
cient size (e.g., the depth or width approaches infinity) are already universal function approximators
[18, 35, 40]. Recently, such results have been well extended to Transformer models [67]: it is not hard
to show that a constant-depth Transformer with sufficient size can solve the above tasks3. However,
the above results become elusive when taking the representation efficiency into account, since it says
nothing about the required model size for any specific task. Below, we would like to give a more
fine-grained analysis of how large the network needs to be by leveraging the tool of complexity theory.
We focus on a realistic setting called the log-precision Transformer [42, 37]: it refers to a Trans-
former whose internal neurons can only store floating-point numbers within a finite O(log n) bit
precision where n is the maximal length of the input sequence (see Appendix A.3 for a formal
definition). Such an assumption well-resembles practical situations, in which the machine precision
(e.g., 16 or 32 bits) is typically much smaller than the input length (e.g., 2048 in GPT), avoiding the
unrealistic (but crucial) assumption of infinite precision made in several prior works [49, 20]. Further-
more, log-precision implies that the number of values each neuron can take is polynomial in the input
length, which is a necessary condition for representing important quantities like positional embedding.
Equipped with the concept of log-precision, we are ready to present a central impossibility result,
showing that the required network size must be prohibitively large for both math problems:
Theorem 3.1. Assume TC0 ̸= NC1. For any prime number p, any integer L, and any polynomial
Q, there exists a problem size n such that no log-precision autoregressive Transformer defined in
Section 2 with depth L and hidden dimension d ≤Q(n) can solve the problem Arithmetic(n, p).
Theorem 3.2. Assume TC0 ̸= NC1. For any prime number p, any integer L, and any polynomial
Q, there exists a problem size m such that no log-precision autoregressive Transformer defined in
Section 2 with depth L and hidden dimension d ≤Q(m) can solve the problem Equation(m, p).
Why does this happen? As presented in Appendices D.2 and E.2, the crux of our proof lies in apply-
ing circuit complexity theory [2]. By framing the finite-precision Transformer as a computation model,
one can precisely delineate its expressivity limitation through an analysis of its circuit complexity.
Here, bounded-depth log-precision Transformers of polynomial size represent a class of shallow cir-
cuits with complexity upper bounded by TC0 [42]. On the other hand, we prove that the complexity of
both math problems above are lower bounded by NC1 by applying reduction from NC1-complete prob-
lems. Consequently, they are intrinsically hard to be solved by a well-parallelized Transformer unless
the two complexity classes collapse (i.e., TC0 = NC1), a scenario widely regarded as impossible [65].
How about generating a CoT solution? We next turn to the setting of generating CoT solutions
for these problems. From an expressivity perspective, one might intuitively perceive this problem as
more challenging as the model is required to express the entire problem solving process, potentially
necessitating a larger model size. However, we show this is not the case: a constant-size autoregressive
Transformer already suffices to generate solutions for both math problems.
3For example, given an input sequence of length n, an n-head self-attention layer with hidden dimension
O(n) can extract the information of the entire sequence into the representation of the last position. Then, an MLP
operating on the last position can universally approximate any (continuous) function over the input sequence.
4

Theorem 3.3. Fix any prime p. For any integer n > 0, there exists an autoregressive Transformer
defined in Section 2 with constant hidden size d (independent of n), depth L = 5, and 5 heads in each
layer that can generate the CoT solution defined in Appendix B for all inputs in Arithmetic(n, p).
Moreover, all parameter values in the Transformer are bounded by O(poly(n)).
Theorem 3.4. Fix any prime p. For any integer m > 0, there exists an autoregressive Transformer
defined in Section 2 with constant hidden size d (independent of m), depth L = 4, and 5 heads in
each layer that can generate the CoT solution defined in Appendix B for all inputs in Equation(m, p).
Moreover, all parameter values in the Transformer are bounded by O(poly(m)).
Remark 3.5. The polynomial upper bound for parameters in Theorems 3.3 and 3.4 readily implies
that these Transformers can be implemented using log-precision without loss of accuracy. See
Appendix A.3 for a detailed discussion on how this can be achieved.
Proof sketch. The proofs of Theorems 3.3 and 3.4 are based on construction. We begin by building
a set of fundamental operations in Appendix C that can be implemented by Transformer layers.
Specifically, the softmax attention head can perform two types of operations called the (conditional)
COPY and MEAN (Lemmas C.7 and C.8). Here, conditional COPY extracts the content of the unique
previous position that satisfies certain conditions, while Conditional MEAN averages the values of a
set of previous positions that satisfy certain conditions. These two operations can be seen as a form
of “gather/scatter” operator in parallel computing. On the other hand, the FFN in a Transformer
layer can perform basic computations within each position, such as multiplication (Lemma C.1),
conditional selection (Lemma C.4), and lookup tables (Lemma C.5). With these basic operations as
“instructions” and by treating autoregressive generation as a loop, it is possible to write “programs”
that can solve fairly complex tasks. As detailed in Appendices D.1 and E.1, we construct parallel
algorithms that can generate CoT sequences for both math problems, thus concluding the proof.
Several discussions are made as follows. Firstly, the constructions in our proof reveal the significance
of several key components in the Transformer design, such as softmax attention, multi-head, feed-
forward networks, and residual connection. Our proofs offer deep insights into the inner workings of
Transformer models when dealing with complex tasks, significantly advancing prior understandings
such as the “induction head” mechanism [45]. Moreover, our results identify an inherent advantage
of Transformers compared to other sequence models like RNNs: indeed, as shown in Appendix F.2,
constant-size RNNs cannot solve any of the above math tasks using the same CoT format. Secondly,
we highlight that in our setting, the CoT derivations of both math problems are purely written in a
readable math language format, largely resembling how humans write solutions. In a broad sense,
our findings justify that LLMs have the potential to convey meaningful human thoughts through
grammatically precise sentences. Finally, one may ask how LLMs equipped with CoT can bypass
the impossibility results outlined in Theorems 3.1 and 3.2. Actually, this can be understood via the
effective depth of the Transformer circuit. By employing CoT, the effective depth is no longer L since
the generated outputs are repeatedly looped back to the input. The dependency between output tokens
leads to a significantly deeper circuit with depth proportional to the length of the CoT solution. Note
that even if the recursive procedure is repeated within a fixed Transformer (or circuit), the expressivity
can still be far beyond TC0: as will be shown in Section 4, with a sufficient number of CoT steps,
autoregressive Transformers can even solve P-complete problems.
4
CoT is the Key to Solving General Decision-Making Problems
The previous section has delineated the critical role of CoT in solving math problems. In this section,
we will switch our attention to a more general setting beyond mathematics. Remarkably, we find that
LLMs with CoT are theoretically capable of emulating a powerful decision-making framework called
Dynamic Programming [6], thus strongly justifying the ability of CoT in solving complex tasks.
4.1
Dynamic Programming
Dynamic programming (DP) is widely regarded as a core technique to solve decision-making
problems [55]. The basic idea of DP lies in breaking down a complex problem into a series of small
subproblems that can be tackled in a sequential manner. Here, the decomposition ensures that there is
a significant interconnection (overlap) among various subproblems, so that each subproblem can be
efficiently solved by utilizing the answers (or other relevant information) obtained from previous ones.
5

Formally, a general DP algorithm can be characterized via three key ingredients: state space I,
transition function T, and aggregation function A. Given a DP problem with N input sequences
s(1), · · · , s(N), denote the problem size to be the vector n = (|s(1)|, · · · , |s(N)|). Fixing the
problem size n, there is an associated state space In ⊂I representing the finite set of decomposed
subproblems, where each state i ∈In is an index signifying a specific subproblem. The size of the
state space In grows with the problem size n. We denote by dp(i) the answer of subproblem i (as
well as other information stored in the DP process). Furthermore, there is a partial order relation
between different states: we say state j precedes state i (denoted as j ≺i) if subproblem j should be
solved before subproblem i, i.e., the value of dp(i) depends on dp(j). This partial order creates a
directed acyclic graph (DAG) within the state space, thereby establishing a reasoning chain where
subproblems are resolved in accordance with the topological ordering of the DAG.
The transition function T characterizes the interconnection among subproblems and defines how a
subproblem can be solved based on the results of previous subproblems. It can be generally written as
dp(i) = T (n, s, i, {(j, dp(j)) : j ≺i}) ,
(4)
where s is the concatenation of all input sequences s(1), · · · , s(N). In this paper, we focus on
a restricted setting where each state i only depends on (i) a finite number of tokens in the input
sequence s and (ii) a finite number of previous states. Under this assumption, we can rewrite (4)
into a more concrete form:
dp(i) = f
 n, i, sg(n,i), dp(h(n, i))

= f
 n, i, sg1(n,i), · · · , sgJ(n,i), dp(h1(n, i)), · · · , dp(hK(n, i))

,
(5)
where functions f, g, h fully determine the transition function T and have the following form
f : NN × I × X J × YK →Y, g : NN × I →(N ∪{∅})J, h : NN × I →(I ∪{∅})K. Here, the
state space I, input space X, and DP output space Y can be arbitrary domains, and J, K are constant
integers. If state i depends on less than J input tokens or less than K previous states, we use the
special symbol ∅to denote a placeholder, such that all terms s∅and dp(∅) are unused in function f.
After solving all subproblems, the aggregation function A is used to combine all results and obtain
the final answer. We consider a general class of aggregation functions with the following form:
A ({(i, dp(i)) : i ∈In}) = u (□i∈Andp(i)) ,
(6)
where An ⊂In is a set of states that need to be aggregated, □is an aggregation function such as
min, max, or P, and u : Y →Z is any function where Z denotes the space of possible answers.
A variety of popular DP problems fits the above framework. As examples, the longest increasing
subsequence (LIS) and edit distance (ED) are two well-known DP problems presented in the “Introduc-
tion to Algorithms” book [17] (see Appendix G.1 for problem descriptions and DP solutions). We list
the state space, transition function, and aggregation function of the two problems in the table below.
Problem
Longest increasing subsequence
Edit distance
Input
A string s of length n
Two strings s(1), s(2) of length n1 = |s(1)|
and n2 = |s(2)|, concatenated together
State space
{(j, k) : j ∈[n], k ∈{0, · · · , j−1}}
{0, · · · , n1} × {0, · · · , n2}
Transition
function
dp(j, k) =







1
if k=0
max(dp(j, k−1),
dp(k, k−1)×
I[sj >sk]+1)
if k>0
dp(j, k) =















ak
if j =0
bj
if k=0
min(dp(j, k−1) + a,
dp(j−1, k) + b,
dp(j−1, k−1)
+ cI[s(1)
j
̸=s(2)
k ])
otherwise
Aggregation
function
maxi∈[n] dp(i, i −1)
dp(n1, n2)
4.2
Theoretical results
We begin by investigating whether LLMs with CoT can solve the general DP problems defined above.
We consider a natural CoT generation process, where the generated sequence has the following form:
s(1)
|
· · ·
|
s(N)
|
(i1, dp(i1))
. . .
(i|In|, dp(i|In|))
final answer
6

Here, the input sequence consists of N strings separated by special symbols, and (i1, · · · , i|In|) is
a feasible topological ordering of the state space In. We assume that all domains I, X, Y, Z belong
to the real vector space so that their elements can be effectively represented and handled by a neural
network. Each (i, dp(i)) ∈I × Y above will be represented as a single vector and generated jointly
in the CoT output. We further assume that I, X, Y, Z are discrete spaces (e.g., integers) so that the
elements can be precisely represented using finite precision. To simplify our analysis, we consider
a regression setting where each element in the CoT output directly corresponds to the output of the
last Transformer layer (without using a softmax layer for tokenization as in Section 3). Instead, the
Transformer output is simply projected to the nearest element in the corresponding discrete space
(e.g., I × Y or Z). Likewise, each generated output is directly looped back to the Transformer input
without using an embedding layer. This regression setting is convenient for manipulating numerical
values and has been extensively adopted in prior works [24, 1].
Before presenting our main result, we make the following assumptions:
Definition 4.1 (Polynomially-efficient approximation). Given neural network Pθ and target function
f : X in →X out where X in ⊂Rdin and X out ⊂Rdout, we say f can be approximated by Pθ with
polynomial efficiency if there exist ρ > 0, λ > 0 such that for any error ϵ > 0 and radius R > 0,
there exists parameter θ satisfying that (i) ∥f(x) −Pθ(x + δ)∥∞< ϵ + λ∥δ∥∞for all x ∈X in,
∥x∥∞≤R and all ∥δ∥∞< ρ; (ii) all elements of parameter θ are bounded by O(poly(R, 1/ϵ)).
Assumption 4.2. The size of the state space can be polynomially upper bounded by the problem size
n, i.e., |In| = O(poly(|s|)). Similarly, all input elements, DP values, and answers are polynomially
upper bounded by the problem size n.
Assumption 4.3. Each function f, g, h and u in (5) and (6) can be approximated with polynomial
efficiency by a perceptron of constant size (with GeLU activation).
Assumption 4.4. The function F : NN × I →I defined as F(n, ik) = ik+1 for n ∈NN,
k ∈[|In| −1] can be approximated with polynomial efficiency by a perceptron of constant size (with
GeLU activation), where (i1, · · · , i|In|) is a feasible topological ordering of the state space In.
Assumption 4.5. The function F : NN × I →{0, 1} defined as F(n, i) = I[i ∈An] (see (6)) can
be approximated with polynomial efficiency by a perceptron of constant size (with GeLU activation).
Remark 4.6. All assumptions above are mild. Assumption 4.2 is necessary to ensure that the
state vectors, inputs, and DP values can be represented using log-precision, and Assumptions 4.3
to 4.5 guarantee that all basic functions that determine the DP process can be well-approximated
by a composition of finite log-precision Transformer layers of constant size. In Appendix G.1, we
show these assumptions are satisfied for LIS and ED problems described above as well as the CFG
Membership Testing problem in Theorem 4.8.
We are now ready to present our main result, which shows that LLMs with CoT can solve all DP
problems satisfying the above assumptions. We give a proof in Appendix G.2.
Theorem 4.7. Consider any DP problem satisfying Assumptions 4.2 to 4.5. For any integer n ∈N,
there exists an autoregressive Transformer with constant depth L, hidden dimension d and attention
heads H (independent of n), such that the answer generated by the Transformer is correct for all input
sequences of length no more than n. Moreover, all parameter values are bounded by O(poly(n)).
To complete the analysis, we next explore whether Transformers can directly predict the answer
of DP problems without generating intermediate CoT sequences. We show generally the answer
is no: many DP problems are intrinsically hard to be solved by a bounded-depth Transformer without
CoT. One celebrated example is the Context-Free Grammar (CFG) Membership Testing, which take
a CFG G and a string v as input and tests whether v belongs to the context-free language generated
by G. A formal definition of this problem and a standard DP solution are given in Appendix G.1.
We have the following impossibility result (see Appendix G.3 for a proof):
Theorem 4.8. Assume TC0 ̸= NC1. There exists a context-free language such that for any depth L
and any polynomial Q, there exists a sequence length n ∈N where no log-precision autoregressive
transformer with depth L and hidden dimension d ≤Q(n) can generate the correct answer for the
CFG Membership Testing problem for all input strings of length n.
In contrast, enabling CoT substantially improves the expressivity of Transformers: as proved in
Jones & Laaser [33], the universal CFG Membership Testing is a celebrated P-complete problem and
is intrinsically hard to be solved by a well-parallelized computation model. Combined with these
results, we conclude that CoT plays a critical role in tackling tasks that are inherently difficult.
7

5
Experiments
In previous sections, we proved by construction that LLMs exhibit sufficient expressive power to
solve mathematical and decision-making tasks. On the other hand, it is still essential to check whether
a Transformer model can learn such ability directly from training data. Below, we will complement
our theoretical results with experimental evidence, showing that the model can easily learn underlying
task solutions when equipped with CoT training demonstrations.
5.1
Experimental Design
Tasks and datasets. We choose four tasks for evaluation: Arithmetic, Equation, LIS, and ED. The
first two tasks (Arithmetic and Equation) as well as their input/CoT formats have been illustrated in
Figure 1. For the LIS task, the goal is to find the length of the longest increasing subsequence of a
given integer sequence. For the ED task, the goal is to calculate the minimum cost required (called
edit distance) to convert one sequence to another using three basic edit operations: insert, delete and
replace. All input sequences, CoT demonstrations, and answers in LIS and ED are bounded-range
integers and can therefore be tokenized (similar to the first two tasks). We consider two settings:
(i) CoT datasets, which consist of <problem, CoT steps, answer> samples; (ii) Direct datasets, which
are used to train models that directly predict the answer without CoT steps. These datasets are
constructed by removing all intermediate derivations from the CoT datasets.
For each task, we construct three datasets with increasing difficulty. For Arithmetic, we build datasets
with different numbers of operators ranging from {4, 5, 6}. For Equation, we build datasets with
different numbers of variables ranging from {3, 4, 5}. For LIS, we build datasets with different input
sequence lengths ranging from {50, 80, 100}. For ED, we build datasets with different string lengths,
where the average length of the two strings ranges from {12, 16, 20}. Creating these datasets allows us
to investigate how model performance varies with the increase of problem size. We generate 1M sam-
ples for each training dataset and 0.1M for testing while ensuring that duplicate samples between train-
ing and testing are removed. More details about the dataset construction can be found in Appendix H.
Model training and inference. For all experiments, we use standard Transformer models with hidden
dimension d = 256, heads H = 4, and different model depths L. We adopt the AdamW optimizer
[39] with β1 = 0.9, β2 = 0.999, lr = 10−4, and weight decay = 0.01 in all experiments. We use
a fixed dropout ratio of 0.1 for all experiments to improve generalization. For CoT datasets, we
optimize the negative log-likelihood loss on all tokens in the CoT steps and answers. This is similar to
the so-called process supervision proposed in a concurrent work [36]. For direct datasets, we optimize
the negative log-likelihood loss on answer tokens. All models are trained on 4 V100 GPUs for 100
epochs. During inference, models trained on the direct datasets are required to output the answer
directly, and models trained on CoT datasets will generate the whole CoT process token-by-token
(using greedy search) until generating the End-of-Sentence token, where the output in the final
step is regarded as the answer. We report the accuracy as the evaluation metric. Please refer to
Appendix H for more training configuration details.
5.2
Experimental Results
Main results. All results are shown in Figure 2, where each subfigure corresponds to a task with
x-axis representing the difficulty level and y-axis representing the test accuracy (%). We repeat each
experiment five times and report the error bars. In each subfigure, the purple bar and blue bars indicate
the performance of the model trained on the CoT and direct datasets, respectively. The model depths
are specified in the legend. From these results, one can easily see that 3-layer Transformers with
CoT already achieve near-perfect performance on all tasks for all difficulty levels, which is consistent
to Theorems 3.3, 3.4 and 4.7 and may further imply that they can learn better solutions with fewer
model depth than our constructions. Moreover, it is worth noting that, for some difficult datasets such
as Equation of 5 variables, a perfect accuracy would imply that the model can precisely generate
a correct CoT with a very long length of roughly 500 for all inputs. In contrast, models trained on
direct datasets perform much worse even when using larger depths (particularly on the Equation task).
While increasing the depth usually helps the performance of direct prediction (which is consistent
with our theory), the performance drops significantly when the length of the input sequence grows.
All these empirical findings verify our theoretical results and clearly demonstrate the benefit of CoT
in autoregressive generation.
8

4
5
6
Number of Operators
20
30
40
50
60
70
80
90
100
Accuracy (%)
Arithmetic Expression
Direct (L=3)
Direct (L=4)
Direct (L=5)
CoT    (L=3)
3
4
5
Number of Variables
0
20
40
60
80
100
Accuracy (%)
Linear Equation
Direct (L=3)
Direct (L=4)
Direct (L=5)
CoT    (L=3)
50
80
100
Input Sequence Length
20
30
40
50
60
70
80
90
100
Accuracy (%)
Longest Increasing Subsequence
Direct (L=3)
Direct (L=4)
Direct (L=5)
CoT    (L=3)
12
16
20
Length of the First String
30
40
50
60
70
80
90
100
Accuracy (%)
Edit Distance
Direct (L=3)
Direct (L=4)
Direct (L=5)
CoT    (L=3)
Figure 2: Model performance on different tasks. For all tasks and various difficulty levels, autore-
gressive Transformers with CoT consistently outperform Transformers trained on direct datasets. In
particular, 3-layer Transformers already succeed in these tasks with almost perfect accuracy, while
deeper Transformers (L = 3, 4, 5) trained on the direct datasets typically fail.
Robustness to data quality. Unlike the synthetic datasets constructed above, real-world training
datasets are not perfect and often involve corruption or miss intermediate steps. This calls into question
whether the model can still perform well when training on low-quality datasets. To investigate this
question, we construct corrupted datasets for the arithmetic task in Appendix I. Surprisingly, our
results show that the 3-layer Transformer model can still achieve more than 95% accuracy even with
30% of the data missing an intermediate CoT step and involving a single-token corruption. This
clearly demonstrates the robustness of CoT training on low-quality datasets.
16
17
18
Number of Operators
0
20
40
60
80
100
Accuracy (%)
Arithmetic Expression Extrapolation
CoT (L=3)
Figure 3: Performance of length extrapola-
tion experiment, tested on sequences that
are longer than those in training.
Length extrapolation. We finally study whether the
learned autoregressive models can further extrapolate to
data with longer lengths. We construct a CoT training
dataset for the arithmetic task with the number of opera-
tors ranging from 1 to 15, and test the model on expres-
sions with the number of operators in {16, 17, 18}. As
shown in Figure 3, our three-layer Transformer model
still performs well on longer sequences, suggesting that
the model indeed learns the solution to some extent (in-
stead of memorizing data distributions). Potentially, we
believe models trained on more data with varying lengths
can eventually reveal the complete arithmetic rules.
6
Related Work
Owing to the tremendous success of Transformers and Large Language Models across diverse
domains, there has been a substantial body of works dedicated to theoretically comprehending their
capabilities and limitations. Initially, researchers primarily focused on exploring the expressive power
of Transformers in the context of function approximation. Yun et al. [67] proved that Transformers
with sufficient size can universally approximate arbitrary continuous sequence-to-sequence functions
on a compact domain. Recently, universality results have been extended to model variants such as
Sparse Transformers [68] and Transformers with relative positional encodings (RPE) [41].
9

More relevant to this paper, another line of works investigated the power of Transformers from a
computation perspective. Early results have shown that both standard encoder-decoder Transformers
[58] and looped Transformer encoders are Turing-complete [49, 47, 20, 8]. However, these results
depend on the unreasonable assumption of infinite precision, yielding a quite unrealistic construction
that does not match practical scenarios. Recently, Giannou [25] demonstrated that a constant-depth
looped Transformer encoder can simulate practical computer programs. Wei et al. [60] showed
that finite-precision encoder-decoder Transformers can approximately simulate Turing machines
with bounded computation time. Liu et al. [37] considered a restricted setting of learning automata,
for which a shallow non-recursive Transformer provably suffices. Yao et al. [66] demonstrated
that Transformers can recognize or generate bounded-depth Dyck language [30], a specific type
of context-free language. Besides affirmative results, other works characterized the expressivity
limitation of Transformers via the perspective of modeling formal languages [26, 7, 62, 66, 14] or
simulating circuits [27, 43, 42]. However, none of these works (except [66]) explored the setting of
autoregressive Transformers typically adopted in LLMs, which we study in this paper. Moreover, we
consider a more practical setting that targets the emergent ability of LLMs in solving basic reasoning
problems via a readable CoT output, which aligns well with real-world scenarios.
Recently, the power of Transformers has regained attention due to the exceptional in-context learn-
ability exhibited by LLMs [9]. Garg et al. [24] demonstrated that autoregressive Transformers can
in-context learn basic function classes (e.g., linear functions, MLPs, and decision trees) via input
sample sequences. Subsequent works further revealed that Transformers can implement learning
algorithms such as linear regression [1], gradient descent [1, 59, 19], and Bayesian inference [64], and
a broad class of machine learning algorithms [3]. The works of [23, 45] studied in-context learning
via the concept of “induction heads”. All the above works investigated the power of (autoregressive)
Transformer models from an expressivity perspective, which shares similarities to this paper. Here,
we focus on the reasoning capability of Transformers and underscore the key role of CoT in improving
the power of LLMs.
7
Limitations and Future Directions
In this work, from a model-capacity perspective, we theoretically analyze why Chain-of-Thought
prompting is essential in solving mathematical and decision-making problems. Focusing on two
basic mathematical problems as well as Dynamic Programming, we show that a bounded-depth
Transformer without CoT struggles with these tasks unless its size grows prohibitively large. In
contrast to our negative results, we prove by construction that when equipped with CoT, constant-size
Transformers are sufficiently capable of addressing these tasks by generating intermediate derivations
sequentially. Extensive experiments show that models trained on CoT datasets can indeed learn
solutions almost perfectly, while direct prediction always fails. We further demonstrate that CoT has
the potential to generalize to unseen data with longer lengths.
Several foundational questions remain to be answered. Firstly, while this paper investigates why
CoT enhances the expressivity of LLMs, we do not yet answer how the CoT generation process
is triggered by specific prompts. Revealing the relation between prompts and outputs is valuable
for better harnessing LLMs. Secondly, it has been empirically observed that scaling the model size
significantly improves the CoT ability [61]. Theoretically understanding how model size plays a role
in CoT would be an interesting research problem. Thirdly, this paper mainly studies the expressivity
of LLMs in generating CoT solutions, without theoretically thinking about their generalization ability.
Given our experimental results, we believe it is an important future direction for theoretically studying
how LLMs can generalize from CoT demonstrations (even in the out-of-distribution setting, e.g.,
length extrapolation (Figure 3)) [60, 13]. Finally, from a practical perspective, it is interesting to
investigate how models can learn CoT solutions when there are only limited CoT demonstrations in
training (or even purely from direct datasets). We would like to leave these questions as future work,
which we believe are beneficial to better reveal the power and limitations of LLMs.
Acknowledgement.
This work is supported by National Key R&D Program of China
(2022ZD0114900) and National Science Foundation of China (NSFC62276005), and is partially
funded by Microsoft Research Asia Collaborative Research Project. The authors are grateful to David
Chiang, who pointed out a mistake regarding the P-completeness of CFG Membership Testing in the
early version of this paper. The authors also thank all reviewers for their valuable suggestions.
10

Author Contributions
Guhao Feng proposed to analyze the expressivity of Transformers using circuit complexity and
proved all impossibility results in this paper, including Theorems 3.1, 3.2 and 4.8. He came up with
the initial idea of Dynamic Programming. He and Bohang Zhang formalized the DP framework and
proved all positive results in this paper, including Theorems 3.3, 3.4 and 4.7. He contributed to the
paper writing of Appendices C to G.
Bohang Zhang supervised all undergraduate students. He raised the the problem of linear equation.
He and Guhao Feng formalized the DP framework and proved all positive results in this paper,
including Theorems 3.3, 3.4 and 4.7. In the experimental part, he helped generate datasets with
Yuntian Gu, conducted hyper-parameter tuning, and finalized all experiments in Section 5. He was
responsible for writing the majority of this paper and checking/correcting all proofs.
Yuntian Gu was responsible for the experimental part. He wrote the entire code, including the model
details, training pipeline, and evaluation. He created the datasets for all four tasks in Section 5 and
conducted extensive experimental exploration during this project (with the help of Bohang Zhang).
Haotian Ye participated in regular discussions, raised ideas, and helped check the code in experiments.
Di He initiated the problem of studying the capability of Transformers in basic tasks like evaluating
arithmetic expressions. Di He and Liwei Wang led and supervised the research, suggested ideas and
experiments, and assisted in writing the paper.
References
[1] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learn-
ing algorithm is in-context learning? investigations with linear models. In The Eleventh
International Conference on Learning Representations, 2023.
[2] Sanjeev Arora and Boaz Barak. Computational complexity: a modern approach. Cambridge
University Press, 2009.
[3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
Transformers as statis-
ticians: Provable in-context learning with in-context algorithm selection.
arXiv preprint
arXiv:2306.04637, 2023.
[4] David A Barrington. Bounded-width polynomial-size branching programs recognize exactly
those languages in nc. In Proceedings of the eighteenth annual ACM symposium on Theory of
computing, pages 1–5, 1986.
[5] David A Mix Barrington and Denis Therien. Finite monoids and the fine structure of nc. Journal
of the ACM (JACM), 35(4):941–952, 1988.
[6] Richard Bellman. The theory of dynamic programming. Bulletin of the American Mathematical
Society, 60(6):503–515, 1954.
[7] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of trans-
formers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 7096–7116, 2020.
[8] Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of trans-
formers and its implications in sequence modeling. In Proceedings of the 24th Conference on
Computational Natural Language Learning, pages 455–475, 2020.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. In Advances in neural information processing systems, volume 33, pages
1877–1901, 2020.
[10] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece
Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.
11

[11] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in
language models without supervision. arXiv preprint arXiv:2212.03827, 2022.
[12] Samuel R Buss. The boolean formula value problem is in alogtime. In Proceedings of the
nineteenth annual ACM symposium on Theory of computing, pages 123–131, 1987.
[13] Stephanie C.Y. Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh,
Pierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties drive
emergent in-context learning in transformers. In Advances in Neural Information Processing
Systems, 2022.
[14] David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer
encoders. In Proceedings of the 40th International Conference on Machine Learning, pages
5544–5562, 2023.
[15] Andrew Chiu, George Davida, and Bruce Litow. Division in logspace-uniform nc1. RAIRO-
Theoretical Informatics and Applications, 35(3):259–275, 2001.
[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[17] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Introduction to
algorithms. MIT press, 2022.
[18] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems, 2(4):303–314, 1989.
[19] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn
in-context? language models secretly perform gradient descent as meta optimizers. arXiv
preprint arXiv:2212.10559, 2022.
[20] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-
versal transformers. In International Conference on Learning Representations, 2019.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-
ence of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186. Association for
Computational Linguistics, 2019.
[22] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and vari-
able creation in self-attention mechanisms. In International Conference on Machine Learning,
pages 5793–5831. PMLR, 2022.
[23] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep
Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,
Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and
Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread,
2021. https://transformer-circuits.pub/2021/framework/index.html.
[24] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers
learn in-context? a case study of simple function classes. In Advances in Neural Information
Processing Systems, 2022.
[25] Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dim-
itris Papailiopoulos.
Looped transformers as programmable computers.
arXiv preprint
arXiv:2301.13196, 2023.
[26] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions
of the Association for Computational Linguistics, 8:156–171, 2020.
12

[27] Yiding Hao, Dana Angluin, and Robert Frank. Formal language recognition by hard atten-
tion transformers: Perspectives from circuit complexity. Transactions of the Association for
Computational Linguistics, 10:800–810, 2022.
[28] Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
arXiv:1606.08415, 2016.
[29] William Hesse. Division is in uniform tc0. In International Colloquium on Automata, Languages,
and Programming, pages 104–114. Springer, 2001.
[30] John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher D Manning. Rnns
can generate bounded hierarchical languages with optimal memory. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
1978–2010, 2020.
[31] IEEE Computer Society. Ieee standard for floating-point arithmetic. IEEE Std 754-2019, 2019.
[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language
models know? Transactions of the Association for Computational Linguistics, 8:423–438,
2020.
[33] Neil D Jones and William T Laaser. Complete problems for deterministic polynomial time. In
Proceedings of the sixth annual ACM symposium on Theory of computing, pages 40–46, 1974.
[34] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Large language models are zero-shot reasoners. In Advances in Neural Information Processing
Systems, 2022.
[35] Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural
networks, 6(6):861–867, 1993.
[36] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
[37] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-
formers learn shortcuts to automata. In The Eleventh International Conference on Learning
Representations, 2023.
[38] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. ACM Computing Surveys, 55(9):1–35, 2023.
[39] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2017.
[40] Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power
of neural networks: A view from the width. Advances in neural information processing systems,
30, 2017.
[41] Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer
may not be as powerful as you expect. In Advances in Neural Information Processing Systems,
2022.
[42] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision
transformers. Transactions of the Association for Computational Linguistics, 2023.
[43] William Merrill, Ashish Sabharwal, and Noah A Smith. Saturated transformers are constant-
depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843–
856, 2022.
[44] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,
David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,
and Augustus Odena. Show your work: Scratchpads for intermediate computation with language
models. In Deep Learning for Code Workshop, 2022.
13

[45] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,
Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson
Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,
Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer
Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-
heads/index.html.
[46] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[47] Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing complete. The Journal
of Machine Learning Research, 22(1):3463–3497, 2021.
[48] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. In International Conference on Learning Representations,
2022.
[49] Jorge Pérez, Javier Marinkovi´c, and Pablo Barceló. On the turing completeness of modern
neural network architectures. In International Conference on Learning Representations, 2019.
[50] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
[51] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[52] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language
models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,
2021.
[53] Itiroo Sakai. Syntax in universal translation. In Proceedings of the International Conference on
Machine Translation and Applied Language Analysis, 1961.
[54] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
2022.
[55] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,
2018.
[56] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-
bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261,
2022.
[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, volume 30, 2017.
[59] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mord-
vintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent. arXiv preprint arXiv:2212.07677, 2022.
[60] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case
study on approximating turing machines with transformers. In Advances in Neural Information
Processing Systems, volume 35, pages 12071–12083, 2022.
14

[61] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language
models. In Advances in Neural Information Processing Systems, 2022.
[62] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International
Conference on Machine Learning, pages 11080–11090. PMLR, 2021.
[63] Noam Wies, Yoav Levine, and Amnon Shashua. Sub-task decomposition enables learning in
sequence to sequence tasks. In The Eleventh International Conference on Learning Representa-
tions, 2023.
[64] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. In International Conference on Learning
Representations, 2022.
[65] Andrew C Yao. Circuits and local computation. In Proceedings of the twenty-first annual ACM
symposium on Theory of computing, pages 186–196, 1989.
[66] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention
networks can process bounded hierarchical languages. In Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th International Joint
Conference on Natural Language Processing (Volume 1: Long Papers), pages 3770–3785, 2021.
[67] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? In International
Conference on Learning Representations, 2020.
[68] Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi,
and Sanjiv Kumar. O (n) connections are expressive enough: Universal approximability of
sparse transformers. In Advances in Neural Information Processing Systems, volume 33, pages
13783–13794, 2020.
[69] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[70] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting
in large language models. In The Eleventh International Conference on Learning Representa-
tions, 2023.
15

Appendix
The Appendix is organized as follows. Appendix A introduces additional mathematical background
and useful notations. Appendix B presents formal definitions and CoT solutions of the arithmetic
expression task and the linear equation task. Appendix C gives several technical lemmas, which
will be frequently used in our subsequent proofs. The formal proofs for arithmetic expression,
linear equation, and dynamic programming tasks are given in Appendices D, E and G, respectively.
Discussions on other architectures such as encoder-decoder Transformers and RNNs are presented in
Appendix F. Finally, we provide experimental details in Appendix H.
A
Additional Background and Notation
A.1
Finite field
Intuitively, a field is a set F on which addition, subtraction, multiplication, and division are defined
and behave as the corresponding operations on rational and real numbers do. Formally, the two
most basic binary operations in a field is the addition (+) and multiplication (×), which satisfy the
following properties:
• Associativity: for any a, b, c ∈F, (a + b) + c = a + (b + c) and (a × b) × c = a × (b × c);
• Commutativity: for any a, b ∈F, a + b = b + a and a × b = b × a;
• Identity: there exist two different elements 0, 1 ∈F such that a + 0 = a and a × 1 = a for
all a ∈F;
• Additive inverses: for any a ∈F, there exists an element in F, denoted as −a, such that
a + (−a) = 0;
• Multiplicative inverses: for any a ∈F and a ̸= 0, there exists an element in F, denoted as
a−1, such that a × a−1 = 1;
• Distributivity of multiplication over addition: for any a, b, c ∈F, a×(b+c) = (a×b)+(a×c).
Then, subtraction (−) is defined by a −b = a + (−b) for all a, b ∈F; division (÷) is defined by
a ÷ b = a × b−1 for all a, b ∈F, b ̸= 0.
Two most widely-used fileds are the rational number field Q and the real number field R, both of
which satisfy the above properties. However, both fields contain an infinite number of elements. In
this paper, we consider a class of fields called finite fields, which contain a finite number of elements.
Given a prime number p, the finite field Zp is the field consisting of p elements, which can be denoted
as 0, 1, · · · , p −1. In Zp, both addition and multiplication are defined by simply adding/multiplying
two input integers and then taking the remainder modulo p. It can be easily checked that the two
operations satisfy the six properties described above. Thus, subtraction and division can be defined
accordingly. Remarkably, a key result in abstract algebra shows that all finite fields with p elements
are isomorphic, which means that the above definitions of addition, subtraction, multiplication, and
division are unique (up to isomorphism).
As an example, consider the finite field Z5. We have that 2 + 3 equals 0, since (2 + 3) mod 5 = 0.
Similarly, 2 × 3 equals 1; 2 −3 equals 4; and 2 ÷ 3 equals 4.
In Section 3, we utilize the field Zp to address the issue of infinite tokens. Both tasks of evaluating
arithmetic expressions and solving linear equations (Section 3.1) are well-defined in this field.
A.2
Circuit complexity
In circuit complexity theory, there are several fundamental complexity classes that capture different
levels of computation power. Below, we provide a brief overview of these classes; however, for a
comprehensive introduction, we recommend readers refer to Arora & Barak [2].
The basic complexity classes we will discuss in this subsection are NC0, AC0, TC0, NC1, and P.
These classes represent increasing levels of computation complexity. The relationships between these
16

classes can be summarized as follows:
NC0 ⊊AC0 ⊊TC0 ⊂NC1 ⊂P
Moreover, in the field of computational theory, it is widely conjectured that all subset relations in
the hierarchy are proper subset relations. This means that each class is believed to capture a strictly
larger set of computational problems than its predecessor in the hierarchy. However, proving some
of these subset relations to be proper remains a critical open question in computational complexity
theory. For example, NC1 = P will imply P = NP, which is widely regarded as impossible but is
still a celebrated open question in computer science.
To formally define these classes, we first introduce the concept of Boolean circuits. A Boolean circuit
with n input bits is a directed acyclic graph (DAG), in which every node is either an input bit or an
internal node representing one bit (also called a gate). The value of each internal node depends on its
direct predecessors. Furthermore, several internal nodes are designated as output nodes, representing
the output of the Boolean circuit. The in-degree of a node is called its fan-in number, and the input
nodes have zero fan-in.
A Boolean circuit can only simulate a computation problem of a fixed number of input bits. When the
input length varies, a series of distinct Boolean circuits will be required, each designed to process a
specific length. In this case, circuit complexity studies how the circuit size (e.g., depth, fan-in number,
width) increases with respect to the input length for a given computation problem. We now describe
each complexity class as follows:
• NC0 is the class of constant-depth, constant-fan-in, polynomial-sized circuits consisting of
AND, OR, and NOT gates. NC0 circuits is the weakest class in the above hierarchy with
limited expressive power because they cannot express functions that depend on a growing
number of inputs as the input size increases. For example, the basic logical-AND function
with an arbitrary number of input bits is not in NC0. In [22], the authors considered a restricted
version of the Transformer model with constant depth and a constant-degree sparse selection
construction, which can be characterized by this complexity class.
• AC0 is the class of constant-depth, unbounded-fan-in, polynomial-sized circuits consisting
of AND, OR, and NOT gates, with NOT gates allowed only at the inputs. It is strictly more
powerful than NC0 mainly because the fan-in number can (polynomially) depend on the input
length. However, there are still several fundamental Boolean functions that are not in this
complexity class, such as the parity function or the majority function (see below).
• TC0 is an extension of AC0 that introduces an additional gate called MAJ (i.e., the majority).
The MAJ gate takes an arbitrary number of input bits and evaluates to false when half or
more of the input bits are false, and true otherwise. Previous work [42, 43] showed that the
log-precision Transformer is in this class.
• NC1 is a complexity class that consists of constant-fan-in, polynomial-sized circuits with a
logarithmic depth of O(log n), where n is the input length. Similar to NC0, the basic logical
gates are AND, OR, and NOT. Allowing the number of layers to depend on the input length
significantly increases the expressiveness of the circuit. On the other hand, the logarithmic
dependency still enables a descent parallelizability. Indeed, NC1 is widely recognized as an
important complexity class that captures efficiently parallelizable algorithms.
• P is the complexity class that contains problems that can be solved by a Turing machine in
polynomial time. It contains a set of problems that do not have an efficient parallel algorithm.
For example, the Context-Free-Grammar Membership Testing is in this class and is proved to
be P-complete [33].
A.3
Log-precision
In this work, we focus on Transformers whose neuron values are restricted to be floating-point
numbers of finite precision, and all computations operated on floating-point numbers will be finally
truncated, similar to how a computer processes real numbers. In practice, the two most common
formats to store real numbers are the fixed-point format and floating-point format (e.g., the IEEE-754
standard [31]). Likewise, there are several popular truncation approaches (also called rounding), such
as round-to-the-nearest, round-to-zero, round-up, and round-down. Our results in this paper hold for
both formats and all these truncation approaches.
17

Specifically, the log-precision assumption means that we can use O(log(n)) bits to represent a real
number, where the length of the input sequence is bounded by n. For any floating-point format
described above with O(log(n)) bits, an important property is that it can represent all real numbers of
magnitude O(poly(n)) within O(poly(1/n)) truncation error. We next analyze how the truncation
error will propagate and magnify in a log-precision Transformer from the input to the output layer.
Note that since the functions represented by Transformers are continuous, the approximation error in
a hidden neuron will smoothly influence the approximation error of subsequent neurons in deeper
layers. This impact can be bounded by the Lipschitz constant of the Transformer, which depends
on its basic layers. In particular, the softmax function (in attention) is 1-Lipschitz, the GeLU
activation is 2-Lipschitz, and the Lipschitz constant of a linear layer depends on the scale of its
weight parameters. Combining these together leads to the following result: given a bounded-depth
log-precision Transformer of polynomial size, when all parameter values of the Transformer are
further bounded by O(poly(n)), all neuron values only yield an O(poly(1/n)) approximation error
compared to the infinite-precision counterpart. Therefore, if a problem can be solved by a bounded-
depth polynomial-size infinite-precision Transformer with polynomially-bounded parameters, it
can also be solved by a log-precision Transformer of the same size. This finding is helpful for
understanding Theorems 3.3, 3.4 and 4.7.
Finally, we point out that a key property of log-precision Transformer is that each neuron can only
hold O(log(n))-bit information and thus cannot store the full information of the entire input sequence.
Therefore, the log-precision assumption captures the idea that the computation must be somehow
distributed on each token, which well-resembles practical situations and the way Transformers work.
B
Formal Definitions of CoT in Section 3
In this section, we will formally define the CoT derivation formats for the two math problems
described in Section 3.
Arithmetic expression. In an arithmetic expression that contains operators, there exists at least one
pair of neighboring numbers connected by an operator that can be calculated, which we refer to as a
handle. More precisely, one can represent an arithmetic expression into a (binary) syntax tree where
each number is a leaf node and each operator is an internal node that has two children. In this case, a
pair of neighboring numbers is a handle if they share the same parent in the syntax tree. For instance,
consider the arithmetic formula 7 × (6 + 5 + 4 × 5). Then, 6 + 5 and 4 × 5 are two handles.
An important observation is that we can determine whether a pair of numbers a and b can form a
handle with the operator op by examining the token before a and the token after b, where these tokens
are either operators, brackets, or empty (i.e., approaching the beginning/ending of the sequence,
including the equal sign ‘=’). Specifically, given subsequence s1 a op b s2, we have that a op b
forms a handle iff one of the following conditions holds:
• op ∈{+, −} and s1 ∈{ ( , empty}, s2 /∈{×, ÷};
• op ∈{×, ÷} and s1 /∈{× , ÷}.
In the proposed chain of thought (CoT), an autoregressive Transformer calculates one handle at each
step. If there are multiple handles, the leftmost handle is selected. The subsequence a op b is then
replaced by the calculated result. For the case of s1 = ( and s2 = ), there will be a pair of redundant
brackets and thus the two tokens are removed. It is easy to see that the resulting sequence is still a
valid arithmetic expression. By following this process, each CoT step reduces one operator and the
formula is gradually simplified until there is only one number left, yielding the final answer.
System of linear equations. Assume that we have a system of m linear equations with variables
x1, x2, . . . , xm. The i-th equation in the input sequence is grammatically written as ai1x1 + ai2x2 +
· · · + aimxm = bi, where aij ∈{0, · · · , p −1} and bi ∈{0, · · · , p −1}. For simplicity, we do not
omit the token aij or aijxj in the input sequence when aij ∈{1, 0}.
We can construct the following CoT to solve the equations by using the Gaussian elimination
algorithm. At each step i, we select an equation k satisfying the following two conditions:
• The coefficient of xi is nonzero.
• The coefficients of x1, · · · , xi−1 are all zero.
18

Such an equation must exist, otherwise the solution is not unique or does not exist. If there are
multiple equations satisfying the above conditions, we choose the k-th equation with the smallest
index k. We then swap it with equation i, so that the i-th equation now satisfy the above conditions.
We then eliminate the variable xi in all other equations by leveraging equation i. Formally, denote
the i-th equation at the i-th step as
a(i)
ii xi + a(i)
i,i+1xi+1 + · · · + a(i)
imxm = bi,
(7)
and denote the coefficient of xi in the j-th equation (j ̸= i) as a(i)
ji . We can multiply (7) by
−(a(i)
ii )−1a(i)
ji and add the resulting equation to the j-th equation. This will eliminate the term
xi in the j-th equation. We further normalize equation i so that the coefficient a(i)
ii becomes 1.
Depending on whether j ≤i or j > i, the resulting equation in the CoT output will have the
following grammatical form:
• If j ≤i, the j-th equation will be written as xj + ˜aj,i+1xi+1 + · · · + ˜ajmxm = ˜bj;
• If j > i, the j-th equation will be written as ˜aj,i+1xi+1 + · · · + ˜ajmxm = ˜bj.
Note that we remove all zero terms ˜ajkxk for k ≤i, k ̸= j in the CoT output and also remove
the coefficient 1 in ˜akkxk for k ≤i, similar to how human write solutions (see Figure 1 for an
illustration). However, to simplify our proof, we reserve the coefficient 0 or 1 (i.e., outputting 0xk or
1xk) when k > i since it cannot be determined easily before computing the coefficient. The above
process is repeated for m −1 steps, and after the final step we obtain the solution.
C
Technical Lemmas
C.1
Technical lemmas for MLP
In this subsection, we will demonstrate the representation efficiency of two-layer MLPs in performing
several basic operations, such as multiplication, linear transformation, conditional selection, and
look-up table. These operations will serve as building blocks in performing complex tasks.
We first show that a two-layer MLP with GeLU activation can efficiently approximate the scalar
multiplication, with all weights bounded by O(poly(1/ϵ)) where ϵ is the approximation error.
Lemma C.1. Let f : R2 →R be a two-layer MLP with GeLU activation, and the hidden dimension
is 4. Then, for any ϵ > 0 and M > 0, there exist MLP parameters with ℓ∞norm upper bounded by
O(poly(M, 1/ϵ)) such that |f(a, b) −ab| ≤ϵ holds for all a, b ∈[−M, M].
Proof. Denote the input vector to the MLP as (a, b) ∈R2. After the first linear layer, it is easy to
construct a weight matrix such that the hidden vector is 1
λ(a + b, −a −b, a −b, −a + b) ∈R4, where
λ is an arbitrary scaling factor. Let σ be the GeLU activation. We can similarly construct a weight
vector such that the final output of the MLP is
f(a, b) =
√
2πλ2
8

σ
a + b
λ

+ σ
−a −b
λ

−σ
a −b
λ

−σ
−a + b
λ

.
We will prove that the above MLP satisfies the theorem by picking an appropriate λ. By definition
of GeLU activation, σ(x) = xΦ(x) where Φ(x) is the standard Gaussian cumulative distribution
function. We thus have σ′(0) = 0.5 and σ′′(0) =
p
2/π. Applying Taylor’s formula and assuming
λ > 2M, we have
σ
a + b
λ

+ σ
−a −b
λ

−σ
a −b
λ

−σ
−a + b
λ

−
8ab
√
2πλ2

≤

1
2
r
2
π
 a + b
λ
2
+
−a −b
λ
2
−
a −b
λ
2
−
−a + b
λ
2!
−
8ab
√
2πλ2

+ 4
3!
(2M)3
λ3
 max
x∈[−1,1] σ(3)(x)

=16M 3
3λ3
max
x∈[−1,1]
1
√
2π (x3 −4x) exp(−x2
2 ) <
80M 3
3
√
2πλ3 .
19

Therefore,|f(a, b)−ab| < 10M 3
3λ . Set λ ≥10M 3
3ϵ , and then we can obtain |f(a, b)−ab| < ϵ. Moreover,
each weight element in the MLP is upper bounded by O(λ2), which is clearly O(poly(M, 1/ϵ)).
Next, we will demonstrate that a two-layer MLP with GeLU activation can efficiently approximate a
two-layer MLP with ReLU activation, with all weights upper bounded by O(poly(1/ϵ)). This result
is useful in proving subsequent lemmas.
Lemma C.2. Let g : Rd1 →Rd2 be a two-layer MLP with ReLU activation, and all parameter
values are upper bounded by M. Then, for any ϵ > 0, there exists a two-layer MLP f of the same size
with GeLU activation and parameters upper bounded by O(poly(M, 1/ϵ)) in the ℓ∞norm, such
that for all x ∈Rd1, we have ∥f(x) −g(x)∥∞≤ϵ.
Proof. Let g(x) = W2 · ReLU(W1x). We construct f(x) = 1
λW2 · GeLU(λW1x) where λ > 0
is a sufficiently large constant. To prove that ∥f(x) −g(x)∥∞≤ϵ for all x ∈Rd1, it suffices to
prove that ∥W2(ReLU(z) −1
λGeLU(λz))∥∞≤ϵ for all z ∈Rd where d is the hidden size. Since
W2

ReLU(z) −1
λGeLU(λz)

∞
≤∥W2∥∞
ReLU(z) −1
λGeLU(λz))

∞
≤Md
ReLU(z) −1
λGeLU(λz))

∞
,
it suffices to consider the scalar setting and prove that | 1
λGeLU(λy) −ReLU(y)| ≤ϵ/Md for all
y ∈R. By definition of ReLU and GeLU, we have

1
λGeLU(λy) −ReLU(y)
 = 1
λ
ReLU(λy) −
Z λy
−∞
λy
√
2π exp

−t2
2

dt
 .
(8)
When y ≥0, (8) becomes
1
λ

Z +∞
−∞
λy
√
2π exp

−t2
2

dt −
Z λy
−∞
λy
√
2π exp

−t2
2

dt
 = 1
λ
Z +∞
λy
λy
√
2π exp

−t2
2

dt.
Combined with the case of y < 0, (8) can be consistently written as

1
λGeLU(λy) −ReLU(y)
 = 1
λ
Z +∞
λ|y|
λ|y|
√
2π exp

−t2
2

dt
≤
1
√
2πλ
Z +∞
λ|y|
t exp

−t2
2

dt =
1
√
2πλ exp

−λ2y2
2

≤
1
√
2πλ.
Picking λ =
Md
√
2πϵ yields the desired result and completes the proof.
Equipped with the above result, we now prove that a two-layer MLP with GeLU activation can
perform linear transformation and conditional selection.
Proposition C.3. Let f : Rd1 →Rd2 be a two-layer MLP with GeLU activation, and the hidden
dimension is 2d2. Let W ∈Rd2×d1 be any matrix and denote M = maxij |Wij|. Then, for any
ϵ > 0, there exist MLP parameters with ℓ∞norm bounded by O(poly(M, 1/ϵ)), such that for any
x ∈Rd1, we have ∥f(x) −W x∥∞≤ϵ.
Proof. We can use a two-layer MLP with ReLU activation to implement g(x) = W x by the
following construction:
W x = ReLU(W x) + ReLU(−W x)
Combined with Lemma C.2, we can also implement g(x) by a two-layer MLP with GeLU activation.
20

Lemma C.4. Define the selection function g : Rd × Rd × R →Rd as follows:
g(x, y, t) =

x
if t ≥0,
y
if t < 0.
(9)
Let f : Rd × Rd × R →Rd be a two-layer MLP with GeLU activation, and the hidden dimension
is 2d + 2. Then, for any ϵ > 0, α > 0, and M > 0, there exist MLP parameters with ℓ∞
norm bounded by O(poly(M, 1/α, 1/ϵ)), such that for all x ∈[−M, M]d, y ∈[−M, M]d, and
t ∈[−∞, −α] ∪[α, +∞], we have ∥f(x, y, t) −g(x, y, t)∥∞≤ϵ.
Proof. We can simply use a two-layer MLP with ReLU activation to implement g by the following
construction:
h(x, y, t) = (h1, h2, h3, h4) := (x + α−1Mt1d, y −α−1Mt1d, α−1Mt, −α−1Mt) ∈R2d+2,
f(x, y, t) = ReLU(h1) −ReLU(h3)1d + ReLU(h2) −ReLU(h4)1d,
where 1d is the all-one vector of d dimension. It is easy to check that, for all x ∈[−M, M]d1,y ∈
[−M, M]d2, and t ∈[−∞, −α] ∪[α, +∞], we have f(x, y, t) = g(x, y, t). Moreover, all parame-
ters are bounded by O(M/α). Therefore, by using Lemma C.2, we can also implement g(x) by a
two-layer MLP with GeLU activation and all parameters are bounded by O(poly(M, 1/α, 1/ϵ)).
We final show that a two-layer MLP can efficiently represent a look-up table. Consider a k-
dimensional table of size dk, where each element in the table is an integer ranging from 1 to d.
Denote the set D = {ei : i ∈[d]}, where ei is a d-dimensional one-hot vector with the i-th element
being 1. The above look-up table can thus be represented as a discrete function g : Dk →D. The
following lemma shows that g can be implemented by a two-layer MLP with GeLU activation.
Lemma C.5. Let g : Dk →D be any function defined above, and let f : Rk×d →Rd be a two-layer
MLP with GeLU activation and bias, and the hidden dimension is dk. Then, for any ϵ > 0, there
exist MLP parameters with ℓ∞norm bounded by O(poly(k, 1/ϵ)), such that for all x ∈Dk ⊂Rk×d
and all perturbation δ ∈[−1/2k, 1/2k]k×d, we have ∥f(x + δ) −g(x)∥∞≤ϵ + 2k∥δ∥∞, where
∥δ∥∞is the vector ℓ∞-norm applied to the flattended matrix δ.
Proof. We can simply use a two-layer MLP with ReLU activation to implement g by the following
construction. Denote the index of the MLP hidden layer as (i1, · · · , ik) ∈[d]k. We can construct the
weights of the first MLP layer such that
h(i1,··· ,ik)(x) = 2(xi1 + xd+i2 · · · + x(k−1)d+ik) −2k + 1.
We can then construct the weights of the second layer such that the final output of the MLP is
fj(x) =
X
gj(ei1,··· ,eik )=1
ReLU(h(i1,··· ,ik)(x)).
One can check that f(x) = g(x) holds for all x ∈Dk ⊂Rd×k. Furthermore, we have
∥f(x + δ) −g(x)∥∞= ∥f(x + δ) −f(x)∥∞
= max
j∈[d]

X
gj(ei1,··· ,eik )=1
 ReLU(h(i1,··· ,ik)(x + δ)) −ReLU(h(i1,··· ,ik)(x)


≤
max
(i1,··· ,ik)∈[d]k
h(i1,··· ,ik)(x + δ) −h(i1,··· ,ik)(x)
 ≤2k∥δ∥∞
for all perturbations δ ∈[−1/2k, 1/2k]k×d. Thus by using Lemma C.2, we can also implement g(x)
by a two-layer MLP with GeLU activation and all parameters are bounded by O(poly(k, 1/ϵ)).
C.2
Technical lemmas for the attention layer
In this subsection, we will introduce two special operations that can be performed by the attention
layer (with causal mask). Below, let n ∈N be an integer and let x1, x2, · · · , xn be a sequence of
vectors where xi = (˜xi, ri, 1) ∈[−M, M]d+2, ˜xi ∈Rd, ri ∈R, and M is a large constant. Let
K, Q, V ∈Rd′×(d+2) be any matrices with ∥V ∥∞≤1, and let 0 < ρ, δ < M be any real numbers.
Denote qi = Qxi, kj = Kxj, vj = V xj, and define the matching set Si = {j ≤i : |qi · kj| ≤ρ}.
Equipped with these notations, we define two basic operations as follows:
21

• COPY: The output is a sequence of vectors u1, · · · , un with ui = vpos(i), where pos(i) =
argmaxj∈Si rj.
• MEAN: The output is a sequence of vectors u1, · · · , un with ui = meanj∈Si vj.
The output ui is undefined when Si = ∅. We next make the following regularity assumption:
Assumption C.6. The matrices Q, K, V and scalars ρ, δ satisfy that for all considered sequences
x1, x2, · · · , xn, the following hold:
• For any i, j ∈[n], either |qi · kj| ≤ρ or qi · kj ≤−δ.
• For any i, j ∈[n], either i = j or |ri −rj| ≥δ.
Assumption C.6 says that there are sufficient gaps between the attended position (e.g., pos(i)) and
other positions. The two lemmas below show that the attention layer with casual mask can implement
both COPY operation and MEAN operation efficiently.
Lemma C.7. Assume Assumption C.6 holds with ρ ≤
δ2
8M . For any ϵ > 0, there exists an at-
tention layer with embedding size O(d) and one causal attention head that can approximate the
COPY operation defined above. Formally, for any considered sequence of vectors x1, x2, . . . , xn,
denote the corresponding attention output as o1, o2, . . . , on. Then, we have ∥oi −ui∥∞≤ϵ
for all i ∈[n] with Si ̸= ∅. Moreover, the ℓ∞norm of attention parameters is bounded by
O(poly(M, 1/δ, log(n), log(1/ϵ))).
Proof. The purpose of the attention head is to focus only on the vector that needs to be copied. To
achieve this, we construct the key, query, and value vectors as follows (by assigning suitable key,
query, and value weight matrices in the attention head):
• Query: (λqi, µ) ∈Rd+1
• Key: (ki, ri) ∈Rd+1
• Value: vi ∈Rd
where λ and µ are constants that will be defined later. Denote aij as the attention score, then
ai,j =
exp(λ(qi · kj) + µrj)
P
j′ exp(λ(qi · kj′) + µrj′) =
exp(λ(qi · kj))
P
j′ exp(λ(qi · kj′) + µ(rj′ −rj)).
Since ρ ≤
δ2
8M and M ≥δ, we have δ −ρ ≥7
8δ. By setting λ = 8M ln( 2nM
ϵ
)
δ2
and µ = 3 ln( 2nM
ϵ
)
δ
(which are bounded by O(poly(M, 1/δ, log(n), log(1/ϵ)))), we have
ai,pos(i) ≥
exp(−λρ)
exp(−λρ) + (n −1) exp(max(−λδ + 2Mµ, λρ −µδ))
(10)
=
1
1 + (n −1) exp(max(−λ(δ −ρ) + 2Mµ, 2λρ −µδ))
≥1 −n exp(max(−λ(δ −ρ) + 2Mµ, 2λρ −µδ))
(11)
≥1 −n exp

max

−M
δ ln
2nM
ϵ

, −ln
2nM
ϵ

≥1 −n exp

−ln
2nM
ϵ

(12)
= 1 −
ϵ
2M ,
where in (10) we use Assumption C.6, which implies that whenever j′ ̸= pos(i), either qi · kj′ ≤−δ
or (qi · kj′ ≤ρ and rj′ −rj ≤−δ); in (11) we use the inequality
1
1+x ≥1 −x for all x ≥0; in (12)
we use the fact that M ≥δ. We thus have
∥oi −ui∥∞=

X
j
aijvj −vpos(i)

∞
≤M∥V ∥∞·

1 −ai,pos(i) +
X
j̸=pos(i)
ai,j


= M∥V ∥∞(2 −2ai,pos(i)) ≤ϵ,
which concludes the proof.
22

Lemma C.8. Assume Assumption C.6 holds with ρ ≤
δϵ
16M ln( 4Mn
ϵ
). For any 0 < ϵ ≤M, there exists
an attention layer with embedding size O(d) and one causal attention head that can approximate the
MEAN operation defined above. Formally, for any considered sequence of vectors x1, x2, . . . , xn,
denote the attention output as o1, o2, . . . , on. Then, we have ∥oi−ui∥∞≤ϵ for all i ∈[n] with Si ̸=
∅. Moreover, the ℓ∞norm of attention parameters is bounded by O(poly(M, 1/δ, log(n), log(1/ϵ))).
Proof. The purpose of the attention head is to average across all tokens that satisfy the condition
qi · kj ≈0. To achieve this, we construct the key, query, and value vectors as follows:
• Query: λqi ∈Rd
• Key: ki ∈Rd
• Value: vi ∈Rd
where λ is a constant which will be defined later. Denote aij as the attention score, then
ai,j =
exp(λqi · kj)
P
j′ exp(λqi · kj′).
By setting λ = 1
δ ln
  4Mn
ϵ

(which is bounded by O(poly(M, 1/δ, log(n), log(1/ϵ)))), we have:
X
j /∈Si
aij ≤
(n −|Si|) exp(−λδ)
(n −|Si|) exp(−λδ) + |Si| exp(−λρ)
(13)
=
1
1 +
|Si|
n−|Si| exp(−λ(ρ −δ))
< n exp(λρ) exp(−λδ))
(14)
≤n exp

ϵ
16M

exp

−ln
4Mn
ϵ

<
ϵ
3M ,
(15)
where in (13) we use Assumption C.6, which implies that qi ·kj ≤−δ for all j /∈Si and qi ·kj ≥−ρ
for all j ∈Si; in (14) we use the inequality
1
1+x < 1
x for all x > 0; in (15) we use that the assumption
that ϵ ≤M and the fact that exp(1/16) < 4/3.
Similarly, for any j ∈Si, we have
aij −
1
|Si|
 ≤max
 1
|Si| −
exp(−ρλ)
|Si| exp(ρλ) + (n −|Si|) exp(−λδ),
exp(ρλ)
|Si| exp(−ρλ) −
1
|Si|

(16)
≤
1
|Si| max

1 −
1
exp(2ρλ) + n exp(−λ(δ −ρ)), exp(2ρλ) −1

≤
1
|Si| max (exp(2ρλ) −1 + n exp(−λ(δ −ρ)), exp(2ρλ) −1)
(17)
=
1
|Si| (exp(2ρλ) −1 + n exp(−λ(δ −ρ)))
≤
1
|Si|

exp
 ϵ
8M

−1 +
ϵ
3M

(18)
≤
2ϵ
3M|Si|
(19)
where in (16) we use Assumption C.6 similarly as before; in (17) we use the inequality 1 −1
x ≤x −1
for all x > 0; in (18) we use the inequality previously derived in (15); in (19) we use the inequality
exp(x) ≤1 + 2x for all 0 ≤x ≤1. We thus obtain
∥oi −ui∥∞=

X
j
aijvj −
1
|Si|
X
j∈Si
vj

∞
≤M∥V ∥∞·

X
j /∈Si
aij +
X
j∈Si
aij −
1
|Si|


≤ϵ,
which concludes the proof.
23

D
Arithmetic Formula
In this section, we prove that the autoregressive Transformer can evaluate arithmetic expressions
when equipped with CoT, whereas the it cannot solve this task without CoT.
D.1
Proof of Theorem 3.3
Before proving this theorem, there is one point that needs to be clarified: all residual connections
in the attention/MLP layers can be replaced by concatenation, in the sense that both architectures
have the same expressive power. Formally, consider an MLP (or an attention layer) denoted as
f : Rd →Rd, and let y = f(x). It is easy to see that we can construct another MLP (or attention
layer) denoted as g : R2d →R2d such that g(x, 0) + (x, 0) = (0, y) + (x, 0) = (x, y), namely, the
residual connection can implement concatenation. Conversely, concatenation can implement residual
connection by using a linear projection. Based on the equivalence, we can use the concatenation
operation instead of residual connection in all subsequent proofs presented in Appendices D, E and G.
Similarly, the output of multi-head attention can be replaced by the concatenation of the output of
each head (instead of performing aggregation via matrices W (l,h)
O
defined in (2)). For clarity, we
further omit the unnecessary parts in the concatenated outputs and only retain the outputs that are
used in subsequent layers.
We now present the proof of Theorem 3.3. For ease of reading, we restate Theorem 3.3 below:
Theorem D.1. For any prime p and integer n > 0, there exists an autoregressive Transformer defined
in Section 2 with hidden size d = O(poly(p)) (independent of n), depth L = 5, and 5 heads in each
layer that can generate the CoT solution defined in Appendix B for all inputs in Arithmetic(n, p).
Moreover, all parameter values in the Transformer are bounded by O(poly(n)).
Proof sketch. The intuition behind our construction is that when the CoT output proceeds to a certain
position, the Transformer can read the context related to this position and determine whether it should
copy a token or perform a calculation. Remarkably, the context only contains a fixed number of
tokens (as discussed in Appendix B). Based on the key observation, we can construct our five-layer
transformer as follows. The first layer collects important positional information. The second and third
layers determine whether to perform a calculation by examining the context related to the current
token, which contains five tokens. The fourth layer and the fifth layers are used to generate the output
via three cases: before/at/after the position that performs a calculation. For the first and the last cases,
the output simply copies a previous token with position computed by the two layers. For the middle
case, the outcome is computed via a look-up table that stores the arithemtic rules (+, -, ×, ÷).
Proof. We construct each layer as follows.
Token Embeddings. Assume that we have a sequence of tokens s1, . . . , si and we want to generate
the next token si+1. For any j ∈[n], let id(sj) be the index of token sj in the embedding dictionary,
with values ranging from 1 to the number of tokens. We can embed the token sj by x(0)
j
=
(eid(sj), j, 1) ∈Rnum_tokens+2, where ej is a one-hot vector with the j-th element being 1, j ∈N+ is
the positional embedding, and the constant embedding 1 is used as a bias term.
Layer 1. The first layer of the autoregressive Transformer uses two attention heads to perform the
following tasks:
1. Count the number of equal signs (‘=’) in previous tokens, denoted as n=
i , i.e., n=
i = |{j ≤i :
sj = ‘=’}|.
2. Copy the position of the last equal sign, denoted as p=
i , i.e., p=
i = max{j : j ≤i, sj = ‘=’}.
If the set {j : j ≤i, sj = ‘=’} is empty, define p=
i = 0.
3. Compute i2.
Based on Appendix C.2 (Lemma C.8), we can use the first attention head to perform the MEAN
operation that counts the percentage of equal signs in the preceding sentences (i.e., n=
i /i). This can
be achieved by setting Q = 0, K = 0, V = (eid(‘=’), 0, 0)⊤(defined in Appendix C.2), so that
qi = 0,
kj = 0,
vj = eid(‘=’) · eid(sj) = I[sj = ‘=’],
Si = [i].
24

Similarly, we can use the second attention head to perform a COPY operation that copies the position
index of the last equal sign (by Lemma C.7). This can be achieved by setting Q = (0, 0, 1)⊤,
K = (eid(‘=’), 0, −1)⊤, V = (0, 1, 0)⊤, rj = j (defined in Appendix C.2), so that
qi = 1,
kj = I[sj = ‘=’] −1,
vj = j,
Si = {j ≤i : sj = ‘=’}.
It is easy to check that the above construction outputs ui = max{j : j ≤i, sj = ‘=’} when Si ̸= ∅.
Note that ui may not equal to p=
i when Si = ∅.
Using the residual connection to perform concatenation, the output of the attention layer has the form
(eid(si), i, 1, n=
i /i, max{j : j ≤i, sj = ‘=’}). We can then use an MLP to multiply n=
i /i and i to
obtain n=
i and use another MLP to compute i2 according to Lemma C.1; Simultaneously, we can
compute the value p=
i using the following way:
p=
i =

max{j : j ≤i, sj = ‘=’}
if n=
i /i ≥1/n,
0
if n=
i /i = 0,
which is a conditional selection operation and can be implemented by an MLP (Lemma C.4). Also
note that the gap in Lemma C.4 is α = 1/2n, which can be implemented within log-precision. The
final output of the first layer has the form x(1)
i
= (eid(si), i, i2, n=
i , p=
i , 1).
Layer 2. The second layer of the Transformer does some tricky preparation work for the next layer.
1. Compute the distance to the nearest and the last equal sign, denoted as d=
i and ˆd=
i , respectively.
Formally, d=
i = i −max{j : j ≤i, sj = ‘=’}, ˆd=
i = i −max{j : j < i, sj = ‘=’}. If the
nearest/last equal sign does not exist, define d=
i = i or ˆd=
i = i. The relation between d=
i , ˆd=
i ,
and p=
i can be expressed as d=
i = i −p=
i , ˆd=
i = i −p=
i−1.
2. Count the number of equal signs in strictly previous tokens, denoted as ˆn=
i , i.e., ˆn=
i = |{j <
i : sj = ‘=’}|.
3. Compute (n=
i )2, (ˆn=
i )2, (d=
i )2, and ( ˆd=
i )2.
The first and the second tasks can be done using the COPY operation by setting
qi = Qx(1)
i
= ((i−1)2, i−1, 1),
kj = Kx(1)
j
= (−1, 2j, −j2),
rj = 0,
vj = (nj, j−p=
j +1).
Under the above construction, we have qi · kj = −(i −j −1)2, and thus Si = {i −1}, namely,
the output is (ˆni, ˆd=
i ). We then use an MLP to calculate (d=
i )2, ( ˆd=
i )2, (n=
i )2, and (ˆn=
i )2 by using
Lemma C.1. The output of the second layer is
x(2)
i
= (eid(si), i, n=
i , ˆn=
i , d=
i , ˆd=
i , (n=
i )2, (ˆn=
i )2, (d=
i )2, ( ˆd=
i )2, 1).
Layer 3. The third Transformer layer judges whether the calculation should be performed at the
current position and computes the result when needed. Based on the CoT format given in Appendix B,
we need to extract five previous tokens related to this position. Formally, we need five attention heads
to perform the following tasks:
1. Copy the embedding eid(sj) located at position j such that ˆn=
j = n=
i −1 and ˆd=
j = d=
i + t
for t ∈{1, 2, 3, 4, 5}, as shown in Figure 4.
2. Check if the copied expression can be evaluated at the current position according to the rule
given in Appendix B. If it can be evaluated, compute the result and determine how much
sentence length will be reduced after this calculation (see Appendix B for details on how
the reduced sentence length depends on brackets); otherwise, keep the token eid(sj) with
ˆn=
j = n=
i −1 and ˆd=
j = d=
i + 1.
We can use the multi-head attention to perform the COPY operation five times in parallel. For each t,
we construct the matrices Q, K, V of the COPY operation such that
qi = Qx(2)
i
= [(n=
i )2 −2n=
i + 1,
1,
n=
i −1,
(d=
i )2 + 2td=
i + t2,
1,
d=
i −t]⊤,
kj = Kx(2)
j
=[
−1,
−(ˆn=
j )2,
2ˆn=
j ,
−1,
−( ˆd=
j )2,
2 ˆd=
j
]⊤,
vj = eid(sj),
25

1 + 2 × (3 + 4) = 1 + 2 ×
�
��
=
��
=
�
1 + 2 =
�
��
=
��
=
�
��
=
Figure 4: Illustration of the proof of Theorem 3.3.
and
Kx(2)
i
· Qx(2)
j
= −(n=
i −ˆn=
j −1)2 −(d=
i −ˆd=
j + t)2.
Therefore, Kx(2)
i
· Qx(2)
j
= 0 only when ˆn=
j = n=
i −1 and ˆd=
j = d=
i + t, and Kx(2)
i
· Qx(2)
j
≤−1
otherwise. It is easy to see that:
• Whenever n=
i > 0, for any t, the number of indices j satisfying qi · kj = 0 is at most one
(i.e., unique).
• Whenever n=
i > 0, for any t, the index j satisfying qi · kj = 0 exists, unless there is a t′ < t
such that the copied token at t′ is an equal sign (‘=’).
In other words, based on Lemma C.7, the above property guarantees that we can copy the desired
tokens until reaching an equal sign, after which the copied tokens are invalid as illustrated in
Figure 4(right). The output of the attention layer can be written as
(eid(si), ej1, ej2, ej3, ej4, ej5, i, n=
i , (n=
i )2, d=
i , ˆn=
i , (ˆn=
i )2, ˆd=
i , ( ˆd=
i )2, 1),
where we slightly abuse notation and use ejt to denote the embedding we copied by the t-th attention
heads.
We can then use an MLP to perform the second task. Note that whether the current position can be
calculated or not depends on the following six tokens (eid(si), ej1, ej2, ej3, ej4, ej5). Concretely,
there are several cases:
• eid(si) corresponds to the embedding of a number or a right bracket. In this case, the current
position should simply output ej1 (which is an operator or a right bracket).
• eid(si) corresponds to the embedding of a left bracket, an operator, or the equal sign ‘=’. In
this case, ej1 corresponds to the embedding of a number or a left bracket. There are two
subcases:
– ej1 corresponds to the embedding of a number. In this case, whether the current posi-
tion can be evaluated depends on (eid(si), ej1, ej2, ej3, ej4) according to Appendix B.
– ej1 corresponds to the embedding of a left bracket. In this case, whether the cur-
rent position can be evaluated simply depends on whether ej5 corresponds to the
embedding of a right bracket.
When all embeddings ejt are one-hot vectors, whether the expression at the current position can
be calculated or not forms a look-up table. Therefore, it can be implemented by a two-layer MLP
with hidden dimension O(p6) according to Lemma C.5. Similarly, the computed result and how
much sentence length will be reduced after this calculation can also be implemented as look-up
tables. However, some of the embeddings ejt may not be one-hot vectors when reaching an equal
sign (as discussed above). In this case, we can similarly implement extra look-up tables that take a
fewer number of inputs, and the result of which lookup table will be used depends on the position
of the equal sign. This corresponds to a multivariate conditional selection operation with multiple
Boolean conditions I[ejt · eid(‘=’) = 1] (for t ∈{1, 2, 3, 4, 5}), which can be similarly implemented
by an MLP by extending Lemma C.4. Moreover, we note that the composition of look-up tables
and the multivariate conditional selection operation can be merged in just one MLP by following the
construction in Lemmas C.4 and C.5 (we omit the details for clarity).
The final output of the third layer is represented by
x(3)
i
= (ej1, n=
i , (n=
i )2, d=
i , ˆn=
i , (ˆn=
i )2, ˆd=
i , ( ˆd=
i )2, fi, eoutcome
i
, nreduce
i
).
Here, fi is a Boolean value recording whether the next output at position i is a computed value,
eoutcome
i
is the one-hot embedding of the outcome when fi is true, and nreduce
i
records the reduced
length after calculation when fi is true. When fi is false, eoutcome
i
and nreduce
i
are undefined.
26

Layer 4. Note that in an arithmetic expression there can be multiple expressions that can be calculated
(or handles defined in Appendix B), all of which are processed in the last layer. Therefore, the fourth
layer of the Transformer should keep only the leftmost calculation and discard other calculations
(according to Appendix B). Meanwhile, for subsequent positions i after the position that has been
calculated, this layer finds the related token that should be copied for position i based on the reduced
setence length. Formally, we need two attention heads to perform the following tasks:
1. Check whether there is an index j ≤i such that n=
j = n=
i and fj is true. Denote the answer
as ˆfi = Pi
j=i−d=
i fj, where ˆfi ≥1 means the answer is yes, and ˆfi = 0 otherwise.
2. If the answer is yes ( ˆfi ≥1), copy the value nreduce
j
at the leftmost position j satisfying fj is
true and n=
j = n=
i . Denote the result as ˆnreduce
i
:= nreduce
j
. If ˆfi = 0, ˆnreduce
i
is undefined.
3. Filter the outcome: if fi is true, then maintain eoutcome
i
, otherwise set eoutcome
i
to ej1.
Similar to the construction of the third layer, we can construct the matrices Q, K and V as follows.
For the first task, we leverage the MEAN operation with
qi = Qx(3)
i
= (1, (n=
i )2, 2n=
i ),
kj = Kx(3)
j
= (−(n=
j )2, −1, n=
j ),
vj = fj.
We have qi · kj = −(n=
i −n=
j )2. Therefore, qi · kj = 0 iff n=
j = n=
i , and qi · kj ≤−1 otherwise.
This attention head thus outputs
1
di+1
Pi
j=i−d=
i fj. For the second task, we leverage the COPY
operation with
qi = Qx(3)
i
= (1, (n=
i )2, 2n=
i , 1, 1),
kj = Kx(3)
j
= (−(n=
j )2, −1, n=
j , fj, −1),
vj = nreduce
j
.
We have qi · kj = −(n=
i −n=
j )2 + fj −1. Therefore, qi · kj = 0 iff n=
j = n=
i and fj = 1, and
qi · kj ≤−1 otherwise. Moreover, we set rj = −j in the COPY operation, by which the attention
head copies nreduce
j
where j = min{j : n=
j = n=
i , fj = 1}, as desired. The output of the attention
layer has the form
 
eoutcome
i
, ej1, n=
i , (n=
i )2, ˆn=
i , (ˆn=
i )2, d=
i , ˆd=
i , ( ˆd=
i )2, fi,
ˆfi
d=
i + 1, ˆnreduce
i
!
.
We next use an MLP to perform the third task, which is a conditional selection operation and can be
done according to Lemma C.4. We can simultaneously obtain ˆfi by multiplying
ˆ
fi
d=
i +1 with (d=
i + 1).
We also compute (d=
i + ˆnreduce
i
)2, which will be used in the next layer. The final output of the fourth
layer is represented by
x(4)
i
= (˜eoutcome
i
, fi, n=
i , (n=
i )2, ˆn=
i , (ˆn=
i )2, ˆd=
i , ( ˆd=
i )2, ˆfi, ˆnreduce
i
, d=
i , (d=
i + ˆnreduce
i
)2),
where ˜eoutcome
i
is either eoutcome
i
or ej1.
Layer 5. The final layer of the Transformer uses one attention head to copy the corresponding
token for generating the output when ˆfi ≥1 and fi is false. Similar to previous layers, we can copy
the embedding eid(sj) located at position j such that ˆn=
j = n=
i −1 and ˆd=
j = d=
i + ˆnreduce
i
. The
output of the attention layer is (˜eoutcome
i
, eid(sj), fi, ˆfi). We then use an MLP to obtain the output: if
ˆfi −fi ≥1, then output eid(sj); otherwise output ˜eoutcome
i
. This corresponds to a conditional selection
operation and can be implemented by an MLP according to Lemma C.4. Finally, we pass the output
through a softmax layer to generate the next token si+1.
Now it remains to conduct an error analysis and determine the scale of parameters. Note that we
can tolerate O(1) error of the final layer output in the sense that the generated token si+1 is still
correct. Based on Lemmas C.1, C.4, C.5, C.7 and C.8, we can guarantee that when all parameters
of the Transformer are bounded by O(poly(n, 1/ϵ)), all intermediate neurons will only induce an
error below ϵ. (Also note that Assumption C.6 in Lemmas C.7 and C.8 is satisfied when ϵ is small
enough.) Therefore, by picking a fixed small ϵ = Θ(1), all parameter values in the Transformer are
bounded by O(poly(n)).
27

D.2
Proof of Theorem 3.1
We now prove that evaluating arithmetic expressions without CoT is extremely difficult for bounded-
depth autoregressive Transformers. We will make the widely-believed assumption that TC0 ̸= NC1
(see Appendix A.2 for definitions of these complexity classes). We further need the notion of
uniformity: informally, this condition says that there exists an efficient algorithm to construct the
circuits. For a rigorous definition, we refer readers to Arora & Barak [2].
We first present some lemmas on the expressive power of the TC0 circuits, based on which we will
give a reduction proof of Theorem 3.1.
Lemma D.2. For any n n-bits binary integers a1, a2, · · · , an, let f : {0, 1}n2 →{0, 1}2n be a
boolean function such that f(a1, a2, · · · , an) = P
i∈[n] ai. Then, f can be implemented by the
uniform TC0 circuits.
This lemma demonstrates that computing the sum of n n-bit integers (known as iterated addition or
simply summation) is in uniform TC0. The detailed proof of this lemma can be found in the previous
works [29, 15].
Lemma D.3. Consider any string s = s1s2 · · · sn of length n containing brackets ‘(’, ‘)’, and other
characters, and all brackets in s are paired. Let f be a boolean function taking s as input and output
n pairs of integers defined as follows:
fi(s) =



(−1, j)
if si is a left bracket and si, sj are paired.
(j, −1)
if si is a right bracket and si, sj are paired.
(j, k)
if si is not a bracket, and sj, sk is the nearest pair of matching brackets containing si.
Then f can be implemented by the TC0 circuits.
Note that the input characters and the output integers are all encoded in binary. This lemma demon-
strates that the task of bracket matching is in TC0.
Proof. Given the input string s, we first get the index i of each character si, which can be hard-coded
into the circuits. Then for each character si, we calculate the following term:
ri =
X
j<i
I[sj = ‘(’] −
X
j<i
I[sj = ‘)’] + I[si = ‘(’].
Then, the output of fi(s) can be written as the following form:
fi(s) =



(−1, min{j > i : sj = ‘)’, rj = ri})
if si = ‘(’,
(max{j < i : sj = ‘(’, rj = ri}, −1)
if si = ‘)’,
(max{j < i : sj = ‘(’, rj = ri}, min{j > i : sj = ‘)’, rj = ri})
if si is not a bracket.
It is simple to verify that the construction can be implemented by TC0 circuits.
Theorem D.4. Assume TC0 ̸= uniform NC1. For any prime number p, any integer L, and any poly-
nomial Q, there exists a problem size n such that no log-precision autoregressive Transformer defined
in Section 2 with depth L and hidden dimension d ≤Q(n) can solve the problem Arithmetic(n, p).
Proof. Our proof is based on leveraging the NC1-completeness of a classic problem: Boolean
Formula Evaluation. According to the Buss reduction [12], calculating whether a Boolean formula is
true or false is complete for uniform NC1. Based on this theorem, it suffices to prove that the Boolean
Formula Evaluation problem can be reduced to evaluating the arithmetic expression. This will yield
the conclusion by using the result that bounded-depth log-precision Transformers with polynomial
size are in TC0 [42]4 as well as the assumption that TC0 ̸= uniform NC1.
Formally, let Σ = {0, 1, ∧, ∨, ¬, (, )} be the alphabet. A Boolean formula is a string defined on
alphabet Σ by the following recursive way:
4While the authors only proved that predicting a binary label using Transformer encoder can be implemented
by TC0 circuits, it is straightforward to extend the result to autoregressive Transformers predicting a label in a
finite vocabulary.
28

• 0 and 1 are Boolean formulae;
• If φ is a Boolean formula, then (¬φ) is a Boolean formula;
• If φ1, φ2 are two Boolean formulae, then both (φ1 ∧φ2) and (φ1 ∨φ2) are Boolean formulae.
The Boolean Formula Evaluation problem aims to compute whether a Boolean formula is true (1) or
false (0). We now show that we can translate this problem into the problem of evaluating arithmetic
expressions. Given a Boolean formula s, the translation function f generates the corresponding
arithmetic expression f(s) that has the same result as s under evaluation. The translation is recursively
defined as follows:
• f(0) = 0 and f(1) = 1;
• For any Boolean formula φ, f(¬φ) = 1 −φ and f((φ)) = (f(φ));
• For any Boolean formulae φ1, φ2, f(φ1 ∧φ2) = f(φ1) × f(φ2);
• For any Boolean formulae φ1, φ2, f(φ1 ∨φ2) = 1 −(1 −f(φ1)) × (1 −f(φ2)).
It is easy to see that for any Boolean formula s, the length of f(s) is upper-bounded by O(|s|). More-
over, the translation function can be simply implemented using the circuits within TC0 complexity.
To do so, we first replace the symbols ¬, ∧, and ∨with 1−, ×, and ×, respectively. Furthermore, for
each operator ∨, we must insert ‘1 −(1−’ after the nearest left bracket containing the operator ∨,
insert a right bracket before ×, insert ‘(1−’ after ×, and insert a right bracket before the nearest right
bracket containing the operator ∨. According to Lemmas D.2 and D.3, all of these operations can be
implemented by the TC0 circuits. Therefore, this translation function can be simply implemented by
TC0 circuits. Also, note that the above construction does not depend on the modulus p. Therefore, by
reduction, we obtain that the problem of evaluating arithmetic expressions is NC1-hard.
E
System of Linear Equations
In this section, we will prove that the autoregressive Transformer equipped with CoT can solve a
system of linear equations, whereas the autoregressive Transformer without CoT cannot solve it.
E.1
Proof of Theorem 3.4
For ease of reading, we restate Theorem 3.3 below:
Theorem E.1. For any prime p and integer m > 0, there exists an autoregressive Transformer defined
in Section 2 with hidden size d = O(poly(p)) (independent of m), depth L = 4, and 5 heads in
each layer that can generate the CoT solution defined in Appendix B for all inputs in Equation(m, p).
Moreover, all parameter values in the Transformer are bounded by O(poly(m)).
Proof. The proof technique is similar to that of Theorem 3.3. We recommend readers to read the
proof Theorem 3.3 first as we will omit redundant details in the subsequent proof. Below, without
abuse of notation, we use x(l)
i
to denote the output at position i after the l-th Transformer layer, and
use xi to denote the i-th variable in linear equations. We also note that m is the upper bound on the
number of variables, and we will construct Transformer parameters such that the Transformer can
solve all linear equations with the number of variables no more than m.
Token Embeddings. Assume that we have a sequence of tokens s1, s2, . . . , st and we want to
generate the next token st+1. We can embed the token si using the format x(0)
i
= (eid(si), li, i, 1):
1. The vector ei represents the one-hot vector with the i-th element being 1, and id(si) is the
index of token si in the vocabulary. Since we hope the embedding dimension is a constant
and does not depend on the number of variable tokens m, we consider representing them
using a unified (single) encoding and distinguishing them via the term li. This means that if
si and sj are two different variables, we have id(si) = id(sj) and li ̸= lj.
2. li ∈R3 is a vector used to distinguish between different variables. Its first element, denoted
as var(si), represents the index of the variable si. If the token si is not a variable, then
29

li = (0, 0, 0) and var(si) = 0. If it is the variable xj for some j ∈[m], then var(si) = j and
li =
 j, m2 sin( 2jπ
m ), m2 cos( 2jπ
m )

.
3. i is the positional embedding, representing the position of the token in the sequence.
4. The constant embedding 1 is used as a bias term.
Layer 1. The first layer of the Transformer uses three attention heads to record some basic informa-
tion:
1. Count the number of ‘;’ (i.e., equations) in previous tokens, denoted as neq
i = |{j ≤i : sj =
‘;’}|.
2. Count the number of ‘=⇒’ in previous tokens, denoted as ncot
i
= |{j ≤i : sj = ‘ =⇒’}|.
Namely, the current position belongs to the ncot
i -th CoT step.
3. Determine the number of variables in the system of linear equations. This can be done by
copying var(sj) for index j such that sj is a variable and var(sj) is the largest. Denote the
result as nvar
i . Note that according to the input format, nvar
i
is correct whenever neq
i ≥1.
Similar to the proof of arithmetic expression, the first and the second tasks can be implemented by
two attention heads, which perform the MEAN operation to obtain the fraction of ‘;’ and ‘=⇒’ tokens
in all previous tokens. The last attention head perform the COPY operation with Si = {j : j ≤i :
sj is a variable}, rj = var(sj), and vj = var(sj). Note that while rj1 = rj2 may hold for different
positions j1, j2, their values are the same (i.e., vj1 = vj2), so the COPY operation still works and
obtains nvar
i
(when neq
i ≥1).
Then, we use MLPs in parallel to calculate neq
i
= (neq
i /i) · i and ncot
i
= (ncot
i /i) · i based on
Lemma C.1. Besides, we use an MLP to compute the auxiliary term i2 that will be used in the next
layer. Therefore, the output of the first layer is
x(1)
i
= (eid(si), li, i, i2, 1, nvar
i , neq
i , ncot
i ).
Layer 2. As described in Appendix B, each CoT step eliminates one variable, and thus at the current
position we are eliminating variable xncot
i . By the uniqueness of the solution, there must exist an
equation with a nonzero coefficient for variable xncot
i . In the second Transformer layer, we can
determine which equation satisfies this condition. More precisely, we record whether the current
equation will be used to eliminate the variable xncot
i +1 in the next CoT step ncot
i
+ 1. We also use
additional attention heads to perform some auxiliary calculations that will be used in subsequent
layers. Concretely, the second layer uses four attention heads to perform the following tasks:
1. Copy the value neq
j with position j corresponding to the nearest ‘=⇒’ token sj (j ≤i).
Clearly, the value is well-defined when ncot
i
≥1, and we define the value to be 0 if ncot
i
= 0.
2. Compute deq
i = neq
i −neq
j + 1, which corresponds to the index of the current equation in the
current CoT step.
3. Copy the embedding eid(sj) with the smallest j satisfying neq
j = neq
i and sj is a number. Note
that eid(sj) is well-defined when si = ‘=’.
4. Compute a Boolean flag (denoted as fi), which is true only when eid(sj) ̸= eid(0), deq
i > ncot
i ,
and si = ‘=’. The definition of fi means that in the ncot
i -th CoT step, we only focus on the
j-th equation when j > ncot
i
and check whether the first number in the equation is non-zero.
If it is non-zero, we set the flag to true at the specific position corresponding to token ‘=’.
5. Copy the embeddings (eid(si−1), li−1) and (eid(si−2), li−2) of the (i −1)-th and (i −2)-th
token.
The first task can be implemented by an attention head via the COPY operation to obtain neq
j when
ncot
i
≥1. For the third task, we construct the matrices Q, K, V of the COPY operation such that
qi = Qx(1)
i
= (−neq
i , 1, 1, 1),
kj = Kx(1)
j
=

1, neq
j ,
X
a∈[p]
I[sj = a], −1

,
vj = eid(sj), and rj = −j. By construction, qi · kj = (neq
j −neq
i ) + P
a∈[p] I[sj = a] −1, and thus
qi · kj = 0 only when neq
j = neq
i and sj is a number, and qi · kj ≤−1 otherwise. Furthermore, the
30

choice of rj guarantees that the leftmost position satisfies qi · kj = 0 is copied. This exactly solves
the third task. For the fifth task, we use two attention heads to perform the COPY operation. We
only give the construction of the first head that copies (eid(si−1), li−1). The matrices Q, K, V of
the COPY operation is constructed such that
qi = Qx(1)
i
= ((i −1)2, i −1, −1),
kj = Kx(1)
j
= (−1, 2j, j2),
vj = (eid(sj), lj),
and qi · kj = 0 iff j = i −1.
We next use an MLP to correct the value of neq
j when ncot
i
= 0 and compute the second task, which is
a linear operation. We also compute an auxiliary flag I[ncot
i
= deq
i ] via an MLP. Regarding the fourth
task, it is a multivariate conditional selection operation and can be similarly implemented by an MLP
by extending Lemma C.4. Note that we can compute the second task and the fourth task in parallel
using a two-layer MLP because both tasks correspond to (multivariate) conditional selection and
can be merged. We finally use multiplication to compute the auxiliary terms (ncot
i )2, (var(si))2 and
(var(si−1))2. The output of the MLP is
x(2)
i
= (eid(si), li, i, i2, 1, nvar
i , ncot
i , (ncot
i )2, deq
i , fi,
eid(si−1), li−1, eid(si−2), li−2, (var(si−1))2, (var(si))2, I[ncot
i
= deq
i ]).
Layer 3. The third layer of the Transformer uses two attention heads to perform the following tasks:
1. Copy the embedding deq
j with the smallest j satisfying fj = 1 and ncot
j
= ncot
i
−1. Denote
the answer as ˆdeq
i .
2. Determine whether the next token si+1 is a number. Denote the result as f num
i
.
3. Determine the output of the next token si+1 if si+1 is not a number. We denote its embedding
as enext
i
. Also, we need to determine the variable index var(si+1) of the next token if the next
token is a variable.
4. Determine the token si+2 if the next token si+1 is a number. There are two cases: si+2 is
a variable, and si+2 is the token ‘;’. Denote the result as enext2
i
and var(si+2) and compute
(var(si+2))2.
5. If the current token si is a variable, copy the embedding eid(sj−1) (which is a number) for
index j satisfying ncot
j
= ncot
i , ncot
j
= deq
j , and var(sj) = var(si). Denote the answer as
ecot_num
i
. When si is not a variable or deq
i ≤ncot
i , ecot_num
i
is undefined.
We can use an attention head to perform the COPY operation that completes the first task. The
construction is similar to the fourth layer in arithmetic expression and we omit it for clarity. The
second attention head performs the fifth task, which can also be done via the COPY operation.
Regarding the second task, whether the next token is a number can be purely determined by deq
i ,
ncot
i , and the current token si. Specifically, si+1 is a number if si = ‘+’, or si = ‘=’, or (si = ‘;’
and deq
i > ncot
i ). Whether the output of the next token is a variable can also be purely determined
by the previous tokens si−1, si and also deq
i and ncot
i . Specifically, si+1 is a variable if si−1 = ‘+’
and si is a number, or si−1 = ‘;’ and si is a number, or (si = ‘;’ or si = ‘ =⇒’) and deq
i ≤ncot
i .
The variable index can be determined by either var(si−2) or deq
i . When the next token is neither a
variable nor a number (i.e., the symbols ‘+’, ‘=’, ‘;’, or ‘=⇒’, we can similarly determine the token
by checking si−1, si, deq
i , and nvar
i . When the next token is a number, si+2 can be determined by
checking the variable si−1 via three cases: (i) if si−1 is a variable and var(si−1) < nvar
i , then si+2
is a variable and var(si+2) = var(si−1) + 1; (ii) if si−1 is a variable and var(si−1) = nvar
i , then
si+2 = ‘;’; (iii) otherwise, si−1 is a number, then si+2 is a variable and var(si+2) = ncot
i
+ 1.
All these tasks can implemented by MLPs that performs the conditional selection or look-up table
based on Lemmas C.4 and C.5. Moreover, the composition of conditional selection and look-
up table can be merged into a single two-layer MLP (as shown in the construction of the third
layer in arithmetic expression). We next use multiplication to compute the auxiliary terms (deq
i )2,
(deq
i + I[si+2 = ‘;’])2, and ( ˆdeq
i )2. However, to compute (var(si+2))2, we cannot use multiplication
directly as the composition of multiplication and conditional selection will require a deeper MLP.
Instead, note that (var(si+2))2 linearly depends on (var(si−1))2 and var(si−1), or linearly depends
on (ncot
i )2 and ncot
i , all of which is already computed. Therefore, we can compute (var(si+2))2
31

without multiplication. The output of this layer has the form
x(3)
i
= (eid(si), li, i, 1, nvar
i , ncot
i , (ncot
i )2, deq
i , (deq
i )2, (deq
i + I[si+2 = ‘;’])2, ˆdeq
i , ( ˆdeq
i )2, f num
i
,
enext
i
, enext2
i
, var(si), var(si+1), var(si+2), (var(si))2, (var(si+2))2, eid(si−1), ecot_num
i
).
Layer 4. The fourth layer of the Transformer performs the the core calculation of equation coefficients
when the next token is a number. There are two equations related to the calculation: the deq
i -th equation
in the last CoT step, and the ˆdeq
i -th equation in the last CoT step. There are also two variables related
to the calculation: the variable xvar(si+2) and xncot
i . Specifically, we need to copy four coefficients
a ˆdeq
i ,ncot
i , a ˆdeq
i ,var(si+2), adeq
i ,ncot
i , adeq
i ,var(si+2) defined as follows:
The ˆdeq
i -th equation:
· · · + a ˆdeq
i ,ncot
i xncot
i + · · · + a ˆdeq
i ,var(si+2)xvar(si+2) + · · · = b ˆdeq
i
The deq
i -th equation:
· · · + adeq
i ,ncot
i xncot
i + · · · + adeq
i ,var(si+2)xvar(si+2) + · · · = bdeq
i
For the case of si+2 = ‘;’, we need to copy coefficients b ˆdeq
i and bdeq
i . To unify the two cases, this
Transformer layer uses four attention heads to perform the following tasks (note that we define
var(sj) = 0 when sj is not a variable):
1. Copy the embedding eid(sj−1) for position j satisfying ncot
j
= ncot
i
−1, deq
j = ˆdeq
i , sj is a
variable, and var(sj) = ncot
i .
2. Copy the embedding eid(sj−1) for position j satisfying ncot
j
= ncot
i
−1, deq
j = ˆdeq
i + I[si+2 =
‘;’], eid(sj) = enext2
i
, and var(sj) = var(si+2).
3. Copy the embeddings eid(sj−1) and ecot_num
j
for position j satisfying ncot
j
= ncot
i −1, deq
j = deq
i ,
sj is a variable, and var(sj) = ncot
i .
4. Copy the embedding eid(sj−1) and ecot_num
j
for position j satisfying ncot
j
= ncot
i
−1, deq
j =
deq
i + I[si+2 = ‘;’], eid(sj) = enext2
i
, and var(sj) = var(si+2).
Note that for each task, there is exactly one index j satisfying the condition, and thus the copied
embeddings contain the four coefficients defined above. Then, we can use an MLP to compute the
desired output adeq
i ,var(si+2) −a ˆdeq
i ,var(si+2)/a ˆdeq
i ,ncot
i · adeq
i ,ncot
i (or bdeq
i −b ˆdeq
i /a ˆdeq
i ,ncot
i · adeq
i ,ncot
i ), which
can be implemented as a look-up table (according to Lemma C.5). However, there are several special
cases we have to consider:
• deq
i = ncot
i . In this case, the coefficient is simply computed by normalizing the ˆdeq
i -th equation,
which can also be implemented via a look-up table.
• deq
i = ˆdeq
i and ˆdeq
i ̸= ncot
i . In this case, the ˆdeq
i -th equation and the ncot
i -th equation are swapped
according to Appendix B, and the coefficient should be instead computed by ancot
i ,var(si+2) −
a ˆdeq
i ,var(si+2)/a ˆdeq
i ,ncot
i · ancot
i ,ncot
i . Fortunately, the embeddings ecot_num
j
in the third and the
fourth tasks contain exactly ancot
i ,var(si+2) and ancot
i ,ncot
i .
Overall, the coefficient can be computed by a composition of look-up tables and (multivariate)
conditional selection operations, which can be merged in a single two-layer MLP.
Now two more things remain to be done. The first is to obtain the 3-dimensional embedding li+1
when si+1 is a variable, while currently we have only obtained var(si+1). However, we cannot
compute the remaining two dimensions m2 sin( 2var(si+1)π
m
) and m2 cos( 2var(si+1)π
m
) since we do
not assume that the MLP can approximate sin and cos functions. Nevertheless, this can be done
by directly copying the embedding lj for any j such that sj is the variable xvar(si+1) by using an
attention head. Finally, the output is conditioned on the flag f num
i
: when f num
i
is true, this layer
outputs the computed coefficient embedding; otherwise, it outputs enext
i
and li+1. We denote the
output of this layer as x(4)
i
= (eout
i , lout
i ).
Linear projection and softmax layer. Finally, we pass it through a softmax layer to predict the
next token si+1. Unlike the proof of arithmetic expression, here the embedding lout
i
is not one-hot
(which contains var(si+1)), so we need to additionally prove the following result: let the output logit
corresponding to token t (before softmax) be zt(e, l) = wt · [e⊤, l⊤]⊤+ bt, where wt and bt are
32

parameters of the linear projection for logit t. Then, there exist parameters {wt, bt}t such that for
any two tokens t and ˜t with t ̸= ˜t
Gap :=zt

eid(t), var(t), m2 sin
2var(t)π
m

, m2 cos
2var(t)π
m

−
zt

eid(˜t), var(˜t), m2 sin
2var(˜t)π
m

, m2 cos
2var(˜t)π
m

≥Θ(1).
To prove the above result, simply set wt =

eid(t), var(t), m2 sin

2var(t)π
m

, m2 cos

2var(t)π
m

.
We have
Gap =1 + (var(t))2 + m4 −I[id(t) = id(˜t)] −var(t)var(˜t)
−m4

sin
2var(t)π
m

sin
2var(˜t)π
m

+ cos
2var(t)π
m

cos
2var(˜t)π
m

=(1 −I[id(t) = id(˜t)]) + var(t)(var(t) −var(˜t)) + m4

1 −cos
2(var(t) −var(˜t))π
m

When var(t) = var(˜t), we have id(t) ̸= id(˜t) and thus Gap = 1. Otherwise,
Gap ≥1 −m2 + m4 (1 −cos(2π/m))
= 1 −m2 + m4 sin2(π/m) ≥1,
where we use the fact that sin(x) ≥x/π whenever 0 < x ≤π/2.
Now it remains to conduct an error analysis and determine the scale of parameters. Similar to
the proof of arithemetic expression, we can prove that all parameter values in the Transformer are
bounded by O(poly(n)).
E.2
Proof of Theorem 3.2
We will now prove that solving a system of linear equations without CoT is extremely difficult for
bounded-depth autoregressive Transformers.
Theorem E.2. Assume TC0 ̸= NC1. For any prime number p, any integer L, and any polynomial
Q, there exists a problem size m such that no log-precision autoregressive Transformer defined in
Section 2 with depth L and hidden dimension d ≤Q(m) can solve the problem Equation(m, p).
Proof. Our proof is based on leveraging the NC1-completeness of a classic problem: Unsolvable
Automaton Membership Testing. According to Barrington’s theorem [4, 5], given a fixed unsolvable
automaton, judging whether the automaton accepts an input is complete in NC1. Below, we will
prove that solving the system of linear equations is NC1-hard by demonstrating that the Unsolvable
Automaton Membership Testing problem is NC0 reducible to the problem of solving a system of
linear equations. This will yield the conclusion since bounded-depth log-precision Transformers with
polynomial size are in TC0 [42].
Let D = (Q, Σ, δ, F, q0) be any automaton, where Q is a set of states, Σ is a set of symbols
(alphabet), δ : Q × Σ →Q is the transition function, F ⊂Q is a set of accept states, and q0
is the initial state. For any input string ω1ω2 · · · ωn, whether D accepts the string can be reduced
into solving a system of linear equations defined as follows. The system of linear equations has
(n + 1)|Q| + 1 variables, which we denote as x∗and xi,q (i ∈{0, · · · , n}, q ∈Q). The equations
are defined as follows:





x∗= P
q∈F xn,q
x0,q0 = 1
x0,q = 0
for q ∈Q\{q0}
xi,q = P
δ(r,ωi)=q xi−1,r
for 0 < i ≤n, q ∈Q
It is easy to see that xi,q = 1 iff the automaton arrives at state q when taking the substring ω1ω2 · · · ωi
as input. Therefore, x∗= 1 iff the automaton accepts the input string. Note that the above solution
does not depend on the modulus p, and the solution of these equations always exists and is unique.
33

Furthermore, the coefficient of each equation only depends on at most one input symbol. This implies
that these equations can be efficiently constructed using a highly parallelizable algorithm within a
complexity of NC0. Therefore, by reduction, we obtain that the problem of judging whether there
exists a solution such that x∗= 1 is NC1-hard.
Now consider solving linear equations using a Transformer without CoT. While the output of the
Transformer contains multiple tokens, we can arrange the order of variables such that the Transformer
has to output the value of x∗first. The parallel complexity of outputting the first token is bounded
by TC0 according to [42]. Therefore, it cannot judge whether there exists a solution satisfying
x∗= 1.
F
Discussion on Other Architectures
F.1
Encoder-Decoder Transformer
In this paper, we choose the autoregressive Transformer as it is simple and the de facto standard for
LLMs (e.g., GPT). However, all of the theoretical results of this paper can be easily transferred to an
encoder-decoder Transformer (such as T5) using the following arguments.
• Given a bounded-depth polynomial-size log-precision encoder-decoder Transformer, its
computation complexity is still bounded by TC0, so our negative results hold.
• Any finite-depth autoregressive Transformer can be mimicked by a finite-depth encoder-
decoder Transformer of roughly the same size, since causal masking for the input sequence
can be mimicked through the use of positional encoding and joint attention can be mimicked
by the integration of cross-attention and self-attention. Thus, all positive results in this paper
are not exclusive to autoregressive Transformers.
F.2
Recurrent Neural Network(RNN)
Although the theorems in this paper can be naturally extended to other popular Transformer ar-
chitectures, such as the encoder-decoder Transformer (e.g., T5), RNNs do not have this property.
Theorem F.1 shows that RNNs cannot generate the CoT sequence using the same format proposed in
our paper for the arithmetic formula task and the linear equation task unless the hidden dimension of
the RNN is at least Ω(
n
log n), where n is the length of the input sequence.
Theorem F.1. For any integer p, any log-precision RNN with constant layers can neither generate
the CoT solution for the problem Arithmetic(n, p) with hidden dimension of o(
n
log n) nor generate
the CoT solution for the problem Eqution(n, p) with the hidden dimension of o( n2
log n).
Proof. Given an RNN model, if the hidden embeddings in each layer of the last symbols of two
different input sequences are the same, the two output sequences will also be exactly the same. When
the RNN finishes processing the input sequence, the input sequence has been compressed into a
hidden state of O(D log n) bits, where D is the hidden dimension and each scalar is represented by
O(log n) bits (by definition of log-precision). Therefore, an RNN with a hidden dimension of D can
only generate at most n2D different output sequences. However, for the problem Arithmetic(n, p),
the first step of the CoT needs to output a sequence of length O(n), which contains O(n) numbers.
Therefore, there are at least O(2O(n)) different solution sequences for the problem Arithmetic(n, p).
Similarly, there are at least O(2O(n2)) different solution sequences for the problem Eqution(n, p).
Thus, by the Pigeon Hole Principle, to be able to generate all 2O(n) different output sequences, we
must have a hidden dimension of D = Ω(
n
log n) for the problem Arithmetic(n, p) and a hidden
dimension of D = Ω( n2
log n) for the problem Eqution(n, p).
Due to the great difference in the model architecture between the RNN and the transformer, most of
the theorems in this paper cannot be simply generalized to the RNN. Moreover, To the best of our
knowledge, there is very little work to investigate the chain of thought in RNN model, either from the
theoretical or the empirical aspect.
34

G
Dynamic Programming
G.1
Examples
Longest Increasing Subsequence (LIS). The LIS problem aims to compute the length of the longest
increasing subsequence given an input sequence s ∈Nn. Formally, ˜s is a subsequence of s if there
exists indices 1 ≤i1 ≤i2 ≤· · · ≤i|˜s| ≤n such that ˜sk = sik holds for all k ∈[|˜s|]. A sequence ˜s
is called increasing if ˜s1 < ˜s2 < · · · < s|˜s|. The LIS problem aims to find an increasing subsequence
of s with maximal length. A standard DP solution is to compute the length of the longest increasing
subsequence that ends at each position i, which we denote as dp(i). It is easy to write the transition
function as follows:
dp(i) = 1 +
max
j<i,sj<si dp(j).
(20)
The final answer will be maxi∈[n] dp(i).
However, the above DP transition function does not match the form of (5), since dp(i) may depend on
(an unbounded number of) all previous dp(j) (j < i). Nevertheless, this issue can be easily addressed
by using a different DP formulation. Let dp(j, k) be the longest increasing subsequence that ends at
position j and the second last position is no more than k (k < j). In this case, it is easy to write the
transition function as follows:
dp(j, k) =

1
if k = 0
max(dp(j, k −1), dp(k, k −1) · I[sj > sk] + 1)
if k > 0
(21)
The final answer will be maxi∈[n] dp(i, i −1). This DP formulation fits our framework (5).
Edit Distance (ED). The ED problem aims to find the minimum operation cost required to convert
a sequence u ∈Σn1 to another sequence v ∈Σn2. There are three types of operations: inserting
a letter into any position, deleting a letter from any position, and replacing a letter at any position
by a new one. The costs of insert, delete, and replace are a, b, and c, respectively. These operations
are sequentially executed and the total operation cost is the summation of all costs of individual
operations.
A standard DP solution is to compute the minimum operation cost to convert the substring u1u2 · · · uj
to the substring v1v2 · · · vk, which we denote as dp(j, k). It is easy to write the transition function as
follows:
dp(j, k) =



ak
if j = 0
bj
if k = 0
min(dp(j, k−1) + a, dp(j−1, k) + b, dp(j−1, k−1) + cI[s(1)
j
̸= s(2)
k ])
otherwise
(22)
The final answer will be dp(n1, n2). This DP formulation fits our framework (5).
CFG Membership Testing.
A context-free grammar (CFG) is a 4-tuple, denoted as G =
(V, Σ, R, S), where
• V is a finite set of non-terminal symbols.
• Σ is a finite set of terminal symbols, disjoint from V.
• R is a finite set of production rules, where each rule has the form A →β, with A ∈V and
β ∈(V ∪Σ)∗(the asterisk represents the Kleene star operation).
• S ∈V is the start symbol.
The non-terminal symbols in V represent (abstract) syntactic categories, while the terminal symbols
in Σ represent the actual words/tokens of the language. The production rules in R specify how a non-
terminal symbol can be replaced by sequences of terminal and non-terminal symbols concatenated
together. Below, we focus on a canonical version of CFG [33], in which the term β in each rule is
either an empty string or contains exactly two symbols:
• A →ϵ, where A ∈V and ϵ is the empty string;
• A →BC, where B, C ∈V ∪Σ.
By introducing additional non-terminal symbols to split complex rules, any CFG can be easily
translated into the canonical version expressing the same language.
35

The (universal) CFG Membership Testing problem is defined as follows: given a canonical CFG G
and a string v, judge whether v can be generated from G. This problem is known to be P-complete
[33]. The CYK algorithm [53], which is based on DP, is a classic algorithm to solve the CFG
Membership Testing problem. Here, we consider a variant of the CYK algorithm that can also handle
rules of the form A →ϵ. Denote R = {R1, · · · , Rm} and let n = |v|. The state space of the DP
process is defined as
I|V|,m,n = {(t, i, j, k, A, r) : 0 ≤t ≤|V|, 0 ≤i ≤k ≤j ≤n, A ∈V, 0 ≤r ≤m},
where dp(t, i, j, k, A, r) stores whether the substring vi+1 · · · vj can be generated by nonterminal A
by first using rule R˜r : A →BC with ˜r ≤r, and there exists index ˜k ≤k such that the substring
vi+1 · · · v˜k can be generated by B and the substring v˜k+1 · · · vj can be generated by C. The index
t represents the number of iterations (which will be detailed later). For the boundary setting when
i = j = k, dp(0, i, i, i, A, r) simply stores whether nonterminal A can generate an empty string by
first using rule R˜r with ˜r ≤r. The transition function can be formally written as
dp(t, i, j, k, A, r) =



















0
if i = j = k, t = 0, r = 0,
I [dp(t, i, j, k, A, r −1) = 1 or Rr : A →ϵ]
if i = j = k, t = 0, r ≥1,
0
if i < j, t = 0,
dp(t −1, i, j, j, A, m)
if t > 0, r = 0, k = i
dp(t, i, j, k −1, A, m)
if t > 0, r = 0, k > i,
I[dp(t, i, j, k, A, r −1) = 1 ∨(Rr : A →BC for some BC s.t.
(B ∈Σ ∧k = i + 1 ∧vk = B) ∨(B ∈V ∧dp(t, i, k, k, B, m) = 1),
(C ∈Σ ∧j = k + 1 ∧vj = C) ∨(C ∈V ∧dp(t, k, j, j, C, m) = 1))]
if t > 0, r > 0.
The final answer will be dp(|V|, 0, n, n, S, m). This DP formulation fits our framework 5.
Regarding Remark 4.6. It can be easily verified that the state spaces of the three problems mentioned
above are of polynomial size, satisfying Assumption 4.2. Additionally, the MLP with the ReLU
activation function can implement (the composition of) the following functions:
• max(a, b) and min(a, b), where a, b ∈R;
• I[a ̸= b], I[a < b], I[a > b], where a, b ∈Z;
• a × b, where a ∈R, b ∈{0, 1};
• linear transformation;
• conditional selection (Lemma C.4), for example,
f select(x) =

f >(x)
if x ≥0,
f <(x)
if x < 0,
where f > and f < are functions that can be implemented by MLPs with ReLU activation, and
x ∈Z.
This implies that the MLP with ReLU activation can approximate the functions f, g, h in the transition
function for the above three DP problems. According to Lemma C.2, these functions can be efficiently
approximated by a perceptron of constant size with GeLU activation. Similarly, the topological
ordering can also be efficiently implemented by MLPs with GeLU activation:
• LIS: (j, k) →

(j, k + 1)
if k < j −1
(j + 1, 0)
if k = j −1
• ED: (j, k) →

(j, k + 1)
if k < n2
(j + 1, 0)
if k = n2
5There is a subtle mismatch between the DP formulation and our framework (5), in that the terms B and C
depend on the input (rather than purely determined by the state). Nevertheless, it is easy to extend Theorem 4.7
to this setting. Specifically, an autoregressive Transformer can first extract B and C using one layer and then
extract dp(t, i, k, k, B, m) and dp(t, k, j, j, C, m)) using an additional layer.
36

• CFG Membership Testing:
(t, i, j, k, A, r)→













(t, i, j, k, A, r + 1)
if r < m
(t, i, j, k, next(A), 0)
if r = m, A ̸= last(V)
(t, i, j, k + 1, first(V), 0)
if r = m, A = last(V), k < j
(t, i+1, j+1, i+1, first(V), 0)
if r = m, A = last(V), k = j < n
(t+1, i+1, j+1, i+1, first(V), 0)
if r = m, A = last(V), k=j =n, t < |V|
(0, 0, j−i+1, 0, first(V), 0)
if r = m, A = last(V), k=j =n, t = |V|
where we order the set V and denote first(V) as the first element of V, last(V) as the last
element of V, and denote next(A) the successor element of A.
Therefore, Assumptions 4.3 to 4.5 are satisfied and all three problems can be solved by autoregressive
Transformers with CoT.
G.2
Proof of Theorem 4.7
In this subsection, we will give proof of the Theorem 4.7.
Theorem G.1. Consider any DP problem satisfying Assumptions 4.2 to 4.5. For any integer n ∈N,
there exists an autoregressive Transformer with constant depth L, hidden dimension d and attention
heads H (independent of n), such that the answer generated by the Transformer is correct for all input
sequences s of length no more than n. Moreover, all parameter values are bounded by O(poly(n)).
Proof. Input Format. Assume that we have a sequence of tokens s1, · · · , st and we want to generate
the next token st+1. We embed the token sk by
x(0)
k
= (einput
k
, estate
k
, edp
k , eanswer
k
, esep
k , k, 1),
where each part of the embedding is defined as follows:
1. einput
k
is the embedding of the input token sk ∈X. If the current position does not represent
an input token, then einput
k
= 0.
2. estate
k
is the embedding of the DP state in I at position k. If the current position corresponds
to an input token or the final answer, then estate
k
= 0. We also assume that for all i ∈I, the
embedding of state i is non-zero.
3. edp
k is the embedding of the DP value in Y at position k. If the current position corresponds to
an input token or the final answer, then edp
k = 0.
4. eanswer
k
is the embedding of the answer token in Z, and eanswer
k
= 0 if the current position
corresponds to an input token or an intermediate DP position.
5. esep
k
is the embedding of the separator | separating different input sequences. We set esep
k
= ej
if the current token sk is the j-th separator, where ej is the one-hot vector with the j-th
element begin 1.
6. The position embedding k indicates the index of the token in the sequence.
Block 1. The first block of the autoregressive Transformer contains several layers. It first uses N
attention heads to perform the following task:
• Copy the positional embedding of the N separators psep,1
k
, · · · , psep,N
k
∈N.
Similar to the previous proofs, this can be achieved via the COPY operation with Sk = {j ≤k :
esep
j
= et} for t ∈[N] and vj = j. Then, several MLPs follow, which perform the following tasks:
• Calculate the problem size nk = (psep,1
k
−1, psep,2
k
−psep,1
k
−1, · · · , psep,N
k
−psep,N−1
k
−1).
• Obtain the next state enext_state
k
. If the current state is already the last state, set enext_state
k
= 0.
The first task is a linear transformation, which can clearly be processed by an MLP Proposition C.3.
According to Assumption 4.4, we can use an MLP to compute the embedding of the next state
enext_state
k
based on the embedding of the current state estate
k
and the problem size n. When the required
MLP in Assumption 4.4 has multiple layers (i.e., ˜L layers), we can use ˜L −1 Transformer layers
37

to implement a ˜L-layer MLP. This can be achieved by just zero the weight matrices in the attention
layers while maintaining the input using residual connections. The output of this block is
x(1)
k
= (einput
k
, estate
k
, enext_state
k
, edp
k , esep
k , nk, k, 1).
Block 2. The second layer of the Transformer does not use attention heads. It only uses the MLP to
perform the following tasks:
• Calculate h(nk, enext_state
k
) and g(nk, enext_state
k
). We assume that the embedding of ∅is 0.
• Set the flag f state
k
representing whether current state estate
k
is the last state.
• Set the flag f answer
k
representing whether current state estate
k
is in the set A, i.e., used in the
aggregation function.
Similar to the first block, we stack several two-layer perceptrons to implement a multilayer perceptron.
According to Assumptions 4.3 and 4.5, we can use an MLP to complete the first and the last tasks.
The second task can be done by checking whether estate
k
̸= 0 and enext_state
k
= 0. We also compute the
auxiliary quantities (h(nk, enext_state
k
))2, (g(nk, enext_state
k
))2, (estate
k
)2, and k2, which are elementwise
square operations and can be implemented by an MLP (Lemma C.1). The output of this block is
x(2)
k
= (einput
k
, estate
k
, enext_state
k
, edp
k , esep
k , nk, h(nk, enext_state
k
), g(nk, enext_state
k
),
(h(nk, enext_state
k
))2, (g(nk, enext_state
k
))2, (estate
k
)2, f state
k
, f answer
k
, k, k2, 1).
Block 3. The third block of the Transformer uses K + J heads to perform the following tasks (where
K and J are defined in (5)):
• Copy the input token embeddings corresponding to sg1(i), · · · , sgJ(i) where i corresponds to
enext_state
k
. When gt(i) = ∅, we set sgt(i) to be a special token.
• Copy the DP value embeddings corresponding to dp(h1(i)), · · · , dp(hK(i)) for i corresponds
to enext_state
k
. When ht(i) = ∅, we set dp(ht(i)) to be a special value.
• Calculate the output dp(i) for i corresponds to enext_state
k
, denoted as enext_dp
k
.
The first two tasks can be done via the COPY operation. To copy DP values, the attention head
attends to positions j with estate
j
matching ht(i) for t ∈[K]. To copy input tokens, the attention head
attends to positions j = gt(i) for t ∈[J]. To handle the special token/value, it is simply a conditional
selection operation and can be handled by an MLP (Lemma C.4). According to Assumption 4.3, we
can calculate the function f (defined in (5)) using an MLP. The output of this layer is
x(3)
k
= (enext_state
k
, edp
k , enext_dp
k
, nk, f state
k
, f answer
k
, k, 1).
Block 4. The fourth block of the autoregressive transformer contains one Transformer layer. De-
pending the aggregation function, it uses one attention head for the operation max or min, or two
attention heads for the operation P. This block performs the following tasks:
• Aggregate the DP values according to the aggregation function Equation (6).
• Generate the output based on the flag f answer
k
.
For the first task, if the aggregation function is max or min, we use one attention head to simply
copy the embedding edp
j for index j such that f answer
j
= 1 and edp
j is the largest/smallest, according to
Lemma C.7. If the aggregation function is P, we use two attention heads, where one attention head
computes the mean of edp
j for index j such that f answer
j
= 1, and the other attention head calculates
the fraction of elements in the sequence such that f answer
j
= 1. Finally, the second task is a conditional
selection operation and thus can be implemented by an MLP (Lemma C.4).
G.3
Proof of the Theorem 4.8
Theorem G.2. Assume TC0 ̸= NC1. There exists a context-free language such that for any depth L
and any polynomial Q, there exists a sequence length n ∈N where no log-precision autoregressive
transformer with depth L and hidden dimension d ≤Q(n) can generate the correct answer for the
CFG Membership Testing problem for all input strings of length n.
38

Proof. According to Barrington’s theorem [4, 5], given a fixed unsolvable automaton, judging whether
the automaton accepts an input is complete in NC1, which is a special case for the CFG Membership
Testing problem. Therefore, the CFG Membership Testing problem is NC1-hard. With the assumption
that TC0 ̸= NC1, the CFG Membership Testing problem is out of the capacity of the log-precision
autoregressive transformer.
H
Experimental Details
In this section, we present the experimental details.
H.1
Datasets
We set the number field p = 11 in the math experiments. In the LIS experiment, we set the number
of different input tokens to 150; in the ED experiment, we set the number of different input tokens
to 26. The vocabulary is constructed by including all symbols. For all tasks and settings (direct v.s.
CoT), the size of the training and testing dataset is 1M and 0.1M respectively. The constructions of
different datasets are introduced below.
Arithmetic Expression.
All arithmetic expression problems are generated according to Algorithm
1. In Algorithm 1, we first create a number that serves as the answer to the problem. We then
decompose the number using sampled operators sequentially, serving as the problem, until the
maximum number of operators is met. The CoT procedure is precisely defined by reversing this
problem generation process. For example, a sample in the direct dataset looks like
1 + 5 × (1 −2) = 7
while the corresponding sample in the CoT data looks like
1 + 5 × (1 −2) = 1 + 5 × 10 = 1 + 6 = 7
Algorithm 1: Arithmetic Expression Problem Generation
Input
:Number of Operators n
Input
:Vocabulary of numbers V = {0, 1...10}// number field p = 11
Output :Arithmetic expression s
1 Sample the first number t uniformly from V ;
2 s = [];
3 Append t to s;
4 for i ←1 to n do
5
Sample p uniformly from {0, 1, ..., len(s) −1}, satisfying s[p] is a number;
6
Sample o uniformly from {+, −, ×, ÷};
7
Sample numbers t1, t2, satisfying the result of o(t1, t2) equals s[p];
8
if s[p −1] = ÷ or ( o ∈{+, −} and s[p −1] ∈{−, ×}) or (o ∈{+, −} and
s[p + 1] ∈{×, ÷}) then
9
pop s[p];
10
insert [(], [t1], [o], [t2], [)] sequentially into s[p];
11
else
12
pop s[p];
13
insert [t1], [o], [t2] sequentially into s[p];
14
end
15 end
Linear Equation.
All linear equation problems are generated according to Algorithm 2. In
Algorithm 2, we consider the linear systems that only have a unique solution. Given a sampled
linear system that satisfies this condition, we “translate” it to a sequence by concatenating all the
equations (separated by commas), which serves as the problem. The answer to the problem is also a
sequence consisting of variables and the corresponding values. The CoT solution of each problem is
39

the calculation process of the Gaussian elimination algorithm applied to each variable sequentially.
For example, a sample in the direct dataset looks like
2x1 + 3x2 + 3x3 = 8, 1x1 + 7x2 + 0x3 = 0, 0x1 + 2x2 + 1x3 = 1, [SEP] x1 = 4, x2 = 1, x3 = 10,
while the corresponding sample in the CoT dataset looks like
2x1 + 3x2 + 3x3 = 8, 1x1 + 7x2 + 0x3 = 0, 0x1 + 2x2 + 1x3 = 1,
[SEP] x1 + 7x2 + 7x3 = 4, 0x2 + 4x3 = 7, 2x2 + 1x3 = 1,
[SEP] x1 + 9x3 = 6, x2 + 6x3 = 6, 4x3 = 7,
[SEP] x1 = 4, x2 = 1, x3 = 10,
Algorithm 2: Linear Equation Data Generation
Input
:Number of Variable n
Input
:Vocabulary of numbers V = {0, 1...10}// number field p = 11
Output :Linear Equation s
1 Sample b uniformly from V n×1;
2 do
3
Sample A uniformly from V n×n;
4 while A is not invertible;
5 s ←"A11x1 + ... + A1nxn = b1, ..., An1x1 + ... + Annxn = bn"
Longest Increasing Subsequence.
All input sequences (i.e., problems) are generated according
to Algorithm 3. To make the task challenging enough, we first concatenate several increasing
subsequences of given length, and then randomly insert numbers into the whole sequence. The
inputs has 150 different tokens, ranging from 101 to 250 to avoid token overlap with DP array. The
CoT solution to the problem is the DP array plus the final answer, which is defined in (20). Here,
we consider the DP formulation (20) because the CoT output length is much shorter than the one
corresponding to formulation (21). This allows us to consider more challenging input sequences with
longer length. While this DP formulation does not precisely obey the theoretical assumption given in
Assumption 4.3, we found that the Transformer can still learn it easily.
For example, a sample in the direct dataset looks like
103 107 109 112 101 103 105 107 115 109 111 113 102 [SEP] 7
while the corresponding sample in the CoT dataset looks like
103 107 109 112 101 103 105 107 115 109 111 113 102
[SEP] 1 2 3 4 1 2 3 4 5 5 6 7 2
[SEP] 7
Edit Distance.
All input sequences (i.e., problems) are generated according to Algorithm 4. In
Algorithm 4, we generate the first string randomly. For the generation of the second string, we use
two methods. In the first method, we generate the second string randomly, corresponding to a large
edit distance. In the second method, we copy the first string with random corruption, corresponding
to a small edit distance. The two strings are concatenated by “|”, and the concatenation is used as the
model input. For the calculation of edit distance, we assign different costs to different operators. The
costs for the ADD and DELETE operators are set to 2, while the REPLACE operator is assigned a
cost of 3, since REPLACE should be more costly than ADD/DELETE while less costly than their
summation. The CoT procedure is also the DP array, defined in Section 4.1. For example, a sample
in the direct dataset looks like
a s | p a s s [SEP] 4
while the corresponding sample in the CoT dataset looks like
a s | p a s s
[SEP] 3 2 4 6
[SEP] 5 4 2 4
[SEP] 4
40

Algorithm 3: LIS Data Generation
Input
:Sequence Length n
Input
:Vocabulary of numbers V = {101, 101...250}
Output :Sequence s
1 Sample l uniformly from {3, 4...n} ;
2 Sample t uniformly from {1, 2, 3} ;
3 a = [] ;
4 push 0 to a ;
5 if t = 2 then
6
Sample j uniformly from {1, 2...⌊l/2⌋+ 1} ;
7
push j to a ;
8 else if t = 3 then
9
Sample j uniformly from {1, 2...⌊l/3⌋+ 1} ;
10
Sample k uniformly from {1, 2...⌊(l −j)/2⌋+ 1} ;
11
push j to a ;
12
push j + k to a ;
13 push l to a;
14 s ←Sample l numbers from V ;
15 for i ←1 to t do
16
Sort s[a[i −1] : a[i]];// This process makes sure the LIS of the generated
sequence s is at least ⌈l/t⌉.
17 end
18 r ←Sample n −l numbers from V ;
19 Randomly insert r into s;
Algorithm 4: ED Data Generation
Input
:Length of the First String n
Input
:Alphabet V = {a, b...z}
Output :Sequence s1, s2
1 Sample t uniformly from {3, 4...10} ;
2 T ←Sample t letters from V ;
3 s1 ←Sample n letters uniformly from T ;
4 Sample p uniformly from [0, 1] ;
5 if p < 0.4 then
6
Sample l uniformly from {n −3, n −2, ..., n + 2};
7
s2 ←Sample l letters uniformly from T ;
8 else
9
do
10
s2 ←s1 ;
11
for i ←1 to n do
12
Sample p uniformly from {0, 1...len(s2) −1};
13
Sample l uniformly from T;
14
Randomly conduct one of the followings: pop s2[p], substitute s2[p] with l, insert l
into s2[p];
15
end
16
while len(s2) not in [n −3, n + 2];
17 end
41

H.2
Model training
We use the minGPT implementation6 for all the experiments, where the detailed Transformer layer
are listed below.
X(0) = LayerNorm
 [v1 + p1, · · · , vn + pn]⊤
(23)
Attn(l)(X) =
H
X
h=1

softmax

XW (l,h)
Q
(XW (l,h)
K
)⊤/
√
d + M

XW (l,h)
V
W (l,h)
O
(24)
FFN(l)(X) = σ(XW (l)
1 )W (l)
2
(25)
Y (l−1) = X(l−1) + Attn(l)(LayerNorm(X(l−1)))
(26)
X(l) = Y (l−1) + FFN(l)(LayerNorm(Y (l−1)))
(27)
We use sinusoidal positional embedding and use Xavier initialization for all the parameters. The
activation function is chosen to be GeLU. The dimension of the embedding is set to 256, and the
number of heads is set to 4. The hidden size in the FFN layer is set to 1024.
We use the same hyperparameter configuration for all experiments, i.e., the performance comparison
between the models trained on the direct and CoT datasets of Arithmetic, Equation, LIS, and ED
tasks, and the additional length extrapolation experiments (which we use relative positional encodings
[48] instead of absolute positional encodings). In detail, we use AdamW optimizer with β1, β2= 0.9,
0.999. The learning rate is set to 1e-4, and the weight decay is set to 0.01. We set the batch size to
512 during training with a linear learning rate decay scheduler. We use learning rate warm-up, and
the warm-up stage is set to 5 epochs. The dropout rate is set to 0.1. The total number of training
epochs is set to 100.
I
Robustness in Training Data Quality
In the training of language models, the quality of data, especially intermediate steps, is crucial
for optimal performance. However, real-world training datasets may contain corrupted or missing
intermediate steps. This highlights the importance of evaluating model resilience under imperfect
conditions. To investigate this, we conducted a series of experiments focused on arithmetic tasks,
introducing varying rates of corruption and omission, encapsulated by the parameter γ. For instance,
when γ = 0.1, it denotes that 10% of the intermediate steps are omitted and 10% of the remaining
steps are corrupted through a single-token random replacement. Our motivation for introducing this
metric is to simulate potential imperfections in real-world datasets, thus assessing the robustness of
our model more thoroughly. The number of operators in the arithmetic task is set to 10.
Table 1: Experimental results of robustness.
γ
0
0.1
0.2
0.3
Accuracy
100.0%
98.5%
97.6%
95.8%
Table 2: Model performance on Arithmetic task with a 3-layer Transformers.
Our experimental results are presented in Table 1. As the rate of corruption and omission increases,
there is a slight decrease in accuracy. These results elucidate the inherent robustness of training CoT
demonstrations. It is noteworthy that the accuracy of the model remains commendably high despite
significant distortions present in the training data, highlighting its potential utility in real-world
scenarios where data quality may be compromised.
6https://github.com/karpathy/minGPT
42

