Improved Algorithms for Stochastic Linear Bandits
Using Tail Bounds for Martingale Mixtures
Hamish Flynn1,2, David Reeb1, Melih Kandemir3, Jan Peters2,4
1Bosch Center for Artificial Intelligence, Renningen, Germany
2Technische Universität Darmstadt, Germany
3University of Southern Denmark, Odense, Denmark
4Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI), Germany
hamish@robot-learning.de, david.reeb@de.bosch.com,
kandemir@imada.sdu.dk, jan.peters@tu-darmstadt.de
Abstract
We present improved algorithms with worst-case regret guarantees for the stochas-
tic linear bandit problem. The widely used “optimism in the face of uncertainty”
principle reduces a stochastic bandit problem to the construction of a confidence
sequence for the unknown reward function. The performance of the resulting bandit
algorithm depends on the size of the confidence sequence, with smaller confidence
sets yielding better empirical performance and stronger regret guarantees. In this
work, we use a novel tail bound for adaptive martingale mixtures to construct
confidence sequences which are suitable for stochastic bandits. These confidence
sequences allow for efficient action selection via convex programming. We prove
that a linear bandit algorithm based on our confidence sequences is guaranteed to
achieve competitive worst-case regret. We show that our confidence sequences are
tighter than competitors, both empirically and theoretically. Finally, we demon-
strate that our tighter confidence sequences give improved performance in several
hyperparameter tuning tasks.
1
Introduction
The stochastic linear bandit problem is a sequential decision-making problem where, in each round
t, a learner chooses an action at and then receives a stochastic reward rt for its choice of action.
The expected value of each reward is a linear function ϕ(at)⊤θ∗of a known feature vector ϕ(at)
associated with the corresponding action, while θ∗is unknown. The linear bandit problem has
attracted a great deal of attention because it is expressive enough to be a faithful model of many
real-world decision-making problems, such as news recommendation (Li et al., 2010) and dynamic
pricing (Cohen et al., 2020), yet it is simple enough to make theoretical analysis tractable.
A popular way to design algorithms for linear bandits is to follow the principle of optimism in the
face of uncertainty. This principle states that we should choose actions as if the expected reward
function is as nice as plausibly possible. For linear bandits, the principle can be instantiated with a
confidence sequence Θ0, Θ1, . . . for the parameter vector θ∗∈Θ of the expected reward function. A
confidence sequence is a sequence of subsets of the full parameter space Θ, which is built iteratively
as data becomes available and is constructed such that with high probability over the observed data,
θ∗is contained in each confidence set Θt. One can then run an upper confidence bound (UCB)
algorithm, which at round t chooses an action at+1 by maximising the UCB maxθ∈Θt

ϕ(a)⊤θ
	
with respect to a.
The popularity of UCB algorithms stems from the fact that they come with worst-case regret guaran-
tees and often perform well in practice. However, the performance of a UCB algorithm is intimately
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

4
2
0
2
4
x
6
4
2
0
2
4
6
8
y
CMM-UCB
4
2
0
2
4
x
6
4
2
0
2
4
6
8
AMM-UCB
4
2
0
2
4
x
6
4
2
0
2
4
6
8
OFUL
Figure 1: Tighter upper and lower confidence bounds via tail bounds for martingale mixtures.
The upper and lower confidence bounds of CMM-UCB (left), AMM-UCB (middle), and OFUL
(Abbasi-Yadkori et al., 2011) (right) for a test function linear in random Fourier features. The bounds
from CMM-UCB and AMM-UCB are visibly closer to the true function (dashed line) than those of
OFUL. The CMM-UCB confidence bounds are slightly tighter than the ones of AMM-UCB.
tied to the size of the confidence sets it uses. The smaller the subsets in the confidence sequence, the
better the regret bound and, perhaps more importantly, the better the algorithm performs in practice.
In this work, we develop a general-purpose tail bound for martingale mixtures, which can be used to
construct confidence sequences. When we specialise our general results to the linear bandit problem,
the maximisation problem to compute the UCB is a convex program. We maximise the UCB over
actions via gradient-based methods, and investigate two procedures for computing the UCB along
with its gradient: (a) Convex Martingale Mixture UCB (CMM-UCB): We employ a convex solver for
the UCB maximisation and calculate its gradients via differentiable convex optimisation (Agrawal
et al., 2019); (b) Analytic Martingale Mixture UCB (AMM-UCB): We exploit weak Lagrangian duality
to obtain an analytic upper bound on the UCB, whose gradient can be computed in closed-form or
via standard automatic differentiation procedures.
Fig. 1 highlights a key observation: both of our UCBs are tighter than those used in the state-of-the-art
OFUL algorithm (Abbasi-Yadkori et al., 2011) for stochastic linear bandits. We verify this claim
empirically in Sec. 8 and prove it in App. C.2. We evaluate CMM-UCB, AMM-UCB, OFUL and
various other linear bandit algorithms in several hyperparameter tuning problems (Sec. 8). We find
that our tighter UCBs result linear bandit algorithms with better performance.
2
Related Work
Algorithms with regret guarantees have been developed for several variants of the stochastic linear
bandit problem. Dani et al. (2008), Abbasi-Yadkori et al. (2009) and Rusmevichientong & Tsitsiklis
(2010) proposed algorithms for a linear bandit problem where the action set is a fixed, possibly
infinite subset of a finite-dimensional vector space. Auer (2002) and Chu et al. (2011) proposed
algorithms for linear bandit problems where the action set has finite cardinality, but may change over
time. Abbasi-Yadkori et al. (2011) proposed the OFUL algorithm for linear bandit problems with a
changing and possibly infinite action set, which is essentially the problem that we investigate. We
consider stochastic linear bandit problems where the reward function is a composition of a possibly
non-linear feature map and a linear function. This can be seen as a restricted version of the stochastic
kernelised bandit problem, where the kernel feature map is finite-dimensional. Srinivas et al. (2010);
Valko et al. (2013); Chowdhury & Gopalan (2017); Camilleri et al. (2021); Salgia et al. (2021) and Li
& Scarlett (2022) proposed algorithms with regret guarantees for various kernelised bandit problems.
In the bandit literature, confidence sets and confidence bounds constructed from online (e.g. non-i.i.d.)
observation points for unknown linear functions have been proposed by Dani et al. (2008); Rus-
mevichientong & Tsitsiklis (2010) and Abbasi-Yadkori et al. (2011). Online confidence sets/bounds
for unknown functions in separable Hilbert spaces and reproducing kernel Hilbert spaces (RKHSs)
have been proposed by Srinivas et al. (2010); Abbasi-Yadkori (2012); Kirschner & Krause (2018) and
Durand et al. (2018). Russo & Van Roy (2013) derived online confidence sets for unknown functions
belonging to arbitrary function classes.
2

We use the term “mixture of martingales”, or martingale mixture, to refer to a martingale of the
form Ev∼P [Mt(v)], where (Mt(v)|t ∈N) is a collection of martingales indexed by the variable
v ∈V. Martingale mixtures can be traced back to (Darling & Robbins, 1968; Robbins, 1970), and
have been used to construct confidence sequences since at least the work of Lai (1976). Proofs of
tail bounds for martingale mixtures typically use the method of mixtures, which was first used by
Robbins & Siegmund (1970) and was later popularised by de la Peña et al. (2004, 2009). Methods
for martingale mixtures have seen renewed interest in the sequential testing literature (Howard et al.,
2020; Kaufmann & Koolen, 2021). Examples of confidence sequences for bandits that use martingale
mixtures include the works of Abbasi-Yadkori et al. (2011); Abbasi-Yadkori (2012); Kirschner &
Krause (2018); Durand et al. (2018); Neiswanger & Ramdas (2021). Unlike in these examples,
we construct confidence sequences based on adaptive martingale mixtures (Ev∼Pt[Mt(v)]|t ∈N),
where the mixture distribution Pt can be refined as more data is acquired at each time t.
3
Problem Statement and Background
We consider a problem in which a learner plays a game over a sequence of T rounds, where T may not
be known in advance. In each round t, the learner observes an action set At and must choose an action
at ∈At. The learner then receives a reward rt = ϕ(at)⊤θ∗+ ϵt. The feature map ϕ : A →Rd is
a known function that maps actions to d-dimensional feature vectors, where A = S
t At. θ∗∈Rd
is an unknown parameter with Euclidean norm bounded by some known B > 0, i.e. ∥θ∗∥2 ≤B.
ϵ1, ϵ2, . . . , ϵT are conditionally zero-mean σ-sub-Gaussian noise variables. These assumptions on θ∗
and ϵ1, ϵ2, . . . , ϵT are standard in the linear bandit literature, see e.g. (Abbasi-Yadkori et al., 2011).
While our regret analysis applies to any action sets, our algorithms focus on the case where the action
sets At are continuous subsets of RdA.
The goal of the learner is to choose a sequence of actions that maximises the total expected re-
ward, which is equal to PT
t=1 ϕ(at)⊤θ∗after T rounds. We use cumulative regret, which is the
difference between the total expected reward of the learner and the optimal strategy, to evaluate
the learner. For a single round, we define the regret as ∆(at) = ϕ(a∗
t )⊤θ∗−ϕ(at)⊤θ∗, where
a∗
t = argmaxa∈At{ϕ(a)⊤θ∗}. After T rounds, the cumulative regret is ∆1:T = PT
t=1 ∆(at).
Confidence Sequences.
For any level δ ∈(0, 1], a (1 −δ)-confidence sequence for the parameter
vector θ∗is a sequence Θ1, Θ2, . . . of subsets of Rd, such that each Θt can be calculated using the
data a1, r1, . . . , at, rt and the sequence satisfies
Pa1,a2,...,
r1,r2,..., [∀t ≥1 : θ∗∈Θt] ≥1 −δ.
A confidence sequence Θ1, Θ2, . . . is thus a sequence of data-dependent confidence sets such that
with high probability over the random actions and rewards, θ∗∈Θt holds for all t ≥1 simultaneously.
We remark that the confidence sets Θt in this paper are random closed sets in the sense of Def. 1.1.1
of (Molchanov, 2005), which implies that the event θ ∈Θt is actually measurable for all θ ∈Rd.
4
UCB Algorithms for Linear Bandits
We describe here how to transform confidence sets for θ∗into a UCB algorithm for the linear bandit
problem. Such algorithms have appeared under various names, such as LinRel (Auer, 2002), LinUCB
(Li et al., 2010) and OFUL (Abbasi-Yadkori et al., 2011). We refer to this meta algorithm as LinUCB,
and give its pseudo-code in Algorithm 1. When run with our confidence sets, we call this algorithm
CMM-UCB resp. AMM-UCB (see Sec. 6).
In each round t, the first step is to construct a confidence set Θt from the previous observations
{(ak, rk)}t
k=1. If θ∗∈Θt with high probability, then for any action a,
UCBΘt(a) := max
θ∈Θt{ϕ(a)⊤θ}
(1)
is an upper confidence bound (UCB) on ϕ(a)⊤θ∗. Taking minθ∈Θt in (1) yields the lower confidence
bound LCBΘt(a). Once a confidence set Θt has been constructed and the next action set At+1 has
been observed, the LinUCB algorithm chooses the action
at+1 = argmax
a∈At+1
{UCBΘt(a)} ,
(2)
3

Algorithm 1: LinUCB
for t = 0, 1, 2, . . . do
Construct a confidence set Θt from {(ak, rk)}t
k=1
Observe next action set At+1
Play next action at+1 = argmaxa∈At+1{UCBΘt(a)}
Observe next reward rt+1
end
which maximises the UCB. The remaining challenge lies in the construction of the confidence sets.
5
Confidence Sequences from Martingale Mixtures
In this section, we develop a general-purpose tail bound for adaptive martingale mixtures. We then
specialise our general result to the stochastic linear bandit setting, described in Sec. 3, and construct
confidence sequences for the parameter θ∗.
5.1
General-Purpose Tail Bound for Adaptive Martingale Mixtures
We consider a general setting in which we are given a filtration (Ht|t ∈N), a sequence of adapted
random functions (Zt : R →R|t ∈N), and a sequence of predictable random variables (λt|t ∈N).
For ft ∈R, we define the conditional cumulant generating function ψt(ft, λt) as
ψt(ft, λt) := ln (E [exp(λtZt(ft))|Ht−1]) ,
where the expectation E is over Zt(ft) and λt (although λt is non-random when conditioned on
Ht−1). We use the shorthand f t := (f1, f2, . . . , ft) and λt := (λ1, λ2, . . . , λt). Let
Mt(f t, λt) = exp
 
t
X
k=1
λkZk(fk) −ψk(fk, λk)
!
.
(3)
Mt(f t, λt) is a slight generalisation of the martingale used in App. B.1 of (Russo & Van Roy,
2013). One can show that for any sequence (ft|t ∈N), (Mt(f t, λt)|t ∈N) is a martingale and
E[Mt(f t, λt)] = 1 (App. A.1). We will now construct an adaptive martingale mixture.
We call a data-dependent sequence of probability distributions (Pt|t ∈N) an adaptive sequence
of mixture distributions if: (a) Pt is a distribution over f t ∈Rt; (b) Pt is Ht−1-measurable;
(c) the distributions are consistent in the sense that their marginals coincide, i.e.
R
Pt(f t)dft =
Pt−1(f t−1) for all t. These conditions on the sequence of distributions ensure that the martingale
mixture (Ef t∼Pt[Mt(f t, λt)]|t ∈N) is in fact a martingale. In App. A.1, we verify this and
show that E[Ef t∼Pt[Mt(f t, λt)]] = 1. From here, we can use Ville’s inequality for non-negative
supermartingales (Ville, 1939) to obtain our general-purpose tail bound.
Theorem 5.1 (Tail Bound for Adaptive Martingale Mixtures). For any δ ∈(0, 1), any sequence
of predictable random variables (λt|t ∈N), and any adaptive sequence of mixture distributions
(Pt|t ∈N), the following holds with probability at least 1 −δ:
ln

E
f t∼Pt
[Mt(f t, λt)]

≤ln(1/δ)
for all t ≥1.
We provide a proof of this result in App. A.2. Note that if each ψk(fk, λk) in (3) is replaced by an
upper bound on ψk(fk, λk), the statement of the theorem still holds.
Thm. 5.1 is closely related to the general-purpose anytime PAC-Bayes bound in Thm. 3.1 of (Chugg
et al., 2023). The main difference is that our inequality holds for adaptive sequences of mixture
distributions or priors. PAC-Bayes bounds with somewhat similar adaptive sequences of mixture
distributions/priors have recently been proposed by Haddouche & Guedj (2022, 2023).
4

5.2
Confidence Sequences for Stochastic Linear Bandits
We now specialise Thm. 5.1 to the stochastic linear bandit setting. For the filtration (Ht|t ∈N), we
set Ht to be the σ-algebra generated by (a1, r1, . . . , at, rt, at+1). For reasons that will become clear,
we choose Zt(ft) = (ft −ϕ(at)⊤θ∗)ϵt. Since Z(ft) is linear in the noise variable ϵt, ψt(ft, λt) can
be upper bounded using the sub-Gaussian property of ϵt. We have
ψt(ft, λt) = ln
 E

exp
 λt(ft −ϕ(at)⊤θ∗)ϵt

|Ht−1

≤λ2
tσ2(ft −ϕ(at)⊤θ∗)2/2.
(4)
With this upper bound on ψt(ft, λt), Thm. 5.1 implies that, with probability at least 1 −δ
Ef t∼Pt
"
exp
(
t
X
k=1
λk(fk −ϕ(ak)⊤θ∗)(rk −ϕ(ak)⊤θ∗) −σ2
2 λ2
k(fk −ϕ(ak)⊤θ∗)2
)#
≤1
δ .
Since Zf(ft) is linear in ft, this integral has a closed-form solution whenever the mixture distribution
is a Gaussian Pt = N(µt, T t). Although there is a closed-form solution for any predictable sequence
(λt|t ∈N) (see App. B.1), we choose λt ≡1/σ2, which yields a relatively simple convex quadratic
constraint for θ∗. Collecting the feature vectors in Φt := [ϕ(a1), . . . , ϕ(at)]⊤∈Rt×d and writing
the reward vector rt := [r1, . . . , rt]⊤, we arrive at (see App. B.2)
∥Φtθ∗−rt∥2
2 ≤(µt −rt)⊤

1 + T t
σ2
−1
(µt −rt) + σ2 ln det

1 + T t
σ2

+ 2σ2 ln 1
δ .
(5)
This inequality has an attractive interpretation. At each step t of the bandit process, the (unknown)
ground-truth reward vector Φ⊤
t θ∗lies within a sphere around the observed reward vector rt, with
squared radius equal to the RHS of (5). One can think of the mean vector µt as a prediction of the
reward vector rt, given the previous data a1, r1, . . . , at−1, rt−1, at. The covariance matrix T t can
be thought of as the uncertainty associated with the prediction µt. If µt is a good prediction of rt,
then the quadratic “prediction error” term in (5) will be close to 0, and we can afford to choose T t
close to zero to minimise the log determinant term. In this situation, (5) can give a much tighter
constraint than the naive bound ∼tσ2, especially when σ is a pessimistic upper bound on the true
sub-Gaussian parameter. This naive bound follows from the observation that ∥Φtθ∗−rt∥2 = ∥ϵt∥2,
where ϵt = (ϵ1, . . . , ϵt). Combining the constraint in (5) with our assumption ∥θ∗∥2 ≤B yields our
confidence sequence for θ∗.
Corollary 5.2 (Martingale Mixture Confidence Sequence). For any adaptive sequence of mixture
distributions Pt = N(µt, T t), it holds with probability at least 1−δ that for all t ≥1 simultaneously,
θ∗lies in the set
Θt =

θ ∈Rd
 ∥Φtθ −rt∥2 ≤RMM,t
and ∥θ∥2 ≤B

,
(6)
where we define R2
MM,t as the right-hand-side of Eq. (5).
The boundaries of the constraints in (6) are both ellipsoids in Rd, which means that each Θt is the
intersection of (the interiors of) two ellipsoids.
6
Martingale Mixture UCB Algorithms
In this section, we describe our CMM-UCB and AMM-UCB algorithms, which are two different
implementations of LinUCB (Algorithm 1) with our confidence sequence from Corollary 5.2.
6.1
UCB Computation and Optimisation
To run the LinUCB action selection rule with our confidence sequence, we need to be able to maximise
UCBΘt(a) with respect to a. The value of the UCB at the action a is the solution of
UCBΘt(a) = max
θ∈Rd ϕ(a)⊤θ
s.t. ∥Φtθ −rt∥2 ≤RMM,t
and
∥θ∥2 ≤B.
(7)
5

This is a convex optimisation problem, which can be efficiently solved via convex programming. If
the action sets have finite cardinality, UCBΘt(a) can be maximised by solving (7) for each a ∈At
and then comparing the solutions. If the action sets are continuous subsets of RdA, then exact
maximisation of UCBΘt(a) is (in general) infeasible. For example, when the feature map ϕ is linear
in a, UCBΘt(·) is the maximum over a set of linear functions, which is a convex function of a (see
Eq. (3.7) in Sec. 3.2.3 of Boyd & Vandenberghe (2004)). Since maximisation of a convex function is
(in general) NP-hard, exact maximisation of UCBΘt(a) is also (in general) NP-hard. For this reason,
when the action sets are continuous subsets of RdA (and ϕ is differentiable), we approximately
maximise UCBΘt(a) via gradient-based local search.
6.2
Convex Martingale Mixture UCB
Our Convex Martingale Mixture UCB (CMM-UCB) algorithm is based on computing (7) using
numerical convex (conic) solvers from the CVXPY library (Diamond & Boyd, 2016; Agrawal et al.,
2018). Note that (7) is already stated in a conic form, which is favourable for conic solvers (Boyd &
Vandenberghe, 2004). Solving (7) numerically gives the tightest UCBs that can be obtained from
our confidence sequence. To compute the gradient of UCBΘt(a) with respect to the action a, we
use recently developed methods for differentiating conic programs at their optimum (Agrawal et al.,
2019), which are implemented in the cvxpylayers library.
6.3
Analytic Martingale Mixture UCB
Our Analytic Martingale Mixture UCB (AMM-UCB) algorithm uses an analytic upper bound on the
solution of (7). The resulting analytic confidence bounds are looser than the numerical confidence
bounds used by CMM-UCB, but are cheaper to evaluate and maximise. Theorem 6.1 states our upper
bound on the solution of (7).
Theorem 6.1 (Analytic UCB). For all α > 0, we have
UCBΘt(a) = max
θ∈Θt

ϕ(a)⊤θ
	
≤ϕ(a)⊤bθα,t + RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a),
(8)
where
bθα,t =
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt,
R2
AMM,t = R2
MM,t + αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt.
In App. C.1, we derive this analytic UCB by partial optimisation of the Lagrangian dual function.
Using strong duality, one can show that the analytic UCB minimised with respect to α is equal to
UCBΘt(a). Due to the closed-form expression of the analytic UCB in (8), its gradient with respect
to a can be computed using standard automatic differentiation packages.
6.4
Choosing the Mixture Distributions
Both of our algorithms require us to choose mixture distributions Pt = N(µt, T t). The mixture
distributions play a role similar to the priors used in the PAC-Bayes (Shawe-Taylor & Williamson,
1997; McAllester, 1998; Guedj, 2019; Alquier, 2021) and luckiness (Grünwald, 2007, 2023) frame-
works. Our confidence sequences and regret bounds are valid for any sequence of adaptive mixture
distributions, but (as seen in Eq. (5)) if better/worse mixture distributions are chosen, then our
confidence sequences will get smaller/bigger and our regret guarantees will get tighter/looser.
Here, we describe some sensible choices for the mixture distributions. In order for a sequence of
Gaussian mixture distributions (N(µt, T t)|t ∈N) to be a sequence of adaptive mixture distributions
(as defined in Section 5.1), we require: (a) µt and T t can only depend on a1, . . . , at and r1, . . . , rt−1;
(b) the first t −1 elements of µt must be equal to µt−1; (c) the upper left t −1 × t −1 block of T t
must be T t−1; (d) T t must be positive (semi-)definite. These conditions are all satisfied if we use a
mean vector µt and covariance matrix T t of the form
µt = [m(a1), m(a2), . . . , m(at)]⊤,
T t =


k(a1, a1)
k(a1, a2)
· · ·
k(a1, at)
k(a2, a1)
k(a2, a2)
· · ·
k(a2, at)
...
...
...
...
k(at, a1)
k(at, a2)
· · ·
k(at, at)

,
(9)
6

where m : A →R is a mean function and k : A × A →R is a positive-definite kernel function. For
the linear bandit problem, it is natural to use a linear mean function m(a) = ϕ(a)⊤θ0 and a linear
kernel function k(a, a′) = ϕ(a)⊤Σ0ϕ(a′) (where Σ0 is symmetric and positive-definite), since the
resulting mixture distribution assigns non-zero probability only to those vectors of function values
f t that could have been generated by a linear reward function. By direct computation, the Gaussian
mixture distribution with this m and k, and with µt and T t as in (9), is Pt = N(Φtθ0, ΦtΣ0Φ⊤
t ).
When θ0 = 0 and Σ0 = 1, we recover what we call the standard mixture distributions Pt =
N(0, ΦtΦ⊤
t ).
Choosing T t = ΦtΣ0Φ⊤
t has the additional benefit that it allows for cheaper computation of the
radius RMM,t. In App. F, we show that one (only) has to compute the inverse and determinant of
a d × d matrix instead of the inverse and determinant of the t × t matrix 1 + T t/σ2. Finally, we
remark that the requirement that (N(µt, T t)|t ∈N) is an adaptive sequence of mixture distributions
allows for “more adaptive” choices of µt and T t. In App E.2, we describe and investigate a method
for updating µt and T t at each round t based on previously observed actions and rewards.
7
Regret Bounds
In this section, we establish cumulative regret bounds for our CMM-UCB and AMM-UCB algorithms.
First, we state a data-dependent regret bound that illustrates how the radius of the analytic UCB from
Sec. 6.3 influences the regret of both algorithms. Then, we prove a data-independent regret bound
which illustrates the worst-case growth rate of the cumulative regret, with explicit dependence on the
feature vector dimension d and the number of rounds T. We begin by stating the assumptions (which
are standard) under which our regret bounds hold.
Assumption
7.1
(Sub-Gaussian
noise).
Let
Ht
denote
(the
σ-algebra
generated
by)
(a1, r1, . . . , at, rt, at+1). Each noise variable ϵt is conditionally zero-mean and σ-sub-Gaussian,
which means
E [ϵt|Ht−1] = 0,
and
∀λ ∈R, E [exp(λϵt)|Ht−1] ≤exp(λ2σ2/2).
Assumption 7.2 (Bounded parameter vector). For some B > 0, ∥θ∗∥2 ≤B.
Assumption 7.3 (Bounded feature vectors). For some L > 0, ∥ϕ(a)∥2 ≤L for all a ∈A.
Assumption 7.4 (Bounded expected reward). For some C > 0, ϕ(a)⊤θ∗∈[−C, C] for all a ∈A.
We remark that to run our algorithms and evaluate the data-dependent regret bound in Thm. 7.5,
we only need to know (upper bounds on) the sub-Gaussian parameter σ and the norm bound B.
Assumption 7.2 and Assumption 7.3 together imply that Assumption 7.4 must hold with C ≤LB.
We nevertheless state it as a separate assumption because this leaves open the possibility that a better
(than LB) value for C is known.
7.1
Data-Dependent Regret Bounds
Several authors (Dani et al., 2008; Abbasi-Yadkori et al., 2011; Russo & Van Roy, 2013) have shown
that the cumulative regret of a UCB algorithm can be upper bounded by the sum of the widths of
the confidence sets or confidence bounds that it uses. The width of a confidence set Θt at the action
a is the difference between the corresponding UCB and the LCB at a (i.e., maxθ∈Θt{ϕ(a)⊤θ} −
minθ∈Θt{ϕ(a)⊤θ}). In App. D.1, we show that if a1, a2, . . . , aT are the actions selected by our
CMM-UCB algorithm, then
T
X
t=1
∆(at) ≤
T
X
t=1
max
θ∈Θt−1{ϕ(at)⊤θ} −min
θ∈Θt−1{ϕ(at)⊤θ}.
(10)
This gives a data-dependent cumulative regret bound for CMM-UCB. AMM-UCB has a similar
data-dependent cumulative regret bound. In App. D.1, we show that if a1, a2, . . . , aT are the actions
selected by our AMM-UCB algorithm, then
T
X
t=1
∆(at) ≤
T
X
t=1
AUCBΘt−1(at) −ALCBΘt−1(at),
(11)
7

where AUCBΘt(a) is the right-hand-side of (8) and ALCBΘt(a) is the equivalent analytic LCB.
Since, the analytic UCB/LCB is an upper/lower bound on the numerical UCB/LCB, the bound in
Equation (11) also holds for the actions selected by CMM-UCB. By substituting in the expressions
for the analytic UCB/LCBs, we obtain the following data-dependent cumulative regret bound for
CMM-UCB and AMM-UCB.
Theorem 7.5. Suppose that assumptions 7.1-7.2 hold. For any adaptive sequence of mixture
distributions Pt = N(µt, T t), any δ ∈(0, 1), any α > 0 and all T ≥1, with probability at least
1 −δ, the cumulative regret of both CMM-UCB and AMM-UCB is bounded by
∆1:T ≤
T
X
t=1
2RAMM,t−1
q
ϕ(at)⊤ Φ⊤
t−1Φt−1 + α1
−1 ϕ(at).
A proof is given in App. D.1. This regret bound tells us that if we choose an adaptive sequence of
mixture distributions Pt = N(µt, T t), such that the radii RAMM,t are small, then we can expect to
have small cumulative regret.
7.2
Data-Independent Regret Bounds
We now state a data-independent cumulative regret bound for the special case when the adaptive
sequence of mixture distributions is Pt = N(0, cΦtΦ⊤
t ), and α = σ2/c. These mixture distributions
are scaled versions of the standard mixture distributions described in Sec. 6.4.
Theorem 7.6. Suppose that assumptions 7.1-7.4 hold. If for any c > 0, the sequence of mixture
distributions is Pt = N(0, cΦtΦ⊤
t ), then for all T ≥1, with probability at least 1−δ, the cumulative
regret of both CMM-UCB and AMM-UCB (with α = σ2/c) is bounded by
∆1:T ≤
2
√
ln 2
max
(
C, σ
s
d ln

1+ cL2T
σ2d

+ B2
c +2 ln 1
δ
) s
dT ln

1+ cL2T
σ2d

≤O(d
√
Tln(T)).
Proof sketch. Choosing Pt = N(0, cΦtΦ⊤
t ) and α = σ2/c means that the two quadratic terms in
R2
AMM,t cancel out. We then find a data-independent upper bound for the log det term. Following
Abbasi-Yadkori et al. (2011), the sum of the norms
q
ϕ(at)⊤(Φ⊤
t−1Φt−1 + α1)−1ϕ(at) is upper
bounded using an elliptical potential lemma. The result is the data-independent regret bound in the
statement of the theorem.
In App. D.2, we give a proof of this special case. In addition, we also treat a more general case
when the sequence of mixture distributions is Pt = N(Φtθ0, σ2
0ΦtΦ⊤
t ) and α is any positive number.
Using Eq. (5) with µt = Φtθ0 and T t = σ2
0ΦtΦ⊤
t , one can interpret θ0 as a guess for θ∗and σ0 as
a guess for the distance between the reward vector rt and the prediction Φtθ0.
Focusing on the dependence on d and T, this regret bound (and the more general one in App. D.2)
is at most O(d
√
Tln(T)), which matches OFUL and is minimax optimal up to the ln(T) factor. If
(upper bounds on) σ2, B, L and C are known, then we can evaluate this cumulative regret bound
before running the algorithm.
8
Experiments
In all our experiments, we set δ = 0.01. When using our analytic UCBs (Thm. 6.1), we always choose
α = σ2. Unless stated otherwise, we use the standard mixture distributions Pt = N(0, ΦtΦ⊤
t ).
8

0
200
400
600
800
1000
T
100
101
Width (log scale)
Scaling With T
CMM-UCB
AMM-UCB
OFUL
0
20
40
60
80
100
d
0
2
4
6
8
Width
Scaling With d
Figure 2: Average confidence bound width for different data set sizes T and feature dimensions d.
8.1
Upper and Lower Confidence Bounds
Compared Methods.
We evaluate the following upper/lower confidence bounds: (a) CMM-UCB:
our numerical UCBs/LCBs from Sect. 6.2; (b) AMM-UCB: our analytic UCBs/LCBs from Thm. 6.1;
(c) OFUL: the UCBs/LCBs used by the OFUL algorithm (Abbasi-Yadkori et al., 2011); (d) Bayes:
a Bayesian credible interval constructed from the Bayesian posterior for linear regression with a
Gaussian prior and likelihood (see App. E.1 for details).
Experimental Setup.
We conduct experiments on randomly generated linear functions of the form
f(x) = ϕ(x)⊤θ∗, with inputs x ∈RdX and θ∗∈Rd, the latter drawn from a standard Gaussian
distribution and if necessary scaled down to ∥θ∗∥2 ≤10 =: B. For the feature map ϕ : RdX →Rd,
we use Random Fourier Features (cf. Algorithm 1 of (Rahimi & Recht, 2007)). We investigate the
properties of upper and lower confidence bounds constructed from random data sets {(xt, yt)}T
t=1,
where yt = ϕ(xt)⊤θ∗+ ϵt, ϵt ∼N(0, σ2) and σ = 0.1.
UCB/LCB Tightness.
Fig. 1 shows the data {(xt, yt)}T
t=1 and the UCBs/LCBs of CMM-UCB,
AMM-UCB and OFUL for a randomly generated linear function with dX = 1 and d = 20.
In this example, the confidence bounds of CMM-UCB are slightly tighter than those of AMM-
UCB, which are considerably tighter than those of OFUL. Next, we investigate the tightness
of the confidence bounds for functions with higher dimensional inputs (dX = 10), a range of
data set sizes (T ∈{1, 2, 5, 10, 20, 50, 100, 200, 500, 1000}) and a range feature vector dimensions
(d ∈{1, 2, 5, 10, 20, 50, 100}). For each T and d, we sample a random feature map ϕ and weight
vector θ of appropriate size. Then, we sample random training data {(xt, yt)}T
t=1 and random test
points {x′
t}100
t=1, where xk and x′
t are drawn uniformly from the dX -dimensional unit hypercube.
Finally, we use the training data to construct confidence bounds with each method and calculate the
average width (UCB minus LCB) at the test points. Fig. 2 shows the average width of the CMM-UCB,
AMM-UCB and OFUL confidence bounds with: d = 10 and varying T (left), and T = 100 and
varying d (right). We observe the same pattern at every d and T: CMM-UCB produces the tightest
confidence bounds followed by AMM-UCB and then OFUL.
Effect of the Mixture Distributions.
Fig. 4 in App. E.1 shows the confidence bounds of CMM-
UCB and a Bayesian credible interval for different choices of the mixture distributions/prior Pt. For
a well-specified prior, either uninformative or informative, the Bayesian credible interval is slightly
tighter than the confidence bounds of CMM-UCB. For a misspecified prior, the confidence bounds of
CMM-UCB become looser whereas the Bayesian credible interval becomes wrong (not containing
the ground-truth function). Here, misspecification refers solely to Bayesian prior misspecification. In
Figs. 5 and 6 in App. E.2 we show that adaptive choices of the mixture distributions Pt can lead to
smaller confidence bounds and smaller cumulative regret in linear bandit problems.
8.2
Linear Bandits
Compared Methods.
We compare: (a) CMM-UCB: cf. Sec. 6.2; (b) AMM-UCB: cf. Sec. 6.3; (c)
OFUL: the OFUL algorithm (Abbasi-Yadkori et al., 2011), with regularisation parameter λ = α = σ2;
(d) IDS: the frequentist Information Directed Sampling (IDS) algorithm (Kirschner & Krause, 2018),
specifically the DIDS-F version; (e) Freq-TS: Thompson Sampling with posterior covariance inflation
(Agrawal & Goyal, 2013), which we call Frequentist Thompson Sampling.
9

Table 1: Average test accuracy and maximum test accuracy of our UCB algorithms and OFUL, IDS
and Freq-TS in the SVM hyperparameter tuning problems after T = 500 rounds (100 repetitions).
Raisin
Maternal
Banknotes
Mean Acc
Max Acc
Mean Acc
Max Acc
Mean Acc
Max Acc
CMM-UCB (Ours)
0.818 ± 0.018
0.893 ± 0.019
0.744 ± 0.020
0.829 ± 0.023
0.954 ± 0.005
1.000 ± 0.000
AMM-UCB (Ours)
0.800 ± 0.017
0.892 ± 0.020
0.736 ± 0.020
0.829 ± 0.023
0.948 ± 0.005
1.000 ± 0.000
OFUL
0.764 ± 0.019
0.891 ± 0.019
0.722 ± 0.019
0.827 ± 0.022
0.929 ± 0.006
1.000 ± 0.000
IDS
0.706 ± 0.048
0.891 ± 0.020
0.714 ± 0.019
0.827 ± 0.024
0.926 ± 0.007
1.000 ± 0.000
Freq-TS
0.527 ± 0.022
0.884 ± 0.019
0.616 ± 0.018
0.823 ± 0.022
0.808 ± 0.012
1.000 ± 0.000
100
200
300
400
500
t
0.5
0.6
0.7
0.8
0.9
Test Accuracy
Raisin
CMM-UCB
AMM-UCB
OFUL
IDS
Freq-TS
100
200
300
400
500
t
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
Maternal
100
200
300
400
500
t
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Banknotes
Figure 3: The smoothed per-round expected reward (test accuracy) of our UCB algorithms compared
with OFUL, IDS and Freq-TS in the SVM hyperparameter tuning experiments on three datasets.
Shown is the mean reward over 100 runs of each experiment, after Gaussian kernel smoothing.
Experimental Setup.
We investigate whether our tighter upper confidence bounds translate to
better UCB algorithms. We use each linear bandit algorithm to optimise the hyperparameters of
a kernel Support Vector Machine (SVM) for three classification data sets from the UCI Machine
Learning Repository (Dua & Graff, 2017): Raisin (Cinar et al., 2020), Maternal (Ahmed et al., 2020),
and Banknotes. The expected reward function f ∗(a) is the average test set accuracy of a kernel SVM
trained using an ARD RBF kernel with hyperparameters a = (C, γ), with C the regularisation and γ
the length-scales. The observed reward rt is the validation set accuracy at at. The feature map ϕ is a
neural network layer with 20 outputs and random weights. We choose σ = 0.05 for the sub-Gaussian
parameter and B = 10, i.e. we assume that f ∗(a) ≈ϕ(a)⊤θ∗for some ∥θ∗∥2 ≤10.
Results.
Fig. 3 compares the average test accuracy (expected reward) obtained by each bandit
algorithm for each data set and at each round t = 1, . . . , 500. Our CMM-UCB and AMM-UCB
methods outperform all other methods. From the reward curves of CMM-UCB, AMM-UCB and
OFUL, we observe that CMM-UCB outperforms AMM-UCB, which outperforms OFUL. Therefore,
we can conclude that our tighter confidence bounds (compared to OFUL’s) lead to UCB algorithms
with improved performance.
9
Conclusion
In this paper, we developed a novel tail bound for adaptive martingale mixtures and showed that it can
be used to construct tighter confidence sequences for linear bandits. We proved that our CMM-UCB
and AMM-UCB algorithms match the worst-case cumulative regret of OFUL, and we found that our
tighter confidence sequences allowed CMM-UCB and AMM-UCB to achieve greater average and
maximum reward in several hyperparameter tuning problems.
A limitation of our algorithms is that they assume a linear expected reward function, which may not
always be a realistic assumption for real-world bandit problems. Our general-purpose tail bound
in Thm. 5.1 already allows one to derive confidence sequences for non-linear reward functions by
simply choosing Zt(ft) = (ft −f ∗(at))ϵt, where f ∗is the non-linear reward function. In the case
where f ∗lies in a reproducing kernel Hilbert space, the corresponding UCB is still the solution of
a convex program. The main challenge in this setting is that the regret bound must now depend on
quantities like the effective dimension or the maximum information gain of the kernel, since the
dimension d of the feature vectors is d = ∞for interesting kernels.
We believe that further investigation into the degree to which adaptive mixture distributions can lead
to improved performance and regret bounds (see App. E.2) is another exciting topic for future work.
10

References
Abbasi-Yadkori, Y. Online learning for linearly parametrized control problems. PhD thesis, Univer-
sity of Alberta, 2012.
Abbasi-Yadkori, Y., Antos, A., and Szepesvári, C. Forced-exploration based algorithms for playing
in stochastic linear bandits. In COLT Workshop on On-line Learning with Limited Feedback,
volume 92, pp. 236, 2009.
Abbasi-Yadkori, Y., Pál, D., and Szepesvári, C. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems, 24, 2011.
Agrawal, A., Verschueren, R., Diamond, S., and Boyd, S. A rewriting system for convex optimization
problems. Journal of Control and Decision, 5(1):42–60, 2018.
Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., and Kolter, J. Z. Differentiable convex
optimization layers. Advances in neural information processing systems, 32, 2019.
Agrawal, S. and Goyal, N. Thompson sampling for contextual bandits with linear payoffs. In
International conference on machine learning, pp. 127–135. PMLR, 2013.
Ahmed, M., Kashem, M. A., Rahman, M., and Khatun, S. Review and analysis of risk factor of
maternal health in remote area using the internet of things (IoT). In InECCE2019: Proceedings
of the 5th International Conference on Electrical, Control & Computer Engineering, Kuantan,
Pahang, Malaysia, 29th July 2019, pp. 357–365. Springer, 2020.
Alquier, P. User-friendly introduction to PAC-Bayes bounds, 2021. URL https://arxiv.org/
abs/2110.11216.
Auer, P. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research, 3(Nov):397–422, 2002.
Boyd, S. and Vandenberghe, L. Convex optimization. Cambridge university press, 2004.
Camilleri, R., Jamieson, K., and Katz-Samuels, J. High-dimensional experimental design and kernel
bandits. In International Conference on Machine Learning, pp. 1227–1237. PMLR, 2021.
Chowdhury, S. R. and Gopalan, A. On kernelized multi-armed bandits. In International Conference
on Machine Learning, pp. 844–853. PMLR, 2017.
Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual bandits with linear payoff functions. In
Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp.
208–214. JMLR Workshop and Conference Proceedings, 2011.
Chugg, B., Wang, H., and Ramdas, A. A unified recipe for deriving (time-uniform) PAC-Bayes
bounds. arXiv preprint arXiv:2302.03421, 2023.
Cinar, I., Koklu, M., and Tasdemir, S. Classification of raisin grains using machine vision and
artificial intelligence methods. Gazi Mühendislik Bilimleri Dergisi, 6(3):200–209, 2020.
Cohen, M. C., Lobel, I., and Paes Leme, R. Feature-based dynamic pricing. Management Science,
66(11):4921–4943, 2020.
Dani, V., Hayes, T. P., and Kakade, S. M. Stochastic linear optimization under bandit feedback. In
COLT, pp. 355–366, 2008.
Darling, D. and Robbins, H. Some further remarks on inequalities for sample sums. Proceedings of
the National Academy of Sciences, 60(4):1175–1182, 1968.
de la Peña, V. H., Klass, M. J., and Leung Lai, T. Self-normalized processes: exponential inequalities,
moment bounds and iterated logarithm laws. Annals of Probability, 32:1902–1933, 2004.
de la Peña, V. H., Lai, T. L., and Shao, Q.-M. Self-normalized processes: Limit theory and Statistical
Applications. Springer, 2009.
11

Diamond, S. and Boyd, S. CVXPY: A Python-embedded modeling language for convex optimization.
Journal of Machine Learning Research, 17(83):1–5, 2016.
Donsker, M. D. and Varadhan, S. S. Asymptotic evaluation of certain Markov process expectations
for large time—iii. Communications on pure and applied Mathematics, 29(4):389–461, 1976.
Dua, D. and Graff, C. UCI machine learning repository, 2017. URL http://archive.ics.uci.
edu/ml.
Durand, A., Maillard, O.-A., and Pineau, J. Streaming kernel regression with provably adaptive mean,
variance, and regularization. The Journal of Machine Learning Research, 19(1):650–683, 2018.
Grünwald, P. D. The minimum description length principle. MIT press, 2007.
Grünwald, P. D. The e-posterior. Philosophical Transactions of the Royal Society A, 381(2247):
20220146, 2023.
Guedj, B. A primer on PAC-Bayesian learning. In Proceedings of the second congress of the French
Mathematical Society, volume 33, 2019. URL https://arxiv.org/abs/1901.05353.
Haddouche, M. and Guedj, B. Online PAC-Bayes learning. In Advances in Neural Information
Processing Systems, volume 35, pp. 25725–25738. Curran Associates, Inc., 2022.
Haddouche, M. and Guedj, B. PAC-Bayes generalisation bounds for heavy-tailed losses through
supermartingales. Transactions on Machine Learning Research [TMLR], 2023. ISSN 2835-8856.
doi: 10.48550/ARXIV.2210.00928. URL https://openreview.net/forum?id=qxrwt6F3sf.
Howard, S. R., Ramdas, A., McAuliffe, J., and Sekhon, J. Time-uniform Chernoff bounds via
nonnegative supermartingales. Probability Surveys, 17, 2020.
Kaufmann, E. and Koolen, W. M. Mixture martingales revisited with applications to sequential tests
and confidence intervals. The Journal of Machine Learning Research, 22(1):11140–11183, 2021.
Kirschner, J. and Krause, A. Information directed sampling and bandits with heteroscedastic noise.
In Conference On Learning Theory, pp. 358–384. PMLR, 2018.
Lai, T. L. On confidence sequences. The Annals of Statistics, pp. 265–280, 1976.
Li, L., Chu, W., Langford, J., and Schapire, R. E. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the 19th international conference on World wide web,
pp. 661–670, 2010.
Li, Z. and Scarlett, J. Gaussian process bandit optimization with few batches. In International
Conference on Artificial Intelligence and Statistics, pp. 92–107. PMLR, 2022.
McAllester, D. A. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference
on Computational learning theory, pp. 230–234, 1998.
Molchanov, I. S. Theory of random sets, volume 19. Springer, 2005.
Neiswanger, W. and Ramdas, A. Uncertainty quantification using martingales for misspecified
Gaussian processes. In Algorithmic Learning Theory, pp. 963–982. PMLR, 2021.
Petersen, K. B., Pedersen, M. S., et al. The matrix cookbook. Technical University of Denmark, 7
(15), 2008.
Rahimi, A. and Recht, B. Random features for large-scale kernel machines. Advances in neural
information processing systems, 20, 2007.
Robbins, H. Statistical methods related to the law of the iterated logarithm. The Annals of Mathemat-
ical Statistics, 41(5):1397–1409, 1970.
Robbins, H. and Siegmund, D. Boundary crossing probabilities for the Wiener process and sample
sums. The Annals of Mathematical Statistics, pp. 1410–1429, 1970.
12

Rusmevichientong, P. and Tsitsiklis, J. N. Linearly parameterized bandits. Mathematics of Operations
Research, 35(2):395–411, 2010.
Russo, D. and Van Roy, B. Eluder dimension and the sample complexity of optimistic exploration.
Advances in Neural Information Processing Systems, 26, 2013.
Salgia, S., Vakili, S., and Zhao, Q. A domain-shrinking based Bayesian optimization algorithm
with order-optimal regret performance. Advances in Neural Information Processing Systems, 34:
28836–28847, 2021.
Shawe-Taylor, J. and Williamson, R. C. A PAC analysis of a Bayesian estimator. In Proceedings of
the tenth annual conference on Computational learning theory, pp. 2–9, 1997.
Sherman, J. and Morrison, W. J. Adjustment of an inverse matrix corresponding to a change in one
element of a given matrix. The Annals of Mathematical Statistics, 21(1):124–127, 1950.
Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit
setting: No regret and experimental design. In Proc. International Conference on Machine Learning
(ICML), 2010.
Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N. Finite-Time Analysis of Kernelised
Contextual Bandits. In Uncertainty in Artificial Intelligence, 2013.
Ville, J. Etude critique de la notion de collectif. Bull. Amer. Math. Soc, 45(11):824, 1939.
13

A
Proof of the General-Purpose Tail Bound for Adaptive Martingale
Mixtures
A.1
Verifying Martingale Properties
First, we recall the definition of (Mt(f t, λt)|t ∈N) in Eq. (3). We are given a filtration (Ht|t ∈N),
a sequence of adapted random functions (Zt : R →R|t ∈N), and a sequence of predictable random
variables (λt|t ∈N).
A filtration is an increasing sequence of σ-algebras H0 ⊆H1 ⊆H2 · · · . Each σ-algebra Ht
represents the information available at time t. (Zt : R →R|t ∈N) being a sequence of adapted (to
the filtration (Ht|t ∈N)) functions means that, when conditioned on Ht, Zt is no longer random.
(λt|t ∈N) being a sequence of predictable random variables means that, when conditioned on Ht−1,
λt is no longer random.
For a sequence of real numbers (ft : t ∈N), we define
Mt(f t, λt) = exp
 
t
X
k=1
λkZk(fk) −ψk(fk, λk)
!
,
where ψt(ft, λt) is the conditional cumulant generating function
ψt(ft, λt) := ln (E [exp(λtZt(ft))|Ht−1]) .
Lemma A.1. For any sequence of real numbers (ft|t ∈N), (Mt(f t, λt)|t ∈N) is a martingale and
E[Mt(f t, λt)] = 1 for all t ∈N.
Proof. For t = 1, we have
E[M1(f 1, λ1)|H0] = E [exp(λ1Z1(f1) −ψ1(f1, λ1))|H0]
= E [exp(λ1Z1(f1))|H0] / exp(ψ1(f1, λ1))
= exp(ψ1(f1, λ1))/ exp(ψ1(f1, λ1))
= 1.
Using the tower rule of conditional expectation, we also have
E[M1(f 1, λ1)] = E[E[M1(f 1, λ1)|H0]] = 1.
Now, we verify the martingale property. For any t ≥2, we have
E [Mt(f t, λt)|Ht−1] = E
"
exp
 
t
X
k=1
λkZk(fk) −ψk(fk, λk)
! Ht−1
#
= exp
 t−1
X
k=1
λkZk(fk) −ψk(fk, λk)
!
E [exp (λtZt(ft) −ψt(ft, λt)) |Ht−1]
= exp
 t−1
X
k=1
λkZk(fk) −ψk(fk, λk)
!
= Mt−1(f t−1, λt−1).
Using the tower rule again, we have for any t ≥2
E[Mt(f t, λt)] = E[E[Mt(f t, λt)|Ht−1]] = E[Mt−1(f t−1, λt−1)].
Therefore, we have
E[Mt(f t, λt)] = E[Mt−1(f t−1, λt−1)] = · · · = E[M1(f 1, λ1)] = 1.
14

Lemma
A.2.
For
any
adaptive
sequence
of
mixture
distributions
(Pt|t
∈
N),
(Ef t∼Pt[Mt(f t, λt)]|t ∈N) is a martingale and E[Ef t∼Pt[Mt(f t, λt)]] = 1 for all t ∈N.
Proof. For any t ≥1, since Mt(f t, λt) is non-negative and Pt is Ht−1-measurable, Tonelli’s
theorem implies
E

Ef t∼Pt[Mt(f t, λt)]|Ht−1

= Ef t∼Pt [E[Mt(f t, λt)|Ht−1]] .
The requirement that the distributions P1, P2, . . . have coinciding marginals, i.e.
R
Pt(f t)dft =
Pt−1(f t−1), means that for all t ≥2
Ef t∼Pt[Mt−1(f t−1, λt−1)] = Ef t−1∼Pt−1[Mt−1(f t−1, λt−1)].
Using these two results, and the fact that (Mt(f t, λt)|t ∈N) is a martingale for any sequence
(ft|t ∈N), we now verify that the martingale mixture (Ef t∼Pt[Mt(f t, λt)]|t ∈N) is a martingale
with expected value 1. For t = 1, we have
E

Ef 1∼P1[M1(f 1, λ1)]|H0

= Ef 1∼P1 [E[M1(f 1, λ1)|H0]]
= Ef 1∼P1 [1]
= 1.
Using the tower rule as before, this also means that E

Ef 1∼P1[M1(f 1, λ1)]

= 1. For any t ≥2,
we have
E

Ef t∼Pt[Mt(f t, λt)]|Ht−1

= Ef t∼Pt [E[Mt(f t, λt)|Ht−1]]
= Ef t∼Pt

Mt−1(f t−1, λt−1)

= Ef t−1∼Pt−1

Mt−1(f t−1, λt−1)

.
Using the tower rule one more time, we have for any t ≥2
E[Ef t∼Pt[Mt(f t, λt)]] = E[E[Ef t∼Pt[Mt(f t, λt)]|Ht−1]] = E[Ef t−1∼Pt−1[Mt−1(f t−1, λt−1)]].
Therefore, we have
E[Ef t∼Pt[Mt(f t, λt)]] = E[Ef 1∼P1[M1(f 1, λ1)]] = 1.
A.2
Proof of Theorem 5.1
To prove Thm. 5.1, we use Ville’s inequality for non-negative supermartingales (Ville, 1939), which
can be thought of as a time-uniform version of Markov’s inequality. Instead of the martingale property
E[Mt|Ht−1] = Mt−1, a supermartingale satisfies E[Mt|Ht−1] ≤Mt−1, which means that any
martingale is also a supermartingale. Therefore, Ville’s inequality for non-negative supermartingales
also holds for the non-negative martingale in Lemma A.2.
Lemma A.3 (Ville’s inequality for non-negative supermartingales (Ville, 1939)). Let (Mt|t ∈N) be
a non-negative supermartingale with respect to the filtration (Ht|t ∈N), which satisfies M0 = 1.
For any δ ∈(0, 1], it holds with probability at least 1 −δ:
∀t ≥1 :
Mt ≤1/δ.
Proof of Thm. 5.1. We choose an arbitrary δ ∈(0, 1]. From Lemma A.2, for any adaptive se-
quence of mixture distributions (Pt|t ∈N), (Ef t∼Pt[Mt(f t, λt)]|t ∈N) is a martingale and
E[Ef t∼Pt[Mt(f t, λt)]] = 1. In addition, (Ef t∼Pt[Mt(f t, λt)]|t ∈N) is clearly non-negative.
Therefore, using Lemma A.3, with probability at least 1 −δ
∀t ≥1,
Ef t∼Pt[Mt(f t, λt)] ≤1/δ.
Taking the logarithm of both sides yields the statement of Thm. 5.1.
15

B
Closed-Form Gaussian Integration
Here, we calculate the integral in the inequality (see beginning of Sec. 5.2):
E
f t∼N(µt,T t)
"
exp
(
t
X
k=1
λk(fk −ϕ(ak)⊤θ∗)(rk −ϕ(ak)⊤θ∗) −σ2
2 λ2
k(fk −ϕ(ak)⊤θ∗)2
)#
≤1
δ .
(12)
First, we rearrange the integrand into a more convenient form. For every k, using rk = ϕ(ak)⊤θ∗+ϵk,
we have
(ϕ(ak)⊤θ∗−rk)2 −(fk −rk)2 = ϵ2
k −(fk −ϕ(ak)⊤θ∗−ϵk)2
= ϵ2
k −(fk −ϕ(ak)⊤θ∗)2 + 2(fk −ϕ(ak)⊤θ∗)ϵk −ϵ2
k
= 2(fk −ϕ(ak)⊤θ∗)(rk −ϕ(ak)⊤θ∗) −(fk −ϕ(ak)⊤θ∗)2.
Therefore, we have that
λk(fk −ϕ(ak)⊤θ∗)(rk −ϕ(ak)⊤θ∗) −σ2
2 λ2
k(fk −ϕ(ak)⊤θ∗)2
= λk
2 (ϕ(ak)⊤θ∗−rk)2 −λk
2 (fk −rk)2 + 1
2(λk −σ2λ2
k)(fk −ϕ(ak)⊤θ∗)2.
Equation (12) can now be re-written as
E
f t∼N(µt,T t)
"
exp
(
t
X
k=1
λk
2 (ϕ(ak)⊤θ∗−rk)2 −λk
2 (fk −rk)2 + 1
2(λk −σ2λ2
k)(fk −ϕ(ak)⊤θ∗)2
)#
≤1
δ .
(13)
In the special case where λt ≡1/σ2, we have λk −σ2λ2
k = 0, which means that 1
2(λk −σ2λ2
k)(fk −
ϕ(ak)⊤θ∗)2 disappears. In addition, (λk/2)(ϕ(ak)⊤θ∗−rk)2 does not depend on fk, so it can be
moved outside the integral.
B.1
General λt
Let Λt be the t × t diagonal matrix with diagonal elements λ1, λ2, . . . , λt. Starting from (13), taking
the logarithm of both sides, rearranging terms and then writing everything in matrix notation, we
arrive at
(Φtθ∗−rt)Λt(Φtθ∗−rt) ≤2 ln(1/δ)
(14)
−2 ln

E
f t∼N(µt,T t)

exp

−1
2(f t −rt)⊤Λt(f t −rt) + 1
2(f t −Φtθ∗)⊤ Λt −σ2Λ2
t

(f t −Φtθ∗)

The expected value inside the logarithm can be re-written as
1
p
(2π)t det(T t)
Z
exp

−1
2(f t −µt)⊤T −1
t (f t −µt) −1
2(f t −rt)⊤Λt(f t −rt)
(15)
+ 1
2(f t −Φtθ∗)⊤ Λt −σ2Λ2
t

(f t −Φtθ∗)

df t.
We will calculate the integral by “completing the square”, i.e. rewriting the exponent in the form
−1
2(f t −b)⊤A(f t −b) + c, to recover the integral of a Gaussian density function. For a symmetric
matrix A, we have
−1
2(f t −b)⊤A(f t −b) + c = −1
2f ⊤
t Af t + b⊤Af t −1
2b⊤Ab + c.
16

We also have
−1
2(f t −µt)⊤T −1
t (f t −µt) = −1
2f ⊤
t T −1
t f t + µ⊤
t T −1
t f t −1
2µ⊤
t T −1
t µt
−1
2(f t −rt)⊤Λt(f t −rt) = −1
2f ⊤
t Λtf t + r⊤
t Λtf t −1
2r⊤
t Λtrt
1
2(f t −Φtθ∗)⊤ Λt −σ2Λ2
t

(f t −Φtθ∗) = 1
2f ⊤
t
 Λt −σ2Λ2
t

f t −θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

f t
+ 1
2θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

Φtθ∗.
We now equate coefficients to find A, b and c. We find that A is
A = T −1
t
+ σ2Λ2
t.
Note that A is symmetric. We find that b is
b⊤A = µ⊤
t T −1
t
+ r⊤
t Λt −θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

=⇒Ab = T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗
=⇒b =
 T −1
t
+ σ2Λ2
t
−1  T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗
.
Finally, we find that c is
c = 1
2b⊤Ab −1
2µ⊤
t T −1
t µt −1
2r⊤
t Λtrt + 1
2θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

Φtθ∗
= 1
2
 T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗⊤ T −1
t
+ σ2Λ2
t
−1  T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗
−1
2µ⊤
t T −1
t µt −1
2r⊤
t Λtrt + 1
2θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

Φtθ∗
Now, we can rewrite and calculate the integral in (15) as
exp(c)
p
(2π)t det(T t)
Z
exp

−1
2(f t −b)⊤A(f t −b)

df t =
exp(c)
q
(2π)t det(A−1)
p
(2π)t det(T t)
= exp(c)
s
det(A−1)
det(T t)
Substituting this into (14), we obtain the constraint
(Φtθ∗−rt)⊤Λt(Φtθ∗−rt) ≤−2 ln

exp(c)
s
det(A−1)
det(T t)

+ 2 ln(1/δ)
= −2c + ln (det(AT t)) + 2 ln(1/δ)
= −
 T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗⊤ T −1
t
+ σ2Λ2
t
−1  T −1
t µt + Λtrt −
 Λt −σ2Λ2
t

Φtθ∗
+ µ⊤
t T −1
t µt + r⊤
t Λtrt −θ∗⊤Φ⊤
t
 Λt −σ2Λ2
t

Φtθ∗+ ln
 det(1 + σ2Λ2
tT t)

+ 2 ln(1/δ)
Note that θ∗appears on both the left-hand-side and right-hand-side of this inequality. However,
when Λt −σ2Λ2
t is the zero matrix (e.g. when λt ≡1/σ2), all the θ∗-dependent terms on the
right-hand-side disappear.
B.2
The Special Case λt ≡1/σ2
Starting from (13), choosing λt ≡1/σ2, taking the logarithm of both sides and then rearranging
terms, we arrive at
∥Φtθ∗−rt∥2
2 ≤−2σ2 ln

E
f t∼N(µt,T t)

exp

−1
2σ2 (f t −rt)⊤(f t −rt)

+ 2σ2 ln(1/δ).
(16)
17

For any t×t covariance matrix T , let Z(T ) denote the normalising constant of a Gaussian distribution
with covariance T , so
Z(T ) =
p
(2π)t det(T ).
For any t-dimensional vectors x and µ, and any t × t covariance matrix T , let p(x|µ, T ) denote
the density function of a Gaussian distribution with mean µ and covariance T , evaluated at x. This
means that
p(x|µ, T ) =
1
Z(T )exp

−1
2(x −µ)⊤T −1(x −µ)

.
We will use the product of Gaussians trick from (Petersen et al., 2008) (Section 8.1.8, Equation 371),
which states
p(x|µ1, Σ1)p(x|µ2, Σ2) = p(µ1|µ2, Σ1 + Σ2)p(x|µc, Σc),
(17)
where
µc =
 Σ−1
1
+ Σ−1
2
−1 (Σ−1
1 µ1 + Σ−1
2 µ2),
Σc =
 Σ−1
1
+ Σ−1
2
−1 .
We have that
E
f t∼N(µt,T t)

exp

−1
2σ2 (f t −rt)⊤(f t −rt)

=
E
f t∼N(µt,T t)

Z(σ21)p(f t|rt, σ21)

= Z(σ21)
Z
Rt p(f t|µt, T t)p(f t|rt, σ21)df t
= Z(σ21)
Z
Rt p(µt|rt, T t + σ21)p(f t|µc, Σc)df t
= Z(σ21)p(µt|rt, T t + σ21)
=
s
det(σ21)
det(T t + σ21) exp

−1
2(µt −rt)⊤(T t + σ21)−1(µt −rt)

Substituting this into (16), the constraint becomes
∥Φtθ∗−rt∥2
2 ≤σ2(µt −rt)⊤(T t + σ21)−1(µt −rt) −2σ2 ln
 s
det(σ21)
det(T t + σ21)
!
+ 2σ2 ln
1
δ

.
= (µt −rt)⊤

1 + T t
σ2
−1
(µt −rt) + σ2 ln det

1 + T t
σ2

+ 2σ2 ln
1
δ

.
C
Computing Upper Confidence Bounds
First, we state and prove some useful lemmas.
Lemma C.1. For any α > 0
(Φtθ−rt)⊤(Φtθ−rt)+αθ⊤θ−R2
MM,t−αB2 = (θ−bθα,t)⊤ Φ⊤
t Φt + α1

(θ−bθα,t)−R2
AMM,t,
where R2
MM,t is the squared radius quantity from Cor. 5.2 and
bθα,t =
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt,
R2
AMM,t = R2
MM,t + αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt.
Proof. For any symmetric matrix A, we have
(θ −b)⊤A(θ −b) + c = θ⊤Aθ −2b⊤Aθ + b⊤Ab + c.
We also have
(Φtθ−rt)⊤(Φtθ−rt)+αθ⊤θ−R2
MM,t−αB2 = θ⊤ Φ⊤
t Φt + α1

θ−2r⊤
t Φtθ+r⊤
t rt−R2
MM,t−αB2.
18

We can now find A, b and c by equating coefficients. We find that
A = Φ⊤
t Φt + α1,
which is a symmetric matrix. We have
b⊤A = r⊤
t Φt
=⇒Ab = Φ⊤
t rt
=⇒b =
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt = bθα,t.
Finally, we have
c = −R2
MM,t −αB2 + r⊤
t rt −b⊤Ab
= −R2
MM,t −αB2 + r⊤
t rt −r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt
= −R2
AMM,t.
Therefore, we have shown that
(Φtθ−rt)⊤(Φtθ−rt)+αθ⊤θ−R2
MM,t−αB2 = (θ−bθα,t)⊤ Φ⊤
t Φt + α1

(θ−bθα,t)−R2
AMM,t.
Lemma C.2. For any symmetric, positive-definite matrix A ∈Rd×d, any vectors a, b ∈Rd, any
R > 0, and any η < 0,
max
θ∈Rd

a⊤θ + η
 (θ −b)⊤A(θ −b) −R2	
= a⊤b −1
4η a⊤A−1a −ηR2.
Proof. Let
f(θ) = a⊤θ + η
 (θ −b)⊤A(θ −b) −R2
The gradient and Hessian of f are
∂
∂θ f(θ) = a + 2ηA(θ −b),
∂2
∂θ2 f(θ) = 2ηA.
Since A is positive-definite and η < 0,
∂2
∂θ2 f(θ) is negative-definite for all θ ∈Rd. Therefore, any
solution θ∗of
∂
∂θ f(θ) = 0 must be a maximiser of f(θ). There is a unique solution, which is
θ∗= b −1
2η A−1a.
The maximum is
f(θ∗) = a⊤b −1
4η a⊤A−1a −ηR2.
Lemma C.3. For any symmetric, positive-definite matrix A ∈Rd×d, any vectors a, b ∈Rd, and
any R > 0,
min
η<0

a⊤b −1
4η a⊤A−1a −ηR2

= a⊤b + R
p
a⊤A−1a.
Proof. Let
g(η) = a⊤b −1
4η a⊤A−1a −ηR2.
19

The first and second derivatives of g are
d
dη g(η) =
1
4η2 a⊤A−1a −R2,
d2
dη2 g(η) = −1
2η3 a⊤A−1a.
Since A is positive-definite,
d2
dη2 g(η) is positive for all η < 0. Therefore, any negative solution η∗of
d
dη g(η) = 0 must be a minimiser of g(η). There is a unique (negative) solution, which is
η∗= −1
2R
p
a⊤A−1a.
The minimum is
g(η∗) = a⊤b + R
p
a⊤A−1a.
C.1
Analytic UCBs
Here, we prove Theorem 6.1, which states that for all α > 0:
max
θ∈Θt

ϕ(a)⊤θ
	
≤ϕ(a)⊤bθα,t + RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a),
(18)
where
bθα,t =
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt,
R2
AMM,t = R2
MM,t + αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt.
Θt is the confidence set at time t in our confidence sequence from Cor. 5.2 and RMM,t is the radius
from Cor. 5.2 and Eq. (5). As well as proving this statement, we will also show that if Θt has an
interior point, then when the right-hand-side of (18) is optimised with respect to α > 0, the inequality
in (18) becomes an equality, i.e.
max
θ∈Θt

ϕ(a)⊤θ
	
= min
α>0

ϕ(a)⊤bθα,t + RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a)

.
(19)
Proof of Thm. 6.1. We use weak Lagrangian duality to prove the upper bound and strong Lagrangian
duality to prove the second part. The convex optimisation problem maxθ∈Θt

ϕ(a)⊤θ
	
can be stated
as
max
θ∈Rd ϕ(a)⊤θ
s.t. (Φtθ −rt)⊤(Φtθ −rt) ≤R2
MM,t
and
θ⊤θ ≤B2.
(20)
Rewriting both constraints in the form f(θ) ≤0, we can see that the Lagrangian for this problem is
L(θ, η1, η2) = ϕ(a)⊤θ + η1
 (Φtθ −rt)⊤(Φtθ −rt) −R2
MM,t

+ η2

θ⊤θ −B2
.
η1 and η2 are called the Lagrange multipliers. The Lagrange dual function (or just dual function) is
g(η1, η2) = max
θ∈Rd {L(θ, η1, η2} .
By weak duality, for any η1, η2 ≤0, the dual function is an upper bound on the solution of the primal
problem in (20), i.e. for any η1, η2 ≤0
max
θ∈Θt

ϕ(a)⊤θ
	
≤g(η1, η2).
(21)
Alternatively, (21) can be verified by starting from the inequality ϕ(a)⊤θ ≤L(θ, η1, η2) for all
θ ∈Θt, η1 ≤0, and η2 ≤0. The challenge is to set the Lagrange multipliers such that the
dual function has a closed-form expression while being as close as possible to its minimum value
minη1,η2≤0 {g(η1, η2)}. We will now show that for any α > 0, minη≤0 {g(η, αη)} has a closed-
form solution, which is the right-hand-side of (18). The Lagrangian, evaluated with the Lagrange
multipliers η and αη, is
L(θ, η, αη) = ϕ(a)⊤θ + η

(Φtθ −rt)⊤(Φtθ −rt) + αθ⊤θ −R2
MM,t −αB2
.
20

Using Lemma C.1, the Lagrangian can be rewritten as
L(θ, η, αη) = ϕ(a)⊤θ + η

(θ −bθα,t)⊤ Φ⊤
t Φt + α1

(θ −bθα,t) −R2
AMM,t

.
Using Lemma C.2, the dual function evaluated at η and αη is
g(η, αη) = max
θ∈Rd
n
ϕ(a)⊤θ + η

(θ −bθα,t)⊤ Φ⊤
t Φt + α1

(θ −bθα,t) −R2
AMM,t
o
= ϕ(a)⊤bθα,t −1
4η ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a) −ηR2
AMM,t.
Using Lemma C.3, we have
min
η≤0 {g(η, αη)} = min
η≤0

ϕ(a)⊤bθα,t −1
4η ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a) −ηR2
AMM,t

= ϕ(a)⊤bθα,t + RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a).
This concludes the proof of Theorem 6.1.
To prove (19), we use strong duality.
Clearly
minα>0 minη≤0 {g(η, αη)} = minη1,η2≤0 {g(η1, η2)}, so if we optimise the upper bound in (18)
with respect to α, then we will recover the minimum of the dual function. If strong duality holds, then
the minimum of the dual function is equal to maxθ∈Θt

ϕ(a)⊤θ
	
. Since maxθ∈Θt

ϕ(a)⊤θ
	
is a
convex optimisation problem, we can use Slater’s condition to obtain a sufficient condition for strong
duality to hold. In particular, if Θt has an interior point, then strong duality holds, which means
max
θ∈Θt{ϕ(a)⊤θ} = min
α>0 min
η≤0 {g(η, αη)}
= min
α>0

ϕ(a)⊤bθα,t + RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a)

.
One can follow the same steps, with a few minor modifications, to prove a similar statement for lower
confidence bounds. For all α > 0
min
θ∈Θt

ϕ(a)⊤θ
	
≥ϕ(a)⊤bθα,t −RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a).
If Θt has an interior point, then
min
θ∈Θt

ϕ(a)⊤θ
	
= max
α>0

ϕ(a)⊤bθα,t −RAMM,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a)

.
C.2
OFUL vs AMM-UCB (and CMM-UCB)
We will now show that for any value of the parameter α, we can choose a sequence of Gaussian
mixture distributions, such that the confidence bounds of AMM-UCB (and therefore also CMM-UCB)
are always tighter than the confidence bounds of OFUL (Abbasi-Yadkori et al., 2011).
To do this, we will use the following lemma.
Lemma C.4. For any γ > 0, v ∈Rt and M ∈Rt×d, we have
v⊤v −v⊤M

M ⊤M + γ1
−1
M ⊤v = v⊤
 1
γ MM ⊤+ 1
−1
v.
Proof. We start with the identity
M

M ⊤M + γ1

=

MM ⊤+ γ1

M.
21

By post-multiplying both sides with

M ⊤M + γ1
−1
and pre-multiplying both sides with

MM ⊤+ γ1
−1
, we obtain

MM ⊤+ γ1
−1
M = M

M ⊤M + γ1
−1
.
(22)
Now, using (22), we have
v⊤v −v⊤M

M ⊤M + γ1
−1
M ⊤v = v⊤v −v⊤
MM ⊤+ γ1
−1
MM ⊤v
= v⊤v −v⊤
MM ⊤+ γ1
−1 
MM ⊤+ γ1 −γ1

v
= v⊤v −v⊤v + γv⊤
MM ⊤+ γ1
−1
v
= v⊤
 1
γ MM ⊤+ 1
−1
v.
With v = rt and M = Φt, we obtain
r⊤
t rt −r⊤
t Φt
 Φ⊤
t Φt + γ1
−1 Φ⊤
t rt = r⊤
t
 1
γ ΦtΦ⊤
t + 1
−1
rt.
We will also use the fact that, due to the Weinstein–Aronszajn identity, for any γ > 0
det(γΦ⊤
t Φt + 1) = det(γΦtΦ⊤
t + 1).
(23)
For any α > 0 (in (Abbasi-Yadkori et al., 2011), what we call α is called λ), the OFUL UCB states
that
ϕ(a)⊤θ∗≤ϕ(a)⊤bθα,t + ROFUL,t
q
ϕ(a)⊤ Φ⊤
t Φt + α1
−1 ϕ(a),
where
ROFUL,t = σ
s
ln

det
 1
αΦ⊤
t Φt + 1

+ 2 ln(1/δ) + √αB.
For any α > 0 and any δ ∈(0, 1], this statement holds with probability at least 1 −δ for all t ≥0
and all a ∈A. By comparison, our AMM-UCB holds uniformly over all t ≥0, all a ∈A and all
α > 0 (i.e. we could optimise the AMM-UCB with respect to α in a data-dependent manner, which
would yield our CMM-UCB).
Notice that for any history a1, r1, a2, r2, . . . and any α > 0, the OFUL UCB is the same as our
AMM-UCB, except that our AMM radius quantity RAMM,t is replaced with ROFUL,t. The same
is true for the LCBs of OFUL and AMM-UCB (with the same ROFUL,t), so we will only focus on
the UCBs. We will now show that for any history a1, r1, a2, r2, . . . and any α > 0, we can chose
a sequence of Gaussian mixture distributions such that RAMM,t ≤ROFUL,t. This means that the
UCBs of our CMM-UCB and AMM-UCB algorithms are never worse than the OFUL UCB.
Without loss of generality, suppose we choose α = σ2/c, for some c > 0. With this choice, the
OFUL radius is
ROFUL,t = σ
r
ln

det
 c
σ2 Φ⊤
t Φt + 1

+ 2 ln(1/δ) + B
√c

.
For any α > 0 and a Gaussian mixture distribution Pt = N(µt, T t), the squared AMM-UCB radius
is
R2
AMM,t = R2
MM,t + αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt
= (µt −rt)⊤

1 + T t
σ2
−1
(µt −rt) + σ2 ln

det

1 + T t
σ2

+ 2σ2 ln
1
δ

+ αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt.
22

For AMM-UCB, we will use α = σ2/c and the scaled standard mixture distributions Pt =
N(0, cΦtΦ⊤
t ) for each t. With these choices, and using Lemma C.4 and (23), the squared AMM-UCB
radius is
R2
AMM,t = r⊤
t
 c
σ2 ΦtΦ⊤
t + 1
−1
rt −r⊤
t rt + r⊤
t Φt

Φ⊤
t Φt + σ2
c 1
−1
Φ⊤
t rt
(24)
+ σ2 ln

det
 c
σ2 ΦtΦ⊤
t + 1

+ 2σ2 ln
1
δ

+ σ2B2
c
= σ2

ln

det
 c
σ2 Φ⊤
t Φt + 1

+ 2 ln
1
δ

+ B2
c

.
Using the basic inequality
√
a + b ≤√a +
√
b for a, b ≥0, we have
RAMM,t = σ
s
ln

det
 c
σ2 Φ⊤
t Φt + 1

+ 2 ln
1
δ

+ B2
c
≤σ
 s
ln

det
 c
σ2 Φ⊤
t Φt + 1

+ 2 ln
1
δ

+ B
√c
!
= ROFUL,t.
Therefore, the confidence bounds of AMM-UCB, with α = σ2/c and Pt = N(0, cΦtΦ⊤
t ),
are never looser than the confidence bounds of OFUL with an arbitrary α = σ2/c.
Since
ln
 det
  c
σ2 Φ⊤
t Φt + 1

+ 2 ln
  1
δ

and B2/c are strictly positive, there is actually a strict inequality.
This means that the AMM-UCB (and CMM-UCB) confidence bounds are always strictly tighter than
the OFUL confidence bounds.
Note that Pt = N(0, cΦtΦ⊤
t ) is not necessarily the best choice for the mixture distribution. With a
better choice of the mixture distribution, e.g. a mixture distribution that is chosen using some prior
knowledge about the expected reward function and/or refined using previously observed rewards,
RAMM,t will be smaller and the gap between AMM-UCB and OFUL will be greater.
D
Cumulative Regret Bounds
In this section, we prove the cumulative regret bounds stated in Section 7. We prove the data-
dependent regret bound in Thm. 7.5. We also prove the data-independent regret bound in Thm. 7.6
and another data-independent regret bound, which holds for more general choices of the mixture
distributions and the α parameter.
For convenience, we use some more compact notation in this section. For a symmetric positive
semi-definite matrix A and vector x, let
∥x∥A :=
√
x⊤Ax.
Before presenting the proof of the main results, we state some useful lemmas.
Lemma D.1 (Donsker-Varadhan Change of Measure (Donsker & Varadhan, 1976)). For any set
X, any measurable function h : X →R and any probability distribution P ∈P(X) (i.e. any
distribution on X), such that Ex∼P [eh(x)] < ∞, we have
sup
Q∈P(X)

E
x∼Q [h(x)] −DKL(Q||P)

= ln

E
x∼P
h
eh(x)i
.
(25)
By rearranging (25), we have
inf
Q∈P(X)

E
x∼Q [h(x)] + DKL(Q||P)

= −ln

E
x∼P
h
e−h(x)i
.
(26)
23

Lemma D.2 (Determinant-Trace Inequality (Abbasi-Yadkori et al., 2011)). If assumption 7.3 holds
(i.e. ∥ϕ(a)∥2 ≤L), then for any γ > 0
ln
 det
 γΦ⊤
t Φt + 1

≤d ln
 1 + γtL2/d

.
(27)
The Determinant-Trace Inequality in Lemma 10 of (Abbasi-Yadkori et al., 2011) is stated in the form
det

Φ⊤
t Φt + 1
γ 1

≤(1/γ + tL2/d)d.
(28)
Since det
 γΦ⊤
t Φt + 1

= det
 Φ⊤
t Φt + (1/γ)1

/ det ((1/γ)1), the statement in (27) follows
from (28).
Lemma D.3. For any σ > 0 and any σ0 > 0, define Σt = ( 1
σ2 Φ⊤
t Φt +
1
σ2
0 1)−1. We have
tr(Φ⊤
t ΦtΣt) = σ2d −σ2
σ2
0
tr(Σt) ≤σ2d.
Proof. Since Σt is positive-definite, its trace is positive. Now
tr(Φ⊤
t ΦtΣt) = σ2tr
 
1
σ2 Φ⊤
t Φt
 1
σ2 Φ⊤
t Φt + 1
σ2
0
1
−1!
= σ2tr
  1
σ2 Φ⊤
t Φt + 1
σ2
0
1
  1
σ2 Φ⊤
t Φt + 1
σ2
0
1
−1
−1
σ2
0
 1
σ2 Φ⊤
t Φt + 1
σ2
0
1
−1!
= σ2d −σ2
σ2
0
tr(Σt)
≤σ2d.
Lemma D.4. For any σ > 0 and any σ0 > 0, the matrix Σt = ( 1
σ2 Φ⊤
t Φt +
1
σ2
0 1)−1 satisfies
tr(Σt) ≤d
σ2
0
.
Proof. Let {γi}d
i=1 denote the eigenvalues of Φ⊤
t Φt. Since Φ⊤
t Φt is positive semi-definite, its
eigenvalues are real and non-negative. From the definition of eigenvalues, one can verify that the
eigenvalues of Σt are {
σ2
γi+σ2/σ2
0 }d
i=1. Using this, we have
tr(Σt) =
d
X
i=1
σ2
γi + σ2/σ2
0
≤
d
X
i=1
σ2
σ2/σ2
0
= d
σ2
0
.
Lemma D.5. Let ϵt denote the vector containing the first t noise variables (so rt = Φtθ∗+ ϵt). For
any α > 0, we have
(Φtθ∗−rt)⊤(Φtθ∗−rt) −r⊤
t rt + r⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t rt ≤
Φ⊤
t ϵt
2
(Φ⊤
t Φt+α1)−1
+ 2α ∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1 .
24

Proof. Using rt = Φtθ∗+ ϵt, we have
(Φtθ∗−rt)⊤(Φtθ∗−rt) = ϵ⊤
t ϵt,
and
−r⊤
t rt + r⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t rt = −(Φtθ∗+ ϵt)⊤(Φtθ∗+ ϵt)
+ (Φtθ∗+ ϵt)⊤Φt(Φ⊤
t Φt + α1)−1Φ⊤
t (Φtθ∗+ ϵt)
= −ϵ⊤
t ϵt −2θ∗⊤Φ⊤
t ϵt −θ∗⊤Φ⊤
t Φtθ∗+ θ∗⊤Φ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t Φtθ∗
+ 2θ∗⊤Φ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt + ϵ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt
≤−ϵ⊤
t ϵt −2θ∗⊤Φ⊤
t ϵt + 2θ∗⊤Φ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt
+ ϵ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt
= −ϵ⊤
t ϵt −2αθ∗⊤(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt + ϵ⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt.
Using the Cauchy-Schwarz inequality, we have
|θ∗⊤(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt| ≤∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1 .
Therefore
−2αθ∗⊤(Φ⊤
t Φt + α1)−1Φ⊤
t ϵt ≤2α ∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1 ,
and
(Φtθ∗−rt)⊤(Φtθ∗−rt) −r⊤
t rt + r⊤
t Φt(Φ⊤
t Φt + α1)−1Φ⊤
t rt ≤ϵ⊤
t ϵt −ϵ⊤
t ϵt +
Φ⊤
t ϵt
2
(Φ⊤
t Φt+α1)−1
+ 2α ∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1
=
Φ⊤
t ϵt
2
(Φ⊤
t Φt+α1)−1 + 2α ∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1 .
Theorem D.6 (Self-Normalised Bound for Vector-Valued Martingales (Theorem 1 of (Abbasi-Yadkori
et al., 2011))). Let (Ht|t ≥0) be a filtration. Let (ϵt|t ≥1) be a real-valued stochastic process such
that ϵt is Ht-measurable and ϵt is conditionally σ-sub-Gaussian for some σ > 0. Let (ϕ(at)|t ≥1)
be an Rd-valued stochastic process such that ϕ(at) is Ht−1-measurable. For any δ ∈(0, 1] and any
α > 0, with probability at least 1 −δ
∀t ≥0,
Φ⊤
t ϵt
2
(Φ⊤
t Φt+α1)−1 ≤σ2 ln

det
 1
αΦ⊤
t Φt + 1

+ 2σ2 ln(1/δ).
Lemma D.7. For any symmetric positive semi-definite matrix A with largest eigenvalue γmax, we
have
∥x∥2
A ≤γmax ∥x∥2
2 .
Proof. Let {γi}d
i=1 and {vi}d
i=1 be the eigenvalues and eigenvectors of A. Since {vi}d
i=1 form a
basis, there are constants {ci}d
i=1 such that x = Pd
i=1 civi. We have
∥x∥2
A =
d
X
i=1,j=1
cicjv⊤
i Avj =
d
X
i=1,j=1
γjcicjv⊤
i vj ≤γmax
d
X
i=1,j=1
cicjv⊤
i vj = γmax ∥x∥2
2 .
Lemma D.8. For all x ≥0,
min(1, x) ≤
1
ln(2) ln(1 + x).
25

Proof. Since ln(1 + x)/ ln(2) is monotonically increasing in x, we only need to prove that x ≤
ln(1 + x)/ ln(2) for all x ∈[0, 1]. For any positive constant a, the function a ln(1 + x) is concave
on the domain [0, 1]. Therefore, if x ≤a ln(1 + x) at the end points x = 0 and x = 1, then
x ≤a ln(1 + x) for every x ∈[0, 1]. At x = 0, we have a ln(1 + x) = 0 for any a, which means
we can choose the smallest a such that 1 ≤a ln(1 + 1). By rearranging this inequality, we obtain
a ≥1/ ln(2).
D.1
Data-Dependent Regret Bound
First, we show that the cumulative regret of both of our algorithms can be upper bounded by the sum
of the widths of the UCB/LCBs that they use. Let
UCBΘt(a) = max
θ∈Θt

ϕ(a)⊤θ
	
,
and
LCBΘt(a) = min
θ∈Θt

ϕ(a)⊤θ
	
.
In words, UCBΘt(a) and LCBΘt(a) are the upper and lower confidence bounds used by CMM-UCB
(evaluated at a). Similarly, let
AUCBΘt(a) = ϕ(a)⊤bθα,t + RAMM,t ∥ϕ(a)∥(Φ⊤
t Φt+α1)−1 ,
ALCBΘt(a) = ϕ(a)⊤bθα,t −RAMM,t ∥ϕ(a)∥(Φ⊤
t Φt+α1)−1 .
AUCBΘt(a) and ALCBΘt(a) are the analytic upper and lower confidence bounds used by AMM-
UCB. Lemma D.9 shows that the cumulative regret of CMM-UCB and AMM-UCB can be upper
bounded by the sum of the widths (UCB minus LCB) of the confidence bounds that they use.
Lemma D.9. Suppose the actions a1, a2, . . . are selected by the CMM-UCB algorithm. For any
adaptive sequence of mixture distributions Pt = N(µt, T t) and any δ ∈(0, 1], with probability at
least 1 −δ
∀T ≥1,
T
X
t=1
∆(at) ≤
T
X
t=1
UCBΘt−1(at) −LCBΘt−1(at).
(29)
Suppose the actions a1, a2, . . . are selected by the AMM-UCB algorithm. For any adaptive sequence
of mixture distributions Pt = N(µt, T t) and any δ ∈(0, 1], with probability at least 1 −δ
∀α > 0, T ≥1,
T
X
t=1
∆(at) ≤
T
X
t=1
AUCBΘt−1(at) −ALCBΘt−1(at).
(30)
Proof. Using Cor. 5.2 (i.e. the fact that Θ1, Θ2, . . . is a confidence sequence), for any adaptive
sequence of mixture distributions Pt = N(µt, T t) and any δ ∈(0, 1], with probability at least 1 −δ
∀a ∈A, t ≥1,
LCBΘt−1(a) ≤ϕ(a)⊤θ∗≤UCBΘt−1(a).
Using Thm. 6.1, this implies
∀α > 0, a ∈A, t ≥1,
ALCBΘt−1(a) ≤ϕ(a)⊤θ∗≤AUCBΘt−1(a).
Let a1, a2, . . . be the actions selected by CMM-UCB, i.e. at = argmaxa∈At

UCBΘt−1(a)
	
. Then,
with probability at least 1 −δ, we have
T
X
t=1
∆(at) =
T
X
t=1
ϕ(a∗
t )⊤θ∗−ϕ(at)⊤θ∗
≤
T
X
t=1
UCBΘt−1(a∗
t ) −LCBΘt−1(at)
≤
T
X
t=1
UCBΘt−1(at) −LCBΘt−1(at).
26

Now, let a1, a2, . . . be the actions selected by AMM-UCB, i.e. at = argmaxa∈At

AUCBΘt−1(a)
	
.
Then, with probability at least 1 −δ, we have
T
X
t=1
∆(at) =
T
X
t=1
ϕ(a∗
t )⊤θ∗−ϕ(at)⊤θ∗
≤
T
X
t=1
AUCBΘt−1(a∗
t ) −ALCBΘt−1(at)
≤
T
X
t=1
AUCBΘt−1(at) −ALCBΘt−1(at).
Since ∀α > 0, a ∈A and t ≥1, AUCBΘt−1(a) ≥UCBΘt−1(a) and ALCBΘt−1(a) ≤
LCBΘt−1(a), (29) implies that (30) also holds when a1, a2, . . . are the actions selected by CMM-
UCB.
Proof of Theorem 7.5. We start by using Lemma D.9. Suppose a1, a2, . . . are the actions selected by
CMM-UCB or AMM-UCB. For any adaptive sequence of mixture distributions Pt = N(µt, T t) and
any δ ∈(0, 1], with probability at least 1 −δ
∀α > 0, T ≥1,
T
X
t=1
∆(at) ≤
T
X
t=1
AUCBΘt−1(at) −ALCBΘt−1(at).
Using the definitions of AUCBΘt−1(at) and ALCBΘt−1(at), we have
∀α > 0, T ≥1,
T
X
t=1
∆(at) ≤
T
X
t=1
2RAMM,t−1 ∥ϕ(at)∥(Φ⊤
t−1Φt−1+α1)−1 .
D.2
Data-Independent Regret Bound
To establish data-independent regret bounds, we first prove data-independent upper bounds on the
radius RAMM,t and the norms ∥ϕ(at)∥(Φ⊤
t−1Φt−1+α1)−1. Then, we take the data-dependent regret
bound in Lemma D.9 and substitute in these bounds on the radius and the norms.
D.2.1
Bounding the Radius
Lemma D.10. If, for any c > 0, the sequence of mixture distributions is Pt = N(0, cΦtΦ⊤
t ) and
α = σ2/c, then
R2
AMM,t ≤σ2d ln

1 + ctL2
σ2d

+ σ2B2
c
+ 2σ2ln(1/δ).
(31)
Proof. In Equation (24), we already saw that for this choice of α and the mixture distributions, we
have
R2
AMM,t = σ2 ln

det
 c
σ2 Φ⊤
t Φt + 1

+ σ2B2
c
+ 2σ2ln(1/δ).
To obtain a data-independent upper bound on the radius, all that remains is to upper bound
ln
 det
  c
σ2 Φ⊤
t Φt + 1

by a data-independent quantity. Using Lemma D.2, we have
ln

det
 c
σ2 Φ⊤
t Φt + 1

≤d ln

1 + ctL2
σ2d

.
Therefore
R2
AMM,t ≤σ2d ln

1 + ctL2
σ2d

+ σ2B2
c
+ 2σ2ln(1/δ).
27

Lemma D.11. If, for any θ0 ∈Rd and any σ0 > 0, the sequence of mixture distributions is
Pt = N(Φtθ0, σ2
0ΦtΦ⊤
t ), then for any δ ∈(0, 1] and any α > 0, with probability at least 1 −δ, for
all t ≥1
R2
AMM,t ≤σ2d + σ2
σ2
0
∥θ∗−θ0∥2
2 + σ2dln

1 + tσ2
0L2
σ2d

+ αB2 + 4σ2ln(1/δ)
+ σ2dln
 1 + tL2/(αd)

+ 2√α ∥θ∗∥2
p
σ2dln (1 + tL2/(αd)) + 2σ2ln(1/δ).
Proof. In App. B.2 (see Equation 16), we saw that the squared radius R2
MM,t can be written as
R2
MM,t = −2σ2 ln
 
E
f t∼N(Φtθ0,σ2
0ΦtΦ⊤
t )

exp

−1
2σ2 (f t −rt)⊤(f t −rt)
!
+ 2σ2 ln(1/δ).
(32)
Using the substitution Φtθ = f t, (32) is equivalent to
R2
MM,t = −2σ2 ln
 
E
θ∼N(θ0,σ2
01)

exp

−1
2σ2 (Φtθ −rt)⊤(Φtθ −rt)
!
+ 2σ2 ln(1/δ). (33)
Using the Donsker-Varadhan change of measure inequality (specifically (26)), the first term on the
right-hand-side of (33) is equal to
inf
Q∈P(Rd)

E
θ∼Q

(Φtθ −rt)⊤(Φtθ −rt)

+ 2σ2DKL(Q||N(θ0, σ2
01))

.
If we evaluate this at any specific distribution Q, we obtain an upper bound on the infimum over Q.
We choose Q = N(θ∗, Σt), where Σt = ( 1
σ2 Φ⊤
t Φt +
1
σ2
0 1)−1. Combining everything so far, we
have
R2
MM,t ≤
E
θ∼N(θ∗,Σt)

(Φtθ −rt)⊤(Φtθ −rt)

+ 2σ2DKL(N(θ∗, Σt)||N(θ0, σ2
01)) + 2σ2 ln(1/δ)
= (Φtθ∗−rt)⊤(Φtθ∗−rt) + tr(Φ⊤
t ΦtΣt) + σ2
σ2
0
tr(Σt)
−σ2d + σ2
σ2
0
∥θ∗−θ0∥2
2 + σ2ln

det(Σ−1
t )
det((1/σ2
0)1)

+ 2σ2 ln(1/δ).
(34)
Using Lemma D.3, we have
tr(Φ⊤
t ΦtΣt) ≤σ2d.
Using Lemma D.4, we have
σ2
σ2
0
tr(Σt) ≤σ2d.
Using Lemma D.2, we have
ln

det(Σ−1
t )
det((1/σ2
0)1)

= ln

det
σ2
0
σ2 Φ⊤
t Φt + 1

≤d ln

1 + σ2
0tL2
σ2d

.
The bound on R2
MM,t in (34) becomes
R2
MM,t ≤(Φtθ∗−rt)⊤(Φtθ∗−rt)+ σ2
σ2
0
∥θ∗−θ0∥2
2+σ2d+σ2d ln

1 + σ2
0tL2
σ2d

+2σ2ln(1/δ).
This means that
R2
AMM,t ≤(Φtθ∗−rt)⊤(Φtθ∗−rt) + σ2
σ2
0
∥θ∗−θ0∥2
2 + σ2d + σ2d ln

1 + σ2
0tL2
σ2d

+ 2σ2ln(1/δ)
+ αB2 −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt.
(35)
28

Finally, using Lemma D.5, then Theorem D.6 and Lemma D.7, and then Lemma D.2, for any
δ ∈(0, 1] and any α > 0, with probability at least 1 −δ, for all t ≥0 simultaneously
(Φtθ∗−rt)⊤(Φtθ∗−rt) −r⊤
t rt + r⊤
t Φt
 Φ⊤
t Φt + α1
−1 Φ⊤
t rt
≤
Φ⊤
t ϵt
2
(Φ⊤
t Φt+α1)−1 + 2α ∥θ∗∥(Φ⊤
t Φt+α1)−1
Φ⊤
t ϵt

(Φ⊤
t Φt+α1)−1
≤σ2 ln

det
 1
αΦ⊤
t Φt + 1

+ 2σ2 ln(1/δ) + 2√α ∥θ∗∥2 σ
s
ln

det
 1
αΦ⊤
t Φt + 1

+ 2 ln(1/δ)
≤σ2d ln

det

1 + tL2
αd

+ 2σ2 ln(1/δ) + 2√α ∥θ∗∥2 σ
s
d ln

det

1 + tL2
αd

+ 2 ln(1/δ).
Substituting this into (35), we have
R2
AMM,t ≤σ2d + σ2
σ2
0
∥θ∗−θ0∥2
2 + σ2dln

1 + tσ2
0L2
σ2d

+ αB2 + 4σ2ln(1/δ)
+ σ2dln
 1 + tL2/(αd)

+ 2√α ∥θ∗∥2
p
σ2dln (1 + tL2/(αd)) + 2σ2ln(1/δ).
D.2.2
Bounding the Sum of Norms
We use the following upper bound on the sum of the squared norms.
Lemma D.12 (Lemma 11 of (Abbasi-Yadkori et al., 2011)). For any α > 0, we have
T
X
t=1
min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+α1)
−1

≤
1
ln(2)d ln

1 + TL2
αd

.
In Lemma 11 of (Abbasi-Yadkori et al., 2011), 1/ ln(2) ≈1.44 is replaced with 2. We achieve an
improved constant by using Lemma D.8 instead of the looser bound min(1, x) ≤2 ln(1 + x), for
x ≥0.
D.2.3
Regret Bounds
We are now ready to prove our data-independent regret bounds.
Proof of Theorem 7.6. Following the same steps as in the proof of Lemma D.9, we can also obtain
the following data-dependent bound on the per-round regret for actions selected by CMM-UCB or
AMM-UCB. For the mixture distributions Pt = N(0t, cΦtΦ⊤
t ), α = σ2/c and any δ ∈(0, 1], with
probability at least 1 −δ
∀t ≥1,
∆(at) ≤2RAMM,t−1 ∥ϕ(at)∥(Φ⊤
t−1Φt−1+ σ2
c 1)−1 .
(36)
From Assumption 7.4 (ϕ(a)⊤θ∗∈[−C, C]), we have another bound on the per-round regret
∆(at) ≤2C.
(37)
The combination of (36) and (37) yields
∆(at) ≤min(2C, 2RAMM,t−1 ∥ϕ(at)∥(Φ⊤
t−1Φt−1+ σ2
c 1)−1)
≤2 max(C, RAMM,t−1) min(1, ∥ϕ(at)∥(Φ⊤
t−1Φt−1+ σ2
c 1)−1).
Starting with the Cauchy-Schwarz inequality, we have
T
X
t=1
∆(at) ≤
v
u
u
tT
T
X
t=1
∆(at)2
(38)
≤
v
u
u
tT
T
X
t=1
4 max

C2, R2
AMM,t−1

min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+ σ2
c 1)−1

.
29

We will now use the upper bound on R2
AMM,t−1 from Lemma D.10. Let U 2
AMM,t−1 denote this upper
bound (i.e. the right-hand-side of (31)). We have
T
X
t=1
∆(at) ≤
v
u
u
tT
T
X
t=1
4 max

C2, U 2
AMM,t−1

min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+ σ2
c 1)−1

≤2 max (C, UAMM,T −1)
v
u
u
tT
T
X
t=1
min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+ σ2
c 1)−1

Finally, using the bound on the sum of norms in Lemma D.12, we have
T
X
t=1
∆(at) ≤
2
p
ln(2)
max
 
C, σ
s
d ln

1 + c(T −1)L2
σ2d

+ B2
c + 2 ln
1
δ
! s
dT ln

1 + cTL2
σ2d

.
Now, we state and prove a cumulative regret bound that holds for more general choices of the mixture
distributions and the parameter α.
Theorem D.13. Suppose that assumptions 7.1-7.4 hold. If, for any θ0 ∈Rd and any σ0 > 0, the
sequence of mixture distributions is Pt = N(Φtθ0, σ2
0ΦtΦ⊤
t ), then for any δ ∈(0, 1/2] and any
α > 0, with probability at least 1 −2δ, for all T ≥1 simultaneously, the cumulative regret of
CMM-UCB and AMM-UCB is bounded by
∆1:T ≤
2
√
ln 2
max {C, UAMM,T −1}
s
dT ln

1+ L2T
αd

= O(d
√
Tln(T)),
where
U 2
AMM,T −1 ≤σ2d + σ2
σ2
0
∥θ∗−θ0∥2
2 + σ2dln

1 + (T −1)σ2
0L2
σ2d

+ αB2 + 4σ2ln(1/δ)
(39)
+ σ2dln

1 + (T −1)L2
αd

+ 2√α ∥θ∗∥2
s
σ2dln

1 + (T −1)L2
αd

+ 2σ2ln(1/δ).
Proof. Following the proof of Theorem 7.6, we obtain (with high probability)
T
X
t=1
∆(at) ≤
v
u
u
tT
T
X
t=1
4 max

C2, R2
AMM,t−1

min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+α1)−1

.
This time, we use the bound on the radius from Lemma D.11. Let UAMM,T −1 denote this bound on
the radius (i.e. the square root of the right-hand-side of (39)). Also, note that this bound on the radius
holds with probability at 1 −δ. Since UAMM,T −1 is monotonically increasing with T, we have
T
X
t=1
∆(at) ≤2 max (C, UAMM,T −1)
v
u
u
tT
T
X
t=1
min

1, ∥ϕ(at)∥2
(Φ⊤
t−1Φt−1+α1)−1

.
Finally, we use Lemma D.12 to obtain
T
X
t=1
∆(at) ≤
2
√
ln 2
max {C, UAMM,T −1}
s
dT ln

1+ L2T
αd

.
We used two inequalities that each hold with probability at least 1 −δ. By a union bound argument,
the cumulative regret bound holds with probability at least 1 −2δ.
30

E
Additional Experiments
In this section, we present the results of some additional experiments in which we investigate the
effect of the mixture distributions (or priors) on our upper and lower confidence bounds.
E.1
The Effect of The Mixture Distributions
We investigate how our CMM-UCB upper and lower confidence bounds behave when we provide an
uninformative but well-specified mixture distribution/prior, an informative and well-specified mixture
distribution/prior, and a misspecified mixture distribution/prior. Here misspecification refers to prior
misspecification in a Bayesian sense. For reference, we compare the behaviour of our upper and
lower confidence bounds with the upper and lower limits of a Bayesian credible interval.
For a fair comparison with CMM-UCB, we attempt to construct a Bayesian credible interval that
holds with high probability for all rounds t ≥0. However, whilst the CMM-UCB confidence
set holds with high probability over the random draw of the data a1, r1, a2, r2, . . . , the Bayesian
credible interval holds with high probability over the random draw of θ∗from a prior (for fixed data
a1, r1, a2, r2, . . . ).
For the Bayesian credible interval, we use a Gaussian prior and assume θ∗∼N(µ0, Σ0). We assume
a Gaussian likelihood function, i.e. rewards are of the form rt = ϕ(at)⊤θ +ϵt, where ϵt ∼N(0, σ2).
The Bayesian posterior for θ∗is another Gaussian N(µt, Σt), where
µt = Σt

Σ−1
0 µ0 + 1
σ2 Φ⊤
t rt

,
Σt =
 1
σ2 Φ⊤
t Φt + Σ−1
0
−1
.
Using Bayes’ rule, at any round t, we have θ∗∼N(µt, Σt). Therefore
(µt −θ∗)⊤Σ−1
t (µt −θ∗) ∼χ2(d),
where χ2(d) is a chi-squared distribution with d degrees of freedom. Let Qd(·) be the quantile
function of the chi-squared distribution with d degrees of freedom. With probability at least 1 −δt
(over the random draw of θ∗from N(µt, Σt))
(µt −θ∗)⊤Σ−1
t (µt −θ∗) ≤Qd(1 −δt).
(40)
Using a union bound argument, if δt =
6δ
(t+1)2π2 , then (40) holds with probability at least 1 −δt
for all t ≥0. Therefore, if θ∗∼N(µ0, Σ0), then with high probability the following credible sets
contain θ∗for all t ≥0 simultaneously:
Θt =

θ ∈Rd
(µt −θ∗)⊤Σ−1
t (µt −θ∗) ≤Qd

1 −
6δ
(t + 1)2π2

.
The upper limit of the credible interval for this credible set (and the one we use in Figure 4) is
sup
θ∈Θt

ϕ(a)T θ
	
= ϕ(a)T µt +
s
Qd

1 −
6δ
(t + 1)2π2
q
ϕ(a)⊤Σtϕ(a).
We compute confidence bounds/credible intervals for a randomly generated linear function of the
form f(x) = ϕ(x)⊤θ∗, with inputs x ∈R and θ∗∈R20, the latter drawn from a standard Gaussian
distribution and if necessary scaled down to ∥θ∗∥2 ≤10 =: B. For the feature map ϕ, we use
Random Fourier Features. We generate random data {(xk, yk)}t
k=1, where yk = ϕ(xk)⊤θ∗+ ηk,
ηk ∼N(0, σ2) and σ = 0.1 (so the Gaussian likelihood is well-specified).
31

4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
CMM-UCB
4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
Bayes
4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
4
2
0
2
4
7.5
5.0
2.5
0.0
2.5
5.0
Figure 4: The upper and lower confidence bounds of our CMM-UCB method (left) and Bayesian
posterior credible intervals (right) with different choices of the prior. The top row uses the prior f t ∼
N(0, ΦtΦ⊤
t ) for CMM-UCB and θ∗∼N(0, 1)) for Bayes. The middle row uses an informative
prior: f t ∼N(Φtθ∗, 0.1ΦtΦ⊤
t ) for CMM-UCB and θ∗∼N(θ∗, 0.11)) for Bayes. The bottom row
uses a misspecified prior: f t ∼N(−Φtθ∗, 0.1ΦtΦ⊤
t ) for CMM-UCB and θ∗∼N(−θ∗, 0.11)) for
Bayes.
Figure 4 shows the CMM-UCB upper and lower confidence bounds (left) and the Bayesian credible
intervals (right) with different choices of the prior. We use roughly equivalent priors for both methods.
If the Bayesian credible interval uses the prior θ∗∼N(µ0, Σ0), then CMM-UCB uses the induced
distribution over the function values Φtθ∗, i.e. f t ∼N(Φtµ0, ΦtΣ0Φ⊤
t ).
In the top and middle rows of Figure 4, where the prior is well-specified, we observe that the CMM-
UCB upper and lower confidence bounds are slightly looser than the Bayesian credible intervals.
In the bottom row of Figure 4, when the prior is misspecified, the CMM-UCB interval gets looser
whereas the Bayesian credible interval becomes wrong. In summary, the Bayesian credible interval
appears to be slightly tighter when the prior is well-specified and at least somewhat informative, but
CMM-UCB is robust to “misspecified” mixture distributions.
32

E.2
Benefits of Adaptive Mixture Distributions
In this section, we investigate a method for refining µt and T t based on previously observed actions
and rewards. Recall that µt and T t must be chosen such that: (a) µt and T t can only depend on
a1, . . . , at and r1, . . . , rt−1; (b) the first t −1 elements of µt must be equal to µt−1; (c) the upper
left t −1 × t −1 block of T t must be T t−1; (d) T t must be positive (semi-)definite.
As in Sec. 6.4, m and k are any fixed mean and kernel functions. Each new row and column of T t is
set using an adaptive kernel function kt−1. For β > 0, define
kt(a, a′) := k(a, a′) −kt(a)⊤(Kt + β1)−1 kt(a′),
where kt(a) = [k(a, a1), . . . , k(a, at)]⊤and Kt is the kernel matrix whose (i, j)th element is
k(ai, aj). For T t, we choose
T t =


k0(a1, a1)
k1(a1, a2)
· · ·
kt−1(a1, at)
k1(a2, a1)
k1(a2, a2)
· · ·
kt−1(a2, at)
...
...
...
...
kt−1(at, a1)
kt−1(at, a2)
· · ·
kt−1(at, at)

.
(41)
The ith column and ith row of this matrix depend only on only a1, r1, . . . , ai. Our motivation for this
kernel function is: (a) generalising the usual Bayesian Gaussian process (GP) posterior covariance,
one can show that if the kernel function k is positive definite, then T t is positive semi-definite; (b) kt
is the Bayesian GP posterior covariance function (with a Gaussian likelihood with variance β). Each
new element of µt is set by evaluating an adaptive mean function mt−1 at the latest action at. Define
mt(a) := m(a) −kt(a)⊤(Kt + β1)−1 (mt −rt) .
For µt, we choose
µt = [m0(a1), m1(a2), . . . , mt−1(at)]⊤.
(42)
The ith element, mi−1(ai), depends on only a1, r1, . . . , ai, so this is a valid choice for µt. Note that
mt is the Bayesian GP posterior mean function (again with a Gaussian likelihood with variance β).
We now compare the standard “non-adaptive” mixture distributions (with µt and T t as in (9)) and an
adaptive sequence of Gaussian mixture distributions (with µt as in (42) and T t as in (41)). With both
sequences of mixture distributions, we use m(a) = 0 and k(a, a′) = ϕ(a)⊤ϕ(a′). For the adaptive
sequence of mixture distributions, we set β = 4σ2.
4
2
0
2
4
6
4
2
0
2
4
Non-Adaptive
Adaptive
Figure 5: The upper and lower confidence bounds of CMM-UCB with the standard (non-adaptive)
sequence of Gaussian mixture distributions (purple) and the adaptive sequence of Gaussian mixture
distributions (red).
Figure 5 shows upper and lower confidence bounds for a randomly generated linear function f(x) =
ϕ(x)⊤θ∗, where x ∈R, θ∗∈R20, and ϕ is a random Fourier feature map. In this example, the
adaptive sequence of mixture distributions leads to tighter upper and lower confidence bounds.
33

100
200
300
400
500
T
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Radius
Radius
AMM-UCB
Adaptive AMM-UCB
100
200
300
400
500
T
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
Test Accuracy
Test Accuracy / Expected Reward
Figure 6: The radius RAMM,t (left) and test accuracy (right) with the standard sequence of Gaussian
mixture distributions (blue) and the adaptive sequence of Gaussian mixture distributions (orange).
On the left, we plot the mean value of the radius RAMM,t over 10 runs and show the mean ± one
standard deviation in the shaded regions. On the right, we plot the mean reward over 10 runs and
after Gaussian kernel smoothing.
Figure 6 shows the radius RAMM,t (left) and test accuracy (right) for AMM-UCB in the SVM
hyperparameter tuning problem with the Raisin data set (described in Sec. 8.2). We observe that
AMM-UCB with the adaptive mixture distributions achieves slightly higher test accuracy.
In Sec. D.2.1, we saw that for a standard sequence of mixture distributions, RAMM,T could be
upper bounded by a data-independent quantity of order O(
p
d ln(T)). In Figure 6, RAMM,T does
appear to grow roughly logarithmically with T when the mixture distributions are the standard choice.
However, RAMM,T appears to be bounded by a constant when the mixture distributions are adaptive.
If, when using adaptive mixture distributions, we could prove a data-independent bound on RAMM,T
of order O(
√
d), then we would be able to improve our data-independent cumulative regret bounds
to O(d
p
Tln(T)) (rather than O(d
√
Tln(T))). This would be within a
p
ln(T) factor of the lower
bound Ω(d
√
T).
F
Efficient Radius Computation
If we compute the squared radius R2
MM,t using the expression in (5), then we have to compute the
inverse and determinant of the t × t matrix 1 + T t/σ2. We will now show that for any mixture
distribution of the form Pt = N(µt, ΦtΣ0Φ⊤
t ), where Σ0 is symmetric and positive-definite, we can
re-write the expression for R2
MM,t such that we instead need to compute the inverse and determinant
of a d × d matrix. When Pt = N(µt, ΦtΣ0Φ⊤
t ), the squared radius R2
MM,t is equal to
R2
MM,t = (µt −rt)⊤

1 + ΦtΣ0Φ⊤
t
σ2
−1
(µt −rt)+σ2 ln

det

1 + ΦtΣ0Φ⊤
t
σ2

+2σ2 ln(1/δ).
By using the Weinstein–Aronszajn identity, and then doing some algebra, we have
det

1 + ΦtΣ0Φ⊤
t
σ2

= det
 
1 + Σ1/2
0
Φ⊤
t ΦtΣ1/2
0
σ2
!
= det(Σ0/σ2) det
 Φ⊤
t Φt + σ2Σ−1
0

.
Using Lemma C.4 with γ = σ2, v = µt −rt and M = ΦtΣ1/2
0
, we have
(µt −rt)⊤

1 + 1
σ2 ΦtΣ0Φ⊤
t
−1
(µt −rt) = (µt −rt)⊤(µt −rt)
−(µt −rt)⊤ΦtΣ1/2
0

Σ1/2
0
Φ⊤
t ΦtΣ1/2
0
+ σ21
−1
Σ1/2
0
Φ⊤
t (µt −rt)
= (µt −rt)⊤(µt −rt) −(µt −rt)⊤ΦtΣ1/2
0

Σ1/2
0
 Φ⊤
t Φt + σ2Σ−1
0

Σ1/2
0
−1
Σ1/2
0
Φ⊤
t (µt −rt)
= (µt −rt)⊤(µt −rt) −(µt −rt)⊤Φt
 Φ⊤
t Φt + σ2Σ−1
0
−1 Φ⊤
t (µt −rt).
34

The resulting expression for R2
MM,t is rather cumbersome, but the upshot is that we now (only) need
to compute the inverse and determinant of the d × d matrix Φ⊤
t Φt + σ2Σ−1
0 . Since, we compute
R2
MM,t at each round t, we can update the inverse of Φ⊤
t Φt + σ2Σ−1
0
incrementally using the
Sherman-Morrison formula (Sherman & Morrison, 1950). The determinant of Φ⊤
t Φt + σ2Σ−1
0
can
be updated incrementally using the relation
det
 Φ⊤
t Φt + σ2Σ−1
0

= det(Φ⊤
t−1Φt−1 + σ2Σ−1
0 )(1 + ϕ(at)⊤(Φ⊤
t−1Φt−1 + σ2Σ−1
0 )−1ϕ(at)),
which can be found in Eq. (6) in Lemma 11 of (Abbasi-Yadkori et al., 2011). Additionally, recall
that (see Eq. (24)) when Pt = N(0, cΦtΦ⊤
t ) and α = σ2/c for any c > 0, the radius RAMM,t of our
analytic confidence bounds simplifies to
R2
AMM,t = σ2 ln

det
 c
σ2 Φ⊤
t Φt + 1

+ σ2B2
c
+ 2σ2ln(1/δ).
Note that the inverse (Φ⊤
t Φt + (σ2/c)1)−1 still appears in the expression for AUCBΘt(a) and in
the incremental determinant update rule.
35

