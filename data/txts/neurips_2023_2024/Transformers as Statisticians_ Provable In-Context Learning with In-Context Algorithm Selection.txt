Transformers as Statisticians: Provable In-Context
Learning with In-Context Algorithm Selection
Yu Bai∗
Salesforce Research
yu.bai@salesforce.com
Fan Chen∗
Massachusetts Institute of Technology
fanchen@mit.edu
Huan Wang
Salesforce Research
huan.wang@salesforce.com
Caiming Xiong
Salesforce Research
cxiong@salesforce.com
Song Mei∗
UC Berkeley
songmei@berkeley.edu
Abstract
Neural sequence models based on the transformer architecture have demonstrated
remarkable in-context learning (ICL) abilities, where they can perform new tasks
when prompted with training and test examples, without any parameter update to the
model. This work first provides a comprehensive statistical theory for transformers
to perform ICL. Concretely, we show that transformers can implement a broad
class of standard machine learning algorithms in context, such as least squares,
ridge regression, Lasso, learning generalized linear models, and gradient descent
on two-layer neural networks, with near-optimal predictive power on various in-
context data distributions. Using an efficient implementation of in-context gradient
descent as the underlying mechanism, our transformer constructions admit mild
size bounds, and can be learned with polynomially many pretraining sequences.
Building on these “base” ICL algorithms, intriguingly, we show that transformers
can implement more complex ICL procedures involving in-context algorithm se-
lection, akin to what a statistician can do in real life—A single transformer can
adaptively select different base ICL algorithms—or even perform qualitatively
different tasks—on different input sequences, without any explicit prompting of the
right algorithm or task. We both establish this in theory by explicit constructions,
and also observe this phenomenon experimentally. In theory, we construct two gen-
eral mechanisms for algorithm selection with concrete examples: pre-ICL testing,
and post-ICL validation. As an example, we use the post-ICL validation mecha-
nism to construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging task—noisy linear models with mixed noise levels. Experimentally,
we demonstrate the strong in-context algorithm selection capabilities of standard
transformer architectures.
1
Introduction
Large neural sequence models have demonstrated remarkable in-context learning (ICL) capabili-
ties [12], where models can make accurate predictions on new tasks when prompted with training
examples from the same task, in a zero-shot fashion without any parameter update to the model. A
prevalent example is large language models based on the transformer architecture [84], which can
perform a diverse range of tasks in context when trained on enormous text [12, 90]. Recent models
∗Equal technical and directional contributions.
Code is available at https://github.com/allenbai01/transformers-as-statisticians.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Data
Which distribution?
dist 1
dist K
̂yN+1 = 𝖠𝗅𝗀k⋆(D, xN+1)
TF
𝖠𝗅𝗀1
𝖠𝗅𝗀K
……
Mechanism 2: Pre-ICL Testing
Data
𝖠𝗅𝗀1(Dtrain)
𝖠𝗅𝗀K(Dtrain)
……
TF
𝖫𝗈𝗌𝗌1(Dval)
𝖫𝗈𝗌𝗌K(Dval)
……
Which loss is the smallest?
̂yN+1 = 𝖠𝗅𝗀k⋆(Dtrain, xN+1)
Train-validation split
Mechanism 1: Post-ICL Validation
Data 1 
(Regression)
Transformer
𝖫𝗂𝗇𝖱𝖾𝗀(D, xN+1)
𝖫𝗈𝗀𝖱𝖾𝗀(D, xN+1)
Data 2 
(Classification)
̂yN+1 :
Example 2: Regression + Classification
Data 1 
(Reg w/noise 
)
σ1
Transformer
𝖱𝗂𝖽𝗀𝖾λ1(D, xN+1)
𝖱𝗂𝖽𝗀𝖾λ2(D, xN+1)
Data 2 
(Reg w/noise 
)
σ2
̂yN+1 :
Example 1: Ridge with different λ
Figure 1: Illustration of in-context algorithm selection, and two mechanisms constructed in our theory.
Left, middle-left: A single transformer can perform ridge regression with different λ’s on input sequences
with different observation noise; we prove this by the post-ICL validation mechanism (Section 4.1). Middle-
right, right: A single transformer can perform linear regression on regression data and logistic regression on
classification data; we prove this via the pre-ICL testing mechanism (Section 4.2).
in this paradigm such as GPT-4 achieve surprisingly impressive ICL performance that makes them
akin to a general-purpose agent in many aspects [65, 14]. Such strong capabilities call for better
understandings, which a recent line of work tackles from various aspects [49, 94, 28, 72, 15, 57, 64].
Recent pioneering work of Garg et al. [31] proposes an interpretable and theoretically amenable
setting for understanding ICL in transformers. They perform ICL experiments where input tokens
are real-valued (input, label) pairs generated from standard statistical models such as linear models
(and the sparse version), neural networks, and decision trees. Garg et al. [31] find that transformers
can learn to perform ICL with prediction power (and fitted functions) matching standard machine
learning algorithms for these settings, such as least squares for linear models, and Lasso for sparse
linear models. Subsequent work further studies the internal mechanisms [2, 86, 18], expressive
power [2, 32], and generalization [47] of transformers in this setting. However, these works only
showcase simple mechanisms such as regularized regression [31, 2, 47] or gradient descent [2, 86, 18],
which are arguably only a small subset of what transformers are capable of in practice; or expressing
universal function classes not specific to ICL [89, 32]. This motivates the following question:
How do transformers learn in context beyond implementing simple algorithms?
This paper makes steps on this question by making two main contributions: (1) We unveil a general
mechanism—in-context algorithm selection—by which a single transformer can adaptively select
different “base” ICL algorithms to use on different ICL instances, without any explicit prompting of
the right algorithm to use in the input sequence. For example, a transformer may choose to perform
ridge regression with regularization λ1 on ICL instance 1, and λ2 on ICL instance 2 (Figure 2); or
perform regression on ICL instance 1 and classification on ICL instance 2 (Figure 5). This adaptivity
allows transformers to achieve much stronger ICL performance than the base ICL algorithms. We
both prove this in theory, and demonstrate this phenomenon empirically on standard transformer
architectures. (2) Along the way, equally importantly, we present a comprehensive theory for ICL in
transformers by establishing end-to-end quantitative guarantees for the expressive power, in-context
prediction performance, and sample complexity of pretraining. These results add upon the recent
line of work on the statistical learning theory of transformers [97, 89, 27, 39], and lay out a foundation
for the intriguing special case where the learning targets are themselves ICL algorithms.
A detailed summary of our contributions is as follows.
• We prove that transformers can implement a broad class of standard machine learning algorithms
in context, such as least squares, ridge regression, Lasso, convex risk minimization for learning
generalized linear models (such as logistic regression), and gradient descent for two-layer neural
networks (Section 3). Our constructions admit mild bounds on the number of layers, heads, and
weight norms, and achieve near-optimal prediction power on many in-context data distributions.
• Technically, the above transformer constructions build on a new efficient implementation of in-
context gradient descent (Appendix D), which could be broaderly applicable. For a broad class
of smooth convex empirical risks over the in-context training data, we construct an (L+1)-layer
transformer that approximates L steps of gradient descent. Notably, the approximation error
accumulates only linearly in L, utilizing a stability-like property of smooth convex optimization.
• We prove that transformers can perform in-context algorithm selection (Section 4). We construct
two algorithm selection mechanisms: Post-ICL validation (Section 4.1), and Pre-ICL testing
2

(a) Noisy linear reg with noise σ1
0
10
20
30
40
in-context examples
0.0
0.2
0.4
0.6
0.8
1.0
square loss
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
(b) Noisy linear reg with noise σ2
0
10
20
30
40
in-context examples
0.4
0.6
0.8
1.0
1.2
1.4
 
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
(c) Task 1 vs. task 2 at token 20
0.10
0.15
0.20
0.25
0.30
noisy_reg_noise_1
0.6
0.8
1.0
1.2
1.4
noisy_reg_noise_2
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
ridge analytical
Bayes_err_noise_1
Bayes_err_noise_2
Figure 2: In-context algorithm selection on two separate noisy linear regression tasks with noise (σ1, σ2) =
(0.1, 0.5). (a,b) A single transformer TF_alg_select simultaneously approaches the performance of the
two individual Bayes predictors ridge_lam_1 on task 1 and ridge_lam_2 on task 2. (c) At token 20 (using
example {0, . . . , 19} for training), TF_alg_select approaches the Bayes error on two tasks simultaneously,
and outperforms ridge regression with any fixed λ. (a,b,c) Note that transformers pretrained on a single
task (TF_noise_1, TF_noise_2) perform near-optimally on that task but suboptimally on the other task. More
details about the setup and training method can be found in Appendix M.2.
(Section 4.2). For both mechanisms, we provide general constructions as well as concrete
examples. Figure 1 provides a pictorial illustration of the two mechanisms.
• As a concrete application, using the post-ICL validation mechanism, we construct a transformer
that can perform nearly Bayes-optimal ICL on noisy linear models with mixed noise levels
(Section 4.1.1), a more complex task than those considered in existing work.
• We provide the first line of results for pretraining transformers to perform the various ICL tasks
above, from polynomially many training sequences (Section 5 & Appendix K).
• Experimentally, we find that learned transformers indeed exhibit strong in-context algorithm
selection capabilities in the settings considered in our theory (Section 6). For example, Figure 2
shows that a single transformer can approach the individual Bayes risks (the optimal risk among
all possible algorithms) simultaneously on two noisy linear models with different noise levels.
Transformers as statisticians
We humbly remark that the typical toolkit of a statistician contains
much more beyond those covered in this work, including and not limited to inference, uncertainty
quantification, and theoretical analysis. This work merely aims to show the algorithm selection
capability of transformers, akin to what a statistician can do.
Related work
Our work is intimately related to the lines of work on in-context learning, theoretical
understandings of transformers, as well as other formulations for learning-to-learn such as meta-
learning. Due to limited space, we discuss these related work in Appendix A.
2
Preliminaries
We consider a sequence of N input vectors {hi}N
i=1 ⊂RD, written compactly as an input matrix
H = [h1, . . . , hN] ∈RD×N, where each hi is a column of H (also a token). Throughout this paper,
we let σ(t) := ReLU(t) = max {t, 0} denote the standard relu activation.
2.1
Transformers
We consider transformer architectures that process any input sequence H ∈RD×N by applying
(encoder-mode2) attention layers and MLP layers formally defined as follows.
Definition 1 (Attention layer). A (self-)attention layer with M heads is denoted as Attnθ(·) with
parameters θ = {(Vm, Qm, Km)}m∈[M] ⊂RD×D. On any input sequence H ∈RD×N,
eH = Attnθ(H) := H + 1
N
PM
m=1(VmH) × σ
 (QmH)⊤(KmH)

∈RD×N,
(1)
where σ : R →R is the ReLU function. In vector form,
ehi = [Attnθ(H)]i = hi + PM
m=1
1
N
PN
j=1 σ(⟨Qmhi, Kmhj⟩) · Vmhj.
2Many of our results can be generalized to decoder-based architectures; see Appendix C for a discussion.
3

Above, (1) uses a normalized ReLU3 activation t 7→σ(t)/N in place of the standard softmax
activation; we remark this activation is also found to work well empirically in recent studies [78, 93].
Definition 2 (MLP layer). A (token-wise) MLP layer with hidden dimension D′ is denoted as
MLPθ(·) with parameters θ = (W1, W2) ∈RD′×D×RD×D′. On any input sequence H ∈RD×N,
eH = MLPθ(H) := H + W2σ(W1H),
where σ : R →R is the ReLU function. In vector form, we have ehi = hi + W2σ(W1hi).
We consider a transformer architecture with L ≥1 transformer layers, each consisting of a self-
attention layer followed by an MLP layer.
Definition 3 (Transformer). An L-layer transformer, denoted as TFθ(·), is a composition of L
self-attention layers each followed by an MLP layer: H(L) = TFθ(H(0)), where H(0) ∈RD×N is
the input sequence, and
H(ℓ) = MLPθ(ℓ)
mlp

Attnθ(ℓ)
attn
 H(ℓ−1)
,
ℓ∈{1, . . . , L}.
Above,
the parameter θ
=
(θ(1:L)
attn , θ(1:L)
mlp ) consists of the attention layers θ(ℓ)
attn
=
{(V(ℓ)
m , Q(ℓ)
m , K(ℓ)
m )}m∈[M (ℓ)] ⊂RD×D and the MLP layers θ(ℓ)
mlp = (W(ℓ)
1 , W(ℓ)
2 ) ∈RD(ℓ)×D ×
RD×D(ℓ). We will frequently consider “attention-only” transformers with W(ℓ)
1 , W(ℓ)
2
= 0, which
we denote as TF0
θ(·) for shorthand, with θ = θ(1:L) := θ(1:L)
attn .
We additionally define the following norm of a transformer TFθ:
|||θ||| := max
ℓ∈[L]
n
max
m∈[M]
n
∥Q(ℓ)
m ∥op, ∥K(ℓ)
m ∥op
o
+
M
X
m=1
∥V(ℓ)
m ∥op + ∥W(ℓ)
1 ∥op + ∥W(ℓ)
2 ∥op
o
.
(2)
In (2), the choices of the operator norm and max/sums are for convenience only and not essential, as
our results (e.g. for pretraining) depend only logarithmically on |||θ|||.
2.2
In-context learning
In an in-context learning (ICL) instance, the model is given a dataset D = {(xi, yi)}i∈[N]
iid
∼P and a
new test input xN+1 ∼Px for some data distribution P, where {xi}i∈[N] ⊆Rd are the input vectors,
{yi}i∈[N] ⊆R are the corresponding labels (e.g. real-valued for regression, or {0, 1}-valued for
binary classification), and xN+1 is the test input on which the model is required to make a prediction.
Different from standard supervised learning, in ICL, each instance (D, xN+1) is in general drawn
from a different distribution Pj, such as a linear model with a new ground truth coefficient w⋆,j ∈Rd.
Our goal is to construct fixed transformer to perform ICL on a large set of Pj’s.
We consider using transformers to perform ICL, in which we encode (D, xN+1) into an input se-
quence H ∈RD×(N+1). In our theory, we use the following format, where the first two rows contain
(D, xN+1) (zero at the location for yN+1), and the third row contains fixed vectors {pi}i∈[N+1] with
ones, zeros, and indicator for being the train token (similar to a positional encoding vector):
H =
"x1
x2
. . .
xN
xN+1
y1
y2
. . .
yN
0
p1
p2
. . .
pN
pN+1
#
∈RD×(N+1),
pi :=
"
0D−(d+3)
1
1{i < N + 1}
#
∈RD−(d+1).
(3)
We will choose D = Θ(d), so that the hidden dimension of H is at most a constant multiple of d. We
then feed H into a transformer to obtain the output eH = TFθ(H) ∈RD×(N+1) with the same shape,
and read out the prediction byN+1 from the (d + 1, N + 1)-th entry of eH = [ehi]i∈[N+1] (the entry
corresponding to the missing test label): byN+1 = ready( eH) := (ehN+1)d+1. The goal is to predict
3For each query index i, the attention weights {σ(⟨Qmhi, Kmhj⟩)/N}j∈[N] is also a set of non-negative
weights that sum to O(1) (similar as a softmax probability distribution) in typical scenarios. Also, our approxi-
mation results can potentially be generalized to softmax attention e.g. using the technique of [32].
4

byN+1 that is close to yN+1 ∼Py|xN+1 measured by proper losses. We emphasize that we consider
predicting only at the last token xN+1, which is without much loss of generality.4
Miscellaneous setups
We assume bounded features and labels throughout the paper (unless oth-
erwise specified, e.g. when xi is Gaussian): ∥xi∥2 ≤Bx and |yi| ≤By with probability one. We
use the standard notation X = [x⊤
1 ; . . . ; x⊤
N] ∈RN×d and y = [y1; . . . ; yN] ∈RN to denote the
matrix of inputs and vector of labels, respectively. To prevent the transformer from blowing up on
tail events, in all our results concerning (statistical) in-context prediction powers, we consider a
clipped prediction byN+1 = g
ready( eH) := clipR((ehN+1)d+1), where clipR(t) := Proj[−R,R](t) is
the standard clipping operator with (a suitably large) radius R ≥0 that varies in different problems.
3
Basic in-context learning algorithms
We begin by constructing transformers that approximately implement a variety of standard machine
learning algorithms in context, with mild size bounds and near-optimal prediction power on many
standard in-context data distributions.
3.1
In-context ridge regression and least squares
Consider the standard ridge regression estimator over the in-context training examples D with
regularization λ ≥0 (reducing to least squares at λ = 0 and N ≥d):
wλ
ridge := arg minw∈Rd
1
2N
PN
i=1 (⟨w, xi⟩−yi)2 + λ
2 ∥w∥2
2 .
(ICRidge)
We show that transformers can approximately implement (ICRidge) (proof in Appendix F.1).
Theorem 4 (Implementing in-context ridge regression). For any λ ≥0, 0 ≤α ≤β with κ := β+λ
α+λ,
Bw > 0, and ε < BxBw/2, there exists an L-layer attention-only transformer TF0
θ with
L = ⌈2κ log(BxBw/(2ε))⌉+ 1,
maxℓ∈[L] M (ℓ) ≤3,
|||θ||| ≤4R + 8(β + λ)−1.
(4)
(with R := max {BxBw, By, 1}) such that the following holds. On any input data (D, xN+1) such
that the problem (ICRidge) is well-conditioned and has a bounded solution:
α ≤λmin(X⊤X/N) ≤λmax(X⊤X/N) ≤β,
wλ
ridge

2 ≤Bw/2,
(5)
TF0
θ approximately implements (ICRidge): The prediction byN+1 = ready(TF0
θ(H)) satisfies
byN+1 −

wλ
ridge, xN+1
 ≤ε.
(6)
Theorem 4 presents the first quantitative construction for end-to-end in-context ridge regression up
to arbitrary precision, and improves upon Akyürek et al. [2] whose construction does not give (or
directly imply) an explicit error bound like (6). Further, the bounds on the number of layers and
heads in (4) are mild (constant heads and logarithmically many layers).
Near-optimal in-context prediction power for linear problems
Combining Theorem 4 with
standard analyses of linear regression yields the following corollaries (proofs in Appendix F.3 & F.4).
Corollary 5 (Near-optimal linear regression with transformers by approximating least squares). For
any N ≥e
O(d), there exists an O(κ log(κN/σ))-layer transformer θ, such that on any P satisfying
standard statistical assumptions for least squares (Assumption A), its ICL prediction byN+1 achieves
E(D,xN+1,yN+1)∼P[(byN+1 −yN+1)2] ≤infw E(x,y)∼P

(y −⟨w, x⟩)2
+ e
O(dσ2/N).
Assumption A requires only generic tail properties such as sub-Gaussianity, and not realizability
(i.e., P follows a true linear model); κ, σ above denote the covariance condition number and the
4Our constructions may be generalized to predicting at every token, by using a decoder architecture and
potentially different input formats correspondingly (cf. Appendix C). Our theory focuses on predicting at the last
token only, which simplifies the setting. Our experiments test both settings.
5

noise level therein. The e
O(dσ2/N) excess risk is known to be rate-optimal for linear regression [38],
and Corollary 5 achieves this in context with a transformer with only logarithmically many layers.
Next, consider Bayesian linear models where each in-context data distribution P = Plin
w⋆is drawn
from a Gaussian prior π : w⋆∼N(0, Id/d), and (x, y) ∼Plin
w⋆is sampled as x ∼N(0, Id),
y = ⟨w⋆, x⟩+ N(0, σ2). It is a standard result that the Bayes estimator of yN+1 given (D, xN+1)
is given by ridge regression (ICRidge): byBayes
N+1 := ⟨wλ
ridge, xN+1⟩with λ = dσ2/N. We show that
transformers achieve nearly-Bayes risk for this problem, and we use
BayesRiskπ := Ew⋆∼π,(D,xN+1,yN+1)∼Plin
w⋆
h
1
2
 byBayes
N+1 −yN+1
2i
to denote the Bayes risk of this problem under prior π.
Corollary 6 (Nearly-Bayes linear regression with transformers by approximating ridge regression).
Under the Bayesian linear model above with N ≥max {d/10, O (log(1/ε))}, there exists a L =
O (log(1/ε))-layer transformer such that Ew⋆,(D,xN+1,yN+1)
 1
2(byN+1−yN+1)2
≤BayesRiskπ +ε.
Generalized linear models
In Appendix G, we extend the above results to generalized linear
models [53] and show that transformers can approximate the corresponding convex risk minimization
algorithm in context (which includes logistic regression for linear classification as an important
special case), and achieve near-optimal excess risk under standard statistical assumptions.
3.2
In-context Lasso
Consider the standard Lasso estimator [82] which minimizes an ℓ1-regularized linear regression loss
bLlasso over the in-context training examples D:
wlasso := arg minw∈Rd bLlasso(w) =
1
2N
PN
i=1 (⟨w, xi⟩−yi)2 + λN ∥w∥1 .
(ICLasso)
We show that transformers can also approximate in-context Lasso with a mild number of layers, and
can perform sparse linear regression in standard sparse linear models (proofs in Appendix H).
Theorem 7 (Implementing in-context Lasso). For any λN ≥0, β > 0, Bw > 0, and ε > 0, there
exists a L-layer transformer TFθ with
L =

βB2
w/ε

+ 1,
maxℓ∈[L] M (ℓ) ≤2,
maxℓ∈[L] D(ℓ) ≤2d,
|||θ||| ≤O
 R + (1 + λN)β−1
(where R := max {BxBw, By, 1}) such that the following holds. On any input data (D, xN+1) such
that λmax(X⊤X/N) ≤β and ∥wlasso∥2 ≤Bw/2, TFθ(H(0)) approximately implements (ICLasso),
in that it outputs byN+1 = ⟨xN+1, bw⟩with bLlasso(bw) −bLlasso(wlasso) ≤ε.
Theorem 8 (Near-optimal sparse linear regression with transformers by approximating Lasso). For
any d, N ≥1, δ > 0, B⋆
w, σ > 0, there exists a e
O((B⋆
w)2/σ2 × (1 + (d/N)))-layer transformer θ
such that the following holds: For any s and N ≥O (s log(d/δ)), suppose that P is an s-sparse
linear model: xi ∼N(0, Id), yi = ⟨w⋆, xi⟩+ N(0, σ2) for any ∥w⋆∥2 ≤B⋆
w and ∥w⋆∥0 ≤s, then
with probability at least 1 −δ (over the randomness of D), the transformer output byN+1 achieves
E(xN+1,yN+1)∼P

(byN+1 −yN+1)2
≤σ2[1 + O(s log(d/δ)/N)].
The e
O(s log d/N) excess risk obtained in Theorem 8 is optimal up to log factors [62, 87]. We remark
that Theorem 8 is not a direct corollary of Theorem 7; Rather, the bound on the number of layers
in Theorem 8 requires a sharper convergence analysis of the (ICLasso) problem under sparse linear
models (Appendix H.2), similar to [1].
3.3
Proof technique: In-context gradient descent
The constructions in Section 3.1 and 3.2 is built on the following result for approximating in-context
(proximal) gradient descent on (regularized) convex losses.
Theorem 9 (ICGD; Informal version of Theorem D.1 & D.2). For a broad class of convex losses
of form w 7→
1
N
PN
i=1 ℓ(w⊤xi, yi) + R(w), there exists an L-layer transformer that takes in any
(D, w0) and outputs bwL such that ∥bwL −wL
{GD,PGD}∥2 ≤O(Lε), by composing L identical layers
each O(ε)-approximating a single step of GD (so that O(Lε) is a linear error accumulation).
6

Theorem 9 is established in two main steps:
• Approximating one-step of ICGD using one attention layer (Proposition E.1), which substantially
generalizes that of von Oswald et al. [86] (which only does GD on square losses with a linear
self-attention), and is simpler than the ones in Akyürek et al. [2] and Giannou et al. [32].
• Stacking L of the above layer to approximate L steps of ICGD. Done naively, the error accumu-
lation of this stacking operation is exponential in L in the worst case. We utilize the stability of
convex gradient descent (Lemma D.1) to obtain the linear in L error accumulation in Theorem 9.
In Appendix D.3, we also give results for non-convex GD on two-layer neural nets, though with a
worse (exponential in L) error accumulation as expected.
4
In-context algorithm selection
We now show that transformers can perform various kinds of in-context algorithm selection, which
allows them to implement more complex ICL procedures by adaptively selecting different “base”
algorithms on different input sequences. We construct two general mechanisms: Post-ICL validation,
and Pre-ICL testing; See Figure 1 for a pictorial illustration.
4.1
Post-ICL validation mechanism
In our first mechanism, post-ICL validation, the transformer begins by implementing a train-validation
split D = (Dtrain, Dval), and running K base ICL algorithms on Dtrain. Let {fk}k∈[K] ⊂(Rd →R)
denote the K learned predictors, and
bLval(f) :=
1
|Dval|
P
(xi,yi)∈Dval ℓ(f(xi), yi)
(7)
denote the validation loss of any predictor f.
We show that (proof in Appendix I.1) a 3-layer transformer can output a predictor bf that achieves
nearly the smallest validation loss, and thus nearly optimal expected loss if bLval concentrates around
the expected loss L. Below, the input sequence H uses a generalized positional encoding pi :=
[0D−(d+3); 1; ti] in (3), where ti := 1 for i ∈Dtrain, ti := −1 for i ∈Dval, and tN+1 := 0.
Proposition 10 (In-context algorithm selection via train-validation split). Suppose that ℓ(·, ·) in (7)
is approximable by sum of relus (Definition D.1, which includes all C3-smooth bivariate functions).
Then there exists a 3-layer transformer TFθ that maps (defining y′
i = yi1{i < N + 1})
hi = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti]
→
h′
i = [xi; y′
i; ∗; bf(xi); 1; ti], i ∈[N + 1],
where the predictor bf : Rd →R is a convex combination of {fk : bLval(fk) ≤mink⋆∈[K] bLval(fk⋆) +
γ}. As a corollary, for any convex risk L : (Rd →R) →R, bf satisfies
L( bf) ≤mink⋆∈[K] L(fk⋆) + maxk∈[K]
bLval(fk) −L(fk)
 + γ.
Ridge regression with in-context regularization selection
As an example, we use Proposition 10
to construct a transformer to perform in-context ridge regression with regularization selection ac-
cording to the unregularized validation loss bLval(w) :=
1
2|Dval|
P
(xi,yi)∈Dval (⟨w, xi⟩−yi)2 (proof
in Appendix I.2). Let λ1, . . . , λK ≥0 be K fixed regularization strengths.
Theorem 11 (Ridge regression with in-context regularization selection). There exists a transformer
with O(log(1/ε)) layers and O(K) heads such that the following holds: On any (D, xN+1) well-
conditioned (cf. (5)) for all {λk}k∈[K], it outputs byN+1 = ⟨bw, xN+1⟩, where
dist

bw, conv{bwλk
ridge,train : bLval(bwλk
ridge,train) ≤mink⋆∈[K] bLval(bwλk⋆
ridge,train) + γ}

≤ε.
Above, bwλ
ridge,train denotes the solution to (ICRidge) on the training split Dtrain.
7

4.1.1
Nearly Bayes-optimal ICL on noisy linear models with mixed noise levels
We build on Theorem 11 to show that transformers can perform nearly Bayes-optimal ICL when data
come from noisy linear models with a mixture of K different noise levels σ1, . . . , σK > 0.
Concretely, consider the following data generating model, where we first sample P = Pw⋆,σk ∼π
from k ∼Λ ∈∆([K]), w⋆∼N(0, Id/d), and then sample data {(xi, yi)}i∈[N+1]
iid
∼Pw⋆,σk as
Pw⋆,σk : xi ∼N(0, Id),
yi = ⟨xi, w⋆⟩+ N(0, σ2
k).
For any fixed (N, d), consider the Bayes risk for predicting yN+1 under this model:
BayesRiskπ := infA Eπ
 1
2(A(D)(xN+1) −yN+1)2
.
By standard Bayesian calculations, the above Bayes risk is attained when A is a certain mixture of K
ridge regressions with regularization λk = dσ2
k/N; however, the mixing weights depend on D in
a highly non-trivial fashion (see Appendix J.2 for a derivation). By using the post-ICL validation
mechanism in Theorem 11, we construct a transformer that achieves nearly the Bayes risk.
Theorem 12 (Nearly Bayes-optimal ICL; Informal version of Theorem J.1). For sufficiently large
N, d, there exists a transformer with O(log N) layers and O(K) heads such that on the above model,
it outputs a prediction byN+1 that is nearly Bayes-optimal:
Eπ
 1
2(yN+1 −byN+1)2
≤BayesRiskπ + O
 (log K/N)1/3
.
(8)
In particular, Theorem 12 applies in the proportional setting where N, d are large and N/d =
Θ(1) [22], in which case BayesRiskπ = Θ(1), and thus the transformer achieves vanishing excess
risk relative to the Bayes risk at large N.
This substantially strengthens the results of Akyürek et al. [2], who empirically find that transformers
can achieve nearly Bayes risk under any fixed noise level. By contrast, Theorem 12 shows that a single
transformer can achieve nearly Bayes risk even under a mixture of K noise levels, with quantitative
guarantees. Also, our proof in fact gives a stronger guarantee: The transformer approaches the
individual Bayes risks on all K noise levels simultaneously (in addition to the overall Bayes risk for
k ∼Λ as in Theorem 12). We demonstrate this empirically in Section 6 (cf. Figure 3b & 2).
Exact Bayes predictor vs. Post-ICL validation mechanism
As BayesRiskπ is the theoretical
lower bound for the risk of any possible ICL algorithm, Theorem 12 implies that our transformer
performs similarly as the exact Bayes estimator5. Notice that our construction builds on the (generic)
post-ICL validation mechanism, rather than a direct attempt of approximating the exact Bayes
predictor, whose structure may vary significantly case-by-case. This highlights post-ICL validation
as a promising mechanism for approximating the Bayes predictor on broader classes of problems
beyond noisy linear models, which we leave as future work.
Generalized linear models with adaptive link function selection
As another example of the
post-ICL validation mechanism, we construct a transformer that can learn a generalized linear model
with adaptively chosen link function for the particular ICL instance; see Theorem J.2.
4.2
Pre-ICL testing mechanism
In our second mechanism, pre-ICL testing, the transformer runs a distribution testing procedure on the
input sequence to determine the right ICL algorithm to use. While the test (and thus the mechanism
itself) could in principle be general, we focus on cases where the test amounts to computing some
simple summary statistics of the input sequence.
To showcase pre-ICL testing, we consider the toy problem of selecting between in-context regression
and in-context classification, by running the following binary type check on the input labels {yi}i∈[N].
Ψbinary(D) = 1
N
N
X
i=1
ψ(yi),
ψ(y) :=



1,
y ∈{0, 1},
0,
y ̸∈[−ε, ε] ∪[1 −ε, 1 + ε],
linear interpolation,
otherwise.
5By the Bayes risk decomposition for square loss, (8) implies that E[(byN+1−byBayes
N+1)2] ≤O((log K/N)1/3).
8

Lemma 13. There exists a single attention layer with 6 heads that implements Ψbinary exactly.
Using this test, we construct a transformer that performs logistic regression when labels are binary,
and linear regression with high probability if the label admits a continuous distribution.
Proposition 14 (Adaptive regression or classification; Informal version of Proposition I.4). There
exists a transformer with O(log(1/ε)) layers such that the following holds: On any D such that
yi ∈{0, 1}, it outputs byN+1 that ε-approximates the prediction of in-context logistic regression.
By contrast, for any distribution P whose marginal distribution of y is not concentrated around {0, 1},
with high probability (over D), byN+1 ε-approximates the prediction of in-context least squares.
The proofs can be found in Appendix I.3. We additionally show that transformers can implement
more complex tests such as a linear correlation test, which can be useful in certain scenarios such as
“confident linear regression” (predict only when the signal-to-noise ratio is high); see Appendix I.4.
5
Analysis of pretraining
Building on the expressivity results in Section 3 & 4, we provide the first line of polynomial sample
complexity results for pretraining transformers to perform ICL (including with in-context algorithm
selection). We begin by providing a generic generalization guarantee for pretraining transformers.
Consider the pretraining ERM problem (TF-ERM), which minimizes the pretraining risk bLicl(·) over
n pretraining sequences. Let Licl(·) denote the corresponding population risk.
Theorem 15 (Generalization of transformers; Informal version of Theorem K.1). The solution bθ
to (TF-ERM) over transformers with L layers, M heads per layer, and hidden dimension D′ satisfies
Licl(bθ) ≤inf
θ Licl(θ) + e
O
 r
L2(MD2 + DD′)
n
!
.
Theorem 15 builds on standard uniform concentration analysis via chaining (Proposition B.4).
Combining Theorem 15 with the in-context linear regression construction in Theorem 4 gives the
following end-to-end result on the excess in-context prediction risk of trained transformers.
Theorem 16 (Pretraining transformers for in-context linear regression; Informal version of The-
orem K.2). Under Assumption A and N
≥
e
O(d), the solution bθ to (TF-ERM) with L =
O(κ log(κN/σ)) layers, M = 3 heads, D′ = 0 (attention-only as in Theorem 4) achieves small
excess ICL risk over the best linear predictor w⋆
P := EP[xx⊤]−1EP[xy] for each P:
Licl(bθ) −EP∼πE(x,y)∼P
1
2(y −⟨w⋆
P, x⟩)2

≤e
O
 r
κ2d2
n
+ dσ2
N
!
,
See Appendix K.2 for similar results in several additional settings.
6
Experiments
We test our theory by studying the ICL and in-context algorithm selection capabilities of transformers,
using the encoder-based architecture in our theoretical constructions (Definition 3). Due to limited
space, additional experimental details can be found in Appendix M.1. Results with a decoder
architecture as in [31, 47] (including the setup of Figure 2) can be found in Appendix M.2.
Training data distributions and evaluation
We train a 12-layer transformer, with two modes for
the training sequence (instance) distribution π. In the “base” mode, similar to [31, 2, 86, 47], we
sample the training instances from one of the following base distributions (tasks), where we first
sample P = Pw⋆∼π by sampling w⋆∼N(0, Id/d), and then sample {(xi, yi)}i∈[N+1]
iid
∼Pw⋆as
xi
iid
∼N(0, Id), and yi from one of the following models studied in Section 3:
1. Linear model: yi = ⟨w⋆, xi⟩;
2. Noisy linear model: yi = ⟨w⋆, xi⟩+ σzi, where σ > 0 is a fixed noise level, and zi ∼N(0, 1).
9

(a) Base ICL capabilities
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Loss
linear_regression
noisy_reg_noise_1
noisy_reg_noise_2
sparse_reg
linear_classification
Transformer
Least Squares
Averaging
3-NN
ridge_lam_1
ridge_lam_2
Lasso_lam=1
Lasso_lam=0.1
Lasso_lam=0.01
Lasso_lam=0.001
Logistic Regression
(b) Noisy reg with two noises
0.10
0.15
0.20
0.25
0.30
noisy_reg_noise_1
0.6
0.8
1.0
1.2
1.4
noisy_reg_noise_2
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
ridge analytical
Bayes_err_noise_1
Bayes_err_noise_2
(c) Reg + Classification
0.0
0.5
1.0
1.5
regression_square_loss
0.20
0.25
0.30
0.35
classification_error
TF_alg_select
TF_reg
TF_cls
Least Squares
Averaging
3-NN
Figure 3: ICL capabilities of the transformer architecture used in our theoretical constructions. (a) On five
representative base tasks, transformers approximately match the best baseline algorithm for each task, when
pretrained on the corresponding task. (b,c) A single transformer TF_alg_select simultaneously approaches
the performance of the strongest baseline algorithm on two separate tasks: (b) noisy linear regression with
two different noise levels σ ∈{0.1, 0.5}, and (c) adaptively selecting between regression and classification.
3. Sparse linear model: yi = ⟨w⋆, xi⟩with ∥w⋆∥0 ≤s, where s < d is a fixed sparsity level, and
in this case we sample w⋆from a special prior supported on s-sparse vectors;
4. Linear classification model: yi = sign(⟨w⋆, xi⟩).
These base tasks have been empirically investigated by Garg et al. [31], though we remark that
our architecture (used in our theory) differs from theirs in several aspects, such as encoder-based
architecture instead of decoder-based, and ReLU activation instead of softmax. All experiments use
d = 20. We choose σ ∈{σ1, σ2} = {0.1, 0.5} and N = 20 for noisy linear regression, s = 3 and
N = 10 for sparse linear regression, and N = 40 for linear regression and linear classification.
In the “mixture” mode, π is the uniform mixture of two or more base distributions. We consider two
representative mixture modes studied in Section 4:
• Linear model + linear classification model;
• Noisy linear model with four noise levels σ ∈{0.1, 0.25, 0.5, 1}.
Transformers trained with the mixture mode will be evaluated on multiple base distributions simulta-
neously. When the base distributions are sufficiently diverse, a transformer performing well on all of
them will likely be performing some level of in-context algorithm selection. We evaluate transformers
against standard machine learning algorithms in context (for each task respectively) as baselines.
Results
Figure 3a shows the ICL performance of transformers on five base tasks, within each the
transformer is trained on the same task. Transformers match the best baseline algorithm in four out
of the five cases, except for the sparse regression task where the Transformer still outperforms least
squares and matches Lasso with some choices of λ (thus utilizing sparsity to some extent). This
demonstrates the strong ICL capability of the transformer architecture considered in our theory.
Figure 3b & 3c examine the in-context algorithm selection capability of transformers, on noisy
linear regression with two different noise levels (Figure 3b), and regression + classification (Figure
3c). In both figures, the transformer trained in the mixture mode (TF_alg_select) approaches the
best baseline algorithm on both tasks simultaneously. By contrast, transformers trained in the base
mode for one of the tasks perform well on that task but behave suboptimally on the other task as
expected. The existence of TF_alg_select showcases a single transformer that performs well on
multiple tasks simultaneously (and thus has to perform in-context algorithm selection to some extent),
supporting our theoretical results in Section 4.
7
Conclusion
This work shows that transformers can perform complex in-context learning procedures with strong
in-context algorithm selection capabilties, by both explicit theoretical constructions and experiments.
We believe our work opens up many exciting directions, such as (1) more mechanisms for in-context
algorithm selection; (2) Bayes-optimal ICL on other problems by either the post-ICL validation
mechanism or new approaches; (3) understanding the internal workings of transformers performing
in-context algorithm selection; (4) other mechanisms for implementing complex ICL procedures
beyond in-context algorithm selection; (5) further statistical analyses, e.g. of pretraining. Besides,
this work focuses on the transformer architecture; alternative sequence-to-sequence architectures
(such as RNNs) are beyond our scope but would be interesting directions for future work.
10

Acknowledgment
The authors would like to thank Tengyu Ma and Jason D. Lee for the many insightful discussions. S.
Mei is supported in part by NSF DMS-2210827 and NSF CCF-2315725.
References
[1] A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient
methods for high-dimensional statistical recovery. Advances in Neural Information Processing
Systems, 23, 2010.
[2] E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is
in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661,
2022.
[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,
2016.
[4] F. Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629–681, 2017.
[5] Y. Bai, M. Chen, P. Zhou, T. Zhao, J. Lee, S. Kakade, H. Wang, and C. Xiong. How important is
the train-validation split in meta-learning? In International Conference on Machine Learning,
pages 543–553. PMLR, 2021.
[6] Y. Bai, S. Mei, H. Wang, and C. Xiong. Don’t just blame over-parametrization for over-
confidence: Theoretical analysis of calibration in binary classification. In International
Conference on Machine Learning, pages 566–576. PMLR, 2021.
[7] J. Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149–198, 2000.
[8] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.
Convex optimization in signal processing and communications, pages 42–88, 2009.
[9] S. Bengio, Y. Bengio, J. Cloutier, and J. Gescei. On the optimization of a synaptic learning
rule. In Optimality in Biological and Artificial Networks?, pages 281–303. Routledge, 2013.
[10] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to
recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.
[11] S. Bhattamishra, A. Patel, and N. Goyal. On the computational power of transformers and its
implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020.
[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in
neural information processing systems, 33:1877–1901, 2020.
[13] S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends® in
Machine Learning, 8(3-4):231–357, 2015.
[14] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,
Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712, 2023.
[15] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and
F. Hill. Data distributional properties drive emergent in-context learning in transformers.
Advances in Neural Information Processing Systems, 35:18878–18891, 2022.
[16] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and
I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances
in neural information processing systems, 34:15084–15097, 2021.
11

[17] K. Chua, Q. Lei, and J. D. Lee. How fine-tuning allows for effective meta-learning. Advances
in Neural Information Processing Systems, 34:8871–8884, 2021.
[18] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can gpt learn in-context? language
models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559,
2022.
[19] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Incremental learning-to-learn with statistical
guarantees. arXiv preprint arXiv:1803.08089, 2018.
[20] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Learning to learn around a common mean.
Advances in Neural Information Processing Systems, 31, 2018.
[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[22] E. Dobriban and S. Wager. High-dimensional asymptotics of prediction: Ridge regression and
classification. The Annals of Statistics, 46(1):247–279, 2018.
[23] L. Dong, S. Xu, and B. Xu. Speech-transformer: a no-recurrence sequence-to-sequence model
for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal
processing (ICASSP), pages 5884–5888. IEEE, 2018.
[24] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey for
in-context learning. arXiv preprint arXiv:2301.00234, 2022.
[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-
hghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[26] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
[27] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-
attention mechanisms. In International Conference on Machine Learning, pages 5793–5831.
PMLR, 2022.
[28] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen,
T. Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits
Thread, 2021.
[29] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International conference on machine learning, pages 1126–1135. PMLR, 2017.
[30] C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online meta-learning. In International
Conference on Machine Learning, pages 1920–1930. PMLR, 2019.
[31] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a
case study of simple function classes. Advances in Neural Information Processing Systems,
35:30583–30598, 2022.
[32] A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped trans-
formers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.
[33] M. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of
the Association for Computational Linguistics, 8:156–171, 2020.
[34] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In
Artificial Neural Networks—ICANN 2001: International Conference Vienna, Austria, August
21–25, 2001 Proceedings 11, pages 87–94. Springer, 2001.
[35] N. Hollmann, S. Müller, K. Eggensperger, and F. Hutter. Tabpfn: A transformer that solves
small tabular classification problems in a second. arXiv preprint arXiv:2207.01848, 2022.
12

[36] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A
survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149–5169,
2021.
[37] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep
attention networks. In International Conference on Machine Learning, pages 4376–4386.
PMLR, 2020.
[38] D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pages 9–1. JMLR Workshop and Conference Proceedings,
2012.
[39] S. Jelassi, M. E. Sander, and Y. Li. Vision transformers provably learn spatial structure. arXiv
preprint arXiv:2210.09221, 2022.
[40] K. Ji, J. D. Lee, Y. Liang, and H. V. Poor. Convergence of meta-learning with task-specific
adaptation over partial parameters. Advances in Neural Information Processing Systems, 33:
11490–11500, 2020.
[41] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool,
R. Bates, A. vZídek, A. Potapenko, et al. Highly accurate protein structure prediction with
alphafold. Nature, 596(7873):583–589, 2021.
[42] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear
and single index models with isotonic regression. Advances in Neural Information Processing
Systems, 24, 2011.
[43] M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar. Adaptive gradient-based meta-learning
methods. Advances in Neural Information Processing Systems, 32, 2019.
[44] L. Kirsch and J. Schmidhuber. Meta learning backpropagation and improving it. Advances in
Neural Information Processing Systems, 34:14122–14134, 2021.
[45] L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by
meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.
[46] K. Li and J. Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
[47] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: General-
ization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067,
2023.
[48] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to
automata. arXiv preprint arXiv:2210.10749, 2022.
[49] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context
examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.
[50] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786, 2021.
[51] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand, R. R. Eguchi, P.-S. Huang,
and R. Socher.
Progen: Language modeling for protein generation.
arXiv preprint
arXiv:2004.03497, 2020.
[52] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning.
Journal of Machine Learning Research, 17(81):1–32, 2016.
[53] P. McCullagh. Generalized linear models. Routledge, 2019.
[54] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. The
Annals of Statistics, 46(6A):2747–2774, 2018.
13

[55] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Noisy channel language model prompting
for few-shot text classification. arXiv preprint arXiv:2108.04106, 2021.
[56] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.
arXiv preprint arXiv:2110.15943, 2021.
[57] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer.
Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837, 2022.
[58] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner.
arXiv preprint arXiv:1707.03141, 2017.
[59] S. Müller, N. Hollmann, S. P. Arango, J. Grabocka, and F. Hutter. Transformers can do
bayesian inference. arXiv preprint arXiv:2112.10510, 2021.
[60] T. Nagler.
Statistical foundations of prior-data fitted networks.
arXiv preprint
arXiv:2305.11097, 2023.
[61] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings
1992] IJCNN International Joint Conference on Neural Networks, volume 1, pages 437–442.
IEEE, 1992.
[62] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for
high-dimensional analysis of m-estimators with decomposable regularizers. 2012.
[63] Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[64] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann,
A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint
arXiv:2209.11895, 2022.
[65] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[66] N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and trends® in Optimization, 1
(3):127–239, 2014.
[67] J. Pérez, J. Marinkovi´c, and P. Barceló. On the turing completeness of modern neural network
architectures. arXiv preprint arXiv:1901.03429, 2019.
[68] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding
by generative pre-training. 2018.
[69] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021.
[70] A. Raventos, M. Paul, F. Chen, and S. Ganguli. The effects of pretraining task diversity
on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and
Empirical Understanding of Foundation Models, 2023.
[71] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International
conference on learning representations, 2017.
[72] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies
on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
[73] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,
Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,
2022.
[74] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. arXiv
preprint arXiv:2112.08633, 2021.
14

[75] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with
memory-augmented neural networks. In International conference on machine learning, pages
1842–1850. PMLR, 2016.
[76] N. Saunshi, A. Gupta, and W. Hu. A representation learning perspective on the importance of
train-validation splitting in meta-learning. In International Conference on Machine Learning,
pages 9333–9343. PMLR, 2021.
[77] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.
[78] K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in
transformer. arXiv preprint arXiv:2302.06461, 2023.
[79] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention
learns. arXiv preprint arXiv:2103.07601, 2021.
[80] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems, 30, 2017.
[81] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 2012.
[82] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996.
[83] N. Tripuraneni, M. Jordan, and C. Jin. On the theory of transfer learning: The importance of
task diversity. Advances in neural information processing systems, 33:7852–7862, 2020.
[84] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.
[85] R. Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
[86] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov,
and M. Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677, 2022.
[87] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge university press, 2019.
[88] X. Wang, S. Yuan, C. Wu, and R. Ge. Guarantees for tuning the step size using a learning-
to-learn approach. In International Conference on Machine Learning, pages 10981–10990.
PMLR, 2021.
[89] C. Wei, Y. Chen, and T. Ma. Statistically meaningful approximation: a case study on approxi-
mating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021.
[90] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,
D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.
[91] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al.
Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846,
2023.
[92] G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In International Conference
on Machine Learning, pages 11080–11090. PMLR, 2021.
[93] M. Wortsman, J. Lee, J. Gilmer, and S. Kornblith. Replacing softmax with relu in vision
transformers. arXiv preprint arXiv:2309.08586, 2023.
[94] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as
implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
15

[95] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process
bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021.
[96] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers
really perform badly for graph representation? Advances in Neural Information Processing
Systems, 34:28877–28888, 2021.
[97] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal
approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.
[98] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling trans-
formers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
[99] Y. Zhang, B. Liu, Q. Cai, L. Wang, and Z. Wang. An analysis of attention via the lens of
exchangeability and latent variable models. arXiv preprint arXiv:2212.14852, 2022.
[100] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot
performance of language models. In International Conference on Machine Learning, pages
12697–12706. PMLR, 2021.
[101] X. Zuo, Z. Chen, H. Yao, Y. Cao, and Q. Gu. Understanding train-validation split in meta-
learning with neural networks. In The Eleventh International Conference on Learning Repre-
sentations, 2023. URL https://openreview.net/forum?id=JVlyfHEEm0k.
A
Related work
In-context learning
The in-context learning (ICL) capability of large language models (LLMs) has
gained significant attention since demonstrated on GPT-3 Brown et al. [12]. A number of subsequent
empirical studies have contributed to a better understanding of the capabilities and limitations of
ICL in LLM systems, which include but are not limited to [49, 55, 56, 50, 100, 74, 72, 28, 45, 91].
For an overview of ICL, see the survey by Dong et al. [24] which highlights some key findings and
advancements in this direction.
A line of recent work investigates why and how LLMs perform ICL [94, 31, 86, 2, 18, 32, 47, 70]. In
particular, Xie et al. [94] propose a Bayesian inference framework explaining how ICL works despite
formatting differences between training and inference distributions. Garg et al. [31] show empirically
that transformers could be trained from scratch to perform ICL of linear models, sparse linear models,
two-layer neural networks, and decision trees. Li et al. [47] analyze the generalization error of
trained ICL transformers from a stability viewpoint. They also experimentally show that transformers
could perform “in-context model selection” (conceptually similar to in-context algorithm selection
considered in this work) in specific tasks and presented related theoretical hypotheses. However,
they do not provide concrete mechanisms or constructions for in-context model selection. A recent
work [99] shows that pretrained transformers can perform Bayesian inference in latent variable models,
which may also be interpreted as a mechanism for ICL. Our experimental findings extend these
results by unveiling and demonstrating the in-context algorithm selection capabilities of transformers.
Closely related to our theoretical results are [86, 2, 18, 32], which show (among many things) that
transformers can perform ICL by simulating gradient descent. However, these results do not provide
quantitative error bounds for simulating multi-step gradient descent, and only handle linear regression
models or their simple variants. Among these works, Akyürek et al. [2] showed that transformers can
implement learning algorithms for linear models based on gradient descent and closed-form ridge
regression; it also presented preliminary evidence that learned transformers perform ICL similar
to Bayes-optimal ridge regression. Our work builds upon and substantially extends this line of
work by (1) providing a more efficient construction for in-context gradient descent; (2) providing
an end-to-end theory with additional results for pretraining and statistical power; (3) analyzing a
broader spectrum of ICL algorithms, including least squares, ridge regression, Lasso, convex risk
minimization for generalized linear models, and gradient descent on two-layer neural networks; and
(4) constructing more complex ICL procedures using in-context algorithm selection.
When in-context data are generated from a prior, the Bayes risk is a theoretical lower bound for the
risk of any possible ICL algorithm, including transformers. Xie et al. [94], Akyürek et al. [2] observe
16

that learned transformers behave closely to the Bayes predictor on a variety of tasks such as hidden
Markov models [94] and noisy linear regression with a fixed noise level [2, 47]. Using the in-context
algorithm selection mechanism (more precisely the post-ICL validation mechanism), we show that
transformers can perform nearly-Bayes optimal ICL in noisy linear models with mixed noise levels (a
strictly more challenging task than considered in [2, 47]), with both concrete theoretical guarantees
(Section 4.1.1) and empirical evidence (Figure 2 & 3b). Complementary to these works, a line of work
on “prior-data fitted networks” [59, 60, 35] also empirically demonstrates the Bayesian optimality of
transformers in various settings. Our expressivity results support these empirical findings and are
applicable beyond the Bayesian setting, e.g. for providing frequentist in-context prediction guarantees
for transformers.
Transformers and its theory
The transformer architecture, introduced by [84], has revolutionized
natural language processing and been adopted in most of the recently developed large language
models such as BERT and GPT [68, 21, 12]. Broaderly, transformers have demonstrated remarkable
performance in many other fields of artificial intelligence such as computer vision, speech, graph
processing, reinforcement learning, and biological applications [23, 25, 51, 69, 96, 16, 41, 73, 65, 14].
Towards a better theoretical understanding, recent work has studied the capabilities [97, 67, 37, 95,
11, 98, 48], limitations [33, 10], and internal workings [28, 79, 92, 27, 64] of transformers.
We remark that the transformer architecture used in our theoretical constructions differs from the
standard one by replacing the softmax activation (in the attention layers) with a (normalized) ReLU
function. Transformers with ReLU activations is experimentally studied in the recent work of Shen
et al. [78], who find that they perform as well as the standard softmax activation in many NLP tasks.
Meta-learning
Training models (such as transformers) to perform ICL can be viewed as an
approach for the broader problem of learning-to-learn or meta-learning [77, 61, 81]. A number of
other approaches has been studied extensively for this problem, including (and not limited to) training
a meta-learner on how to update the parameters of a downstream learner [9, 46], learning parameter
initializations that quickly adapt to downstream tasks [29, 71], learning latent embeddings that allow
for effective similarity search [80]. Most relevant to the ICL setting are approaches that directly take
as input examples from a downstream task and a query input and produce the corresponding output
[34, 58, 75, 44]. For a comprehensive overview, see the survey [36].
Theoretical aspects of meta-learning have received significant recent interest [7, 52, 26, 83, 19, 30,
43, 40, 88, 20, 5, 76, 17, 101]. In particular, [52, 26, 83] analyzed the benefit of multi-task learning
through a representation learning perspective, and [88, 20, 5, 76, 101] studied the statistical properties
of learning the parameter initialization for downstream tasks.
Techniques
We build on various existing techniques from the statistics and learning theory literature
to establish our approximation and generalization guarantees for transformers. For the approximation
component, we rely on a technical result of Bach [4] on the approximation power of ReLU networks.
We use this result to show that transformers can approximate gradient descent (GD) on a broad range
of loss functions, substantially extending the results of [86, 2, 18] who primarily consider the square
loss. The recent work of Giannou et al. [32] also approximates GD with general loss functions by
transformers, though using a different technique of forcing the softmax activations to act as sigmoids.
Our analyses of Lasso and generalized linear models build on [87, 62, 1, 54]. Our generalization
bound for transformers (used in our pretraining results) build on a chaining argument [87].
B
Technical tools
Additional notation for proofs
We say a random variable X is σ2-sub-Gaussian (or SG(σ)
interchangeably) if E[exp(X2/σ2)] ≤2. A random vector x ∈Rd is σ2-sub-Gaussian if ⟨v, x⟩
is σ2-sub-Gaussian for all ∥v∥2 = 1. A random variable X is K-sub-Exponential (or SE(K)
interchangeably) if E[exp(|X| /K)] ≤2.
B.1
Concentration inequalities
Lemma B.1. Let β ∼N(0, Id/d). Then we have
P

∥β∥2
2 ≥(1 + δ)2
≤e−dδ2/2.
17

Lemma B.2 (Theorem 6.1 of [87]). Let X = [Xij] ∈Rn×d be a Gaussian random matrix with
Xij ∼N(0, 1). Let σmin(X) and σmin(X) be the minimum and maximum singular value of X,
respectively. Then we have
P

σmax(X)/√n ≥1 +
p
d/n + δ

≤e−nδ2/2,
P

σmin(X)/√n ≤1 −
p
d/n −δ

≤e−nδ2/2.
The following lemma is a standard result of covariance concentration, see e.g. [85, Theorem 4.6.1].
Lemma B.3. Suppose that x1, · · · , xN are independent d-dimensional K-sub-Gaussian random
vectors. Then as long as N ≥C0d, with probability at least 1 −exp(−N/C0) we have

1
N
N
X
i=1
xix⊤
i

op
≤8K2,
where C0 is a universal constant.
Lemma B.4. For random matrix X = [xij] ∈RN×d with xij
iid
∼N(0, 1) and ε = [εi] ∈RN with
εi
iid
∼N(0, σ2), it holds that
P
X⊤ε

∞≥
p
8Nσ2 log(2d/δ)

≤δ + exp(−N/2).
Proof. We consider uj := [xij]i ∈RN, then
X⊤ε

∞= maxi∈[d] |⟨uj, ε⟩|. Notice that the
random variables ⟨u1, ε⟩, · · · , ⟨ud, ε⟩are independent N(0, ∥ε∥2
2), and hence
P

max
i∈[d] |⟨uj, ε⟩| ≥t
 ε

≤2d exp
 
−
t2
2 ∥ε∥2
2
!
.
Further, by Lemma B.1, P(∥ε∥2 ≥2σ
√
N) ≤exp(−N/2). Taking t =
p
8Nσ2 log(2d/δ)
completes the proof.
B.2
Approximation theory
For any signed measure µ over a space W, let TV(µ) :=
R
W |dµ(w)| ∈[0, ∞] denote its total
measure. Recall σ(·) = ReLU(·) is the standard relu activation, and Bk
∞(R) = [−R, R]k denotes
the standard ℓ∞ball in Rk with radius R > 0.
Definition B.1 (Sufficiently smooth k-variable function). We say a function g : Rk →R is (R, Cℓ)-
smooth, if for s = ⌈(k −1)/2⌉+ 2, g is a Cs function on Bk
∞(R), and
sup
z∈Bk∞(R)
∇ig(z)

∞=
sup
z∈Bk
∞(R)
max
j1,...,ji∈[k] |∂xj1...xjig(x)| ≤Li
for all i ∈{0, 1, . . . , s}, with max0≤i≤s LiRi ≤Cℓ.
The following result for expressing smooth functions as a random feature model with relu activation
is adapted from Bach [4, Proposition 5].
Lemma B.5 (Expressing sufficiently smooth functions by relu random features). Suppose func-
tion g : Rk →R is (R, Cℓ) smooth.
Then there exists a signed measure µ over W =
{w ∈Rk+1 : ∥w∥1 = 1} such that
g(x) =
Z
W
1
Rσ(w⊤[x; R])dµ(w),
∀x ∈X
and TV(µ) ≤C(k)Cℓ, where C(k) < ∞is a constant that only depends on k.
Lemma B.6 (Uniform finite-neuron approximation). Let X be a space equipped with a distance
function dX (·, ·) : X × X →R≥0. Suppose function g : X →R is given by
g(x) =
Z
W
ϕ(x; w)dµ(w),
18

where ϕ(·; ·) : X × W →[−B, B] is L-Lipschitz (in dX ) in the first argument, and µ is a signed
measure over W with finite total measure A = TV(µ) < ∞. Then for any ε > 0, there exists
α1, · · · , αK ∈{±1}, w1, · · · , wK ∈W with K = O(A2B2 log N(X, dX ,
ε
3AL)/ε2), such that
sup
x∈X
g(x) −A
K
K
X
i=1
αiϕ(x; wi)
 ≤ε,
where N(X, dX ,
ε
3AL) denotes the (
ε
3AL)-covering number of X in dX .
Proof. Let α(w) := sign(dµ(w)) ∈{±1} denote the sign of the density dµ(w). We have
g(x) = A
Z
W
α(w)ϕ(x; w) × |dµ(w)|
A
.
(9)
Note that |dµ(w)|/A is the density of a probability distribution over W. Thus for any x ∈X,
as long as K ≥O(A2B2 log(1/δ)/ε2), we can sample w1, . . . , wK
iid
∼|dµ(·)|/A, and obtain by
Hoeffding’s inequality that with probability at least 1 −δ,
g(x) −A
K
K
X
i=1
α(wi)ϕ(x; wi)
 ≤ε.
Let N(
ε
3AL)
:=
N(X, dX ,
ε
3AL) for shorthand.
By union bound, as long as K
≥
O(A2B2 log(N(
ε
3AL)/δ)/ε2), we have with probability at least 1−δ that for every bx in the covering
set corresponding to N(
ε
3AL),
g(bx) −A
K
K
X
i=1
α(wi)ϕ(bx; wi)
 ≤ε/3.
Taking δ = 1/2 (for which K = O(A2B2 log N(
ε
3AL)/ε2)), by the probabilistic method, there
exists a deterministic set {wi}i∈[K] ⊂W and {αi := α(wi)}i∈[K] ∈{±1} such that the above
holds.
Next, note that both g (by (9)) and the function x 7→A
K
PK
i=1 α(wi)ϕ(x; wi) are (AL)-Lipschitz.
Therefore, for any x ∈X, taking bx to be the point in the covereing set with dX (x, bx) ≤
ε
3AL, we
have
g(x) −A
K
K
X
i=1
α(wi)ϕ(x; wi)

≤|g(x) −g(bx)| +
g(bx) −A
K
K
X
i=1
α(wi)ϕ(bx; wi)
 +

A
K
K
X
i=1
α(wi)ϕ(bx; wi) −A
K
K
X
i=1
α(wi)ϕ(x; wi)

≤AL ·
ε
3AL + ε
3 + AL ·
ε
3AL = ε.
This proves the lemma.
Proposition B.1 (Approximating smooth k-variable functions). For any εapprox > 0, R ≥1,
Cℓ> 0, we have the following: Any (R, Cℓ)-smooth function (Definition B.1) g : Rk →R
is (εapprox, R, M, C)-approximable by sum of relus (Definition D.1) with M ≤C(k)C2
ℓlog(1 +
Cℓ/εapprox)/ε2
approx) and C ≤C(k)Cℓ, where C(k) > 0 is a constant that depends only on k. In
other words, there exists
f(z) =
M
X
m=1
cmσ(a⊤
m[z; 1])
with
M
X
m=1
|cm| ≤C,
max
m∈[M] ∥am∥1 ≤1,
such that supz∈[−R,R]k |f(z) −g(z)| ≤εapprox.
19

Proof. As function g : Bk
∞(R) →R is (R, Cℓ)-smooth, we can apply Lemma B.5 to obtain that
there exists a signed measure µ over W := {w ∈Rk+1 : ∥w∥1 ≤1} such that
g(z) =
Z
W
1
Rσ(w⊤[z; R])dµ(w),
∀z ∈[−R, R]k,
and A = TV(µ) ≤C(k)Cℓwhere C(k) > 0 denotes a constant depending only on k.
We now apply Lemma B.6 to approximate the above random feature by finitely many neurons. Let
x := [z; R] ∈X := [−R, R]k × {R}. Then, the function ϕ(x; w) := 1
Rσ(w⊤x) = σ( 1
Rw⊤[z; R])
is bounded by B = 1 and (1/R)-Lipschitz in x (in the standard ℓ∞-distance). Further, we have
log N(X, ∥· −·∥∞, εapprox
3A/R ) ≤O(k log(1 + A/εapprox)). We can thus apply Lemma B.6 to obtain
that, for
M = O
 kA2 log(1 + A/εapprox)/ε2
approx

= C(k)C2
ℓlog(1 + Cℓ/εapprox)/ε2
approx,
there exists α = {αm}m∈[M] ⊂{±1} and W = {wm}m∈[M] ⊂W = {w ∈Rk+1 : lonew = 1}
such that
sup
z∈[−R,R]2 |g(z) −fα,W(z)| ≤εapprox,
where (recalling z = [s; t])
fα,W(z) = A
M
M
X
m=1
αmσ
 1
Rw⊤
m[z; R]

=
M
X
m=1
Aαm
M
| {z }
cm
σ
 h 1
Rwm,1:k; wm,k+1
⊤
|
{z
}
a⊤
m
[z; 1]).
Note that we have PM
m=1 |cm| = A ≤C(k)Cℓ, and ∥am∥1 ≤∥wm∥1 = 1. This is the desired
result.
B.3
Optimization
The following convergence result for minimizing a smooth and strongly convex function is standard
from the convex optimization literature, see e.g. Bubeck [13, Theorem 3.10].
Proposition B.2 (Gradient descent for smooth and strongly convex functions). Suppose L : Rd →R
is α-strongly convex and β-smooth for some 0 < α ≤β. Then, the gradient descent iterates
wt+1
GD := wt
GD −η∇L(wt
GD) with learning rate η = 1/β and initialization w0
GD ∈Rd satisfies for
any t ≥1,
wt
GD −w⋆2
2 ≤exp (−t/κ) ·
w0
GD −w⋆2
2 ,
L(wt
GD) −L(w⋆) ≤β
2 exp (−t/κ) ·
w0
GD −w⋆2
2 ,
where κ := β/α is the condition number of L, and w⋆:= arg minw∈Rd L(w) is the minimizer of L.
The following convergence result of proximal gradient descent (PGD) on convex composite mini-
mization problem is also standard, see e.g. [8].
Proposition B.3 (Proximal gradient descent for convex function). Suppose L = f + h, f : Rd →R
is convex and β-smooth for some β > 0, h : Rd →R is a simple convex function. Then, the proximal
gradient descent iterates wt+1
PGD := proxηh(wt
PGD −η∇f(wt
PGD)) with learning rate η = 1/β
and initialization w0
GD ∈Rd satisfies the following for any t ≥1:
1. {L(wt
PGD)} is a decreasing sequence.
2. For any minimizer w⋆∈arg minw∈Rd L(w),
L(wt+1
GD ) −L(w⋆) ≤β
2
wt
PGD −w⋆2
2 −
wt+1
PGD −w⋆2
2

,
and hence
n
∥wt
PGD −w⋆∥2
2
o
is also a decreasing sequence.
3. For k ≥1, t ≥0, it holds that
L(wt+k
GD ) −L(w⋆) ≤β
2k
wt
PGD −w⋆2
2 .
20

B.4
Uniform convergence
The following result is shown in [87, Section 5.6].
Theorem B.1. Suppose that ψ : [0, +∞) →[0, +∞) is a convex, non-decreasing function that
satisfies ψ(x + y) ≥ψ(x)ψ(y). For any random variable X, we consider the Orlicz norm induced
by ψ: ∥X∥ψ := inf {K > 0 : Eψ(|X| /K)} ≤1.
Suppose that {Xθ}θ is a zero-mean random process indexed by θ ∈Θ such that ∥Xθ −Xθ′∥ψ ≤
ρ(θ, θ′) for some metric ρ on the space Θ. Then it holds that
P
 
sup
θ,θ′∈Θ
|Xθ −Xθ′| ≤8(J + t)
!
≤
1
ψ(t/D) ∀t ≥0,
where D is the diameter of the metric space (Θ, ρ), and the generalized Dudley entropy integral J is
given by
J :=
Z D
0
ψ−1(N(δ; Θ, ρ))dδ,
where N(δ; Θ, ρ) is the δ-covering number of (Θ, ρ).
As a corollary of Theorem B.1, we have the following result.
Proposition B.4 (Uniform concentration bound by chaining). Suppose that {Xθ}θ∈Θ is a zero-mean
random process given by
Xθ := 1
N
N
X
i=1
f(zi; θ) −Ez[f(z; θ)],
where z1, · · · , zN are i.i.d samples from a distribution Pz such that the following assumption holds:
(a) The index set Θ is equipped with a distance ρ and diameter D. Further, assume that for
some constant A, for any ball Θ′ of radius r in Θ, the covering number admits upper bound
log N(δ; Θ′, ρ) ≤d log(2Ar/δ) for all 0 < δ ≤2r.
(b) For any fixed θ ∈Θ and z sampled from Pz, the random variable f(z; θ) is a SG(B0)-sub-
Gaussian random variable.
(c) For any θ, θ′ ∈Θ and z sampled from Pz, the random variable f(z; θ) −f(z; θ′) is a
SG(B1ρ(θ, θ′))-sub-Gaussian random variable.
Then with probability at least 1 −δ, it holds that
sup
θ∈Θ
|Xθ| ≤CB0
r
d log(2Aκ) + log(1/δ)
N
,
where C is a universal constant, and we denote κ = 1 + B1D/B0.
Furthermore, if we replace the SG in assumption (b) and (c) by SE, then with probability at least
1 −δ, it holds that
sup
θ∈Θ
|Xθ| ≤CB0
"r
d log(2Aκ) + log(1/δ)
N
+ d log(2Aκ) + log(1/δ)
N
#
.
Proof. Fix a D0 ∈(0, D] to be specified later. We pick a (D0/2)-covering Θ0 of Θ so that
log |Θ0| ≤d log(2AD/D0). Then, by the standard uniform covering of independent sub-Gaussian
random variables, we have with probability at least 1 −δ/2,
sup
θ∈Θ0
|Xθ| ≤CB0
r
d log(2AD/D0) + log(2/δ)
N
.
21

Assume that Θ0 = {θ1, · · · , θn}. For each j ∈[n], we consider Θj is the ball centered at θj of radius
D0 in (Θ, ρ). Then θ ∈Θj has diameter D0 and admits covering number bound log N(Θj, δ) ≤
d log(AD0/δ). Hence, we can apply Theorem B.1 with the process {Xθ}θ∈Θj, then
ψ = ψ2,
∥Xθ −Xθ′∥ψ ≤B1
√
N
ρ(θ, θ′),
and a simple calculation yields
P
 
sup
θ,θ′∈Θj
|Xθ −Xθ′| ≤C′B1D0
 r
d log(2A)
N
+ t
!!
≤2 exp(−Nt2) ∀t ≥0.
Therefore, we can let t ≤
p
log(2n/δ)/N in the above inequality and taking the union bound over
j ∈[n], and hence with probability at least 1 −δ/2, it holds that for all j ∈[n],
sup
θ,θ′∈Θj
|Xθ −Xθ′| ≤C′B1D0
r
2d log(2AD/D0) + log(4/δ)
N
.
Notice that for each θ ∈Θ, there exists j ∈[n] such that θ ∈Θj, and hence
|Xθ| ≤
Xθj
 +
Xθ −Xθj
 .
Thus, with probability at least 1 −δ, it holds
sup
θ∈Θ
|Xθ| ≤sup
θ∈Θ0
|Xθ| + sup
j
sup
θ∈Θj
Xθ −Xθj
 ≤C′′(B0 + B1D0)
r
d log(2AD/D0) + log(2/δ)
N
.
Taking D0 = D/κ completes the proof of SG case.
We next consider the SE case. The idea is the same as the SG case, but in this case we need to
consider the following Orlicz-norm:
ψN(t) := exp
 Nt2
t + 1

−1.
Then Bernstein’s inequality of SE random variables yields
∥Xθ −Xθ′∥ψN ≤C0B1ρ(θ, θ′)
for some universal constant C0. Therefore, we can repeat the argument above to deduce that with
probability at least 1 −δ, it holds
sup
θ∈Θ
|Xθ| ≤C′′(B0 + B1D0)
"r
d log(2AD/D0) + log(2/δ)
N
+ d log(2AD/D0) + log(2/δ)
N
#
.
Taking D0 = D/κ completes the proof.
B.5
Useful properties of transformers
The following result can be obtained immediately by “joining” the attention heads and MLP layers of
two single-layer transformers.
Proposition B.5 (Joining parallel single-layer transformers). Suppose that P1 : R(D0+D1)×N →
RD1×N, P2 : R(D0+D2)×N →RD2×N are two sequence-to-sequence functions that are implemented
by single-layer transformers, i.e. there exists θ1, θ2 such that
TFθ1 :H1 =
"
h(0)
i
h(1)
i
#
1≤i≤N
∈R(D0+D1)×N 7→

H(0)
P1(H1)

,
TFθ2 :H2 =
"
h(0)
i
h(2)
i
#
1≤i≤N
∈R(D0+D2)×N 7→

H(0)
P2(H2)

.
22

Then, there exists θ such that for H′ that takes form h′
i = [h(0)
i ; h(1)
i ; h(2)
i ], with h(0)
i
∈RD0, h(1)
i
∈
RD1, h(2)
i
∈RD2, we have
TFθ :H′ =


h(0)
i
h(1)
i
h(2)
i


1≤i≤N
∈R(D0+D1+D2)×N 7→


H(0)
P1(H1)
P2(H2)

.
Further, θ has at most M ≤M1 + M2 heads, D′ ≤D′
1 + D′
2 hidden dimension in its MLP layer,
and norm bound |||θ||| ≤|||θ1||| + |||θ2|||.
Proposition B.6 (Joining parallel multi-layer transformers). Suppose that P1 : R(D0+D1)×N →
RD1×N, P2 : R(D0+D2)×N →RD2×N are two sequence-to-sequence functions that are implemented
by multi-layer transformers, i.e. there exists θ1, θ2 such that
TFθ1 :H1 =
"
h(0)
i
h(1)
i
#
1≤i≤N
∈R(D0+D1)×N 7→

H(0)
P1(H1)

,
TFθ2 :H2 =
"
h(0)
i
h(2)
i
#
1≤i≤N
∈R(D0+D2)×N 7→

H(0)
P2(H2)

.
Then, there exists θ such that for H′ that takes form h′
i = [h(0)
i ; h(1)
i ; h(2)
i ], with h(0)
i
∈RD0, h(1)
i
∈
RD1, h(2)
i
∈RD2, we have
TFθ :H′ =


h(0)
i
h(1)
i
h(2)
i


1≤i≤N
∈R(D0+D1+D2)×N 7→


H(0)
P1(H1)
P2(H2)

.
Further, θ has at most L ≤max {L1, L2} layers, maxℓ∈[L] M (ℓ) ≤maxℓ∈[L]

M (ℓ)
1
+ M (ℓ)
2

heads, maxℓ∈[L] D(ℓ) ≤maxℓ∈[L]

D(ℓ)
1
+ D(ℓ)
2

hidden dimension in its MLP layer (understanding
the size of the empty layers as 0), and norm bound |||θ||| ≤|||θ1||| + |||θ2|||.
Proof. When L1 = L2 (θ1 and θ2 have the same number of layers), the result follows directly by
applying Proposition B.5 repeatedly for all L1 layers and the definition of the norm (2).
If (without loss of generality) L1 < L2, we can augment θ1 to L2 layers by adding (L2 −L1) layers
with zero attention heads, and zero MLP hidden dimension (note that this does not change M1, D′
1,
and |||θ1|||). Due to the residual structure, the transformer maintains the output P1(H1) throughout
layer L1 + 1, . . . , L2, and it reduces to the case L1 = L2.
C
Extension to decoder-based architecture
Here we briefly discuss how our theoretical results can be adapted to decoder-based architectures
(henceforth decoder TFs). Adopting the setting as in Section 2, we consider a sequence of N input
vectors {hi}N
i=1 ⊂RD, written compactly as an input matrix H = [h1, . . . , hN] ∈RD×N. Recall
that σ(t) := ReLU(t) = max {t, 0} denotes the standard relu activation.
C.1
Decoder-based transformers
Decoder TFs are the same as encoder TFs, except that the attention layers are replaced by masked
attention layers with a specific decoder-based (causal) attention mask.
Definition C.1 (Masked attention layer). A masked attention layer with M heads is denoted as
MAttnθ(·) with parameters θ = {(Vm, Qm, Km)}m∈[M] ⊂RD×D. On any input sequence
H ∈RD×N′ with N ′ ≤N,
eH = MAttnθ(H) := H + PM
m=1(VmH) ×

(MSK1:N′,1:N′) ◦σ
 (QmH)⊤(KmH)

∈RD×N′,
(10)
23

where ◦denotes the entry-wise (Hadamard) product of two matrices, and MSK ∈RN×N is the mask
matrix given by
MSK =


1
1/2
1/3
· · ·
1/N
0
1/2
1/3
· · ·
1/N
0
0
1/3
· · ·
1/N
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
0
· · ·
1/N

.
In vector form, we have
ehi = [Attnθ(H)]i = hi + PM
m=1
1
i
Pi
j=1 σ(⟨Qmhi, Kmhj⟩) · Vmhj.
Notice that standard masked attention definitions use the pre-activation additive masks (with mask
value −∞) [84]. The post-activation multiplicative masks we use is equivalent to the pre-activation
additive masks, and the modified presentation is for notational convenience. We also use a nor-
malized ReLU activation t 7→σ(t)/i in place of the standard softmax activation to be consis-
tent with Definition 1. Note that the normalization 1/i is to ensure that the attention weights
{σ(⟨Qmhi, Kmhj⟩)/i}j∈[i] is a set of non-negative weights that sum to O(1). The motivation of
masked attention layer is to ensure that, when processing a sequence of tokens, the computations at
any token do not see any later token.
We next define the decoder-based transformers with L ≥1 transformer layers, each consisting of
a masked attention layer (c.f. Definition C.1) followed by an MLP layer (c.f. Definition 2). This
definition is similar to the definition of encoder-based transformers (c.f., Definition 3), except that we
replace the attention layers by masked attention layers.
Definition C.2 (Decoder-based Transformer). An L-layer decoder-based transformer, denoted as
DTFθ(·), is a composition of L self-attention layers each followed by an MLP layer: H(L) =
DTFθ(H(0)), where H(0) ∈RD×N is the input sequence, and
H(ℓ) = MLPθ(ℓ)
mlp

MAttnθ(ℓ)
mattn
 H(ℓ−1)
,
ℓ∈{1, . . . , L}.
Above, the parameter θ = (θ(1:L)
mattn, θ(1:L)
mlp ) is the parameter consisting of the attention layers
θ(ℓ)
mattn = {(V(ℓ)
m , Q(ℓ)
m , K(ℓ)
m )}m∈[M (ℓ)] ⊂RD×D and the MLP layers θ(ℓ)
mlp = (W(ℓ)
1 , W(ℓ)
2 ) ∈
RD(ℓ)×D × RD×D(ℓ). We will frequently consider “attention-only” decoder-based transformers with
W(ℓ)
1 , W(ℓ)
2
= 0, which we denote as DTF0
θ(·) for shorthand, with θ = θ(1:L) := θ(1:L)
mattn.
We also use (2) to define the norm of DTFθ.
C.2
In-context learning with decoder-based transformers
We consider using decoder-based TFs to perform ICL. We encode (D, xN+1), which follows the
generating rule as described in Section 2.2, into an input sequence H ∈RD×(2N+1). In our theory,
we use the following format, where the first two rows contain (D, xN+1) which alternating between
[xi; 0] ∈Rd+1 and [0d×1; yi] ∈Rd+1 (the same setup as adopted in [31, 2]); The third row contains
fixed vectors {pi}i∈[N+1] with ones, zeros, the example index, and indicator for being the covariate
token (similar to a positional encoding vector):
H =
"x1
0
. . .
xN
0
xN+1
0
y1
. . .
0
yN
0
p1
p2
. . .
p2N−1
p2N
p2N+1
#
,
pi :=


0D−(d+4)
⌈i/2⌉
1
mod(i + 1, 2)

∈RD−(d+1).
(11)
(11) is different from out input format (3) for encoder-based TFs. The main difference is that (xi, yi)
are in different tokens in (11), whereas (xi, yi) are in the same token in (3). The reason for the former
(i.e., different tokens in decoder) is that we want to avoid every [xi; 0] token seeing the information
of yi, since we will evaluate the loss at every token. The reason for the latter (i.e., the same token in
encoder) is for presentation convenience: since we only evaluate the loss at the last token, it is not
necessary to alternate between [xi; 0] and [0; yi] to avoid information leakage.
24

We then feed H into a decoder TF to obtain the output eH = DTFθ(H) ∈RD×(2N+1) with the same
shape, and read out the prediction byN+1 from the (d + 1, 2N + 1)-th entry of eH = [ehi]i∈[2N+1] (the
entry corresponding to the last missing test label): byN+1 = ready( eH) := (eh2N+1)d+1. The goal is
to predict byN+1 that is close to yN+1 ∼Py|xN+1 measured by proper losses.
The benefit of using the decoder architecture is that, during the pre-training phase, one can construct
the training loss function by using all the predictions {byj}j∈[N+1], where byj gives the (d+1, 2j −1)-
th entry of eH = [ehi]i∈[2N+1] for each j ∈[N + 1] (the entry corresponding to the missing test label
of the 2j −1’th token): byj = ready,j( eH) := (eh2j−1)d+1. Given a loss function ℓ: R × R →R
associated to a single response, the training loss associated to the whole input sequence can be defined
by ℓ(H) = PN+1
j=1 ℓ(yj, byj). This potentially enables less training sequences in the pre-training stage,
and some generalization bound analysis justifying this benefit was provided in [47].
C.3
Results
We discuss how our theoretical results upon encoder TFs can be converted to those of the decoder TFs.
Taking the implementation of (ICGD) (a key mechanism that enables most basic ICL algorithms such
as ridge regression; cf. Appendix D.1) as an example, this conversion is enabled by the following
facts: (a) the input format (11) of decoders can be converted to the input format (3) of encoders by a
2-layer decoder TF; (b) the encoder TF that implements (ICGD) with input format (3), by a slight
parameter modification, can be converted to a decoder TF that implements the (ICGD) algorithm
with a converted input format.
Input format conversion
Despite the difference between the input format (11) and (3), we show
that there exists a 2-layer decoder TF that can convert the input format (11) to format (3). The proof
can be found in Appendix C.4.
Proposition C.1 (Input format conversion). There exists a 2-layer decoder TF DTF with 3 heads
per layer, hidden dimension 2 and |||θ||| ≤12 such that upon taking input H of format (11), it outputs
eH = DTF(H) with
eH =
"x1
x1
. . .
xN
xN
xN+1
0
y1
. . .
0
yN
0
p1
p2
. . .
p2N−1
p2N
p2N+1
#
.
(12)
In particular, format (12) contains format (3) as a submatrix, by restricting to the {1, 2, . . . , D −
1, D −2, D} rows and {2, 4, . . . , 2N −2, 2N, 2N + 1} columns.
Generalization TF constructions to decoder architecture
The construction in Theorem D.1 can
be generalized to using the input format (12) along with a decoder TF, by using the scratch pad within
the last token to record the gradient descent iterates. Further, if we slightly change the normalization
in MSK from 1/i to 1/((i −1) ∨1), then the same construction performs (ICGD) (with training
examples {1, . . . , j}) at every token i = 2j + 1 (corresponding to predicting at xj+1). Building on
this extension, all our constructions in Section 3 and Section 4.2 can be generalized to decoder TFs.
C.4
Proof of Proposition C.1
For the simplicity of presentation, we write ci = ⌈i/2⌉, ti = mod(i + 1, 2), ui = hi[1 : d] ∈Rd+1
be the vector of first d entries of hi 6, and let vi = hi[d + 1] be the (d + 1)-th entry of hi. With such
notations, the input sequence H = [hi]i can be compactly written as
hi = [ui; vi; 0D−d−4; ci; 1; ti].
In the following, we construct the desired θ = (θ(1), θ(2)) as follows.
Step 1: construction of θ(1) = (θ(1)
mattn, θ(1)
mlp), so that MLPθ(1)
mlp ◦MAttnθ(1)
mattn maps
hi
MAttn
θ(1)
mattn
−−−−−−−→
h′
i = [ui; vi; 0D−d−6; ti(c2
i + 0.5); tici; ci; 1; ti]
6In other words, when 2 ∤i, ui = x(i−1)/2; when 2 | i, ui = 0d.
25

MLP
θ(1)
mlp
−−−−−→
h(1)
i
= [ui; vi; 0D−d−6; tic2
i ; tici; ci; 1; ti].
For m ∈{0, 1}, we define matrices Q(1)
m , K(1)
m , V(1)
m ∈RD×D such that
Q(1)
0 hi = Q(1)
1 hi =

ti
0

,
K(1)
0 hj = K(1)
1 hj =

cj
0

,
V(1)
0 hj =
"0D−4
3cj
03
#
,
V(1)
1 hj =
"0D−3
2
02
#
,
for all i, j. By the structure of hi, these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
m
Q(1)
m

op ≤1,
max
m
K(1)
m

op ≤1,
X
m
V(1)
m

op ≤5.
Now, for every i,
1
i
i
X
j=1
X
m∈{0,1}
σ
D
Q(1)
m hi, K(1)
m hj
E
V(1)
m hj = 1
i
i
X
j=1
ti · [0D−4; 3c2
j; 2cj; 0; 0].
Notice that ti ̸= 0 only when 2 | i, we then compute for i = 2k that
i
X
j=1
3c2
j = 3 · k(k −1)(2k −1)
3
+ 3k2 = 2k3 + k,
i
X
j=1
2cj = 2 · k(k −1) + 2k = 2k2.
Therefore, the θ(1)
mattn = {(Q(1)
m , K(1)
m , V(1)
m ∈RD×D)}m∈{0,1} we construct above is indeed the
desired attention layer. The existence of the desired θ(1)
mlp is clear, and θ(1)
mlp = (W(1)
1 , W(1)
2 ) can
further be chosen so that ∥W(1)
1 ∥op ≤1, ∥W(1)
2 ∥op ≤1.
Step 2: construction of θ(2). For every m ∈{−1, 0, 1}, we define matrices Q(2)
m , K(2)
m , V(2)
m ∈
RD×D such that
Q(2)
0 h(1)
i
= Q(2)
1 h(1)
i
= Q(2)
−1h(1)
i
=


tic2
i
tici
0

,
K(2)
0 h(1)
j
=
" 1
−cj
0
#
,
K(2)
1 h(1)
j
=
"
1
−(cj + 1)
0
#
,
K(2)
1 h(1)
j
=
"
1
−(cj −1)
0
#
,
V(2)
0 h(1)
j
=

−4uj
0D−d

,
V(2)
1 h(1)
j
= V(2)
−1h(1)
j
=

2uj
0D−d

,
for all i, j. By the structure of h(1)
i , these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
m
Q(2)
m

op ≤1,
max
m
K(2)
m

op ≤2,
X
m
V(2)
m

op ≤8.
Now, for every i, j, we have
X
m∈{−1,0,1}
σ
D
Q(2)
m h(1)
i , K(2)
m h(1)
j
E
V(2)
m h(1)
j
=

−2σ
 tic2
i −ticicj

+ σ
 tic2
i −tici(cj + 1)

+ σ
 tic2
i −tici(cj −1)
	
· 2[uj; 0D−d]
= {−2σ(ci −cj) + σ((ci −cj) −1) + σ((ci −cj) + 1)} · 2citi[uj; 0D−d]
= I(ci = cj) · 2citi[uj; 0D−d],
where the last equality follows from the fact that
−2σ(x) + σ(x −1) + σ(x + 1) =



0,
x ≥1 or x ≤−1,
x + 1,
x ∈[−1, 0],
1 −x,
x ∈[0, 1].
26

Therefore,
1
i
i
X
j=1
X
m∈{−1,0,1}
σ
D
Q(2)
m h(1)
i , K(2)
m h(1)
j
E
V(2)
m h(1)
j
= 1
i
i
X
j=1
2I(ci = cj)citi[uj; 0D−d]
=
[xk; 0D−d],
i = 2k
0D,
otherwise .
Therefore, the θ(2)
mattn = {(Q(2)
m , K(2)
m , V(2)
m ∈RD×D)}m∈{−1,0,1} we construct above maps
h(1)
i
→
h′′
i = [x⌈i/2⌉; vi; 0D−d−6; tic2
i ; tici; ci; 1; ti].
Finally, we only need to take a MLP layer θ(2)
mlp = (W(2)
1 , W(2)
2 ) with hidden dimension 2 that maps
h′′
i
→
h(2)
i
= [x⌈i/2⌉; vi; 0D−d−6; 0; 0; ci; 1; ti],
which clearly exists and can be chosen so that ∥W(2)
1 ∥op ≤1, ∥W(2)
2 ∥op ≤1.
Combining the two steps above, we complete the proof of Proposition C.1.
D
Mechanism: In-context gradient descent
Technically, the constructions in Section 3.1-3.2 rely on a new efficient construction for transformers
to implement in-context gradient descent and its variants, which we present as follows. We begin by
presenting the result for implementing (vanilla) gradient descent on convex empirical risks.
Compact notation of input
We will often use shorthand y′
i ∈R defined as y′
i = yi for i ∈[N] and
y′
N+1 = 0 to simplify our notation, with which the input sequence H ∈RD×(N+1) can be compactly
written as hi = [xi; y′
i; pi] = [xi; y′
i; 0D−d−3; 1; ti] for i ∈[N + 1], where ti := 1{i < N + 1} is
the indicator for the training examples.
D.1
Gradient descent on convex empirical risk
Let ℓ(·, ·) : R2 →R be a loss function. Let bLN(w) :=
1
N
PN
i=1 ℓ(w⊤xi, yi) denote the empirical
risk with loss function ℓon dataset {(xi, yi)}i∈[N], and
wt+1
GD := wt
GD −η∇bLN(wt
GD)
(ICGD)
denote the gradient descent trajectory on bLN with initialization w0
GD ∈Rd and learning rate η > 0.
We require the partial derivative of the loss ∂sℓ: (s, t) 7→∂sℓ(s, t) (as a bivariate function) to be
approximable by a sum of relus, defined as follows.
Definition D.1 (Approximability by sum of relus). A function g : Rk →R is (εapprox, R, M, C)-
approximable by sum of relus, if there exists a “(M, C)-sum of relus” function
fM,C(z) = PM
m=1 cmσ(a⊤
m[z; 1]) with
PM
m=1 |cm| ≤C, maxm∈[M] ∥am∥1 ≤1, am ∈Rk+1, cm ∈R,
such that supz∈[−R,R]k |g(z) −fM,C(z)| ≤εapprox.
Definition D.1 is known to contain broad class of functions. For example, any mildly smooth k-
variate function is approximable by a sum of relus for any (εapprox, R), with mild bounds on (M, C)
(Proposition B.1, building on results of Bach [4]). Also, any function that is a (M, C)-sum of relus
itself (which includes all piecewise linear functions) is by definition (0, ∞, M, C)-approximable by
sum of relus.
We show that L steps of (ICGD) can be approximately implemented by an (L + 1)-layer transformer.
Theorem D.1 (Convex ICGD). Fix any Bw > 0, L > 1, η > 0, and ε ≤Bw/(2L). Suppose that
1. The loss ℓ(·, ·) is convex in the first argument;
2. ∂sℓis (ε, R, M, C)-approximable by sum of relus with R = max {BxBw, By, 1}.
27

=
+
1
N
N
∑
i=1
M
∑
m=1 σ ( ⟨
,
⟩) ×
ht+1
N+1
Qm
Vm
ht
N+1
ht
N+1
ht
i
ht
i
Km
am σ ( bm ⟨
,
⟩+ cm
) ×
=
−
η
N
N
∑
i=1
M
∑
m=1
wt+1
xi
yi
wt
xi
yi
xi
wt
yi
xi
0
0
∂s ℓ( ⟨
,
⟩,
) ×
=
−
η
N
N
∑
i=1
wt+1
wt
xi
wt
yi
xi
∂sℓ(s, t) = ∑M
m=1 am ⋅σ( bm ⋅s + cm ⋅t )
Attention
Gradient 
descent
Weight 
construction
Universal 
approximation
ht
i
wt
xi
yi
=
Vm
xi
0
0
= −ηam ×
wt
xi
yi
,
,
,
xi
Km
=
wt
xi
yi
yi
Qm
=
wt*
wt
1
bm ×
*
cm ×
Figure 4: Illustration of our main mechanism for implementing basic ICL algorithms: One attention layer
implements a single (ICGD) iterate (Proposition E.1 & Theorem D.1). Top: the attention mechanism as in
Definition 1. Bottom: A single (ICGD) iterate. Middle: Linear algebraic illustration of the attention layer for
implementing a GD update.
Then, there exists an attention-only transformer TF0
θ with (L + 1) layers, maxℓ∈[L] M (ℓ) ≤M
heads within the first L layers, and M (L+1) = 2 such that for any input data (D, xN+1) such that
sup
∥w∥2≤Bw
λmax(∇2bLN(w)) ≤2/η,
∃w⋆∈arg min
w∈Rd
bLN(w) such that ∥w⋆∥2 ≤Bw/2,
TF0
θ(H(0)) approximately implements (ICGD) with initialization w0
GD = 0:
1. (Parameter space) For every ℓ∈[L], the ℓ-th layer’s output H(ℓ) = TF0
θ(1:ℓ)(H(0)) approx-
imates ℓsteps of (ICGD): We have h(ℓ)
i
= [xi; y′
i; bwℓ; 0D−2d−3; 1; ti] for every i ∈[N + 1],
where
bwℓ−wℓ
GD

2 ≤ε · (ℓηBx).
Note that the bound scales as O(ℓ), a linear error accumulation.
2. (Prediction space) The final output H(L+1) = TF0
θ(H(0)) approximates the prediction of L steps
of (ICGD): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0D−2d−3; 1; ti], where byN+1 =

bwL, xN+1

so that
byN+1 −

wL
GD, xN+1
 ≤ε · (LηB2
x).
Further, the transformer admits norm bound |||θ||| ≤2 + R + 2ηC.
The proof can be found in Appendix E.2. Theorem D.1 substantially generalizes that of von Oswald
et al. [86] (which only does GD on square losses with a linear self-attention), and is simpler than the
ones in Akyürek et al. [2] and Giannou et al. [32]. See Figure 4 for a pictorial illustration of the basic
component of the construction, which implements a single step of gradient descent using a single
attention layer (Proposition E.1).
Technically, we utilize the stability of convex gradient descent as in the following lemma (proof
in Appendix E.3) to obtain the linear error accumulation in Theorem D.1; the error accumulation
will become exponential in L in the non-convex case in general; see Lemma D.3(b).
Lemma D.1 (Composition of error for approximating convex GD). Suppose f : Rd →R is a convex
function. Let w⋆∈arg minw∈Rd f(w), R ≥2∥w⋆∥2, and assume that ∇f is Lf-smooth on Bd
2(R).
Let sequences {bwℓ}ℓ≥0 ⊂Rd and {wℓ
GD}ℓ≥0 ⊂Rd be given by bw0 = w0
GD = 0,
(
bwℓ+1 = bwℓ−η∇f(bwℓ) + εℓ,
∥εℓ∥2 ≤ε,
wℓ+1
GD = wℓ
GD −η∇f(wℓ
GD),
for all ℓ≥0. Then as long as η ≤2/Lf, for any 0 ≤L ≤R/(2ε), it holds that
bwL −wL
GD

2 ≤
Lε and ∥bwL∥2 ≤R
2 + Lε ≤R.
28

D.2
Proximal gradient descent for regularized convex losses
Proximal gradient descent (PGD) is a variant of gradient descent that is suitable for minimizing
regularized risks [66], in particular those with a non-smooth regularizer such as the ℓ1 norm. In this
section, we show that transformers can approximate PGD with similar quantitative guarantees as for
GD in Appendix D.1.
Let ℓ(·, ·) : R2 →R be a loss function. Let bLN(w) :=
1
N
PN
i=1 ℓ(w⊤xi, yi) + R(w) denote
the regularized empirical risk with loss function ℓon dataset {(xi, yi)}i∈[N] and regularizer R.
To minimize bLN, we consider the proximal gradient descent trajectory on bLN with initialization
w0
GD = 0 ∈Rd and learning rate η > 0:
wt+1
PGD := proxηR

wt
PGD −η∇bL0
N(wt
PGD)

,
(ICPGD)
where we denote bL0
N(w) := 1
N
PN
i=1 ℓ(w⊤xi, yi).
To approximate (ICPGD) by transformers, in addition to the requirement on the loss ℓas in Theo-
rem D.1, we additionally require the the proximal operator proxηR(·) to be approximable by an
MLP layer (as a vector-valued analog of Definition D.1) defined as follows.
Definition D.2 (Approximability by MLP). An operator P : Rd →Rd is (ε, R, D, C)-approximable
by MLP, if there exists a there exists a MLP θmlp = (W1, W2) ∈RD×d × Rd×D with hidden
dimension D, ∥W1∥op + ∥W2∥op ≤C′, such that sup∥w∥2≤R
P(w) −MLPθmlp(w)

2 ≤ε.
The definition above captures the proximal operator proxηR for a broad class of regularizers, such
as the (commonly-used) L1 and L2 regularizer listed in the following proposition, for all of which
one can directly check that they can be exactly implemented by an MLP as stated below.
Proposition D.1 (Proximal operators for commonly-used regularizers). For regularizer R in
{λ ∥·∥1 , λ
2 ∥·∥2
2 , IB∞(B)(·)}, the operator proxηR : Rd →Rd is exactly approximable by MLP.
More concretely, we have
1. For R = λ ∥·∥1, proxηR is (0, +∞, 4d, 4 + 2ηλ)-approximable by MLP.
2. For R = λ
2 ∥·∥2
2, proxηR is (0, +∞, 2d, 2 + 2ηλ)-approximable by MLP.
3. For R = IB∞(B)(·), proxηR = ProjB∞(B) is (0, +∞, 2d, 2 + 2B)-approximable by MLP.
Theorem D.2 (Convex ICPGD). Fix any Bw > 0, L > 1, η > 0, and ε + ε′ ≤Bw/(2L). Suppose
that
1. The loss ℓ(·, ·) is convex in the first argument;
2. ∂sℓis (ε, R, M, C)-approximable by sum of relus with R = max {BxBw, By, 1}.
3. R convex, and the proximal operator proxηR(w) is (ηε′, R′, D′, C′)-approximable by MLP
with R′ = sup∥w∥2≤Bw
w+
η

2 + ηε.
Then there exists a transformer TFθ with (L + 1) layers, maxℓ∈[L] M (ℓ) ≤M heads within the first
L layers, M (L+1) = 2, and hidden dimension D′ such that, for any input data (D, xN+1) such that
sup
∥w∥2≤Bw
λmax(∇2bLN(w)) ≤2/η,
∃w⋆∈arg min
w∈Rd
bLN(w) such that ∥w⋆∥2 ≤Bw/2,
TFθ(H(0)) approximately implements (ICGD):
1. (Parameter space) For every ℓ∈[L], the ℓ-th layer’s output H(ℓ) = TFθ(1:ℓ)(H(0)) approx-
imates ℓsteps of (ICGD): We have h(ℓ)
i
= [xi; y′
i; bwℓ; 0D−2d−3; 1; ti] for every i ∈[N + 1],
where
bwℓ−wℓ
PGD

2 ≤(ε + ε′) · (LηBx).
2. (Prediction space) The final output H(L+1) = TFθ(H(0)) approximates the prediction of L steps
of (ICGD): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0D−2d−3; 1; ti], where byN+1 =

bwL, xN+1

so that
byN+1 −

wL
PGD, xN+1
 ≤(ε + ε′) · (2LηB2
x).
29

Further, the weight matrices have norm bounds |||θ||| ≤3 + R + 2ηC + C′.
The proof of Theorem D.2 is essentially similar to the proof of Theorem D.1, using the following
generalized version of Lemma D.1.
Lemma D.2 (Composition of error for approximating convex PGD). Suppose f : Rd →R is a
convex function and R is a convex regularizer. Let w⋆∈arg minw∈Rd f(w) + R(w), R ≥2∥w⋆∥2,
and assume that ∇f is Lf-smooth on Bd
2(R). Let sequences {bwℓ}ℓ≥0 ⊂Rd and {wℓ
GD}ℓ≥0 ⊂Rd
be given by bw0 = w0
GD = 0,
(
bwℓ+1 = proxηR
 bwℓ−η∇f(bwℓ)

+ εℓ,
∥εℓ∥2 ≤ε,
wℓ+1
GD = proxηR
 wℓ
GD −η∇f(wℓ
GD)

,
for all ℓ≥0. Then as long as η ≤2/Lf, for any 0 ≤L ≤R/(2ε), it holds that
bwL −wL
GD

2 ≤
Lε and ∥bwL∥2 ≤R
2 + Lε ≤R.
The proof of the above lemma is done by utilizing the non-expansiveness of the PGD operator
w 7→proxηR(w −η∇f(w)) and otherwise following the same arguments as for Lemma D.1.
D.3
Gradient descent on two-layer neural networks
We now move beyond the convex setting by showing that transformers can implement gradient
descent on two-layer neural networks in context.
Suppose that the prediction function pred(x; w) := PK
k=1 ukr(v⊤
k x) is given by a two-layer neural
network, parameterized by w = [vk; uk]k∈[K] ∈RK(d+1). Consider the empirical risk minimization
problem:
min
w∈W
bLN(w) :=
1
2N
N
X
i=1
ℓ(pred(xi; w), yi) =
1
2N
N
X
i=1
ℓ
 K
X
k=1
ukr(v⊤
k xi), yi
!
,
(13)
where W is a bounded domain. For the sake of simplicity, in the following discussion we assume that
ProjW can be exactly implemented by a MLP layer (e.g. W = B∞(Rw) for some Rw > 0).
Theorem D.3 (Approximate ICGD on two-layer NNs). Fix any Bv, Bu > 0, L ≥1, η > 0, and
ε > 0. Suppose that
1. Both the activation function r and the loss function ℓis C4-smooth;
2. W is a closed domain such that W ⊂{w = [vk; uk]k∈[K] ∈RK(d+1) : ∥vk∥2 ≤Bv, |uk| ≤
Bu}, and ProjW = MLPθmlp for some MLP layer θmlp with hidden dimension Dw and
θmlp
 ≤Cw;
Then there exists a (2L)-layer transformer TFθ with
max
ℓ∈[2L] M (ℓ) ≤e
O
 ε−2
,
max
ℓ∈[2L] D(ℓ) ≤e
O
 ε−2
+ Dw,
|||θ||| ≤O (1 + η) + Cw,
where O (·) hides the constants that depend on K, the radius parameters Bx, By, Bu, Bv and the
smoothness of r and ℓ, such that for any input data (D, xN+1) such that input sequence H(0) ∈
RD×(N+1) takes form (3), TFθ(H(0)) approximately implements in-context gradient descent on
risk (13): For every ℓ∈[L], the 2ℓ-th layer’s output h(2ℓ)
i
= [xi; y′
i; bwℓ; 0; 1; ti] for every i ∈[N +1],
and
bwℓ= ProjW

bwℓ−1 −η(∇bLN(bwℓ−1) + εℓ−1)

,
bw0 = 0,
(14)
where
εℓ−1
2 ≤ε is an error term.
As a direct corollary, the transformer constructed above can approximate the true gradient descent
trajectory {wℓ
GD}ℓ≥0 on (16), defined as w0
GD = 0 and wℓ+1
GD = wℓ
GD −η∇bLN(wℓ
GD) for all ℓ≥0.
30

Corollary D.1 (Approximating multi-step ICGD on two-layer NNs). For any L ≥1, under the
same setting as Theorem D.3, the (2L)-layer transformer TFθ there approximates the true gradient
descent trajectory {wℓ
GD}ℓ≥0: For the intermediate iterates {bwℓ}ℓ∈[L] considered therein, we have
bwℓ−wℓ
GD

2 ≤L−1
f (1 + ηLf)ℓε,
where Lf = supw∈W
∇2bLN(w)

op denotes the smoothness of bLN within W.
Remark on error accumulation
Note that in Corollary D.1, the error accumulates exponentially
in ℓrather than linearly as in Theorem D.1. This is as expected, since gradient descent on non-convex
objectives is inherently unstable at a high level (a slight error added upon each step may result in a
drastically different trajectories); technically, this happens as the stability-like property Lemma D.1
no longer holds for the non-convex case.
Corollary D.1 is a simple implication of Theorem D.3 and Part (b) of the following convergence
and trajectory closeness result for inexact gradient descent. For any closed convex set W ⊂Rd, any
function f : W →R, and any initial point w ∈W, let
Gf
W,η(w) := w −ProjW(w −η∇f(w))
η
denote the gradient mapping at w with step size η, a standard measure of stationarity in constrained
optimization [63]. Note that Gf
W,η(w) = ∇f(w) when w −η∇f(w) ∈W (so that the projection
does not take effect).
Lemma D.3 (Convergence and trajectory closeness of inexact GD). Suppose f : W →R, where
W ⊂Rd is a convex closed domain and ∇f is Lf-Lipschitz on W. Let sequence {bwℓ}ℓ≥0 ⊂Rd be
given by bw0 = w0,
bwℓ+1 = ProjW
 bwℓ−η(∇f(bwℓ) + εℓ)

,
∥εℓ∥2 ≤ε,
for all ℓ≥0. Then the following holds.
(a) As long as η ≤1/Lf, for all L ≥1,
min
ℓ∈[L−1]
Gf
W,η(bwℓ)

2
2 ≤1
L
L−1
X
ℓ=0
Gf
W,η(bwℓ)

2
2 ≤8(f(w0) −infw∈W f(w))
ηL
+ 10ε2.
(b) Let the sequences {wℓ
GD}ℓ≥0 ⊂Rd and be given by w0
GD = w0 and wℓ+1
GD
=
ProjW(wℓ
GD −η∇f(wℓ
GD)). Then it holds that
bwℓ−wℓ
GD

2 ≤L−1
f (1 + ηLf)ℓε,
∀ℓ≥0.
E
Proofs for Section D
E.1
Approximating a single GD step
Proposition E.1 (Approximating a single GD step by a single attention layer). Let ℓ(·, ·) :
R2 →R be a loss function such that ∂1ℓis (ε, R, M, C)-approximable by sum of relus with
R = max{BxBw, By, 1}. Let bLN(w) := 1
N
PN
i=1 ℓ(w⊤xi, yi) denote the empirical risk with loss
function ℓon dataset {(xi, yi)}i∈[N].
Then, for any ε > 0, there exists an attention layer θ = {(Qm, Km, Vm)}m∈[M] with M heads
such that, for any input sequence that takes form hi = [xi; y′
i; w; 0D−2d−3; 1; ti] with ∥w∥2 ≤Bw,
it gives output ehi = [Attnθ(H)]i = [xi; y′
i; ew; 0D−2d−3; 1; ti] for all i ∈[N + 1], where
ew −(w −η∇bLN(w))

2 ≤ε · (ηBx).
Further, |||θ||| ≤2 + R + 2ηC.
31

Proof of Proposition E.1. As ∂sℓis (ε, R, M, C)-approximable by sum of relus, there exists a func-
tion f : [−R, R]2 →R of form
f(s, t) =
M
X
m=1
cmσ(ams + bmt + dm) with
M
X
m=1
|cm| ≤C, |am| + |bm| + |dm| ≤1, ∀m ∈[M],
such that sup(s,t)∈[−R,R]2 |f(s, t) −∂sℓ(s, t)| ≤ε.
Next, for every m ∈[M], we define matrices Qm, Km, Vm ∈RD×D such that
Qmhi =


amw
bm
dm
−2
0

,
Kmhj =


xj
y′
j
1
R(1 −tj)
0

,
Vmhj = −(N + 1)ηcm
N
·


0d
0
xj
0D−2d−1


for all i, j ∈[N + 1]. As the input has structure hi = [xi; y′
i; w; 0D−2d−3; 1; ti], these matrices
indeed exist, and further it is straightforward to check that they have norm bounds
max
m∈[M] ∥Qm∥op ≤3,
max
m∈[M] ∥Km∥op ≤2 + R,
X
m∈[M]
∥Vm∥op ≤2ηC.
Consequently, |||θ||| ≤2 + R + 2ηC.
Now, for every i, j ∈[N + 1], we have
σ(⟨Qmhi, Kmhj⟩) = σ
 amw⊤xj + bm(1 −tj)yj + dm −2Rtj

= σ
 amw⊤xj + bmyj + dm

1{tj = 1},
where the last equality follows from the bound
amw⊤xj + bm(1 −tj)yj + dm
 ≤|am|BxBw + R ≤2R,
(15)
so that the above relu equals 0 if tj ≤0. Therefore,
M
X
m=1
σ(⟨Qmhi, Kmhj⟩)Vmhj
=
 M
X
m=1
cmσ
 amw⊤xj + bmyj + dm

!
· −(N + 1)η
N
1{tj = 0}[0d+1; xj; 02]
= f(w⊤xj, yj) · −(N + 1)η
N
1{tj = 0}[0d+1; xj; 0D−2d−1].
Thus letting the attention layer θ = {(Vm, Qm, Km)}m∈[M], we have
ehi = [Attnθ(H)]i = hi +
1
N + 1
N+1
X
j=1
M
X
m=1
σ(⟨Qmhi, Kmhj⟩)Vmhj
= hi −η
N
N
X
j=1
f(w⊤xj, yj)[0d+1; xj; 02]
= [xi; yi; w; 1; ti] −η
N
N
X
j=1
∂sℓ(w⊤xj, yj)[0d+1; xj; 0D−2d−1]
|
{z
}
[0d+1;−η∇bLN(w);0D−2d−1]
+[0d+1; ε; 0D−2d−1]
= [xi; yi; w+
η + ε; 0D−2d−3; 1; ti],
where the error vector ε ∈Rd satisfies
∥ε∥2 =

−η
N
N
X
j=1
 f(w⊤xj, yj) −∂sℓ(w⊤xj, yj)

xj

2
32

≤η
N
N
X
j=1
f(w⊤xj, yj) −∂sℓ(w⊤xj, yj)
 · ∥xj∥2
≤η
N · N · ε · Bx = ε · (ηBx).
This is the desired result.
E.2
Proof of Theorem D.1
We first prove part (a), which requires constructing the first L layers of θ. Note that by our precondi-
tion L ≤Bw/(2ε).
By our precondition, the partial derivative of the loss ∂sℓis (ε, R, M, C)-approximable by sum of
relus. Therefore we can apply Proposition E.1 to obtain that, there exists a single attention layer θ(1) =
{(Qm, Km, Vm)}m∈[M] with M heads (and norm bounds specified in Proposition E.1), such that for
any w with ∥w∥2 ≤Bw, the attention layer Attnθ(1) maps the input hi = [xi; y′
i; w; 0D−2d−3; 1; ti]
to output h′
i = [xi; y′
i; bw; 0D−2d−3; 1; ti] for all i ∈[N + 1], where
bw −

w −η∇bLN(w)

2 ≤ε · (ηBx) =: ε′.
Consider the L-layer transformer θ1:L = (θ(1), . . . , θ(1)) which stacks the same attention layer
θ(1) for L times, and for the given input h(0)
i
= [xi; y′
i; w0; 0D−2d−3; 1; ti], its ℓ-th layer’s output
h(ℓ)
i
= [xi; y′
i; bwℓ; 0D−2d−3; 1; ti].
We now inductively show that
bwℓ
2 ≤Bw and
bwℓ−wℓ
GD

2 ≤ℓε for all ℓ∈[L]. The base case
of ℓ= 0 is trivial. Suppose the claim holds for ℓ. Then for ℓ+ 1 ≤L ≤Bw/(2ε), the sequence
{bwi}i≤ℓ+1 and {wi
GD}i≤ℓ+1 satisfies the precondition of the error composition lemma (Lemma D.1)
with error bound ε, from which we obtain
bwℓ+1
2 ≤Bw and
bwℓ+1 −wℓ+1
GD

2 ≤(ℓ+ 1)ε′.
This finishes the induction, and gives the following approximation guarantee for all ℓ∈[L]:
bwℓ−wℓ
GD

2 ≤ℓε′ ≤ε · (LηBx),
which proves part (a).
We now prove part (b), which requires constructing the last attention layer θ(L+1). Recall h(L)
i
=
[xi; y′
i; bwL; 0D−2d−3; 1; ti] for all i ∈[N + 1]. We construct a 2-head attention layer θ(L+1) =
{(Q(L+1)
m
, K(L+1)
m
, V(L+1)
m
)}m=1,2 such that for every i, j ∈[N + 1],
Q(L+1)
1
h(L)
i
= [xi; 0D−d], K(L+1)
1
h(L)
j
= [bwL; 0D−d], V(L+1)
1
h(L)
j
= [0d; 1; 0D−d−1],
Q(L+1)
2
h(L)
i
= [xi; 0D−d], K(L+1)
2
h(L)
j
= [−bwL; 0D−d], V(L+1)
2
h(L)
j
= [0d; −1; 0D−d−1].
Note that the weight matrices have norm bound
max
i=1,2
Q(L+1)
i

op ≤1,
max
i=1,2
K(L+1)
i

op ≤1,
2
X
i=1
V(L+1)
i

op ≤2.
Then we have
h(L+1)
N+1 = h(L)
N+1 +
1
N + 1
N+1
X
j=1
2
X
m=1
σ
D
Q(L+1)h(L)
N+1, K(L+1)h(L)
j
E
V(L+1)h(L)
j
= [xi; 0; bwL; 0D−2d−3; 1; 1] +
 σ(

bwL, xN+1

) −σ(−

bwL, xN+1

)

· [0d; 1; 0D−d−1]
(i)
= [xi; 0; bwL; 0D−2d−3; 1; 1] + [0d;

bwL, xN+1

; 0D−d−1]
= [xi;

bwL, xN+1

|
{z
}
byN+1
; bwL; 0D−2d−3; 1; 1],
33

Above, (i) uses the identity t = σ(t) −σ(−t). Further by part (a) we have
byN+1 −

wL
GD, xN+1
 =

bwL −wL
GD, xN+1
 ≤ε · (LηB2
x).
This proves part (b), and also finishes the proof Theorem D.1 where the overall (L + 1)-layer
attention-only transformer is given by TF0
θ with
θ = (θ(1), . . . , θ(1)
|
{z
}
L times
, θ(L+1)).
E.3
Proof of Lemma D.1
As f is a convex, Lf smooth function on Bd
2(R), the mapping Tη : w 7→w −η∇f(w) is non-
expansive in ∥·∥2: Indeed, for any w, w′ ∈Bd
2(R) we have
∥Tη(w) −Tη(w′)∥2 = ∥w −η∇f(w) −(w′ −η∇f(w′))∥2
2
= ∥w −w′∥2
2 −2η ⟨w −w′, ∇f(w) −∇f(w′)⟩+ η2 ∥∇f(w) −∇f(w′)∥2
2
(i)
≤∥w −w′∥2
2 −
 2η/Lf −η2
∥∇f(w) −∇f(w′)∥2
2
(ii)
≤∥w −w′∥2
2 .
Above, (i) uses the property ⟨w −w′, ∇f(w) −∇f(w′)⟩≥
1
Lf ∥∇f(w) −∇f(w′)∥2
2 for smooth
convex functions [63, Theorem 2.1.5]; (ii) uses the precondition that η ≤2/Lf.
The lemma then follows directly by induction on L. The base case of L = 0 follows directly
by assumption that bw0 = w0
GD ∈Bd
2(R/2). Suppose the claim holds for iterate L. For iterate
L + 1 ≤R/(2ε), we have
bwL+1 −wL+1
GD

2 =
Tη(bwL) + εL −Tη(wL
GD)

2
≤
Tη(bwL) −Tη(wL
GD)

2 +
εL
2
(i)
≤
bwL −wL
GD

2 + ε
(ii)
≤(L + 1)ε.
Above, (i) uses the non-expansiveness, and (ii) uses the inductive hypothesis. Similarly, by our
assumption w⋆= Tη(w⋆),
bwL+1 −w⋆
2 =
Tη(bwL) + εL −Tη(w⋆)

2 ≤
bwL −w⋆
2 +
εL
2 ≤R
2 + (L + 1)ε ≤R.
This finishes the induction.
E.4
Convex ICGD with ℓ2 regularization
In the same setting as Theorem D.1, consider the ICGD dynamics over an ℓ2-regularized empirical
risk:
wt+1
GD := wt
GD −η∇bLλ
N(wt
GD)
(ICGD-ℓ2)
with initialization w0
GD ∈Rd and learning rate η > 0, where bLλ
N(w) := bLN(w) + λ
2 ∥w∥2
2 denotes
the ℓ2-regularized empirical risk.
Corollary E.1 (Convex ICGD with ℓ2 regularization). Fix any Bw > 0, L > 1, η > 0, and
ε < BxBw. Suppose the loss ℓ(·, ·) is convex in the first argument, and ∂sℓis (ε, R, M, C)-
approximable by sum of relus with R = max {BxBw, By, 1}.
Then, there exists an attention-only transformer TF0
θ with (L + 1) layers, maxℓ∈[L] M (ℓ) ≤M + 1
heads within the first L layers, and M (L+1) = 2 such that for any input data (D, xN+1) with
sup
∥w∥2≤Bw
λmax(∇2bLλ
N(w)) ≤2η−1,
∃w⋆∈arg min
w∈Rd
bLλ
N(w) such that ∥w⋆∥2 ≤Bw/2,
TF0
θ(H(0)) approximately implements (ICGD-ℓ2):
34

1. (Parameter space) For every ℓ∈[L], the ℓ-th layer’s output H(ℓ) = TFθ(1:ℓ)(H(0)) approxi-
mates ℓsteps of (ICGD-ℓ2): We have h(ℓ)
i
= [xi; y′
i; bwℓ; 0D−2d−3; 1; ti] for every i ∈[N + 1],
where
bwℓ−wℓ
GD

2 ≤ε · (2LηBx).
2. (Prediction space) The final output H(L+1) = TFθ(H(0)) approximates the prediction of L
steps of (ICGD-ℓ2): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0D−2d−3; 1; 0], where
byN+1 −

wL
GD, xN+1
 ≤ε · (2LηB2
x).
Further, the transformer admits norm bound |||θ||| ≤2 + R + (2C + λ)η.
Proof. This construction is the same as in the proof of Theorem D.1, except that within each layer
ℓ∈[L], we add one more attention head (Q(ℓ), K(ℓ), V(ℓ)) ⊂RD×D which when acting on its input
h(ℓ−1)
i
= [∗; ∗; bwℓ−1; 1; ∗] gives
Q(ℓ)h(ℓ−1)
i
=

1
0D−1

,
K(ℓ)h(ℓ−1)
j
=

1
0D−1

,
V(ℓ)h(ℓ−1)
j
=


0d+1
−ηλbwℓ−1
02


for all i, j ∈[N + 1]. Note that
Q(ℓ)
op =
K(ℓ)
op = 1, and
V(ℓ)
op = ηλ. Further, it is
straightforward to check that the output of this attention head on every h(ℓ)
i
is
1
N + 1
N+1
X
j=1
σ
D
Q(ℓ)h(ℓ−1)
i
, K(ℓ)h(ℓ−1)
j
E
V(ℓ)h(ℓ−1)
j
=


0d+1
−ηλbwℓ−1
02

.
Adding this onto the original output of the ℓ-th layer exactly implements the gradient of the regularizer
w 7→λ
2 ∥w∥2
2. The rest of the proof follows by repeating the argument of Theorem D.1, and combining
the norm bound for the additional attention head here with the norm bound therein.
E.5
Proof of Theorem D.3
We only need to prove the following single-step version of Theorem D.3.
Proposition E.2. Under the assumptions of Theorem D.3, there exists a 2-layer transformer TFθ
with the same bounds on the number of heads, hidden dimension and the norm, such that for any
input data (D, xN+1) and any w ∈Rd, TFθ maps
hi = [xi; y′
i; w; 0; 1; ti]
→
h′
i = [xi; y′
i; w+
η ; 0; 1; ti],
where
w+
η = ProjW

w −η∇bLN(w) + ε(w)

,
∥ε(w)∥2 ≤ηε.
Before we present the formal (and technical) proof of Proposition E.2, we first provide some intuitions.
To begin with, we first note that
∇w bLN(w) = 1
N
N
X
i=1
∂1ℓ(pred(xi; w), yi) · ∇wpred(xi; w),
(16)
where ∂1ℓis the partial derivative of ℓwith respect to the first component, and
∇wpred(xi; w) =


u1 · r′(⟨v1, xi⟩) · xi
r(⟨v1, xi⟩)
...
uK · r′(⟨vK, xi⟩) · xi
r(⟨vK, xi⟩)


∈RK(d+1).
(17)
35

Therefore, the basic idea is that we can use an attention layer to approximate (xi, w) 7→pred(xi; w),
then use an MLP layer to implement (pred(xi; w), y′
i, ti) 7→1{i < N + 1} · ∂1ℓ(pred(xi; w), yi),
and then use an attention layer to compute the gradient descent step w 7→w −η∇LN(w), and
finally use an MLP layer to implement the projection into W.
Based on the observations above, we now present the proof of Proposition E.2.
Proof of Proposition E.2. We write D0 = d + 1 + K(d + 1) be the length of the vector [xi; yi; w].
We also define
Br :=
max
|t|≤BxBu |r(t)| ,
Bg :=
max
|t|≤KBr,|y|≤By |∂tℓ(t, y)| .
Let us fix εr, εp, εℓ> 0 that will be specified later in proof (see (18)). By our assumption and
Proposition B.1, the following facts hold.
(1) The function r(t) is (εr, R1, M1, C1) for R1 = max {BxBu, 1}, M1 ≤e
O
 C2
1ε−2
r

, where
C1 depends only on R1 and the C2-smoothness of r. Therefore, there exists
r(t) =
M
X
m=1
c1
mσ(

a1
m, [t; 1]

) with
M
X
m=1
c1
m
 ≤C1,
a1
m

1 ≤1, ∀m ∈[M1],
such that supt∈[−R1,R1] |r(t) −r(t)| ≤εr.
(2) The function (t, y) 7→∂1ℓ(t, y) is (εℓ, R2, M2, C2) for R2 = max {KBr, By, 1} M2 ≤
e
O
 C2
2ε−2
ℓ

, where C2 depends only on R2 and the C3-smoothness of ∂1ℓ. Therefore, there
exists
g(t, y) =
M
X
m=1
c2
mσ(

a2
m, [t; y; 1]

) with
M
X
m=1
c2
m
 ≤C2,
a2
m

1 ≤1, ∀m ∈[M2],
such that sup(t,y)∈[−R2,R2]2 |g(t, y) −∂1ℓ(t, y)| ≤εℓ.
(3) The function (s, t) 7→s · r′(t) is (εp, R3, M3, C3) for R3 = max {BxBu, BgBu, 1},
M3 ≤e
O
 C2
3ε−2
p

, where C3 depends only on R3 and the C3-smoothness of r′. Therefore,
there exists
P(s, t) =
M
X
m=1
c3
mσ(

a3
m, [s; t; 1]

) with
M
X
m=1
c3
m
 ≤C3,
a3
m

1 ≤1, ∀m ∈[M3],
such that sup(s,t)∈[−R3,R3]2 |P(s, t) −s · r′(t)| ≤εp.
In the following, we proceed to construct the desired transformer step by step.
Step 1: construction of θ(1)
attn. We consider the matrices {Q(1)
k,m, K(1)
k,m, V(1)
k,m}k∈[K],m∈[M1] so that
for all i, j ∈[N + 1], we have
Q(1)
k,mhi =


a1
m[1] · xi
a1
m[2]
0

,
K(1)
k,mhj =
"vk
1
0
#
,
V(1)
k,mhj = c1
m · ukeD0+1.
As the input has structure hi = [xi; y′
i; w; 0; 1; ti], these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
k,m
Q(1)
k,m

op ≤1,
max
k,m
K(1)
k,m

op ≤1,
X
k,m
V(1)
k,m

op ≤C1.
A simple calculation shows that
X
m∈[M1],k∈[K]
σ
D
Q(1)
k,mhi, K(1)
k,mhj
E
V(1)
k,mhj =
K
X
k=1
ukr(⟨vk, xi⟩) · eD0+1.
36

For simplicity, we denote pred(x; w) := PK
k=1 ukr(⟨vk, x⟩) in the following analysis. Thus, letting
the attention layer θ(1)
attn = {(V(1)
k,m, Q(1)
k,m, K(1)
k,m)}(k,m), we have
Attnθ(1)
attn : hi 7→h(0.5)
i
= [xi; y′
i; w; pred(xi; w); 0; 1; ti].
Step 2: construction of θ(1)
mlp. We pick matrices W1, W2 so that W1 maps
W1h(0.5)
i
=
h
a2
m[1] · pred(xi; w) + a2
m[2] · y′
i + a2
m[3] −R2(1 −ti)
i
m∈[M2] ∈RM2,
and W2 ∈RD×M3 with entries being (W2)(j,m) = c2
m1{j = D0 + 2}. It is clear that ∥W1∥op ≤
R2 + 1, ∥W2∥op ≤C2. Then we have
W2σ(W1h(0.5)
i
) =
X
m∈[M3]
σ
 
a2
m, [pred(xi; w); y′
i; 1]

−R2(1 −tj)

· c2
meD0+2
= 1{tj = 1} · g(pred(xi; w), y′
i) · eD0+2.
In the following, we abbreviate gi = 1{tj = 1} · g(pred(xi; w), y′
i). Hence, θmlp maps
MLPθmlp : h(0.5)
i
7→h(1)
i
= [xi; y′
i; w; pred(xi; w); gi; 0; 1; ti].
By the definition of the function g, for each i ∈[N],
|gi −∂1ℓ(pred(xi; w), yi)| ≤εℓ+ BuLℓεr,
where Lℓ:= max|t|≤KBr,|y|≤By
∂2
ttℓ(t, y)
 is the smoothness of ∂1ℓ. Also, gN+1 = 0 by definition.
Step 3: construction of θ(2)
attn. We consider the matrices {Q(2)
k,1,m, K(2)
k,1,m, V(2)
k,1,m}k∈[K],m∈[M3] so
that for all i, j ∈[N + 1], we have
Q(2)
k,1,mh(1)
i
=


a3
m[1] · uk
a3
m[2] · vk
a3
m[3]
0

,
K(2)
k,1,mh(1)
j
=


gj
xj
1
0

,
V(2)
k,1,mh(1)
j
= −(N + 1)ηc3
m
N
·
"0k(d+1)
xj
0
#
.
We further consider the matrices {Q(2)
k,2,m, K(2)
k,2,m, V(2)
k,2,m}k∈[K],m∈[M1] so that for all i, j ∈[N +1],
we have
Q(2)
k,2,mh(1)
i
=


a1
m[1] · vk
a1
m[2]
0

,
K(2)
k,2,mh(1)
j
=
"xj
1
0
#
,
V(2)
k,2,mh(1)
j
= −(N + 1)ηc1
m
N
·
"0k(d+1)+d
gj
0
#
.
By the structure of the input h(1)
i , these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
(k,w,m)
Q(2)
k,w,m

op ≤1,
max
(k,w,m)
K(2)
k,w,m

op ≤1,
X
(k,w,m)
V(2)
k,w,m

op ≤2ηC1 + 2ηC3.
Furthermore, a simple calculation shows that
g(w) =:
1
N + 1
N+1
X
i=1
X
(k,w,m)
σ
D
Q(2)
k,w,mhi, K(2)
k,w,mhj
E
V(2)
k,w,mhj = −η
N
N+1
X
j=1


0d+1
P(u1gj, ⟨v1, xj⟩) · xj
r(⟨v1, xj⟩) · gj
...
P(uKgj, ⟨vK, xj⟩) · xj
r(⟨vK, xj⟩) · gj
0


,
where the summation is taken over all possibilities of the tuple (k, w, m), i.e. over the union of
[K] × {1} × [M3] and [K] × {2} × [M1].
37

By our definition, we have |P(s, t) −sr′(t)| ≤εp for all s, t ∈[−R3, R3]. Therefore, for each
i ∈[N], k ∈[K],
|P(ukgj, ⟨vk, xj⟩) −∂1ℓ(pred(xj; w), yj) · uk · r′(⟨vk, xj⟩)| ≤εp + |gj −∂1ℓ(pred(xi; w), yi)| · |uk| · |r′(⟨vk, xj⟩)|
≤εp + BuLr(εℓ+ BuLℓεr),
where Lr := max|t|≤BxBu |r′(t)| is the upper bound of r′. Similarly, for each i ∈[N], k ∈[K], we
have
|r(⟨vk, xj⟩) · gj −r(⟨vk, xj⟩) · ∂1ℓ(pred(xj; w), yj)| ≤2Bgεr + 2Br(εℓ+ BuL2
ℓεr).
As for the case i = N + 1, we have gN+1 = 0 and |P(ukgN+1, ⟨vk, xN+1⟩)| ≤εp for each k ∈[K]
by defintion. Combining these estimations and using (16) and (17), we can conclude that
η−1g(w) + ∇bLN(w)

2 ≤
√
KBx · [εp + BuLr(εℓ+ BuLℓεr)] + 2
√
K[Bgεr + Br(εℓ+ BuLℓεr)].
Thus, to ensure
η−1g(w) + ∇bLN(w)

2 ≤ε, we only need to choose εp, εℓ, εr as
εp =
ε
3
√
KBx
,
εℓ=
ε
9
√
K max {Br, LrBxBu}
,
εr =
ε
15
√
K max {Bg, LℓBrBu, LrLℓBxBrB2u}
.
(18)
Thus, letting the attention layer θ(2)
attn = {(V(2)
k,w,m, Q(2)
k,w,m, K(2)
k,w,m)}(k,w,m), we have
Attnθ(2)
attn : h(1)
i
7→h(1.5)
i
= [xi; y′
i; w + ηg(w); pred(xi; w); gi; 0; 1; ti].
Step 4: construction of θ(2)
mlp. We only need to pick θ(2)
mlp so that it maps
h(1.5)
i
= [xi; y′
i; w + ηg(w); pred(xi; w); gi; 0; 1; ti]
MLP
θ(2)
mlp
−−−−−→h(2)
i
= [xi; y′
i; ProjW(w −ηg(w)); 0; 0; 0; 1; ti].
By our assumption on the map ProjW, this is easy.
Combining the four steps above and taking θ = (θ(1)
attn, θ(1)
mlp, θ(2)
attn, θ(2)
mlp) completes the proof.
E.6
Proof of Lemma D.3
For every ℓ≥0, define the intermediate iterates (before projection)
bwℓ+ 1
2 := bwℓ−η
 ∇f(bwℓ) + εℓ
,
w
ℓ+ 1
2
GD := wℓ
GD −η∇f(wℓ
GD),
so that bwℓ+1 = ProjW(bwℓ+ 1
2 ) and wℓ+1
GD = ProjW(w
ℓ+ 1
2
GD ).
We first prove part (a). We begin by deriving a relation between
bwℓ+1 −bwℓ2
2 and
ηGf
W,η(bwℓ)

2
2.
Let ewℓ+ 1
2 := bwℓ−η∇f(bwℓ) and ewℓ+1 := ProjW(ewℓ+ 1
2 ) denote the exact projected gradient
iterate starting from bwℓ. We have
bwℓ+1 −bwℓ2
2
(i)
≥1
2
ewℓ+1 −bwℓ2
2 −
bwℓ+1 −ewℓ+12
2
(ii)
≥1
2
ewℓ+1 −bwℓ2
2 −
bwℓ+ 1
2 −ewℓ+ 1
2

2
2
(iii)
= η2
2
Gf
W,η(bwℓ)

2
2 −∥ηε∥2
2 ≥η2
2
Gf
W,η(bwℓ)

2
2 −η2ε2.
(19)
Above, (i) uses the inequality ∥a −b∥2
2 ≥1
2 ∥a∥2
2 −∥b∥2
2; (ii) uses the fact that projection to a convex
set is a non-expansion; (iii) uses the definition of the gradient mapping.
By the Lf-smoothness of f within W, we have
f(bwℓ+1) −f(bwℓ) ≤

∇f(bwℓ), bwℓ+1 −bwℓ
+ Lf
2
bwℓ+1 −bwℓ2
2
38

=
*
bwℓ−bwℓ+ 1
2
η
−εℓ, bwℓ+1 −bwℓ
+
+ Lf
2
bwℓ+1 −bwℓ2
2
(i)
≤
 bwℓ−bwℓ+1
η
, bwℓ+1 −bwℓ

+

εℓ, bwℓ−bwℓ+1
+ Lf
2
bwℓ+1 −bwℓ2
2
=

−1
η + Lf
2
 bwℓ+1 −bwℓ2
2 + 1
4η
bwℓ+1 −bwℓ2
2 + η
εℓ2
2
(ii)
≤−1
4η
bwℓ+1 −bwℓ2
2 + η
εℓ2
2
(iii)
≤−1
4η
η2
2
Gf
W,η(bwℓ)

2
2 −
ηεℓ2
2

+ η
εℓ2
2
≤−η
8
Gf
W,η(bwℓ)

2
2 + 5η
4 ε2.
Above, (i) uses the property
D
bwℓ+1 −bwℓ+ 1
2 , bwℓ+1 −bwℓE
≤0 of the projection bwℓ+1 =
ProjW(bwℓ+ 1
2 ) (using bwℓ∈W); (ii) uses Lf/2 ≤1/(2η) by our choice of η ≤1/Lf; (iii)
uses (19).
Rearranging and summing the above over ℓ= 0, . . . , L −1, we obtain
η
8
L−1
X
ℓ=0
Gf
W,η(bwℓ)

2
2 ≤f(w0) −f(bwL) + 5ηL
4 ε2.
Dividing both sides by ηL/8 yields part (a).
Next, we prove part (b). Let C := 1 + ηLf. We prove by induction that
bwℓ−wℓ
GD

2 ≤Cℓ−1
C −1 · ηε
(20)
for all ℓ≥0. The base case of ℓ= 0 follows by definition that bw0 = w0
GD = w0. Suppose the result
holds for ℓ. Then for ℓ+ 1, we have
bwℓ+1 −wℓ+1
GD

2
(i)
≤
bwℓ+ 1
2 −w
ℓ+ 1
2
GD

2 =
bwℓ−η
 ∇f(bwℓ) −εℓ
−
 wℓ
GD −η∇f(wℓ
GD)

2
(ii)
≤C
bwℓ−wℓ
GD

2 + ηε
(iii)
≤C · Cℓ−1
C −1 · ηε + ηε = Cℓ+1 −1
C −1
· ηε.
Above, (i) uses again the non-expansiveness of the convex projection ProjW; (ii) uses the fact that
the operator w 7→w −η∇f(w) is (1 + ηLf) = C-Lipschitz; and (iii) uses the inductive hypothesis.
This proves the case for ℓ+ 1 and thus finishes the induction. We can further relax (20) into
bwℓ−wℓ
GD

2 ≤
Cℓ
1 + ηLf −1 · ηε = L−1
f (1 + ηLf)ℓε.
This proves part (b).
F
Proofs for Section 3.1
F.1
Proof of Theorem 4
Fix λ ≥0, 0 ≤α ≤β with κ := β+λ
α+λ, and Bw > 0, and consider any in-context data D such that
the precondition of Theorem 4 holds. Let
Lridge(w) :=
1
2N
N
X
i=1
(⟨w, xi⟩−yi)2 + λ
2 ∥w∥2
2
39

denote the ridge regression loss in (ICRidge), so that wλ
ridge = arg minw∈Rd Lridge(w). It is a
standard result that ∇2Lridge(w) = X⊤X/N + λId, so that Lridge is (α + λ)-strongly convex and
(β + λ)-smooth over Rd.
Consider the gradient descent algorithm on the ridge loss
wt+1
GD = wt
GD −η∇Lridge(wt
GD)
with initialization, learning rate, and number of steps
w0
GD := 0d,
η :=
1
β + λ,
T :=

2κ log
BxBw
2ε

.
By standard convergence results for strongly convex and smooth functions (Proposition B.2), we
have for all t ≥1 that
wt
GD −wλ
ridge
2
2 ≤exp

−t
κ
 w0
GD −wλ
ridge
2
2 = exp

−t
κ
 wλ
ridge
2
2 .
Further, we have
wT
GD −wλ
ridge

2 ≤exp

−T
2κ
 wλ
ridge

2 ≤
2ε
BxBw
· Bw
2
≤ε
Bx
.
(21)
It remains to construct a transformer to approximate wT
GD. Notice that the problem (ICRidge)
corresponds to an ℓ2-regularized ERM with the square loss ℓ(s, t) :=
1
2(s −t)2, whose partial
derivative ∂sℓ(s, t) = s −t is exactly a sum of two relus:
∂sℓ(s, t) = 2σ((s −t)/2) −2σ(−(s −t)/2).
In particular, this shows that ∂sℓ(s, t) is (0, R, 2, 4)-approximable for any R > 0, in particular for
R = max {BxBw, By, 1}.
Therefore, we can apply Corollary E.1 with the square loss ℓ, learning rate η, regularization strength
λ and accuracy parameter ε = 0 to obtain that there exists an attention-only transformer TF0
θ with
(T + 1) := L layers such that the final output h(L)
N+1 = [xN+1; byN+1; ∗] with
byN+1 −

wT
GD, xN+1
 = 0,
(22)
and number of heads M (ℓ) = 3 for all ℓ∈[L −1] (can be taken as 2 in the unregularized case λ = 0
directly by Theorem D.1), and M (L) = 2. Further, θ admits norm bound |||θ||| ≤2 + R + 8+λ
β+λ ≤
3R + 8(β + λ)−1 + 1 ≤4R + 8(β + λ)−1.
Combining (21) and (22), we obtain that
byN+1 −

wλ
ridge, xN+1
 =

wT
GD −wλ
ridge, xN+1
 ≤(ε/Bx) · Bx = ε.
Further, we have readw(hT
i ) = wT
GD for all i ∈[N +1], where readw(h) := h(d+2):(2d+1) (cf. Corol-
lary E.1), so that ∥readw(hT
i ) −wλ
ridge∥2 ≤ε/Bx as shown above. This finishes the proof.
F.2
Statistical analysis of in-context least squares
Consider the standard least-squares algorithm ALS and least-squares estimator bwLS ∈Rd defined as
ALS(D)(xN+1) := ⟨bwLS, xN+1⟩,
bwLS =
 X⊤X
−1X⊤y ∈Rd.
(ICLS)
For any distribution P over (x, y) ∈Rd × R and any estimator w ∈Rd, let
LP(w) := E(x′,y)∼P
h
1
2(⟨w, x′⟩−y′)2i
denote the expected risk of w over a new test example (x′, y′) ∼P.
Assumption A (Well-posedness for learning linear predictors). We say a distribution P on Rd × R is
well-posed for learning linear predictors, if (x, y) ∼P satisfies
40

(1) ∥x∥2 ≤Bx and |y| ≤By almost surely;
(2) The covariance ΣP := EP[xx⊤] satisfies λminId ⪯ΣP ⪯λmaxId, with 0 < λmin ≤λmax,
and κ := λmax/λmin.
(3) The whitened vector Σ−1/2
P
x is K2-sub-Gaussian for some K ≥1.
(4) The best linear predictor w⋆
P := EP[xx⊤]−1EP[xy] satisfies ∥w⋆
P∥2 ≤B⋆
w.
(5) We have E[(y −⟨x, w⋆
P⟩)2|x] ≤σ2 with probability one (over x).
Further, we say P is well-posed with canonical parameters if
Bx = Θ(
√
d),
By = Θ(1),
B⋆
w = Θ(1),
σ ≤O(1),
λmax = Θ(1),
K = Θ(1),
(23)
where Θ(·) and O(·) only hides absolute constants.
The following result bounds the excess risk of least squares under Assumption A with a clipping
operation on the predictor; the clipping allows the result to only depend on the second moment of the
noise (cf. Assumption A(5)) instead of e.g. its sub-Gaussianity, and also makes the result convenient
to be directly translated to a result for transformers.
Proposition F.1 (Guarantees for in-context least squares). Suppose distribution P satisfies Assump-
tion A. Then as long as N ≥O(dK4 log(1/δ)), we have the following:
(a) The (clipped) least squares predictor achieves small expected excess risk (fast rate) over the best
linear predictor: For any clipping radius R ≥By,
ED,xN+1,yN+1∼P
1
2(clipR(⟨bwLS, xN+1⟩) −yN+1)2

≤inf
w∈Rd LP(w)
|
{z
}
LP(w⋆
P)
+O

R2δ + dσ2
N

.
(24)
(b) We have P(Ecov ∩Ew) ≥1 −δ/10, where
Ecov = Ecov(D) :=
1
2Id ⪯Σ−1/2
P
bΣΣ−1/2
P
⪯2Id

,
(25)
Ew = Ew(D) :=


∥bwLS∥2 ≤B⋆
w +
s
80dσ2
δNλmin


.
(26)
Proof. We first show P(Ecov) ≥1−δ/20. Let bΣ := 1
N
PN
i=1 xix⊤
i , and let the whitened covariance
and noise variables be denoted as
exi = Σ−1/2
P
xi,
eΣ := 1
N
N
X
i=1
exiex⊤
i = Σ−1/2
P
bΣΣ−1/2
P
.
Also let zi := yi −⟨xi, w⋆
P⟩denote the “noise” variables. Note that
Ecov =
1
2Id ⪯eΣ ⪯2Id

is exactly a covariance concentration of the whitened vectors {exi}i∈[N]. Recall that E[exiex⊤
i ] = Id,
and exi are K2-sub-Gaussian by assumption. Therefore, we can apply [85, Theorem 4.6.1], we have
with probability at least 1 −δ/10 that
eΣ −Id

op ≤O
 
K2 max
(r
d + log(1/δ)
N
, d + log(1/δ)
N
)!
.
Setting N ≥O(K4(d + log(1/δ))) ensures that the right-hand side above is at most 1/2, on which
event we have
1
2Id ⪯eΣ ⪯3
2Id ⪯2Id,
(27)
41

i.e. Ecov holds. This shows that P(Ec
cov) ≤δ/10.
Next, we show (24). Using Ecov, we decompose the risk as
E
1
2(clipR(⟨bwLS, xN+1⟩) −yN+1)2

= E
1
2(clipR(⟨bwLS, xN+1⟩) −yN+1)21{Ecov}

+ E
1
2(clipR(⟨bwLS, xN+1⟩) −yN+1)21{Ec
cov}

(i)
≤E
1
2(⟨bwLS, xN+1⟩−yN+1)21{Ecov}

+ 2R2 · (δ/20)
(ii)
= ED,xN+1
1
2(⟨bwLS −w⋆
P, xN+1⟩)21{Ecov}

+ ExN+1,yN+1
1
2(⟨w⋆
P, xN+1⟩−yN+1)21{Ecov}

+ O(R2δ)
≤ED
1
2 ∥bwLS −w⋆
P∥2
ΣP 1{Ecov}

+ ExN+1,yN+1
1
2(⟨w⋆
P, xN+1⟩−yN+1)2

|
{z
}
LP(w⋆
P)
+O(R2δ).
(28)
Above, (i) follows by assumption that |yN+1| ≤By ≤R almost surely, so that removing
the clipping can only potentially increase the distance in the first term, and the square loss is
upper bounded by
1
2 · (2R)2 almost surely in the second term; (ii) follows by the fact that
ExN+1,yN+1[⟨bwLS −w⋆
P, xN+1⟩(⟨w⋆
P, xN+1⟩−yN+1)] = 0 by the definition of w⋆
P, as well as
the fact that 1{Ecov} is independent of (xN+1, yN+1).
It thus remains to bound ED
h
1
2 ∥bwLS −w⋆
P∥2
ΣP 1{Ecov}
i
. Note that on the event Ecov, we have
Σ1/2
P
bΣ−1Σ1/2
P
=

Σ−1/2
P
bΣΣ−1/2
P
−1
⪯2Id.
Therefore,
1
2 ∥bwLS −w⋆
P∥2
ΣP 1{Ecov} = 1
2
 (X⊤X)−1X⊤y −w⋆
P
⊤ΣP
 (X⊤X)−1X⊤y −w⋆
P

1{Ecov}
= 1
2z⊤X(X⊤X)−1ΣP(X⊤X)−1X⊤z · 1{Ecov}
=
1
2N 2 z⊤XΣ−1/2
P

Σ1/2
P
bΣ−1Σ1/2
P
2
Σ−1/2
P
X⊤z · 1{Ecov}
≤
2
N 2
Σ−1/2
P
X⊤z

2
2 1{Ecov} =
2
N 2

N
X
i=1
exizi

2
2
1{Ecov} ≤
2
N 2

N
X
i=1
exizi

2
2
.
Note that E[exizi] = Σ−1/2
P
E[xi(yi −⟨w⋆
P, xi⟩)] = 0. Therefore, taking expectation on the above
(over D), we get
ED
1
2 ∥bwLS −w⋆
P∥2
ΣP 1{Ecov}

≤
2
N 2 E



N
X
i=1
exizi

2
2

= 2
N E
h
∥ex1z1∥2
2
i
= 2
N E

z2
1x⊤
1 Σ−1
P x1

(29)
(i)
≤2σ2
N E

x⊤
1 Σ−1
P x1

= 2dσ2
N
.
(30)
Above, (i) follows by conditioning on x1 and using Assumption A(5). Combining with (28), we
obtain
E
1
2(clipR(⟨bwLS, xN+1⟩) −yN+1)2

≤LP(w⋆
P) + O

R2δ + dσ2
N

.
This proves (24).
Finally, we show P(Ecov ∩Ew) ≥1 −δ/10. Using (29) and ΣP ⪰λminId by assumption, we get
E
h
∥bwLS −w⋆
P∥2
2 1{Ecov}
i
≤4dσ2
Nλmin
.
42

Therefore, using an argument similar to Chebyshev’s inequality,
P(Ecov ∩Ec
w) = E

1{Ecov} × 1{∥bwLS∥2 >
s
20
δ · 4dσ2
Nλmin
+ B⋆
w}


≤E

1{Ecov} × 1{∥bwLS −w⋆
P∥2 >
s
20
δ · 4dσ2
Nλmin
}


≤E
"
1{Ecov} × ∥bwLS −w⋆
P∥2
2
20
δ ·
4dσ2
Nλmin
#
≤δ/20.
This implies that
P(Ecov ∩Ew) = P(Ecov) −P(Ecov ∩Ec
w) ≥1 −δ/20 −δ/20 ≥1 −δ/10.
This is the desired result.
F.3
Proof of Corollary 5
The proof follows by first checking the well-conditionedness of the data D (cf. (5)) with high
probability, then invoking Theorem 4 (for approximation least squares) and Proposition F.1 (for the
statistical power of least squares).
First, as P satisfies Assumption A, by Proposition F.1, as long as N ≥O(K4(d + log(1/δ))), we
have with probability at least 1 −δ/10 that event Ecov ∩Ew holds. On this event, we have
1
2λminId ⪯1
2ΣP ⪯bΣ = X⊤X/N ⪯2ΣP ⪯2λmaxId,
∥bwLS∥2 ≤Bw/2 := O

B⋆
w +
s
dσ2
δNλmin

,
and thus the dataset D is well-conditioned (in the sense of (5)) with parameters α = λmin/2,
β = 2λmax, and Bw defined as above. Note that the condition number of bΣ is upper bounded
by β/α = 4λmax/λmin ≤4κ, where κ is the upper bound on the condition number of ΣP as
in Assumption A(c).
Define parameters
ε =
r
dσ2
N ,
δ = dσ2
B2yN ∧1.
(31)
Note that Bw ≤O(B⋆
w +
q
B2y/λmin) by the above choice of δ.
We can thus apply Theorem 4 in the unregularized case (λ = 0) to obtain that, there exists a
transformer θ with maxℓ∈[L] M (ℓ) ≤3, |||θ||| ≤4R + 4/λmax (with R = max {BxBw, By, 1}), and
number of layers
L ≤O

κ log BxBw
ε

≤O
 
κ log
 
Bx
r
N
dσ2
 
B⋆
w +
B2
y
√λmin
!!!
,
such that on Ecov ∩Ew (so that D is well-conditioned), we have (choosing the clipping radius in
g
ready(·) = clipBy(ready(·)) to be By):
 g
ready(TF0
θ(H)) −clipBy(⟨bwLS, xN+1⟩)
 ≤
ready(TF0
θ(H)) −⟨bwLS, xN+1⟩
 ≤ε =
r
dσ2
N .
(32)
We now bound the excess risk of the above transformer. Combining Proposition F.1 and (32), we
have
E

g
ready(TF0
θ(H)) −yN+1
2
43

= E

g
ready(TF0
θ(H)) −yN+1
2
1{Ecov ∩Ew}

+ E

g
ready(TF0
θ(H)) −yN+1
2
1{(Ecov ∩Ew)c}

≤2E

g
ready(TF0
θ(H)) −clipBy(⟨bwLS, xN+1⟩)
2
1{Ecov ∩Ew}

+ 2E

clipBy(⟨bwLS, xN+1⟩) −yN+1
2
1{Ecov ∩Ew}

+ 2B2
y · δ/10
(i)
≤2ε2 + LP(w⋆
P) + O

B2
yδ + dσ2
N

+ O
 B2
yδ

≤LP(w⋆
P) + O

B2
yδ + dσ2
N

≤O
dσ2
N

.
Above, (i) uses the approximation guarantee (32) as well as Proposition F.1(a) (with clipping radius
By). This proves the desired excess risk guarantee.
Finally, under the canonical choice of parameters (23), the bounds for L, M, |||θ||| simplify to
L ≤O

κ log Nκ
σ

,
max
ℓ∈[L] M (ℓ) ≤3,
|||θ||| ≤O(
√
κd),
(33)
and the requirement for N simplifies to N ≥O(d + log(1/δ)) = e
O(d) (as K = Θ(1)). This proves
the claim about the required N and L.
F.4
Proof of Corollary 6
Fix parameters δ, ε > 0 to be specified later and a large universal constant C0. Let us set
α = max
n
0, 1/2 −
p
d/N
o2
,
β = 25,
B⋆
w := 1 + 2
r
log(4/δ)
d
,
Bw = C0(B⋆
w + σ),
Bx = C0
p
d log(N/δ),
By = C0(B⋆
w + σ)
p
log(N/δ).
Consider the following good events (below ε = [εi]i∈[N] ∈RN is given by εi = yi −⟨w⋆, xi⟩)
Eπ =
n
∥w⋆∥2 ≤B⋆
w, ∥ε∥2 ≤2
√
Nσ
o
,
Ew =

α ≤λmin(X⊤X/N) ≤λmax(X⊤X/N) ≤β
	
,
Eb = {∀i ∈[N], ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,N+1 = {∥xN+1∥2 ≤Bx, |yN+1| ≤By},
and we define E := Eπ ∩Ew ∩Eb ∩Eb,N+1. Under the event E, the problem (ICRidge) is well-
conditioned and ∥wλ
ridge∥≤Bw/2 (by Lemma F.1).
Therefore, Theorem 4 implies that for κ = α+λ
β+λ, there exists a L = ⌈2κ log(Bw/ε)⌉+ 1-layer
transformer θ with prediction byN+1 := g
ready(TF0
θ(H)) (clipped by By), such that under the good
event E, we have byN+1 = clipBy(⟨xN+1, bw⟩) and ∥bw −wλ
ridge∥≤ε.
In the following, we show that θ is indeed the desired transformer (when ε and δ is suitably chosen).
Notice that we have
E(byN+1 −yN+1)2 = E

1{E}(byN+1 −yN+1)2
+ E

1{Ec}(byN+1 −yN+1)2
,
and we analyze these two parts separately.
Prediction risk under good event E.
We first note that
E

1{E}(byN+1 −yN+1)2
= E
h
1{E}(clipBy(⟨xN+1, bw⟩) −yN+1)2i
≤E

1{E}(⟨xN+1, bw⟩−yN+1)2
,
44

where the inequality is because yN+1 ∈[−By, By] under the good event E. Notice that by our
construction, under the good event E, bw = bw(D) depends only on the dataset D7. Therefore, we
have ∥bw(D) −wλ
ridge(D)∥≤ε as long as the event E0 := Eπ ∩Ew ∩Eb holds for (w⋆, D). Thus,
under E0,
E

1{E}(⟨xN+1, bw⟩−yN+1)2 w⋆, D

= E

1{E}(⟨xN+1, bw(D)⟩−yN+1)2 w⋆, D

≤E

(⟨xN+1, bw(D)⟩−yN+1)2 w⋆, D

= E

(⟨xN+1, bw(D)⟩−⟨xN+1, w⋆⟩)2 w⋆, D

+ σ2
= ∥bw(D) −w⋆∥2
2 + σ2,
and we also have
∥bw(D) −w⋆∥2
2 ≤
wλ
ridge −w⋆
2
2 + 2
wλ
ridge −w⋆

2
bw(D) −wλ
ridge

2 +
bw(D) −wλ
ridge
2
2
≤
wλ
ridge −w⋆
2
2 + 2ε
wλ
ridge −w⋆

2 + ε2.
Recall that 2BayesRiskπ = Ew⋆,D∥wλ
ridge −w⋆∥2
2 + σ2. Note that 2BayesRiskπ ≤1 + σ2 by
definition. Therefore, we can conclude that
E

1{E}(byN+1 −yN+1)2
≤2BayesRiskπ + 2ε + ε2.
Prediction risk under bad event Ec.
Notice that
E

1{Ec}(byN+1 −yN+1)2
≤
p
P(Ec)E[(byN+1 −yN+1)4].
We can upper bound P(Ec) = P(Ec
π ∪Ec
w ∪Ec
b ∪Ec
b,N+1) by Lemma B.1, Lemma B.2 and the
sub-Gaussian tail bound:
P(Ec
π) ≤δ
2 + exp(−N/8),
P(Ec
w) ≤2 exp(−N/8),
P(Ec
b ∪Ec
b,N+1) ≤δ
4.
Thus, as long as N ≥8 log(12/δ), we have P(Ec) ≤δ. Further, a simple calculation yields
E(byN+1 −yN+1)4 ≤8Eby4
N+1 + 8Ey4
N+1 ≤8B2
y + 8Ey4
N+1.
Notice that yN+1|w⋆∼N(0, ∥w⋆∥2
2+σ2), hence Ey4
N+1 = 3E(∥w⋆∥2
2+σ2)2 ≤3(3+2σ2+σ4) ≤
B4
y. Thus, we can conclude that
E

1{Ec}(byN+1 −yN+1)2
≤4
√
δBy.
Choosing ε and δ.
Combining the inequalities above, we have
E(byN+1 −yN+1)2 ≤2BayesRiskπ +
h
2ε
p
2BayesRiskπ + ε2 + 4
√
δBy
i
.
To ensure 1
2E(byN+1 −yN+1)2 ≤BayesRiskπ + ε, we only need to take (ε, δ) so that the following
constraints are satisfied:
ε = 1
2 min

ε, √ε
	
,
4
√
δBy ≤ε
2,
N ≥8 log(12/δ).
Therefore, it suffices to take δ =
c0
log2(N)

ε2
1+σ2
2
for some small constant c0, then as long as
N ≥C log
σ2 + 1
ε

+ C.
our choice of ε and δ is feasible. Note that κ ≤O
 1 + σ−2
, and hence under such choice of (ε, δ),
we have L = O(log(1/ε)) and |||θ||| = eO
√
d

. This is the desired result.
7We need this, as on Ec, the transformer output at this location could in principle depend additionally on
xN+1, as (15) may not hold due to the potential unbounededness of its input. A similar fact will also appear in
later proofs (for generalized linear models and Lasso).
45

Lemma F.1. Under the event Eπ ∩Ew, we have
wλ
ridge

2 ≤O (B⋆
w + σ).
Proof of Lemma F.1. By the definition of wλ
ridge and recall that λ = dσ2/N, we have wλ
ridge =
(X⊤X + dσ2Id)−1X⊤y.
Therefore, we only need to prove the following fact: for any γ > 0 and bβ = (X⊤X + dγId)−1X⊤y,
we have
∥bβ∥2 ≤B⋆
w + 10σ(1 + γ−1/2).
(34)
We now prove (34). Note that we have
∥bβ∥2 = ∥(X⊤X + dγId)−1X⊤(Xw⋆+ ε)∥2 ≤∥B1∥op ∥w⋆∥2 + ∥B2∥op ∥ε∥2
where B1 = X⊤X(X⊤X + dγId)−1, B2 = (X⊤X + dγId)−1X⊤. Note that ∥B1∥op ≤1 clearly
holds, and under Eπ we also have ∥ε∥2 ≤2
√
Nσ. Therefore, it remains to bound the term ∥B2∥op.
Consider the SVD decomposition of X = UΣV , Σ = diag(λ1, · · · , λd), and U ∈RN×d, V ∈Rd×d
are orthonormal matrices. Then B2 = V ⊤(Σ2 + dγId)−1ΣU ⊤, and hence
∥B2∥op =
(Σ2 + dγId)−1Σ

op = max
i
λi
λ2
i + dγ .
When N ≤36d, we directly have ∥B2∥op ≤
1
2(dγ)−1/2 ≤3(Nγ)−1/2. Otherwise, we have
N ≥36d, and then for each i ∈[d], λi ≥
p
λmin(X⊤X) ≥
√
αN ≥
√
N/3. Hence, in this case
we also have ∥B2∥op ≤maxi λ−1
i
≤3N −1/2. Combining the both cases completes the proof of
(34).
G
In-context learning of generalized linear models
As a natural generalization of linear regression, we now show that transformers can recover learn
generalized linear models (GLMs) [53] (which includes logistic regression for linear classification as
an important special case), by implementing the corresponding convex risk minimization algorithm
in context, and achieve near-optimal excess risk under standard statistical assumptions.
Let g : R →R be a link function that is non-decreasing and C2-smooth. We consider the following
convex empirical risk minimization (ERM) problem
wGLM := arg min
w∈Rd
bLN(w) := 1
N
N
X
i=1
ℓ(⟨xi, w⟩, yi),
(ICGLM)
where ℓ(t, y) := −yt+
R t
0 g(s)ds is the convex (integral) loss associated with g. A canonical example
of (ICGLM) is logistic regression, in which g(t) = σlog(t) := (1 + e−t)−1 is the sigmoid function,
and the resulting ℓ(t, y) = ℓlog(t, y) = −yt + log(1 + et) is the logistic loss.
The following result (proof in Appendix G.1) shows that, as long as the empirical risk bLN satisfies
strong convexity and bounded solution conditions (similar as in Theorem 4), transformers can
approximately implement the ERM predictor g(⟨xN+1, wGLM⟩), with wGLM given by (ICGLM).
Theorem G.1 (Implementing convex risk minimization for GLMs). For any 0 < α < β with κ := β
α,
Bw > 0, Bx > 0, κw := LgB2
x/α + 1 and ε < Bw/2, there exists an attention-only transformer
TF0
θ with
L = ⌈2κ log(LgBwBx/ε)⌉+ 1,
max
ℓ∈[L] M (ℓ) ≤e
O
 C2
gκ2
wε−2
,
|||θ||| ≤O
 R + β−1Cg

,
(where Lg := supt |g′(t)|, R := max {BxBw, By, 1}, and Cg > 0 is a constant that depends only
on R and the C2-smoothness of g within [−R, R]), such that the following holds. On any input data
(D, xN+1) such that
α ≤λmin(∇2bLN(w)) ≤λmax(∇2bLN(w)) ≤β for all w ∈B2(Bw),
∥wGLM∥2 ≤Bw/2,
(35)
46

TF0
θ(H(0)) approximately implements (ICGLM): We have h(L+1)
N+1 := [xN+1; byN+1; bw; 1; 1], where
|byN+1 −g(⟨xN+1, wGLM⟩)| ≤ε.
In Theorem G.1, the number of heads scales as e
O(1/ε2) as opposed to Θ(1) as in ridge regression
(Theorem 4), due to the fact that the gradient of the loss is in general a smooth function that can be
only approximately expressed as a sum-of-relus (cf. Definition D.1 & Lemma B.5) rather than exactly
expressed as in the case for the square loss.
In-context prediction power
We next show that (proof in Appendix G.2) the transformer con-
structed in Theorem G.1 achieves desirable statistical power if the in-context data distribution satisfies
standard statistical assumptions for learning GLMs. Let LP(w) := E(x,y)∼P[ℓ(⟨w, x⟩, y)] denote the
corresponding population risk for any distribution P of (x, y). When P is realizable by a generalized
linear model of link function g and parameter β in the sense that EP[y|x] = g(⟨β, x⟩), it is a standard
result that β is indeed a minimizer of LP [42] (see also [6, Appendix A.3]).
Theorem G.2 (Statistical guarantee for generalized linear models). For any fixed set of param-
eters defined in Assumption B, there exists a transformer θ with L ≤O (log(N)) layers and
maxℓ∈[L] M (ℓ) ≤e
O
 d3N

, such that for any distribution P satisfying Assumption B with those pa-
rameters, as long as N ≥O (d), that outputs byN+1 = g
ready(TFθ(H)) and bw = g
readw(TFθ(H)) ∈
Rd (for another read-out function g
readw) satisfying the following.
(a) bw achieves small excess risk under the population loss, i.e. for the linear prediction
bylin
N+1 := ⟨xN+1, bw⟩,
E(D,xN+1,yN+1)∼P

ℓ(bylin
N+1, yN+1)

−min
β LP(β) ≤O (d/N) .
(36)
(b) (Realizable setting) If there exists a β ∈Rd such that under P, E[y|x] = g(⟨β, x⟩) almost
surely, then
E(D,xN+1,yN+1)∼P

(byN+1 −yN+1)2
≤E(xN+1,yN+1)∼P

(g(⟨β, xN+1⟩) −yN+1)2
+ O (d/N) ,
(37)
or equivalently, E[(byN+1 −E[yN+1|xN+1])2] ≤O (d/N).
Above, O (·) hides constants that depend polynomially on the parameters in Assumption B. Similar
as in Corollary 5, the O(d/N) excess risk obtained here matches the optimal (fast) rate for typical
learning problems with d parameters and N samples [87].
Assumption B (Well-posedness for learning GLMs). We assume that there is some Bµ > 0 such
that for any t ∈[−Bµ, Bµ], g′(t) ≥µg > 0.
We also assume that for each i ∈[N + 1], (xi, yi) is independently sampled from P such that the
following holds.
(a) Under the law (x, y) ∼P, We have x ∼SG(Kx), y ∼SG(Ky) and g(⟨w, x⟩) ∼
SG(Ky) ∀w ∈B2(Bw).
(b) For some µx > 0, it holds that
E[1{|x⊤w| ≤Bµ/2}xx⊤] ⪰µxId
∀w ∈B2(Bw).
(c) For β⋆= arg min LP, it holds ∥β⋆∥2 ≤Bw/4.
Applying Theorem G.2 to logistic regression, we have the following result as a direct corollary. Below,
the Gaussian input assumption is for convenience only and can be generalized to e.g. sub-Gaussian
input.
Corollary G.1 (In-context logistic regression). Consider any in-context data distribution P satisfying
x ∼N(0, Id),
y ∈{0, 1},
arg min
β∈Rd
LP(β) ∈B2(B⋆
w).
For the link function g = σlog and B⋆
w = O (1), we can choose Bw, Bµ, µg, Lg, µx, Kx, Ky = Θ (1)
so that Assumption B holds. In that case, when N ≥O (d), there exists a transformer θ with
L = O (log(N)) layers, such that for any P considered above,
47

(a) The estimation bw = g
readw(TFθ(H)) outputted by θ achieves excess risk bound (36).
(b) (Realizable setting) Consider the logistic in-context data distribution
Plog
β :
x ∼N(0, Id),
y|x ∼Bernoulli(g(⟨β, x⟩)).
Then, for any distribution P = Plog
β
with ∥β∥2
≤B⋆
w, the prediction byN+1
=
g
ready(TFθ(H)) of θ additionally achieves the square loss excess risk (37).
G.1
Proof of Theorem G.1
Let us fix parameters εg > 0 and T > 0 (that we specify later in proof).
Define R = max{BxBw, By, 1} and
Cg := max
i=0,1,2

Ri
max
s∈[−B,B]
g(i)(s)


.
By Proposition B.1, g is (εg, M, R, C) with
C ≤O (Cg) ,
M ≤O
 C2
gε−2
g
log(1 + Cgε−1
g )

.
Therefore, we can invoke Theorem D.1 to obtain that, as long as 2Tεg ≤Bw, there exists a T-layer
attention-only transformer θ(1:T ) with M heads per layer, such that for any input H of format (3)
and satisfies (35), its last layer outputs h(T )
i
= [xi; y′
i; bwT ; 0D−2d−3; 1; ti], such that
bwT −wT
GD

2 ≤εg · (Lβ−1Bx),
where {wℓ
GD}ℓ∈[L] is the sequence of gradient descent iterates with stepsize β−1 and initialization
w0
GD = 0. Notice that Proposition B.2 implies (with κ := β/α)
wT
GD −wGLM

2 ≤exp(−T/(2κ)) ∥wGLM∥2 ≤exp(−T/(2κ)) · Bw
2
:= εo.
Furthermore, we can show that (similar to the proof of Theorem D.1 (b)), there exists a single attention
layer θ(T +1) with M heads such that it outputs h(T +1)
N+1
= [xN+1; byN+1; bwT ; 0D−2d−3; 1; 0], where
byN+1 −g(

xN+1, bwT 
)
 ≤εg.
In the following, we show that for suitably chosen (T, εg), θ = (θ(1:T ), θ(T +1)) is the desired
transformer. First notice that its output h(T +1)
N+1
= [xN+1; byN+1; bwT ; 0D−2d−3; 1; 0] satisfies
|byN+1 −g(⟨xN+1, wGLM⟩)| ≤
byN+1 −g(

xN+1, bwT 
)
 + Lg

xN+1, bwT 
−⟨xN+1, wGLM⟩

≤εg + LgBx
bwT −wT
GD

2 + LgBx
wT
GD −wGLM

2
≤εg(1 + LgBx · Tβ−1Bx) + LgBxεo.
Therefore, for any fixed ε > 0, we can take
T = ⌈2κ log(LgBxBw/ε)⌉,
εg = 1
2
ε
1 + T · (LgB2xβ−1),
so that the θ we construct above ensures |byN+1 −g(⟨xN+1, wGLM⟩)| ≤ε for any input H that
satisfies (35). The upper bound on |||θ||| follows immediately from Theorem D.1.
G.2
Proof of Theorem G.2
We summarize some basic and useful facts about GLM in the following theorem. Its proof is presented
in Appendix G.3 - G.6.
Theorem G.3. Under Assumption B, the following statements hold with universal constant C0 and
constant C1, C2 that depend only on the parameters (Kx, Ky, Bµ, Bw, µx, Lg, µg).
48

(a) As long as N ≥C1 · d, the following event happens with probability at least 1 −2e−N/C1:
Ew :
1
8µgµx ≤λmin(∇2bLN(w)) ≤λmax(∇2bLN(w)) ≤8LgK2
x,
∀w ∈B2(Bw).
(b) For any δ > 0, we have with probability at least 1 −δ that
εstat :=
sup
w∈B2(Bw)
∇w bLN(w) −∇wE[bLN(w)]

2 ≤C0KxKy max
(r
dι + log(1/δ)
N
, dι + log(1/δ)
N
)
,
where we denote ι = log(2 + LgK2
xBw/Ky).
(c) Condition on (a) holds and N ≥C2 · d, the event Er := {∥wGLM∥2 ≤Bw/2} happens with
probability at least 1 −eN/C2.
(d) For any w ∈B2(Bw), it holds that
Lp(w) −Lp(β) ≤
4
µgµx

ε2
stat +
∇bLN(w)
2
2

.
(e) (Realizable setting) As long as wGLM ∈B2(Bw), it holds that
Ex(g(⟨x, wGLM⟩) −g(⟨x, β⟩))2 ≤
Lg
µxµg
ε2
stat.
Therefore, we can set
α = µgµx
8
,
β = 8LgK2
x,
Bx = C0Kx
p
d log(N/δ),
By = C0Ky
p
log(N/δ).
Consider the following good events
Eb = {∀i ∈[N], ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,N+1 = {∥xN+1∥2 ≤Bx, |yN+1| ≤By},
E = Er ∩Ew ∩Eb ∩Eb,N+1.
Under the event E and our choice of α, β, the problem (ICGLM) is well-conditioned (i.e. (35) holds).
Theorem G.1 implies that there exists a transformer θ such that for any input H of the form (3),
TFθ outputs h′
N+1 = [xN+1; eyN+1; ew; 0D−2d−3; 1; 0], such that the output is given by byN+1 =
g
ready(TFθ(H)) = clipBy(eyN+1) and bw = g
readw(TFθ(H)) := ProjB2(Bw)(ew), and the following
holds on the good event E:
(a) eyN+1 = fD(xN+1), where fD = A(D) is a predictor such that |fD(x) −g(⟨x, wGLM⟩)| ≤
ε for all x ∈B2(Bx).
(b) ew = ew(D) ∈B2(Bw) depends only on D (by the proof of Theorem G.1 and Theorem D.1),
such that
∇bLN(ew)

2 ≤
βε
LgBw .
In the following, we show that θ constructed above fulfills both (a) & (b) of Theorem G.2. The
bounds on number of layers and heads and |||θ||| follows from plugging our choice of Bx, By in our
proof of Theorem G.1.
Proof of Theorem G.2 (a). Notice that under the good event E, we have bw = ew = ew(D) depends
only on D. Then we have
E(D,xN+1,yN+1)

ℓ(bylin
N+1, yN+1)

= E(D,xN+1,yN+1)

1{E}ℓ(bylin
N+1, yN+1)

+ E(D,xN+1,yN+1)

1{Ec}ℓ(bylin
N+1, yN+1)

= E(D,xN+1,yN+1)[1{E}ℓ(⟨xN+1, ew(D)⟩, yN+1)] + E(D,xN+1,yN+1)

1{Ec}ℓ(bylin
N+1, yN+1)

.
Thus, we can consider E0 = Er ∩Ew ∩Eb, and then
E(D,xN+1,yN+1)[1{E}ℓ(⟨xN+1, ew(D)⟩, yN+1)]
49

= E(D,xN+1,yN+1)[1{E0}ℓ(⟨xN+1, ew(D)⟩, yN+1)] −E(D,xN+1,yN+1)[1{E0 −E}ℓ(⟨xN+1, ew(D)⟩, yN+1)]
= E(D,xN+1,yN+1)[1{E0}Lp(ew(D))] −E(D,xN+1,yN+1)[1{E0 −E}ℓ(⟨xN+1, ew(D)⟩, yN+1)],
where the second equality follows from Lp(bw(D)) = E(xN+1,yN+1)|Dℓ(⟨xN+1, ew(D)⟩, yN+1).
Therefore,
E(D,xN+1,yN+1)

ℓ(bylin
N+1, yN+1)

−ED[1{E0}Lp(ew(D))]
= E(D,xN+1,yN+1)

1{Ec}ℓ(bylin
N+1, yN+1)

−E(D,xN+1,yN+1)[1{E0 −E}ℓ(⟨xN+1, ew(D)⟩, yN+1)]
≤2
q
P(Ec) · max

E

ℓ(bylin
N+1, yN+1)4
, E[ℓ(⟨xN+1, ew(D)⟩, yN+1)4]
	
= O
 B2
ℓ
N 5

,
where the last line follows from Cauchy inequality and the fact P(Ec) = O
 N −10
, and Bℓis
defined in Lemma G.1.
Notice that by Theorem G.3 (d), we have
ED[1{E0}(Lp(ew) −inf Lp)] ≤
4
µgµx

E[ε2
stat] + E
h
1{E0}
∇bLN(ew)
2
2
i
,
and by Theorem G.3 (b) and taking integration over δ > 0, we have
E[ε2
stat] ≤O (1) · K2
xK2
y
 
dι
N +
dι
N
2!
.
Also, we have inf Lp = Lp(β⋆) ≤Bℓby Lemma G.1. Therefore, we can conclude that
E(D,xN+1,yN+1)

ℓ(bylin
N+1, yN+1)

≤inf Lp + O (1) ·
 
K2
xK2
yι
µgµx
d
N +
K4
x
µgµxBw
ε2 + B2
ℓ
N 5
!
.
Taking ε2 ≤
K2
yι
BwK2x
d
N completes the proof.
Proof of Theorem G.2 (b). Similar to the proof of Corollary 6, we have
E(byN+1 −yN+1)2 = E

1{E}(byN+1 −yN+1)2
+ E

1{Ec}(byN+1 −yN+1)2
≤E

1{E}(eyN+1 −yN+1)2
+
p
P(Ec)E(byN+1 −yN+1)4,
where the inequality follows from yN+1 ∈[−By, By] on event E. For the first part, we have
E
h
1{E}(eyN+1 −yN+1)2i
= E
h
1{E}(fD(xN+1) −yN+1)2i
≤ED
h
1{E0} · E(x,y)∼P
h
1{∥x∥2 ≤Bx}(fD(x) −y)2ii
,
where we use the fact that the conditional distribution of (xN+1, yN+1)|D agrees with P. Thus,
E
h
1{E}(eyN+1 −yN+1)2i
−E(x,y)∼P(g(⟨x, β⟩) −y)2
≤ED
h
1{E0} ·

E(x,y)∼P1{∥x∥2 ≤Bx}(fD(x) −y)2 −E(x,y)∼P(g(⟨x, β⟩) −y)2i
≤ED
h
1{E0} · Ex1{∥x∥2 ≤Bx}(fD(x) −g(⟨x, β⟩))2i
≤2ED
h
1{E0} · Ex1{∥x∥2 ≤Bx}(fD(x) −g(⟨x, wGLM⟩))2i
+ 2ED
h
1{E0} · Ex(g(⟨x, wGLM⟩) −g(⟨x, β⟩))2i
≤2ε2 + 2Lg
µxµg
E[ε2
stat] ≤2ε2 + O (1) · LgK2
xK2
yι
µxµg
d
N .
For the second part, we know P(Ec) = O
 N −10
and
E(byN+1 −yN+1)4 ≤8Eby2
N+1 + 8Ey4
N+1 = O
 B4
y

.
In conclusion, we have
E(byN+1 −yN+1)2 ≤E(yN+1 −g(⟨xN+1, β⟩))2 + 2ε2 + O (1) · LgK2
xK2
yι
µxµg
d
N + O
 
B2
y
N 5
!
.
Taking ε2 ≤
LgK2
xK2
yι
µxµg
d
N completes the proof.
50

Lemma G.1. Suppose that x ∼SG(Kx), y ∼SG(Ky), and w is a (possibly random) vector such
that ∥w∥2 ≤Bw. Then
E

ℓ(⟨x, w⟩, y)41/4 ≤O
 LgK2
xB2
wd + KxKyBwd

=: Bℓ.
Proof. Notice that by our assumption, |g(0)| ≤2Ky. Therefore, by the definition of ℓ,
|ℓ(t, y)| =
−yt +
Z t
0
g(s)ds
 ≤|t(g(0) −y)| +

Z t
0
(g(s) −g(0))ds
 ≤|t| (2Ky + |y|) + 2Lgt2.
The proof is then done by bounding the moment by E |y|8 ≤O
 K8
y

and E |⟨x, w⟩|8 ≤
B8
wE ∥x∥8
2 ≤O

(
√
dBwKx)8
, which is standard (by utilizing the tail bound of sub-Gaussian/sub-
Exponential random variable).
G.3
Proof of Theorem G.3 (a)
We begin with the upper bound on λmax(∇2bLN(w)). By Lemma B.3, as long as N ≥C0 · d, the
following event
Ew,0 :

1
N
N
X
i=1
xix⊤
i

op
≤8K2.
happens with probability at least 1 −exp(−N/C0). By the assumption that sup |g′| ≤Lg, it is clear
that when Ew,0 holds, we have λmax(∇2bLN(w)) ≤8LgK2
x ∀w ∈Rd.
In the following, we analyze the quantity λmax(∇2bLN(w)). We have to invoke the following
covering argument (see e.g. [85, Section 4.1.1]).
Lemma G.2. Suppose that V is a ε-covering of Sd−1 with ε ∈[0, 1). Then the following holds:
1. For any d × d symmetric matrix A, ∥A∥op ≤
1
1−2ε maxv∈V
v⊤Av
 and
λmin(A) ≥min
v∈V v⊤Av −2ε ∥A∥op
2. For any vector x ∈Rd, ∥x∥2 ≤
1
1−ε maxv∈V |⟨v, x⟩|.
Notice that
∇2bLN(w) = 1
N
N
X
i=1
g′(⟨w, xi⟩)xix⊤
i ⪰1
N
N
X
i=1
µgI(|⟨w, xi⟩| ≤Bµ)xix⊤
i
⪰1
N
N
X
i=1
µg

1 −|⟨w, xi⟩|
Bµ

+
xix⊤
i .
Therefore, we can define h(t) := (Bµ −|t|)+ (which is a 1-Lipschitz function), and we have
∇2bLN(w) ⪰µg
Bµ
1
N
N
X
i=1
h(⟨w, xi⟩)xix⊤
i
|
{z
}
=:A(w)
.
In the following, we pick a εv-covering V of Sd−1 such that |V| ≤(3/εv)d (we will specify εv later
in proof). Then for any w ∈B2(Bw),
λmin(A(w)) ≥min
v∈V v⊤A(w)v −2εv ∥A(w)∥op
By our definition of A(w), we have (for any fixed Bxv)
min
v∈V v⊤A(w)v = min
v∈V
1
N
N
X
i=1
h(⟨w, xi⟩) ⟨v, xi⟩2
51

≥min
v∈V
1
N
N
X
i=1
h(⟨w, xi⟩) min
n
⟨v, xi⟩2 , B2
xv
o
|
{z
}
=:Uv(w)
≥min
v∈V E[Uv(w)] + min
v∈V (Uv(w) −E[Uv(w)]).
By Lemma G.3, we can choose Bxv = Kx(15 + log(K2
x/µx)), and then E[Uv(w)] ≥3Bµµx/8.
Thus, combining the inequalities above, we can take εv = 128K2
x
µx
in the following, so that under
event Ew,0,
λmin(∇2bLN(w)) ≥µgµx
8
+ µg
Bµ
Bµµx
16
−max
v∈V (E[Uv(w)] −Uv(w))

.
In the following, we consider the random process

U v(w) := Uv(w) −E[Uv(w)]
	
w, which is
zero-mean and indexed by w ∈B2(Bw). For any fixed v, consider applying Proposition B.4 to the
random process

U v(w)
	
w. We need to verify the preconditions:
(a) With norm ρ(w, w′) = ∥w −w′∥2, log N(Bρ(w, r), δ) ≤d log(2Ar/δ) with constant A = 2;
(b) Let f(x; w) := h(⟨w, xi⟩) min
n
⟨v, xi⟩2 , B2
xv
o
, then |f(x; w)| ≤BµB2
xv and hence in
SG(CBµB2
xv) for any random x;
(c) For w, w′ ∈W, we have |h(⟨w, xi⟩) −h(⟨w′, xi⟩)| ≤|⟨w −w′, xi⟩|. Hence, because x ∼
SG(Kx), the random variable h(⟨w, x⟩) −h(⟨w′, x⟩) is SG(CKx∥w −w′∥2), and the random
variable f(x; w) −f(x; w′) is SG(CKxB2
xv∥w −w′∥2).
Therefore, we can apply Proposition B.4 to obtain that with probability 1 −δ0, it holds
sup
w
U v(w)
 ≤C′BµB2
xv
"r
d log(2κg) + log(1/δ0)
N
#
,
where we denote κg = 1 + KxBw/Bµ. Setting δ0 = δ/ |V| and taking the union bound over v ∈V,
we obtain that with probability at least 1 −δ,
max
v∈V
sup
∥w∥2≤Bw
U v(w)
 ≤C′BµB2
xv
"r
d log(8κg/εv) + log(1/δ)
N
#
,
where we use log |V| ≤d log(4/εv). Therefore, we plug in the definition of εv and Bxv to deduce
that, if we set
C1 =
16C′B2
xv
µx
2
log(8κg/εv),
εv = 128K2
x
µx
,
Bxv = Kx(15 + log(K2
x/µx)),
then as long as N ≥C1 · d, it holds maxv∈V E[Uv(w)] −Uv(w) ≤µxBµ
16
with probability at least
1 −exp(−N/C1). This is the desired result.
Lemma G.3. Under Assumption B, for Bxv = Kx(15 + log(K2
x/µx)), it holds
inf
w∈B2(Bw),v∈Sd−1 E[1{|x⊤w| ≤Bµ/2}(x⊤v)21{|x⊤v| ≤Bxv}] ≥3µx/4.
Proof. For any fixed w ∈B2(Bw), v ∈Sd−1,
E[1{|x⊤w| ≤Bµ/2}(x⊤v)21{|x⊤v| ≤Bxv}]
= E[1{|x⊤w| ≤Bµ/2}(x⊤v)2}] −E[1{|x⊤w| ≤Bµ/2}(x⊤v)21{|x⊤v| > Bxv}]
≥µx −E[(x⊤v)21{|x⊤v| > Bxv}].
Because x ∼SG(Kx), x⊤v ∼SG(Kx), and a simple calculation yields
E[(x⊤v)21{|x⊤v| > tKx}] ≤2K2
x(t2 + 1) exp(−t2).
Taking t = 15 + log(K2
x/µx) gives E[(x⊤v)21{|x⊤v| > Bxv}] ≤µx/4, which completes the
proof.
52

G.4
Proof of Theorem G.3 (b)
Notice that
∇bLN(w) = 1
N
N
X
i=1
(g(⟨w, xi⟩) −yi)xi.
In the following, we pick a minimal 1/2-covering of Sd−1 (so |V| ≤5d). Then by Lemma G.2, it
holds
∇bLN(w) −E[∇bLN(w)]

2 ≤2 max
v∈V
 ⟨∇bLN(w), v⟩−E[⟨∇bLN(w), v⟩]
|
{z
}
=:Xv(w)
Fix a v ∈Sd−1 and set δ′ = δ/|V|. We proceed to bound supw |Xv(w)| by applying Proposition B.4
to the random process {Xv(w)}w. We need to verify the preconditions:
(a) With norm ρ(w, w′) = ∥w −w′∥2, log N(δ; Bρ(r), ρ) ≤d log(2Ar/δ) with constant A = 2;
(b) For z = [x; y], we let f(z; w) := (g(⟨w, x⟩) −y) ⟨x, v⟩, then f(z; w) ∼SE(CKxKy) for any
w by our assumption on (x, y);
(c) For w, w′ ∈W, we have |g(⟨w, x⟩) −g(⟨w′, x⟩)| ≤Lg |⟨w −w′, x⟩|. Hence, because x ∼
SG(Kx), the random variable g(⟨w, xi⟩) −g(⟨w′, xi⟩) is sub-Gaussian in SG(KxLg∥w −w′∥2).
Thus, f(z; w) −f(z; w′) is sub-exponential in SE(CK2
xLg∥w −w′∥2).
Therefore, we can apply Proposition B.4 to obtain that with probability 1 −δ0, it holds
sup
w |Xv(w)| ≤C′KxKy
"r
d log(2κy) + log(1/δ0)
N
+ d log(2κy) + log(1/δ0)
N
#
,
where we denote κy = 1 + LgK2
xBw/Ky. Setting δ0 = δ/ |V| and taking the union bound over
v ∈V, we obtain that with probability at least 1 −δ,
max
v∈V
sup
∥w∥2≤Bw
|Xv(w)| ≤C′KxKy
"r
d log(10κy) + log(1/δ)
N
+ d log(10κy) + log(1/δ)
N
#
.
This is the desired result.
G.5
Proof of Theorem G.3 (c)
In the following, we condition on (a) holds, i.e. bLN is α-strongly-convex and β smooth over B2(Bw)
with α = µxµg/8 and β = 8LgK2
x. We define
ew = arg min
w∈B2(Bw)
bLN(w).
Then by standard convex analysis, we have
α ∥ew −β⋆∥2
2 ≤
D
∇bLN(ew) −∇bLN(β⋆), ew −β⋆E
≤
D
−∇bLN(β⋆), ew −β⋆E
≤
∇bLN(β⋆)

2 ∥ew −β⋆∥2 .
Notice that
∇bLN(β⋆)

2 ≤εstat, we can conclude that
∥ew∥2 ≤∥β⋆∥2 + εstat
α .
Recall that we assume ∥β⋆∥2 ≤Bw/4, we can then consider Es := {εstat < αBw/4}. Once
Es holds, our argument above yields ∥ew∥2 < Bw, which implies ∇bLN(ew) = 0. Therefore,
ew = arg minw∈Rd bLN(w). Further, by Theorem G.3, we can set
C2 := max
(
2ι
32αKxKy
Bw
2
, 2ι · 32αKxKy
Bw
)
,
so that as long as N ≥C2d, the event Es holds with probability at least 1 −exp(−N/C2). This is
the desired result.
53

G.6
Proof of Theorem G.3 (d) & (e)
We first prove Theorem G.3 (d). Notice that
∇2Lp(w) = E

g′(⟨x, w⟩)xx⊤
⪰E

µgI(|⟨x, w⟩| ≤Bµ)xx⊤
⪰µgµxId, ∀w ∈B2(Bw).
Therefore, Lp is (µgµx)-strongly-convex over B2(Bw). Therefore, because β⋆∈B2(Bw) is the
global minimum of Lp, it holds that for all w ∈B2(Bw),
Lp(w) −Lp(β⋆) ≤
1
2µgµx
∥∇Lp(w)∥2
2 .
By the definition of εstat, ∥∇Lp(w)∥2 ≤εstat + ∥∇bLN(w)∥2, and hence the proof of Theorem G.3
(d) is completed.
We next prove Theorem G.3 (e), where we assume that E[y|x] = g(⟨x, β⟩) (which implies β⋆= β
directly) and wGLM ∈B2(Bw). Notice that
∇Lp(w) = E
h
∇bLN(w)
i
= E[(g(⟨x, w⟩) −y)x] = E[(g(⟨x, w⟩) −g(⟨w, β⟩))x],
and hence
⟨∇Lp(wGLM), wGLM −β⟩= E[(g(⟨x, wGLM⟩) −g(⟨w, β⟩)) · (⟨x, wGLM⟩−⟨w, β⟩)]
≥1
Lg
E

(g(⟨x, wGLM⟩) −g(⟨w, β⟩))2
.
On the other hand, by the (µgµx)-strong-convexity of Lp over B2(Bw), it holds that
⟨∇Lp(wGLM), wGLM −β⟩≤
1
µgµx
∥∇Lp(wGLM)∥2
2 .
Finally, using the definition of wGLM, we have ∇bLN(wGLM) = 0, and hence ∥∇Lp(wGLM)∥2 ≤
εstat, which completes the proof of Theorem G.3 (e).
H
Proofs for Section 3.2
H.1
Proof of Theorem 7
Fix λN ≥0, β > 0 and Bw > 0, and consider any in-context data D such that the precondition
of Theorem 7 holds. Recall that
Llasso(w) :=
1
2N
N
X
i=1
(⟨w, xi⟩−yi)2 + λN ∥w∥1
denotes the lasso regression loss in (ICLasso), so that wlasso = arg minw∈Rd Llasso(w). We further
write
bL0
N(w) :=
1
2N
N
X
i=1
(⟨w, xi⟩−yi)2,
R(w) := λN ∥w∥1 .
Note that ∇2bL0
N(w) = X⊤X/N and thus bL0
N is β-smooth over Rd.
Consider the proximal gradient descent algorithm on the ridge loss
wt+1
PGD = proxηR

wt
PGD −η∇bL0
N(wt
PGD)

with initialization w0
PGD := 0d, learning rate η := β−1, and number of steps T to be specified later.
Similar to the proof of Theorem 4, we can construct a transformer to approximate wT
GD. Consider
ℓ(s, t) = 1
2(s −t)2 and R(w) = λN ∥w∥1, then ∂sℓ(s, t) is (0, +∞, 2, 4)-approximable by sum
of relus (cf. Definition D.1), and proxηR is (0, +∞, 4d, 4 + 2ηλN)-approximable by sum of relus
(Proposition D.1). Therefore, we can apply Theorem D.2 with the square loss ℓ, regularizer R,
learning rate η and accuracy parameter 0 to obtain that there exists a transformer TFθ with (T + 1)
54

layers, number of heads M (ℓ) = 2 for all ℓ∈[L], and hidden dimension D′ = 2d, such that the
final output h(L)
N+1 = [xN+1; byN+1; wT
PGD; ∗] with byN+1 =

wT
PGD, xN+1

. Further, the weight
matrices have norm bounds |||θ||| ≤10R + (8 + 2λN)β−1.
By the standard convergence result for proximal gradient descent (Proposition B.3), we have for all
t ≥1 that
Llasso(wt
PGD) −Llasso(wlasso) ≤β
2t ∥wlasso∥2
2 .
Plugging in ∥wlasso∥2 ≤Bw/2 and T = L −1 =

βB2
w/ε

finishes the proof.
H.2
Sharper convergence analysis of proximal gradient descent for Lasso
Collection of parameters
Throughout the rest of this section, we consider fixed N ≥1, λN =
q
ρν log d
N
for ρ ≥0, ν ≥0 fixed (and to be determined), fixed 0 < α ≤β, and fixed B⋆
w > 0. We
write κ := β/α, κs := β(B⋆
w)2/ν2, and ωN := ρ
α
s log d
N
.
Here we present a sharper convergence analysis on the proximal gradient descent algorithm for Llasso
under the following well-conditionedness assumption, which will be useful for proving Theorem 8 in
the sequel.
Assumption C (Well-conditioned property for Lasso). We say the (ICLasso) problem is well-
conditioned with sparsity s if the following conditions hold:
1. The (α, ρ)-RSC condition holds:
∥Xw∥2
2
N
≥α ∥w∥2
2 −ρlog d
N
∥w∥2
1 ,
∀w ∈Rd.
(38)
Further, λmax(X⊤X/N) ≤β.
2. The data (X, y) is “approximately generated from a s-sparse linear model”: There exists a
w⋆∈Rd such that ∥w⋆∥2 ≤B⋆
w, ∥w⋆∥0 ≤s and for the residue ε = y −Xw⋆,
X⊤ε

∞≤1
2NλN.
3. It holds that N ≥32 ρ
α · s log d (i.e. 32ωN ≤1).
Assumption C1 imposes the standard restricted strong convexity (RSC) condition for the feature
matrix X ∈RN×d, and Assumption C2 asserts that the data is approximately generated from a
sparse linear model, with a bound on the L∞norm of the error vector X⊤ε. Assumption C is
entirely deterministic in nature, and suffices to imply the following convergence result. In the proof
of Theorem 8, we show that Assumption C is satisfied with high probability when data is generated
from the standard sparse linear model considered therein.
Theorem H.1 (Sharper convergence guarantee for Lasso). Under Assumption C, for the PGD
iterates {wt}t≥0 on loss function bLlasso with stepsize η = 1/β and starting point w0 = 0, we have
bLlasso(wT ) −bLlasso(wlasso) ≤ε for all
T ≥C
β(B⋆
w)2
ν
+ κ log

C · κ · β(B⋆
w)2
ν
· ν
ε

+ κνω2
N
ε

,
where C is a universal constant.
The proof can be found in Appendix H.4. Combining Theorem H.1 with the construction in Theorem 7,
we directly obtain the following result as a corollary.
Theorem H.2 (In-context Lasso with transformers with sharper convergence). For any N, d, s ≥1,
0 < α ≤β, ν ≥0, ρ ≥0, there exists a L-layer transformer TFθ with
L =

C
 κs + κ(log(Cκs/ε) + νω2
N/ε)

,
max
ℓ∈[L] M (ℓ) ≤2,
max
ℓ∈[L] D(ℓ) ≤2d,
55

|||θ||| ≤3 + R + (8 + 2λN)β−1,
such that the following holds. On any input data (D, xN+1) such that the (ICLasso) problem
satisfies Assumption C (which implies ∥wlasso∥2 ≤Bw/2 with Bw = 2B⋆
w +
p
ν/α), TFθ(H(0))
approximately implements (ICLasso), in that it outputs byN+1 = ready(TFθ(H)) = ⟨xN+1, bw⟩with
bLlasso(bw) −bLlasso(wlasso) ≤ε.
H.3
Basic properties for Lasso
Lemma H.1 (Relaxed basic inequality). Suppose that Assumption C2 holds. Then it holds that
∥w −w⋆∥1 ≤4√s ∥w −w⋆∥2 + 2
λN

bLlasso(w) −bLlasso(w⋆)

,
∀w ∈Rd.
As a corollary, ∥wlasso −w⋆∥1 ≤4√s ∥wlasso −w⋆∥2.
Proof. Let us first fix any w ∈Rd. Denote ∆= w −w⋆, and let S = supp(w⋆) be the set of
indexes of nonzero entries of w⋆. Then by definition, y = Xw⋆+ ε and |S| ≤s, and hence
∥Xw −y∥2
2 −∥Xw⋆−y∥2
2 = ∥X∆−ε∥2
2 −∥ε∥2
2 = ∥X∆∥2
2 −2ε⊤X∆,
∥w∥1 −∥w⋆∥1 =
X
j∈S
(|w[j]| −|w⋆[j]|) +
X
j̸∈S
|w[j]|
≥−
X
j∈S
|w[j] −w⋆[j]| +
X
j̸∈S
|w[j]| = ∥∆Sc∥1 −∥∆S∥1 .
Combining these inequalities, we obtain
0 ≤
1
2N ∥X∆∥2
2 ≤ε⊤X∆
N
+ λN(∥∆S∥1 −∥∆Sc∥1) + bLlasso(w) −bLlasso(w⋆)
≤λN
2 ∥∆∥1 + λN(∥∆S∥1 −∥∆Sc∥1) + bLlasso(w) −bLlasso(w⋆)
= λN
2 (3 ∥∆S∥1 −∥∆Sc∥1) + bLlasso(w) −bLlasso(w⋆),
(39)
where the second inequality follows from ε⊤X∆
N
≤∥X⊤ε∥∞
N
∥∆∥1 and our assumption that
2∥X⊤ε∥∞
N
≤λN, and the last inequality is due to ∥∆∥1 = ∥∆S∥1 + ∥∆Sc∥1. Therefore, we
have
∥∆∥1 = ∥∆S∥1 + ∥∆Sc∥1 ≤4 ∥∆S∥1 + 2
λN

bLlasso(w) −bLlasso(w⋆)

≤4√s ∥∆∥2 + 2
λN

bLlasso(w) −bLlasso(w⋆)

,
where the last inequality follows from ∥∆S∥1 ≤√s ∥∆S∥2 ≤√s ∥∆∥2. This completes the proof
of our main inequality. As for the corollary, we only need to use the definition that bLlasso(wlasso) ≤
bLlasso(w⋆).
Proposition H.1 (Gap to parameter estimation error). Suppose that Assumption C holds. Then for all
w ∈Rd,
∥w −w⋆∥2
2 ≤C
sλ2
N
α2 + ν−1gap2 + gap

,
where we write gap := bLlasso(w) −bLlasso(wlasso), and C = 120 is a universal constant. In
particular, we have ∥wlasso −w⋆∥2
2 ≤10 ρν
α2
s log d
N
.
56

Proof. We follow the notation in the proof of Lemma H.1. By (39), we have
0 ≤
1
2N ∥X∆∥2
2 ≤λN
2 (3 ∥∆S∥1 −∥∆Sc∥1) + bLlasso(w) −bLlasso(w⋆),
and hence ∥∆∥1 ≤4√s ∥∆∥2 + 2gap
λN due to bLlasso(w) −bLlasso(w⋆) ≤gap. On the other hand, by
the RSC condition (38), it holds that
∥X∆∥2
2
N
≥α ∥∆∥2
2 −ρlog d
N
∥∆∥2
1 .
Therefore, we have
α ∥∆∥2
2 ≤3λN
√s ∥∆∥2 + ρlog d
N
∥∆∥2
1 + 2gap
≤3λN
√s ∥∆∥2 + ρlog d
N

4√s ∥∆∥2 + 2gap
λN
2
+ 2gap
≤5sλ2
N
α
+ α
6 ∥∆∥2
2 + ρ20s log d
λ2
NN
∥∆∥2
2 + ρ20 log d
N
gap2 + 2gap,
where the last inequality uses AM-GM inequality and Cauchy inequality. Notice that ρ 20s log d
N
≤2
3α,
we now derive that
∥∆∥2
2 ≤30sλ2
N
α2
+ ρ120 log d
λ2
NN
gap2 + 12gap.
Plugging in λN =
q
ρν log d
N
completes the proof. The corollary follows immediately by letting
w = wlasso in above proof (hence gap = 0).
Lemma H.2 (Growth). It holds that
1
2N ∥X(w −wlasso)∥2
2 ≤bLlasso(w) −bLlasso(wlasso),
∀w.
Proof. For simplicity we denote wlasso := wlasso. By the first order optimality condition, it holds
that
0 ∈1
N X⊤(Xwlasso −y) + ∂R(wlasso),
where we write R(w) := λN ∥w∥1. Then by the convexity of R, we have
R(w) −R(wlasso) ≥⟨∂R(wlasso), w −wlasso⟩=

−1
N X⊤(Xwlasso −y), w −wlasso

= −1
N ⟨Xwlasso −y, (Xw −y) −(Xwlasso −y)⟩
= −1
2N ∥Xw −y∥2
2 + 1
2N ∥Xwlasso −y∥2
2 + 1
2N ∥X(w −wlasso)∥2
2 .
Rearranging completes the proof.
H.4
Proof of Theorem H.1
For the simplicity of presentation, we write wlasso = wlasso and we denote gapt := bLlasso(wt) −
bLlasso(wlasso).
By Lemma H.1, we have ∥wt −w⋆∥1 ≤4√s ∥wt −w⋆∥2 + 2gapt
λN , which implies
wt −wlasso

1 ≤
wt −w⋆

1 + ∥wlasso −w⋆∥1 ≤4√s
wt −wlasso

2 + 8√s ∥wlasso −w⋆∥2 + 2gapt
λN
.
57

We denote µN = ρ2 log d
N . Using the assumption that X is (α, ρ)-RSC, we obtain that
1
N
X(wt −wlasso)
2
2 ≥α
wt −wlasso
2
2 −µN
wt −wlasso
2
1
≥α
wt −wlasso
2
2 −µN

20s
wt −wlasso
2
2 + 640s ∥wlasso −w⋆∥2
2 + 40
λ2
N
(gapt)2

.
Thus, as long as N ≥30ρ2s log d
α
, we have
α
3
wt −wlasso
2
2 ≤1
N
X(wt −wlasso)
2
2 + 640sµN ∥wlasso −w⋆∥2
2 + 40µN
λ2
N
(gapt)2
≤2gapt + 40ν−1(gapt)2 + 640sµN ∥wlasso −w⋆∥2
2 ,
where the last inequality follows from Lemma H.2 and the definition of λN, µN.
We define εstat := 640sµN ∥wlasso −w⋆∥2
2, T0 := 10βν−1 ∥wlasso∥2
2. By Proposition B.3(3), it
holds that for t ≥T0,
gapt ≤β
2t ∥wlasso∥2
2 ≤
β
2T0
∥wlasso∥2
2 = ν
20.
Then for all t ≥T0 −1, we have (the second ≤below uses Proposition B.3(2))
α
3
wt+1 −wlasso
2
2 ≤4gapt+1 + εstat ≤2β
wt −wlasso
2
2 −
wt+1 −wlasso
2
2

+ εstat,
⇒
wt+1 −wlasso
2
2 −3εstat
α
≤

1 + α
6β
−1wt −wlasso
2
2 −3εstat
α

.
Therefore, for t ≥T0 −1,
wt −wlasso
2
2 ≤exp

−α
12β (t −⌈T0⌉+ 1)
 w⌈T0⌉−1 −wlasso

2
2 + 3εstat
α
≤exp

−α
8β (t −T0)

∥wlasso∥2
2 + 3εstat
α
,
where the last inequality follows from Proposition B.3(2). Further, by Proposition B.3(3), we have
gapt+k ≤β
2k
wt −wlasso
2
2 ≤β
2k

exp

−α
8β (t −T0)

∥wlasso∥2
2 + 3εstat
α

,
∀t ≥T0 −1, k ≥0.
Hence, we can conclude that gapT ≤ε for all T such that
T ≥10βν−1 ∥wlasso∥2
2 + 8κ log
 
β ∥wlasso∥2
2
ε
!
+ 3κεstat
ε
+ 1.
Now, by Proposition H.1, it holds that ∥wlasso −w⋆∥2
2 ≤10 ρν
α2
s log d
N
, and hence
∥wlasso∥2
2 ≤2 ∥w⋆∥2
2 + 2 ∥wlasso −w⋆∥2
2 ≤2(B⋆
w)2 + 20ρνs log d
α2N
.
Plugging in our definition of
µN = ρ log d
N
,
εstat := 400sµN ∥wlasso −w⋆∥2
2 ,
ωN = ρ
α
s log d
N
≤1
completes the proof.
H.5
Proof of Theorem 8
In this section, we present the proof of Theorem 8 based on Theorem H.2. We begin by recalling the
following RSC property of a Gaussian random matrix [87, Theorem 7.16], a classical result in the
high-dimensional statistics literature.
58

Proposition H.2 (RSC for Gaussian random design). Suppose that X = [x1; · · · ; xN]⊤∈RN×d
is a random matrix with each row xi being i.i.d. samples from N(0, Σ). Then there are universal
constants c1 = 1
8, c2 = 50 such that with probability at least 1 −
e−N/32
1−e−N/32 ,
∥Xw∥2
2
N
≥c1 ∥w∥2
Σ −c2ρ(Σ)log d
N
∥w∥2
1 ,
∀w ∈Rd,
(40)
where ρ(Σ) = maxi∈[d] Σii is the maximum of diagonal entries of Σ.
Fix a parameter δ1 ≤δ (which we will specify in proof) and a large universal constant C0. Let us set
α = c1 = Θ (1) ,
β = 8(1 + (d/N)),
ρ = c2 = Θ (1) ,
Bx = C0
p
d log(N/δ1),
By = C0(B⋆
w + σ)
p
log(N/δ1).
Similar to the proof of Corollary 6 (Appendix F.4), we consider the following good events (where
ε = Xw⋆−y)
Ew =

λmax(X⊤X/N) ≤β and X is (α, ρ)-RSC
	
,
Er =
nX⊤ε

∞≥4σ
p
N log(4d/δ)
o
,
Eb = {∀i ∈[N], ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,N+1 = {∥xN+1∥2 ≤Bx, |yN+1| ≤By},
and we define E := Ew ∩Er ∩Eb ∩Eb,N+1.
Furthermore, we choose ν > 0 that correspond to the choice λN = 8σ
q
log(4d/δ)
N
, and we also
assume N ≥32c2
c1 · s log d. Then, Assumption C holds on the event E.
Therefore, we can apply Theorem H.2 with ε = νωN, which implies that there exists a L-layer
transformer θ such that its prediction byN+1 := g
ready(TF0
θ(H)), so that under the good event E we
have byN+1 = clipBy(⟨xN+1, bw⟩), where
Llasso(bw) −Llasso(wlasso) ≤νωN.
In the following, we show that θ is indeed the desired transformer (similarly to the proof in Ap-
pendix F.4). Consider the conditional prediction error
E

(byN+1 −yN+1)2 D

= E

1{E}(byN+1 −yN+1)2 D

+ E

1{Ec}(byN+1 −yN+1)2 D

,
and we analyze these two parts separately under the good event E0 := Ew ∩Er ∩Eb of D.
Part I.
We first note that
E

1{E}(byN+1 −yN+1)2 D

= E
h
1{E}(clipBy(⟨xN+1, bw⟩) −yN+1)2 D
i
≤E

1{E}(⟨xN+1, bw⟩−yN+1)2 D

,
where the inequality is because yN+1 ∈[−By, By] under the good event E. Notice that by our
construction, under the good event E, bw = bw(D) depends only on the dataset D (because it is the
(L −1)-th iterate of PGD on (ICLasso) problem). Applying Proposition H.1 to bw(D) and using the
definition of ωN and our choice of λN, we obtain that (under E0)
∥bw(D) −w⋆∥2
2 ≤C ·
sλ2
N
α2 + νω2
N + νωN

= O
σ2s log(d/δ)
N

.
Therefore, under E0,
E

1{E}(⟨xN+1, bw⟩−yN+1)2 D

= E

1{E}(⟨xN+1, bw(D)⟩−yN+1)2 D

≤E

(⟨xN+1, bw(D)⟩−yN+1)2 D

= E

(⟨xN+1, bw(D)⟩−⟨xN+1, w⋆⟩)2 D

+ σ2
= ∥bw(D) −w⋆∥2
2 + σ2
= σ2

1 + O
s log(d/δ)
N

.
59

Part II.
Notice that under good event E0, the bad event Ec holds if and only if Ec
b,N+1 holds, and
hence
E

1{Ec}(byN+1 −yN+1)2 D

= E

1{Ec
b,N+1}(byN+1 −yN+1)2 D

≤
q
P(Ec
b,N+1)E[(byN+1 −yN+1)4].
With a large enough constant C0, we clearly have P(Ec
b,N+1) ≤(δ1/N)10. Further, a simple
calculation yields
E(byN+1 −yN+1)4 ≤8E(by4
N+1 + y4
N+1) ≤8B4
y + 8Ey4
N+1 ≤16B4
y,
where the last inequality is because the marginal distribution of yN+1 is simply N(0, σ2 + ∥w⋆∥2
2).
Combining these yields
E

1{Ec}(byN+1 −yN+1)2 D

≤O
 
δ5
1B2
y
N 5
!
≤O
δ5
1((B⋆
w)2 + σ2) log(1/δ1)
N 4

.
Therefore, choosing δ1 = min{δ,
σ
B⋆
w } is enough for our purpose, and under such choice of δ1,
E

1{Ec}(byN+1 −yN+1)2 D

≤O
 σ2
N 4

.
Conclusion.
Combining the inequalities above, we can conclude that under E0,
E

(byN+1 −yN+1)2 D

≤σ2

1 + O
s log(d/δ)
N

.
It remains to show that P(E0) ≥1 −δ. By Proposition H.2, Lemma B.2 and Lemma B.4, we have
P(Ew) ≤3 exp(−N/32),
P(Er) ≤δ
2,
P(Eb) ≤δ
4.
Therefore, as long as N ≥32 log(12/δ), we have P(E0) ≥1 −δ. This completes the proof.
We also remark that in the construction above,
R = O

(B⋆
w + σ)
√
d log(N · (1 + B⋆
w/σ))

,
which would be useful for bounding |||θ|||.
I
Proofs for Section 4
I.1
Proof of Proposition 10
We begin by restating Proposition 10 into the following version, which contains additional size
bounds on θ.
Theorem I.1 (Full statement of Proposition 10). Suppose that for
bLval(f) :=
1
|Dval|
X
(xi,yi)∈Dval
ℓ(f(xi), yi),
ℓ(·, ·) is (γ/3, R, M, C)-approximable by sum of relus (Definition D.1). Then there exists a 3-layer
transformer TFθ with
max
ℓ∈[3] M (ℓ) ≤(M + 3)K,
max
ℓ∈[3] D(ℓ) ≤K2 + K + 1,
|||θ||| ≤2NKC
|Dval| + 3γ−1 + 7KR.
that maps
hi = [∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti]
→
h′
i = [∗; bf(xi); 1; ti], i ∈[N + 1],
where the predictor bf : Rd →R is a convex combination of {fk : bLval(fk) ≤mink⋆∈[K] bLval(fk⋆) +
γ}. As a corollary, for any convex risk L : (Rd →R) →R, bf satisfies
L( bf) ≤mink⋆∈[K] L(fk⋆) + maxk∈[K]
bLval(fk) −L(fk)
 + γ.
60

To prove Theorem I.1, we first state and prove the following two propositions.
Proposition I.1 (Evaluation layer). There exists a 1-layer transformer TFθ with MK heads and
|||θ||| ≤3R + 2NKC/ |Dval| such that for all H such that maxi{|y′
i|} ≤R, maxi,k{|fk(xi)|} ≤R,
TFθ maps
hi = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti]
→
h′
i = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); eLval(f1); · · · ; eLval(fK); 0; 1; ti],
i ∈[N + 1],
where eLval(·) is a functional such that maxk
eLval(fk) −bLval(fk)
 ≤ε.
Proof of Proposition I.1. As ℓis (ε, R, M, C)-approximable by sum of relus, there exists a function
g : R2 →R of form
g(s, t) =
M
X
m=1
cmσ(ams + bmt + dm) with
M
X
m=1
|cm| ≤C, |am| + |bm| + |dm| ≤1, ∀m ∈[M],
such that sup(s,t)∈[−R,R]2 |g(s, t) −ℓ(s, t)| ≤ε. We define
eLval(f) :=
1
|Dval|
X
(xi,yi)∈Dval
g(f(xi), yi),
Next, for every m ∈[M] and k ∈[K], we define matrices Qm,k, Km,k, Vm,k ∈RD×D such that
for all i, j ∈[N + 1],
Qm,khi =


am
bm
dm
−2
0

,
Km,khj =


fk(xj)
yj
1
R(1 + tj)
0

,
Vm,khj = (N + 1)cm
|Dval|
· eD−(K−k)−3
where es ∈RD is the vector with s-th entry being 1 and others being 0. As the input has struc-
ture hi = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti], these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
m∈[M],k∈[K] ∥Qm,k∥op ≤3,
max
m∈[M],k∈[K] ∥Km,k∥op ≤2 + R,
X
m∈[M],k∈[K]
∥Vm,k∥op ≤K(N + 1)C
|Dval|
.
Now, for every i, j ∈[N + 1], we have
σ(⟨Qm,khi, Km,khj⟩) = σ(amfk(xj) + bmyj + dm −2R(1 + tj))
= σ
 amw⊤xj + bmyj + dm

1{tj = −1},
where the last equality follows from the bound |amfk(xj) + bmyj + dm| ≤R(|am|+|bm|)+dm ≤
2R, so that the above relu equals 0 if tj ≤0. Therefore, for each i ∈[N + 1] and k ∈[K],
M
X
m=1
σ(⟨Qm,khi, Km,khj⟩)Vm,khj
=
 M
X
m=1
cmσ
 amw⊤xj + bmyj + dm

!
· (N + 1)
|Dval| 1{tj = −1}eD−(K−k)−3
= g(fk(xj), yj) · (N + 1)
|Dval| 1{tj = −1}eD−(K−k)−3.
Thus letting the attention layer θ = {(Vm,k, Qm,k, Km,k)}(m,k)∈[M]×[K], we have
ehi = [Attnθ(H)]i = hi +
1
N + 1
N+1
X
j=1
X
m,k
σ(⟨Qm,khi, Km,khj⟩)Vm,khj
61

= hi +
1
|Dval|
N+1
X
j=1
K
X
k=1
g(fk(xj), yj) · 1{tj = −1}eD−(K−k)−3
= hi +
K
X
k=1


1
|Dval|
X
(xj,yj)∈Dval
g(fk(xj), yj)

eD−(K−k)−3
= hi +
K
X
k=1
eLval(fk) · eD−(K−k)−3
= [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti] + [0D−K−3; eLval(f1); · · · ; eLval(fK); 0; 0; 0]
= [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); eLval(f1); · · · ; eLval(fK); 0; 1; ti],
i ∈[N + 1].
This is the desired result.
Proposition I.2 (Selection layer). There exists a 3-layer transformer TFθ with
max
ℓ∈[3] M (ℓ) ≤2K + 2,
max
ℓ∈[3] D(ℓ) ≤K2 + K + 1,
|||θ||| ≤γ−1 + 3KR + 2.
such that TFθ maps
hi = [∗; f1(xi); · · · ; fK(xi); L1; · · · ; LK; 0; 1; ti]
→
h′
i = [∗; f1(xi); · · · ; fK(xi); ∗; · · · ; ∗; bf(xi); 1; ti],
i ∈[N + 1],
where bf = PK
k=1 λkfk is an aggregated predictor, where the weights λ1, · · · , λK ≥0 are functions
only on L1, · · · , Lk such that
K
X
k=1
λk = 1,
λk > 0 only if Lk ≤min
k⋆∈[K] Lk⋆+ γ.
Proof of Proposition I.2. We construct a θ which is a composition of 2 MLP layers followed by an
attention layer (θ(1)
mlp, θ(2)
mlp, θ(3)
attn).
Step 1: construction of θ(1)
mlp. We consider matrix W(1)
1
that maps
h = [∗D−K−3; L1; · · · ; LK; ∗; ∗; ∗]
7→W(1)
1 h = [L1 −L2; · · · ; L1 −LK; · · · ; LK −LK−1; L1; −L1; · · · ; LK; −LK],
i.e. W(1)
1 h is a K2 + K dimensional vector so that its entry contains {Lk −Ll}k,l∈[K] and
{Lk, −Lk}k∈[K]. Clearly, such W(1)
1
exists and can be chosen so that
W(1)
1

op ≤2K. We then
consider a matrix W(1)
2
that maps
σ(W(1)
1 h) 7→W(1)
2 σ(W(1)
1 h) = [0D−K−3; c1 −L1; · · · ; cK −LK; 03] ∈RD,
where ck = ck(L) := P
l̸=k σ(Lk −Ll). Notice that
ck −Lk = −σ(Lk) + σ(−Lk) +
X
l̸=k
σ(Lk −Ll),
and hence such W(1)
2
exists and can be chosen so that
W(1)
2

op ≤K + 1. We set θ(1)
mlp =
(W(1)
1 , W(1)
2 ), then MLPθ(1)
mlp maps hi to
h(1)
i
= [∗; f1(xi); · · · ; fK(xi); c1; · · · ; cK; 0; 1; ti].
The basic property of {ck}k∈[K] is that, if ck ≤γ, then Lk ≤mink⋆∈[K] Lk⋆+ γ.
Step 2: construction of θ(2)
mlp. We consider matrix W(2)
1
that maps
h = [∗D−K−3; c1; · · · ; cK; ∗; 1; ∗]
62

7→W(2)
1 h = [1 −γ−1c1; c1; −c1; · · · ; 1 −γ−1cK; cK; −cK] ∈R3K,
and W(2)
1
can be chosen so that
W(2)
1

op ≤K + 1 + γ−1. We then consider a matrix W(2)
2
that
maps
σ(W(2)
1 h) 7→W(2)
2 σ(W(1)
1 h) = [0D−K−3; σ(1 −γ−1c1) −c1; · · · ; σ(1 −γ−1cK) −cK; 03] ∈RD,
which exists and can be chosen so that
W(1)
2

op ≤2. We set θ(2)
mlp = (W(2)
1 , W(2)
2 ), then MLPθ(2)
mlp
maps h(1)
i
to
h(2)
i
= [∗; f1(xi); · · · ; fK(xi); u1; · · · ; uK; 0; 1; ti],
where uk = σ(1 −γ−1ck)∀k ∈[K]. Clearly, uk ∈[0, 1], and uk > 0 if and only if ck ≤γ.
Step 3: construction of θ(3)
attn. We define
λ1 = 1 −σ(1 −u1),
λk = σ(1 −u1 −· · · −uk−1) −σ(1 −u1 −· · · −uk) ∀k ≥2.
Clearly, λk ≥0, and P
k λk = 1. Further,
λk > 0 ⇒uk > 0 ⇒ck ≤γ ⇒Lk ≤min
k⋆∈[K] Lk⋆+ γ.
Therefore, it remains to construct θ(3)
attn that implements bf = PK
k=1 λkfk based on [h(2)
i ]i. Notice
that
bf(xi) = σ(1) · f1(xi) +
K−1
X
k=1
σ(1 −u1 −· · · −uk−1) · (fk(xi) −fk−1(xi))
−σ(1 −u1 −· · · −uK) · fK(xi),
(41)
and hence we construct θ(3)
attn as follows: for every k ∈[K + 1] and w ∈{0, 1}, we define matrices
Qk,w, Kk,w, Vk,w ∈RD×D such that for all k ∈[K + 1]
Qk,0h(2)
i
=

(fk(xi) + R) · 1k
0

,
Qk,1h(2)
i
=

(fk−1(xi) + R) · 1k
0

,
Kk,0h(2)
j
= Kk,1h(2)
j
=


1
−u1
...
−uk−1
0


,
Vk,0h(2)
j
= eD−2 = −Vk,1h(2)
j ,
for all i, j ∈[N + 1], where we understand f0 = fK+1 = 0 and 1k is the k-dimensional vector
with all entries being 1. By the structure of h(2)
i , these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
k∈[K+1],w∈{0,1} ∥Qk,w∥op ≤KR,
max
k∈[K+1],w∈{0,1} ∥Kk∥op ≤1,
X
k∈[K+1],w∈{0,1}
∥Vk,w∥op ≤2K + 2.
Now, for every i, j ∈[N + 1], k ∈[K + 1], w ∈{0, 1}, we have
σ
D
Qk,wh(2)
i , Kk,wh(2)
j
E
= σ((1 −u1 −· · · −uk−1)(fk−w(xi) + R))
= σ(1 −u1 −· · · −uk−1) · (fk−w(xi) + R),
where the last equality follows from fk(xi) + R ≥0∀k ∈[K]. Therefore,
X
k∈[K+1],w∈{0,1}
σ
D
Qm,kh(2)
i , Km,kh(2)
j
E
Vm,kh(2)
j
=
K
X
k=1
h
σ(1 −u1 −· · · −uk−1) · (fk(xi) + R) −σ(1 −u1 −· · · −uk−1) · (fk−1(xi) + R)
i
· eD−2
63

= bf(xi) · eD−2,
where the last equality is due to (41).
Thus letting the attention layer θ(3)
attn
=
{(Vk,w, Qk,w, Kk,w)}(k,w)∈[K+1]×{0,1}, we have
h(3)
i
=
h
Attnθ(H(2))
i
i = hi +
1
N + 1
N+1
X
j=1
X
k,w
σ
D
Qk,wh(2)
i , Kk,wh(2)
j
E
Vk,wh(2)
j
= h(2)
i
+ bf(xi) · eD−2
= [∗; f1(xi); · · · ; fK(xi); u1; · · · ; uK; bf(xi); 1; ti].
This is the desired result.
Now, we are ready to prove Theorem I.1.
Proof of Theorem I.1
As ℓ(·, ·) is (γ/3, R, M, C)-approximable by sum of relus, we can invoke
Proposition I.1 to show that there exists a single attention layer θ(1)
attn so that Attnθ(1)
attn maps
hi
→
h′
i = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); eLval(f1); · · · ; eLval(fK); 0; 1; ti],
i ∈[N + 1],
for any input H = [hi]i of the form described in Theorem I.1, and eLval(·) is a functional such that
maxk
eLval(fk) −bLval(fk)
 ≤γ/3.
Next, by the proof of Proposition I.2, there exists (θ(1)
mlp, θ(2)
mlp, θ(3)
attn) that maps
h′
i
→
h(3)
i
=
"
xi; y′
i; ∗; f1(xi); · · · ; fK(xi); ∗;
K
X
k=1
λkfk(xi); 1; ti
#
,
i ∈[N + 1],
where λ = (λ1, · · · , λK) ∈∆([K]) and λk > 0 only when eLval(fk) ≤mink⋆eLval(fk⋆) + γ/3.
Using the fact that maxk |eLval(fk) −bLval(fk)| ≤γ/3, we deduce that λ is supported on {k :
bLval(fk) ≤mink⋆∈[K] bLval(fk⋆) + γ}.
Therefore, θ = (θ(1)
attn, θ(1)
mlp, θ(2)
mlp, θ(3)
attn) is the desired transformer, with
max
ℓ∈[3] M (ℓ) ≤(M + 3)K,
max
ℓ∈[3] D(ℓ) ≤K2 + K + 1,
and
|||θ||| ≤max

3R + 2NKC
|Dval| + 3K + 1, K + 3 + γ−1, KR + 2K + 2

≤7KR + 2NKC
|Dval| + γ−1.
This completes the proof.
I.2
Proof of Theorem 11
We first restate Theorem 11 into the following version which provides additional size bounds
for θ. For the simplicity of presentation, throughout this subsection and Appendix J, we denote
It = {i : (xi, yi) ∈Dtrain}, Iv = {i : (xi, yi) ∈Dval}, Xtrain = [xi]i∈It to be the input matrix
corresponding to the training split only, and Ntrain = |Dtrain|, Nval = |Dval|.
Theorem I.2. For any sequence of regularizations {λk}k∈[K], 0 ≤α ≤β with κ := maxk
β+λk
α+λk ,
Bw > 0, γ > 0, and ε < Bw/2, suppose in input format (3) we have D ≥Θ(Kd). Then there exists
an L-layer transformer TFθ with
L = ⌈2κ log(Bw/(2ε))⌉+ 4,
max
ℓ∈[L] M (ℓ) ≤3K + 1,
max
ℓ∈[L] D(ℓ) ≤K2 + K + 1,
64

|||θ||| ≤O

KR + (β + λ)−1 + N
Nval
+ γ−1

,
R := max{BxBw, By, 1},
such that the following holds. On any input data (D, xN+1) such that the problem (ICRidge) is
well-conditioned and has a bounded solution:
α ≤λmin(X⊤
trainXtrain/Ntrain) ≤λmax(X⊤
trainXtrain/Ntrain) ≤β,
max
k∈[K]
wλk
ridge(Dtrain)

2 ≤Bw/2,
(42)
TF0
θ approximately implements ridge selection: its prediction
byN+1 = ready(TF0
θ(H)) = ⟨bw, xN+1⟩,
bw =
K
X
k=1
λk bwk
satisfies the following.
1. For each k ∈[K], bwk = bwk(Dtrain) approximates the ridge estimator wλk
ridge(Dtrain), i.e.
bwk −wλk
ridge(Dtrain)

2 ≤ε.
2. λ = (λ1, · · · , λK) ∈∆([K]) so that
λk > 0 only if bLval(bwk) ≤min
k⋆∈[K]
bLval(bwk⋆) + γ.
In particular, if we set γ′ = 2(BxBw + By)Bxε + γ, then it holds that8
dist

bw, conv{bwλk
ridge,train : bLval(bwλk
ridge,train) ≤min
k⋆∈[K]
bLval(bwλk⋆
ridge,train) + γ′}

≤ε,
where we denote bwλk
ridge,train := wλk
ridge(Dtrain).
To prove Theorem I.2, we first show that, for the squared validation loss, there exists a 3-layer
transformer that performs predictor selection based on the exactly evaluated bLval(fk) for each
k ∈[K]. (Proof in Appendix I.2.1.)
Theorem I.3 (Square-loss version of Theorem I.1). Consider the squared validation loss
bLval(f) :=
1
2|Dval|
X
(xi,yi)∈Dval
(f(xi) −yi)2.
Then there exists a 3-layer transformer TFθ with
max
ℓ∈[3] M (ℓ) ≤2K + 2,
max
ℓ∈[3] D(ℓ) ≤K2 + K + 1,
|||θ||| ≤7KR + 2N
|Dval| + γ−1,
such that for any input H that takes form
hi = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K; ∗; 1; ti],
where TFθ outputs hN+1 = [xN+1; bf(xN+1); ∗; 1; 0], where the predictor bf : Rd →R is a convex
combination of {fk : bLval(fk) ≤mink⋆∈[K] bLval(fk⋆) + γ}. As a corollary, for any convex risk
L : (Rd →R) →R, bf satisfies
L( bf) ≤mink⋆∈[K] L(fk⋆) + maxk∈[K]
bLval(fk) −L(fk)
 + γ.
Proof of Theorem I.2
First, by the proof9 of Theorem 4 and Proposition B.6, for each k ∈[K],
there exists a T = L −3 layer transformer θ(1:T ) such that TFθ(1:T ) maps
hi
→
h(T )
i
= [xi; y′
i; ∗; ⟨bw1, xi⟩; · · · ; ⟨bwK, xi⟩; 0K; 1; ti],
8This is because bLval(w) is (BxBw + By)Bx-Lipschitz w.r.t. w ∈B2(Bw).
9Technically, an adapted version where the underlying ICGD mechanism operates on the training split (with
ti = 1) with size Ntrain instead of on all N training examples, which only changes |||θ||| by at most a constant
factor, and does not change the number of layers and heads.
65

so that if (42) holds, we have
bwk −wλk
ridge

2 ≤ε and bwk ∈B2(Bw).
Next, by Theorem I.3, there exists a 3-layer transformer θ(T +1:T +3) that outputs
h(T +3)
N+1
= [xN+1; ⟨bw, xN+1⟩; ∗; 1; ti],
where bw = PK
k=1 λk bwk, λ = (λ1, · · · , λK) ∈∆([K]) so that
λk > 0 only if bLval(bwk) ≤min
k⋆∈[K]
bLval(bwk⋆) + γ.
This is the desired result.
I.2.1
Proof of Theorem I.3
Similar to the proof of Proposition 10, Theorem I.3 is a direct corollary by combining Proposition I.3
with Proposition I.2.
Proposition I.3 (Evaluation layer for the squared loss). There exists an attention layer TFθ with 2K
heads and |||θ||| ≤3R + 2NK/ |Dval| such that TFθ maps
hi = [∗; f1(xi); · · · ; fK(xi); 0K; ∗; 1; ti]
→
h′
i = [∗; f1(xi); · · · ; fK(xi); bLval(f1); · · · ; bLval(fK); ∗; 1; ti],
i ∈[N + 1].
Proof of Proposition I.3. For every k ∈[K], we define matrices Qm,k, Km,k, Vm,k ∈RD×D such
that for all i, j ∈[N + 1],
Qk,0hi =


1
−1
−2
0

,
Qk,1hi =


−1
1
−2
0

,
Kk,0hj = Kk,1hj =


fk(xj)
yj
R(1 + tj)
0

,
Vk,0hj = −Vk,1hj = (N + 1)
2 |Dval| · (fk(xj) −yj)eD−(K−k)−3.
As the input has structure hi = [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti], these matrices indeed
exist, and further it is straightforward to check that they have norm bounds
max
k∈[K],w∈{0,1} ∥Qk,w∥op ≤3,
max
k∈[K],w∈{0,1} ∥Kk,w∥op ≤1 + R,
X
k∈[K],w∈{0,1}
∥Vk,w∥op ≤K(N + 1)
|Dval|
.
Now, for every i, j ∈[N + 1], we have
X
w∈{0,1}
σ(⟨Qk,whi, Kk,whj⟩)Vk,whj
= [σ(fk(xj) −yj −2R(1 + tj)) −σ(yj −fk(xj) −2R(1 + tj))] · (N + 1)
2 |Dval| (fk(xj) −yj)eD−(K−k)−3
= 1{tj = −1} · [σ(fk(xj) −yj) −σ(yj −fk(xj))] · (N + 1)
2 |Dval| (fk(xj) −yj)eD−(K−k)−3
= 1{tj = −1} · (N + 1)
2 |Dval| (fk(xj) −yj)2eD−(K−k)−3,
where the second equality follows from the bound |fk(xj) −yj| ≤2R, so that the relus equals 0 if
tj ≤0. Thus letting the attention layer θ = {(Vk,w, Qk,w, Kk,w)}(k,w)∈[K]×{0,1}, we have
ehi = [Attnθ(H)]i = hi +
1
N + 1
N+1
X
j=1
X
k,w
σ(⟨Qk,whi, Kk,whj⟩)Vk,whj
66

= hi +
1
2|Dval|
N+1
X
j=1
K
X
k=1
(fk(xj) −yj)2 · 1{tj = −1}eD−(K−k)−3
= hi +
K
X
k=1


1
2|Dval|
X
(xj,yj)∈Dval
(fk(xj) −yj)2

eD−(K−k)−3
= hi +
K
X
k=1
bLval(fk) · eD−(K−k)−3
= [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); 0K+1; 1; ti] + [0D−K−3; bLval(f1); · · · ; bLval(fK); 0; 0; 0]
= [xi; y′
i; ∗; f1(xi); · · · ; fK(xi); bLval(f1); · · · ; bLval(fK); 0; 1; ti],
i ∈[N + 1].
This is the desired result.
I.3
Proofs for Section 4.2
I.3.1
Proof of Lemma 13
It is straightforward to check that the binary type check ψ : R →R can be expressed as a linear
combination of 6 relu’s (recalling σ(·) = ReLU(·)):
ψ(y) = σ
y + ε
ε

−2σ
y
ε

+ σ
y −ε
ε

+ σ
y −(1 −ε)
ε

−2σ
y −1
ε

+ σ
y −(1 + ε)
ε

=:
6
X
m=1
amσ(bmy + cm),
with P
m |am| = 8/ε, maxm max {|bm|, |cm|} ≤2. We can thus construct an attention layer
θ = {(Qm, Km, Vm)}6
m=1 with 6 heads such that
Qmhi = [bm; cm; 0D−2],
Kmhj = [yj; 1; 0D−2],
Vmhj =
N + 1
N
am · tj; 0D−1

,
which gives that for every i ∈[N + 1],
6
X
m=1
1
N + 1
X
j∈[N+1]
σ(⟨Qmhi, Kmhj⟩)[Vmhj]1
=
6
X
m=1
1
N
N
X
j=1
σ(bmyj + cm)am = 1
N
N
X
j=1
ψ(yj) = Ψbinary(D).
Further, we have |||θ||| ≤18/ε = O(1/ε). This is the desired result.
By composing the above attention layer with one additional layer (with 2 heads) that implement the
following function
σ(2(t −1/2)) −σ(2(t −1)),
on the output Ψbinary(D), we directly obtain the following corollary.
Corollary I.1 (Thresholded binary test). There exists a two-layer attention-only transformer with
maxℓ∈[2] M (ℓ) ≤6 and |||θ||| ≤O(1/ε) that exactly implements the thresholded binary test
Ψbinary
thres (D) :=







1,
if Ψbinary(D) ≥1,
0,
if Ψbinary(D) ≤1
2,
linear interpolation,
o.w.
(43)
at every token i ∈[N + 1], where we recall the definition of Ψbinary in Lemma 13.
67

I.3.2
Formal statement and proof of Proposition 14
We say a distribution Py on R is (C, ε0)-not-concentrated around {0, 1} if
Py([−ε, ε] ∪[1 −ε, 1 + ε]) ≤Cε
for all ε ∈(0, ε0]. A sufficient condition is that the density py is upper bounded by C within
[−ε0, ε0] ∪[1 −ε0, 1 + ε0].
Throughout this section, let σlog(t) := (1 + e−t)−1 denote the sigmoid activation, and let bwlog
denote the solution to the in-context logistic regression problem, i.e. (ICGLM) with g(·) = σlog(·).
Proposition I.4 (Adaptive regression or classification; Formal version of Proposition 14). For any
Bw > 0, ε ≤BxBw/10, 0 < α ≤β with κ := β/α, and any (C, ε0), there exists a L-layer
attention-only transformer with
L ≤O

κ log BxBw
ε

,
max
ℓ∈[L] M (ℓ) ≤O

1 + B4
x
α2

ε−2

,
|||θ||| ≤O

R + 1
β + 1
ε

(with R := max {BxBw, By, 1}, and ε depending only on (C, ε0)) such that the following holds.
Suppose the input format is (3) with dimension D ≥3d + 4.
On any classification instance (D, xN+1) (such that {yi}i∈[N] ⊂{0, 1}) that is well-conditioned
for logistic regression in the sense of (35), it outputs byN+1 that ε-approximates the prediction of
in-context logistic regression:
|byN+1 −σlog(⟨xN+1, bwlog⟩)| ≤ε.
On the contrary, for regression problems, i.e. any in-context distribution P whose marginal Py is
(C, ε0)-not-concentrated around {0, 1}, with probability at least 1−exp(−cN) over D (where c > 0
depends only on (C, ε0)), byN+1 ε-approximates the prediction of in-context least squares if the data
is well-conditioned:
|byN+1 −⟨xN+1, bwLS⟩| ≤ε
whenever D satisfies (5) with λ = 0,
where bwLS denotes the in-context least squares estimator, i.e. (ICRidge) with λ = 0.
Proof. The result follows by combining the binary test in Corollary I.1 with Theorem 4 and Theo-
rem G.1. By those results, there exists three attention-only transformers θLS, θlog, θbin, with (below
Lg, Cg = Θ(1) for g = σlog(·))
LLS ≤O

κ log BxBw
ε

,
max
ℓ∈[LLS] M (ℓ)
LS ≤3,
|||θLS||| ≤O

R + 1
β

,
Llog ≤O

κ log LgBxBw
ε

,
max
ℓ∈[Llog] M (ℓ)
log ≤O
 
C2
g
 
1 + L2
gB4
x
α2
!
ε−2
!
,
|||θlog||| ≤O

R + Cg
β

,
Lbin = 2,
max
ℓ∈[2] M (ℓ)
bin ≤6,
|||θbin||| ≤O(1/ε),
that outputs prediction byLS
N+1, bylog
N+1 (at the (N + 1)-th token) and Ψbinary
thres (D) (at every token)
respectively, which satisfy
bylog
N+1 −σlog(⟨xN+1, bwlog⟩)
 ≤ε,
byLS
N+1 −⟨xN+1, bwLS⟩
 ≤ε.
when the corresponding well-conditionednesses are satisfied. In particular, we can make bwlog well-
defined on non-binary data, by multiplying Ψbinary
thres (D) onto the xi’s (which can be implemented by
slightly modifying θlog without changing the order of the number of layers, heads, and norms) so
that bwlog = 0 on any data where Ψbinary
thres (D) = 0.
By joining θLS and θlog using Proposition B.6, concatenating with θbin before, and concatenating
with one additional attention layer with 2 heads after to implement
Ψbinary
thres (D)bylog
N+1 +

1 −Ψbinary
thres (D)

byLS
N+1,
(44)
68

we obtain a single transformer θ with
L ≤O

κ log BxBw
ε

,
max
ℓ∈[L] M (ℓ) ≤O

1 + B4
x
α2

ε−2

,
|||θLS||| ≤O

R + 1
β + 1
ε

,
which outputs (44) as its prediction (at the location for byN+1).
It remains to show that (44) reduces to either one of bylog
N+1 or byLS
N+1. When the data are binary
(yi ∈{0, 1}), we have Ψbinary(D) = 1 and Ψbinary
thres (D) = 1, in which case (44) becomes exactly
bylog
N+1. By contrast, when data is sampled from a distribution that is (C, ε0)-not-concentrated
around {0, 1}, we have for any fixed ε ≤ε0 ∧
1
4C that, letting Bε := [−ε, ε] ∪[1 −ε, 1 + ε] and
pε := Py(Bε) ≤Cε ≤1
4, by Hoeffding’s inequality,
P(Ψbinary
thres (D) ̸= 0) = P

Ψbinary
thres (D) ≥1
2

= P
 
1
N
N
X
i=1
1{yj ∈Bε} ≥1
2
!
≤exp

−c(1/2 −pε)2N

≤exp(−c′N),
where c′ > 0 is an absolute constant. On the event Ψbinary
thres (D) = 0 (which happens with probability
at least 1 −exp(−c′N)), (44) becomes exactly byLS
N+1. This finishes the proof.
I.4
Linear correlation test and application
In this section, we give another instantiation of the pre-ICL testing mechanism by showing that the
transformer can implement a linear correlation test that tests whether the correlation vector E[xy] has
a large norm. We then use this test to construct a transformer to perform “confident linear regression”,
i.e. output a prediction from linear regression only when the signal-to-noise ratio is high.
For any fixed parameters λmin, B⋆
w > 0, consider the linear correlation test over data D defined as
Ψlin(D) :=
1
λ2
min(B⋆w)2/2 ·
h
σ

∥bt∥2
2 −(λminB⋆
w/4)2
−σ

∥bt∥2
2 −(3λminB⋆
w/4)2i
=



0,
∥bt∥2
2 ≤(λminB⋆
w/4)2,
1,
∥bt∥2
2 ≥(3λminB⋆
w/4)2,
linear interpolation,
o.w.,
where bt = T(D) := 1
N
N
X
i=1
xiyi.
(45)
Recall that σ(·) = ReLU(·) above denotes the relu activation.
We show that Ψlin can be exactly implemented by a 3-layer transformer.
Lemma I.1 (Expressing Ψlin by transformer). There exists a 3-layer attention-only transformer TFθ
with at most 2 heads per layer and |||θ||| ≤O(1 + λ2
min(B⋆
w)2) such that on input sequence H of
the form (3) with D ≥2d + 4, the transformer exactly implements Ψlin: it outputs eH such that
ehi = [xi; yiti; ∗; Ψlin(D); 1] for all i ∈[N + 1].
Proof. We begin by noting the following basic facts:
• Identity function can be implemented exactly by two ReLUs: t = σ(t) −σ(−t).
• Squared ℓ2 norm can be implemented exactly by a single attention head (assuming every input
hi contains the same vector g): ∥g∥2
2 = σ(⟨g, g⟩).
We construct the transformer θ as follows.
Layer 1: Use 2 heads to implement bt = 1
N
PN
i=1 xiyi, where V(1)
{1,2}hj = [±xj; 0D−d], Q(1)
{1,2}hi =
[ N+1
N ; 0D−1], and K(1)
{1,2}hj = [±yjtj; 0D−1] = [±yj1{j < N + 1}; 0D−1] (where we recall
tj = 1{j < N + 1} and note that yjtj corresponds exactly to the location for yj in H, cf. (3)).
69

By manipulating the output dimension in V(1), write the result bt into blank memory space with
dimension d at every token i ∈[N + 1].
Layer 2: Use a single head to compute ∥bt∥2
2: Q(2)
1 h(1)
i
= [bt; 0D−d], K(2)
1 h(1)
j
= [bt; 0D−d], and
V(2)
1 h(1)
j
= [1; 0D−1]. By manipulating the output dimension in V(2), write the result ∥bt∥2
2 into
blank memory space with dimension 1 at every token i ∈[N + 1]. After layer 2, we have h(3)
i
=
[xi; yiti; ∗; ∥bt∥2
2; ∗; 1].
Layer 3: Use 2 heads to implement two ReLU functions with bias: ∥bt∥2
2 7→
1
B−A(σ(∥bt∥2
2 −A) −
σ(∥bt∥2
2 −B)). The two query (or key) matrices contain values A and B. In our problem we take
A = (λminB⋆
w/4)2,
B = (3λminB⋆
w/4)2,
so that the above ReLU function implements Ψlin(D) exactly. Write the result into a blank memory
space with dimension 1. We finish the proof by noting that |||θ||| ≤O(1 + λ2
min(B⋆
w)2).
Statistical guarantee for Ψlin
We consider the following well-posedness assumption for the linear
correlation test Ψlin. Note that, similar as Assumption A, the assumption does not require the data
to be generated from any true linear model, but rather only requires some properties about the best
linear fit w⋆
P, as well as sub-Gaussianity conditions.
Assumption D (Well-posedness for linear correlation test). We say a distribution P on Rd × R is
well-posed for linear independence tests, if (x, y) ∼P satisfies
(1) ∥x∥2 ≤Bx and |y| ≤By almost surely;
(2) The covariance ΣP := EP[xx⊤] satisfies λminId ⪯ΣP ⪯λmaxId, with 0 < λmin ≤λmax,
and κ := λmax/λmin.
(3) The whitened vector Σ−1/2
P
x is K2-sub-Gaussian for some K ≥1.
(4) The best linear predictor w⋆
P := EP[xx⊤]−1EP[xy] satisfies ∥w⋆
P∥2 ≤B⋆w.
(5) The label y is σ2-sub-Gaussian.
(6) The residual z := y −⟨x, w⋆
P⟩is σ2-sub-Gaussian with probability one (over x).
The following results states that Ψlin achieves high power as long as the sample size is high enough,
and the signal ∥w⋆
P∥2 is either sufficiently high or sufficiently low.
Proposition I.5 (Power of linear correlation test). Suppose distribution P satisfies Assumption D with
parameters λmin, λmax, B⋆w. Then, for the linear correlation test Ψlin with parameters (λmin, B⋆
w)
with B⋆
w ≤B⋆w and any N ≥e
O

max {K4, λmaxK2σ2
(B⋆
w)2λ2
min } · d

, we have
1. If ∥w⋆
P∥2 ≥B⋆
w, then with probability at least 1 −δ over D, we have Ψlin(D) = 1.
2. If ∥w⋆
P∥2 ≤
λmin
10λmax B⋆
w, then with probability at least 1 −δ over D, we have Ψlin(D) = 0.
Proof. For any P satisfying Assumption D, note that E[xz] = E[x(y−⟨w⋆
P, x⟩)] = 0 by construction.
Therefore, by standard sub-Gaussian and sub-exponential concentration combined with union bound,
the following events hold simultaneously with probability at least 1 −δ:
0.9ΣP ⪯bΣ = 1
N
N
X
i=1
xix⊤
i ⪯1.1ΣP
as N ≥e
O(dK4) by (27),

1
N
N
X
i=1
xizi

2
≤λ1/2
max ·

1
N
N
X
i=1
Σ−1/2
P
xizi

2
≤e
O
 
λ1/2
max
 
Kσ
√
d
√
N
+ Kσd
N
!!
≤e
O
 
λ1/2
maxKσ
r
d
N
!
≤λminB⋆
w
8
,
as N ≥e
O
λmaxdK2σ2
(B⋆w)2λ2
min

.
70

On the above event, we have
bt

2 =

1
N
N
X
i=1
xi(⟨xi, w⋆
P⟩+ zi)

2
=

bΣw⋆
P + 1
N
N
X
i=1
xizi

2
.
Therefore, in case 1, we have
bt

2 ≥
bΣw⋆
P

2 −

1
N
N
X
i=1
xizi

2
≥0.9λmin ∥w⋆
P∥2 −λminB⋆
w
8
≥3λminB⋆
w
4
.
In case 2, we have
bt

2 ≤
bΣw⋆
P

2 +

1
N
N
X
i=1
xizi

2
≤λmax · λminB⋆
w
10λmax
+ λminB⋆
w
8
≤λminB⋆
w
4
.
The proof is finished by recalling the definition of Ψlin in (45), so that Ψlin(D) = 1 if ∥bt∥2 ≥
3λminB⋆
w/4, and Ψlin(D) = 0 if ∥bt∥2 ≤λminB⋆
w/4.
Application: Confident linear regression
By directly composing the linear correlation test
in Lemma I.1 with the transformer construction in Corollary 5 (using an argument similar as the
proof of Proposition I.4), and using the power of the linear correlation test Proposition I.5, we imme-
diately obtain the following result, which outputs a prediction from (approximately) least squares if
bψ := Ψlin(D) = 1, and abstains from predicting if bψ = 0. This can be viewed as a form of “confident
linear regression”, where the model predicts only if it thinks the linear signal is strong enough.
Proposition I.6 (Confident linear regression). For any Bw > 0, 0 < B⋆
w ≤B⋆w, 0 ≤λmin ≤λmax,
ε ≤BxBw/10, 0 < α ≤β with κ := β/α, there exists a L-layer attention-only transformer with
L ≤O

κ log BxBw
ε

,
max
ℓ∈[L] M (ℓ) ≤O(1),
|||θ||| ≤O

R + 1
β + λ2
min(B⋆
w)2

(with
R
:=
max {BxBw, By, 1})
such
that
the
following
holds.
Let
N
≥
e
O

max {K4, λmaxK2σ2
(B⋆
w)2λ2
min } · d

. Suppose the input format is (3) with dimension D ≥2d + 4. Let ICL
instance (D, xN+1) be drawn from any distribution P satisfying Assumption D. Then the transformer
outputs a 2-dimensional prediction (within the test token ehN+1)
(byN+1, bψ) ∈R × {0, 1}
such that the following holds:
1. If ∥w⋆
P∥2 ≥B⋆
w, then with probability at least 1−δ over D, we have |byN+1−⟨bwLS, xN+1⟩| ≤ε,
and bψ = 1 if D is in addition well-conditioned for least squares (in the sense of (5) with λ = 0).
2. If ∥w⋆
P∥2 ≤
λmin
10λmax B⋆
w, then with probability at least 1 −δ over D, we have byN+1 = 0 and
bψ = 0.
J
Proof of Theorem 12: Noisy linear model with mixed noise levels
For each fixed k ∈[K], we consider the following data generating model Pk, where we first sample
P = Pw⋆,σk ∼π from w⋆∼N(0, Id/d), and then sample data {(xi, yi)}i∈[N+1]
iid
∼Pw⋆,σk as
Pw⋆,σk : xi ∼N(0, Id),
yi = ⟨xi, w⋆⟩+ εi,
εi ∼N(0, σ2
k).
Also, recall that the Bayes optimal estimator on Pk is given by byBayes
N+1 =
D
wλk
ridge(D), xN+1
E
with
ridge λk = σ2
kd/N, and the Bayes risk on Pk is given by
BayesRiskk := infA Ek
 1
2(A(D)(xN+1) −yN+1)2
= Ek
h
1
2
 byBayes
N+1 −yN+1
2i
.
71

Recall that in Section 4.1.1, we consider a mixture law Pπ that generates data from Pk with k ∼Λ. It
is clear that we have (pushing infA into Ek∼Λ does not increase the value) we have
BayesRiskπ ≥Ek∼Λ[BayesRiskk],
i.e., the Bayes risk can only be greater if we consider a mixture of models. In other words, if a
transformer can achieve near-Bayes ICL on each meta-task Pk, then it can perform near-Bayes ICL
on any meta-task π which is a mixture of Pk with k ∼Λ. Therefore, to prove Theorem 12, it suffices
to show the following (strengthened) result.
Theorem J.1 (Formal version of Theorem 12). Suppose that N ≥0.1d and we write σmax =
maxk{σk, 1}, σmin = mink{σk, 1}. Suppose in input format (3) we have D ≥Θ(Kd). Then there
exists a transformer θ with
L ≤O
 σ−2
min log(N/σmin)

,
max
ℓ∈[L] M (ℓ) ≤O (K) ,
max
ℓ∈[L] D(ℓ) ≤O(K2),
|||θ||| ≤O (σmaxKd log(N)) ,
such that for any k ∈[K], it holds that
Ek
1
2(yN+1 −byN+1)2

≤BayesRiskk + e
O
 
σ2
max
σ2/3
min
log K
N
1/3
!
if we choose Nval := |Dval| ≍N 2/3[log K]1/3.
The core of the proof of Theorem J.1 is to show that any estimator bw that achieves small validation
loss bLval must achieve small population loss.
Throughout the rest of this section, recall that we define Ntrain = |Dtrain| , Nval = |Dval|, Itrain = {i :
(xi, yi) ∈Dtrain}, Ival = {i : (xi, yi) ∈Dval}, and Xtrain = [xi]i∈Itrain.
J.1
Proof of Theorem J.1
Fix parameters δ, ε, γ > 0 and a large universal constant C0. Let us set
α = max
n
0, 1/2 −
p
d/Ntrain
o2
,
β = 25,
B⋆
w = 1 + C0
r
log(N)
d
,
Bw = C0(B⋆
w + σmax/σmin),
Bx = C0
p
d log(N),
By = C0(B⋆
w + σmax)
p
log(N),
Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):
Eπ = {∥w⋆∥2 ≤B⋆
w, ∥ε∥2 ≤2σmax
√
N},
Ew = {α ≤λmin(X⊤
trainXtrain/Ntrain) ≤λmax(X⊤
trainXtrain/Ntrain) ≤β},
Eb,train = {∀(xi, yi) ∈Dtrain, ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,val = {∀(xi, yi) ∈Dval, ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,N+1 = {∥xN+1∥2 ≤Bx, |yN+1| ≤By}.
For the good event E := Eπ ∩Ew ∩Eb,train ∩Eb,test ∩Eb,N+1, we can show that P(Ec) ≤O
 N −10
.
Further, by the proof of Lemma F.1 (see e.g. (34)), we know that maxk∈[K]
wλk
ridge(Dtrain)

2 ≤
Bw/2 holds under the good event E.
For the ridge λk =
dσ2
k
Ntrain and parameters (α, β, γ, ε), we consider the transformer θ constructed in
Theorem I.2, with a clipped prediction byN+1 = g
ready(TFθ(H)).
In the following, we upper bound the quantity Ek(byN+1 −yN+1)2 for any fixed k. Similar to the
proof of Corollary 6 (Appendix F.4), we decompose
Ek(byN+1 −yN+1)2 = Ek

1{E}(byN+1 −yN+1)2
+ Ek

1{Ec}(byN+1 −yN+1)2
,
and we analyze these two parts separately.
72

Part I.
Recall that by our construction, when E holds, we have byN+1 = clipBy(⟨bw, xN+1⟩) and
the statements of Theorem I.2 hold for bw. Thus, we have
Ek

1{E}(byN+1 −yN+1)2
= Ek
h
1{E}(clipBy(⟨xN+1, bw⟩) −yN+1)2i
≤Ek

1{E}(⟨xN+1, bw⟩−yN+1)2
.
Let us consider the following risk functional
Lval,w⋆(w) = E(x,y)∼Pw⋆,σk
h
1
2(⟨w, x⟩−y)2i
= 1
2

∥w −w⋆∥2
2 + σ2
k

.
Then, under the good event E0 := Eπ ∩Ew ∩Eb,train ∩Eb,test of (w⋆, D),
Ek

1{E}(⟨xN+1, bw⟩−yN+1)2 w⋆, D

= Ek

1{E}(⟨xN+1, bw(D)⟩−yN+1)2 w⋆, D

≤Ek

(⟨xN+1, bw(D)⟩−yN+1)2 w⋆, D

= E(x,y)∼Pw⋆,σk

(⟨xN+1, bw(D)⟩−yN+1)2
= Lval,w⋆(bw(D)).
By our construction, under the good event E0, we have
Lval,w⋆(bw(D)) ≤Lval,w⋆(bwk(Dtrain)) + max
l∈[K]
bLval(bwl(Dtrain)) −Lval,w⋆(bwl(Dtrain))
 + γ,
where
bwl(Dtrain)) −wλl
ridge(Dtrain)

2 ≤ε for each l ∈[K]. Clearly,
2Ek[1{E0}Lval,w⋆(bwk(Dtrain))] = Ek
h
1{E0}

∥bwk(Dtrain) −w⋆∥2
2 + σ2
k
i
≤Ek
h
1{E0}
wλk
ridge(Dtrain) −w⋆
2
2 + 2ε
wλk
ridge(Dtrain) −w⋆

2 + ε2i
+ σ2
k
≤Ek
hwλk
ridge(Dtrain) −w⋆
2
2 + 2ε
wλk
ridge(Dtrain) −w⋆

2 + ε2i
+ σ2
k
≤2Riskk,train + 2ε
p
2Riskk,train + ε2,
where we denote 2Riskk,train = Ek
wλk
ridge(Dtrain) −w⋆
2
2 + σ2
k, and we also note that Riskk,train ≤
1 + σ2
k by definition. By Lemma J.1, we have
Riskk,train ≤BayesRiskk + O

(σ2
k + 1)Nval
N

.
We next deal with the term εval := maxl∈[K]
bLval(bwl(Dtrain)) −Lval,w⋆(bwl(Dtrain))
. Note that for
the good event Etrain := Eπ ∩Ew ∩Eb,train of (w⋆, Dtrain), we have
Ek[1{E0}εval] ≤Ek[1{Etrain}εval] ≤Ew⋆,Dtrain∼Pk[1{Etrain} · EDval [εval| w⋆, Dtrain]].
Thus, Lemma J.2 yields
Ek[1{E0}εval] ≤O
 B2
w

·
"r
log K
Nval
+ log K
Nval
#
.
Therefore, we can conclude that
Ek

1{E}(byN+1 −yN+1)2
≤2BayesRiskk + O
 
εσmax + ε2 + σ2
maxNval
N
+ B2
w
r
log K
Nval
+ B2
w log K
Nval
!
.
Therefore, we can choose (ε, Nval) so that Nval ≤N/2 as
Nval = max
( B2
w
σ2max
N
2/3
log1/3(K), log K
)
,
ε = σmax
N
.
It is worth noting that such choice of Nval is feasible as long as N ≳
B4
w
σ4
max log K. Under such choice,
we obtain
1
2Ek

1{E}(byN+1 −yN+1)2
≤BayesRiskk + O
 
σ4/3
maxB2/3
w
log K
N
1/3!
.
73

Part II.
Similar to the proof of Corollary 6, we have
E

1{Ec}(byN+1 −yN+1)2
≤O
 
B2
y
N 5
!
≤O
σ2
max
N 4

.
Conclusion.
Combining the both cases, we obtain
Ek
 1
2(yN+1 −byN+1)2
≤BayesRiskk + O

σ4/3
maxB2/3
w

log K
N
1/3
≤BayesRiskk + O

σ2
max
σ2/3
min

log K
N
1/3
+ σ4/3
max
log2/3(N) log1/3(K)
d2/3N1/3

≤BayesRiskk + e
O

σ2
max
σ2/3
min

log K
N
1/3
,
where we plug in our choice of By. The bounds on M (ℓ), D(ℓ) and |||θ||| follows immediately from
Theorem I.2. This completes the proof.
J.2
Derivation of the exact Bayes predictor
Let (D, xN+1, yN+1) be (N + 1) observations from the data generating model π considered in Sec-
tion 4.1.1. On observing (D, xN+1), the Bayes predictor of yN+1 is given by its posterior mean:
Eπ[yN+1|D, xN+1] = Eπ[⟨xN+1, w⋆⟩+ εN+1|D, xN+1] = ⟨xN+1, Eπ[w⋆|D]⟩.
It thus remains to derive Eπ[w⋆|D]. Recall that our data generating model is given by k ∼Λ, By
Bayes’ rule, we have
Eπ[w⋆|D] =
X
k′∈[K]
Pπ(k = k′|D) · Eπ[w⋆|D, k = k′].
(46)
On k = k′, the data is generated from the noisy linear model w⋆∼N(0, Id/d), and y = Xw⋆+ ε
where εi
iid
∼N(0, σ2
k′). It is a standard result that Eπ[w⋆|D, k = k′] is given by the ridge estimator
Eπ[w⋆|D, k = k′] =
 X⊤X + dσ2
k′
−1
|
{z
}
bΣ−1
k′
X⊤y =: bwk′
=
X⊤X
N
+ dσ2
k′
N
−1 X⊤y
N
.
(Note that the sample covariance within bΣk′ is not normalized by N, which is not to be confused
with remaining parts within the paper.) Therefore, the posterior mean (46) is exactly a weighted
combination of K ridge regression estimators, each with regularization dσ2
k/N.
It remains to derive the mixing weights Pπ(k = k′|D) for all k′ ∈[K]. By Bayes’ rule, we have
Pπ(k = k′|D) ∝k′ Pπ(k = k′) ·
Z
w⋆
p(w⋆) · pk′,w⋆(D|w⋆)dw⋆
∝Λk′ ·
Z
w
1
(2πd)d/2(2πσ2
k′)N/2 exp
 
−d∥w∥2
2
2
−∥Xw −y∥2
2
2σ2
k′
!
dw
∝Λk′ ·
Z
w
1
(2πσ2
k′)N/2 exp
 
−1
2w⊤
X⊤X
σ2
k′
+ dId

w +

w, X⊤y
σ2
k′

−∥y∥2
2
2σ2
k′
!
dw
∝Λk′ ·
Z
w
1
(2πσ2
k′)N/2 exp

−1
2σ2
k′ (w −bwk′)⊤bΣk′(w −bwk′) −
1
2σ2
k′

∥y∥2
2 −y⊤XbΣ−1
k′ X⊤y

dw
∝Λk′ · det(bΣk′/σ2
k′)−1/2
σN
k′
exp

−1
2σ2
k′

∥y∥2
2 −y⊤XbΣ−1
k′ X⊤y

74

∝Λk′ ·
1
σN−d
k′
det(X⊤X + dσ2
k′Id)1/2 exp

−1
2σ2
k′

∥y∥2
2 −⟨y, Xbwk′⟩

.
Note that such mixing weights involve the determinant of the matrix bΣk′ = X⊤X + dσ2
k′Id, which
depends on the data X in a non-trivial fashion; Any transformer has to approximate these weights if
their mechanism is to directly approximate the exact Bayesian predictor (46).
J.3
Useful lemmas
Lemma J.1. For 2Riskk,train = Ek
wλk
ridge(Dtrain) −w⋆
2
2 + σ2
k, there exists universal constant C
such that
Riskk,train ≤BayesRiskk + C(σ2
k + 1)Nval
N .
Proof. Recall that under Pk, we have
w⋆∼N(0, Id/d),
yi = ⟨xi, w⋆⟩+ εi,
εi ∼N(0, σ2).
We denote yt = [yi]i∈Itrain, then by definition wλk
ridge(Dtrain) = (X⊤
trainXtrain +dσ2
k)−1Xtrainyt (with
λk = dσ2
k/Ntrain). Thus, a simple calculation yields
2Riskk,train = Ek
wλk
ridge(Dtrain) −w⋆
2
2 + σ2
k = σ2
kEtr
 (X⊤
trainXtrain + dσ2
k)−1
+ σ2
k,
and analogously, 2BayesRiskk = σ2
kEtr
 (X⊤X + dσ2
kId)−1
+ σ2
k. Therefore,
2Riskk,train −2BayesRiskk = σ2
kEtr
 (X⊤
trainXtrain + dσ2
kId)−1
−σ2
kEtr
 (X⊤X + dσ2
kId)−1
≤σ2
kNvalEk[λmin(Σ)−1],
where in the above inequality we denote Σ := X⊤
trainXtrain + dσ2
kId and use the following fact:
tr
 Σ−1
−tr
 (Σ + X⊤
v Xv)−1
= tr

Σ−1/2(Id −(Id + Σ−1/2X⊤
v XvΣ−1/2)−1)Σ−1/2
= tr

Σ−1/2(Id + Σ−1/2X⊤
v XvΣ−1/2)−1Σ−1/2X⊤
v XvΣ−1
=
D
(Id + Σ−1/2X⊤
v XvΣ−1/2)−1Σ−1/2X⊤
v XvΣ−1/2, Σ−1E
≤rank(Σ−1/2X⊤
v XvΣ−1/2)λmax(Σ−1) ≤Nvalλmin(Σ)−1.
Case 1. We first suppose that Ntrain ≤16d. Then by definition Σ ⪰dσ2
kId, and hence
σ2
kNvalEk[λmin(Σ)−1] ≤σ2
kNval
dσ2
k
≤16Nval
Ntrain
≤32Nval
N
.
Case 2. When Ntrain ≥9d, then we consider the event Et := {λmin(X⊤
trainXtrain/Ntrain) ≥
1
16}. By
Lemma B.2 we have P(Ec
t ) ≤exp(−Ntrain/8). Therefore,
σ2
kNvalEk[λmin(Σ)−1] = σ2
kNvalEk[1{Et}λmin(Σ)−1] + σ2
kNvalEk[1{Ec
t }λmin(Σ)−1]
≤16σ2
kNval
Ntrain
· P(Et) + Nval
d
· P(Ec
t )
≤32σ2
kNval
N
+ Nval
d
· exp(−N/16) = O
(σ2
k + 1)Nval
N

.
Combining these two cases finishes the proof.
Lemma J.2. Condition on the event Etrain, we have
EDval∼Pk|w⋆,Dtrain

max
l∈[K]
bLval(bwl) −Lval,w⋆(bwl)


≤CB2
w

log(2K)
Nval
+
s
log(2K)
Nval

,
where we denote bwl = bwl(Dtrain).
75

Proof. We only need to work with a fixed pair of (w⋆, Dtrain) such that Etrain holds. Hence, in the
following we only consider the randomness of Dval conditional on such a (w⋆, Dtrain).
Recall that for any w,
bLval(w) =
1
2 |Dval|
X
(xi,yi)∈Dval
(⟨xi, w⟩−yi)2,
and we have EDv[bLval(w)] = Lval,w⋆(w). For each i ∈Ival,
yi −⟨xi, bwl⟩= εi −⟨xi, w⋆−bwl⟩∼SG(σ2
k + ∥w⋆−bwl∥2).
Note that under Etrain, we have bwl ∈B2(Bw) for all l ∈[K], and hence σ2
k + ∥w⋆−bwl∥2 ≤5B2
w.
We then have (yi −⟨xi, bwl⟩)2’s are (conditional) i.i.d random variables in SE(CB4
w). Then, by
Bernstein’s inequality, we have
PDval
bLval(bwl) −Lval,w⋆(bwl)
 ≥t

≤2 exp

−cNval min
 t2
B2w
,
t
Bw

,
where c is a universal constant. Applying the union bound, we obtain
PDval

max
l∈[K]
bLval(bwl) −Lval,w⋆(bwl)
 ≥t

≤K exp

−cNval min
 t2
B2w
,
t
Bw

.
Taking integration completes the proof.
J.4
Generalized linear models with adaptive link function selection
Suppose that (gk : R →R)k∈[K] is a set of link functions such that gk is non-decreasing and
C2-smooth for each k ∈[K]. We consider the input format we introduce in Section 4.1 with
|Dtrain| = ⌈N/2⌉, |Dval| = ⌊N/2⌋.
Theorem J.2 (GLMs with adaptive link function selection). For any fixed set of parameters defined
in Assumption B, as long as N ≥O (d), there exists a transformer θ with L ≤O (log(N)) layers,
input dimension D = Θ (dK) and maxℓ∈[L] M (ℓ) ≤e
O
 d3N

, such that the following holds.
For any k⋆∈[K] and any distribution P that is a generalized linear model of the link function gk⋆
and some parameter β, if Assumption B holds for each pair (P, gk), then
E(D,xN+1,yN+1)∼P

(byN+1 −yN+1)2
≤E(x,y)∼P

(gk⋆(⟨x, β⟩) −y)2
+ O
 
d
N +
r
log(K)
N
!
,
or equivalently, E(D,xN+1)∼P[(byN+1 −E[yN+1|xN+1])2] ≤O

d/N +
p
log(K)/N

.
Proof. For each k ∈[K], we consider optimizing the following training loss:
w(k)
GLM := arg min
w
bL(k)
train(w) :=
1
Ntrain
X
(xi,yi)∈Dtrain
ℓk(⟨xi, w⟩, yi),
where ℓk(t, y) := −yt+
R t
0 gk(s)ds is the convex (integral) loss associated with gk (as in Section 3.1).
Also, for each predictor f : Rd →R, we consider the squared validation loss bLval:
bLval(f) :=
1
2Nval
X
(xi,yi)∈Dval
(f(xi) −yi)2.
Fix a large universal constant C0. Let us set
α = µgµx/8,
β = 8LgKx,
Bx = C0Kx
p
d log(N),
By = C0Ky
p
log(N),
76

Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):
Ew =
n
∀k ∈[K], ∀w ∈B2(Bw), α ≤λmin(∇2bL(k)
train(w)) ≤λmax(∇2bL(k)
train(w)) ≤β,
o
,
Er =
n
∀k ∈[K],
w(k)
GLM

2 ≤Bw/2
o
,
Eb,train = {∀(xi, yi) ∈Dtrain, ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,val = {∀(xi, yi) ∈Dval, ∥xi∥2 ≤Bx, |yi| ≤By},
Eb,N+1 = {∥xN+1∥2 ≤Bx, |yN+1| ≤By}.
Similar to the proof of Theorem G.2 (Appendix G.2), we know the good event E := Ew ∩Er ∩
Eb,train ∩Eb,test ∩Eb,N+1 holds with high probability: P(Ec) ≤O
 N −10
.
Similar to the proof of Theorem I.2, we can show that there exists a transformer θ with prediction
byN+1 = g
ready(TFθ(H)) (clipped by By), such that (for any P) the following holds under E:
(a) For each k ∈[K], fk = Ak(Dtrain) is a predictor such that
fk(xi) −gk(⟨xi, w(k)
GLM⟩)
 ≤ε
for all i ∈[N + 1] (where ε is chosen as in Appendix G.2).
(b) byN+1 = clipBy( bf(xN+1)), where bf = ATF(D) is an aggregated predictor given by bf =
P
k λkfk, such that (λk) is a distribution supported on k ∈[K] such that bLval(fk) ≤
mink′∈[K] bLval(fk′) + γ.
Similar to the proof of Theorem G.2, for E0 := Ew ∩Er ∩Eb,train ∩Eb,test, we have
E(D,xN+1,yN+1)∼P(byN+1 −yN+1)2 ≤ED∼P
h
1{E0}Lval( bf)
i
+ O
 
B2
y
N 5
!
,
where we denote Lval(f) := E(x,y)∼P
h
1{∥x∥2 ≤Bx}(f(x) −y)2i
for each predictor f. By the
definition of bf, we then have (under E0)
Lval( bf) ≤Lval(fk⋆) + max
l
bLval(fl) −Lval(fl)
 + γ.
For the first term, repeating the argument in the proof of Theorem G.2 directly yields that for
Etrain := Ew ∩Er ∩Eb,train,
EDtrain∼P[1{Etrain}Lval(fk⋆)] ≤E(x,y)∼P(gk⋆(⟨x, β⟩) −y)2 + O (d/Ntrain) .
For the second term, similar to Lemma J.2, we can show that conditional on Dtrain such that Etrain
holds, it holds
EDval∼P|Dtrain

1{E0} max
l
bLval(fl) −Lval(fl)


≤O
 K2
y

·
 r
log K
Nval
+ log K
Nval
!
.
Combining these inequalities and suitably choosing γ complete the proof.
K
Analysis of pretraining
Thus far, we have established the existence of transformers for performing various ICL tasks with
good in-context statistical performance. We now analyze the sample complexity of pretraining these
transformers from a finite number of training ICL instances.
K.1
Generalization guarantee for pretraining
Setup
At pretraining time, each training ICL instance has form Z := (H, yN+1), where H :=
H(D, xN+1) ∈RD×(N+1) denote the input sequence formatted as in (3). We consider the square
loss between the in-context prediction and the ground truth label:
ℓicl(θ; Z) := 1
2

yN+1 −clipBy
 ready
|
{z
}
g
ready
(TFR
θ (H))
2
.
77

Above, clipBy(t) := max {min {t, By}, −By} is the standard clipping operator onto [−By, By],
and TFR
θ the transformer architecture as in Definition 3 with clipping operators after each layer: let
H(0) = clipR(H),
H(ℓ) = clipR

MLPθ(ℓ)
mlp

Attnθ(ℓ)
attn

H(ℓ−1)
for all ℓ∈[L],
clipR(H) := [Proj∥h∥2≤R(hi)]i.
The clipping operator is used to control the Lipschitz constant of TFθ with respect to θ, and we
typically choose a sufficiently large clipping radius R so that it does not modify the behavior of the
transformer on any input sequence of our concern.
We draw ICL instances Z := (H, yN+1) = (D, (xN+1, yN+1)) from a (meta-)distribution denoted as
π, which first sample an in-context data distribution P ∼π, then sample iid examples (xi, yi)N+1
i=1
iid
∼
P⊗(N+1) and form D = {(xi, yi)}i∈[N]. Our pretraining loss is the average ICL loss on n pretraining
instances Z(1:n) iid
∼π, and we consider the corresponding test ICL loss on a new test instance:
bLicl(θ) := 1
n
n
X
j=1
ℓicl(θ; Zj),
Licl(θ) := EP∼π,Z∼P⊗(N+1)[ℓicl(θ; Z)].
Our pretraining algorithm is to solve a standard constrained empirical risk minimization (ERM)
problem over transformers with L layers, M heads, and norm bound B (recall the definition of the
|||·||| norm in (2)):
bθ :=
arg min
θ∈ΘL,M,D′,B
bLicl(θ),
ΘL,M,D′,B :=

θ = (θ(1:L)
attn , θ(1:L)
mlp ) : max
ℓ∈[L] M (ℓ) ≤M, max
ℓ∈[L] D(ℓ) ≤D′, |||θ||| ≤B

.
(TF-ERM)
Generalization guarantee
By standard uniform concentration analysis via chaining arguments
(Proposition B.4; see also [87, Chapter 5] for similar arguments), we have the following excess loss
guarantee for (TF-ERM). The proof can be found in Appendix L.2.
Theorem K.1 (Generalization for pretraining). With probability at least 1 −ξ (over the pretraining
instances {Zj}j∈[n]), the solution bθ to (TF-ERM) satisfies
Licl(bθ) ≤
inf
θ∈ΘL,M,D′,B
Licl(θ) + O
 
B2
y
r
L2(MD2 + DD′)ι + log(1/ξ)
n
!
,
where ι = log(2 + max {B, R, By}) is a log factor.
K.2
Examples of pretraining for in-context regression problems
In Theorem K.1, the comparator infθ∈ΘL,M,D′,B Licl(θ) is simply the smallest expected ICL loss
for ICL instances drawn from π, among all transformers within the norm ball ΘL,M,D′,B. Using
our constructions in Section 3 & 4, we show that this comparator loss is small on various (meta-
)distribution π’s, by which we obtain end-to-end guarantees for pretraining transformers with small
ICL loss at test time. Here we showcase this argument on several representative regression problems.
Linear regression
For any in-context data distribution P, let w⋆
P := EP[xx⊤]−1EP[xy] denote
the best linear predictor for P. We show that with mild choices of L, M, B, the learned transformer
can perform in-context linear regression with near-optimal statistical power, in that on the sampled
P ∼π and ICL instance {(xi, yi)}i∈[N+1]
iid
∼P, it competes with the best linear predictor w⋆
P for
this particular P. The proof follows directly by on combining Corollary 5 with Theorem K.1, and
can be found in Appendix L.3.
Theorem K.2 (Pretraining transformers for in-context linear regression). Suppose P ∼π is almost
surely well-posed for in-context linear regression (Assumption A) with the canonical parameters.
Then, for N ≥e
O(d), with probability at least 1 −ξ (over the training instances Z(1:n)), the solution
78

bθ of (TF-ERM) with L = O(κ log(κN/σ)) layers, M = 3 heads, D′ = 0 (attention-only), and
B = O(
√
κd) achieves small excess ICL risk over w⋆
P:
Licl(bθ) −EP∼πE(x,y)∼P
1
2(y −⟨w⋆
P, x⟩)2

≤e
O
 r
κ2d2 + log(1/ξ)
n
+ dσ2
N
!
,
where e
O(·) only hides polylogarithmic factors in κ, N, 1/σ.
To our best knowledge, Theorem K.2 offers the first end-to-end result for pretraining a transformer
to perform in-context linear regression with explicit excess loss bounds. The e
O(
p
κ2d2/n) term
originates from the generalization of pretraining (Theorem K.1), where as the e
O(dσ2/N) term
agrees with the standard fast rate for the excess loss of linear regression [38]. Further, as long as
n ≥e
O(κ2N/σ2), the excess risk achieves the optimal rate e
O(dσ2/N) (up to log factors).
Additional examples
By similar arguments as in the proof of Theorem K.2, we can directly turn
most of our other expressivity results into results on the pretrained transformers. Here we present
three such additional examples (proofs in Appendix L.4-L.6). The first example is for the sparse
linear regression problem considered in Theorem 8.
Theorem K.3 (Pretraining transformers for in-context sparse linear regression). Suppose each P ∼π
is almost surely an instance of the sparse linear model specified in Theorem 8 with parameters B⋆
w
and σ. Suppose N ≥e
O(s log((d ∨N)/σ)) and let κ := B⋆
w/σ.
Then with probability at least 1 −ξ (over the training instances Z(1:n)), the solution bθ of (TF-ERM)
with L = e
O(κ2(1 + d/N)) layers, M = 2 heads, D′ = 2d, and B = e
O(poly(d, B⋆
w, σ)) achieves
small excess ICL risk:
Licl(bθ) −σ2 ≤e
O
 r
κ4d2(1 + d/N)2 + log(1/ξ)
n
+ σ2 s log d
N
!
,
where e
O(·) only hides polylogarithmic factors in d, N, 1/σ.
Our next example is for the problem of noisy linear regression with mixed noise levels considered
in Theorem 12 and Theorem J.1. There, the constructed transformer uses the post-ICL validation
mechanism to perform ridge regression with an adaptive regulariation strength depending on the
particular input sequence.
Theorem K.4 (Pretraining transformers for in-context noisy linear regression with algorithm selec-
tion). Suppose π is the data generating model (noisy linear model with mixed noise levels) considered
in Theorem J.1, with σmax ≤O(1). Let N ≥d/10.
Then, with probability at least 1 −ξ (over the training instances Z(1:n)), the solution bθ of (TF-
ERM) with input dimension D = Θ(dK), L = O(σ−2
min log(N/σmin)) layers, M = O(K) heads,
D′ = O(K2), and B = O(poly(K, σ−1
min, d, N)) achieves small excess ICL risk:
Licl(bθ) −BayesRiskπ ≤e
O


s
σ−4
minK3d2 + log(1/ξ)
n
+ σ2
max
σ2/3
min
log K
N
1/3

,
where e
O(·) only hides polylogarithmic factors in d, N, K, 1/σmin.
Our final example is for in-context logistic regression. For simplicity we consider the realizable case.
Theorem K.5 (Pretraining transformers for in-context logistic regression; square loss guarantee).
Suppose for P ∼π, P is almost surely a realizable logistic model (i.e. P = Plog
β with ∥β∥2 ≤B⋆
w as
in Corollary G.1). Suppose that B⋆
w = O (1) and N ≥O (d).
Then, with probability at least 1 −ξ (over the training instances Z(1:n)), the solution bθ of (TF-ERM)
with L = O(log(N)) layers, M = e
O
 d3N

heads, D′ = 0, and B = O(poly(d, N)) achieves
small excess ICL risk:
Licl(bθ) −EPlog
β ∼πE(x,y)∼Plog
β
1
2(y −σlog(⟨β, x⟩))2

≤e
O
 r
d5N + log(1/ξ)
n
+ d
N
!
,
where e
O(·) only hides polylogarithmic factors in d, N.
79

Remark on generality of transformer
All results above are established by the expressivity results
in Section 3 & 4 for transformers to implement various ICL procedures (such as least squares, Lasso,
GLM, and ridge regression with in-context algorithm selection), combined with the generalization
bound (Theorem K.1). However, the transformer itself was not specified to encode any actual structure
about the problem at hand in any result above, other than having sufficiently large number of layers,
number of heads, and weight norms, which illustrates the flexibility of the transformer architecture.
L
Proofs for Section K
L.1
Lipschitzness of transformers
For any p ∈[1, ∞], let ∥H∥2,p := (PN
i=1 ∥hi∥p
2)1/p denote the column-wise (2, p)-norm of H. For
any radius R > 0, we denote HR := {H : ∥H∥2,∞≤R} be the ball of radius R under norm ∥·∥2,∞.
Lemma L.1. For a single MLP layer θmlp = (W1, W2), we introduce its norm (as in (2))
θmlp
 = ∥W1∥op + ∥W2∥op .
For any fixed hidden dimension D′, we consider
Θmlp,B :=

θmlp :
θmlp
 ≤B
	
.
Then for H ∈HR, θmlp ∈Θmlp,B, the function (θmlp, H) 7→MLPθmlp(H) is (BR)-Lipschitz w.r.t.
θmlp and (1 + B2)-Lipschitz w.r.t. H.
Proof. Recall that by our definition, for the parameter θmlp = (W1, W2) ∈Θmlp,B and the input
H = [hi] ∈RD×N, the output MLPθmlp(H) = H + W2σ(W1H) = [hi + W2σ(W1hi)]i.
Therefore, for θ′
mlp = (W′
1, W′
2) ∈Θmlp,B, we have
MLPθmlp(H) −MLPθ′
mlp(H)

2,∞
= max
i
∥W2σ(W1hi) −W′
2σ(W′
1hi)∥2
= max
i
∥(W2 −W′
2)σ(W1hi) + W′
2(σ(W1hi) −σ(W′
1hi))∥2
≤max
i
∥W2 −W′
2∥op ∥σ(W1hi)∥2 + ∥W′
2∥op ∥σ(W1hi) −σ(W′
1hi)∥2
≤max
i
∥W2 −W′
2∥op ∥W1hi∥2 + ∥W′
2∥op ∥W1hi −W′
1hi∥2
≤BR ∥W2 −W′
2∥op + BR ∥W1 −W′
1∥op ,
where the second inequality follows from the 1-Lipschitznees of σ = [·]+. Similarly, for H′ = [h′
i] ∈
RD×N,
MLPθmlp(H) −MLPθmlp(H′)

2,∞= max
i
∥hi + W1σ(W2hi) −h′
i −W1σ(W2h′
i)∥2
≤∥H −H′∥2,∞+ max
i
∥W1(σ(W2hi) −σ(W2h′
i))∥2
≤∥H −H′∥2,∞+ max
i
B ∥σ(W2hi) −σ(W2h′
i)∥2
≤∥H −H′∥2,∞+ B2 ∥H −H′∥2,∞.
Lemma L.2. For a single attention layer θattn = {(Vm, Qm, Km)}m∈[M] ⊂RD×D, we introduce
its norm (as in (2))
|||θattn||| := max
m∈[M]
n
∥Qm∥op , ∥Km∥op
o
+
M
X
m=1
∥Vm∥op .
For any fixed dimension D, we consider
Θattn,B := {θattn : |||θattn||| ≤B}.
Then for H ∈HR, θattn ∈Θattn,B, the function (θattn, H) 7→Attnθattn(H) is (B2R3)-Lipschitz
w.r.t. θattn and (1 + B3R2)-Lipschitz w.r.t. H.
80

Proof. Recall that by our definition, for the parameter θattn = {(Vm, Qm, Km)}m∈[M] ∈Θattn,B
and the input H = [hi] ∈RD×N, the output Attnθattn(H) = [ehi] is given by
ehi = hi +
M
X
m=1
1
N
N
X
j=1
σ(⟨Qmhi, Kmhj⟩) · Vmhj.
Now, for θ′
attn = {(V′
m, Q′
m, K′
m)}m∈[M], we consider
eh′
i =

Attnθ′
attn(H)

i = hi +
M
X
m=1
1
N
N
X
j=1
σ(⟨Q′
mhi, K′
mhj⟩) · V′
mhj,
∀i ∈[N].
Clearly
Attnθattn(H) −Attnθ′
attn(H)

2,∞= maxi
ehi −eh′
i

2. For any i ∈[N], we have
ehi −eh′
i

2 =

M
X
m=1
1
N
N
X
j=1
[σ(⟨Qmhi, Kmhj⟩)Vmhj −σ(⟨Q′
mhi, K′
mhj⟩)V′
mhj]

2
≤
M
X
m=1
1
N
N
X
j=1
∥σ(⟨Qmhi, Kmhj⟩)Vm −σ(⟨Q′
mhi, K′
mhj⟩)V′
m∥op ∥hj∥2
≤
M
X
m=1
1
N
N
X
j=1
∥hj∥2
nσ(⟨Qmhi, Kmhj⟩)
 · ∥Vm −V′
m∥op
+
σ(⟨Qmhi, Kmhj⟩) −σ(⟨Q′
mhi, Kmhj⟩)
 · ∥V′
m∥op
+
σ(⟨Q′
mhi, Kmhj⟩) −σ(⟨Q′
mhi, K′
mhj⟩)
 · ∥V′
m∥op
o
≤
M
X
m=1
1
N
N
X
j=1
R
n
B2R2 · ∥Vm −V′
m∥op + ∥Qmhi −Q′
mhi∥2 · ∥Kmhj∥2 · ∥V′
m∥op
+ ∥Q′
mhi∥2 · ∥Kmhj −K′
mhj∥2 · ∥V′
m∥op
o
≤
M
X
m=1
R
n
B2R2 ∥Vm −V′
m∥op + BR2 ∥Qm −Q′
m∥op · ∥V′
m∥op + BR2 ∥Km −K′
m∥op · ∥V′
m∥op
o
≤B2R3n
M
X
m=1
∥Vm −V′
m∥op + max
m ∥Qm −Q′
m∥op + max
m ∥Km −K′
m∥op
o
= B2R3|||θattn −θ′
attn|||,
where the second inequality uses the definition of operator norm, the third inequality follows from
the triangle inequality, the forth inequality is because ∥Qmhi∥2 ≤BR, ∥Kmhj∥2 ≤BR, and σ is
1-Lipschitz. This completes the proof the Lipschitzness w.r.t. θattn.
Similarly, we consider H′ = [h′
i], and
eh′
i =

Attnθ′
attn(H)

i = h′
i +
M
X
m=1
1
N
N
X
j=1
σ
 
Qmh′
i, Kmh′
j

· Vmh′
j,
∀i ∈[N].
By definition, we can similarly bound


eh′
i −h′
i

−

ehi −hi

2
=

M
X
m=1
1
N
N
X
j=1

σ(⟨Qmhi, Kmhj⟩)Vmhj −σ
 
Qmh′
i, Kmh′
j

Vmh′
j


2
≤
M
X
m=1
1
N
N
X
j=1
∥Vm∥op
σ(⟨Qmhi, Kmhj⟩)hj −σ
 
Qmh′
i, Kmh′
j

h′
j

2
81

≤
M
X
m=1
1
N
N
X
j=1
∥Vm∥op
nσ(⟨Qmhi, Kmhj⟩)
 ·
hj −h′
j

2
+
σ(⟨Qmhi, Kmhj⟩) −σ(⟨Qmh′
i, Kmhj⟩)
 ·
h′
j

2
+
σ(⟨Qmh′
i, Kmhj⟩) −σ
 
Qmh′
i, Kmh′
j
 ·
h′
j

2
o
≤
M
X
m=1
1
N
N
X
j=1
∥Vm∥op · 3 ∥Qm∥op ∥Km∥op R2 hj −h′
j

2
≤R2 ∥H −H′∥2,∞· 3 max
m∈[M] ∥Qm∥op ∥Km∥op ·
M
X
m=1
∥Vm∥op
≤B3R2 ∥H −H′∥2,∞,
where the last inequality uses |||θattn||| ≤B and the AM-GM inequality. This completes the proof the
Lipschitzness w.r.t. H.
Corollary L.1. For a fixed number of heads M and hidden dimension D′, we consider
ΘTF,1,B =

θ = (θattn, θmlp) : M heads, hidden dimension D′, |||θ||| ≤B
	
.
Then for the function TFR given by
TFR : (θ, H) 7→clipR
 MLPθmlp(Attnθattn(H))

,
θ ∈ΘTF,1,B, H ∈HR
TFR is BΘ-Lipschitz w.r.t θ and LH-Lipschitz w.r.t. H, where BΘ := BR(1 + BR2 + B3R2) and
BH := (1 + B2)(1 + B2R3).
Proof. For any θ = (θattn, θmlp), H ∈HR, and θ′ = (θ′
attn, θ′
mlp), we have
∥TFθ(H) −TFθ′(H)∥2,∞≤
MLPθmlp(Attnθattn(H)) −MLPθmlp
 Attnθ′
attn(H)

2,∞
+
MLPθmlp
 Attnθ′
attn(H)

−MLPθ′
mlp
 Attnθ′
attn(H)

2,∞
≤(1 + B2)
Attnθattn(H) −Attnθ′
attn(H)

2,∞+ BR
θmlp −θ′
mlp

≤(1 + B2)B2R3|||θattn −θ′
attn||| + BR
θmlp −θ′
mlp

≤BΘ|||θ −θ′|||,
where the second inequality follows from Lemma L.2 and Lemma L.1 and the fact that
∥Attnθattn(H)∥2,∞≤R := R + B3R3 for all H ∈HR.
Furthermore, for H′ ∈HR, we have
∥TFθ(H) −TFθ(H′)∥2,∞≤(1 + B2) ∥Attnθattn(H) −Attnθattn(H′)∥2,∞
≤(1 + B2)(1 + B3R2) ∥H −H′∥2,∞,
which also follows from Lemma L.2 and Lemma L.1.
Proposition L.1 (Lipschitzness of transformers). For a fixed number of heads M and hidden
dimension D′, we consider
ΘTF,L,B =
n
θ = (θ(1:L)
attn , θ(1:L)
mlp ) : M (ℓ) = M, D(ℓ) = D′, |||θ||| ≤B
o
.
Then the function TFR is (LBL−1
H
BΘ)-Lipschitz w.r.t θ ∈ΘTF,L,B for any fixed H.
Proof. For θ = θ(1:L) ∈ΘTF,L,B, eθ = eθ(1:L) ∈ΘTF,L,B, we have
TFR
θ(H) −TFR
eθ(H)

2,∞
82

≤
L
X
ℓ=1
TFR
θ(ℓ+1:L)

TFR
θ(ℓ)

TFR
eθ(1:ℓ−1)(H)

−TFR
θ(ℓ+1:L)

TFR
eθ(ℓ)

TFR
eθ(1:ℓ−1)(H)

2,∞
≤
L
X
ℓ=1
BL−ℓ
Θ
TFR
θ(ℓ)

TFR
eθ(1:ℓ−1)(H)

−TFR
eθ(ℓ)

TFR
eθ(1:ℓ−1)(H)

2,∞
≤
L
X
ℓ=1
BL−ℓ
H
BΘ ·


θ(ℓ) −eθ(ℓ)

 ≤LBL−1
H
BΘ ·


θ −eθ


,
where the second inequality follows from Corollary L.1, and the last inequality is because BH ≥
1.
L.2
Proof of Theorem K.1
In this section, we prove a slightly more general result by considering the general ICL loss
ℓicl(θ; Z) := ℓ( g
ready(TFR
θ(H)), yN+1).
We assume that the loss function ℓsatisfies sup |ℓ| ≤B0
ℓand sup |∂1ℓ| ≤B1
ℓ. For the special case
ℓ(s, t) = 1
2(s −t)2, we can take B0
ℓ= 4B2
y, B1
ℓ= 2By.
We then consider
Xθ := 1
n
n
X
j=1
ℓicl(θ; Zj) −EZ[ℓicl(θ; Z)],
where Z(1:n) are i.i.d copies of Z ∼P, P ∼π. It remains to apply Proposition B.4 to the random
process {Xθ}. We verify the preconditions:
(a) By [87, Example 5.8], it holds that log N(δ; B|||·|||(r), |||·|||) ≤L(3MD2 + 2DD′) log(1 + 2r/δ),
where B|||·|||(r) is any ball of radius r under norm |||·|||.
(b) |ℓicl(θ; Z)| ≤B0
ℓand hence B0
ℓ-sub-Gaussian.
(c)
ℓicl(θ; Z) −ℓicl(eθ; Z)
 ≤B1
ℓ· (LBL−1
H
BΘ) ·


θ −eθ


, by Proposition L.1.
Therefore, we can apply the uniform concentration result in Proposition B.4 to obtain that, with
probability at least 1 −ξ,
sup
θ
|Xθ| ≤CB0
ℓ
r
L(MD2 + DD′)ι + log(1/ξ)
n
,
where ι = log(2 + B · LBL−1
H
BΘB1
ℓ/B0
ℓ) ≤20L log(2 + max{B, R, B1
ℓ/B0
ℓ}). Recalling that
Licl(bθ) ≤inf
θ Licl(θ) + 2 sup
θ
|Xθ|
completes the proof.
L.3
Proof of Theorem K.2
By Corollary 5, there exists a transformer TFθ such that for every P satisfying Assumption A with
canonical parameters (and thus in expectation over P ∼π) and every N ≥e
O(d), it outputs prediction
byN+1 = g
ready(TFθ(H)) such that
Licl(θ) = EP∼π,(D,xN+1,yN+1∼P)
1
2(byN+1 −yN+1)2

≤EP∼π[LP(w⋆
P)] + O
dσ2
N

,
where we recall that LP(w⋆
P) := 1
2E(x,y)∼P

(y −⟨w⋆
P, x⟩)2
. By inspecting the proof, the same
result holds if we change TFθ to the clipped version TFR
θ if we choose R2 = O(B2
x+B2
y+B2
w+1) =
O(d + κ), so that on the good event Ecov ∩Ew considered therein, all intermediate outputs within
83

TFθ has ∥·∥2,∞≤R and thus the clipping does not modify the transformer output on Ecov ∩Ew.
Further, recall by (33) that θ has size bounds
L ≤O

κ log Nκ
σ

,
max
ℓ∈[L] M (ℓ) ≤3,
|||θ||| ≤O(
√
κd).
We can thus apply Theorem K.1 to obtain that the solution bθ to (TF-ERM) with the above choice of
(L, M, B) and D′ = 0 (attention-only) satisfies the following with probability at least 1 −ξ:
Licl(bθ) ≤
inf
θ′∈ΘL,M,D′,B
Licl(θ′) + O
 r
L2MD2ι + log(1/ξ)
n
!
≤Licl(θ) + e
O
 r
L2MD2 + log(1/ξ)
n
!
≤e
O
 r
κ2d2 + log(1/ξ)
n
+ dσ2
N
!
.
Above, ι = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
L.4
Proof of Theorem K.3
We invoke Theorem 8 (using the construction in Theorem 7 with a different choice of L) with the
following parameters:
L = e
O
 (B⋆
w)2/σ2 × (1 + d/N)

= e
O
 κ2(1 + d/N)

,
M = Θ(1),
D′ = 2d,
Bx = e
O(
√
d),
By = e
O(B⋆
w + σ),
δ =

σ2
1
B2yN
2
,
|||θ||| ≤B = O
 R + (1 + λN)β−1
≤O

R + σ
p
log d

≤e
O(poly(d, B⋆
w, σ)),
where e
O(·) hides polylogarithmic factors in d, N, B⋆
w, κ.
Then, Theorem 8 shows that there exists a transformer θ with L layers, maxℓ∈[L] M (ℓ) ≤M heads,
D′ hidden dimension for the MLP layers, and |||θ||| ≤B such that, on almost surely every P ∼π, it
returns a prediction byN+1 such that, on the good event E0 considered therein (over D ∼P) which
satisfies P(E0) ≥1 −δ,
E(xN+1,yN+1)∼P
h
(byN+1 −yN+1)2i
≤σ2[1 + O(s log(d/δ)/N)].
By inspecting the proof, the same result holds if we change TFθ to the clipped version TFR
θ if we
choose R2 = O(B2
x + B2
y + (B⋆
w)2 + 1) = O(d + (B⋆
w)2 + σ2), so that on the good event E0
considered therein, all intermediate outputs within TFθ has ∥·∥2,∞≤R and thus the clipping does
not modify the transformer output on the good event. On the bad event Ec
0, using the same argument
as in the proof of Theorem 8, we have
ED,(xN+1,yN+1)∼P

1{Ec
0}(byN+1 −yN+1)2
≤
q
PD(Ec
0) ·
 8EyN+1∼P

B4
y + y4
N+1
1/2 ≤e
O
σ2
N

.
Combining the above two bounds and further taking expectation over P ∼π gives
Licl(θ) = EP∼π,(D,xN+1,yN+1)∼P
1
2(byN+1 −yN+1)2

≤σ2 + e
O
 σ2s log d/N

.
We can thus apply Theorem K.1 to obtain that the solution bθ to (TF-ERM) with the above choice of
(L, M, B, D′) satisfies the following with probability at least 1 −ξ:
Licl(bθ) ≤
inf
θ′∈ΘL,M,D′,B
Licl(θ′) + O
 r
L2(MD2 + DD′)ι + log(1/ξ)
n
!
≤Licl(θ) + e
O
 r
L2(MD2 + DD′) + log(1/ξ)
n
!
≤σ2 + e
O
 r
κ4d2(1 + d/N)2 + log(1/ξ)
n
+ σ2 s log d
N
!
.
Above, ι = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
84

L.5
Proof of Theorem K.4
We invoke Theorem 12 and Theorem J.1, which shows that (recalling the input dimension D =
Θ(Kd)) there exists a transformer θ with the following size bounds:
L ≤O
 σ−2
min log(N/σmin)

,
max
ℓ∈[L] M (ℓ) ≤M = O (K) ,
max
ℓ∈[L] D(ℓ) ≤D′ = O(K2),
|||θ||| ≤O (σmaxKd log(N)) ,
such that it outputs byN+1 that satisfies
Eπ
1
2(yN+1 −byN+1)2

≤BayesRiskπ + e
O
 
σ2
max
σ2/3
min
log K
N
1/3
!
.
By inspecting the proof, the same result holds if we change TFθ to the clipped version TFR
θ if we
choose R2 = O(B2
x + B2
y + (B⋆
w)2 + 1) = O(d + σ2
max), so that on the good event considered
therein, all intermediate outputs within TFθ has ∥·∥2,∞≤R and thus the clipping does not modify
the transformer output on the good event. Using this clipping radius, we obtain
Licl(θ) = EP∼π,(D,xN+1,yN+1)∼P
1
2(byN+1 −yN+1)2

≤BayesRiskπ + e
O
 
σ2
max
σ2/3
min
log K
N
1/3
!
.
We can thus apply Theorem K.1 to obtain that the solution bθ to (TF-ERM) with the above choice of
(L, M, B, D′) satisfies the following with probability at least 1 −ξ:
Licl(bθ) ≤
inf
θ′∈ΘL,M,D′,B
Licl(θ′) + O
 r
L2(MD2 + DD′)ι + log(1/ξ)
n
!
≤Licl(θ) + e
O
 r
L2(MD2 + DD′) + log(1/ξ)
n
!
≤BayesRiskπ + e
O


s
σ−4
minK3d2 + log(1/ξ)
n
+ σ2
max
σ2/3
min
log K
N
1/3

.
Above, ι = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
L.6
Proof of Theorem K.5
The proof follows from similar arguments as of Theorem K.3 and Theorem K.4, where we plug in the
size bounds (number of layers, heads, and weight norms) from Theorem G.2 and Corollary G.1.
M
Experimental details and additional studies
M.1
Additional details for Section 6
Architecture and optimization
We train a 12-layer encoder-only transformer, where each layer
consists of an attention layer as in Definition 1 with M = 8 heads, hidden dimension D = 64, and
ReLU activation (normalized by the sequence length), as well as an MLP layer as in Definition 2
hidden dimension D′ = 64. We add Layer Normalization [3] after each attention and MLP layer
to help optimization, as in standard implementations [84]. We append linear read-in layer and
linear read-out layer before and after the transformer respectively, both applying a same affine
transform to all tokens in the sequence and are trainable. The read-in layer maps any input vector
to a D-dimensional hidden state, and the read-out layer maps a D-dimensional hidden state to a
1-dimensional scalar.
Each training sequence corresponds to a single ICL instance with N in-context training examples
{(xi, yi)}N
i=1 ⊂Rd × R and test input xN+1 ∈Rd. The input to the transformer is formatted as
85

in (3) where each token has dimension d + 1 (no zero-paddings). The transformer is trained by
minimizing the following loss with fresh mini-batches:
L(θ) = EP∼π,(H,yN+1)∼P[ℓP(ready(TFθ(H)), yN+1)],
(47)
where the loss function ℓP : R2 →R may depend on the training data distribution P in general;
we use the square loss when P is regression data, and the logistic loss when P is classification data.
We use the Adam optimizer with a fixed learning rate 10−4, which we find works well for all our
experiments. Throughout all our experiments except for the sparse linear regression experiment
in Figure 3a, we train the model for 300K steps, where each step consists of a (fresh) minibatch with
batch size 64 in the base mode, and K minibatches each with batch size 64 in the mixture mode.
For the sparse linear regression experiment, we find that minimizing the training objective (47) alone
was not enough, e.g. for the learned transformer to achieve better loss than the least squares algorithm
(which achieves much higher test loss than the Lasso; cf. Figure 3a). To help optimization, we
augment (47) with another loss that encourages the second-to-last hidden states to recover the true
(sparse) coefficient w⋆:
Lfit-w(θ) = 1
N0
N0
X
j=1
EP=Pw⋆∼π,(H,yN+1)∼P
"
h
TF(1:L−1)
θ
(H)
i
j,(D−d+1):D −w⋆

2
2
#
.
(48)
Specifically, the above loss encourages the first N0 ≤N tokens within the second-to-last layer to
be close to w⋆. We choose N0 = 5 (recall that the total number of tokens is N = 10 and sequence
length is N + 1 = 11 for this experiment). We minimize the loss L(θ) + λLfit-w(θ) with λ = 0.1
for 2M steps for this task.
Evaluation
All evaluations are done on the trained transformer with 6400 test instances. We use
the square loss for regression tasks, and the classification error (1−accuracy) between the true label
yN+1 ∈{0, 1} and the predicted label 1{byN+1 ≥1/2}. We report the means in all experiments,
as well as their standard deviations (using one-std error bars) in Figure 2a, 2b, 5a, 5b. In Figure
2c, 3b, 3c 5c, all standard deviations are sufficiently small (not significantly exceeding the width of
the markers), thus we did not show error bars in those plots.
Baseline algorithms
We implement various baseline machine learning algorithms to compare with
the learned transformers. A superset of the algorithms is shown in Figure 3a:
• Least squares, Logistic regression: Standard algorithms for linear regression and linear
classification, respectively. Note that least squares is also a valid algorithm for classification.
• Averaging: The simple algorithm which computes the linear predictor bw = 1
N
PN
i=1 yixi and
predicts byN+1 = ⟨bw, xN+1⟩;
• 3-NN: 3-Nearest Neighbors.
• Ridge: Standard ridge regression as in (ICRidge). We specifically consider two λ’s (denoted
as lam_1 and lam_2): λ1, λ2 = (0.005, 0.125). These are the Bayes-optimal regularization
strengths for the noise levels (σ1, σ2) = (0.1, 0.5) respectively under the noisy linear model
(cf. Corollary 6), using the formula λ⋆= dσ2/N, with (d, N) = (20, 40).
• Lasso: Standard Lasso as in (ICLasso) with λ ∈{1, 0.1, 0.01, 0.001}.
In Figure 2c, the ridge_analytical curve plots the expected risk of ridge regression under the
noisy linear model over 20 geometrically spaced values of λ’s in between (λ1, λ2), using analytical
formulae (with Monte Carlo simulations). The Bayes_err_{1,2} indicate the expected risks of λ1
on task 1 (with noise σ1) and λ2 on task 2 (with noise σ2), respectively.
M.2
Decoder-based architecture
ICL capabilities have also been demonstrated in the literature for decoder-based architectures [31,
2, 47]. There, the transformer can do in-context predictions at every token xi using past tokens
{(xj, yj)}j≤i−1 as training examples. Here we show that such architectures is also able to perform
in-context algorithm selection at every token; For results for this architecture on “base” ICL tasks
(such as those considered in Figure 3a), we refer the readers to Garg et al. [31].
86

(a) Linear regression
0
10
20
30
40
in-context examples
0.0
0.2
0.4
0.6
0.8
1.0
square loss
TF_alg_select
TF_reg
TF_cls
Least Squares
(b) Linear classification
0
10
20
30
40
in-context examples
0.15
0.20
0.25
0.30
0.35
0.40
error
TF_alg_select
TF_reg
TF_cls
Logistic Regression
(c) Reg vs. cls at token 40
0
1
2
3
4
regression_square_loss
0.20
0.25
0.30
0.35
classification_error
TF_alg_select
TF_reg
TF_cls
Least Squares
Averaging
3-NN
Figure 5: In-context algorithm selection abilities of transformers between linear regression and linear clas-
sification. (a,b) On these two tasks, a single transformer TF_alg_select simultaneously approaches
the performance of the strongest baseline algorithm Least Squares on linear regression and Logistic
Regression on linear classification. (c) At token 40 (using example {0, . . . , 39} for training), TF_alg_select
matches the performance of the best baseline algorithm for both tasks. (a,b,c) Note that transformers pretrained
on a single task (TF_reg, TF_cls) perform near-optimally on their pretraining task but suboptimally on the
other task.
Setup
Our setup is the same as the two “mixture” modes (linear model + linear classification
model, and noisy linear models with two different noise levels) as in Section 6, except that the
architecture is GPT-2 following Garg et al. [31], and the input format is changed to (11) (so that the
input sequence has 2N + 1 tokens) without positional encodings. For every i ∈[N + 1], we extract
the prediction byi using a linear read-out function applied on output token 2i −1, and the (learnable)
linear read-out function is the same across all tokens, similar as in Appendix M.1. The rest of the
setup (optimization, training, and evaluation) is the same as in Section 6 & M.1. Note that we also
train on the objective (47) for all tokens averaged, instead of for the last test token as in Section 6.
Result
Figure 2 shows the results for noisy linear models with two different noise levels, and Figure
5 shows the results for linear model + linear classification model. We observe that at every token,
In both cases, TF_alg_select nearly matches the strongest baseline for both tasks simultaneously,
whereas transformers trained on a single task perform suboptimally on the other task. Further, this
phenomenon consistently shows up at every token. For example, in Figure 2a & 2b, TF_alg_select
matches ridge regression with the optimal λ on all tokens i ∈{1, . . . , N} (N = 40). In Figure
5a & 5b, TF_alg_select matches least squares on the regression task and logistic regression on
the classification task on all tokens i ∈[N]. This demonstrates the in-context algorithm selection
capabilities of standard decoder-based transformer architectures.
M.3
Computational resource
All our experiments are performed on 8 Nvidia Tesla A100 GPUs (40GB memory). The total GPU
time is approximately 5 days (on 8 GPUs), with the largest individual training run taking about a
single day on a single GPU.
87

