Transformers as Statisticians: Provable In-Context
Learning with In-Context Algorithm Selection
Yu Baiâˆ—
Salesforce Research
yu.bai@salesforce.com
Fan Chenâˆ—
Massachusetts Institute of Technology
fanchen@mit.edu
Huan Wang
Salesforce Research
huan.wang@salesforce.com
Caiming Xiong
Salesforce Research
cxiong@salesforce.com
Song Meiâˆ—
UC Berkeley
songmei@berkeley.edu
Abstract
Neural sequence models based on the transformer architecture have demonstrated
remarkable in-context learning (ICL) abilities, where they can perform new tasks
when prompted with training and test examples, without any parameter update to the
model. This work first provides a comprehensive statistical theory for transformers
to perform ICL. Concretely, we show that transformers can implement a broad
class of standard machine learning algorithms in context, such as least squares,
ridge regression, Lasso, learning generalized linear models, and gradient descent
on two-layer neural networks, with near-optimal predictive power on various in-
context data distributions. Using an efficient implementation of in-context gradient
descent as the underlying mechanism, our transformer constructions admit mild
size bounds, and can be learned with polynomially many pretraining sequences.
Building on these â€œbaseâ€ ICL algorithms, intriguingly, we show that transformers
can implement more complex ICL procedures involving in-context algorithm se-
lection, akin to what a statistician can do in real lifeâ€”A single transformer can
adaptively select different base ICL algorithmsâ€”or even perform qualitatively
different tasksâ€”on different input sequences, without any explicit prompting of the
right algorithm or task. We both establish this in theory by explicit constructions,
and also observe this phenomenon experimentally. In theory, we construct two gen-
eral mechanisms for algorithm selection with concrete examples: pre-ICL testing,
and post-ICL validation. As an example, we use the post-ICL validation mecha-
nism to construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging taskâ€”noisy linear models with mixed noise levels. Experimentally,
we demonstrate the strong in-context algorithm selection capabilities of standard
transformer architectures.
1
Introduction
Large neural sequence models have demonstrated remarkable in-context learning (ICL) capabili-
ties [12], where models can make accurate predictions on new tasks when prompted with training
examples from the same task, in a zero-shot fashion without any parameter update to the model. A
prevalent example is large language models based on the transformer architecture [84], which can
perform a diverse range of tasks in context when trained on enormous text [12, 90]. Recent models
âˆ—Equal technical and directional contributions.
Code is available at https://github.com/allenbai01/transformers-as-statisticians.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Data
Which distribution?
dist 1
dist K
Ì‚yN+1 = ğ– ğ—…ğ—€kâ‹†(D, xN+1)
TF
ğ– ğ—…ğ—€1
ğ– ğ—…ğ—€K
â€¦â€¦
Mechanism 2: Pre-ICL Testing
Data
ğ– ğ—…ğ—€1(Dtrain)
ğ– ğ—…ğ—€K(Dtrain)
â€¦â€¦
TF
ğ–«ğ—ˆğ—Œğ—Œ1(Dval)
ğ–«ğ—ˆğ—Œğ—ŒK(Dval)
â€¦â€¦
Which loss is the smallest?
Ì‚yN+1 = ğ– ğ—…ğ—€kâ‹†(Dtrain, xN+1)
Train-validation split
Mechanism 1: Post-ICL Validation
Data 1â€¨
(Regression)
Transformer
ğ–«ğ—‚ğ—‡ğ–±ğ–¾ğ—€(D, xN+1)
ğ–«ğ—ˆğ—€ğ–±ğ–¾ğ—€(D, xN+1)
Data 2â€¨
(Classification)
Ì‚yN+1 :
Example 2: Regression + Classification
Data 1â€¨
(Reg w/noise 
)
Ïƒ1
Transformer
ğ–±ğ—‚ğ–½ğ—€ğ–¾Î»1(D, xN+1)
ğ–±ğ—‚ğ–½ğ—€ğ–¾Î»2(D, xN+1)
Data 2â€¨
(Reg w/noise 
)
Ïƒ2
Ì‚yN+1 :
Example 1: Ridge with different Î»
Figure 1: Illustration of in-context algorithm selection, and two mechanisms constructed in our theory.
Left, middle-left: A single transformer can perform ridge regression with different Î»â€™s on input sequences
with different observation noise; we prove this by the post-ICL validation mechanism (Section 4.1). Middle-
right, right: A single transformer can perform linear regression on regression data and logistic regression on
classification data; we prove this via the pre-ICL testing mechanism (Section 4.2).
in this paradigm such as GPT-4 achieve surprisingly impressive ICL performance that makes them
akin to a general-purpose agent in many aspects [65, 14]. Such strong capabilities call for better
understandings, which a recent line of work tackles from various aspects [49, 94, 28, 72, 15, 57, 64].
Recent pioneering work of Garg et al. [31] proposes an interpretable and theoretically amenable
setting for understanding ICL in transformers. They perform ICL experiments where input tokens
are real-valued (input, label) pairs generated from standard statistical models such as linear models
(and the sparse version), neural networks, and decision trees. Garg et al. [31] find that transformers
can learn to perform ICL with prediction power (and fitted functions) matching standard machine
learning algorithms for these settings, such as least squares for linear models, and Lasso for sparse
linear models. Subsequent work further studies the internal mechanisms [2, 86, 18], expressive
power [2, 32], and generalization [47] of transformers in this setting. However, these works only
showcase simple mechanisms such as regularized regression [31, 2, 47] or gradient descent [2, 86, 18],
which are arguably only a small subset of what transformers are capable of in practice; or expressing
universal function classes not specific to ICL [89, 32]. This motivates the following question:
How do transformers learn in context beyond implementing simple algorithms?
This paper makes steps on this question by making two main contributions: (1) We unveil a general
mechanismâ€”in-context algorithm selectionâ€”by which a single transformer can adaptively select
different â€œbaseâ€ ICL algorithms to use on different ICL instances, without any explicit prompting of
the right algorithm to use in the input sequence. For example, a transformer may choose to perform
ridge regression with regularization Î»1 on ICL instance 1, and Î»2 on ICL instance 2 (Figure 2); or
perform regression on ICL instance 1 and classification on ICL instance 2 (Figure 5). This adaptivity
allows transformers to achieve much stronger ICL performance than the base ICL algorithms. We
both prove this in theory, and demonstrate this phenomenon empirically on standard transformer
architectures. (2) Along the way, equally importantly, we present a comprehensive theory for ICL in
transformers by establishing end-to-end quantitative guarantees for the expressive power, in-context
prediction performance, and sample complexity of pretraining. These results add upon the recent
line of work on the statistical learning theory of transformers [97, 89, 27, 39], and lay out a foundation
for the intriguing special case where the learning targets are themselves ICL algorithms.
A detailed summary of our contributions is as follows.
â€¢ We prove that transformers can implement a broad class of standard machine learning algorithms
in context, such as least squares, ridge regression, Lasso, convex risk minimization for learning
generalized linear models (such as logistic regression), and gradient descent for two-layer neural
networks (Section 3). Our constructions admit mild bounds on the number of layers, heads, and
weight norms, and achieve near-optimal prediction power on many in-context data distributions.
â€¢ Technically, the above transformer constructions build on a new efficient implementation of in-
context gradient descent (Appendix D), which could be broaderly applicable. For a broad class
of smooth convex empirical risks over the in-context training data, we construct an (L+1)-layer
transformer that approximates L steps of gradient descent. Notably, the approximation error
accumulates only linearly in L, utilizing a stability-like property of smooth convex optimization.
â€¢ We prove that transformers can perform in-context algorithm selection (Section 4). We construct
two algorithm selection mechanisms: Post-ICL validation (Section 4.1), and Pre-ICL testing
2

(a) Noisy linear reg with noise Ïƒ1
0
10
20
30
40
in-context examples
0.0
0.2
0.4
0.6
0.8
1.0
square loss
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
(b) Noisy linear reg with noise Ïƒ2
0
10
20
30
40
in-context examples
0.4
0.6
0.8
1.0
1.2
1.4
 
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
(c) Task 1 vs. task 2 at token 20
0.10
0.15
0.20
0.25
0.30
noisy_reg_noise_1
0.6
0.8
1.0
1.2
1.4
noisy_reg_noise_2
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
ridge analytical
Bayes_err_noise_1
Bayes_err_noise_2
Figure 2: In-context algorithm selection on two separate noisy linear regression tasks with noise (Ïƒ1, Ïƒ2) =
(0.1, 0.5). (a,b) A single transformer TF_alg_select simultaneously approaches the performance of the
two individual Bayes predictors ridge_lam_1 on task 1 and ridge_lam_2 on task 2. (c) At token 20 (using
example {0, . . . , 19} for training), TF_alg_select approaches the Bayes error on two tasks simultaneously,
and outperforms ridge regression with any fixed Î». (a,b,c) Note that transformers pretrained on a single
task (TF_noise_1, TF_noise_2) perform near-optimally on that task but suboptimally on the other task. More
details about the setup and training method can be found in Appendix M.2.
(Section 4.2). For both mechanisms, we provide general constructions as well as concrete
examples. Figure 1 provides a pictorial illustration of the two mechanisms.
â€¢ As a concrete application, using the post-ICL validation mechanism, we construct a transformer
that can perform nearly Bayes-optimal ICL on noisy linear models with mixed noise levels
(Section 4.1.1), a more complex task than those considered in existing work.
â€¢ We provide the first line of results for pretraining transformers to perform the various ICL tasks
above, from polynomially many training sequences (Section 5 & Appendix K).
â€¢ Experimentally, we find that learned transformers indeed exhibit strong in-context algorithm
selection capabilities in the settings considered in our theory (Section 6). For example, Figure 2
shows that a single transformer can approach the individual Bayes risks (the optimal risk among
all possible algorithms) simultaneously on two noisy linear models with different noise levels.
Transformers as statisticians
We humbly remark that the typical toolkit of a statistician contains
much more beyond those covered in this work, including and not limited to inference, uncertainty
quantification, and theoretical analysis. This work merely aims to show the algorithm selection
capability of transformers, akin to what a statistician can do.
Related work
Our work is intimately related to the lines of work on in-context learning, theoretical
understandings of transformers, as well as other formulations for learning-to-learn such as meta-
learning. Due to limited space, we discuss these related work in Appendix A.
2
Preliminaries
We consider a sequence of N input vectors {hi}N
i=1 âŠ‚RD, written compactly as an input matrix
H = [h1, . . . , hN] âˆˆRDÃ—N, where each hi is a column of H (also a token). Throughout this paper,
we let Ïƒ(t) := ReLU(t) = max {t, 0} denote the standard relu activation.
2.1
Transformers
We consider transformer architectures that process any input sequence H âˆˆRDÃ—N by applying
(encoder-mode2) attention layers and MLP layers formally defined as follows.
Definition 1 (Attention layer). A (self-)attention layer with M heads is denoted as AttnÎ¸(Â·) with
parameters Î¸ = {(Vm, Qm, Km)}mâˆˆ[M] âŠ‚RDÃ—D. On any input sequence H âˆˆRDÃ—N,
eH = AttnÎ¸(H) := H + 1
N
PM
m=1(VmH) Ã— Ïƒ
 (QmH)âŠ¤(KmH)

âˆˆRDÃ—N,
(1)
where Ïƒ : R â†’R is the ReLU function. In vector form,
ehi = [AttnÎ¸(H)]i = hi + PM
m=1
1
N
PN
j=1 Ïƒ(âŸ¨Qmhi, KmhjâŸ©) Â· Vmhj.
2Many of our results can be generalized to decoder-based architectures; see Appendix C for a discussion.
3

Above, (1) uses a normalized ReLU3 activation t 7â†’Ïƒ(t)/N in place of the standard softmax
activation; we remark this activation is also found to work well empirically in recent studies [78, 93].
Definition 2 (MLP layer). A (token-wise) MLP layer with hidden dimension Dâ€² is denoted as
MLPÎ¸(Â·) with parameters Î¸ = (W1, W2) âˆˆRDâ€²Ã—DÃ—RDÃ—Dâ€². On any input sequence H âˆˆRDÃ—N,
eH = MLPÎ¸(H) := H + W2Ïƒ(W1H),
where Ïƒ : R â†’R is the ReLU function. In vector form, we have ehi = hi + W2Ïƒ(W1hi).
We consider a transformer architecture with L â‰¥1 transformer layers, each consisting of a self-
attention layer followed by an MLP layer.
Definition 3 (Transformer). An L-layer transformer, denoted as TFÎ¸(Â·), is a composition of L
self-attention layers each followed by an MLP layer: H(L) = TFÎ¸(H(0)), where H(0) âˆˆRDÃ—N is
the input sequence, and
H(â„“) = MLPÎ¸(â„“)
mlp

AttnÎ¸(â„“)
attn
 H(â„“âˆ’1)
,
â„“âˆˆ{1, . . . , L}.
Above,
the parameter Î¸
=
(Î¸(1:L)
attn , Î¸(1:L)
mlp ) consists of the attention layers Î¸(â„“)
attn
=
{(V(â„“)
m , Q(â„“)
m , K(â„“)
m )}mâˆˆ[M (â„“)] âŠ‚RDÃ—D and the MLP layers Î¸(â„“)
mlp = (W(â„“)
1 , W(â„“)
2 ) âˆˆRD(â„“)Ã—D Ã—
RDÃ—D(â„“). We will frequently consider â€œattention-onlyâ€ transformers with W(â„“)
1 , W(â„“)
2
= 0, which
we denote as TF0
Î¸(Â·) for shorthand, with Î¸ = Î¸(1:L) := Î¸(1:L)
attn .
We additionally define the following norm of a transformer TFÎ¸:
|||Î¸||| := max
â„“âˆˆ[L]
n
max
mâˆˆ[M]
n
âˆ¥Q(â„“)
m âˆ¥op, âˆ¥K(â„“)
m âˆ¥op
o
+
M
X
m=1
âˆ¥V(â„“)
m âˆ¥op + âˆ¥W(â„“)
1 âˆ¥op + âˆ¥W(â„“)
2 âˆ¥op
o
.
(2)
In (2), the choices of the operator norm and max/sums are for convenience only and not essential, as
our results (e.g. for pretraining) depend only logarithmically on |||Î¸|||.
2.2
In-context learning
In an in-context learning (ICL) instance, the model is given a dataset D = {(xi, yi)}iâˆˆ[N]
iid
âˆ¼P and a
new test input xN+1 âˆ¼Px for some data distribution P, where {xi}iâˆˆ[N] âŠ†Rd are the input vectors,
{yi}iâˆˆ[N] âŠ†R are the corresponding labels (e.g. real-valued for regression, or {0, 1}-valued for
binary classification), and xN+1 is the test input on which the model is required to make a prediction.
Different from standard supervised learning, in ICL, each instance (D, xN+1) is in general drawn
from a different distribution Pj, such as a linear model with a new ground truth coefficient wâ‹†,j âˆˆRd.
Our goal is to construct fixed transformer to perform ICL on a large set of Pjâ€™s.
We consider using transformers to perform ICL, in which we encode (D, xN+1) into an input se-
quence H âˆˆRDÃ—(N+1). In our theory, we use the following format, where the first two rows contain
(D, xN+1) (zero at the location for yN+1), and the third row contains fixed vectors {pi}iâˆˆ[N+1] with
ones, zeros, and indicator for being the train token (similar to a positional encoding vector):
H =
"x1
x2
. . .
xN
xN+1
y1
y2
. . .
yN
0
p1
p2
. . .
pN
pN+1
#
âˆˆRDÃ—(N+1),
pi :=
"
0Dâˆ’(d+3)
1
1{i < N + 1}
#
âˆˆRDâˆ’(d+1).
(3)
We will choose D = Î˜(d), so that the hidden dimension of H is at most a constant multiple of d. We
then feed H into a transformer to obtain the output eH = TFÎ¸(H) âˆˆRDÃ—(N+1) with the same shape,
and read out the prediction byN+1 from the (d + 1, N + 1)-th entry of eH = [ehi]iâˆˆ[N+1] (the entry
corresponding to the missing test label): byN+1 = ready( eH) := (ehN+1)d+1. The goal is to predict
3For each query index i, the attention weights {Ïƒ(âŸ¨Qmhi, KmhjâŸ©)/N}jâˆˆ[N] is also a set of non-negative
weights that sum to O(1) (similar as a softmax probability distribution) in typical scenarios. Also, our approxi-
mation results can potentially be generalized to softmax attention e.g. using the technique of [32].
4

byN+1 that is close to yN+1 âˆ¼Py|xN+1 measured by proper losses. We emphasize that we consider
predicting only at the last token xN+1, which is without much loss of generality.4
Miscellaneous setups
We assume bounded features and labels throughout the paper (unless oth-
erwise specified, e.g. when xi is Gaussian): âˆ¥xiâˆ¥2 â‰¤Bx and |yi| â‰¤By with probability one. We
use the standard notation X = [xâŠ¤
1 ; . . . ; xâŠ¤
N] âˆˆRNÃ—d and y = [y1; . . . ; yN] âˆˆRN to denote the
matrix of inputs and vector of labels, respectively. To prevent the transformer from blowing up on
tail events, in all our results concerning (statistical) in-context prediction powers, we consider a
clipped prediction byN+1 = g
ready( eH) := clipR((ehN+1)d+1), where clipR(t) := Proj[âˆ’R,R](t) is
the standard clipping operator with (a suitably large) radius R â‰¥0 that varies in different problems.
3
Basic in-context learning algorithms
We begin by constructing transformers that approximately implement a variety of standard machine
learning algorithms in context, with mild size bounds and near-optimal prediction power on many
standard in-context data distributions.
3.1
In-context ridge regression and least squares
Consider the standard ridge regression estimator over the in-context training examples D with
regularization Î» â‰¥0 (reducing to least squares at Î» = 0 and N â‰¥d):
wÎ»
ridge := arg minwâˆˆRd
1
2N
PN
i=1 (âŸ¨w, xiâŸ©âˆ’yi)2 + Î»
2 âˆ¥wâˆ¥2
2 .
(ICRidge)
We show that transformers can approximately implement (ICRidge) (proof in Appendix F.1).
Theorem 4 (Implementing in-context ridge regression). For any Î» â‰¥0, 0 â‰¤Î± â‰¤Î² with Îº := Î²+Î»
Î±+Î»,
Bw > 0, and Îµ < BxBw/2, there exists an L-layer attention-only transformer TF0
Î¸ with
L = âŒˆ2Îº log(BxBw/(2Îµ))âŒ‰+ 1,
maxâ„“âˆˆ[L] M (â„“) â‰¤3,
|||Î¸||| â‰¤4R + 8(Î² + Î»)âˆ’1.
(4)
(with R := max {BxBw, By, 1}) such that the following holds. On any input data (D, xN+1) such
that the problem (ICRidge) is well-conditioned and has a bounded solution:
Î± â‰¤Î»min(XâŠ¤X/N) â‰¤Î»max(XâŠ¤X/N) â‰¤Î²,
wÎ»
ridge

2 â‰¤Bw/2,
(5)
TF0
Î¸ approximately implements (ICRidge): The prediction byN+1 = ready(TF0
Î¸(H)) satisfies
byN+1 âˆ’

wÎ»
ridge, xN+1
 â‰¤Îµ.
(6)
Theorem 4 presents the first quantitative construction for end-to-end in-context ridge regression up
to arbitrary precision, and improves upon AkyÃ¼rek et al. [2] whose construction does not give (or
directly imply) an explicit error bound like (6). Further, the bounds on the number of layers and
heads in (4) are mild (constant heads and logarithmically many layers).
Near-optimal in-context prediction power for linear problems
Combining Theorem 4 with
standard analyses of linear regression yields the following corollaries (proofs in Appendix F.3 & F.4).
Corollary 5 (Near-optimal linear regression with transformers by approximating least squares). For
any N â‰¥e
O(d), there exists an O(Îº log(ÎºN/Ïƒ))-layer transformer Î¸, such that on any P satisfying
standard statistical assumptions for least squares (Assumption A), its ICL prediction byN+1 achieves
E(D,xN+1,yN+1)âˆ¼P[(byN+1 âˆ’yN+1)2] â‰¤infw E(x,y)âˆ¼P

(y âˆ’âŸ¨w, xâŸ©)2
+ e
O(dÏƒ2/N).
Assumption A requires only generic tail properties such as sub-Gaussianity, and not realizability
(i.e., P follows a true linear model); Îº, Ïƒ above denote the covariance condition number and the
4Our constructions may be generalized to predicting at every token, by using a decoder architecture and
potentially different input formats correspondingly (cf. Appendix C). Our theory focuses on predicting at the last
token only, which simplifies the setting. Our experiments test both settings.
5

noise level therein. The e
O(dÏƒ2/N) excess risk is known to be rate-optimal for linear regression [38],
and Corollary 5 achieves this in context with a transformer with only logarithmically many layers.
Next, consider Bayesian linear models where each in-context data distribution P = Plin
wâ‹†is drawn
from a Gaussian prior Ï€ : wâ‹†âˆ¼N(0, Id/d), and (x, y) âˆ¼Plin
wâ‹†is sampled as x âˆ¼N(0, Id),
y = âŸ¨wâ‹†, xâŸ©+ N(0, Ïƒ2). It is a standard result that the Bayes estimator of yN+1 given (D, xN+1)
is given by ridge regression (ICRidge): byBayes
N+1 := âŸ¨wÎ»
ridge, xN+1âŸ©with Î» = dÏƒ2/N. We show that
transformers achieve nearly-Bayes risk for this problem, and we use
BayesRiskÏ€ := Ewâ‹†âˆ¼Ï€,(D,xN+1,yN+1)âˆ¼Plin
wâ‹†
h
1
2
 byBayes
N+1 âˆ’yN+1
2i
to denote the Bayes risk of this problem under prior Ï€.
Corollary 6 (Nearly-Bayes linear regression with transformers by approximating ridge regression).
Under the Bayesian linear model above with N â‰¥max {d/10, O (log(1/Îµ))}, there exists a L =
O (log(1/Îµ))-layer transformer such that Ewâ‹†,(D,xN+1,yN+1)
 1
2(byN+1âˆ’yN+1)2
â‰¤BayesRiskÏ€ +Îµ.
Generalized linear models
In Appendix G, we extend the above results to generalized linear
models [53] and show that transformers can approximate the corresponding convex risk minimization
algorithm in context (which includes logistic regression for linear classification as an important
special case), and achieve near-optimal excess risk under standard statistical assumptions.
3.2
In-context Lasso
Consider the standard Lasso estimator [82] which minimizes an â„“1-regularized linear regression loss
bLlasso over the in-context training examples D:
wlasso := arg minwâˆˆRd bLlasso(w) =
1
2N
PN
i=1 (âŸ¨w, xiâŸ©âˆ’yi)2 + Î»N âˆ¥wâˆ¥1 .
(ICLasso)
We show that transformers can also approximate in-context Lasso with a mild number of layers, and
can perform sparse linear regression in standard sparse linear models (proofs in Appendix H).
Theorem 7 (Implementing in-context Lasso). For any Î»N â‰¥0, Î² > 0, Bw > 0, and Îµ > 0, there
exists a L-layer transformer TFÎ¸ with
L =

Î²B2
w/Îµ

+ 1,
maxâ„“âˆˆ[L] M (â„“) â‰¤2,
maxâ„“âˆˆ[L] D(â„“) â‰¤2d,
|||Î¸||| â‰¤O
 R + (1 + Î»N)Î²âˆ’1
(where R := max {BxBw, By, 1}) such that the following holds. On any input data (D, xN+1) such
that Î»max(XâŠ¤X/N) â‰¤Î² and âˆ¥wlassoâˆ¥2 â‰¤Bw/2, TFÎ¸(H(0)) approximately implements (ICLasso),
in that it outputs byN+1 = âŸ¨xN+1, bwâŸ©with bLlasso(bw) âˆ’bLlasso(wlasso) â‰¤Îµ.
Theorem 8 (Near-optimal sparse linear regression with transformers by approximating Lasso). For
any d, N â‰¥1, Î´ > 0, Bâ‹†
w, Ïƒ > 0, there exists a e
O((Bâ‹†
w)2/Ïƒ2 Ã— (1 + (d/N)))-layer transformer Î¸
such that the following holds: For any s and N â‰¥O (s log(d/Î´)), suppose that P is an s-sparse
linear model: xi âˆ¼N(0, Id), yi = âŸ¨wâ‹†, xiâŸ©+ N(0, Ïƒ2) for any âˆ¥wâ‹†âˆ¥2 â‰¤Bâ‹†
w and âˆ¥wâ‹†âˆ¥0 â‰¤s, then
with probability at least 1 âˆ’Î´ (over the randomness of D), the transformer output byN+1 achieves
E(xN+1,yN+1)âˆ¼P

(byN+1 âˆ’yN+1)2
â‰¤Ïƒ2[1 + O(s log(d/Î´)/N)].
The e
O(s log d/N) excess risk obtained in Theorem 8 is optimal up to log factors [62, 87]. We remark
that Theorem 8 is not a direct corollary of Theorem 7; Rather, the bound on the number of layers
in Theorem 8 requires a sharper convergence analysis of the (ICLasso) problem under sparse linear
models (Appendix H.2), similar to [1].
3.3
Proof technique: In-context gradient descent
The constructions in Section 3.1 and 3.2 is built on the following result for approximating in-context
(proximal) gradient descent on (regularized) convex losses.
Theorem 9 (ICGD; Informal version of Theorem D.1 & D.2). For a broad class of convex losses
of form w 7â†’
1
N
PN
i=1 â„“(wâŠ¤xi, yi) + R(w), there exists an L-layer transformer that takes in any
(D, w0) and outputs bwL such that âˆ¥bwL âˆ’wL
{GD,PGD}âˆ¥2 â‰¤O(LÎµ), by composing L identical layers
each O(Îµ)-approximating a single step of GD (so that O(LÎµ) is a linear error accumulation).
6

Theorem 9 is established in two main steps:
â€¢ Approximating one-step of ICGD using one attention layer (Proposition E.1), which substantially
generalizes that of von Oswald et al. [86] (which only does GD on square losses with a linear
self-attention), and is simpler than the ones in AkyÃ¼rek et al. [2] and Giannou et al. [32].
â€¢ Stacking L of the above layer to approximate L steps of ICGD. Done naively, the error accumu-
lation of this stacking operation is exponential in L in the worst case. We utilize the stability of
convex gradient descent (Lemma D.1) to obtain the linear in L error accumulation in Theorem 9.
In Appendix D.3, we also give results for non-convex GD on two-layer neural nets, though with a
worse (exponential in L) error accumulation as expected.
4
In-context algorithm selection
We now show that transformers can perform various kinds of in-context algorithm selection, which
allows them to implement more complex ICL procedures by adaptively selecting different â€œbaseâ€
algorithms on different input sequences. We construct two general mechanisms: Post-ICL validation,
and Pre-ICL testing; See Figure 1 for a pictorial illustration.
4.1
Post-ICL validation mechanism
In our first mechanism, post-ICL validation, the transformer begins by implementing a train-validation
split D = (Dtrain, Dval), and running K base ICL algorithms on Dtrain. Let {fk}kâˆˆ[K] âŠ‚(Rd â†’R)
denote the K learned predictors, and
bLval(f) :=
1
|Dval|
P
(xi,yi)âˆˆDval â„“(f(xi), yi)
(7)
denote the validation loss of any predictor f.
We show that (proof in Appendix I.1) a 3-layer transformer can output a predictor bf that achieves
nearly the smallest validation loss, and thus nearly optimal expected loss if bLval concentrates around
the expected loss L. Below, the input sequence H uses a generalized positional encoding pi :=
[0Dâˆ’(d+3); 1; ti] in (3), where ti := 1 for i âˆˆDtrain, ti := âˆ’1 for i âˆˆDval, and tN+1 := 0.
Proposition 10 (In-context algorithm selection via train-validation split). Suppose that â„“(Â·, Â·) in (7)
is approximable by sum of relus (Definition D.1, which includes all C3-smooth bivariate functions).
Then there exists a 3-layer transformer TFÎ¸ that maps (defining yâ€²
i = yi1{i < N + 1})
hi = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti]
â†’
hâ€²
i = [xi; yâ€²
i; âˆ—; bf(xi); 1; ti], i âˆˆ[N + 1],
where the predictor bf : Rd â†’R is a convex combination of {fk : bLval(fk) â‰¤minkâ‹†âˆˆ[K] bLval(fkâ‹†) +
Î³}. As a corollary, for any convex risk L : (Rd â†’R) â†’R, bf satisfies
L( bf) â‰¤minkâ‹†âˆˆ[K] L(fkâ‹†) + maxkâˆˆ[K]
bLval(fk) âˆ’L(fk)
 + Î³.
Ridge regression with in-context regularization selection
As an example, we use Proposition 10
to construct a transformer to perform in-context ridge regression with regularization selection ac-
cording to the unregularized validation loss bLval(w) :=
1
2|Dval|
P
(xi,yi)âˆˆDval (âŸ¨w, xiâŸ©âˆ’yi)2 (proof
in Appendix I.2). Let Î»1, . . . , Î»K â‰¥0 be K fixed regularization strengths.
Theorem 11 (Ridge regression with in-context regularization selection). There exists a transformer
with O(log(1/Îµ)) layers and O(K) heads such that the following holds: On any (D, xN+1) well-
conditioned (cf. (5)) for all {Î»k}kâˆˆ[K], it outputs byN+1 = âŸ¨bw, xN+1âŸ©, where
dist

bw, conv{bwÎ»k
ridge,train : bLval(bwÎ»k
ridge,train) â‰¤minkâ‹†âˆˆ[K] bLval(bwÎ»kâ‹†
ridge,train) + Î³}

â‰¤Îµ.
Above, bwÎ»
ridge,train denotes the solution to (ICRidge) on the training split Dtrain.
7

4.1.1
Nearly Bayes-optimal ICL on noisy linear models with mixed noise levels
We build on Theorem 11 to show that transformers can perform nearly Bayes-optimal ICL when data
come from noisy linear models with a mixture of K different noise levels Ïƒ1, . . . , ÏƒK > 0.
Concretely, consider the following data generating model, where we first sample P = Pwâ‹†,Ïƒk âˆ¼Ï€
from k âˆ¼Î› âˆˆâˆ†([K]), wâ‹†âˆ¼N(0, Id/d), and then sample data {(xi, yi)}iâˆˆ[N+1]
iid
âˆ¼Pwâ‹†,Ïƒk as
Pwâ‹†,Ïƒk : xi âˆ¼N(0, Id),
yi = âŸ¨xi, wâ‹†âŸ©+ N(0, Ïƒ2
k).
For any fixed (N, d), consider the Bayes risk for predicting yN+1 under this model:
BayesRiskÏ€ := infA EÏ€
 1
2(A(D)(xN+1) âˆ’yN+1)2
.
By standard Bayesian calculations, the above Bayes risk is attained when A is a certain mixture of K
ridge regressions with regularization Î»k = dÏƒ2
k/N; however, the mixing weights depend on D in
a highly non-trivial fashion (see Appendix J.2 for a derivation). By using the post-ICL validation
mechanism in Theorem 11, we construct a transformer that achieves nearly the Bayes risk.
Theorem 12 (Nearly Bayes-optimal ICL; Informal version of Theorem J.1). For sufficiently large
N, d, there exists a transformer with O(log N) layers and O(K) heads such that on the above model,
it outputs a prediction byN+1 that is nearly Bayes-optimal:
EÏ€
 1
2(yN+1 âˆ’byN+1)2
â‰¤BayesRiskÏ€ + O
 (log K/N)1/3
.
(8)
In particular, Theorem 12 applies in the proportional setting where N, d are large and N/d =
Î˜(1) [22], in which case BayesRiskÏ€ = Î˜(1), and thus the transformer achieves vanishing excess
risk relative to the Bayes risk at large N.
This substantially strengthens the results of AkyÃ¼rek et al. [2], who empirically find that transformers
can achieve nearly Bayes risk under any fixed noise level. By contrast, Theorem 12 shows that a single
transformer can achieve nearly Bayes risk even under a mixture of K noise levels, with quantitative
guarantees. Also, our proof in fact gives a stronger guarantee: The transformer approaches the
individual Bayes risks on all K noise levels simultaneously (in addition to the overall Bayes risk for
k âˆ¼Î› as in Theorem 12). We demonstrate this empirically in Section 6 (cf. Figure 3b & 2).
Exact Bayes predictor vs. Post-ICL validation mechanism
As BayesRiskÏ€ is the theoretical
lower bound for the risk of any possible ICL algorithm, Theorem 12 implies that our transformer
performs similarly as the exact Bayes estimator5. Notice that our construction builds on the (generic)
post-ICL validation mechanism, rather than a direct attempt of approximating the exact Bayes
predictor, whose structure may vary significantly case-by-case. This highlights post-ICL validation
as a promising mechanism for approximating the Bayes predictor on broader classes of problems
beyond noisy linear models, which we leave as future work.
Generalized linear models with adaptive link function selection
As another example of the
post-ICL validation mechanism, we construct a transformer that can learn a generalized linear model
with adaptively chosen link function for the particular ICL instance; see Theorem J.2.
4.2
Pre-ICL testing mechanism
In our second mechanism, pre-ICL testing, the transformer runs a distribution testing procedure on the
input sequence to determine the right ICL algorithm to use. While the test (and thus the mechanism
itself) could in principle be general, we focus on cases where the test amounts to computing some
simple summary statistics of the input sequence.
To showcase pre-ICL testing, we consider the toy problem of selecting between in-context regression
and in-context classification, by running the following binary type check on the input labels {yi}iâˆˆ[N].
Î¨binary(D) = 1
N
N
X
i=1
Ïˆ(yi),
Ïˆ(y) :=
ï£±
ï£²
ï£³
1,
y âˆˆ{0, 1},
0,
y Ì¸âˆˆ[âˆ’Îµ, Îµ] âˆª[1 âˆ’Îµ, 1 + Îµ],
linear interpolation,
otherwise.
5By the Bayes risk decomposition for square loss, (8) implies that E[(byN+1âˆ’byBayes
N+1)2] â‰¤O((log K/N)1/3).
8

Lemma 13. There exists a single attention layer with 6 heads that implements Î¨binary exactly.
Using this test, we construct a transformer that performs logistic regression when labels are binary,
and linear regression with high probability if the label admits a continuous distribution.
Proposition 14 (Adaptive regression or classification; Informal version of Proposition I.4). There
exists a transformer with O(log(1/Îµ)) layers such that the following holds: On any D such that
yi âˆˆ{0, 1}, it outputs byN+1 that Îµ-approximates the prediction of in-context logistic regression.
By contrast, for any distribution P whose marginal distribution of y is not concentrated around {0, 1},
with high probability (over D), byN+1 Îµ-approximates the prediction of in-context least squares.
The proofs can be found in Appendix I.3. We additionally show that transformers can implement
more complex tests such as a linear correlation test, which can be useful in certain scenarios such as
â€œconfident linear regressionâ€ (predict only when the signal-to-noise ratio is high); see Appendix I.4.
5
Analysis of pretraining
Building on the expressivity results in Section 3 & 4, we provide the first line of polynomial sample
complexity results for pretraining transformers to perform ICL (including with in-context algorithm
selection). We begin by providing a generic generalization guarantee for pretraining transformers.
Consider the pretraining ERM problem (TF-ERM), which minimizes the pretraining risk bLicl(Â·) over
n pretraining sequences. Let Licl(Â·) denote the corresponding population risk.
Theorem 15 (Generalization of transformers; Informal version of Theorem K.1). The solution bÎ¸
to (TF-ERM) over transformers with L layers, M heads per layer, and hidden dimension Dâ€² satisfies
Licl(bÎ¸) â‰¤inf
Î¸ Licl(Î¸) + e
O
 r
L2(MD2 + DDâ€²)
n
!
.
Theorem 15 builds on standard uniform concentration analysis via chaining (Proposition B.4).
Combining Theorem 15 with the in-context linear regression construction in Theorem 4 gives the
following end-to-end result on the excess in-context prediction risk of trained transformers.
Theorem 16 (Pretraining transformers for in-context linear regression; Informal version of The-
orem K.2). Under Assumption A and N
â‰¥
e
O(d), the solution bÎ¸ to (TF-ERM) with L =
O(Îº log(ÎºN/Ïƒ)) layers, M = 3 heads, Dâ€² = 0 (attention-only as in Theorem 4) achieves small
excess ICL risk over the best linear predictor wâ‹†
P := EP[xxâŠ¤]âˆ’1EP[xy] for each P:
Licl(bÎ¸) âˆ’EPâˆ¼Ï€E(x,y)âˆ¼P
1
2(y âˆ’âŸ¨wâ‹†
P, xâŸ©)2

â‰¤e
O
 r
Îº2d2
n
+ dÏƒ2
N
!
,
See Appendix K.2 for similar results in several additional settings.
6
Experiments
We test our theory by studying the ICL and in-context algorithm selection capabilities of transformers,
using the encoder-based architecture in our theoretical constructions (Definition 3). Due to limited
space, additional experimental details can be found in Appendix M.1. Results with a decoder
architecture as in [31, 47] (including the setup of Figure 2) can be found in Appendix M.2.
Training data distributions and evaluation
We train a 12-layer transformer, with two modes for
the training sequence (instance) distribution Ï€. In the â€œbaseâ€ mode, similar to [31, 2, 86, 47], we
sample the training instances from one of the following base distributions (tasks), where we first
sample P = Pwâ‹†âˆ¼Ï€ by sampling wâ‹†âˆ¼N(0, Id/d), and then sample {(xi, yi)}iâˆˆ[N+1]
iid
âˆ¼Pwâ‹†as
xi
iid
âˆ¼N(0, Id), and yi from one of the following models studied in Section 3:
1. Linear model: yi = âŸ¨wâ‹†, xiâŸ©;
2. Noisy linear model: yi = âŸ¨wâ‹†, xiâŸ©+ Ïƒzi, where Ïƒ > 0 is a fixed noise level, and zi âˆ¼N(0, 1).
9

(a) Base ICL capabilities
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Loss
linear_regression
noisy_reg_noise_1
noisy_reg_noise_2
sparse_reg
linear_classification
Transformer
Least Squares
Averaging
3-NN
ridge_lam_1
ridge_lam_2
Lasso_lam=1
Lasso_lam=0.1
Lasso_lam=0.01
Lasso_lam=0.001
Logistic Regression
(b) Noisy reg with two noises
0.10
0.15
0.20
0.25
0.30
noisy_reg_noise_1
0.6
0.8
1.0
1.2
1.4
noisy_reg_noise_2
TF_alg_select
TF_noise_1
TF_noise_2
ridge_lam_1
ridge_lam_2
ridge analytical
Bayes_err_noise_1
Bayes_err_noise_2
(c) Reg + Classification
0.0
0.5
1.0
1.5
regression_square_loss
0.20
0.25
0.30
0.35
classification_error
TF_alg_select
TF_reg
TF_cls
Least Squares
Averaging
3-NN
Figure 3: ICL capabilities of the transformer architecture used in our theoretical constructions. (a) On five
representative base tasks, transformers approximately match the best baseline algorithm for each task, when
pretrained on the corresponding task. (b,c) A single transformer TF_alg_select simultaneously approaches
the performance of the strongest baseline algorithm on two separate tasks: (b) noisy linear regression with
two different noise levels Ïƒ âˆˆ{0.1, 0.5}, and (c) adaptively selecting between regression and classification.
3. Sparse linear model: yi = âŸ¨wâ‹†, xiâŸ©with âˆ¥wâ‹†âˆ¥0 â‰¤s, where s < d is a fixed sparsity level, and
in this case we sample wâ‹†from a special prior supported on s-sparse vectors;
4. Linear classification model: yi = sign(âŸ¨wâ‹†, xiâŸ©).
These base tasks have been empirically investigated by Garg et al. [31], though we remark that
our architecture (used in our theory) differs from theirs in several aspects, such as encoder-based
architecture instead of decoder-based, and ReLU activation instead of softmax. All experiments use
d = 20. We choose Ïƒ âˆˆ{Ïƒ1, Ïƒ2} = {0.1, 0.5} and N = 20 for noisy linear regression, s = 3 and
N = 10 for sparse linear regression, and N = 40 for linear regression and linear classification.
In the â€œmixtureâ€ mode, Ï€ is the uniform mixture of two or more base distributions. We consider two
representative mixture modes studied in Section 4:
â€¢ Linear model + linear classification model;
â€¢ Noisy linear model with four noise levels Ïƒ âˆˆ{0.1, 0.25, 0.5, 1}.
Transformers trained with the mixture mode will be evaluated on multiple base distributions simulta-
neously. When the base distributions are sufficiently diverse, a transformer performing well on all of
them will likely be performing some level of in-context algorithm selection. We evaluate transformers
against standard machine learning algorithms in context (for each task respectively) as baselines.
Results
Figure 3a shows the ICL performance of transformers on five base tasks, within each the
transformer is trained on the same task. Transformers match the best baseline algorithm in four out
of the five cases, except for the sparse regression task where the Transformer still outperforms least
squares and matches Lasso with some choices of Î» (thus utilizing sparsity to some extent). This
demonstrates the strong ICL capability of the transformer architecture considered in our theory.
Figure 3b & 3c examine the in-context algorithm selection capability of transformers, on noisy
linear regression with two different noise levels (Figure 3b), and regression + classification (Figure
3c). In both figures, the transformer trained in the mixture mode (TF_alg_select) approaches the
best baseline algorithm on both tasks simultaneously. By contrast, transformers trained in the base
mode for one of the tasks perform well on that task but behave suboptimally on the other task as
expected. The existence of TF_alg_select showcases a single transformer that performs well on
multiple tasks simultaneously (and thus has to perform in-context algorithm selection to some extent),
supporting our theoretical results in Section 4.
7
Conclusion
This work shows that transformers can perform complex in-context learning procedures with strong
in-context algorithm selection capabilties, by both explicit theoretical constructions and experiments.
We believe our work opens up many exciting directions, such as (1) more mechanisms for in-context
algorithm selection; (2) Bayes-optimal ICL on other problems by either the post-ICL validation
mechanism or new approaches; (3) understanding the internal workings of transformers performing
in-context algorithm selection; (4) other mechanisms for implementing complex ICL procedures
beyond in-context algorithm selection; (5) further statistical analyses, e.g. of pretraining. Besides,
this work focuses on the transformer architecture; alternative sequence-to-sequence architectures
(such as RNNs) are beyond our scope but would be interesting directions for future work.
10

Acknowledgment
The authors would like to thank Tengyu Ma and Jason D. Lee for the many insightful discussions. S.
Mei is supported in part by NSF DMS-2210827 and NSF CCF-2315725.
References
[1] A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence rates of gradient
methods for high-dimensional statistical recovery. Advances in Neural Information Processing
Systems, 23, 2010.
[2] E. AkyÃ¼rek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is
in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661,
2022.
[3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,
2016.
[4] F. Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research, 18(1):629â€“681, 2017.
[5] Y. Bai, M. Chen, P. Zhou, T. Zhao, J. Lee, S. Kakade, H. Wang, and C. Xiong. How important is
the train-validation split in meta-learning? In International Conference on Machine Learning,
pages 543â€“553. PMLR, 2021.
[6] Y. Bai, S. Mei, H. Wang, and C. Xiong. Donâ€™t just blame over-parametrization for over-
confidence: Theoretical analysis of calibration in binary classification. In International
Conference on Machine Learning, pages 566â€“576. PMLR, 2021.
[7] J. Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149â€“198, 2000.
[8] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal recovery.
Convex optimization in signal processing and communications, pages 42â€“88, 2009.
[9] S. Bengio, Y. Bengio, J. Cloutier, and J. Gescei. On the optimization of a synaptic learning
rule. In Optimality in Biological and Artificial Networks?, pages 281â€“303. Routledge, 2013.
[10] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to
recognize formal languages. arXiv preprint arXiv:2009.11264, 2020.
[11] S. Bhattamishra, A. Patel, and N. Goyal. On the computational power of transformers and its
implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020.
[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in
neural information processing systems, 33:1877â€“1901, 2020.
[13] S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and TrendsÂ® in
Machine Learning, 8(3-4):231â€“357, 2015.
[14] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,
Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv:2303.12712, 2023.
[15] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and
F. Hill. Data distributional properties drive emergent in-context learning in transformers.
Advances in Neural Information Processing Systems, 35:18878â€“18891, 2022.
[16] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and
I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances
in neural information processing systems, 34:15084â€“15097, 2021.
11

[17] K. Chua, Q. Lei, and J. D. Lee. How fine-tuning allows for effective meta-learning. Advances
in Neural Information Processing Systems, 34:8871â€“8884, 2021.
[18] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei. Why can gpt learn in-context? language
models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559,
2022.
[19] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Incremental learning-to-learn with statistical
guarantees. arXiv preprint arXiv:1803.08089, 2018.
[20] G. Denevi, C. Ciliberto, D. Stamos, and M. Pontil. Learning to learn around a common mean.
Advances in Neural Information Processing Systems, 31, 2018.
[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[22] E. Dobriban and S. Wager. High-dimensional asymptotics of prediction: Ridge regression and
classification. The Annals of Statistics, 46(1):247â€“279, 2018.
[23] L. Dong, S. Xu, and B. Xu. Speech-transformer: a no-recurrence sequence-to-sequence model
for speech recognition. In 2018 IEEE international conference on acoustics, speech and signal
processing (ICASSP), pages 5884â€“5888. IEEE, 2018.
[24] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey for
in-context learning. arXiv preprint arXiv:2301.00234, 2022.
[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-
hghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[26] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, and Q. Lei. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.
[27] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-
attention mechanisms. In International Conference on Machine Learning, pages 5793â€“5831.
PMLR, 2022.
[28] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen,
T. Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits
Thread, 2021.
[29] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International conference on machine learning, pages 1126â€“1135. PMLR, 2017.
[30] C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online meta-learning. In International
Conference on Machine Learning, pages 1920â€“1930. PMLR, 2019.
[31] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a
case study of simple function classes. Advances in Neural Information Processing Systems,
35:30583â€“30598, 2022.
[32] A. Giannou, S. Rajput, J.-y. Sohn, K. Lee, J. D. Lee, and D. Papailiopoulos. Looped trans-
formers as programmable computers. arXiv preprint arXiv:2301.13196, 2023.
[33] M. Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of
the Association for Computational Linguistics, 8:156â€“171, 2020.
[34] S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In
Artificial Neural Networksâ€”ICANN 2001: International Conference Vienna, Austria, August
21â€“25, 2001 Proceedings 11, pages 87â€“94. Springer, 2001.
[35] N. Hollmann, S. MÃ¼ller, K. Eggensperger, and F. Hutter. Tabpfn: A transformer that solves
small tabular classification problems in a second. arXiv preprint arXiv:2207.01848, 2022.
12

[36] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: A
survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149â€“5169,
2021.
[37] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep
attention networks. In International Conference on Machine Learning, pages 4376â€“4386.
PMLR, 2020.
[38] D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pages 9â€“1. JMLR Workshop and Conference Proceedings,
2012.
[39] S. Jelassi, M. E. Sander, and Y. Li. Vision transformers provably learn spatial structure. arXiv
preprint arXiv:2210.09221, 2022.
[40] K. Ji, J. D. Lee, Y. Liang, and H. V. Poor. Convergence of meta-learning with task-specific
adaptation over partial parameters. Advances in Neural Information Processing Systems, 33:
11490â€“11500, 2020.
[41] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool,
R. Bates, A. vZÃ­dek, A. Potapenko, et al. Highly accurate protein structure prediction with
alphafold. Nature, 596(7873):583â€“589, 2021.
[42] S. M. Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear
and single index models with isotonic regression. Advances in Neural Information Processing
Systems, 24, 2011.
[43] M. Khodak, M.-F. F. Balcan, and A. S. Talwalkar. Adaptive gradient-based meta-learning
methods. Advances in Neural Information Processing Systems, 32, 2019.
[44] L. Kirsch and J. Schmidhuber. Meta learning backpropagation and improving it. Advances in
Neural Information Processing Systems, 34:14122â€“14134, 2021.
[45] L. Kirsch, J. Harrison, J. Sohl-Dickstein, and L. Metz. General-purpose in-context learning by
meta-learning transformers. arXiv preprint arXiv:2212.04458, 2022.
[46] K. Li and J. Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
[47] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: General-
ization and implicit model selection in in-context learning. arXiv preprint arXiv:2301.07067,
2023.
[48] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to
automata. arXiv preprint arXiv:2210.10749, 2022.
[49] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What makes good in-context
examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.
[50] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint
arXiv:2104.08786, 2021.
[51] A. Madani, B. McCann, N. Naik, N. S. Keskar, N. Anand, R. R. Eguchi, P.-S. Huang,
and R. Socher.
Progen: Language modeling for protein generation.
arXiv preprint
arXiv:2004.03497, 2020.
[52] A. Maurer, M. Pontil, and B. Romera-Paredes. The benefit of multitask representation learning.
Journal of Machine Learning Research, 17(81):1â€“32, 2016.
[53] P. McCullagh. Generalized linear models. Routledge, 2019.
[54] S. Mei, Y. Bai, and A. Montanari. The landscape of empirical risk for nonconvex losses. The
Annals of Statistics, 46(6A):2747â€“2774, 2018.
13

[55] S. Min, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Noisy channel language model prompting
for few-shot text classification. arXiv preprint arXiv:2108.04106, 2021.
[56] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.
arXiv preprint arXiv:2110.15943, 2021.
[57] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer.
Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837, 2022.
[58] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner.
arXiv preprint arXiv:1707.03141, 2017.
[59] S. MÃ¼ller, N. Hollmann, S. P. Arango, J. Grabocka, and F. Hutter. Transformers can do
bayesian inference. arXiv preprint arXiv:2112.10510, 2021.
[60] T. Nagler.
Statistical foundations of prior-data fitted networks.
arXiv preprint
arXiv:2305.11097, 2023.
[61] D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings
1992] IJCNN International Joint Conference on Neural Networks, volume 1, pages 437â€“442.
IEEE, 1992.
[62] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for
high-dimensional analysis of m-estimators with decomposable regularizers. 2012.
[63] Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
[64] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann,
A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint
arXiv:2209.11895, 2022.
[65] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[66] N. Parikh, S. Boyd, et al. Proximal algorithms. Foundations and trendsÂ® in Optimization, 1
(3):127â€“239, 2014.
[67] J. PÃ©rez, J. MarinkoviÂ´c, and P. BarcelÃ³. On the turing completeness of modern neural network
architectures. arXiv preprint arXiv:1901.03429, 2019.
[68] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding
by generative pre-training. 2018.
[69] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages 8748â€“8763. PMLR, 2021.
[70] A. Raventos, M. Paul, F. Chen, and S. Ganguli. The effects of pretraining task diversity
on in-context learning of ridge regression. In ICLR 2023 Workshop on Mathematical and
Empirical Understanding of Foundation Models, 2023.
[71] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In International
conference on learning representations, 2017.
[72] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies
on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
[73] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,
Y. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,
2022.
[74] O. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. arXiv
preprint arXiv:2112.08633, 2021.
14

[75] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. Meta-learning with
memory-augmented neural networks. In International conference on machine learning, pages
1842â€“1850. PMLR, 2016.
[76] N. Saunshi, A. Gupta, and W. Hu. A representation learning perspective on the importance of
train-validation splitting in meta-learning. In International Conference on Machine Learning,
pages 9333â€“9343. PMLR, 2021.
[77] J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
learn: the meta-meta-... hook. PhD thesis, Technische UniversitÃ¤t MÃ¼nchen, 1987.
[78] K. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in
transformer. arXiv preprint arXiv:2302.06461, 2023.
[79] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention
learns. arXiv preprint arXiv:2103.07601, 2021.
[80] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. Advances in
neural information processing systems, 30, 2017.
[81] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media, 2012.
[82] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267â€“288, 1996.
[83] N. Tripuraneni, M. Jordan, and C. Jin. On the theory of transfer learning: The importance of
task diversity. Advances in neural information processing systems, 33:7852â€“7862, 2020.
[84] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.
[85] R. Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
[86] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov,
and M. Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677, 2022.
[87] M. J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge university press, 2019.
[88] X. Wang, S. Yuan, C. Wu, and R. Ge. Guarantees for tuning the step size using a learning-
to-learn approach. In International Conference on Machine Learning, pages 10981â€“10990.
PMLR, 2021.
[89] C. Wei, Y. Chen, and T. Ma. Statistically meaningful approximation: a case study on approxi-
mating turing machines with transformers. arXiv preprint arXiv:2107.13163, 2021.
[90] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,
D. Zhou, D. Metzler, et al. Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682, 2022.
[91] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al.
Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846,
2023.
[92] G. Weiss, Y. Goldberg, and E. Yahav. Thinking like transformers. In International Conference
on Machine Learning, pages 11080â€“11090. PMLR, 2021.
[93] M. Wortsman, J. Lee, J. Gilmer, and S. Kornblith. Replacing softmax with relu in vision
transformers. arXiv preprint arXiv:2309.08586, 2023.
[94] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as
implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
15

[95] S. Yao, B. Peng, C. Papadimitriou, and K. Narasimhan. Self-attention networks can process
bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021.
[96] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. Do transformers
really perform badly for graph representation? Advances in Neural Information Processing
Systems, 34:28877â€“28888, 2021.
[97] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal
approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.
[98] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling trans-
formers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
[99] Y. Zhang, B. Liu, Q. Cai, L. Wang, and Z. Wang. An analysis of attention via the lens of
exchangeability and latent variable models. arXiv preprint arXiv:2212.14852, 2022.
[100] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot
performance of language models. In International Conference on Machine Learning, pages
12697â€“12706. PMLR, 2021.
[101] X. Zuo, Z. Chen, H. Yao, Y. Cao, and Q. Gu. Understanding train-validation split in meta-
learning with neural networks. In The Eleventh International Conference on Learning Repre-
sentations, 2023. URL https://openreview.net/forum?id=JVlyfHEEm0k.
A
Related work
In-context learning
The in-context learning (ICL) capability of large language models (LLMs) has
gained significant attention since demonstrated on GPT-3 Brown et al. [12]. A number of subsequent
empirical studies have contributed to a better understanding of the capabilities and limitations of
ICL in LLM systems, which include but are not limited to [49, 55, 56, 50, 100, 74, 72, 28, 45, 91].
For an overview of ICL, see the survey by Dong et al. [24] which highlights some key findings and
advancements in this direction.
A line of recent work investigates why and how LLMs perform ICL [94, 31, 86, 2, 18, 32, 47, 70]. In
particular, Xie et al. [94] propose a Bayesian inference framework explaining how ICL works despite
formatting differences between training and inference distributions. Garg et al. [31] show empirically
that transformers could be trained from scratch to perform ICL of linear models, sparse linear models,
two-layer neural networks, and decision trees. Li et al. [47] analyze the generalization error of
trained ICL transformers from a stability viewpoint. They also experimentally show that transformers
could perform â€œin-context model selectionâ€ (conceptually similar to in-context algorithm selection
considered in this work) in specific tasks and presented related theoretical hypotheses. However,
they do not provide concrete mechanisms or constructions for in-context model selection. A recent
work [99] shows that pretrained transformers can perform Bayesian inference in latent variable models,
which may also be interpreted as a mechanism for ICL. Our experimental findings extend these
results by unveiling and demonstrating the in-context algorithm selection capabilities of transformers.
Closely related to our theoretical results are [86, 2, 18, 32], which show (among many things) that
transformers can perform ICL by simulating gradient descent. However, these results do not provide
quantitative error bounds for simulating multi-step gradient descent, and only handle linear regression
models or their simple variants. Among these works, AkyÃ¼rek et al. [2] showed that transformers can
implement learning algorithms for linear models based on gradient descent and closed-form ridge
regression; it also presented preliminary evidence that learned transformers perform ICL similar
to Bayes-optimal ridge regression. Our work builds upon and substantially extends this line of
work by (1) providing a more efficient construction for in-context gradient descent; (2) providing
an end-to-end theory with additional results for pretraining and statistical power; (3) analyzing a
broader spectrum of ICL algorithms, including least squares, ridge regression, Lasso, convex risk
minimization for generalized linear models, and gradient descent on two-layer neural networks; and
(4) constructing more complex ICL procedures using in-context algorithm selection.
When in-context data are generated from a prior, the Bayes risk is a theoretical lower bound for the
risk of any possible ICL algorithm, including transformers. Xie et al. [94], AkyÃ¼rek et al. [2] observe
16

that learned transformers behave closely to the Bayes predictor on a variety of tasks such as hidden
Markov models [94] and noisy linear regression with a fixed noise level [2, 47]. Using the in-context
algorithm selection mechanism (more precisely the post-ICL validation mechanism), we show that
transformers can perform nearly-Bayes optimal ICL in noisy linear models with mixed noise levels (a
strictly more challenging task than considered in [2, 47]), with both concrete theoretical guarantees
(Section 4.1.1) and empirical evidence (Figure 2 & 3b). Complementary to these works, a line of work
on â€œprior-data fitted networksâ€ [59, 60, 35] also empirically demonstrates the Bayesian optimality of
transformers in various settings. Our expressivity results support these empirical findings and are
applicable beyond the Bayesian setting, e.g. for providing frequentist in-context prediction guarantees
for transformers.
Transformers and its theory
The transformer architecture, introduced by [84], has revolutionized
natural language processing and been adopted in most of the recently developed large language
models such as BERT and GPT [68, 21, 12]. Broaderly, transformers have demonstrated remarkable
performance in many other fields of artificial intelligence such as computer vision, speech, graph
processing, reinforcement learning, and biological applications [23, 25, 51, 69, 96, 16, 41, 73, 65, 14].
Towards a better theoretical understanding, recent work has studied the capabilities [97, 67, 37, 95,
11, 98, 48], limitations [33, 10], and internal workings [28, 79, 92, 27, 64] of transformers.
We remark that the transformer architecture used in our theoretical constructions differs from the
standard one by replacing the softmax activation (in the attention layers) with a (normalized) ReLU
function. Transformers with ReLU activations is experimentally studied in the recent work of Shen
et al. [78], who find that they perform as well as the standard softmax activation in many NLP tasks.
Meta-learning
Training models (such as transformers) to perform ICL can be viewed as an
approach for the broader problem of learning-to-learn or meta-learning [77, 61, 81]. A number of
other approaches has been studied extensively for this problem, including (and not limited to) training
a meta-learner on how to update the parameters of a downstream learner [9, 46], learning parameter
initializations that quickly adapt to downstream tasks [29, 71], learning latent embeddings that allow
for effective similarity search [80]. Most relevant to the ICL setting are approaches that directly take
as input examples from a downstream task and a query input and produce the corresponding output
[34, 58, 75, 44]. For a comprehensive overview, see the survey [36].
Theoretical aspects of meta-learning have received significant recent interest [7, 52, 26, 83, 19, 30,
43, 40, 88, 20, 5, 76, 17, 101]. In particular, [52, 26, 83] analyzed the benefit of multi-task learning
through a representation learning perspective, and [88, 20, 5, 76, 101] studied the statistical properties
of learning the parameter initialization for downstream tasks.
Techniques
We build on various existing techniques from the statistics and learning theory literature
to establish our approximation and generalization guarantees for transformers. For the approximation
component, we rely on a technical result of Bach [4] on the approximation power of ReLU networks.
We use this result to show that transformers can approximate gradient descent (GD) on a broad range
of loss functions, substantially extending the results of [86, 2, 18] who primarily consider the square
loss. The recent work of Giannou et al. [32] also approximates GD with general loss functions by
transformers, though using a different technique of forcing the softmax activations to act as sigmoids.
Our analyses of Lasso and generalized linear models build on [87, 62, 1, 54]. Our generalization
bound for transformers (used in our pretraining results) build on a chaining argument [87].
B
Technical tools
Additional notation for proofs
We say a random variable X is Ïƒ2-sub-Gaussian (or SG(Ïƒ)
interchangeably) if E[exp(X2/Ïƒ2)] â‰¤2. A random vector x âˆˆRd is Ïƒ2-sub-Gaussian if âŸ¨v, xâŸ©
is Ïƒ2-sub-Gaussian for all âˆ¥vâˆ¥2 = 1. A random variable X is K-sub-Exponential (or SE(K)
interchangeably) if E[exp(|X| /K)] â‰¤2.
B.1
Concentration inequalities
Lemma B.1. Let Î² âˆ¼N(0, Id/d). Then we have
P

âˆ¥Î²âˆ¥2
2 â‰¥(1 + Î´)2
â‰¤eâˆ’dÎ´2/2.
17

Lemma B.2 (Theorem 6.1 of [87]). Let X = [Xij] âˆˆRnÃ—d be a Gaussian random matrix with
Xij âˆ¼N(0, 1). Let Ïƒmin(X) and Ïƒmin(X) be the minimum and maximum singular value of X,
respectively. Then we have
P

Ïƒmax(X)/âˆšn â‰¥1 +
p
d/n + Î´

â‰¤eâˆ’nÎ´2/2,
P

Ïƒmin(X)/âˆšn â‰¤1 âˆ’
p
d/n âˆ’Î´

â‰¤eâˆ’nÎ´2/2.
The following lemma is a standard result of covariance concentration, see e.g. [85, Theorem 4.6.1].
Lemma B.3. Suppose that x1, Â· Â· Â· , xN are independent d-dimensional K-sub-Gaussian random
vectors. Then as long as N â‰¥C0d, with probability at least 1 âˆ’exp(âˆ’N/C0) we have

1
N
N
X
i=1
xixâŠ¤
i

op
â‰¤8K2,
where C0 is a universal constant.
Lemma B.4. For random matrix X = [xij] âˆˆRNÃ—d with xij
iid
âˆ¼N(0, 1) and Îµ = [Îµi] âˆˆRN with
Îµi
iid
âˆ¼N(0, Ïƒ2), it holds that
P
XâŠ¤Îµ

âˆâ‰¥
p
8NÏƒ2 log(2d/Î´)

â‰¤Î´ + exp(âˆ’N/2).
Proof. We consider uj := [xij]i âˆˆRN, then
XâŠ¤Îµ

âˆ= maxiâˆˆ[d] |âŸ¨uj, ÎµâŸ©|. Notice that the
random variables âŸ¨u1, ÎµâŸ©, Â· Â· Â· , âŸ¨ud, ÎµâŸ©are independent N(0, âˆ¥Îµâˆ¥2
2), and hence
P

max
iâˆˆ[d] |âŸ¨uj, ÎµâŸ©| â‰¥t
 Îµ

â‰¤2d exp
 
âˆ’
t2
2 âˆ¥Îµâˆ¥2
2
!
.
Further, by Lemma B.1, P(âˆ¥Îµâˆ¥2 â‰¥2Ïƒ
âˆš
N) â‰¤exp(âˆ’N/2). Taking t =
p
8NÏƒ2 log(2d/Î´)
completes the proof.
B.2
Approximation theory
For any signed measure Âµ over a space W, let TV(Âµ) :=
R
W |dÂµ(w)| âˆˆ[0, âˆ] denote its total
measure. Recall Ïƒ(Â·) = ReLU(Â·) is the standard relu activation, and Bk
âˆ(R) = [âˆ’R, R]k denotes
the standard â„“âˆball in Rk with radius R > 0.
Definition B.1 (Sufficiently smooth k-variable function). We say a function g : Rk â†’R is (R, Câ„“)-
smooth, if for s = âŒˆ(k âˆ’1)/2âŒ‰+ 2, g is a Cs function on Bk
âˆ(R), and
sup
zâˆˆBkâˆ(R)
âˆ‡ig(z)

âˆ=
sup
zâˆˆBk
âˆ(R)
max
j1,...,jiâˆˆ[k] |âˆ‚xj1...xjig(x)| â‰¤Li
for all i âˆˆ{0, 1, . . . , s}, with max0â‰¤iâ‰¤s LiRi â‰¤Câ„“.
The following result for expressing smooth functions as a random feature model with relu activation
is adapted from Bach [4, Proposition 5].
Lemma B.5 (Expressing sufficiently smooth functions by relu random features). Suppose func-
tion g : Rk â†’R is (R, Câ„“) smooth.
Then there exists a signed measure Âµ over W =
{w âˆˆRk+1 : âˆ¥wâˆ¥1 = 1} such that
g(x) =
Z
W
1
RÏƒ(wâŠ¤[x; R])dÂµ(w),
âˆ€x âˆˆX
and TV(Âµ) â‰¤C(k)Câ„“, where C(k) < âˆis a constant that only depends on k.
Lemma B.6 (Uniform finite-neuron approximation). Let X be a space equipped with a distance
function dX (Â·, Â·) : X Ã— X â†’Râ‰¥0. Suppose function g : X â†’R is given by
g(x) =
Z
W
Ï•(x; w)dÂµ(w),
18

where Ï•(Â·; Â·) : X Ã— W â†’[âˆ’B, B] is L-Lipschitz (in dX ) in the first argument, and Âµ is a signed
measure over W with finite total measure A = TV(Âµ) < âˆ. Then for any Îµ > 0, there exists
Î±1, Â· Â· Â· , Î±K âˆˆ{Â±1}, w1, Â· Â· Â· , wK âˆˆW with K = O(A2B2 log N(X, dX ,
Îµ
3AL)/Îµ2), such that
sup
xâˆˆX
g(x) âˆ’A
K
K
X
i=1
Î±iÏ•(x; wi)
 â‰¤Îµ,
where N(X, dX ,
Îµ
3AL) denotes the (
Îµ
3AL)-covering number of X in dX .
Proof. Let Î±(w) := sign(dÂµ(w)) âˆˆ{Â±1} denote the sign of the density dÂµ(w). We have
g(x) = A
Z
W
Î±(w)Ï•(x; w) Ã— |dÂµ(w)|
A
.
(9)
Note that |dÂµ(w)|/A is the density of a probability distribution over W. Thus for any x âˆˆX,
as long as K â‰¥O(A2B2 log(1/Î´)/Îµ2), we can sample w1, . . . , wK
iid
âˆ¼|dÂµ(Â·)|/A, and obtain by
Hoeffdingâ€™s inequality that with probability at least 1 âˆ’Î´,
g(x) âˆ’A
K
K
X
i=1
Î±(wi)Ï•(x; wi)
 â‰¤Îµ.
Let N(
Îµ
3AL)
:=
N(X, dX ,
Îµ
3AL) for shorthand.
By union bound, as long as K
â‰¥
O(A2B2 log(N(
Îµ
3AL)/Î´)/Îµ2), we have with probability at least 1âˆ’Î´ that for every bx in the covering
set corresponding to N(
Îµ
3AL),
g(bx) âˆ’A
K
K
X
i=1
Î±(wi)Ï•(bx; wi)
 â‰¤Îµ/3.
Taking Î´ = 1/2 (for which K = O(A2B2 log N(
Îµ
3AL)/Îµ2)), by the probabilistic method, there
exists a deterministic set {wi}iâˆˆ[K] âŠ‚W and {Î±i := Î±(wi)}iâˆˆ[K] âˆˆ{Â±1} such that the above
holds.
Next, note that both g (by (9)) and the function x 7â†’A
K
PK
i=1 Î±(wi)Ï•(x; wi) are (AL)-Lipschitz.
Therefore, for any x âˆˆX, taking bx to be the point in the covereing set with dX (x, bx) â‰¤
Îµ
3AL, we
have
g(x) âˆ’A
K
K
X
i=1
Î±(wi)Ï•(x; wi)

â‰¤|g(x) âˆ’g(bx)| +
g(bx) âˆ’A
K
K
X
i=1
Î±(wi)Ï•(bx; wi)
 +

A
K
K
X
i=1
Î±(wi)Ï•(bx; wi) âˆ’A
K
K
X
i=1
Î±(wi)Ï•(x; wi)

â‰¤AL Â·
Îµ
3AL + Îµ
3 + AL Â·
Îµ
3AL = Îµ.
This proves the lemma.
Proposition B.1 (Approximating smooth k-variable functions). For any Îµapprox > 0, R â‰¥1,
Câ„“> 0, we have the following: Any (R, Câ„“)-smooth function (Definition B.1) g : Rk â†’R
is (Îµapprox, R, M, C)-approximable by sum of relus (Definition D.1) with M â‰¤C(k)C2
â„“log(1 +
Câ„“/Îµapprox)/Îµ2
approx) and C â‰¤C(k)Câ„“, where C(k) > 0 is a constant that depends only on k. In
other words, there exists
f(z) =
M
X
m=1
cmÏƒ(aâŠ¤
m[z; 1])
with
M
X
m=1
|cm| â‰¤C,
max
mâˆˆ[M] âˆ¥amâˆ¥1 â‰¤1,
such that supzâˆˆ[âˆ’R,R]k |f(z) âˆ’g(z)| â‰¤Îµapprox.
19

Proof. As function g : Bk
âˆ(R) â†’R is (R, Câ„“)-smooth, we can apply Lemma B.5 to obtain that
there exists a signed measure Âµ over W := {w âˆˆRk+1 : âˆ¥wâˆ¥1 â‰¤1} such that
g(z) =
Z
W
1
RÏƒ(wâŠ¤[z; R])dÂµ(w),
âˆ€z âˆˆ[âˆ’R, R]k,
and A = TV(Âµ) â‰¤C(k)Câ„“where C(k) > 0 denotes a constant depending only on k.
We now apply Lemma B.6 to approximate the above random feature by finitely many neurons. Let
x := [z; R] âˆˆX := [âˆ’R, R]k Ã— {R}. Then, the function Ï•(x; w) := 1
RÏƒ(wâŠ¤x) = Ïƒ( 1
RwâŠ¤[z; R])
is bounded by B = 1 and (1/R)-Lipschitz in x (in the standard â„“âˆ-distance). Further, we have
log N(X, âˆ¥Â· âˆ’Â·âˆ¥âˆ, Îµapprox
3A/R ) â‰¤O(k log(1 + A/Îµapprox)). We can thus apply Lemma B.6 to obtain
that, for
M = O
 kA2 log(1 + A/Îµapprox)/Îµ2
approx

= C(k)C2
â„“log(1 + Câ„“/Îµapprox)/Îµ2
approx,
there exists Î± = {Î±m}mâˆˆ[M] âŠ‚{Â±1} and W = {wm}mâˆˆ[M] âŠ‚W = {w âˆˆRk+1 : lonew = 1}
such that
sup
zâˆˆ[âˆ’R,R]2 |g(z) âˆ’fÎ±,W(z)| â‰¤Îµapprox,
where (recalling z = [s; t])
fÎ±,W(z) = A
M
M
X
m=1
Î±mÏƒ
 1
RwâŠ¤
m[z; R]

=
M
X
m=1
AÎ±m
M
| {z }
cm
Ïƒ
 h 1
Rwm,1:k; wm,k+1
âŠ¤
|
{z
}
aâŠ¤
m
[z; 1]).
Note that we have PM
m=1 |cm| = A â‰¤C(k)Câ„“, and âˆ¥amâˆ¥1 â‰¤âˆ¥wmâˆ¥1 = 1. This is the desired
result.
B.3
Optimization
The following convergence result for minimizing a smooth and strongly convex function is standard
from the convex optimization literature, see e.g. Bubeck [13, Theorem 3.10].
Proposition B.2 (Gradient descent for smooth and strongly convex functions). Suppose L : Rd â†’R
is Î±-strongly convex and Î²-smooth for some 0 < Î± â‰¤Î². Then, the gradient descent iterates
wt+1
GD := wt
GD âˆ’Î·âˆ‡L(wt
GD) with learning rate Î· = 1/Î² and initialization w0
GD âˆˆRd satisfies for
any t â‰¥1,
wt
GD âˆ’wâ‹†2
2 â‰¤exp (âˆ’t/Îº) Â·
w0
GD âˆ’wâ‹†2
2 ,
L(wt
GD) âˆ’L(wâ‹†) â‰¤Î²
2 exp (âˆ’t/Îº) Â·
w0
GD âˆ’wâ‹†2
2 ,
where Îº := Î²/Î± is the condition number of L, and wâ‹†:= arg minwâˆˆRd L(w) is the minimizer of L.
The following convergence result of proximal gradient descent (PGD) on convex composite mini-
mization problem is also standard, see e.g. [8].
Proposition B.3 (Proximal gradient descent for convex function). Suppose L = f + h, f : Rd â†’R
is convex and Î²-smooth for some Î² > 0, h : Rd â†’R is a simple convex function. Then, the proximal
gradient descent iterates wt+1
PGD := proxÎ·h(wt
PGD âˆ’Î·âˆ‡f(wt
PGD)) with learning rate Î· = 1/Î²
and initialization w0
GD âˆˆRd satisfies the following for any t â‰¥1:
1. {L(wt
PGD)} is a decreasing sequence.
2. For any minimizer wâ‹†âˆˆarg minwâˆˆRd L(w),
L(wt+1
GD ) âˆ’L(wâ‹†) â‰¤Î²
2
wt
PGD âˆ’wâ‹†2
2 âˆ’
wt+1
PGD âˆ’wâ‹†2
2

,
and hence
n
âˆ¥wt
PGD âˆ’wâ‹†âˆ¥2
2
o
is also a decreasing sequence.
3. For k â‰¥1, t â‰¥0, it holds that
L(wt+k
GD ) âˆ’L(wâ‹†) â‰¤Î²
2k
wt
PGD âˆ’wâ‹†2
2 .
20

B.4
Uniform convergence
The following result is shown in [87, Section 5.6].
Theorem B.1. Suppose that Ïˆ : [0, +âˆ) â†’[0, +âˆ) is a convex, non-decreasing function that
satisfies Ïˆ(x + y) â‰¥Ïˆ(x)Ïˆ(y). For any random variable X, we consider the Orlicz norm induced
by Ïˆ: âˆ¥Xâˆ¥Ïˆ := inf {K > 0 : EÏˆ(|X| /K)} â‰¤1.
Suppose that {XÎ¸}Î¸ is a zero-mean random process indexed by Î¸ âˆˆÎ˜ such that âˆ¥XÎ¸ âˆ’XÎ¸â€²âˆ¥Ïˆ â‰¤
Ï(Î¸, Î¸â€²) for some metric Ï on the space Î˜. Then it holds that
P
 
sup
Î¸,Î¸â€²âˆˆÎ˜
|XÎ¸ âˆ’XÎ¸â€²| â‰¤8(J + t)
!
â‰¤
1
Ïˆ(t/D) âˆ€t â‰¥0,
where D is the diameter of the metric space (Î˜, Ï), and the generalized Dudley entropy integral J is
given by
J :=
Z D
0
Ïˆâˆ’1(N(Î´; Î˜, Ï))dÎ´,
where N(Î´; Î˜, Ï) is the Î´-covering number of (Î˜, Ï).
As a corollary of Theorem B.1, we have the following result.
Proposition B.4 (Uniform concentration bound by chaining). Suppose that {XÎ¸}Î¸âˆˆÎ˜ is a zero-mean
random process given by
XÎ¸ := 1
N
N
X
i=1
f(zi; Î¸) âˆ’Ez[f(z; Î¸)],
where z1, Â· Â· Â· , zN are i.i.d samples from a distribution Pz such that the following assumption holds:
(a) The index set Î˜ is equipped with a distance Ï and diameter D. Further, assume that for
some constant A, for any ball Î˜â€² of radius r in Î˜, the covering number admits upper bound
log N(Î´; Î˜â€², Ï) â‰¤d log(2Ar/Î´) for all 0 < Î´ â‰¤2r.
(b) For any fixed Î¸ âˆˆÎ˜ and z sampled from Pz, the random variable f(z; Î¸) is a SG(B0)-sub-
Gaussian random variable.
(c) For any Î¸, Î¸â€² âˆˆÎ˜ and z sampled from Pz, the random variable f(z; Î¸) âˆ’f(z; Î¸â€²) is a
SG(B1Ï(Î¸, Î¸â€²))-sub-Gaussian random variable.
Then with probability at least 1 âˆ’Î´, it holds that
sup
Î¸âˆˆÎ˜
|XÎ¸| â‰¤CB0
r
d log(2AÎº) + log(1/Î´)
N
,
where C is a universal constant, and we denote Îº = 1 + B1D/B0.
Furthermore, if we replace the SG in assumption (b) and (c) by SE, then with probability at least
1 âˆ’Î´, it holds that
sup
Î¸âˆˆÎ˜
|XÎ¸| â‰¤CB0
"r
d log(2AÎº) + log(1/Î´)
N
+ d log(2AÎº) + log(1/Î´)
N
#
.
Proof. Fix a D0 âˆˆ(0, D] to be specified later. We pick a (D0/2)-covering Î˜0 of Î˜ so that
log |Î˜0| â‰¤d log(2AD/D0). Then, by the standard uniform covering of independent sub-Gaussian
random variables, we have with probability at least 1 âˆ’Î´/2,
sup
Î¸âˆˆÎ˜0
|XÎ¸| â‰¤CB0
r
d log(2AD/D0) + log(2/Î´)
N
.
21

Assume that Î˜0 = {Î¸1, Â· Â· Â· , Î¸n}. For each j âˆˆ[n], we consider Î˜j is the ball centered at Î¸j of radius
D0 in (Î˜, Ï). Then Î¸ âˆˆÎ˜j has diameter D0 and admits covering number bound log N(Î˜j, Î´) â‰¤
d log(AD0/Î´). Hence, we can apply Theorem B.1 with the process {XÎ¸}Î¸âˆˆÎ˜j, then
Ïˆ = Ïˆ2,
âˆ¥XÎ¸ âˆ’XÎ¸â€²âˆ¥Ïˆ â‰¤B1
âˆš
N
Ï(Î¸, Î¸â€²),
and a simple calculation yields
P
 
sup
Î¸,Î¸â€²âˆˆÎ˜j
|XÎ¸ âˆ’XÎ¸â€²| â‰¤Câ€²B1D0
 r
d log(2A)
N
+ t
!!
â‰¤2 exp(âˆ’Nt2) âˆ€t â‰¥0.
Therefore, we can let t â‰¤
p
log(2n/Î´)/N in the above inequality and taking the union bound over
j âˆˆ[n], and hence with probability at least 1 âˆ’Î´/2, it holds that for all j âˆˆ[n],
sup
Î¸,Î¸â€²âˆˆÎ˜j
|XÎ¸ âˆ’XÎ¸â€²| â‰¤Câ€²B1D0
r
2d log(2AD/D0) + log(4/Î´)
N
.
Notice that for each Î¸ âˆˆÎ˜, there exists j âˆˆ[n] such that Î¸ âˆˆÎ˜j, and hence
|XÎ¸| â‰¤
XÎ¸j
 +
XÎ¸ âˆ’XÎ¸j
 .
Thus, with probability at least 1 âˆ’Î´, it holds
sup
Î¸âˆˆÎ˜
|XÎ¸| â‰¤sup
Î¸âˆˆÎ˜0
|XÎ¸| + sup
j
sup
Î¸âˆˆÎ˜j
XÎ¸ âˆ’XÎ¸j
 â‰¤Câ€²â€²(B0 + B1D0)
r
d log(2AD/D0) + log(2/Î´)
N
.
Taking D0 = D/Îº completes the proof of SG case.
We next consider the SE case. The idea is the same as the SG case, but in this case we need to
consider the following Orlicz-norm:
ÏˆN(t) := exp
 Nt2
t + 1

âˆ’1.
Then Bernsteinâ€™s inequality of SE random variables yields
âˆ¥XÎ¸ âˆ’XÎ¸â€²âˆ¥ÏˆN â‰¤C0B1Ï(Î¸, Î¸â€²)
for some universal constant C0. Therefore, we can repeat the argument above to deduce that with
probability at least 1 âˆ’Î´, it holds
sup
Î¸âˆˆÎ˜
|XÎ¸| â‰¤Câ€²â€²(B0 + B1D0)
"r
d log(2AD/D0) + log(2/Î´)
N
+ d log(2AD/D0) + log(2/Î´)
N
#
.
Taking D0 = D/Îº completes the proof.
B.5
Useful properties of transformers
The following result can be obtained immediately by â€œjoiningâ€ the attention heads and MLP layers of
two single-layer transformers.
Proposition B.5 (Joining parallel single-layer transformers). Suppose that P1 : R(D0+D1)Ã—N â†’
RD1Ã—N, P2 : R(D0+D2)Ã—N â†’RD2Ã—N are two sequence-to-sequence functions that are implemented
by single-layer transformers, i.e. there exists Î¸1, Î¸2 such that
TFÎ¸1 :H1 =
"
h(0)
i
h(1)
i
#
1â‰¤iâ‰¤N
âˆˆR(D0+D1)Ã—N 7â†’

H(0)
P1(H1)

,
TFÎ¸2 :H2 =
"
h(0)
i
h(2)
i
#
1â‰¤iâ‰¤N
âˆˆR(D0+D2)Ã—N 7â†’

H(0)
P2(H2)

.
22

Then, there exists Î¸ such that for Hâ€² that takes form hâ€²
i = [h(0)
i ; h(1)
i ; h(2)
i ], with h(0)
i
âˆˆRD0, h(1)
i
âˆˆ
RD1, h(2)
i
âˆˆRD2, we have
TFÎ¸ :Hâ€² =
ï£®
ï£¯ï£°
h(0)
i
h(1)
i
h(2)
i
ï£¹
ï£ºï£»
1â‰¤iâ‰¤N
âˆˆR(D0+D1+D2)Ã—N 7â†’
ï£®
ï£°
H(0)
P1(H1)
P2(H2)
ï£¹
ï£».
Further, Î¸ has at most M â‰¤M1 + M2 heads, Dâ€² â‰¤Dâ€²
1 + Dâ€²
2 hidden dimension in its MLP layer,
and norm bound |||Î¸||| â‰¤|||Î¸1||| + |||Î¸2|||.
Proposition B.6 (Joining parallel multi-layer transformers). Suppose that P1 : R(D0+D1)Ã—N â†’
RD1Ã—N, P2 : R(D0+D2)Ã—N â†’RD2Ã—N are two sequence-to-sequence functions that are implemented
by multi-layer transformers, i.e. there exists Î¸1, Î¸2 such that
TFÎ¸1 :H1 =
"
h(0)
i
h(1)
i
#
1â‰¤iâ‰¤N
âˆˆR(D0+D1)Ã—N 7â†’

H(0)
P1(H1)

,
TFÎ¸2 :H2 =
"
h(0)
i
h(2)
i
#
1â‰¤iâ‰¤N
âˆˆR(D0+D2)Ã—N 7â†’

H(0)
P2(H2)

.
Then, there exists Î¸ such that for Hâ€² that takes form hâ€²
i = [h(0)
i ; h(1)
i ; h(2)
i ], with h(0)
i
âˆˆRD0, h(1)
i
âˆˆ
RD1, h(2)
i
âˆˆRD2, we have
TFÎ¸ :Hâ€² =
ï£®
ï£¯ï£°
h(0)
i
h(1)
i
h(2)
i
ï£¹
ï£ºï£»
1â‰¤iâ‰¤N
âˆˆR(D0+D1+D2)Ã—N 7â†’
ï£®
ï£°
H(0)
P1(H1)
P2(H2)
ï£¹
ï£».
Further, Î¸ has at most L â‰¤max {L1, L2} layers, maxâ„“âˆˆ[L] M (â„“) â‰¤maxâ„“âˆˆ[L]

M (â„“)
1
+ M (â„“)
2

heads, maxâ„“âˆˆ[L] D(â„“) â‰¤maxâ„“âˆˆ[L]

D(â„“)
1
+ D(â„“)
2

hidden dimension in its MLP layer (understanding
the size of the empty layers as 0), and norm bound |||Î¸||| â‰¤|||Î¸1||| + |||Î¸2|||.
Proof. When L1 = L2 (Î¸1 and Î¸2 have the same number of layers), the result follows directly by
applying Proposition B.5 repeatedly for all L1 layers and the definition of the norm (2).
If (without loss of generality) L1 < L2, we can augment Î¸1 to L2 layers by adding (L2 âˆ’L1) layers
with zero attention heads, and zero MLP hidden dimension (note that this does not change M1, Dâ€²
1,
and |||Î¸1|||). Due to the residual structure, the transformer maintains the output P1(H1) throughout
layer L1 + 1, . . . , L2, and it reduces to the case L1 = L2.
C
Extension to decoder-based architecture
Here we briefly discuss how our theoretical results can be adapted to decoder-based architectures
(henceforth decoder TFs). Adopting the setting as in Section 2, we consider a sequence of N input
vectors {hi}N
i=1 âŠ‚RD, written compactly as an input matrix H = [h1, . . . , hN] âˆˆRDÃ—N. Recall
that Ïƒ(t) := ReLU(t) = max {t, 0} denotes the standard relu activation.
C.1
Decoder-based transformers
Decoder TFs are the same as encoder TFs, except that the attention layers are replaced by masked
attention layers with a specific decoder-based (causal) attention mask.
Definition C.1 (Masked attention layer). A masked attention layer with M heads is denoted as
MAttnÎ¸(Â·) with parameters Î¸ = {(Vm, Qm, Km)}mâˆˆ[M] âŠ‚RDÃ—D. On any input sequence
H âˆˆRDÃ—Nâ€² with N â€² â‰¤N,
eH = MAttnÎ¸(H) := H + PM
m=1(VmH) Ã—

(MSK1:Nâ€²,1:Nâ€²) â—¦Ïƒ
 (QmH)âŠ¤(KmH)

âˆˆRDÃ—Nâ€²,
(10)
23

where â—¦denotes the entry-wise (Hadamard) product of two matrices, and MSK âˆˆRNÃ—N is the mask
matrix given by
MSK =
ï£®
ï£¯ï£¯ï£¯ï£°
1
1/2
1/3
Â· Â· Â·
1/N
0
1/2
1/3
Â· Â· Â·
1/N
0
0
1/3
Â· Â· Â·
1/N
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
0
Â· Â· Â·
1/N
ï£¹
ï£ºï£ºï£ºï£».
In vector form, we have
ehi = [AttnÎ¸(H)]i = hi + PM
m=1
1
i
Pi
j=1 Ïƒ(âŸ¨Qmhi, KmhjâŸ©) Â· Vmhj.
Notice that standard masked attention definitions use the pre-activation additive masks (with mask
value âˆ’âˆ) [84]. The post-activation multiplicative masks we use is equivalent to the pre-activation
additive masks, and the modified presentation is for notational convenience. We also use a nor-
malized ReLU activation t 7â†’Ïƒ(t)/i in place of the standard softmax activation to be consis-
tent with Definition 1. Note that the normalization 1/i is to ensure that the attention weights
{Ïƒ(âŸ¨Qmhi, KmhjâŸ©)/i}jâˆˆ[i] is a set of non-negative weights that sum to O(1). The motivation of
masked attention layer is to ensure that, when processing a sequence of tokens, the computations at
any token do not see any later token.
We next define the decoder-based transformers with L â‰¥1 transformer layers, each consisting of
a masked attention layer (c.f. Definition C.1) followed by an MLP layer (c.f. Definition 2). This
definition is similar to the definition of encoder-based transformers (c.f., Definition 3), except that we
replace the attention layers by masked attention layers.
Definition C.2 (Decoder-based Transformer). An L-layer decoder-based transformer, denoted as
DTFÎ¸(Â·), is a composition of L self-attention layers each followed by an MLP layer: H(L) =
DTFÎ¸(H(0)), where H(0) âˆˆRDÃ—N is the input sequence, and
H(â„“) = MLPÎ¸(â„“)
mlp

MAttnÎ¸(â„“)
mattn
 H(â„“âˆ’1)
,
â„“âˆˆ{1, . . . , L}.
Above, the parameter Î¸ = (Î¸(1:L)
mattn, Î¸(1:L)
mlp ) is the parameter consisting of the attention layers
Î¸(â„“)
mattn = {(V(â„“)
m , Q(â„“)
m , K(â„“)
m )}mâˆˆ[M (â„“)] âŠ‚RDÃ—D and the MLP layers Î¸(â„“)
mlp = (W(â„“)
1 , W(â„“)
2 ) âˆˆ
RD(â„“)Ã—D Ã— RDÃ—D(â„“). We will frequently consider â€œattention-onlyâ€ decoder-based transformers with
W(â„“)
1 , W(â„“)
2
= 0, which we denote as DTF0
Î¸(Â·) for shorthand, with Î¸ = Î¸(1:L) := Î¸(1:L)
mattn.
We also use (2) to define the norm of DTFÎ¸.
C.2
In-context learning with decoder-based transformers
We consider using decoder-based TFs to perform ICL. We encode (D, xN+1), which follows the
generating rule as described in Section 2.2, into an input sequence H âˆˆRDÃ—(2N+1). In our theory,
we use the following format, where the first two rows contain (D, xN+1) which alternating between
[xi; 0] âˆˆRd+1 and [0dÃ—1; yi] âˆˆRd+1 (the same setup as adopted in [31, 2]); The third row contains
fixed vectors {pi}iâˆˆ[N+1] with ones, zeros, the example index, and indicator for being the covariate
token (similar to a positional encoding vector):
H =
"x1
0
. . .
xN
0
xN+1
0
y1
. . .
0
yN
0
p1
p2
. . .
p2Nâˆ’1
p2N
p2N+1
#
,
pi :=
ï£®
ï£¯ï£°
0Dâˆ’(d+4)
âŒˆi/2âŒ‰
1
mod(i + 1, 2)
ï£¹
ï£ºï£»âˆˆRDâˆ’(d+1).
(11)
(11) is different from out input format (3) for encoder-based TFs. The main difference is that (xi, yi)
are in different tokens in (11), whereas (xi, yi) are in the same token in (3). The reason for the former
(i.e., different tokens in decoder) is that we want to avoid every [xi; 0] token seeing the information
of yi, since we will evaluate the loss at every token. The reason for the latter (i.e., the same token in
encoder) is for presentation convenience: since we only evaluate the loss at the last token, it is not
necessary to alternate between [xi; 0] and [0; yi] to avoid information leakage.
24

We then feed H into a decoder TF to obtain the output eH = DTFÎ¸(H) âˆˆRDÃ—(2N+1) with the same
shape, and read out the prediction byN+1 from the (d + 1, 2N + 1)-th entry of eH = [ehi]iâˆˆ[2N+1] (the
entry corresponding to the last missing test label): byN+1 = ready( eH) := (eh2N+1)d+1. The goal is
to predict byN+1 that is close to yN+1 âˆ¼Py|xN+1 measured by proper losses.
The benefit of using the decoder architecture is that, during the pre-training phase, one can construct
the training loss function by using all the predictions {byj}jâˆˆ[N+1], where byj gives the (d+1, 2j âˆ’1)-
th entry of eH = [ehi]iâˆˆ[2N+1] for each j âˆˆ[N + 1] (the entry corresponding to the missing test label
of the 2j âˆ’1â€™th token): byj = ready,j( eH) := (eh2jâˆ’1)d+1. Given a loss function â„“: R Ã— R â†’R
associated to a single response, the training loss associated to the whole input sequence can be defined
by â„“(H) = PN+1
j=1 â„“(yj, byj). This potentially enables less training sequences in the pre-training stage,
and some generalization bound analysis justifying this benefit was provided in [47].
C.3
Results
We discuss how our theoretical results upon encoder TFs can be converted to those of the decoder TFs.
Taking the implementation of (ICGD) (a key mechanism that enables most basic ICL algorithms such
as ridge regression; cf. Appendix D.1) as an example, this conversion is enabled by the following
facts: (a) the input format (11) of decoders can be converted to the input format (3) of encoders by a
2-layer decoder TF; (b) the encoder TF that implements (ICGD) with input format (3), by a slight
parameter modification, can be converted to a decoder TF that implements the (ICGD) algorithm
with a converted input format.
Input format conversion
Despite the difference between the input format (11) and (3), we show
that there exists a 2-layer decoder TF that can convert the input format (11) to format (3). The proof
can be found in Appendix C.4.
Proposition C.1 (Input format conversion). There exists a 2-layer decoder TF DTF with 3 heads
per layer, hidden dimension 2 and |||Î¸||| â‰¤12 such that upon taking input H of format (11), it outputs
eH = DTF(H) with
eH =
"x1
x1
. . .
xN
xN
xN+1
0
y1
. . .
0
yN
0
p1
p2
. . .
p2Nâˆ’1
p2N
p2N+1
#
.
(12)
In particular, format (12) contains format (3) as a submatrix, by restricting to the {1, 2, . . . , D âˆ’
1, D âˆ’2, D} rows and {2, 4, . . . , 2N âˆ’2, 2N, 2N + 1} columns.
Generalization TF constructions to decoder architecture
The construction in Theorem D.1 can
be generalized to using the input format (12) along with a decoder TF, by using the scratch pad within
the last token to record the gradient descent iterates. Further, if we slightly change the normalization
in MSK from 1/i to 1/((i âˆ’1) âˆ¨1), then the same construction performs (ICGD) (with training
examples {1, . . . , j}) at every token i = 2j + 1 (corresponding to predicting at xj+1). Building on
this extension, all our constructions in Section 3 and Section 4.2 can be generalized to decoder TFs.
C.4
Proof of Proposition C.1
For the simplicity of presentation, we write ci = âŒˆi/2âŒ‰, ti = mod(i + 1, 2), ui = hi[1 : d] âˆˆRd+1
be the vector of first d entries of hi 6, and let vi = hi[d + 1] be the (d + 1)-th entry of hi. With such
notations, the input sequence H = [hi]i can be compactly written as
hi = [ui; vi; 0Dâˆ’dâˆ’4; ci; 1; ti].
In the following, we construct the desired Î¸ = (Î¸(1), Î¸(2)) as follows.
Step 1: construction of Î¸(1) = (Î¸(1)
mattn, Î¸(1)
mlp), so that MLPÎ¸(1)
mlp â—¦MAttnÎ¸(1)
mattn maps
hi
MAttn
Î¸(1)
mattn
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’
hâ€²
i = [ui; vi; 0Dâˆ’dâˆ’6; ti(c2
i + 0.5); tici; ci; 1; ti]
6In other words, when 2 âˆ¤i, ui = x(iâˆ’1)/2; when 2 | i, ui = 0d.
25

MLP
Î¸(1)
mlp
âˆ’âˆ’âˆ’âˆ’âˆ’â†’
h(1)
i
= [ui; vi; 0Dâˆ’dâˆ’6; tic2
i ; tici; ci; 1; ti].
For m âˆˆ{0, 1}, we define matrices Q(1)
m , K(1)
m , V(1)
m âˆˆRDÃ—D such that
Q(1)
0 hi = Q(1)
1 hi =

ti
0

,
K(1)
0 hj = K(1)
1 hj =

cj
0

,
V(1)
0 hj =
"0Dâˆ’4
3cj
03
#
,
V(1)
1 hj =
"0Dâˆ’3
2
02
#
,
for all i, j. By the structure of hi, these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
m
Q(1)
m

op â‰¤1,
max
m
K(1)
m

op â‰¤1,
X
m
V(1)
m

op â‰¤5.
Now, for every i,
1
i
i
X
j=1
X
mâˆˆ{0,1}
Ïƒ
D
Q(1)
m hi, K(1)
m hj
E
V(1)
m hj = 1
i
i
X
j=1
ti Â· [0Dâˆ’4; 3c2
j; 2cj; 0; 0].
Notice that ti Ì¸= 0 only when 2 | i, we then compute for i = 2k that
i
X
j=1
3c2
j = 3 Â· k(k âˆ’1)(2k âˆ’1)
3
+ 3k2 = 2k3 + k,
i
X
j=1
2cj = 2 Â· k(k âˆ’1) + 2k = 2k2.
Therefore, the Î¸(1)
mattn = {(Q(1)
m , K(1)
m , V(1)
m âˆˆRDÃ—D)}mâˆˆ{0,1} we construct above is indeed the
desired attention layer. The existence of the desired Î¸(1)
mlp is clear, and Î¸(1)
mlp = (W(1)
1 , W(1)
2 ) can
further be chosen so that âˆ¥W(1)
1 âˆ¥op â‰¤1, âˆ¥W(1)
2 âˆ¥op â‰¤1.
Step 2: construction of Î¸(2). For every m âˆˆ{âˆ’1, 0, 1}, we define matrices Q(2)
m , K(2)
m , V(2)
m âˆˆ
RDÃ—D such that
Q(2)
0 h(1)
i
= Q(2)
1 h(1)
i
= Q(2)
âˆ’1h(1)
i
=
ï£®
ï£°
tic2
i
tici
0
ï£¹
ï£»,
K(2)
0 h(1)
j
=
" 1
âˆ’cj
0
#
,
K(2)
1 h(1)
j
=
"
1
âˆ’(cj + 1)
0
#
,
K(2)
1 h(1)
j
=
"
1
âˆ’(cj âˆ’1)
0
#
,
V(2)
0 h(1)
j
=

âˆ’4uj
0Dâˆ’d

,
V(2)
1 h(1)
j
= V(2)
âˆ’1h(1)
j
=

2uj
0Dâˆ’d

,
for all i, j. By the structure of h(1)
i , these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
m
Q(2)
m

op â‰¤1,
max
m
K(2)
m

op â‰¤2,
X
m
V(2)
m

op â‰¤8.
Now, for every i, j, we have
X
mâˆˆ{âˆ’1,0,1}
Ïƒ
D
Q(2)
m h(1)
i , K(2)
m h(1)
j
E
V(2)
m h(1)
j
=

âˆ’2Ïƒ
 tic2
i âˆ’ticicj

+ Ïƒ
 tic2
i âˆ’tici(cj + 1)

+ Ïƒ
 tic2
i âˆ’tici(cj âˆ’1)
	
Â· 2[uj; 0Dâˆ’d]
= {âˆ’2Ïƒ(ci âˆ’cj) + Ïƒ((ci âˆ’cj) âˆ’1) + Ïƒ((ci âˆ’cj) + 1)} Â· 2citi[uj; 0Dâˆ’d]
= I(ci = cj) Â· 2citi[uj; 0Dâˆ’d],
where the last equality follows from the fact that
âˆ’2Ïƒ(x) + Ïƒ(x âˆ’1) + Ïƒ(x + 1) =
ï£±
ï£²
ï£³
0,
x â‰¥1 or x â‰¤âˆ’1,
x + 1,
x âˆˆ[âˆ’1, 0],
1 âˆ’x,
x âˆˆ[0, 1].
26

Therefore,
1
i
i
X
j=1
X
mâˆˆ{âˆ’1,0,1}
Ïƒ
D
Q(2)
m h(1)
i , K(2)
m h(1)
j
E
V(2)
m h(1)
j
= 1
i
i
X
j=1
2I(ci = cj)citi[uj; 0Dâˆ’d]
=
[xk; 0Dâˆ’d],
i = 2k
0D,
otherwise .
Therefore, the Î¸(2)
mattn = {(Q(2)
m , K(2)
m , V(2)
m âˆˆRDÃ—D)}mâˆˆ{âˆ’1,0,1} we construct above maps
h(1)
i
â†’
hâ€²â€²
i = [xâŒˆi/2âŒ‰; vi; 0Dâˆ’dâˆ’6; tic2
i ; tici; ci; 1; ti].
Finally, we only need to take a MLP layer Î¸(2)
mlp = (W(2)
1 , W(2)
2 ) with hidden dimension 2 that maps
hâ€²â€²
i
â†’
h(2)
i
= [xâŒˆi/2âŒ‰; vi; 0Dâˆ’dâˆ’6; 0; 0; ci; 1; ti],
which clearly exists and can be chosen so that âˆ¥W(2)
1 âˆ¥op â‰¤1, âˆ¥W(2)
2 âˆ¥op â‰¤1.
Combining the two steps above, we complete the proof of Proposition C.1.
D
Mechanism: In-context gradient descent
Technically, the constructions in Section 3.1-3.2 rely on a new efficient construction for transformers
to implement in-context gradient descent and its variants, which we present as follows. We begin by
presenting the result for implementing (vanilla) gradient descent on convex empirical risks.
Compact notation of input
We will often use shorthand yâ€²
i âˆˆR defined as yâ€²
i = yi for i âˆˆ[N] and
yâ€²
N+1 = 0 to simplify our notation, with which the input sequence H âˆˆRDÃ—(N+1) can be compactly
written as hi = [xi; yâ€²
i; pi] = [xi; yâ€²
i; 0Dâˆ’dâˆ’3; 1; ti] for i âˆˆ[N + 1], where ti := 1{i < N + 1} is
the indicator for the training examples.
D.1
Gradient descent on convex empirical risk
Let â„“(Â·, Â·) : R2 â†’R be a loss function. Let bLN(w) :=
1
N
PN
i=1 â„“(wâŠ¤xi, yi) denote the empirical
risk with loss function â„“on dataset {(xi, yi)}iâˆˆ[N], and
wt+1
GD := wt
GD âˆ’Î·âˆ‡bLN(wt
GD)
(ICGD)
denote the gradient descent trajectory on bLN with initialization w0
GD âˆˆRd and learning rate Î· > 0.
We require the partial derivative of the loss âˆ‚sâ„“: (s, t) 7â†’âˆ‚sâ„“(s, t) (as a bivariate function) to be
approximable by a sum of relus, defined as follows.
Definition D.1 (Approximability by sum of relus). A function g : Rk â†’R is (Îµapprox, R, M, C)-
approximable by sum of relus, if there exists a â€œ(M, C)-sum of relusâ€ function
fM,C(z) = PM
m=1 cmÏƒ(aâŠ¤
m[z; 1]) with
PM
m=1 |cm| â‰¤C, maxmâˆˆ[M] âˆ¥amâˆ¥1 â‰¤1, am âˆˆRk+1, cm âˆˆR,
such that supzâˆˆ[âˆ’R,R]k |g(z) âˆ’fM,C(z)| â‰¤Îµapprox.
Definition D.1 is known to contain broad class of functions. For example, any mildly smooth k-
variate function is approximable by a sum of relus for any (Îµapprox, R), with mild bounds on (M, C)
(Proposition B.1, building on results of Bach [4]). Also, any function that is a (M, C)-sum of relus
itself (which includes all piecewise linear functions) is by definition (0, âˆ, M, C)-approximable by
sum of relus.
We show that L steps of (ICGD) can be approximately implemented by an (L + 1)-layer transformer.
Theorem D.1 (Convex ICGD). Fix any Bw > 0, L > 1, Î· > 0, and Îµ â‰¤Bw/(2L). Suppose that
1. The loss â„“(Â·, Â·) is convex in the first argument;
2. âˆ‚sâ„“is (Îµ, R, M, C)-approximable by sum of relus with R = max {BxBw, By, 1}.
27

=
+
1
N
N
âˆ‘
i=1
M
âˆ‘
m=1 Ïƒ ( âŸ¨
,
âŸ©) Ã—
ht+1
N+1
Qm
Vm
ht
N+1
ht
N+1
ht
i
ht
i
Km
am Ïƒ ( bm âŸ¨
,
âŸ©+ cm
) Ã—
=
âˆ’
Î·
N
N
âˆ‘
i=1
M
âˆ‘
m=1
wt+1
xi
yi
wt
xi
yi
xi
wt
yi
xi
0
0
âˆ‚s â„“( âŸ¨
,
âŸ©,
) Ã—
=
âˆ’
Î·
N
N
âˆ‘
i=1
wt+1
wt
xi
wt
yi
xi
âˆ‚sâ„“(s, t) = âˆ‘M
m=1 am â‹…Ïƒ( bm â‹…s + cm â‹…t )
Attention
Gradient 
descent
Weight 
construction
Universal 
approximation
ht
i
wt
xi
yi
=
Vm
xi
0
0
= âˆ’Î·am Ã—
wt
xi
yi
,
,
,
xi
Km
=
wt
xi
yi
yi
Qm
=
wt*
wt
1
bm Ã—
*
cm Ã—
Figure 4: Illustration of our main mechanism for implementing basic ICL algorithms: One attention layer
implements a single (ICGD) iterate (Proposition E.1 & Theorem D.1). Top: the attention mechanism as in
Definition 1. Bottom: A single (ICGD) iterate. Middle: Linear algebraic illustration of the attention layer for
implementing a GD update.
Then, there exists an attention-only transformer TF0
Î¸ with (L + 1) layers, maxâ„“âˆˆ[L] M (â„“) â‰¤M
heads within the first L layers, and M (L+1) = 2 such that for any input data (D, xN+1) such that
sup
âˆ¥wâˆ¥2â‰¤Bw
Î»max(âˆ‡2bLN(w)) â‰¤2/Î·,
âˆƒwâ‹†âˆˆarg min
wâˆˆRd
bLN(w) such that âˆ¥wâ‹†âˆ¥2 â‰¤Bw/2,
TF0
Î¸(H(0)) approximately implements (ICGD) with initialization w0
GD = 0:
1. (Parameter space) For every â„“âˆˆ[L], the â„“-th layerâ€™s output H(â„“) = TF0
Î¸(1:â„“)(H(0)) approx-
imates â„“steps of (ICGD): We have h(â„“)
i
= [xi; yâ€²
i; bwâ„“; 0Dâˆ’2dâˆ’3; 1; ti] for every i âˆˆ[N + 1],
where
bwâ„“âˆ’wâ„“
GD

2 â‰¤Îµ Â· (â„“Î·Bx).
Note that the bound scales as O(â„“), a linear error accumulation.
2. (Prediction space) The final output H(L+1) = TF0
Î¸(H(0)) approximates the prediction of L steps
of (ICGD): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0Dâˆ’2dâˆ’3; 1; ti], where byN+1 =

bwL, xN+1

so that
byN+1 âˆ’

wL
GD, xN+1
 â‰¤Îµ Â· (LÎ·B2
x).
Further, the transformer admits norm bound |||Î¸||| â‰¤2 + R + 2Î·C.
The proof can be found in Appendix E.2. Theorem D.1 substantially generalizes that of von Oswald
et al. [86] (which only does GD on square losses with a linear self-attention), and is simpler than the
ones in AkyÃ¼rek et al. [2] and Giannou et al. [32]. See Figure 4 for a pictorial illustration of the basic
component of the construction, which implements a single step of gradient descent using a single
attention layer (Proposition E.1).
Technically, we utilize the stability of convex gradient descent as in the following lemma (proof
in Appendix E.3) to obtain the linear error accumulation in Theorem D.1; the error accumulation
will become exponential in L in the non-convex case in general; see Lemma D.3(b).
Lemma D.1 (Composition of error for approximating convex GD). Suppose f : Rd â†’R is a convex
function. Let wâ‹†âˆˆarg minwâˆˆRd f(w), R â‰¥2âˆ¥wâ‹†âˆ¥2, and assume that âˆ‡f is Lf-smooth on Bd
2(R).
Let sequences {bwâ„“}â„“â‰¥0 âŠ‚Rd and {wâ„“
GD}â„“â‰¥0 âŠ‚Rd be given by bw0 = w0
GD = 0,
(
bwâ„“+1 = bwâ„“âˆ’Î·âˆ‡f(bwâ„“) + Îµâ„“,
âˆ¥Îµâ„“âˆ¥2 â‰¤Îµ,
wâ„“+1
GD = wâ„“
GD âˆ’Î·âˆ‡f(wâ„“
GD),
for all â„“â‰¥0. Then as long as Î· â‰¤2/Lf, for any 0 â‰¤L â‰¤R/(2Îµ), it holds that
bwL âˆ’wL
GD

2 â‰¤
LÎµ and âˆ¥bwLâˆ¥2 â‰¤R
2 + LÎµ â‰¤R.
28

D.2
Proximal gradient descent for regularized convex losses
Proximal gradient descent (PGD) is a variant of gradient descent that is suitable for minimizing
regularized risks [66], in particular those with a non-smooth regularizer such as the â„“1 norm. In this
section, we show that transformers can approximate PGD with similar quantitative guarantees as for
GD in Appendix D.1.
Let â„“(Â·, Â·) : R2 â†’R be a loss function. Let bLN(w) :=
1
N
PN
i=1 â„“(wâŠ¤xi, yi) + R(w) denote
the regularized empirical risk with loss function â„“on dataset {(xi, yi)}iâˆˆ[N] and regularizer R.
To minimize bLN, we consider the proximal gradient descent trajectory on bLN with initialization
w0
GD = 0 âˆˆRd and learning rate Î· > 0:
wt+1
PGD := proxÎ·R

wt
PGD âˆ’Î·âˆ‡bL0
N(wt
PGD)

,
(ICPGD)
where we denote bL0
N(w) := 1
N
PN
i=1 â„“(wâŠ¤xi, yi).
To approximate (ICPGD) by transformers, in addition to the requirement on the loss â„“as in Theo-
rem D.1, we additionally require the the proximal operator proxÎ·R(Â·) to be approximable by an
MLP layer (as a vector-valued analog of Definition D.1) defined as follows.
Definition D.2 (Approximability by MLP). An operator P : Rd â†’Rd is (Îµ, R, D, C)-approximable
by MLP, if there exists a there exists a MLP Î¸mlp = (W1, W2) âˆˆRDÃ—d Ã— RdÃ—D with hidden
dimension D, âˆ¥W1âˆ¥op + âˆ¥W2âˆ¥op â‰¤Câ€², such that supâˆ¥wâˆ¥2â‰¤R
P(w) âˆ’MLPÎ¸mlp(w)

2 â‰¤Îµ.
The definition above captures the proximal operator proxÎ·R for a broad class of regularizers, such
as the (commonly-used) L1 and L2 regularizer listed in the following proposition, for all of which
one can directly check that they can be exactly implemented by an MLP as stated below.
Proposition D.1 (Proximal operators for commonly-used regularizers). For regularizer R in
{Î» âˆ¥Â·âˆ¥1 , Î»
2 âˆ¥Â·âˆ¥2
2 , IBâˆ(B)(Â·)}, the operator proxÎ·R : Rd â†’Rd is exactly approximable by MLP.
More concretely, we have
1. For R = Î» âˆ¥Â·âˆ¥1, proxÎ·R is (0, +âˆ, 4d, 4 + 2Î·Î»)-approximable by MLP.
2. For R = Î»
2 âˆ¥Â·âˆ¥2
2, proxÎ·R is (0, +âˆ, 2d, 2 + 2Î·Î»)-approximable by MLP.
3. For R = IBâˆ(B)(Â·), proxÎ·R = ProjBâˆ(B) is (0, +âˆ, 2d, 2 + 2B)-approximable by MLP.
Theorem D.2 (Convex ICPGD). Fix any Bw > 0, L > 1, Î· > 0, and Îµ + Îµâ€² â‰¤Bw/(2L). Suppose
that
1. The loss â„“(Â·, Â·) is convex in the first argument;
2. âˆ‚sâ„“is (Îµ, R, M, C)-approximable by sum of relus with R = max {BxBw, By, 1}.
3. R convex, and the proximal operator proxÎ·R(w) is (Î·Îµâ€², Râ€², Dâ€², Câ€²)-approximable by MLP
with Râ€² = supâˆ¥wâˆ¥2â‰¤Bw
w+
Î·

2 + Î·Îµ.
Then there exists a transformer TFÎ¸ with (L + 1) layers, maxâ„“âˆˆ[L] M (â„“) â‰¤M heads within the first
L layers, M (L+1) = 2, and hidden dimension Dâ€² such that, for any input data (D, xN+1) such that
sup
âˆ¥wâˆ¥2â‰¤Bw
Î»max(âˆ‡2bLN(w)) â‰¤2/Î·,
âˆƒwâ‹†âˆˆarg min
wâˆˆRd
bLN(w) such that âˆ¥wâ‹†âˆ¥2 â‰¤Bw/2,
TFÎ¸(H(0)) approximately implements (ICGD):
1. (Parameter space) For every â„“âˆˆ[L], the â„“-th layerâ€™s output H(â„“) = TFÎ¸(1:â„“)(H(0)) approx-
imates â„“steps of (ICGD): We have h(â„“)
i
= [xi; yâ€²
i; bwâ„“; 0Dâˆ’2dâˆ’3; 1; ti] for every i âˆˆ[N + 1],
where
bwâ„“âˆ’wâ„“
PGD

2 â‰¤(Îµ + Îµâ€²) Â· (LÎ·Bx).
2. (Prediction space) The final output H(L+1) = TFÎ¸(H(0)) approximates the prediction of L steps
of (ICGD): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0Dâˆ’2dâˆ’3; 1; ti], where byN+1 =

bwL, xN+1

so that
byN+1 âˆ’

wL
PGD, xN+1
 â‰¤(Îµ + Îµâ€²) Â· (2LÎ·B2
x).
29

Further, the weight matrices have norm bounds |||Î¸||| â‰¤3 + R + 2Î·C + Câ€².
The proof of Theorem D.2 is essentially similar to the proof of Theorem D.1, using the following
generalized version of Lemma D.1.
Lemma D.2 (Composition of error for approximating convex PGD). Suppose f : Rd â†’R is a
convex function and R is a convex regularizer. Let wâ‹†âˆˆarg minwâˆˆRd f(w) + R(w), R â‰¥2âˆ¥wâ‹†âˆ¥2,
and assume that âˆ‡f is Lf-smooth on Bd
2(R). Let sequences {bwâ„“}â„“â‰¥0 âŠ‚Rd and {wâ„“
GD}â„“â‰¥0 âŠ‚Rd
be given by bw0 = w0
GD = 0,
(
bwâ„“+1 = proxÎ·R
 bwâ„“âˆ’Î·âˆ‡f(bwâ„“)

+ Îµâ„“,
âˆ¥Îµâ„“âˆ¥2 â‰¤Îµ,
wâ„“+1
GD = proxÎ·R
 wâ„“
GD âˆ’Î·âˆ‡f(wâ„“
GD)

,
for all â„“â‰¥0. Then as long as Î· â‰¤2/Lf, for any 0 â‰¤L â‰¤R/(2Îµ), it holds that
bwL âˆ’wL
GD

2 â‰¤
LÎµ and âˆ¥bwLâˆ¥2 â‰¤R
2 + LÎµ â‰¤R.
The proof of the above lemma is done by utilizing the non-expansiveness of the PGD operator
w 7â†’proxÎ·R(w âˆ’Î·âˆ‡f(w)) and otherwise following the same arguments as for Lemma D.1.
D.3
Gradient descent on two-layer neural networks
We now move beyond the convex setting by showing that transformers can implement gradient
descent on two-layer neural networks in context.
Suppose that the prediction function pred(x; w) := PK
k=1 ukr(vâŠ¤
k x) is given by a two-layer neural
network, parameterized by w = [vk; uk]kâˆˆ[K] âˆˆRK(d+1). Consider the empirical risk minimization
problem:
min
wâˆˆW
bLN(w) :=
1
2N
N
X
i=1
â„“(pred(xi; w), yi) =
1
2N
N
X
i=1
â„“
 K
X
k=1
ukr(vâŠ¤
k xi), yi
!
,
(13)
where W is a bounded domain. For the sake of simplicity, in the following discussion we assume that
ProjW can be exactly implemented by a MLP layer (e.g. W = Bâˆ(Rw) for some Rw > 0).
Theorem D.3 (Approximate ICGD on two-layer NNs). Fix any Bv, Bu > 0, L â‰¥1, Î· > 0, and
Îµ > 0. Suppose that
1. Both the activation function r and the loss function â„“is C4-smooth;
2. W is a closed domain such that W âŠ‚{w = [vk; uk]kâˆˆ[K] âˆˆRK(d+1) : âˆ¥vkâˆ¥2 â‰¤Bv, |uk| â‰¤
Bu}, and ProjW = MLPÎ¸mlp for some MLP layer Î¸mlp with hidden dimension Dw and
Î¸mlp
 â‰¤Cw;
Then there exists a (2L)-layer transformer TFÎ¸ with
max
â„“âˆˆ[2L] M (â„“) â‰¤e
O
 Îµâˆ’2
,
max
â„“âˆˆ[2L] D(â„“) â‰¤e
O
 Îµâˆ’2
+ Dw,
|||Î¸||| â‰¤O (1 + Î·) + Cw,
where O (Â·) hides the constants that depend on K, the radius parameters Bx, By, Bu, Bv and the
smoothness of r and â„“, such that for any input data (D, xN+1) such that input sequence H(0) âˆˆ
RDÃ—(N+1) takes form (3), TFÎ¸(H(0)) approximately implements in-context gradient descent on
risk (13): For every â„“âˆˆ[L], the 2â„“-th layerâ€™s output h(2â„“)
i
= [xi; yâ€²
i; bwâ„“; 0; 1; ti] for every i âˆˆ[N +1],
and
bwâ„“= ProjW

bwâ„“âˆ’1 âˆ’Î·(âˆ‡bLN(bwâ„“âˆ’1) + Îµâ„“âˆ’1)

,
bw0 = 0,
(14)
where
Îµâ„“âˆ’1
2 â‰¤Îµ is an error term.
As a direct corollary, the transformer constructed above can approximate the true gradient descent
trajectory {wâ„“
GD}â„“â‰¥0 on (16), defined as w0
GD = 0 and wâ„“+1
GD = wâ„“
GD âˆ’Î·âˆ‡bLN(wâ„“
GD) for all â„“â‰¥0.
30

Corollary D.1 (Approximating multi-step ICGD on two-layer NNs). For any L â‰¥1, under the
same setting as Theorem D.3, the (2L)-layer transformer TFÎ¸ there approximates the true gradient
descent trajectory {wâ„“
GD}â„“â‰¥0: For the intermediate iterates {bwâ„“}â„“âˆˆ[L] considered therein, we have
bwâ„“âˆ’wâ„“
GD

2 â‰¤Lâˆ’1
f (1 + Î·Lf)â„“Îµ,
where Lf = supwâˆˆW
âˆ‡2bLN(w)

op denotes the smoothness of bLN within W.
Remark on error accumulation
Note that in Corollary D.1, the error accumulates exponentially
in â„“rather than linearly as in Theorem D.1. This is as expected, since gradient descent on non-convex
objectives is inherently unstable at a high level (a slight error added upon each step may result in a
drastically different trajectories); technically, this happens as the stability-like property Lemma D.1
no longer holds for the non-convex case.
Corollary D.1 is a simple implication of Theorem D.3 and Part (b) of the following convergence
and trajectory closeness result for inexact gradient descent. For any closed convex set W âŠ‚Rd, any
function f : W â†’R, and any initial point w âˆˆW, let
Gf
W,Î·(w) := w âˆ’ProjW(w âˆ’Î·âˆ‡f(w))
Î·
denote the gradient mapping at w with step size Î·, a standard measure of stationarity in constrained
optimization [63]. Note that Gf
W,Î·(w) = âˆ‡f(w) when w âˆ’Î·âˆ‡f(w) âˆˆW (so that the projection
does not take effect).
Lemma D.3 (Convergence and trajectory closeness of inexact GD). Suppose f : W â†’R, where
W âŠ‚Rd is a convex closed domain and âˆ‡f is Lf-Lipschitz on W. Let sequence {bwâ„“}â„“â‰¥0 âŠ‚Rd be
given by bw0 = w0,
bwâ„“+1 = ProjW
 bwâ„“âˆ’Î·(âˆ‡f(bwâ„“) + Îµâ„“)

,
âˆ¥Îµâ„“âˆ¥2 â‰¤Îµ,
for all â„“â‰¥0. Then the following holds.
(a) As long as Î· â‰¤1/Lf, for all L â‰¥1,
min
â„“âˆˆ[Lâˆ’1]
Gf
W,Î·(bwâ„“)

2
2 â‰¤1
L
Lâˆ’1
X
â„“=0
Gf
W,Î·(bwâ„“)

2
2 â‰¤8(f(w0) âˆ’infwâˆˆW f(w))
Î·L
+ 10Îµ2.
(b) Let the sequences {wâ„“
GD}â„“â‰¥0 âŠ‚Rd and be given by w0
GD = w0 and wâ„“+1
GD
=
ProjW(wâ„“
GD âˆ’Î·âˆ‡f(wâ„“
GD)). Then it holds that
bwâ„“âˆ’wâ„“
GD

2 â‰¤Lâˆ’1
f (1 + Î·Lf)â„“Îµ,
âˆ€â„“â‰¥0.
E
Proofs for Section D
E.1
Approximating a single GD step
Proposition E.1 (Approximating a single GD step by a single attention layer). Let â„“(Â·, Â·) :
R2 â†’R be a loss function such that âˆ‚1â„“is (Îµ, R, M, C)-approximable by sum of relus with
R = max{BxBw, By, 1}. Let bLN(w) := 1
N
PN
i=1 â„“(wâŠ¤xi, yi) denote the empirical risk with loss
function â„“on dataset {(xi, yi)}iâˆˆ[N].
Then, for any Îµ > 0, there exists an attention layer Î¸ = {(Qm, Km, Vm)}mâˆˆ[M] with M heads
such that, for any input sequence that takes form hi = [xi; yâ€²
i; w; 0Dâˆ’2dâˆ’3; 1; ti] with âˆ¥wâˆ¥2 â‰¤Bw,
it gives output ehi = [AttnÎ¸(H)]i = [xi; yâ€²
i; ew; 0Dâˆ’2dâˆ’3; 1; ti] for all i âˆˆ[N + 1], where
ew âˆ’(w âˆ’Î·âˆ‡bLN(w))

2 â‰¤Îµ Â· (Î·Bx).
Further, |||Î¸||| â‰¤2 + R + 2Î·C.
31

Proof of Proposition E.1. As âˆ‚sâ„“is (Îµ, R, M, C)-approximable by sum of relus, there exists a func-
tion f : [âˆ’R, R]2 â†’R of form
f(s, t) =
M
X
m=1
cmÏƒ(ams + bmt + dm) with
M
X
m=1
|cm| â‰¤C, |am| + |bm| + |dm| â‰¤1, âˆ€m âˆˆ[M],
such that sup(s,t)âˆˆ[âˆ’R,R]2 |f(s, t) âˆ’âˆ‚sâ„“(s, t)| â‰¤Îµ.
Next, for every m âˆˆ[M], we define matrices Qm, Km, Vm âˆˆRDÃ—D such that
Qmhi =
ï£®
ï£¯ï£¯ï£¯ï£°
amw
bm
dm
âˆ’2
0
ï£¹
ï£ºï£ºï£ºï£»,
Kmhj =
ï£®
ï£¯ï£¯ï£¯ï£°
xj
yâ€²
j
1
R(1 âˆ’tj)
0
ï£¹
ï£ºï£ºï£ºï£»,
Vmhj = âˆ’(N + 1)Î·cm
N
Â·
ï£®
ï£¯ï£°
0d
0
xj
0Dâˆ’2dâˆ’1
ï£¹
ï£ºï£»
for all i, j âˆˆ[N + 1]. As the input has structure hi = [xi; yâ€²
i; w; 0Dâˆ’2dâˆ’3; 1; ti], these matrices
indeed exist, and further it is straightforward to check that they have norm bounds
max
mâˆˆ[M] âˆ¥Qmâˆ¥op â‰¤3,
max
mâˆˆ[M] âˆ¥Kmâˆ¥op â‰¤2 + R,
X
mâˆˆ[M]
âˆ¥Vmâˆ¥op â‰¤2Î·C.
Consequently, |||Î¸||| â‰¤2 + R + 2Î·C.
Now, for every i, j âˆˆ[N + 1], we have
Ïƒ(âŸ¨Qmhi, KmhjâŸ©) = Ïƒ
 amwâŠ¤xj + bm(1 âˆ’tj)yj + dm âˆ’2Rtj

= Ïƒ
 amwâŠ¤xj + bmyj + dm

1{tj = 1},
where the last equality follows from the bound
amwâŠ¤xj + bm(1 âˆ’tj)yj + dm
 â‰¤|am|BxBw + R â‰¤2R,
(15)
so that the above relu equals 0 if tj â‰¤0. Therefore,
M
X
m=1
Ïƒ(âŸ¨Qmhi, KmhjâŸ©)Vmhj
=
 M
X
m=1
cmÏƒ
 amwâŠ¤xj + bmyj + dm

!
Â· âˆ’(N + 1)Î·
N
1{tj = 0}[0d+1; xj; 02]
= f(wâŠ¤xj, yj) Â· âˆ’(N + 1)Î·
N
1{tj = 0}[0d+1; xj; 0Dâˆ’2dâˆ’1].
Thus letting the attention layer Î¸ = {(Vm, Qm, Km)}mâˆˆ[M], we have
ehi = [AttnÎ¸(H)]i = hi +
1
N + 1
N+1
X
j=1
M
X
m=1
Ïƒ(âŸ¨Qmhi, KmhjâŸ©)Vmhj
= hi âˆ’Î·
N
N
X
j=1
f(wâŠ¤xj, yj)[0d+1; xj; 02]
= [xi; yi; w; 1; ti] âˆ’Î·
N
N
X
j=1
âˆ‚sâ„“(wâŠ¤xj, yj)[0d+1; xj; 0Dâˆ’2dâˆ’1]
|
{z
}
[0d+1;âˆ’Î·âˆ‡bLN(w);0Dâˆ’2dâˆ’1]
+[0d+1; Îµ; 0Dâˆ’2dâˆ’1]
= [xi; yi; w+
Î· + Îµ; 0Dâˆ’2dâˆ’3; 1; ti],
where the error vector Îµ âˆˆRd satisfies
âˆ¥Îµâˆ¥2 =

âˆ’Î·
N
N
X
j=1
 f(wâŠ¤xj, yj) âˆ’âˆ‚sâ„“(wâŠ¤xj, yj)

xj

2
32

â‰¤Î·
N
N
X
j=1
f(wâŠ¤xj, yj) âˆ’âˆ‚sâ„“(wâŠ¤xj, yj)
 Â· âˆ¥xjâˆ¥2
â‰¤Î·
N Â· N Â· Îµ Â· Bx = Îµ Â· (Î·Bx).
This is the desired result.
E.2
Proof of Theorem D.1
We first prove part (a), which requires constructing the first L layers of Î¸. Note that by our precondi-
tion L â‰¤Bw/(2Îµ).
By our precondition, the partial derivative of the loss âˆ‚sâ„“is (Îµ, R, M, C)-approximable by sum of
relus. Therefore we can apply Proposition E.1 to obtain that, there exists a single attention layer Î¸(1) =
{(Qm, Km, Vm)}mâˆˆ[M] with M heads (and norm bounds specified in Proposition E.1), such that for
any w with âˆ¥wâˆ¥2 â‰¤Bw, the attention layer AttnÎ¸(1) maps the input hi = [xi; yâ€²
i; w; 0Dâˆ’2dâˆ’3; 1; ti]
to output hâ€²
i = [xi; yâ€²
i; bw; 0Dâˆ’2dâˆ’3; 1; ti] for all i âˆˆ[N + 1], where
bw âˆ’

w âˆ’Î·âˆ‡bLN(w)

2 â‰¤Îµ Â· (Î·Bx) =: Îµâ€².
Consider the L-layer transformer Î¸1:L = (Î¸(1), . . . , Î¸(1)) which stacks the same attention layer
Î¸(1) for L times, and for the given input h(0)
i
= [xi; yâ€²
i; w0; 0Dâˆ’2dâˆ’3; 1; ti], its â„“-th layerâ€™s output
h(â„“)
i
= [xi; yâ€²
i; bwâ„“; 0Dâˆ’2dâˆ’3; 1; ti].
We now inductively show that
bwâ„“
2 â‰¤Bw and
bwâ„“âˆ’wâ„“
GD

2 â‰¤â„“Îµ for all â„“âˆˆ[L]. The base case
of â„“= 0 is trivial. Suppose the claim holds for â„“. Then for â„“+ 1 â‰¤L â‰¤Bw/(2Îµ), the sequence
{bwi}iâ‰¤â„“+1 and {wi
GD}iâ‰¤â„“+1 satisfies the precondition of the error composition lemma (Lemma D.1)
with error bound Îµ, from which we obtain
bwâ„“+1
2 â‰¤Bw and
bwâ„“+1 âˆ’wâ„“+1
GD

2 â‰¤(â„“+ 1)Îµâ€².
This finishes the induction, and gives the following approximation guarantee for all â„“âˆˆ[L]:
bwâ„“âˆ’wâ„“
GD

2 â‰¤â„“Îµâ€² â‰¤Îµ Â· (LÎ·Bx),
which proves part (a).
We now prove part (b), which requires constructing the last attention layer Î¸(L+1). Recall h(L)
i
=
[xi; yâ€²
i; bwL; 0Dâˆ’2dâˆ’3; 1; ti] for all i âˆˆ[N + 1]. We construct a 2-head attention layer Î¸(L+1) =
{(Q(L+1)
m
, K(L+1)
m
, V(L+1)
m
)}m=1,2 such that for every i, j âˆˆ[N + 1],
Q(L+1)
1
h(L)
i
= [xi; 0Dâˆ’d], K(L+1)
1
h(L)
j
= [bwL; 0Dâˆ’d], V(L+1)
1
h(L)
j
= [0d; 1; 0Dâˆ’dâˆ’1],
Q(L+1)
2
h(L)
i
= [xi; 0Dâˆ’d], K(L+1)
2
h(L)
j
= [âˆ’bwL; 0Dâˆ’d], V(L+1)
2
h(L)
j
= [0d; âˆ’1; 0Dâˆ’dâˆ’1].
Note that the weight matrices have norm bound
max
i=1,2
Q(L+1)
i

op â‰¤1,
max
i=1,2
K(L+1)
i

op â‰¤1,
2
X
i=1
V(L+1)
i

op â‰¤2.
Then we have
h(L+1)
N+1 = h(L)
N+1 +
1
N + 1
N+1
X
j=1
2
X
m=1
Ïƒ
D
Q(L+1)h(L)
N+1, K(L+1)h(L)
j
E
V(L+1)h(L)
j
= [xi; 0; bwL; 0Dâˆ’2dâˆ’3; 1; 1] +
 Ïƒ(

bwL, xN+1

) âˆ’Ïƒ(âˆ’

bwL, xN+1

)

Â· [0d; 1; 0Dâˆ’dâˆ’1]
(i)
= [xi; 0; bwL; 0Dâˆ’2dâˆ’3; 1; 1] + [0d;

bwL, xN+1

; 0Dâˆ’dâˆ’1]
= [xi;

bwL, xN+1

|
{z
}
byN+1
; bwL; 0Dâˆ’2dâˆ’3; 1; 1],
33

Above, (i) uses the identity t = Ïƒ(t) âˆ’Ïƒ(âˆ’t). Further by part (a) we have
byN+1 âˆ’

wL
GD, xN+1
 =

bwL âˆ’wL
GD, xN+1
 â‰¤Îµ Â· (LÎ·B2
x).
This proves part (b), and also finishes the proof Theorem D.1 where the overall (L + 1)-layer
attention-only transformer is given by TF0
Î¸ with
Î¸ = (Î¸(1), . . . , Î¸(1)
|
{z
}
L times
, Î¸(L+1)).
E.3
Proof of Lemma D.1
As f is a convex, Lf smooth function on Bd
2(R), the mapping TÎ· : w 7â†’w âˆ’Î·âˆ‡f(w) is non-
expansive in âˆ¥Â·âˆ¥2: Indeed, for any w, wâ€² âˆˆBd
2(R) we have
âˆ¥TÎ·(w) âˆ’TÎ·(wâ€²)âˆ¥2 = âˆ¥w âˆ’Î·âˆ‡f(w) âˆ’(wâ€² âˆ’Î·âˆ‡f(wâ€²))âˆ¥2
2
= âˆ¥w âˆ’wâ€²âˆ¥2
2 âˆ’2Î· âŸ¨w âˆ’wâ€², âˆ‡f(w) âˆ’âˆ‡f(wâ€²)âŸ©+ Î·2 âˆ¥âˆ‡f(w) âˆ’âˆ‡f(wâ€²)âˆ¥2
2
(i)
â‰¤âˆ¥w âˆ’wâ€²âˆ¥2
2 âˆ’
 2Î·/Lf âˆ’Î·2
âˆ¥âˆ‡f(w) âˆ’âˆ‡f(wâ€²)âˆ¥2
2
(ii)
â‰¤âˆ¥w âˆ’wâ€²âˆ¥2
2 .
Above, (i) uses the property âŸ¨w âˆ’wâ€², âˆ‡f(w) âˆ’âˆ‡f(wâ€²)âŸ©â‰¥
1
Lf âˆ¥âˆ‡f(w) âˆ’âˆ‡f(wâ€²)âˆ¥2
2 for smooth
convex functions [63, Theorem 2.1.5]; (ii) uses the precondition that Î· â‰¤2/Lf.
The lemma then follows directly by induction on L. The base case of L = 0 follows directly
by assumption that bw0 = w0
GD âˆˆBd
2(R/2). Suppose the claim holds for iterate L. For iterate
L + 1 â‰¤R/(2Îµ), we have
bwL+1 âˆ’wL+1
GD

2 =
TÎ·(bwL) + ÎµL âˆ’TÎ·(wL
GD)

2
â‰¤
TÎ·(bwL) âˆ’TÎ·(wL
GD)

2 +
ÎµL
2
(i)
â‰¤
bwL âˆ’wL
GD

2 + Îµ
(ii)
â‰¤(L + 1)Îµ.
Above, (i) uses the non-expansiveness, and (ii) uses the inductive hypothesis. Similarly, by our
assumption wâ‹†= TÎ·(wâ‹†),
bwL+1 âˆ’wâ‹†
2 =
TÎ·(bwL) + ÎµL âˆ’TÎ·(wâ‹†)

2 â‰¤
bwL âˆ’wâ‹†
2 +
ÎµL
2 â‰¤R
2 + (L + 1)Îµ â‰¤R.
This finishes the induction.
E.4
Convex ICGD with â„“2 regularization
In the same setting as Theorem D.1, consider the ICGD dynamics over an â„“2-regularized empirical
risk:
wt+1
GD := wt
GD âˆ’Î·âˆ‡bLÎ»
N(wt
GD)
(ICGD-â„“2)
with initialization w0
GD âˆˆRd and learning rate Î· > 0, where bLÎ»
N(w) := bLN(w) + Î»
2 âˆ¥wâˆ¥2
2 denotes
the â„“2-regularized empirical risk.
Corollary E.1 (Convex ICGD with â„“2 regularization). Fix any Bw > 0, L > 1, Î· > 0, and
Îµ < BxBw. Suppose the loss â„“(Â·, Â·) is convex in the first argument, and âˆ‚sâ„“is (Îµ, R, M, C)-
approximable by sum of relus with R = max {BxBw, By, 1}.
Then, there exists an attention-only transformer TF0
Î¸ with (L + 1) layers, maxâ„“âˆˆ[L] M (â„“) â‰¤M + 1
heads within the first L layers, and M (L+1) = 2 such that for any input data (D, xN+1) with
sup
âˆ¥wâˆ¥2â‰¤Bw
Î»max(âˆ‡2bLÎ»
N(w)) â‰¤2Î·âˆ’1,
âˆƒwâ‹†âˆˆarg min
wâˆˆRd
bLÎ»
N(w) such that âˆ¥wâ‹†âˆ¥2 â‰¤Bw/2,
TF0
Î¸(H(0)) approximately implements (ICGD-â„“2):
34

1. (Parameter space) For every â„“âˆˆ[L], the â„“-th layerâ€™s output H(â„“) = TFÎ¸(1:â„“)(H(0)) approxi-
mates â„“steps of (ICGD-â„“2): We have h(â„“)
i
= [xi; yâ€²
i; bwâ„“; 0Dâˆ’2dâˆ’3; 1; ti] for every i âˆˆ[N + 1],
where
bwâ„“âˆ’wâ„“
GD

2 â‰¤Îµ Â· (2LÎ·Bx).
2. (Prediction space) The final output H(L+1) = TFÎ¸(H(0)) approximates the prediction of L
steps of (ICGD-â„“2): We have h(L+1)
N+1 = [xN+1; byN+1; bwL; 0Dâˆ’2dâˆ’3; 1; 0], where
byN+1 âˆ’

wL
GD, xN+1
 â‰¤Îµ Â· (2LÎ·B2
x).
Further, the transformer admits norm bound |||Î¸||| â‰¤2 + R + (2C + Î»)Î·.
Proof. This construction is the same as in the proof of Theorem D.1, except that within each layer
â„“âˆˆ[L], we add one more attention head (Q(â„“), K(â„“), V(â„“)) âŠ‚RDÃ—D which when acting on its input
h(â„“âˆ’1)
i
= [âˆ—; âˆ—; bwâ„“âˆ’1; 1; âˆ—] gives
Q(â„“)h(â„“âˆ’1)
i
=

1
0Dâˆ’1

,
K(â„“)h(â„“âˆ’1)
j
=

1
0Dâˆ’1

,
V(â„“)h(â„“âˆ’1)
j
=
ï£®
ï£°
0d+1
âˆ’Î·Î»bwâ„“âˆ’1
02
ï£¹
ï£»
for all i, j âˆˆ[N + 1]. Note that
Q(â„“)
op =
K(â„“)
op = 1, and
V(â„“)
op = Î·Î». Further, it is
straightforward to check that the output of this attention head on every h(â„“)
i
is
1
N + 1
N+1
X
j=1
Ïƒ
D
Q(â„“)h(â„“âˆ’1)
i
, K(â„“)h(â„“âˆ’1)
j
E
V(â„“)h(â„“âˆ’1)
j
=
ï£®
ï£°
0d+1
âˆ’Î·Î»bwâ„“âˆ’1
02
ï£¹
ï£».
Adding this onto the original output of the â„“-th layer exactly implements the gradient of the regularizer
w 7â†’Î»
2 âˆ¥wâˆ¥2
2. The rest of the proof follows by repeating the argument of Theorem D.1, and combining
the norm bound for the additional attention head here with the norm bound therein.
E.5
Proof of Theorem D.3
We only need to prove the following single-step version of Theorem D.3.
Proposition E.2. Under the assumptions of Theorem D.3, there exists a 2-layer transformer TFÎ¸
with the same bounds on the number of heads, hidden dimension and the norm, such that for any
input data (D, xN+1) and any w âˆˆRd, TFÎ¸ maps
hi = [xi; yâ€²
i; w; 0; 1; ti]
â†’
hâ€²
i = [xi; yâ€²
i; w+
Î· ; 0; 1; ti],
where
w+
Î· = ProjW

w âˆ’Î·âˆ‡bLN(w) + Îµ(w)

,
âˆ¥Îµ(w)âˆ¥2 â‰¤Î·Îµ.
Before we present the formal (and technical) proof of Proposition E.2, we first provide some intuitions.
To begin with, we first note that
âˆ‡w bLN(w) = 1
N
N
X
i=1
âˆ‚1â„“(pred(xi; w), yi) Â· âˆ‡wpred(xi; w),
(16)
where âˆ‚1â„“is the partial derivative of â„“with respect to the first component, and
âˆ‡wpred(xi; w) =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
u1 Â· râ€²(âŸ¨v1, xiâŸ©) Â· xi
r(âŸ¨v1, xiâŸ©)
...
uK Â· râ€²(âŸ¨vK, xiâŸ©) Â· xi
r(âŸ¨vK, xiâŸ©)
ï£¹
ï£ºï£ºï£ºï£ºï£»
âˆˆRK(d+1).
(17)
35

Therefore, the basic idea is that we can use an attention layer to approximate (xi, w) 7â†’pred(xi; w),
then use an MLP layer to implement (pred(xi; w), yâ€²
i, ti) 7â†’1{i < N + 1} Â· âˆ‚1â„“(pred(xi; w), yi),
and then use an attention layer to compute the gradient descent step w 7â†’w âˆ’Î·âˆ‡LN(w), and
finally use an MLP layer to implement the projection into W.
Based on the observations above, we now present the proof of Proposition E.2.
Proof of Proposition E.2. We write D0 = d + 1 + K(d + 1) be the length of the vector [xi; yi; w].
We also define
Br :=
max
|t|â‰¤BxBu |r(t)| ,
Bg :=
max
|t|â‰¤KBr,|y|â‰¤By |âˆ‚tâ„“(t, y)| .
Let us fix Îµr, Îµp, Îµâ„“> 0 that will be specified later in proof (see (18)). By our assumption and
Proposition B.1, the following facts hold.
(1) The function r(t) is (Îµr, R1, M1, C1) for R1 = max {BxBu, 1}, M1 â‰¤e
O
 C2
1Îµâˆ’2
r

, where
C1 depends only on R1 and the C2-smoothness of r. Therefore, there exists
r(t) =
M
X
m=1
c1
mÏƒ(

a1
m, [t; 1]

) with
M
X
m=1
c1
m
 â‰¤C1,
a1
m

1 â‰¤1, âˆ€m âˆˆ[M1],
such that suptâˆˆ[âˆ’R1,R1] |r(t) âˆ’r(t)| â‰¤Îµr.
(2) The function (t, y) 7â†’âˆ‚1â„“(t, y) is (Îµâ„“, R2, M2, C2) for R2 = max {KBr, By, 1} M2 â‰¤
e
O
 C2
2Îµâˆ’2
â„“

, where C2 depends only on R2 and the C3-smoothness of âˆ‚1â„“. Therefore, there
exists
g(t, y) =
M
X
m=1
c2
mÏƒ(

a2
m, [t; y; 1]

) with
M
X
m=1
c2
m
 â‰¤C2,
a2
m

1 â‰¤1, âˆ€m âˆˆ[M2],
such that sup(t,y)âˆˆ[âˆ’R2,R2]2 |g(t, y) âˆ’âˆ‚1â„“(t, y)| â‰¤Îµâ„“.
(3) The function (s, t) 7â†’s Â· râ€²(t) is (Îµp, R3, M3, C3) for R3 = max {BxBu, BgBu, 1},
M3 â‰¤e
O
 C2
3Îµâˆ’2
p

, where C3 depends only on R3 and the C3-smoothness of râ€². Therefore,
there exists
P(s, t) =
M
X
m=1
c3
mÏƒ(

a3
m, [s; t; 1]

) with
M
X
m=1
c3
m
 â‰¤C3,
a3
m

1 â‰¤1, âˆ€m âˆˆ[M3],
such that sup(s,t)âˆˆ[âˆ’R3,R3]2 |P(s, t) âˆ’s Â· râ€²(t)| â‰¤Îµp.
In the following, we proceed to construct the desired transformer step by step.
Step 1: construction of Î¸(1)
attn. We consider the matrices {Q(1)
k,m, K(1)
k,m, V(1)
k,m}kâˆˆ[K],mâˆˆ[M1] so that
for all i, j âˆˆ[N + 1], we have
Q(1)
k,mhi =
ï£®
ï£°
a1
m[1] Â· xi
a1
m[2]
0
ï£¹
ï£»,
K(1)
k,mhj =
"vk
1
0
#
,
V(1)
k,mhj = c1
m Â· ukeD0+1.
As the input has structure hi = [xi; yâ€²
i; w; 0; 1; ti], these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
k,m
Q(1)
k,m

op â‰¤1,
max
k,m
K(1)
k,m

op â‰¤1,
X
k,m
V(1)
k,m

op â‰¤C1.
A simple calculation shows that
X
mâˆˆ[M1],kâˆˆ[K]
Ïƒ
D
Q(1)
k,mhi, K(1)
k,mhj
E
V(1)
k,mhj =
K
X
k=1
ukr(âŸ¨vk, xiâŸ©) Â· eD0+1.
36

For simplicity, we denote pred(x; w) := PK
k=1 ukr(âŸ¨vk, xâŸ©) in the following analysis. Thus, letting
the attention layer Î¸(1)
attn = {(V(1)
k,m, Q(1)
k,m, K(1)
k,m)}(k,m), we have
AttnÎ¸(1)
attn : hi 7â†’h(0.5)
i
= [xi; yâ€²
i; w; pred(xi; w); 0; 1; ti].
Step 2: construction of Î¸(1)
mlp. We pick matrices W1, W2 so that W1 maps
W1h(0.5)
i
=
h
a2
m[1] Â· pred(xi; w) + a2
m[2] Â· yâ€²
i + a2
m[3] âˆ’R2(1 âˆ’ti)
i
mâˆˆ[M2] âˆˆRM2,
and W2 âˆˆRDÃ—M3 with entries being (W2)(j,m) = c2
m1{j = D0 + 2}. It is clear that âˆ¥W1âˆ¥op â‰¤
R2 + 1, âˆ¥W2âˆ¥op â‰¤C2. Then we have
W2Ïƒ(W1h(0.5)
i
) =
X
mâˆˆ[M3]
Ïƒ
 
a2
m, [pred(xi; w); yâ€²
i; 1]

âˆ’R2(1 âˆ’tj)

Â· c2
meD0+2
= 1{tj = 1} Â· g(pred(xi; w), yâ€²
i) Â· eD0+2.
In the following, we abbreviate gi = 1{tj = 1} Â· g(pred(xi; w), yâ€²
i). Hence, Î¸mlp maps
MLPÎ¸mlp : h(0.5)
i
7â†’h(1)
i
= [xi; yâ€²
i; w; pred(xi; w); gi; 0; 1; ti].
By the definition of the function g, for each i âˆˆ[N],
|gi âˆ’âˆ‚1â„“(pred(xi; w), yi)| â‰¤Îµâ„“+ BuLâ„“Îµr,
where Lâ„“:= max|t|â‰¤KBr,|y|â‰¤By
âˆ‚2
ttâ„“(t, y)
 is the smoothness of âˆ‚1â„“. Also, gN+1 = 0 by definition.
Step 3: construction of Î¸(2)
attn. We consider the matrices {Q(2)
k,1,m, K(2)
k,1,m, V(2)
k,1,m}kâˆˆ[K],mâˆˆ[M3] so
that for all i, j âˆˆ[N + 1], we have
Q(2)
k,1,mh(1)
i
=
ï£®
ï£¯ï£°
a3
m[1] Â· uk
a3
m[2] Â· vk
a3
m[3]
0
ï£¹
ï£ºï£»,
K(2)
k,1,mh(1)
j
=
ï£®
ï£¯ï£°
gj
xj
1
0
ï£¹
ï£ºï£»,
V(2)
k,1,mh(1)
j
= âˆ’(N + 1)Î·c3
m
N
Â·
"0k(d+1)
xj
0
#
.
We further consider the matrices {Q(2)
k,2,m, K(2)
k,2,m, V(2)
k,2,m}kâˆˆ[K],mâˆˆ[M1] so that for all i, j âˆˆ[N +1],
we have
Q(2)
k,2,mh(1)
i
=
ï£®
ï£°
a1
m[1] Â· vk
a1
m[2]
0
ï£¹
ï£»,
K(2)
k,2,mh(1)
j
=
"xj
1
0
#
,
V(2)
k,2,mh(1)
j
= âˆ’(N + 1)Î·c1
m
N
Â·
"0k(d+1)+d
gj
0
#
.
By the structure of the input h(1)
i , these matrices indeed exist, and further it is straightforward to
check that they have norm bounds
max
(k,w,m)
Q(2)
k,w,m

op â‰¤1,
max
(k,w,m)
K(2)
k,w,m

op â‰¤1,
X
(k,w,m)
V(2)
k,w,m

op â‰¤2Î·C1 + 2Î·C3.
Furthermore, a simple calculation shows that
g(w) =:
1
N + 1
N+1
X
i=1
X
(k,w,m)
Ïƒ
D
Q(2)
k,w,mhi, K(2)
k,w,mhj
E
V(2)
k,w,mhj = âˆ’Î·
N
N+1
X
j=1
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
0d+1
P(u1gj, âŸ¨v1, xjâŸ©) Â· xj
r(âŸ¨v1, xjâŸ©) Â· gj
...
P(uKgj, âŸ¨vK, xjâŸ©) Â· xj
r(âŸ¨vK, xjâŸ©) Â· gj
0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
,
where the summation is taken over all possibilities of the tuple (k, w, m), i.e. over the union of
[K] Ã— {1} Ã— [M3] and [K] Ã— {2} Ã— [M1].
37

By our definition, we have |P(s, t) âˆ’srâ€²(t)| â‰¤Îµp for all s, t âˆˆ[âˆ’R3, R3]. Therefore, for each
i âˆˆ[N], k âˆˆ[K],
|P(ukgj, âŸ¨vk, xjâŸ©) âˆ’âˆ‚1â„“(pred(xj; w), yj) Â· uk Â· râ€²(âŸ¨vk, xjâŸ©)| â‰¤Îµp + |gj âˆ’âˆ‚1â„“(pred(xi; w), yi)| Â· |uk| Â· |râ€²(âŸ¨vk, xjâŸ©)|
â‰¤Îµp + BuLr(Îµâ„“+ BuLâ„“Îµr),
where Lr := max|t|â‰¤BxBu |râ€²(t)| is the upper bound of râ€². Similarly, for each i âˆˆ[N], k âˆˆ[K], we
have
|r(âŸ¨vk, xjâŸ©) Â· gj âˆ’r(âŸ¨vk, xjâŸ©) Â· âˆ‚1â„“(pred(xj; w), yj)| â‰¤2BgÎµr + 2Br(Îµâ„“+ BuL2
â„“Îµr).
As for the case i = N + 1, we have gN+1 = 0 and |P(ukgN+1, âŸ¨vk, xN+1âŸ©)| â‰¤Îµp for each k âˆˆ[K]
by defintion. Combining these estimations and using (16) and (17), we can conclude that
Î·âˆ’1g(w) + âˆ‡bLN(w)

2 â‰¤
âˆš
KBx Â· [Îµp + BuLr(Îµâ„“+ BuLâ„“Îµr)] + 2
âˆš
K[BgÎµr + Br(Îµâ„“+ BuLâ„“Îµr)].
Thus, to ensure
Î·âˆ’1g(w) + âˆ‡bLN(w)

2 â‰¤Îµ, we only need to choose Îµp, Îµâ„“, Îµr as
Îµp =
Îµ
3
âˆš
KBx
,
Îµâ„“=
Îµ
9
âˆš
K max {Br, LrBxBu}
,
Îµr =
Îµ
15
âˆš
K max {Bg, Lâ„“BrBu, LrLâ„“BxBrB2u}
.
(18)
Thus, letting the attention layer Î¸(2)
attn = {(V(2)
k,w,m, Q(2)
k,w,m, K(2)
k,w,m)}(k,w,m), we have
AttnÎ¸(2)
attn : h(1)
i
7â†’h(1.5)
i
= [xi; yâ€²
i; w + Î·g(w); pred(xi; w); gi; 0; 1; ti].
Step 4: construction of Î¸(2)
mlp. We only need to pick Î¸(2)
mlp so that it maps
h(1.5)
i
= [xi; yâ€²
i; w + Î·g(w); pred(xi; w); gi; 0; 1; ti]
MLP
Î¸(2)
mlp
âˆ’âˆ’âˆ’âˆ’âˆ’â†’h(2)
i
= [xi; yâ€²
i; ProjW(w âˆ’Î·g(w)); 0; 0; 0; 1; ti].
By our assumption on the map ProjW, this is easy.
Combining the four steps above and taking Î¸ = (Î¸(1)
attn, Î¸(1)
mlp, Î¸(2)
attn, Î¸(2)
mlp) completes the proof.
E.6
Proof of Lemma D.3
For every â„“â‰¥0, define the intermediate iterates (before projection)
bwâ„“+ 1
2 := bwâ„“âˆ’Î·
 âˆ‡f(bwâ„“) + Îµâ„“
,
w
â„“+ 1
2
GD := wâ„“
GD âˆ’Î·âˆ‡f(wâ„“
GD),
so that bwâ„“+1 = ProjW(bwâ„“+ 1
2 ) and wâ„“+1
GD = ProjW(w
â„“+ 1
2
GD ).
We first prove part (a). We begin by deriving a relation between
bwâ„“+1 âˆ’bwâ„“2
2 and
Î·Gf
W,Î·(bwâ„“)

2
2.
Let ewâ„“+ 1
2 := bwâ„“âˆ’Î·âˆ‡f(bwâ„“) and ewâ„“+1 := ProjW(ewâ„“+ 1
2 ) denote the exact projected gradient
iterate starting from bwâ„“. We have
bwâ„“+1 âˆ’bwâ„“2
2
(i)
â‰¥1
2
ewâ„“+1 âˆ’bwâ„“2
2 âˆ’
bwâ„“+1 âˆ’ewâ„“+12
2
(ii)
â‰¥1
2
ewâ„“+1 âˆ’bwâ„“2
2 âˆ’
bwâ„“+ 1
2 âˆ’ewâ„“+ 1
2

2
2
(iii)
= Î·2
2
Gf
W,Î·(bwâ„“)

2
2 âˆ’âˆ¥Î·Îµâˆ¥2
2 â‰¥Î·2
2
Gf
W,Î·(bwâ„“)

2
2 âˆ’Î·2Îµ2.
(19)
Above, (i) uses the inequality âˆ¥a âˆ’bâˆ¥2
2 â‰¥1
2 âˆ¥aâˆ¥2
2 âˆ’âˆ¥bâˆ¥2
2; (ii) uses the fact that projection to a convex
set is a non-expansion; (iii) uses the definition of the gradient mapping.
By the Lf-smoothness of f within W, we have
f(bwâ„“+1) âˆ’f(bwâ„“) â‰¤

âˆ‡f(bwâ„“), bwâ„“+1 âˆ’bwâ„“
+ Lf
2
bwâ„“+1 âˆ’bwâ„“2
2
38

=
*
bwâ„“âˆ’bwâ„“+ 1
2
Î·
âˆ’Îµâ„“, bwâ„“+1 âˆ’bwâ„“
+
+ Lf
2
bwâ„“+1 âˆ’bwâ„“2
2
(i)
â‰¤
 bwâ„“âˆ’bwâ„“+1
Î·
, bwâ„“+1 âˆ’bwâ„“

+

Îµâ„“, bwâ„“âˆ’bwâ„“+1
+ Lf
2
bwâ„“+1 âˆ’bwâ„“2
2
=

âˆ’1
Î· + Lf
2
 bwâ„“+1 âˆ’bwâ„“2
2 + 1
4Î·
bwâ„“+1 âˆ’bwâ„“2
2 + Î·
Îµâ„“2
2
(ii)
â‰¤âˆ’1
4Î·
bwâ„“+1 âˆ’bwâ„“2
2 + Î·
Îµâ„“2
2
(iii)
â‰¤âˆ’1
4Î·
Î·2
2
Gf
W,Î·(bwâ„“)

2
2 âˆ’
Î·Îµâ„“2
2

+ Î·
Îµâ„“2
2
â‰¤âˆ’Î·
8
Gf
W,Î·(bwâ„“)

2
2 + 5Î·
4 Îµ2.
Above, (i) uses the property
D
bwâ„“+1 âˆ’bwâ„“+ 1
2 , bwâ„“+1 âˆ’bwâ„“E
â‰¤0 of the projection bwâ„“+1 =
ProjW(bwâ„“+ 1
2 ) (using bwâ„“âˆˆW); (ii) uses Lf/2 â‰¤1/(2Î·) by our choice of Î· â‰¤1/Lf; (iii)
uses (19).
Rearranging and summing the above over â„“= 0, . . . , L âˆ’1, we obtain
Î·
8
Lâˆ’1
X
â„“=0
Gf
W,Î·(bwâ„“)

2
2 â‰¤f(w0) âˆ’f(bwL) + 5Î·L
4 Îµ2.
Dividing both sides by Î·L/8 yields part (a).
Next, we prove part (b). Let C := 1 + Î·Lf. We prove by induction that
bwâ„“âˆ’wâ„“
GD

2 â‰¤Câ„“âˆ’1
C âˆ’1 Â· Î·Îµ
(20)
for all â„“â‰¥0. The base case of â„“= 0 follows by definition that bw0 = w0
GD = w0. Suppose the result
holds for â„“. Then for â„“+ 1, we have
bwâ„“+1 âˆ’wâ„“+1
GD

2
(i)
â‰¤
bwâ„“+ 1
2 âˆ’w
â„“+ 1
2
GD

2 =
bwâ„“âˆ’Î·
 âˆ‡f(bwâ„“) âˆ’Îµâ„“
âˆ’
 wâ„“
GD âˆ’Î·âˆ‡f(wâ„“
GD)

2
(ii)
â‰¤C
bwâ„“âˆ’wâ„“
GD

2 + Î·Îµ
(iii)
â‰¤C Â· Câ„“âˆ’1
C âˆ’1 Â· Î·Îµ + Î·Îµ = Câ„“+1 âˆ’1
C âˆ’1
Â· Î·Îµ.
Above, (i) uses again the non-expansiveness of the convex projection ProjW; (ii) uses the fact that
the operator w 7â†’w âˆ’Î·âˆ‡f(w) is (1 + Î·Lf) = C-Lipschitz; and (iii) uses the inductive hypothesis.
This proves the case for â„“+ 1 and thus finishes the induction. We can further relax (20) into
bwâ„“âˆ’wâ„“
GD

2 â‰¤
Câ„“
1 + Î·Lf âˆ’1 Â· Î·Îµ = Lâˆ’1
f (1 + Î·Lf)â„“Îµ.
This proves part (b).
F
Proofs for Section 3.1
F.1
Proof of Theorem 4
Fix Î» â‰¥0, 0 â‰¤Î± â‰¤Î² with Îº := Î²+Î»
Î±+Î», and Bw > 0, and consider any in-context data D such that
the precondition of Theorem 4 holds. Let
Lridge(w) :=
1
2N
N
X
i=1
(âŸ¨w, xiâŸ©âˆ’yi)2 + Î»
2 âˆ¥wâˆ¥2
2
39

denote the ridge regression loss in (ICRidge), so that wÎ»
ridge = arg minwâˆˆRd Lridge(w). It is a
standard result that âˆ‡2Lridge(w) = XâŠ¤X/N + Î»Id, so that Lridge is (Î± + Î»)-strongly convex and
(Î² + Î»)-smooth over Rd.
Consider the gradient descent algorithm on the ridge loss
wt+1
GD = wt
GD âˆ’Î·âˆ‡Lridge(wt
GD)
with initialization, learning rate, and number of steps
w0
GD := 0d,
Î· :=
1
Î² + Î»,
T :=

2Îº log
BxBw
2Îµ

.
By standard convergence results for strongly convex and smooth functions (Proposition B.2), we
have for all t â‰¥1 that
wt
GD âˆ’wÎ»
ridge
2
2 â‰¤exp

âˆ’t
Îº
 w0
GD âˆ’wÎ»
ridge
2
2 = exp

âˆ’t
Îº
 wÎ»
ridge
2
2 .
Further, we have
wT
GD âˆ’wÎ»
ridge

2 â‰¤exp

âˆ’T
2Îº
 wÎ»
ridge

2 â‰¤
2Îµ
BxBw
Â· Bw
2
â‰¤Îµ
Bx
.
(21)
It remains to construct a transformer to approximate wT
GD. Notice that the problem (ICRidge)
corresponds to an â„“2-regularized ERM with the square loss â„“(s, t) :=
1
2(s âˆ’t)2, whose partial
derivative âˆ‚sâ„“(s, t) = s âˆ’t is exactly a sum of two relus:
âˆ‚sâ„“(s, t) = 2Ïƒ((s âˆ’t)/2) âˆ’2Ïƒ(âˆ’(s âˆ’t)/2).
In particular, this shows that âˆ‚sâ„“(s, t) is (0, R, 2, 4)-approximable for any R > 0, in particular for
R = max {BxBw, By, 1}.
Therefore, we can apply Corollary E.1 with the square loss â„“, learning rate Î·, regularization strength
Î» and accuracy parameter Îµ = 0 to obtain that there exists an attention-only transformer TF0
Î¸ with
(T + 1) := L layers such that the final output h(L)
N+1 = [xN+1; byN+1; âˆ—] with
byN+1 âˆ’

wT
GD, xN+1
 = 0,
(22)
and number of heads M (â„“) = 3 for all â„“âˆˆ[L âˆ’1] (can be taken as 2 in the unregularized case Î» = 0
directly by Theorem D.1), and M (L) = 2. Further, Î¸ admits norm bound |||Î¸||| â‰¤2 + R + 8+Î»
Î²+Î» â‰¤
3R + 8(Î² + Î»)âˆ’1 + 1 â‰¤4R + 8(Î² + Î»)âˆ’1.
Combining (21) and (22), we obtain that
byN+1 âˆ’

wÎ»
ridge, xN+1
 =

wT
GD âˆ’wÎ»
ridge, xN+1
 â‰¤(Îµ/Bx) Â· Bx = Îµ.
Further, we have readw(hT
i ) = wT
GD for all i âˆˆ[N +1], where readw(h) := h(d+2):(2d+1) (cf. Corol-
lary E.1), so that âˆ¥readw(hT
i ) âˆ’wÎ»
ridgeâˆ¥2 â‰¤Îµ/Bx as shown above. This finishes the proof.
F.2
Statistical analysis of in-context least squares
Consider the standard least-squares algorithm ALS and least-squares estimator bwLS âˆˆRd defined as
ALS(D)(xN+1) := âŸ¨bwLS, xN+1âŸ©,
bwLS =
 XâŠ¤X
âˆ’1XâŠ¤y âˆˆRd.
(ICLS)
For any distribution P over (x, y) âˆˆRd Ã— R and any estimator w âˆˆRd, let
LP(w) := E(xâ€²,y)âˆ¼P
h
1
2(âŸ¨w, xâ€²âŸ©âˆ’yâ€²)2i
denote the expected risk of w over a new test example (xâ€², yâ€²) âˆ¼P.
Assumption A (Well-posedness for learning linear predictors). We say a distribution P on Rd Ã— R is
well-posed for learning linear predictors, if (x, y) âˆ¼P satisfies
40

(1) âˆ¥xâˆ¥2 â‰¤Bx and |y| â‰¤By almost surely;
(2) The covariance Î£P := EP[xxâŠ¤] satisfies Î»minId âª¯Î£P âª¯Î»maxId, with 0 < Î»min â‰¤Î»max,
and Îº := Î»max/Î»min.
(3) The whitened vector Î£âˆ’1/2
P
x is K2-sub-Gaussian for some K â‰¥1.
(4) The best linear predictor wâ‹†
P := EP[xxâŠ¤]âˆ’1EP[xy] satisfies âˆ¥wâ‹†
Pâˆ¥2 â‰¤Bâ‹†
w.
(5) We have E[(y âˆ’âŸ¨x, wâ‹†
PâŸ©)2|x] â‰¤Ïƒ2 with probability one (over x).
Further, we say P is well-posed with canonical parameters if
Bx = Î˜(
âˆš
d),
By = Î˜(1),
Bâ‹†
w = Î˜(1),
Ïƒ â‰¤O(1),
Î»max = Î˜(1),
K = Î˜(1),
(23)
where Î˜(Â·) and O(Â·) only hides absolute constants.
The following result bounds the excess risk of least squares under Assumption A with a clipping
operation on the predictor; the clipping allows the result to only depend on the second moment of the
noise (cf. Assumption A(5)) instead of e.g. its sub-Gaussianity, and also makes the result convenient
to be directly translated to a result for transformers.
Proposition F.1 (Guarantees for in-context least squares). Suppose distribution P satisfies Assump-
tion A. Then as long as N â‰¥O(dK4 log(1/Î´)), we have the following:
(a) The (clipped) least squares predictor achieves small expected excess risk (fast rate) over the best
linear predictor: For any clipping radius R â‰¥By,
ED,xN+1,yN+1âˆ¼P
1
2(clipR(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1)2

â‰¤inf
wâˆˆRd LP(w)
|
{z
}
LP(wâ‹†
P)
+O

R2Î´ + dÏƒ2
N

.
(24)
(b) We have P(Ecov âˆ©Ew) â‰¥1 âˆ’Î´/10, where
Ecov = Ecov(D) :=
1
2Id âª¯Î£âˆ’1/2
P
bÎ£Î£âˆ’1/2
P
âª¯2Id

,
(25)
Ew = Ew(D) :=
ï£±
ï£²
ï£³âˆ¥bwLSâˆ¥2 â‰¤Bâ‹†
w +
s
80dÏƒ2
Î´NÎ»min
ï£¼
ï£½
ï£¾.
(26)
Proof. We first show P(Ecov) â‰¥1âˆ’Î´/20. Let bÎ£ := 1
N
PN
i=1 xixâŠ¤
i , and let the whitened covariance
and noise variables be denoted as
exi = Î£âˆ’1/2
P
xi,
eÎ£ := 1
N
N
X
i=1
exiexâŠ¤
i = Î£âˆ’1/2
P
bÎ£Î£âˆ’1/2
P
.
Also let zi := yi âˆ’âŸ¨xi, wâ‹†
PâŸ©denote the â€œnoiseâ€ variables. Note that
Ecov =
1
2Id âª¯eÎ£ âª¯2Id

is exactly a covariance concentration of the whitened vectors {exi}iâˆˆ[N]. Recall that E[exiexâŠ¤
i ] = Id,
and exi are K2-sub-Gaussian by assumption. Therefore, we can apply [85, Theorem 4.6.1], we have
with probability at least 1 âˆ’Î´/10 that
eÎ£ âˆ’Id

op â‰¤O
 
K2 max
(r
d + log(1/Î´)
N
, d + log(1/Î´)
N
)!
.
Setting N â‰¥O(K4(d + log(1/Î´))) ensures that the right-hand side above is at most 1/2, on which
event we have
1
2Id âª¯eÎ£ âª¯3
2Id âª¯2Id,
(27)
41

i.e. Ecov holds. This shows that P(Ec
cov) â‰¤Î´/10.
Next, we show (24). Using Ecov, we decompose the risk as
E
1
2(clipR(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1)2

= E
1
2(clipR(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1)21{Ecov}

+ E
1
2(clipR(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1)21{Ec
cov}

(i)
â‰¤E
1
2(âŸ¨bwLS, xN+1âŸ©âˆ’yN+1)21{Ecov}

+ 2R2 Â· (Î´/20)
(ii)
= ED,xN+1
1
2(âŸ¨bwLS âˆ’wâ‹†
P, xN+1âŸ©)21{Ecov}

+ ExN+1,yN+1
1
2(âŸ¨wâ‹†
P, xN+1âŸ©âˆ’yN+1)21{Ecov}

+ O(R2Î´)
â‰¤ED
1
2 âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
Î£P 1{Ecov}

+ ExN+1,yN+1
1
2(âŸ¨wâ‹†
P, xN+1âŸ©âˆ’yN+1)2

|
{z
}
LP(wâ‹†
P)
+O(R2Î´).
(28)
Above, (i) follows by assumption that |yN+1| â‰¤By â‰¤R almost surely, so that removing
the clipping can only potentially increase the distance in the first term, and the square loss is
upper bounded by
1
2 Â· (2R)2 almost surely in the second term; (ii) follows by the fact that
ExN+1,yN+1[âŸ¨bwLS âˆ’wâ‹†
P, xN+1âŸ©(âŸ¨wâ‹†
P, xN+1âŸ©âˆ’yN+1)] = 0 by the definition of wâ‹†
P, as well as
the fact that 1{Ecov} is independent of (xN+1, yN+1).
It thus remains to bound ED
h
1
2 âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
Î£P 1{Ecov}
i
. Note that on the event Ecov, we have
Î£1/2
P
bÎ£âˆ’1Î£1/2
P
=

Î£âˆ’1/2
P
bÎ£Î£âˆ’1/2
P
âˆ’1
âª¯2Id.
Therefore,
1
2 âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
Î£P 1{Ecov} = 1
2
 (XâŠ¤X)âˆ’1XâŠ¤y âˆ’wâ‹†
P
âŠ¤Î£P
 (XâŠ¤X)âˆ’1XâŠ¤y âˆ’wâ‹†
P

1{Ecov}
= 1
2zâŠ¤X(XâŠ¤X)âˆ’1Î£P(XâŠ¤X)âˆ’1XâŠ¤z Â· 1{Ecov}
=
1
2N 2 zâŠ¤XÎ£âˆ’1/2
P

Î£1/2
P
bÎ£âˆ’1Î£1/2
P
2
Î£âˆ’1/2
P
XâŠ¤z Â· 1{Ecov}
â‰¤
2
N 2
Î£âˆ’1/2
P
XâŠ¤z

2
2 1{Ecov} =
2
N 2

N
X
i=1
exizi

2
2
1{Ecov} â‰¤
2
N 2

N
X
i=1
exizi

2
2
.
Note that E[exizi] = Î£âˆ’1/2
P
E[xi(yi âˆ’âŸ¨wâ‹†
P, xiâŸ©)] = 0. Therefore, taking expectation on the above
(over D), we get
ED
1
2 âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
Î£P 1{Ecov}

â‰¤
2
N 2 E
ï£®
ï£°

N
X
i=1
exizi

2
2
ï£¹
ï£»= 2
N E
h
âˆ¥ex1z1âˆ¥2
2
i
= 2
N E

z2
1xâŠ¤
1 Î£âˆ’1
P x1

(29)
(i)
â‰¤2Ïƒ2
N E

xâŠ¤
1 Î£âˆ’1
P x1

= 2dÏƒ2
N
.
(30)
Above, (i) follows by conditioning on x1 and using Assumption A(5). Combining with (28), we
obtain
E
1
2(clipR(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1)2

â‰¤LP(wâ‹†
P) + O

R2Î´ + dÏƒ2
N

.
This proves (24).
Finally, we show P(Ecov âˆ©Ew) â‰¥1 âˆ’Î´/10. Using (29) and Î£P âª°Î»minId by assumption, we get
E
h
âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
2 1{Ecov}
i
â‰¤4dÏƒ2
NÎ»min
.
42

Therefore, using an argument similar to Chebyshevâ€™s inequality,
P(Ecov âˆ©Ec
w) = E
ï£®
ï£°1{Ecov} Ã— 1{âˆ¥bwLSâˆ¥2 >
s
20
Î´ Â· 4dÏƒ2
NÎ»min
+ Bâ‹†
w}
ï£¹
ï£»
â‰¤E
ï£®
ï£°1{Ecov} Ã— 1{âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2 >
s
20
Î´ Â· 4dÏƒ2
NÎ»min
}
ï£¹
ï£»
â‰¤E
"
1{Ecov} Ã— âˆ¥bwLS âˆ’wâ‹†
Pâˆ¥2
2
20
Î´ Â·
4dÏƒ2
NÎ»min
#
â‰¤Î´/20.
This implies that
P(Ecov âˆ©Ew) = P(Ecov) âˆ’P(Ecov âˆ©Ec
w) â‰¥1 âˆ’Î´/20 âˆ’Î´/20 â‰¥1 âˆ’Î´/10.
This is the desired result.
F.3
Proof of Corollary 5
The proof follows by first checking the well-conditionedness of the data D (cf. (5)) with high
probability, then invoking Theorem 4 (for approximation least squares) and Proposition F.1 (for the
statistical power of least squares).
First, as P satisfies Assumption A, by Proposition F.1, as long as N â‰¥O(K4(d + log(1/Î´))), we
have with probability at least 1 âˆ’Î´/10 that event Ecov âˆ©Ew holds. On this event, we have
1
2Î»minId âª¯1
2Î£P âª¯bÎ£ = XâŠ¤X/N âª¯2Î£P âª¯2Î»maxId,
âˆ¥bwLSâˆ¥2 â‰¤Bw/2 := O
ï£«
ï£­Bâ‹†
w +
s
dÏƒ2
Î´NÎ»min
ï£¶
ï£¸,
and thus the dataset D is well-conditioned (in the sense of (5)) with parameters Î± = Î»min/2,
Î² = 2Î»max, and Bw defined as above. Note that the condition number of bÎ£ is upper bounded
by Î²/Î± = 4Î»max/Î»min â‰¤4Îº, where Îº is the upper bound on the condition number of Î£P as
in Assumption A(c).
Define parameters
Îµ =
r
dÏƒ2
N ,
Î´ = dÏƒ2
B2yN âˆ§1.
(31)
Note that Bw â‰¤O(Bâ‹†
w +
q
B2y/Î»min) by the above choice of Î´.
We can thus apply Theorem 4 in the unregularized case (Î» = 0) to obtain that, there exists a
transformer Î¸ with maxâ„“âˆˆ[L] M (â„“) â‰¤3, |||Î¸||| â‰¤4R + 4/Î»max (with R = max {BxBw, By, 1}), and
number of layers
L â‰¤O

Îº log BxBw
Îµ

â‰¤O
 
Îº log
 
Bx
r
N
dÏƒ2
 
Bâ‹†
w +
B2
y
âˆšÎ»min
!!!
,
such that on Ecov âˆ©Ew (so that D is well-conditioned), we have (choosing the clipping radius in
g
ready(Â·) = clipBy(ready(Â·)) to be By):
 g
ready(TF0
Î¸(H)) âˆ’clipBy(âŸ¨bwLS, xN+1âŸ©)
 â‰¤
ready(TF0
Î¸(H)) âˆ’âŸ¨bwLS, xN+1âŸ©
 â‰¤Îµ =
r
dÏƒ2
N .
(32)
We now bound the excess risk of the above transformer. Combining Proposition F.1 and (32), we
have
E

g
ready(TF0
Î¸(H)) âˆ’yN+1
2
43

= E

g
ready(TF0
Î¸(H)) âˆ’yN+1
2
1{Ecov âˆ©Ew}

+ E

g
ready(TF0
Î¸(H)) âˆ’yN+1
2
1{(Ecov âˆ©Ew)c}

â‰¤2E

g
ready(TF0
Î¸(H)) âˆ’clipBy(âŸ¨bwLS, xN+1âŸ©)
2
1{Ecov âˆ©Ew}

+ 2E

clipBy(âŸ¨bwLS, xN+1âŸ©) âˆ’yN+1
2
1{Ecov âˆ©Ew}

+ 2B2
y Â· Î´/10
(i)
â‰¤2Îµ2 + LP(wâ‹†
P) + O

B2
yÎ´ + dÏƒ2
N

+ O
 B2
yÎ´

â‰¤LP(wâ‹†
P) + O

B2
yÎ´ + dÏƒ2
N

â‰¤O
dÏƒ2
N

.
Above, (i) uses the approximation guarantee (32) as well as Proposition F.1(a) (with clipping radius
By). This proves the desired excess risk guarantee.
Finally, under the canonical choice of parameters (23), the bounds for L, M, |||Î¸||| simplify to
L â‰¤O

Îº log NÎº
Ïƒ

,
max
â„“âˆˆ[L] M (â„“) â‰¤3,
|||Î¸||| â‰¤O(
âˆš
Îºd),
(33)
and the requirement for N simplifies to N â‰¥O(d + log(1/Î´)) = e
O(d) (as K = Î˜(1)). This proves
the claim about the required N and L.
F.4
Proof of Corollary 6
Fix parameters Î´, Îµ > 0 to be specified later and a large universal constant C0. Let us set
Î± = max
n
0, 1/2 âˆ’
p
d/N
o2
,
Î² = 25,
Bâ‹†
w := 1 + 2
r
log(4/Î´)
d
,
Bw = C0(Bâ‹†
w + Ïƒ),
Bx = C0
p
d log(N/Î´),
By = C0(Bâ‹†
w + Ïƒ)
p
log(N/Î´).
Consider the following good events (below Îµ = [Îµi]iâˆˆ[N] âˆˆRN is given by Îµi = yi âˆ’âŸ¨wâ‹†, xiâŸ©)
EÏ€ =
n
âˆ¥wâ‹†âˆ¥2 â‰¤Bâ‹†
w, âˆ¥Îµâˆ¥2 â‰¤2
âˆš
NÏƒ
o
,
Ew =

Î± â‰¤Î»min(XâŠ¤X/N) â‰¤Î»max(XâŠ¤X/N) â‰¤Î²
	
,
Eb = {âˆ€i âˆˆ[N], âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,N+1 = {âˆ¥xN+1âˆ¥2 â‰¤Bx, |yN+1| â‰¤By},
and we define E := EÏ€ âˆ©Ew âˆ©Eb âˆ©Eb,N+1. Under the event E, the problem (ICRidge) is well-
conditioned and âˆ¥wÎ»
ridgeâˆ¥â‰¤Bw/2 (by Lemma F.1).
Therefore, Theorem 4 implies that for Îº = Î±+Î»
Î²+Î», there exists a L = âŒˆ2Îº log(Bw/Îµ)âŒ‰+ 1-layer
transformer Î¸ with prediction byN+1 := g
ready(TF0
Î¸(H)) (clipped by By), such that under the good
event E, we have byN+1 = clipBy(âŸ¨xN+1, bwâŸ©) and âˆ¥bw âˆ’wÎ»
ridgeâˆ¥â‰¤Îµ.
In the following, we show that Î¸ is indeed the desired transformer (when Îµ and Î´ is suitably chosen).
Notice that we have
E(byN+1 âˆ’yN+1)2 = E

1{E}(byN+1 âˆ’yN+1)2
+ E

1{Ec}(byN+1 âˆ’yN+1)2
,
and we analyze these two parts separately.
Prediction risk under good event E.
We first note that
E

1{E}(byN+1 âˆ’yN+1)2
= E
h
1{E}(clipBy(âŸ¨xN+1, bwâŸ©) âˆ’yN+1)2i
â‰¤E

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2
,
44

where the inequality is because yN+1 âˆˆ[âˆ’By, By] under the good event E. Notice that by our
construction, under the good event E, bw = bw(D) depends only on the dataset D7. Therefore, we
have âˆ¥bw(D) âˆ’wÎ»
ridge(D)âˆ¥â‰¤Îµ as long as the event E0 := EÏ€ âˆ©Ew âˆ©Eb holds for (wâ‹†, D). Thus,
under E0,
E

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2 wâ‹†, D

= E

1{E}(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 wâ‹†, D

â‰¤E

(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 wâ‹†, D

= E

(âŸ¨xN+1, bw(D)âŸ©âˆ’âŸ¨xN+1, wâ‹†âŸ©)2 wâ‹†, D

+ Ïƒ2
= âˆ¥bw(D) âˆ’wâ‹†âˆ¥2
2 + Ïƒ2,
and we also have
âˆ¥bw(D) âˆ’wâ‹†âˆ¥2
2 â‰¤
wÎ»
ridge âˆ’wâ‹†
2
2 + 2
wÎ»
ridge âˆ’wâ‹†

2
bw(D) âˆ’wÎ»
ridge

2 +
bw(D) âˆ’wÎ»
ridge
2
2
â‰¤
wÎ»
ridge âˆ’wâ‹†
2
2 + 2Îµ
wÎ»
ridge âˆ’wâ‹†

2 + Îµ2.
Recall that 2BayesRiskÏ€ = Ewâ‹†,Dâˆ¥wÎ»
ridge âˆ’wâ‹†âˆ¥2
2 + Ïƒ2. Note that 2BayesRiskÏ€ â‰¤1 + Ïƒ2 by
definition. Therefore, we can conclude that
E

1{E}(byN+1 âˆ’yN+1)2
â‰¤2BayesRiskÏ€ + 2Îµ + Îµ2.
Prediction risk under bad event Ec.
Notice that
E

1{Ec}(byN+1 âˆ’yN+1)2
â‰¤
p
P(Ec)E[(byN+1 âˆ’yN+1)4].
We can upper bound P(Ec) = P(Ec
Ï€ âˆªEc
w âˆªEc
b âˆªEc
b,N+1) by Lemma B.1, Lemma B.2 and the
sub-Gaussian tail bound:
P(Ec
Ï€) â‰¤Î´
2 + exp(âˆ’N/8),
P(Ec
w) â‰¤2 exp(âˆ’N/8),
P(Ec
b âˆªEc
b,N+1) â‰¤Î´
4.
Thus, as long as N â‰¥8 log(12/Î´), we have P(Ec) â‰¤Î´. Further, a simple calculation yields
E(byN+1 âˆ’yN+1)4 â‰¤8Eby4
N+1 + 8Ey4
N+1 â‰¤8B2
y + 8Ey4
N+1.
Notice that yN+1|wâ‹†âˆ¼N(0, âˆ¥wâ‹†âˆ¥2
2+Ïƒ2), hence Ey4
N+1 = 3E(âˆ¥wâ‹†âˆ¥2
2+Ïƒ2)2 â‰¤3(3+2Ïƒ2+Ïƒ4) â‰¤
B4
y. Thus, we can conclude that
E

1{Ec}(byN+1 âˆ’yN+1)2
â‰¤4
âˆš
Î´By.
Choosing Îµ and Î´.
Combining the inequalities above, we have
E(byN+1 âˆ’yN+1)2 â‰¤2BayesRiskÏ€ +
h
2Îµ
p
2BayesRiskÏ€ + Îµ2 + 4
âˆš
Î´By
i
.
To ensure 1
2E(byN+1 âˆ’yN+1)2 â‰¤BayesRiskÏ€ + Îµ, we only need to take (Îµ, Î´) so that the following
constraints are satisfied:
Îµ = 1
2 min

Îµ, âˆšÎµ
	
,
4
âˆš
Î´By â‰¤Îµ
2,
N â‰¥8 log(12/Î´).
Therefore, it suffices to take Î´ =
c0
log2(N)

Îµ2
1+Ïƒ2
2
for some small constant c0, then as long as
N â‰¥C log
Ïƒ2 + 1
Îµ

+ C.
our choice of Îµ and Î´ is feasible. Note that Îº â‰¤O
 1 + Ïƒâˆ’2
, and hence under such choice of (Îµ, Î´),
we have L = O(log(1/Îµ)) and |||Î¸||| = eO
âˆš
d

. This is the desired result.
7We need this, as on Ec, the transformer output at this location could in principle depend additionally on
xN+1, as (15) may not hold due to the potential unbounededness of its input. A similar fact will also appear in
later proofs (for generalized linear models and Lasso).
45

Lemma F.1. Under the event EÏ€ âˆ©Ew, we have
wÎ»
ridge

2 â‰¤O (Bâ‹†
w + Ïƒ).
Proof of Lemma F.1. By the definition of wÎ»
ridge and recall that Î» = dÏƒ2/N, we have wÎ»
ridge =
(XâŠ¤X + dÏƒ2Id)âˆ’1XâŠ¤y.
Therefore, we only need to prove the following fact: for any Î³ > 0 and bÎ² = (XâŠ¤X + dÎ³Id)âˆ’1XâŠ¤y,
we have
âˆ¥bÎ²âˆ¥2 â‰¤Bâ‹†
w + 10Ïƒ(1 + Î³âˆ’1/2).
(34)
We now prove (34). Note that we have
âˆ¥bÎ²âˆ¥2 = âˆ¥(XâŠ¤X + dÎ³Id)âˆ’1XâŠ¤(Xwâ‹†+ Îµ)âˆ¥2 â‰¤âˆ¥B1âˆ¥op âˆ¥wâ‹†âˆ¥2 + âˆ¥B2âˆ¥op âˆ¥Îµâˆ¥2
where B1 = XâŠ¤X(XâŠ¤X + dÎ³Id)âˆ’1, B2 = (XâŠ¤X + dÎ³Id)âˆ’1XâŠ¤. Note that âˆ¥B1âˆ¥op â‰¤1 clearly
holds, and under EÏ€ we also have âˆ¥Îµâˆ¥2 â‰¤2
âˆš
NÏƒ. Therefore, it remains to bound the term âˆ¥B2âˆ¥op.
Consider the SVD decomposition of X = UÎ£V , Î£ = diag(Î»1, Â· Â· Â· , Î»d), and U âˆˆRNÃ—d, V âˆˆRdÃ—d
are orthonormal matrices. Then B2 = V âŠ¤(Î£2 + dÎ³Id)âˆ’1Î£U âŠ¤, and hence
âˆ¥B2âˆ¥op =
(Î£2 + dÎ³Id)âˆ’1Î£

op = max
i
Î»i
Î»2
i + dÎ³ .
When N â‰¤36d, we directly have âˆ¥B2âˆ¥op â‰¤
1
2(dÎ³)âˆ’1/2 â‰¤3(NÎ³)âˆ’1/2. Otherwise, we have
N â‰¥36d, and then for each i âˆˆ[d], Î»i â‰¥
p
Î»min(XâŠ¤X) â‰¥
âˆš
Î±N â‰¥
âˆš
N/3. Hence, in this case
we also have âˆ¥B2âˆ¥op â‰¤maxi Î»âˆ’1
i
â‰¤3N âˆ’1/2. Combining the both cases completes the proof of
(34).
G
In-context learning of generalized linear models
As a natural generalization of linear regression, we now show that transformers can recover learn
generalized linear models (GLMs) [53] (which includes logistic regression for linear classification as
an important special case), by implementing the corresponding convex risk minimization algorithm
in context, and achieve near-optimal excess risk under standard statistical assumptions.
Let g : R â†’R be a link function that is non-decreasing and C2-smooth. We consider the following
convex empirical risk minimization (ERM) problem
wGLM := arg min
wâˆˆRd
bLN(w) := 1
N
N
X
i=1
â„“(âŸ¨xi, wâŸ©, yi),
(ICGLM)
where â„“(t, y) := âˆ’yt+
R t
0 g(s)ds is the convex (integral) loss associated with g. A canonical example
of (ICGLM) is logistic regression, in which g(t) = Ïƒlog(t) := (1 + eâˆ’t)âˆ’1 is the sigmoid function,
and the resulting â„“(t, y) = â„“log(t, y) = âˆ’yt + log(1 + et) is the logistic loss.
The following result (proof in Appendix G.1) shows that, as long as the empirical risk bLN satisfies
strong convexity and bounded solution conditions (similar as in Theorem 4), transformers can
approximately implement the ERM predictor g(âŸ¨xN+1, wGLMâŸ©), with wGLM given by (ICGLM).
Theorem G.1 (Implementing convex risk minimization for GLMs). For any 0 < Î± < Î² with Îº := Î²
Î±,
Bw > 0, Bx > 0, Îºw := LgB2
x/Î± + 1 and Îµ < Bw/2, there exists an attention-only transformer
TF0
Î¸ with
L = âŒˆ2Îº log(LgBwBx/Îµ)âŒ‰+ 1,
max
â„“âˆˆ[L] M (â„“) â‰¤e
O
 C2
gÎº2
wÎµâˆ’2
,
|||Î¸||| â‰¤O
 R + Î²âˆ’1Cg

,
(where Lg := supt |gâ€²(t)|, R := max {BxBw, By, 1}, and Cg > 0 is a constant that depends only
on R and the C2-smoothness of g within [âˆ’R, R]), such that the following holds. On any input data
(D, xN+1) such that
Î± â‰¤Î»min(âˆ‡2bLN(w)) â‰¤Î»max(âˆ‡2bLN(w)) â‰¤Î² for all w âˆˆB2(Bw),
âˆ¥wGLMâˆ¥2 â‰¤Bw/2,
(35)
46

TF0
Î¸(H(0)) approximately implements (ICGLM): We have h(L+1)
N+1 := [xN+1; byN+1; bw; 1; 1], where
|byN+1 âˆ’g(âŸ¨xN+1, wGLMâŸ©)| â‰¤Îµ.
In Theorem G.1, the number of heads scales as e
O(1/Îµ2) as opposed to Î˜(1) as in ridge regression
(Theorem 4), due to the fact that the gradient of the loss is in general a smooth function that can be
only approximately expressed as a sum-of-relus (cf. Definition D.1 & Lemma B.5) rather than exactly
expressed as in the case for the square loss.
In-context prediction power
We next show that (proof in Appendix G.2) the transformer con-
structed in Theorem G.1 achieves desirable statistical power if the in-context data distribution satisfies
standard statistical assumptions for learning GLMs. Let LP(w) := E(x,y)âˆ¼P[â„“(âŸ¨w, xâŸ©, y)] denote the
corresponding population risk for any distribution P of (x, y). When P is realizable by a generalized
linear model of link function g and parameter Î² in the sense that EP[y|x] = g(âŸ¨Î², xâŸ©), it is a standard
result that Î² is indeed a minimizer of LP [42] (see also [6, Appendix A.3]).
Theorem G.2 (Statistical guarantee for generalized linear models). For any fixed set of param-
eters defined in Assumption B, there exists a transformer Î¸ with L â‰¤O (log(N)) layers and
maxâ„“âˆˆ[L] M (â„“) â‰¤e
O
 d3N

, such that for any distribution P satisfying Assumption B with those pa-
rameters, as long as N â‰¥O (d), that outputs byN+1 = g
ready(TFÎ¸(H)) and bw = g
readw(TFÎ¸(H)) âˆˆ
Rd (for another read-out function g
readw) satisfying the following.
(a) bw achieves small excess risk under the population loss, i.e. for the linear prediction
bylin
N+1 := âŸ¨xN+1, bwâŸ©,
E(D,xN+1,yN+1)âˆ¼P

â„“(bylin
N+1, yN+1)

âˆ’min
Î² LP(Î²) â‰¤O (d/N) .
(36)
(b) (Realizable setting) If there exists a Î² âˆˆRd such that under P, E[y|x] = g(âŸ¨Î², xâŸ©) almost
surely, then
E(D,xN+1,yN+1)âˆ¼P

(byN+1 âˆ’yN+1)2
â‰¤E(xN+1,yN+1)âˆ¼P

(g(âŸ¨Î², xN+1âŸ©) âˆ’yN+1)2
+ O (d/N) ,
(37)
or equivalently, E[(byN+1 âˆ’E[yN+1|xN+1])2] â‰¤O (d/N).
Above, O (Â·) hides constants that depend polynomially on the parameters in Assumption B. Similar
as in Corollary 5, the O(d/N) excess risk obtained here matches the optimal (fast) rate for typical
learning problems with d parameters and N samples [87].
Assumption B (Well-posedness for learning GLMs). We assume that there is some BÂµ > 0 such
that for any t âˆˆ[âˆ’BÂµ, BÂµ], gâ€²(t) â‰¥Âµg > 0.
We also assume that for each i âˆˆ[N + 1], (xi, yi) is independently sampled from P such that the
following holds.
(a) Under the law (x, y) âˆ¼P, We have x âˆ¼SG(Kx), y âˆ¼SG(Ky) and g(âŸ¨w, xâŸ©) âˆ¼
SG(Ky) âˆ€w âˆˆB2(Bw).
(b) For some Âµx > 0, it holds that
E[1{|xâŠ¤w| â‰¤BÂµ/2}xxâŠ¤] âª°ÂµxId
âˆ€w âˆˆB2(Bw).
(c) For Î²â‹†= arg min LP, it holds âˆ¥Î²â‹†âˆ¥2 â‰¤Bw/4.
Applying Theorem G.2 to logistic regression, we have the following result as a direct corollary. Below,
the Gaussian input assumption is for convenience only and can be generalized to e.g. sub-Gaussian
input.
Corollary G.1 (In-context logistic regression). Consider any in-context data distribution P satisfying
x âˆ¼N(0, Id),
y âˆˆ{0, 1},
arg min
Î²âˆˆRd
LP(Î²) âˆˆB2(Bâ‹†
w).
For the link function g = Ïƒlog and Bâ‹†
w = O (1), we can choose Bw, BÂµ, Âµg, Lg, Âµx, Kx, Ky = Î˜ (1)
so that Assumption B holds. In that case, when N â‰¥O (d), there exists a transformer Î¸ with
L = O (log(N)) layers, such that for any P considered above,
47

(a) The estimation bw = g
readw(TFÎ¸(H)) outputted by Î¸ achieves excess risk bound (36).
(b) (Realizable setting) Consider the logistic in-context data distribution
Plog
Î² :
x âˆ¼N(0, Id),
y|x âˆ¼Bernoulli(g(âŸ¨Î², xâŸ©)).
Then, for any distribution P = Plog
Î²
with âˆ¥Î²âˆ¥2
â‰¤Bâ‹†
w, the prediction byN+1
=
g
ready(TFÎ¸(H)) of Î¸ additionally achieves the square loss excess risk (37).
G.1
Proof of Theorem G.1
Let us fix parameters Îµg > 0 and T > 0 (that we specify later in proof).
Define R = max{BxBw, By, 1} and
Cg := max
i=0,1,2

Ri
max
sâˆˆ[âˆ’B,B]
g(i)(s)


.
By Proposition B.1, g is (Îµg, M, R, C) with
C â‰¤O (Cg) ,
M â‰¤O
 C2
gÎµâˆ’2
g
log(1 + CgÎµâˆ’1
g )

.
Therefore, we can invoke Theorem D.1 to obtain that, as long as 2TÎµg â‰¤Bw, there exists a T-layer
attention-only transformer Î¸(1:T ) with M heads per layer, such that for any input H of format (3)
and satisfies (35), its last layer outputs h(T )
i
= [xi; yâ€²
i; bwT ; 0Dâˆ’2dâˆ’3; 1; ti], such that
bwT âˆ’wT
GD

2 â‰¤Îµg Â· (LÎ²âˆ’1Bx),
where {wâ„“
GD}â„“âˆˆ[L] is the sequence of gradient descent iterates with stepsize Î²âˆ’1 and initialization
w0
GD = 0. Notice that Proposition B.2 implies (with Îº := Î²/Î±)
wT
GD âˆ’wGLM

2 â‰¤exp(âˆ’T/(2Îº)) âˆ¥wGLMâˆ¥2 â‰¤exp(âˆ’T/(2Îº)) Â· Bw
2
:= Îµo.
Furthermore, we can show that (similar to the proof of Theorem D.1 (b)), there exists a single attention
layer Î¸(T +1) with M heads such that it outputs h(T +1)
N+1
= [xN+1; byN+1; bwT ; 0Dâˆ’2dâˆ’3; 1; 0], where
byN+1 âˆ’g(

xN+1, bwT 
)
 â‰¤Îµg.
In the following, we show that for suitably chosen (T, Îµg), Î¸ = (Î¸(1:T ), Î¸(T +1)) is the desired
transformer. First notice that its output h(T +1)
N+1
= [xN+1; byN+1; bwT ; 0Dâˆ’2dâˆ’3; 1; 0] satisfies
|byN+1 âˆ’g(âŸ¨xN+1, wGLMâŸ©)| â‰¤
byN+1 âˆ’g(

xN+1, bwT 
)
 + Lg

xN+1, bwT 
âˆ’âŸ¨xN+1, wGLMâŸ©

â‰¤Îµg + LgBx
bwT âˆ’wT
GD

2 + LgBx
wT
GD âˆ’wGLM

2
â‰¤Îµg(1 + LgBx Â· TÎ²âˆ’1Bx) + LgBxÎµo.
Therefore, for any fixed Îµ > 0, we can take
T = âŒˆ2Îº log(LgBxBw/Îµ)âŒ‰,
Îµg = 1
2
Îµ
1 + T Â· (LgB2xÎ²âˆ’1),
so that the Î¸ we construct above ensures |byN+1 âˆ’g(âŸ¨xN+1, wGLMâŸ©)| â‰¤Îµ for any input H that
satisfies (35). The upper bound on |||Î¸||| follows immediately from Theorem D.1.
G.2
Proof of Theorem G.2
We summarize some basic and useful facts about GLM in the following theorem. Its proof is presented
in Appendix G.3 - G.6.
Theorem G.3. Under Assumption B, the following statements hold with universal constant C0 and
constant C1, C2 that depend only on the parameters (Kx, Ky, BÂµ, Bw, Âµx, Lg, Âµg).
48

(a) As long as N â‰¥C1 Â· d, the following event happens with probability at least 1 âˆ’2eâˆ’N/C1:
Ew :
1
8ÂµgÂµx â‰¤Î»min(âˆ‡2bLN(w)) â‰¤Î»max(âˆ‡2bLN(w)) â‰¤8LgK2
x,
âˆ€w âˆˆB2(Bw).
(b) For any Î´ > 0, we have with probability at least 1 âˆ’Î´ that
Îµstat :=
sup
wâˆˆB2(Bw)
âˆ‡w bLN(w) âˆ’âˆ‡wE[bLN(w)]

2 â‰¤C0KxKy max
(r
dÎ¹ + log(1/Î´)
N
, dÎ¹ + log(1/Î´)
N
)
,
where we denote Î¹ = log(2 + LgK2
xBw/Ky).
(c) Condition on (a) holds and N â‰¥C2 Â· d, the event Er := {âˆ¥wGLMâˆ¥2 â‰¤Bw/2} happens with
probability at least 1 âˆ’eN/C2.
(d) For any w âˆˆB2(Bw), it holds that
Lp(w) âˆ’Lp(Î²) â‰¤
4
ÂµgÂµx

Îµ2
stat +
âˆ‡bLN(w)
2
2

.
(e) (Realizable setting) As long as wGLM âˆˆB2(Bw), it holds that
Ex(g(âŸ¨x, wGLMâŸ©) âˆ’g(âŸ¨x, Î²âŸ©))2 â‰¤
Lg
ÂµxÂµg
Îµ2
stat.
Therefore, we can set
Î± = ÂµgÂµx
8
,
Î² = 8LgK2
x,
Bx = C0Kx
p
d log(N/Î´),
By = C0Ky
p
log(N/Î´).
Consider the following good events
Eb = {âˆ€i âˆˆ[N], âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,N+1 = {âˆ¥xN+1âˆ¥2 â‰¤Bx, |yN+1| â‰¤By},
E = Er âˆ©Ew âˆ©Eb âˆ©Eb,N+1.
Under the event E and our choice of Î±, Î², the problem (ICGLM) is well-conditioned (i.e. (35) holds).
Theorem G.1 implies that there exists a transformer Î¸ such that for any input H of the form (3),
TFÎ¸ outputs hâ€²
N+1 = [xN+1; eyN+1; ew; 0Dâˆ’2dâˆ’3; 1; 0], such that the output is given by byN+1 =
g
ready(TFÎ¸(H)) = clipBy(eyN+1) and bw = g
readw(TFÎ¸(H)) := ProjB2(Bw)(ew), and the following
holds on the good event E:
(a) eyN+1 = fD(xN+1), where fD = A(D) is a predictor such that |fD(x) âˆ’g(âŸ¨x, wGLMâŸ©)| â‰¤
Îµ for all x âˆˆB2(Bx).
(b) ew = ew(D) âˆˆB2(Bw) depends only on D (by the proof of Theorem G.1 and Theorem D.1),
such that
âˆ‡bLN(ew)

2 â‰¤
Î²Îµ
LgBw .
In the following, we show that Î¸ constructed above fulfills both (a) & (b) of Theorem G.2. The
bounds on number of layers and heads and |||Î¸||| follows from plugging our choice of Bx, By in our
proof of Theorem G.1.
Proof of Theorem G.2 (a). Notice that under the good event E, we have bw = ew = ew(D) depends
only on D. Then we have
E(D,xN+1,yN+1)

â„“(bylin
N+1, yN+1)

= E(D,xN+1,yN+1)

1{E}â„“(bylin
N+1, yN+1)

+ E(D,xN+1,yN+1)

1{Ec}â„“(bylin
N+1, yN+1)

= E(D,xN+1,yN+1)[1{E}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)] + E(D,xN+1,yN+1)

1{Ec}â„“(bylin
N+1, yN+1)

.
Thus, we can consider E0 = Er âˆ©Ew âˆ©Eb, and then
E(D,xN+1,yN+1)[1{E}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)]
49

= E(D,xN+1,yN+1)[1{E0}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)] âˆ’E(D,xN+1,yN+1)[1{E0 âˆ’E}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)]
= E(D,xN+1,yN+1)[1{E0}Lp(ew(D))] âˆ’E(D,xN+1,yN+1)[1{E0 âˆ’E}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)],
where the second equality follows from Lp(bw(D)) = E(xN+1,yN+1)|Dâ„“(âŸ¨xN+1, ew(D)âŸ©, yN+1).
Therefore,
E(D,xN+1,yN+1)

â„“(bylin
N+1, yN+1)

âˆ’ED[1{E0}Lp(ew(D))]
= E(D,xN+1,yN+1)

1{Ec}â„“(bylin
N+1, yN+1)

âˆ’E(D,xN+1,yN+1)[1{E0 âˆ’E}â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)]
â‰¤2
q
P(Ec) Â· max

E

â„“(bylin
N+1, yN+1)4
, E[â„“(âŸ¨xN+1, ew(D)âŸ©, yN+1)4]
	
= O
 B2
â„“
N 5

,
where the last line follows from Cauchy inequality and the fact P(Ec) = O
 N âˆ’10
, and Bâ„“is
defined in Lemma G.1.
Notice that by Theorem G.3 (d), we have
ED[1{E0}(Lp(ew) âˆ’inf Lp)] â‰¤
4
ÂµgÂµx

E[Îµ2
stat] + E
h
1{E0}
âˆ‡bLN(ew)
2
2
i
,
and by Theorem G.3 (b) and taking integration over Î´ > 0, we have
E[Îµ2
stat] â‰¤O (1) Â· K2
xK2
y
 
dÎ¹
N +
dÎ¹
N
2!
.
Also, we have inf Lp = Lp(Î²â‹†) â‰¤Bâ„“by Lemma G.1. Therefore, we can conclude that
E(D,xN+1,yN+1)

â„“(bylin
N+1, yN+1)

â‰¤inf Lp + O (1) Â·
 
K2
xK2
yÎ¹
ÂµgÂµx
d
N +
K4
x
ÂµgÂµxBw
Îµ2 + B2
â„“
N 5
!
.
Taking Îµ2 â‰¤
K2
yÎ¹
BwK2x
d
N completes the proof.
Proof of Theorem G.2 (b). Similar to the proof of Corollary 6, we have
E(byN+1 âˆ’yN+1)2 = E

1{E}(byN+1 âˆ’yN+1)2
+ E

1{Ec}(byN+1 âˆ’yN+1)2
â‰¤E

1{E}(eyN+1 âˆ’yN+1)2
+
p
P(Ec)E(byN+1 âˆ’yN+1)4,
where the inequality follows from yN+1 âˆˆ[âˆ’By, By] on event E. For the first part, we have
E
h
1{E}(eyN+1 âˆ’yN+1)2i
= E
h
1{E}(fD(xN+1) âˆ’yN+1)2i
â‰¤ED
h
1{E0} Â· E(x,y)âˆ¼P
h
1{âˆ¥xâˆ¥2 â‰¤Bx}(fD(x) âˆ’y)2ii
,
where we use the fact that the conditional distribution of (xN+1, yN+1)|D agrees with P. Thus,
E
h
1{E}(eyN+1 âˆ’yN+1)2i
âˆ’E(x,y)âˆ¼P(g(âŸ¨x, Î²âŸ©) âˆ’y)2
â‰¤ED
h
1{E0} Â·

E(x,y)âˆ¼P1{âˆ¥xâˆ¥2 â‰¤Bx}(fD(x) âˆ’y)2 âˆ’E(x,y)âˆ¼P(g(âŸ¨x, Î²âŸ©) âˆ’y)2i
â‰¤ED
h
1{E0} Â· Ex1{âˆ¥xâˆ¥2 â‰¤Bx}(fD(x) âˆ’g(âŸ¨x, Î²âŸ©))2i
â‰¤2ED
h
1{E0} Â· Ex1{âˆ¥xâˆ¥2 â‰¤Bx}(fD(x) âˆ’g(âŸ¨x, wGLMâŸ©))2i
+ 2ED
h
1{E0} Â· Ex(g(âŸ¨x, wGLMâŸ©) âˆ’g(âŸ¨x, Î²âŸ©))2i
â‰¤2Îµ2 + 2Lg
ÂµxÂµg
E[Îµ2
stat] â‰¤2Îµ2 + O (1) Â· LgK2
xK2
yÎ¹
ÂµxÂµg
d
N .
For the second part, we know P(Ec) = O
 N âˆ’10
and
E(byN+1 âˆ’yN+1)4 â‰¤8Eby2
N+1 + 8Ey4
N+1 = O
 B4
y

.
In conclusion, we have
E(byN+1 âˆ’yN+1)2 â‰¤E(yN+1 âˆ’g(âŸ¨xN+1, Î²âŸ©))2 + 2Îµ2 + O (1) Â· LgK2
xK2
yÎ¹
ÂµxÂµg
d
N + O
 
B2
y
N 5
!
.
Taking Îµ2 â‰¤
LgK2
xK2
yÎ¹
ÂµxÂµg
d
N completes the proof.
50

Lemma G.1. Suppose that x âˆ¼SG(Kx), y âˆ¼SG(Ky), and w is a (possibly random) vector such
that âˆ¥wâˆ¥2 â‰¤Bw. Then
E

â„“(âŸ¨x, wâŸ©, y)41/4 â‰¤O
 LgK2
xB2
wd + KxKyBwd

=: Bâ„“.
Proof. Notice that by our assumption, |g(0)| â‰¤2Ky. Therefore, by the definition of â„“,
|â„“(t, y)| =
âˆ’yt +
Z t
0
g(s)ds
 â‰¤|t(g(0) âˆ’y)| +

Z t
0
(g(s) âˆ’g(0))ds
 â‰¤|t| (2Ky + |y|) + 2Lgt2.
The proof is then done by bounding the moment by E |y|8 â‰¤O
 K8
y

and E |âŸ¨x, wâŸ©|8 â‰¤
B8
wE âˆ¥xâˆ¥8
2 â‰¤O

(
âˆš
dBwKx)8
, which is standard (by utilizing the tail bound of sub-Gaussian/sub-
Exponential random variable).
G.3
Proof of Theorem G.3 (a)
We begin with the upper bound on Î»max(âˆ‡2bLN(w)). By Lemma B.3, as long as N â‰¥C0 Â· d, the
following event
Ew,0 :

1
N
N
X
i=1
xixâŠ¤
i

op
â‰¤8K2.
happens with probability at least 1 âˆ’exp(âˆ’N/C0). By the assumption that sup |gâ€²| â‰¤Lg, it is clear
that when Ew,0 holds, we have Î»max(âˆ‡2bLN(w)) â‰¤8LgK2
x âˆ€w âˆˆRd.
In the following, we analyze the quantity Î»max(âˆ‡2bLN(w)). We have to invoke the following
covering argument (see e.g. [85, Section 4.1.1]).
Lemma G.2. Suppose that V is a Îµ-covering of Sdâˆ’1 with Îµ âˆˆ[0, 1). Then the following holds:
1. For any d Ã— d symmetric matrix A, âˆ¥Aâˆ¥op â‰¤
1
1âˆ’2Îµ maxvâˆˆV
vâŠ¤Av
 and
Î»min(A) â‰¥min
vâˆˆV vâŠ¤Av âˆ’2Îµ âˆ¥Aâˆ¥op
2. For any vector x âˆˆRd, âˆ¥xâˆ¥2 â‰¤
1
1âˆ’Îµ maxvâˆˆV |âŸ¨v, xâŸ©|.
Notice that
âˆ‡2bLN(w) = 1
N
N
X
i=1
gâ€²(âŸ¨w, xiâŸ©)xixâŠ¤
i âª°1
N
N
X
i=1
ÂµgI(|âŸ¨w, xiâŸ©| â‰¤BÂµ)xixâŠ¤
i
âª°1
N
N
X
i=1
Âµg

1 âˆ’|âŸ¨w, xiâŸ©|
BÂµ

+
xixâŠ¤
i .
Therefore, we can define h(t) := (BÂµ âˆ’|t|)+ (which is a 1-Lipschitz function), and we have
âˆ‡2bLN(w) âª°Âµg
BÂµ
1
N
N
X
i=1
h(âŸ¨w, xiâŸ©)xixâŠ¤
i
|
{z
}
=:A(w)
.
In the following, we pick a Îµv-covering V of Sdâˆ’1 such that |V| â‰¤(3/Îµv)d (we will specify Îµv later
in proof). Then for any w âˆˆB2(Bw),
Î»min(A(w)) â‰¥min
vâˆˆV vâŠ¤A(w)v âˆ’2Îµv âˆ¥A(w)âˆ¥op
By our definition of A(w), we have (for any fixed Bxv)
min
vâˆˆV vâŠ¤A(w)v = min
vâˆˆV
1
N
N
X
i=1
h(âŸ¨w, xiâŸ©) âŸ¨v, xiâŸ©2
51

â‰¥min
vâˆˆV
1
N
N
X
i=1
h(âŸ¨w, xiâŸ©) min
n
âŸ¨v, xiâŸ©2 , B2
xv
o
|
{z
}
=:Uv(w)
â‰¥min
vâˆˆV E[Uv(w)] + min
vâˆˆV (Uv(w) âˆ’E[Uv(w)]).
By Lemma G.3, we can choose Bxv = Kx(15 + log(K2
x/Âµx)), and then E[Uv(w)] â‰¥3BÂµÂµx/8.
Thus, combining the inequalities above, we can take Îµv = 128K2
x
Âµx
in the following, so that under
event Ew,0,
Î»min(âˆ‡2bLN(w)) â‰¥ÂµgÂµx
8
+ Âµg
BÂµ
BÂµÂµx
16
âˆ’max
vâˆˆV (E[Uv(w)] âˆ’Uv(w))

.
In the following, we consider the random process

U v(w) := Uv(w) âˆ’E[Uv(w)]
	
w, which is
zero-mean and indexed by w âˆˆB2(Bw). For any fixed v, consider applying Proposition B.4 to the
random process

U v(w)
	
w. We need to verify the preconditions:
(a) With norm Ï(w, wâ€²) = âˆ¥w âˆ’wâ€²âˆ¥2, log N(BÏ(w, r), Î´) â‰¤d log(2Ar/Î´) with constant A = 2;
(b) Let f(x; w) := h(âŸ¨w, xiâŸ©) min
n
âŸ¨v, xiâŸ©2 , B2
xv
o
, then |f(x; w)| â‰¤BÂµB2
xv and hence in
SG(CBÂµB2
xv) for any random x;
(c) For w, wâ€² âˆˆW, we have |h(âŸ¨w, xiâŸ©) âˆ’h(âŸ¨wâ€², xiâŸ©)| â‰¤|âŸ¨w âˆ’wâ€², xiâŸ©|. Hence, because x âˆ¼
SG(Kx), the random variable h(âŸ¨w, xâŸ©) âˆ’h(âŸ¨wâ€², xâŸ©) is SG(CKxâˆ¥w âˆ’wâ€²âˆ¥2), and the random
variable f(x; w) âˆ’f(x; wâ€²) is SG(CKxB2
xvâˆ¥w âˆ’wâ€²âˆ¥2).
Therefore, we can apply Proposition B.4 to obtain that with probability 1 âˆ’Î´0, it holds
sup
w
U v(w)
 â‰¤Câ€²BÂµB2
xv
"r
d log(2Îºg) + log(1/Î´0)
N
#
,
where we denote Îºg = 1 + KxBw/BÂµ. Setting Î´0 = Î´/ |V| and taking the union bound over v âˆˆV,
we obtain that with probability at least 1 âˆ’Î´,
max
vâˆˆV
sup
âˆ¥wâˆ¥2â‰¤Bw
U v(w)
 â‰¤Câ€²BÂµB2
xv
"r
d log(8Îºg/Îµv) + log(1/Î´)
N
#
,
where we use log |V| â‰¤d log(4/Îµv). Therefore, we plug in the definition of Îµv and Bxv to deduce
that, if we set
C1 =
16Câ€²B2
xv
Âµx
2
log(8Îºg/Îµv),
Îµv = 128K2
x
Âµx
,
Bxv = Kx(15 + log(K2
x/Âµx)),
then as long as N â‰¥C1 Â· d, it holds maxvâˆˆV E[Uv(w)] âˆ’Uv(w) â‰¤ÂµxBÂµ
16
with probability at least
1 âˆ’exp(âˆ’N/C1). This is the desired result.
Lemma G.3. Under Assumption B, for Bxv = Kx(15 + log(K2
x/Âµx)), it holds
inf
wâˆˆB2(Bw),vâˆˆSdâˆ’1 E[1{|xâŠ¤w| â‰¤BÂµ/2}(xâŠ¤v)21{|xâŠ¤v| â‰¤Bxv}] â‰¥3Âµx/4.
Proof. For any fixed w âˆˆB2(Bw), v âˆˆSdâˆ’1,
E[1{|xâŠ¤w| â‰¤BÂµ/2}(xâŠ¤v)21{|xâŠ¤v| â‰¤Bxv}]
= E[1{|xâŠ¤w| â‰¤BÂµ/2}(xâŠ¤v)2}] âˆ’E[1{|xâŠ¤w| â‰¤BÂµ/2}(xâŠ¤v)21{|xâŠ¤v| > Bxv}]
â‰¥Âµx âˆ’E[(xâŠ¤v)21{|xâŠ¤v| > Bxv}].
Because x âˆ¼SG(Kx), xâŠ¤v âˆ¼SG(Kx), and a simple calculation yields
E[(xâŠ¤v)21{|xâŠ¤v| > tKx}] â‰¤2K2
x(t2 + 1) exp(âˆ’t2).
Taking t = 15 + log(K2
x/Âµx) gives E[(xâŠ¤v)21{|xâŠ¤v| > Bxv}] â‰¤Âµx/4, which completes the
proof.
52

G.4
Proof of Theorem G.3 (b)
Notice that
âˆ‡bLN(w) = 1
N
N
X
i=1
(g(âŸ¨w, xiâŸ©) âˆ’yi)xi.
In the following, we pick a minimal 1/2-covering of Sdâˆ’1 (so |V| â‰¤5d). Then by Lemma G.2, it
holds
âˆ‡bLN(w) âˆ’E[âˆ‡bLN(w)]

2 â‰¤2 max
vâˆˆV
 âŸ¨âˆ‡bLN(w), vâŸ©âˆ’E[âŸ¨âˆ‡bLN(w), vâŸ©]
|
{z
}
=:Xv(w)
Fix a v âˆˆSdâˆ’1 and set Î´â€² = Î´/|V|. We proceed to bound supw |Xv(w)| by applying Proposition B.4
to the random process {Xv(w)}w. We need to verify the preconditions:
(a) With norm Ï(w, wâ€²) = âˆ¥w âˆ’wâ€²âˆ¥2, log N(Î´; BÏ(r), Ï) â‰¤d log(2Ar/Î´) with constant A = 2;
(b) For z = [x; y], we let f(z; w) := (g(âŸ¨w, xâŸ©) âˆ’y) âŸ¨x, vâŸ©, then f(z; w) âˆ¼SE(CKxKy) for any
w by our assumption on (x, y);
(c) For w, wâ€² âˆˆW, we have |g(âŸ¨w, xâŸ©) âˆ’g(âŸ¨wâ€², xâŸ©)| â‰¤Lg |âŸ¨w âˆ’wâ€², xâŸ©|. Hence, because x âˆ¼
SG(Kx), the random variable g(âŸ¨w, xiâŸ©) âˆ’g(âŸ¨wâ€², xiâŸ©) is sub-Gaussian in SG(KxLgâˆ¥w âˆ’wâ€²âˆ¥2).
Thus, f(z; w) âˆ’f(z; wâ€²) is sub-exponential in SE(CK2
xLgâˆ¥w âˆ’wâ€²âˆ¥2).
Therefore, we can apply Proposition B.4 to obtain that with probability 1 âˆ’Î´0, it holds
sup
w |Xv(w)| â‰¤Câ€²KxKy
"r
d log(2Îºy) + log(1/Î´0)
N
+ d log(2Îºy) + log(1/Î´0)
N
#
,
where we denote Îºy = 1 + LgK2
xBw/Ky. Setting Î´0 = Î´/ |V| and taking the union bound over
v âˆˆV, we obtain that with probability at least 1 âˆ’Î´,
max
vâˆˆV
sup
âˆ¥wâˆ¥2â‰¤Bw
|Xv(w)| â‰¤Câ€²KxKy
"r
d log(10Îºy) + log(1/Î´)
N
+ d log(10Îºy) + log(1/Î´)
N
#
.
This is the desired result.
G.5
Proof of Theorem G.3 (c)
In the following, we condition on (a) holds, i.e. bLN is Î±-strongly-convex and Î² smooth over B2(Bw)
with Î± = ÂµxÂµg/8 and Î² = 8LgK2
x. We define
ew = arg min
wâˆˆB2(Bw)
bLN(w).
Then by standard convex analysis, we have
Î± âˆ¥ew âˆ’Î²â‹†âˆ¥2
2 â‰¤
D
âˆ‡bLN(ew) âˆ’âˆ‡bLN(Î²â‹†), ew âˆ’Î²â‹†E
â‰¤
D
âˆ’âˆ‡bLN(Î²â‹†), ew âˆ’Î²â‹†E
â‰¤
âˆ‡bLN(Î²â‹†)

2 âˆ¥ew âˆ’Î²â‹†âˆ¥2 .
Notice that
âˆ‡bLN(Î²â‹†)

2 â‰¤Îµstat, we can conclude that
âˆ¥ewâˆ¥2 â‰¤âˆ¥Î²â‹†âˆ¥2 + Îµstat
Î± .
Recall that we assume âˆ¥Î²â‹†âˆ¥2 â‰¤Bw/4, we can then consider Es := {Îµstat < Î±Bw/4}. Once
Es holds, our argument above yields âˆ¥ewâˆ¥2 < Bw, which implies âˆ‡bLN(ew) = 0. Therefore,
ew = arg minwâˆˆRd bLN(w). Further, by Theorem G.3, we can set
C2 := max
(
2Î¹
32Î±KxKy
Bw
2
, 2Î¹ Â· 32Î±KxKy
Bw
)
,
so that as long as N â‰¥C2d, the event Es holds with probability at least 1 âˆ’exp(âˆ’N/C2). This is
the desired result.
53

G.6
Proof of Theorem G.3 (d) & (e)
We first prove Theorem G.3 (d). Notice that
âˆ‡2Lp(w) = E

gâ€²(âŸ¨x, wâŸ©)xxâŠ¤
âª°E

ÂµgI(|âŸ¨x, wâŸ©| â‰¤BÂµ)xxâŠ¤
âª°ÂµgÂµxId, âˆ€w âˆˆB2(Bw).
Therefore, Lp is (ÂµgÂµx)-strongly-convex over B2(Bw). Therefore, because Î²â‹†âˆˆB2(Bw) is the
global minimum of Lp, it holds that for all w âˆˆB2(Bw),
Lp(w) âˆ’Lp(Î²â‹†) â‰¤
1
2ÂµgÂµx
âˆ¥âˆ‡Lp(w)âˆ¥2
2 .
By the definition of Îµstat, âˆ¥âˆ‡Lp(w)âˆ¥2 â‰¤Îµstat + âˆ¥âˆ‡bLN(w)âˆ¥2, and hence the proof of Theorem G.3
(d) is completed.
We next prove Theorem G.3 (e), where we assume that E[y|x] = g(âŸ¨x, Î²âŸ©) (which implies Î²â‹†= Î²
directly) and wGLM âˆˆB2(Bw). Notice that
âˆ‡Lp(w) = E
h
âˆ‡bLN(w)
i
= E[(g(âŸ¨x, wâŸ©) âˆ’y)x] = E[(g(âŸ¨x, wâŸ©) âˆ’g(âŸ¨w, Î²âŸ©))x],
and hence
âŸ¨âˆ‡Lp(wGLM), wGLM âˆ’Î²âŸ©= E[(g(âŸ¨x, wGLMâŸ©) âˆ’g(âŸ¨w, Î²âŸ©)) Â· (âŸ¨x, wGLMâŸ©âˆ’âŸ¨w, Î²âŸ©)]
â‰¥1
Lg
E

(g(âŸ¨x, wGLMâŸ©) âˆ’g(âŸ¨w, Î²âŸ©))2
.
On the other hand, by the (ÂµgÂµx)-strong-convexity of Lp over B2(Bw), it holds that
âŸ¨âˆ‡Lp(wGLM), wGLM âˆ’Î²âŸ©â‰¤
1
ÂµgÂµx
âˆ¥âˆ‡Lp(wGLM)âˆ¥2
2 .
Finally, using the definition of wGLM, we have âˆ‡bLN(wGLM) = 0, and hence âˆ¥âˆ‡Lp(wGLM)âˆ¥2 â‰¤
Îµstat, which completes the proof of Theorem G.3 (e).
H
Proofs for Section 3.2
H.1
Proof of Theorem 7
Fix Î»N â‰¥0, Î² > 0 and Bw > 0, and consider any in-context data D such that the precondition
of Theorem 7 holds. Recall that
Llasso(w) :=
1
2N
N
X
i=1
(âŸ¨w, xiâŸ©âˆ’yi)2 + Î»N âˆ¥wâˆ¥1
denotes the lasso regression loss in (ICLasso), so that wlasso = arg minwâˆˆRd Llasso(w). We further
write
bL0
N(w) :=
1
2N
N
X
i=1
(âŸ¨w, xiâŸ©âˆ’yi)2,
R(w) := Î»N âˆ¥wâˆ¥1 .
Note that âˆ‡2bL0
N(w) = XâŠ¤X/N and thus bL0
N is Î²-smooth over Rd.
Consider the proximal gradient descent algorithm on the ridge loss
wt+1
PGD = proxÎ·R

wt
PGD âˆ’Î·âˆ‡bL0
N(wt
PGD)

with initialization w0
PGD := 0d, learning rate Î· := Î²âˆ’1, and number of steps T to be specified later.
Similar to the proof of Theorem 4, we can construct a transformer to approximate wT
GD. Consider
â„“(s, t) = 1
2(s âˆ’t)2 and R(w) = Î»N âˆ¥wâˆ¥1, then âˆ‚sâ„“(s, t) is (0, +âˆ, 2, 4)-approximable by sum
of relus (cf. Definition D.1), and proxÎ·R is (0, +âˆ, 4d, 4 + 2Î·Î»N)-approximable by sum of relus
(Proposition D.1). Therefore, we can apply Theorem D.2 with the square loss â„“, regularizer R,
learning rate Î· and accuracy parameter 0 to obtain that there exists a transformer TFÎ¸ with (T + 1)
54

layers, number of heads M (â„“) = 2 for all â„“âˆˆ[L], and hidden dimension Dâ€² = 2d, such that the
final output h(L)
N+1 = [xN+1; byN+1; wT
PGD; âˆ—] with byN+1 =

wT
PGD, xN+1

. Further, the weight
matrices have norm bounds |||Î¸||| â‰¤10R + (8 + 2Î»N)Î²âˆ’1.
By the standard convergence result for proximal gradient descent (Proposition B.3), we have for all
t â‰¥1 that
Llasso(wt
PGD) âˆ’Llasso(wlasso) â‰¤Î²
2t âˆ¥wlassoâˆ¥2
2 .
Plugging in âˆ¥wlassoâˆ¥2 â‰¤Bw/2 and T = L âˆ’1 =

Î²B2
w/Îµ

finishes the proof.
H.2
Sharper convergence analysis of proximal gradient descent for Lasso
Collection of parameters
Throughout the rest of this section, we consider fixed N â‰¥1, Î»N =
q
ÏÎ½ log d
N
for Ï â‰¥0, Î½ â‰¥0 fixed (and to be determined), fixed 0 < Î± â‰¤Î², and fixed Bâ‹†
w > 0. We
write Îº := Î²/Î±, Îºs := Î²(Bâ‹†
w)2/Î½2, and Ï‰N := Ï
Î±
s log d
N
.
Here we present a sharper convergence analysis on the proximal gradient descent algorithm for Llasso
under the following well-conditionedness assumption, which will be useful for proving Theorem 8 in
the sequel.
Assumption C (Well-conditioned property for Lasso). We say the (ICLasso) problem is well-
conditioned with sparsity s if the following conditions hold:
1. The (Î±, Ï)-RSC condition holds:
âˆ¥Xwâˆ¥2
2
N
â‰¥Î± âˆ¥wâˆ¥2
2 âˆ’Ïlog d
N
âˆ¥wâˆ¥2
1 ,
âˆ€w âˆˆRd.
(38)
Further, Î»max(XâŠ¤X/N) â‰¤Î².
2. The data (X, y) is â€œapproximately generated from a s-sparse linear modelâ€: There exists a
wâ‹†âˆˆRd such that âˆ¥wâ‹†âˆ¥2 â‰¤Bâ‹†
w, âˆ¥wâ‹†âˆ¥0 â‰¤s and for the residue Îµ = y âˆ’Xwâ‹†,
XâŠ¤Îµ

âˆâ‰¤1
2NÎ»N.
3. It holds that N â‰¥32 Ï
Î± Â· s log d (i.e. 32Ï‰N â‰¤1).
Assumption C1 imposes the standard restricted strong convexity (RSC) condition for the feature
matrix X âˆˆRNÃ—d, and Assumption C2 asserts that the data is approximately generated from a
sparse linear model, with a bound on the Lâˆnorm of the error vector XâŠ¤Îµ. Assumption C is
entirely deterministic in nature, and suffices to imply the following convergence result. In the proof
of Theorem 8, we show that Assumption C is satisfied with high probability when data is generated
from the standard sparse linear model considered therein.
Theorem H.1 (Sharper convergence guarantee for Lasso). Under Assumption C, for the PGD
iterates {wt}tâ‰¥0 on loss function bLlasso with stepsize Î· = 1/Î² and starting point w0 = 0, we have
bLlasso(wT ) âˆ’bLlasso(wlasso) â‰¤Îµ for all
T â‰¥C
Î²(Bâ‹†
w)2
Î½
+ Îº log

C Â· Îº Â· Î²(Bâ‹†
w)2
Î½
Â· Î½
Îµ

+ ÎºÎ½Ï‰2
N
Îµ

,
where C is a universal constant.
The proof can be found in Appendix H.4. Combining Theorem H.1 with the construction in Theorem 7,
we directly obtain the following result as a corollary.
Theorem H.2 (In-context Lasso with transformers with sharper convergence). For any N, d, s â‰¥1,
0 < Î± â‰¤Î², Î½ â‰¥0, Ï â‰¥0, there exists a L-layer transformer TFÎ¸ with
L =

C
 Îºs + Îº(log(CÎºs/Îµ) + Î½Ï‰2
N/Îµ)

,
max
â„“âˆˆ[L] M (â„“) â‰¤2,
max
â„“âˆˆ[L] D(â„“) â‰¤2d,
55

|||Î¸||| â‰¤3 + R + (8 + 2Î»N)Î²âˆ’1,
such that the following holds. On any input data (D, xN+1) such that the (ICLasso) problem
satisfies Assumption C (which implies âˆ¥wlassoâˆ¥2 â‰¤Bw/2 with Bw = 2Bâ‹†
w +
p
Î½/Î±), TFÎ¸(H(0))
approximately implements (ICLasso), in that it outputs byN+1 = ready(TFÎ¸(H)) = âŸ¨xN+1, bwâŸ©with
bLlasso(bw) âˆ’bLlasso(wlasso) â‰¤Îµ.
H.3
Basic properties for Lasso
Lemma H.1 (Relaxed basic inequality). Suppose that Assumption C2 holds. Then it holds that
âˆ¥w âˆ’wâ‹†âˆ¥1 â‰¤4âˆšs âˆ¥w âˆ’wâ‹†âˆ¥2 + 2
Î»N

bLlasso(w) âˆ’bLlasso(wâ‹†)

,
âˆ€w âˆˆRd.
As a corollary, âˆ¥wlasso âˆ’wâ‹†âˆ¥1 â‰¤4âˆšs âˆ¥wlasso âˆ’wâ‹†âˆ¥2.
Proof. Let us first fix any w âˆˆRd. Denote âˆ†= w âˆ’wâ‹†, and let S = supp(wâ‹†) be the set of
indexes of nonzero entries of wâ‹†. Then by definition, y = Xwâ‹†+ Îµ and |S| â‰¤s, and hence
âˆ¥Xw âˆ’yâˆ¥2
2 âˆ’âˆ¥Xwâ‹†âˆ’yâˆ¥2
2 = âˆ¥Xâˆ†âˆ’Îµâˆ¥2
2 âˆ’âˆ¥Îµâˆ¥2
2 = âˆ¥Xâˆ†âˆ¥2
2 âˆ’2ÎµâŠ¤Xâˆ†,
âˆ¥wâˆ¥1 âˆ’âˆ¥wâ‹†âˆ¥1 =
X
jâˆˆS
(|w[j]| âˆ’|wâ‹†[j]|) +
X
jÌ¸âˆˆS
|w[j]|
â‰¥âˆ’
X
jâˆˆS
|w[j] âˆ’wâ‹†[j]| +
X
jÌ¸âˆˆS
|w[j]| = âˆ¥âˆ†Scâˆ¥1 âˆ’âˆ¥âˆ†Sâˆ¥1 .
Combining these inequalities, we obtain
0 â‰¤
1
2N âˆ¥Xâˆ†âˆ¥2
2 â‰¤ÎµâŠ¤Xâˆ†
N
+ Î»N(âˆ¥âˆ†Sâˆ¥1 âˆ’âˆ¥âˆ†Scâˆ¥1) + bLlasso(w) âˆ’bLlasso(wâ‹†)
â‰¤Î»N
2 âˆ¥âˆ†âˆ¥1 + Î»N(âˆ¥âˆ†Sâˆ¥1 âˆ’âˆ¥âˆ†Scâˆ¥1) + bLlasso(w) âˆ’bLlasso(wâ‹†)
= Î»N
2 (3 âˆ¥âˆ†Sâˆ¥1 âˆ’âˆ¥âˆ†Scâˆ¥1) + bLlasso(w) âˆ’bLlasso(wâ‹†),
(39)
where the second inequality follows from ÎµâŠ¤Xâˆ†
N
â‰¤âˆ¥XâŠ¤Îµâˆ¥âˆ
N
âˆ¥âˆ†âˆ¥1 and our assumption that
2âˆ¥XâŠ¤Îµâˆ¥âˆ
N
â‰¤Î»N, and the last inequality is due to âˆ¥âˆ†âˆ¥1 = âˆ¥âˆ†Sâˆ¥1 + âˆ¥âˆ†Scâˆ¥1. Therefore, we
have
âˆ¥âˆ†âˆ¥1 = âˆ¥âˆ†Sâˆ¥1 + âˆ¥âˆ†Scâˆ¥1 â‰¤4 âˆ¥âˆ†Sâˆ¥1 + 2
Î»N

bLlasso(w) âˆ’bLlasso(wâ‹†)

â‰¤4âˆšs âˆ¥âˆ†âˆ¥2 + 2
Î»N

bLlasso(w) âˆ’bLlasso(wâ‹†)

,
where the last inequality follows from âˆ¥âˆ†Sâˆ¥1 â‰¤âˆšs âˆ¥âˆ†Sâˆ¥2 â‰¤âˆšs âˆ¥âˆ†âˆ¥2. This completes the proof
of our main inequality. As for the corollary, we only need to use the definition that bLlasso(wlasso) â‰¤
bLlasso(wâ‹†).
Proposition H.1 (Gap to parameter estimation error). Suppose that Assumption C holds. Then for all
w âˆˆRd,
âˆ¥w âˆ’wâ‹†âˆ¥2
2 â‰¤C
sÎ»2
N
Î±2 + Î½âˆ’1gap2 + gap

,
where we write gap := bLlasso(w) âˆ’bLlasso(wlasso), and C = 120 is a universal constant. In
particular, we have âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 â‰¤10 ÏÎ½
Î±2
s log d
N
.
56

Proof. We follow the notation in the proof of Lemma H.1. By (39), we have
0 â‰¤
1
2N âˆ¥Xâˆ†âˆ¥2
2 â‰¤Î»N
2 (3 âˆ¥âˆ†Sâˆ¥1 âˆ’âˆ¥âˆ†Scâˆ¥1) + bLlasso(w) âˆ’bLlasso(wâ‹†),
and hence âˆ¥âˆ†âˆ¥1 â‰¤4âˆšs âˆ¥âˆ†âˆ¥2 + 2gap
Î»N due to bLlasso(w) âˆ’bLlasso(wâ‹†) â‰¤gap. On the other hand, by
the RSC condition (38), it holds that
âˆ¥Xâˆ†âˆ¥2
2
N
â‰¥Î± âˆ¥âˆ†âˆ¥2
2 âˆ’Ïlog d
N
âˆ¥âˆ†âˆ¥2
1 .
Therefore, we have
Î± âˆ¥âˆ†âˆ¥2
2 â‰¤3Î»N
âˆšs âˆ¥âˆ†âˆ¥2 + Ïlog d
N
âˆ¥âˆ†âˆ¥2
1 + 2gap
â‰¤3Î»N
âˆšs âˆ¥âˆ†âˆ¥2 + Ïlog d
N

4âˆšs âˆ¥âˆ†âˆ¥2 + 2gap
Î»N
2
+ 2gap
â‰¤5sÎ»2
N
Î±
+ Î±
6 âˆ¥âˆ†âˆ¥2
2 + Ï20s log d
Î»2
NN
âˆ¥âˆ†âˆ¥2
2 + Ï20 log d
N
gap2 + 2gap,
where the last inequality uses AM-GM inequality and Cauchy inequality. Notice that Ï 20s log d
N
â‰¤2
3Î±,
we now derive that
âˆ¥âˆ†âˆ¥2
2 â‰¤30sÎ»2
N
Î±2
+ Ï120 log d
Î»2
NN
gap2 + 12gap.
Plugging in Î»N =
q
ÏÎ½ log d
N
completes the proof. The corollary follows immediately by letting
w = wlasso in above proof (hence gap = 0).
Lemma H.2 (Growth). It holds that
1
2N âˆ¥X(w âˆ’wlasso)âˆ¥2
2 â‰¤bLlasso(w) âˆ’bLlasso(wlasso),
âˆ€w.
Proof. For simplicity we denote wlasso := wlasso. By the first order optimality condition, it holds
that
0 âˆˆ1
N XâŠ¤(Xwlasso âˆ’y) + âˆ‚R(wlasso),
where we write R(w) := Î»N âˆ¥wâˆ¥1. Then by the convexity of R, we have
R(w) âˆ’R(wlasso) â‰¥âŸ¨âˆ‚R(wlasso), w âˆ’wlassoâŸ©=

âˆ’1
N XâŠ¤(Xwlasso âˆ’y), w âˆ’wlasso

= âˆ’1
N âŸ¨Xwlasso âˆ’y, (Xw âˆ’y) âˆ’(Xwlasso âˆ’y)âŸ©
= âˆ’1
2N âˆ¥Xw âˆ’yâˆ¥2
2 + 1
2N âˆ¥Xwlasso âˆ’yâˆ¥2
2 + 1
2N âˆ¥X(w âˆ’wlasso)âˆ¥2
2 .
Rearranging completes the proof.
H.4
Proof of Theorem H.1
For the simplicity of presentation, we write wlasso = wlasso and we denote gapt := bLlasso(wt) âˆ’
bLlasso(wlasso).
By Lemma H.1, we have âˆ¥wt âˆ’wâ‹†âˆ¥1 â‰¤4âˆšs âˆ¥wt âˆ’wâ‹†âˆ¥2 + 2gapt
Î»N , which implies
wt âˆ’wlasso

1 â‰¤
wt âˆ’wâ‹†

1 + âˆ¥wlasso âˆ’wâ‹†âˆ¥1 â‰¤4âˆšs
wt âˆ’wlasso

2 + 8âˆšs âˆ¥wlasso âˆ’wâ‹†âˆ¥2 + 2gapt
Î»N
.
57

We denote ÂµN = Ï2 log d
N . Using the assumption that X is (Î±, Ï)-RSC, we obtain that
1
N
X(wt âˆ’wlasso)
2
2 â‰¥Î±
wt âˆ’wlasso
2
2 âˆ’ÂµN
wt âˆ’wlasso
2
1
â‰¥Î±
wt âˆ’wlasso
2
2 âˆ’ÂµN

20s
wt âˆ’wlasso
2
2 + 640s âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 + 40
Î»2
N
(gapt)2

.
Thus, as long as N â‰¥30Ï2s log d
Î±
, we have
Î±
3
wt âˆ’wlasso
2
2 â‰¤1
N
X(wt âˆ’wlasso)
2
2 + 640sÂµN âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 + 40ÂµN
Î»2
N
(gapt)2
â‰¤2gapt + 40Î½âˆ’1(gapt)2 + 640sÂµN âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 ,
where the last inequality follows from Lemma H.2 and the definition of Î»N, ÂµN.
We define Îµstat := 640sÂµN âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2, T0 := 10Î²Î½âˆ’1 âˆ¥wlassoâˆ¥2
2. By Proposition B.3(3), it
holds that for t â‰¥T0,
gapt â‰¤Î²
2t âˆ¥wlassoâˆ¥2
2 â‰¤
Î²
2T0
âˆ¥wlassoâˆ¥2
2 = Î½
20.
Then for all t â‰¥T0 âˆ’1, we have (the second â‰¤below uses Proposition B.3(2))
Î±
3
wt+1 âˆ’wlasso
2
2 â‰¤4gapt+1 + Îµstat â‰¤2Î²
wt âˆ’wlasso
2
2 âˆ’
wt+1 âˆ’wlasso
2
2

+ Îµstat,
â‡’
wt+1 âˆ’wlasso
2
2 âˆ’3Îµstat
Î±
â‰¤

1 + Î±
6Î²
âˆ’1wt âˆ’wlasso
2
2 âˆ’3Îµstat
Î±

.
Therefore, for t â‰¥T0 âˆ’1,
wt âˆ’wlasso
2
2 â‰¤exp

âˆ’Î±
12Î² (t âˆ’âŒˆT0âŒ‰+ 1)
 wâŒˆT0âŒ‰âˆ’1 âˆ’wlasso

2
2 + 3Îµstat
Î±
â‰¤exp

âˆ’Î±
8Î² (t âˆ’T0)

âˆ¥wlassoâˆ¥2
2 + 3Îµstat
Î±
,
where the last inequality follows from Proposition B.3(2). Further, by Proposition B.3(3), we have
gapt+k â‰¤Î²
2k
wt âˆ’wlasso
2
2 â‰¤Î²
2k

exp

âˆ’Î±
8Î² (t âˆ’T0)

âˆ¥wlassoâˆ¥2
2 + 3Îµstat
Î±

,
âˆ€t â‰¥T0 âˆ’1, k â‰¥0.
Hence, we can conclude that gapT â‰¤Îµ for all T such that
T â‰¥10Î²Î½âˆ’1 âˆ¥wlassoâˆ¥2
2 + 8Îº log
 
Î² âˆ¥wlassoâˆ¥2
2
Îµ
!
+ 3ÎºÎµstat
Îµ
+ 1.
Now, by Proposition H.1, it holds that âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 â‰¤10 ÏÎ½
Î±2
s log d
N
, and hence
âˆ¥wlassoâˆ¥2
2 â‰¤2 âˆ¥wâ‹†âˆ¥2
2 + 2 âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 â‰¤2(Bâ‹†
w)2 + 20ÏÎ½s log d
Î±2N
.
Plugging in our definition of
ÂµN = Ï log d
N
,
Îµstat := 400sÂµN âˆ¥wlasso âˆ’wâ‹†âˆ¥2
2 ,
Ï‰N = Ï
Î±
s log d
N
â‰¤1
completes the proof.
H.5
Proof of Theorem 8
In this section, we present the proof of Theorem 8 based on Theorem H.2. We begin by recalling the
following RSC property of a Gaussian random matrix [87, Theorem 7.16], a classical result in the
high-dimensional statistics literature.
58

Proposition H.2 (RSC for Gaussian random design). Suppose that X = [x1; Â· Â· Â· ; xN]âŠ¤âˆˆRNÃ—d
is a random matrix with each row xi being i.i.d. samples from N(0, Î£). Then there are universal
constants c1 = 1
8, c2 = 50 such that with probability at least 1 âˆ’
eâˆ’N/32
1âˆ’eâˆ’N/32 ,
âˆ¥Xwâˆ¥2
2
N
â‰¥c1 âˆ¥wâˆ¥2
Î£ âˆ’c2Ï(Î£)log d
N
âˆ¥wâˆ¥2
1 ,
âˆ€w âˆˆRd,
(40)
where Ï(Î£) = maxiâˆˆ[d] Î£ii is the maximum of diagonal entries of Î£.
Fix a parameter Î´1 â‰¤Î´ (which we will specify in proof) and a large universal constant C0. Let us set
Î± = c1 = Î˜ (1) ,
Î² = 8(1 + (d/N)),
Ï = c2 = Î˜ (1) ,
Bx = C0
p
d log(N/Î´1),
By = C0(Bâ‹†
w + Ïƒ)
p
log(N/Î´1).
Similar to the proof of Corollary 6 (Appendix F.4), we consider the following good events (where
Îµ = Xwâ‹†âˆ’y)
Ew =

Î»max(XâŠ¤X/N) â‰¤Î² and X is (Î±, Ï)-RSC
	
,
Er =
nXâŠ¤Îµ

âˆâ‰¥4Ïƒ
p
N log(4d/Î´)
o
,
Eb = {âˆ€i âˆˆ[N], âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,N+1 = {âˆ¥xN+1âˆ¥2 â‰¤Bx, |yN+1| â‰¤By},
and we define E := Ew âˆ©Er âˆ©Eb âˆ©Eb,N+1.
Furthermore, we choose Î½ > 0 that correspond to the choice Î»N = 8Ïƒ
q
log(4d/Î´)
N
, and we also
assume N â‰¥32c2
c1 Â· s log d. Then, Assumption C holds on the event E.
Therefore, we can apply Theorem H.2 with Îµ = Î½Ï‰N, which implies that there exists a L-layer
transformer Î¸ such that its prediction byN+1 := g
ready(TF0
Î¸(H)), so that under the good event E we
have byN+1 = clipBy(âŸ¨xN+1, bwâŸ©), where
Llasso(bw) âˆ’Llasso(wlasso) â‰¤Î½Ï‰N.
In the following, we show that Î¸ is indeed the desired transformer (similarly to the proof in Ap-
pendix F.4). Consider the conditional prediction error
E

(byN+1 âˆ’yN+1)2 D

= E

1{E}(byN+1 âˆ’yN+1)2 D

+ E

1{Ec}(byN+1 âˆ’yN+1)2 D

,
and we analyze these two parts separately under the good event E0 := Ew âˆ©Er âˆ©Eb of D.
Part I.
We first note that
E

1{E}(byN+1 âˆ’yN+1)2 D

= E
h
1{E}(clipBy(âŸ¨xN+1, bwâŸ©) âˆ’yN+1)2 D
i
â‰¤E

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2 D

,
where the inequality is because yN+1 âˆˆ[âˆ’By, By] under the good event E. Notice that by our
construction, under the good event E, bw = bw(D) depends only on the dataset D (because it is the
(L âˆ’1)-th iterate of PGD on (ICLasso) problem). Applying Proposition H.1 to bw(D) and using the
definition of Ï‰N and our choice of Î»N, we obtain that (under E0)
âˆ¥bw(D) âˆ’wâ‹†âˆ¥2
2 â‰¤C Â·
sÎ»2
N
Î±2 + Î½Ï‰2
N + Î½Ï‰N

= O
Ïƒ2s log(d/Î´)
N

.
Therefore, under E0,
E

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2 D

= E

1{E}(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 D

â‰¤E

(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 D

= E

(âŸ¨xN+1, bw(D)âŸ©âˆ’âŸ¨xN+1, wâ‹†âŸ©)2 D

+ Ïƒ2
= âˆ¥bw(D) âˆ’wâ‹†âˆ¥2
2 + Ïƒ2
= Ïƒ2

1 + O
s log(d/Î´)
N

.
59

Part II.
Notice that under good event E0, the bad event Ec holds if and only if Ec
b,N+1 holds, and
hence
E

1{Ec}(byN+1 âˆ’yN+1)2 D

= E

1{Ec
b,N+1}(byN+1 âˆ’yN+1)2 D

â‰¤
q
P(Ec
b,N+1)E[(byN+1 âˆ’yN+1)4].
With a large enough constant C0, we clearly have P(Ec
b,N+1) â‰¤(Î´1/N)10. Further, a simple
calculation yields
E(byN+1 âˆ’yN+1)4 â‰¤8E(by4
N+1 + y4
N+1) â‰¤8B4
y + 8Ey4
N+1 â‰¤16B4
y,
where the last inequality is because the marginal distribution of yN+1 is simply N(0, Ïƒ2 + âˆ¥wâ‹†âˆ¥2
2).
Combining these yields
E

1{Ec}(byN+1 âˆ’yN+1)2 D

â‰¤O
 
Î´5
1B2
y
N 5
!
â‰¤O
Î´5
1((Bâ‹†
w)2 + Ïƒ2) log(1/Î´1)
N 4

.
Therefore, choosing Î´1 = min{Î´,
Ïƒ
Bâ‹†
w } is enough for our purpose, and under such choice of Î´1,
E

1{Ec}(byN+1 âˆ’yN+1)2 D

â‰¤O
 Ïƒ2
N 4

.
Conclusion.
Combining the inequalities above, we can conclude that under E0,
E

(byN+1 âˆ’yN+1)2 D

â‰¤Ïƒ2

1 + O
s log(d/Î´)
N

.
It remains to show that P(E0) â‰¥1 âˆ’Î´. By Proposition H.2, Lemma B.2 and Lemma B.4, we have
P(Ew) â‰¤3 exp(âˆ’N/32),
P(Er) â‰¤Î´
2,
P(Eb) â‰¤Î´
4.
Therefore, as long as N â‰¥32 log(12/Î´), we have P(E0) â‰¥1 âˆ’Î´. This completes the proof.
We also remark that in the construction above,
R = O

(Bâ‹†
w + Ïƒ)
âˆš
d log(N Â· (1 + Bâ‹†
w/Ïƒ))

,
which would be useful for bounding |||Î¸|||.
I
Proofs for Section 4
I.1
Proof of Proposition 10
We begin by restating Proposition 10 into the following version, which contains additional size
bounds on Î¸.
Theorem I.1 (Full statement of Proposition 10). Suppose that for
bLval(f) :=
1
|Dval|
X
(xi,yi)âˆˆDval
â„“(f(xi), yi),
â„“(Â·, Â·) is (Î³/3, R, M, C)-approximable by sum of relus (Definition D.1). Then there exists a 3-layer
transformer TFÎ¸ with
max
â„“âˆˆ[3] M (â„“) â‰¤(M + 3)K,
max
â„“âˆˆ[3] D(â„“) â‰¤K2 + K + 1,
|||Î¸||| â‰¤2NKC
|Dval| + 3Î³âˆ’1 + 7KR.
that maps
hi = [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti]
â†’
hâ€²
i = [âˆ—; bf(xi); 1; ti], i âˆˆ[N + 1],
where the predictor bf : Rd â†’R is a convex combination of {fk : bLval(fk) â‰¤minkâ‹†âˆˆ[K] bLval(fkâ‹†) +
Î³}. As a corollary, for any convex risk L : (Rd â†’R) â†’R, bf satisfies
L( bf) â‰¤minkâ‹†âˆˆ[K] L(fkâ‹†) + maxkâˆˆ[K]
bLval(fk) âˆ’L(fk)
 + Î³.
60

To prove Theorem I.1, we first state and prove the following two propositions.
Proposition I.1 (Evaluation layer). There exists a 1-layer transformer TFÎ¸ with MK heads and
|||Î¸||| â‰¤3R + 2NKC/ |Dval| such that for all H such that maxi{|yâ€²
i|} â‰¤R, maxi,k{|fk(xi)|} â‰¤R,
TFÎ¸ maps
hi = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti]
â†’
hâ€²
i = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); eLval(f1); Â· Â· Â· ; eLval(fK); 0; 1; ti],
i âˆˆ[N + 1],
where eLval(Â·) is a functional such that maxk
eLval(fk) âˆ’bLval(fk)
 â‰¤Îµ.
Proof of Proposition I.1. As â„“is (Îµ, R, M, C)-approximable by sum of relus, there exists a function
g : R2 â†’R of form
g(s, t) =
M
X
m=1
cmÏƒ(ams + bmt + dm) with
M
X
m=1
|cm| â‰¤C, |am| + |bm| + |dm| â‰¤1, âˆ€m âˆˆ[M],
such that sup(s,t)âˆˆ[âˆ’R,R]2 |g(s, t) âˆ’â„“(s, t)| â‰¤Îµ. We define
eLval(f) :=
1
|Dval|
X
(xi,yi)âˆˆDval
g(f(xi), yi),
Next, for every m âˆˆ[M] and k âˆˆ[K], we define matrices Qm,k, Km,k, Vm,k âˆˆRDÃ—D such that
for all i, j âˆˆ[N + 1],
Qm,khi =
ï£®
ï£¯ï£¯ï£¯ï£°
am
bm
dm
âˆ’2
0
ï£¹
ï£ºï£ºï£ºï£»,
Km,khj =
ï£®
ï£¯ï£¯ï£¯ï£°
fk(xj)
yj
1
R(1 + tj)
0
ï£¹
ï£ºï£ºï£ºï£»,
Vm,khj = (N + 1)cm
|Dval|
Â· eDâˆ’(Kâˆ’k)âˆ’3
where es âˆˆRD is the vector with s-th entry being 1 and others being 0. As the input has struc-
ture hi = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti], these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
mâˆˆ[M],kâˆˆ[K] âˆ¥Qm,kâˆ¥op â‰¤3,
max
mâˆˆ[M],kâˆˆ[K] âˆ¥Km,kâˆ¥op â‰¤2 + R,
X
mâˆˆ[M],kâˆˆ[K]
âˆ¥Vm,kâˆ¥op â‰¤K(N + 1)C
|Dval|
.
Now, for every i, j âˆˆ[N + 1], we have
Ïƒ(âŸ¨Qm,khi, Km,khjâŸ©) = Ïƒ(amfk(xj) + bmyj + dm âˆ’2R(1 + tj))
= Ïƒ
 amwâŠ¤xj + bmyj + dm

1{tj = âˆ’1},
where the last equality follows from the bound |amfk(xj) + bmyj + dm| â‰¤R(|am|+|bm|)+dm â‰¤
2R, so that the above relu equals 0 if tj â‰¤0. Therefore, for each i âˆˆ[N + 1] and k âˆˆ[K],
M
X
m=1
Ïƒ(âŸ¨Qm,khi, Km,khjâŸ©)Vm,khj
=
 M
X
m=1
cmÏƒ
 amwâŠ¤xj + bmyj + dm

!
Â· (N + 1)
|Dval| 1{tj = âˆ’1}eDâˆ’(Kâˆ’k)âˆ’3
= g(fk(xj), yj) Â· (N + 1)
|Dval| 1{tj = âˆ’1}eDâˆ’(Kâˆ’k)âˆ’3.
Thus letting the attention layer Î¸ = {(Vm,k, Qm,k, Km,k)}(m,k)âˆˆ[M]Ã—[K], we have
ehi = [AttnÎ¸(H)]i = hi +
1
N + 1
N+1
X
j=1
X
m,k
Ïƒ(âŸ¨Qm,khi, Km,khjâŸ©)Vm,khj
61

= hi +
1
|Dval|
N+1
X
j=1
K
X
k=1
g(fk(xj), yj) Â· 1{tj = âˆ’1}eDâˆ’(Kâˆ’k)âˆ’3
= hi +
K
X
k=1
ï£«
ï£­
1
|Dval|
X
(xj,yj)âˆˆDval
g(fk(xj), yj)
ï£¶
ï£¸eDâˆ’(Kâˆ’k)âˆ’3
= hi +
K
X
k=1
eLval(fk) Â· eDâˆ’(Kâˆ’k)âˆ’3
= [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti] + [0Dâˆ’Kâˆ’3; eLval(f1); Â· Â· Â· ; eLval(fK); 0; 0; 0]
= [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); eLval(f1); Â· Â· Â· ; eLval(fK); 0; 1; ti],
i âˆˆ[N + 1].
This is the desired result.
Proposition I.2 (Selection layer). There exists a 3-layer transformer TFÎ¸ with
max
â„“âˆˆ[3] M (â„“) â‰¤2K + 2,
max
â„“âˆˆ[3] D(â„“) â‰¤K2 + K + 1,
|||Î¸||| â‰¤Î³âˆ’1 + 3KR + 2.
such that TFÎ¸ maps
hi = [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); L1; Â· Â· Â· ; LK; 0; 1; ti]
â†’
hâ€²
i = [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); âˆ—; Â· Â· Â· ; âˆ—; bf(xi); 1; ti],
i âˆˆ[N + 1],
where bf = PK
k=1 Î»kfk is an aggregated predictor, where the weights Î»1, Â· Â· Â· , Î»K â‰¥0 are functions
only on L1, Â· Â· Â· , Lk such that
K
X
k=1
Î»k = 1,
Î»k > 0 only if Lk â‰¤min
kâ‹†âˆˆ[K] Lkâ‹†+ Î³.
Proof of Proposition I.2. We construct a Î¸ which is a composition of 2 MLP layers followed by an
attention layer (Î¸(1)
mlp, Î¸(2)
mlp, Î¸(3)
attn).
Step 1: construction of Î¸(1)
mlp. We consider matrix W(1)
1
that maps
h = [âˆ—Dâˆ’Kâˆ’3; L1; Â· Â· Â· ; LK; âˆ—; âˆ—; âˆ—]
7â†’W(1)
1 h = [L1 âˆ’L2; Â· Â· Â· ; L1 âˆ’LK; Â· Â· Â· ; LK âˆ’LKâˆ’1; L1; âˆ’L1; Â· Â· Â· ; LK; âˆ’LK],
i.e. W(1)
1 h is a K2 + K dimensional vector so that its entry contains {Lk âˆ’Ll}k,lâˆˆ[K] and
{Lk, âˆ’Lk}kâˆˆ[K]. Clearly, such W(1)
1
exists and can be chosen so that
W(1)
1

op â‰¤2K. We then
consider a matrix W(1)
2
that maps
Ïƒ(W(1)
1 h) 7â†’W(1)
2 Ïƒ(W(1)
1 h) = [0Dâˆ’Kâˆ’3; c1 âˆ’L1; Â· Â· Â· ; cK âˆ’LK; 03] âˆˆRD,
where ck = ck(L) := P
lÌ¸=k Ïƒ(Lk âˆ’Ll). Notice that
ck âˆ’Lk = âˆ’Ïƒ(Lk) + Ïƒ(âˆ’Lk) +
X
lÌ¸=k
Ïƒ(Lk âˆ’Ll),
and hence such W(1)
2
exists and can be chosen so that
W(1)
2

op â‰¤K + 1. We set Î¸(1)
mlp =
(W(1)
1 , W(1)
2 ), then MLPÎ¸(1)
mlp maps hi to
h(1)
i
= [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); c1; Â· Â· Â· ; cK; 0; 1; ti].
The basic property of {ck}kâˆˆ[K] is that, if ck â‰¤Î³, then Lk â‰¤minkâ‹†âˆˆ[K] Lkâ‹†+ Î³.
Step 2: construction of Î¸(2)
mlp. We consider matrix W(2)
1
that maps
h = [âˆ—Dâˆ’Kâˆ’3; c1; Â· Â· Â· ; cK; âˆ—; 1; âˆ—]
62

7â†’W(2)
1 h = [1 âˆ’Î³âˆ’1c1; c1; âˆ’c1; Â· Â· Â· ; 1 âˆ’Î³âˆ’1cK; cK; âˆ’cK] âˆˆR3K,
and W(2)
1
can be chosen so that
W(2)
1

op â‰¤K + 1 + Î³âˆ’1. We then consider a matrix W(2)
2
that
maps
Ïƒ(W(2)
1 h) 7â†’W(2)
2 Ïƒ(W(1)
1 h) = [0Dâˆ’Kâˆ’3; Ïƒ(1 âˆ’Î³âˆ’1c1) âˆ’c1; Â· Â· Â· ; Ïƒ(1 âˆ’Î³âˆ’1cK) âˆ’cK; 03] âˆˆRD,
which exists and can be chosen so that
W(1)
2

op â‰¤2. We set Î¸(2)
mlp = (W(2)
1 , W(2)
2 ), then MLPÎ¸(2)
mlp
maps h(1)
i
to
h(2)
i
= [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); u1; Â· Â· Â· ; uK; 0; 1; ti],
where uk = Ïƒ(1 âˆ’Î³âˆ’1ck)âˆ€k âˆˆ[K]. Clearly, uk âˆˆ[0, 1], and uk > 0 if and only if ck â‰¤Î³.
Step 3: construction of Î¸(3)
attn. We define
Î»1 = 1 âˆ’Ïƒ(1 âˆ’u1),
Î»k = Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1) âˆ’Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’uk) âˆ€k â‰¥2.
Clearly, Î»k â‰¥0, and P
k Î»k = 1. Further,
Î»k > 0 â‡’uk > 0 â‡’ck â‰¤Î³ â‡’Lk â‰¤min
kâ‹†âˆˆ[K] Lkâ‹†+ Î³.
Therefore, it remains to construct Î¸(3)
attn that implements bf = PK
k=1 Î»kfk based on [h(2)
i ]i. Notice
that
bf(xi) = Ïƒ(1) Â· f1(xi) +
Kâˆ’1
X
k=1
Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1) Â· (fk(xi) âˆ’fkâˆ’1(xi))
âˆ’Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’uK) Â· fK(xi),
(41)
and hence we construct Î¸(3)
attn as follows: for every k âˆˆ[K + 1] and w âˆˆ{0, 1}, we define matrices
Qk,w, Kk,w, Vk,w âˆˆRDÃ—D such that for all k âˆˆ[K + 1]
Qk,0h(2)
i
=

(fk(xi) + R) Â· 1k
0

,
Qk,1h(2)
i
=

(fkâˆ’1(xi) + R) Â· 1k
0

,
Kk,0h(2)
j
= Kk,1h(2)
j
=
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
1
âˆ’u1
...
âˆ’ukâˆ’1
0
ï£¹
ï£ºï£ºï£ºï£ºï£»
,
Vk,0h(2)
j
= eDâˆ’2 = âˆ’Vk,1h(2)
j ,
for all i, j âˆˆ[N + 1], where we understand f0 = fK+1 = 0 and 1k is the k-dimensional vector
with all entries being 1. By the structure of h(2)
i , these matrices indeed exist, and further it is
straightforward to check that they have norm bounds
max
kâˆˆ[K+1],wâˆˆ{0,1} âˆ¥Qk,wâˆ¥op â‰¤KR,
max
kâˆˆ[K+1],wâˆˆ{0,1} âˆ¥Kkâˆ¥op â‰¤1,
X
kâˆˆ[K+1],wâˆˆ{0,1}
âˆ¥Vk,wâˆ¥op â‰¤2K + 2.
Now, for every i, j âˆˆ[N + 1], k âˆˆ[K + 1], w âˆˆ{0, 1}, we have
Ïƒ
D
Qk,wh(2)
i , Kk,wh(2)
j
E
= Ïƒ((1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1)(fkâˆ’w(xi) + R))
= Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1) Â· (fkâˆ’w(xi) + R),
where the last equality follows from fk(xi) + R â‰¥0âˆ€k âˆˆ[K]. Therefore,
X
kâˆˆ[K+1],wâˆˆ{0,1}
Ïƒ
D
Qm,kh(2)
i , Km,kh(2)
j
E
Vm,kh(2)
j
=
K
X
k=1
h
Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1) Â· (fk(xi) + R) âˆ’Ïƒ(1 âˆ’u1 âˆ’Â· Â· Â· âˆ’ukâˆ’1) Â· (fkâˆ’1(xi) + R)
i
Â· eDâˆ’2
63

= bf(xi) Â· eDâˆ’2,
where the last equality is due to (41).
Thus letting the attention layer Î¸(3)
attn
=
{(Vk,w, Qk,w, Kk,w)}(k,w)âˆˆ[K+1]Ã—{0,1}, we have
h(3)
i
=
h
AttnÎ¸(H(2))
i
i = hi +
1
N + 1
N+1
X
j=1
X
k,w
Ïƒ
D
Qk,wh(2)
i , Kk,wh(2)
j
E
Vk,wh(2)
j
= h(2)
i
+ bf(xi) Â· eDâˆ’2
= [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); u1; Â· Â· Â· ; uK; bf(xi); 1; ti].
This is the desired result.
Now, we are ready to prove Theorem I.1.
Proof of Theorem I.1
As â„“(Â·, Â·) is (Î³/3, R, M, C)-approximable by sum of relus, we can invoke
Proposition I.1 to show that there exists a single attention layer Î¸(1)
attn so that AttnÎ¸(1)
attn maps
hi
â†’
hâ€²
i = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); eLval(f1); Â· Â· Â· ; eLval(fK); 0; 1; ti],
i âˆˆ[N + 1],
for any input H = [hi]i of the form described in Theorem I.1, and eLval(Â·) is a functional such that
maxk
eLval(fk) âˆ’bLval(fk)
 â‰¤Î³/3.
Next, by the proof of Proposition I.2, there exists (Î¸(1)
mlp, Î¸(2)
mlp, Î¸(3)
attn) that maps
hâ€²
i
â†’
h(3)
i
=
"
xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); âˆ—;
K
X
k=1
Î»kfk(xi); 1; ti
#
,
i âˆˆ[N + 1],
where Î» = (Î»1, Â· Â· Â· , Î»K) âˆˆâˆ†([K]) and Î»k > 0 only when eLval(fk) â‰¤minkâ‹†eLval(fkâ‹†) + Î³/3.
Using the fact that maxk |eLval(fk) âˆ’bLval(fk)| â‰¤Î³/3, we deduce that Î» is supported on {k :
bLval(fk) â‰¤minkâ‹†âˆˆ[K] bLval(fkâ‹†) + Î³}.
Therefore, Î¸ = (Î¸(1)
attn, Î¸(1)
mlp, Î¸(2)
mlp, Î¸(3)
attn) is the desired transformer, with
max
â„“âˆˆ[3] M (â„“) â‰¤(M + 3)K,
max
â„“âˆˆ[3] D(â„“) â‰¤K2 + K + 1,
and
|||Î¸||| â‰¤max

3R + 2NKC
|Dval| + 3K + 1, K + 3 + Î³âˆ’1, KR + 2K + 2

â‰¤7KR + 2NKC
|Dval| + Î³âˆ’1.
This completes the proof.
I.2
Proof of Theorem 11
We first restate Theorem 11 into the following version which provides additional size bounds
for Î¸. For the simplicity of presentation, throughout this subsection and Appendix J, we denote
It = {i : (xi, yi) âˆˆDtrain}, Iv = {i : (xi, yi) âˆˆDval}, Xtrain = [xi]iâˆˆIt to be the input matrix
corresponding to the training split only, and Ntrain = |Dtrain|, Nval = |Dval|.
Theorem I.2. For any sequence of regularizations {Î»k}kâˆˆ[K], 0 â‰¤Î± â‰¤Î² with Îº := maxk
Î²+Î»k
Î±+Î»k ,
Bw > 0, Î³ > 0, and Îµ < Bw/2, suppose in input format (3) we have D â‰¥Î˜(Kd). Then there exists
an L-layer transformer TFÎ¸ with
L = âŒˆ2Îº log(Bw/(2Îµ))âŒ‰+ 4,
max
â„“âˆˆ[L] M (â„“) â‰¤3K + 1,
max
â„“âˆˆ[L] D(â„“) â‰¤K2 + K + 1,
64

|||Î¸||| â‰¤O

KR + (Î² + Î»)âˆ’1 + N
Nval
+ Î³âˆ’1

,
R := max{BxBw, By, 1},
such that the following holds. On any input data (D, xN+1) such that the problem (ICRidge) is
well-conditioned and has a bounded solution:
Î± â‰¤Î»min(XâŠ¤
trainXtrain/Ntrain) â‰¤Î»max(XâŠ¤
trainXtrain/Ntrain) â‰¤Î²,
max
kâˆˆ[K]
wÎ»k
ridge(Dtrain)

2 â‰¤Bw/2,
(42)
TF0
Î¸ approximately implements ridge selection: its prediction
byN+1 = ready(TF0
Î¸(H)) = âŸ¨bw, xN+1âŸ©,
bw =
K
X
k=1
Î»k bwk
satisfies the following.
1. For each k âˆˆ[K], bwk = bwk(Dtrain) approximates the ridge estimator wÎ»k
ridge(Dtrain), i.e.
bwk âˆ’wÎ»k
ridge(Dtrain)

2 â‰¤Îµ.
2. Î» = (Î»1, Â· Â· Â· , Î»K) âˆˆâˆ†([K]) so that
Î»k > 0 only if bLval(bwk) â‰¤min
kâ‹†âˆˆ[K]
bLval(bwkâ‹†) + Î³.
In particular, if we set Î³â€² = 2(BxBw + By)BxÎµ + Î³, then it holds that8
dist

bw, conv{bwÎ»k
ridge,train : bLval(bwÎ»k
ridge,train) â‰¤min
kâ‹†âˆˆ[K]
bLval(bwÎ»kâ‹†
ridge,train) + Î³â€²}

â‰¤Îµ,
where we denote bwÎ»k
ridge,train := wÎ»k
ridge(Dtrain).
To prove Theorem I.2, we first show that, for the squared validation loss, there exists a 3-layer
transformer that performs predictor selection based on the exactly evaluated bLval(fk) for each
k âˆˆ[K]. (Proof in Appendix I.2.1.)
Theorem I.3 (Square-loss version of Theorem I.1). Consider the squared validation loss
bLval(f) :=
1
2|Dval|
X
(xi,yi)âˆˆDval
(f(xi) âˆ’yi)2.
Then there exists a 3-layer transformer TFÎ¸ with
max
â„“âˆˆ[3] M (â„“) â‰¤2K + 2,
max
â„“âˆˆ[3] D(â„“) â‰¤K2 + K + 1,
|||Î¸||| â‰¤7KR + 2N
|Dval| + Î³âˆ’1,
such that for any input H that takes form
hi = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K; âˆ—; 1; ti],
where TFÎ¸ outputs hN+1 = [xN+1; bf(xN+1); âˆ—; 1; 0], where the predictor bf : Rd â†’R is a convex
combination of {fk : bLval(fk) â‰¤minkâ‹†âˆˆ[K] bLval(fkâ‹†) + Î³}. As a corollary, for any convex risk
L : (Rd â†’R) â†’R, bf satisfies
L( bf) â‰¤minkâ‹†âˆˆ[K] L(fkâ‹†) + maxkâˆˆ[K]
bLval(fk) âˆ’L(fk)
 + Î³.
Proof of Theorem I.2
First, by the proof9 of Theorem 4 and Proposition B.6, for each k âˆˆ[K],
there exists a T = L âˆ’3 layer transformer Î¸(1:T ) such that TFÎ¸(1:T ) maps
hi
â†’
h(T )
i
= [xi; yâ€²
i; âˆ—; âŸ¨bw1, xiâŸ©; Â· Â· Â· ; âŸ¨bwK, xiâŸ©; 0K; 1; ti],
8This is because bLval(w) is (BxBw + By)Bx-Lipschitz w.r.t. w âˆˆB2(Bw).
9Technically, an adapted version where the underlying ICGD mechanism operates on the training split (with
ti = 1) with size Ntrain instead of on all N training examples, which only changes |||Î¸||| by at most a constant
factor, and does not change the number of layers and heads.
65

so that if (42) holds, we have
bwk âˆ’wÎ»k
ridge

2 â‰¤Îµ and bwk âˆˆB2(Bw).
Next, by Theorem I.3, there exists a 3-layer transformer Î¸(T +1:T +3) that outputs
h(T +3)
N+1
= [xN+1; âŸ¨bw, xN+1âŸ©; âˆ—; 1; ti],
where bw = PK
k=1 Î»k bwk, Î» = (Î»1, Â· Â· Â· , Î»K) âˆˆâˆ†([K]) so that
Î»k > 0 only if bLval(bwk) â‰¤min
kâ‹†âˆˆ[K]
bLval(bwkâ‹†) + Î³.
This is the desired result.
I.2.1
Proof of Theorem I.3
Similar to the proof of Proposition 10, Theorem I.3 is a direct corollary by combining Proposition I.3
with Proposition I.2.
Proposition I.3 (Evaluation layer for the squared loss). There exists an attention layer TFÎ¸ with 2K
heads and |||Î¸||| â‰¤3R + 2NK/ |Dval| such that TFÎ¸ maps
hi = [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K; âˆ—; 1; ti]
â†’
hâ€²
i = [âˆ—; f1(xi); Â· Â· Â· ; fK(xi); bLval(f1); Â· Â· Â· ; bLval(fK); âˆ—; 1; ti],
i âˆˆ[N + 1].
Proof of Proposition I.3. For every k âˆˆ[K], we define matrices Qm,k, Km,k, Vm,k âˆˆRDÃ—D such
that for all i, j âˆˆ[N + 1],
Qk,0hi =
ï£®
ï£¯ï£°
1
âˆ’1
âˆ’2
0
ï£¹
ï£ºï£»,
Qk,1hi =
ï£®
ï£¯ï£°
âˆ’1
1
âˆ’2
0
ï£¹
ï£ºï£»,
Kk,0hj = Kk,1hj =
ï£®
ï£¯ï£°
fk(xj)
yj
R(1 + tj)
0
ï£¹
ï£ºï£»,
Vk,0hj = âˆ’Vk,1hj = (N + 1)
2 |Dval| Â· (fk(xj) âˆ’yj)eDâˆ’(Kâˆ’k)âˆ’3.
As the input has structure hi = [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti], these matrices indeed
exist, and further it is straightforward to check that they have norm bounds
max
kâˆˆ[K],wâˆˆ{0,1} âˆ¥Qk,wâˆ¥op â‰¤3,
max
kâˆˆ[K],wâˆˆ{0,1} âˆ¥Kk,wâˆ¥op â‰¤1 + R,
X
kâˆˆ[K],wâˆˆ{0,1}
âˆ¥Vk,wâˆ¥op â‰¤K(N + 1)
|Dval|
.
Now, for every i, j âˆˆ[N + 1], we have
X
wâˆˆ{0,1}
Ïƒ(âŸ¨Qk,whi, Kk,whjâŸ©)Vk,whj
= [Ïƒ(fk(xj) âˆ’yj âˆ’2R(1 + tj)) âˆ’Ïƒ(yj âˆ’fk(xj) âˆ’2R(1 + tj))] Â· (N + 1)
2 |Dval| (fk(xj) âˆ’yj)eDâˆ’(Kâˆ’k)âˆ’3
= 1{tj = âˆ’1} Â· [Ïƒ(fk(xj) âˆ’yj) âˆ’Ïƒ(yj âˆ’fk(xj))] Â· (N + 1)
2 |Dval| (fk(xj) âˆ’yj)eDâˆ’(Kâˆ’k)âˆ’3
= 1{tj = âˆ’1} Â· (N + 1)
2 |Dval| (fk(xj) âˆ’yj)2eDâˆ’(Kâˆ’k)âˆ’3,
where the second equality follows from the bound |fk(xj) âˆ’yj| â‰¤2R, so that the relus equals 0 if
tj â‰¤0. Thus letting the attention layer Î¸ = {(Vk,w, Qk,w, Kk,w)}(k,w)âˆˆ[K]Ã—{0,1}, we have
ehi = [AttnÎ¸(H)]i = hi +
1
N + 1
N+1
X
j=1
X
k,w
Ïƒ(âŸ¨Qk,whi, Kk,whjâŸ©)Vk,whj
66

= hi +
1
2|Dval|
N+1
X
j=1
K
X
k=1
(fk(xj) âˆ’yj)2 Â· 1{tj = âˆ’1}eDâˆ’(Kâˆ’k)âˆ’3
= hi +
K
X
k=1
ï£«
ï£­
1
2|Dval|
X
(xj,yj)âˆˆDval
(fk(xj) âˆ’yj)2
ï£¶
ï£¸eDâˆ’(Kâˆ’k)âˆ’3
= hi +
K
X
k=1
bLval(fk) Â· eDâˆ’(Kâˆ’k)âˆ’3
= [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); 0K+1; 1; ti] + [0Dâˆ’Kâˆ’3; bLval(f1); Â· Â· Â· ; bLval(fK); 0; 0; 0]
= [xi; yâ€²
i; âˆ—; f1(xi); Â· Â· Â· ; fK(xi); bLval(f1); Â· Â· Â· ; bLval(fK); 0; 1; ti],
i âˆˆ[N + 1].
This is the desired result.
I.3
Proofs for Section 4.2
I.3.1
Proof of Lemma 13
It is straightforward to check that the binary type check Ïˆ : R â†’R can be expressed as a linear
combination of 6 reluâ€™s (recalling Ïƒ(Â·) = ReLU(Â·)):
Ïˆ(y) = Ïƒ
y + Îµ
Îµ

âˆ’2Ïƒ
y
Îµ

+ Ïƒ
y âˆ’Îµ
Îµ

+ Ïƒ
y âˆ’(1 âˆ’Îµ)
Îµ

âˆ’2Ïƒ
y âˆ’1
Îµ

+ Ïƒ
y âˆ’(1 + Îµ)
Îµ

=:
6
X
m=1
amÏƒ(bmy + cm),
with P
m |am| = 8/Îµ, maxm max {|bm|, |cm|} â‰¤2. We can thus construct an attention layer
Î¸ = {(Qm, Km, Vm)}6
m=1 with 6 heads such that
Qmhi = [bm; cm; 0Dâˆ’2],
Kmhj = [yj; 1; 0Dâˆ’2],
Vmhj =
N + 1
N
am Â· tj; 0Dâˆ’1

,
which gives that for every i âˆˆ[N + 1],
6
X
m=1
1
N + 1
X
jâˆˆ[N+1]
Ïƒ(âŸ¨Qmhi, KmhjâŸ©)[Vmhj]1
=
6
X
m=1
1
N
N
X
j=1
Ïƒ(bmyj + cm)am = 1
N
N
X
j=1
Ïˆ(yj) = Î¨binary(D).
Further, we have |||Î¸||| â‰¤18/Îµ = O(1/Îµ). This is the desired result.
By composing the above attention layer with one additional layer (with 2 heads) that implement the
following function
Ïƒ(2(t âˆ’1/2)) âˆ’Ïƒ(2(t âˆ’1)),
on the output Î¨binary(D), we directly obtain the following corollary.
Corollary I.1 (Thresholded binary test). There exists a two-layer attention-only transformer with
maxâ„“âˆˆ[2] M (â„“) â‰¤6 and |||Î¸||| â‰¤O(1/Îµ) that exactly implements the thresholded binary test
Î¨binary
thres (D) :=
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
1,
if Î¨binary(D) â‰¥1,
0,
if Î¨binary(D) â‰¤1
2,
linear interpolation,
o.w.
(43)
at every token i âˆˆ[N + 1], where we recall the definition of Î¨binary in Lemma 13.
67

I.3.2
Formal statement and proof of Proposition 14
We say a distribution Py on R is (C, Îµ0)-not-concentrated around {0, 1} if
Py([âˆ’Îµ, Îµ] âˆª[1 âˆ’Îµ, 1 + Îµ]) â‰¤CÎµ
for all Îµ âˆˆ(0, Îµ0]. A sufficient condition is that the density py is upper bounded by C within
[âˆ’Îµ0, Îµ0] âˆª[1 âˆ’Îµ0, 1 + Îµ0].
Throughout this section, let Ïƒlog(t) := (1 + eâˆ’t)âˆ’1 denote the sigmoid activation, and let bwlog
denote the solution to the in-context logistic regression problem, i.e. (ICGLM) with g(Â·) = Ïƒlog(Â·).
Proposition I.4 (Adaptive regression or classification; Formal version of Proposition 14). For any
Bw > 0, Îµ â‰¤BxBw/10, 0 < Î± â‰¤Î² with Îº := Î²/Î±, and any (C, Îµ0), there exists a L-layer
attention-only transformer with
L â‰¤O

Îº log BxBw
Îµ

,
max
â„“âˆˆ[L] M (â„“) â‰¤O

1 + B4
x
Î±2

Îµâˆ’2

,
|||Î¸||| â‰¤O

R + 1
Î² + 1
Îµ

(with R := max {BxBw, By, 1}, and Îµ depending only on (C, Îµ0)) such that the following holds.
Suppose the input format is (3) with dimension D â‰¥3d + 4.
On any classification instance (D, xN+1) (such that {yi}iâˆˆ[N] âŠ‚{0, 1}) that is well-conditioned
for logistic regression in the sense of (35), it outputs byN+1 that Îµ-approximates the prediction of
in-context logistic regression:
|byN+1 âˆ’Ïƒlog(âŸ¨xN+1, bwlogâŸ©)| â‰¤Îµ.
On the contrary, for regression problems, i.e. any in-context distribution P whose marginal Py is
(C, Îµ0)-not-concentrated around {0, 1}, with probability at least 1âˆ’exp(âˆ’cN) over D (where c > 0
depends only on (C, Îµ0)), byN+1 Îµ-approximates the prediction of in-context least squares if the data
is well-conditioned:
|byN+1 âˆ’âŸ¨xN+1, bwLSâŸ©| â‰¤Îµ
whenever D satisfies (5) with Î» = 0,
where bwLS denotes the in-context least squares estimator, i.e. (ICRidge) with Î» = 0.
Proof. The result follows by combining the binary test in Corollary I.1 with Theorem 4 and Theo-
rem G.1. By those results, there exists three attention-only transformers Î¸LS, Î¸log, Î¸bin, with (below
Lg, Cg = Î˜(1) for g = Ïƒlog(Â·))
LLS â‰¤O

Îº log BxBw
Îµ

,
max
â„“âˆˆ[LLS] M (â„“)
LS â‰¤3,
|||Î¸LS||| â‰¤O

R + 1
Î²

,
Llog â‰¤O

Îº log LgBxBw
Îµ

,
max
â„“âˆˆ[Llog] M (â„“)
log â‰¤O
 
C2
g
 
1 + L2
gB4
x
Î±2
!
Îµâˆ’2
!
,
|||Î¸log||| â‰¤O

R + Cg
Î²

,
Lbin = 2,
max
â„“âˆˆ[2] M (â„“)
bin â‰¤6,
|||Î¸bin||| â‰¤O(1/Îµ),
that outputs prediction byLS
N+1, bylog
N+1 (at the (N + 1)-th token) and Î¨binary
thres (D) (at every token)
respectively, which satisfy
bylog
N+1 âˆ’Ïƒlog(âŸ¨xN+1, bwlogâŸ©)
 â‰¤Îµ,
byLS
N+1 âˆ’âŸ¨xN+1, bwLSâŸ©
 â‰¤Îµ.
when the corresponding well-conditionednesses are satisfied. In particular, we can make bwlog well-
defined on non-binary data, by multiplying Î¨binary
thres (D) onto the xiâ€™s (which can be implemented by
slightly modifying Î¸log without changing the order of the number of layers, heads, and norms) so
that bwlog = 0 on any data where Î¨binary
thres (D) = 0.
By joining Î¸LS and Î¸log using Proposition B.6, concatenating with Î¸bin before, and concatenating
with one additional attention layer with 2 heads after to implement
Î¨binary
thres (D)bylog
N+1 +

1 âˆ’Î¨binary
thres (D)

byLS
N+1,
(44)
68

we obtain a single transformer Î¸ with
L â‰¤O

Îº log BxBw
Îµ

,
max
â„“âˆˆ[L] M (â„“) â‰¤O

1 + B4
x
Î±2

Îµâˆ’2

,
|||Î¸LS||| â‰¤O

R + 1
Î² + 1
Îµ

,
which outputs (44) as its prediction (at the location for byN+1).
It remains to show that (44) reduces to either one of bylog
N+1 or byLS
N+1. When the data are binary
(yi âˆˆ{0, 1}), we have Î¨binary(D) = 1 and Î¨binary
thres (D) = 1, in which case (44) becomes exactly
bylog
N+1. By contrast, when data is sampled from a distribution that is (C, Îµ0)-not-concentrated
around {0, 1}, we have for any fixed Îµ â‰¤Îµ0 âˆ§
1
4C that, letting BÎµ := [âˆ’Îµ, Îµ] âˆª[1 âˆ’Îµ, 1 + Îµ] and
pÎµ := Py(BÎµ) â‰¤CÎµ â‰¤1
4, by Hoeffdingâ€™s inequality,
P(Î¨binary
thres (D) Ì¸= 0) = P

Î¨binary
thres (D) â‰¥1
2

= P
 
1
N
N
X
i=1
1{yj âˆˆBÎµ} â‰¥1
2
!
â‰¤exp

âˆ’c(1/2 âˆ’pÎµ)2N

â‰¤exp(âˆ’câ€²N),
where câ€² > 0 is an absolute constant. On the event Î¨binary
thres (D) = 0 (which happens with probability
at least 1 âˆ’exp(âˆ’câ€²N)), (44) becomes exactly byLS
N+1. This finishes the proof.
I.4
Linear correlation test and application
In this section, we give another instantiation of the pre-ICL testing mechanism by showing that the
transformer can implement a linear correlation test that tests whether the correlation vector E[xy] has
a large norm. We then use this test to construct a transformer to perform â€œconfident linear regressionâ€,
i.e. output a prediction from linear regression only when the signal-to-noise ratio is high.
For any fixed parameters Î»min, Bâ‹†
w > 0, consider the linear correlation test over data D defined as
Î¨lin(D) :=
1
Î»2
min(Bâ‹†w)2/2 Â·
h
Ïƒ

âˆ¥btâˆ¥2
2 âˆ’(Î»minBâ‹†
w/4)2
âˆ’Ïƒ

âˆ¥btâˆ¥2
2 âˆ’(3Î»minBâ‹†
w/4)2i
=
ï£±
ï£²
ï£³
0,
âˆ¥btâˆ¥2
2 â‰¤(Î»minBâ‹†
w/4)2,
1,
âˆ¥btâˆ¥2
2 â‰¥(3Î»minBâ‹†
w/4)2,
linear interpolation,
o.w.,
where bt = T(D) := 1
N
N
X
i=1
xiyi.
(45)
Recall that Ïƒ(Â·) = ReLU(Â·) above denotes the relu activation.
We show that Î¨lin can be exactly implemented by a 3-layer transformer.
Lemma I.1 (Expressing Î¨lin by transformer). There exists a 3-layer attention-only transformer TFÎ¸
with at most 2 heads per layer and |||Î¸||| â‰¤O(1 + Î»2
min(Bâ‹†
w)2) such that on input sequence H of
the form (3) with D â‰¥2d + 4, the transformer exactly implements Î¨lin: it outputs eH such that
ehi = [xi; yiti; âˆ—; Î¨lin(D); 1] for all i âˆˆ[N + 1].
Proof. We begin by noting the following basic facts:
â€¢ Identity function can be implemented exactly by two ReLUs: t = Ïƒ(t) âˆ’Ïƒ(âˆ’t).
â€¢ Squared â„“2 norm can be implemented exactly by a single attention head (assuming every input
hi contains the same vector g): âˆ¥gâˆ¥2
2 = Ïƒ(âŸ¨g, gâŸ©).
We construct the transformer Î¸ as follows.
Layer 1: Use 2 heads to implement bt = 1
N
PN
i=1 xiyi, where V(1)
{1,2}hj = [Â±xj; 0Dâˆ’d], Q(1)
{1,2}hi =
[ N+1
N ; 0Dâˆ’1], and K(1)
{1,2}hj = [Â±yjtj; 0Dâˆ’1] = [Â±yj1{j < N + 1}; 0Dâˆ’1] (where we recall
tj = 1{j < N + 1} and note that yjtj corresponds exactly to the location for yj in H, cf. (3)).
69

By manipulating the output dimension in V(1), write the result bt into blank memory space with
dimension d at every token i âˆˆ[N + 1].
Layer 2: Use a single head to compute âˆ¥btâˆ¥2
2: Q(2)
1 h(1)
i
= [bt; 0Dâˆ’d], K(2)
1 h(1)
j
= [bt; 0Dâˆ’d], and
V(2)
1 h(1)
j
= [1; 0Dâˆ’1]. By manipulating the output dimension in V(2), write the result âˆ¥btâˆ¥2
2 into
blank memory space with dimension 1 at every token i âˆˆ[N + 1]. After layer 2, we have h(3)
i
=
[xi; yiti; âˆ—; âˆ¥btâˆ¥2
2; âˆ—; 1].
Layer 3: Use 2 heads to implement two ReLU functions with bias: âˆ¥btâˆ¥2
2 7â†’
1
Bâˆ’A(Ïƒ(âˆ¥btâˆ¥2
2 âˆ’A) âˆ’
Ïƒ(âˆ¥btâˆ¥2
2 âˆ’B)). The two query (or key) matrices contain values A and B. In our problem we take
A = (Î»minBâ‹†
w/4)2,
B = (3Î»minBâ‹†
w/4)2,
so that the above ReLU function implements Î¨lin(D) exactly. Write the result into a blank memory
space with dimension 1. We finish the proof by noting that |||Î¸||| â‰¤O(1 + Î»2
min(Bâ‹†
w)2).
Statistical guarantee for Î¨lin
We consider the following well-posedness assumption for the linear
correlation test Î¨lin. Note that, similar as Assumption A, the assumption does not require the data
to be generated from any true linear model, but rather only requires some properties about the best
linear fit wâ‹†
P, as well as sub-Gaussianity conditions.
Assumption D (Well-posedness for linear correlation test). We say a distribution P on Rd Ã— R is
well-posed for linear independence tests, if (x, y) âˆ¼P satisfies
(1) âˆ¥xâˆ¥2 â‰¤Bx and |y| â‰¤By almost surely;
(2) The covariance Î£P := EP[xxâŠ¤] satisfies Î»minId âª¯Î£P âª¯Î»maxId, with 0 < Î»min â‰¤Î»max,
and Îº := Î»max/Î»min.
(3) The whitened vector Î£âˆ’1/2
P
x is K2-sub-Gaussian for some K â‰¥1.
(4) The best linear predictor wâ‹†
P := EP[xxâŠ¤]âˆ’1EP[xy] satisfies âˆ¥wâ‹†
Pâˆ¥2 â‰¤Bâ‹†w.
(5) The label y is Ïƒ2-sub-Gaussian.
(6) The residual z := y âˆ’âŸ¨x, wâ‹†
PâŸ©is Ïƒ2-sub-Gaussian with probability one (over x).
The following results states that Î¨lin achieves high power as long as the sample size is high enough,
and the signal âˆ¥wâ‹†
Pâˆ¥2 is either sufficiently high or sufficiently low.
Proposition I.5 (Power of linear correlation test). Suppose distribution P satisfies Assumption D with
parameters Î»min, Î»max, Bâ‹†w. Then, for the linear correlation test Î¨lin with parameters (Î»min, Bâ‹†
w)
with Bâ‹†
w â‰¤Bâ‹†w and any N â‰¥e
O

max {K4, Î»maxK2Ïƒ2
(Bâ‹†
w)2Î»2
min } Â· d

, we have
1. If âˆ¥wâ‹†
Pâˆ¥2 â‰¥Bâ‹†
w, then with probability at least 1 âˆ’Î´ over D, we have Î¨lin(D) = 1.
2. If âˆ¥wâ‹†
Pâˆ¥2 â‰¤
Î»min
10Î»max Bâ‹†
w, then with probability at least 1 âˆ’Î´ over D, we have Î¨lin(D) = 0.
Proof. For any P satisfying Assumption D, note that E[xz] = E[x(yâˆ’âŸ¨wâ‹†
P, xâŸ©)] = 0 by construction.
Therefore, by standard sub-Gaussian and sub-exponential concentration combined with union bound,
the following events hold simultaneously with probability at least 1 âˆ’Î´:
0.9Î£P âª¯bÎ£ = 1
N
N
X
i=1
xixâŠ¤
i âª¯1.1Î£P
as N â‰¥e
O(dK4) by (27),

1
N
N
X
i=1
xizi

2
â‰¤Î»1/2
max Â·

1
N
N
X
i=1
Î£âˆ’1/2
P
xizi

2
â‰¤e
O
 
Î»1/2
max
 
KÏƒ
âˆš
d
âˆš
N
+ KÏƒd
N
!!
â‰¤e
O
 
Î»1/2
maxKÏƒ
r
d
N
!
â‰¤Î»minBâ‹†
w
8
,
as N â‰¥e
O
Î»maxdK2Ïƒ2
(Bâ‹†w)2Î»2
min

.
70

On the above event, we have
bt

2 =

1
N
N
X
i=1
xi(âŸ¨xi, wâ‹†
PâŸ©+ zi)

2
=

bÎ£wâ‹†
P + 1
N
N
X
i=1
xizi

2
.
Therefore, in case 1, we have
bt

2 â‰¥
bÎ£wâ‹†
P

2 âˆ’

1
N
N
X
i=1
xizi

2
â‰¥0.9Î»min âˆ¥wâ‹†
Pâˆ¥2 âˆ’Î»minBâ‹†
w
8
â‰¥3Î»minBâ‹†
w
4
.
In case 2, we have
bt

2 â‰¤
bÎ£wâ‹†
P

2 +

1
N
N
X
i=1
xizi

2
â‰¤Î»max Â· Î»minBâ‹†
w
10Î»max
+ Î»minBâ‹†
w
8
â‰¤Î»minBâ‹†
w
4
.
The proof is finished by recalling the definition of Î¨lin in (45), so that Î¨lin(D) = 1 if âˆ¥btâˆ¥2 â‰¥
3Î»minBâ‹†
w/4, and Î¨lin(D) = 0 if âˆ¥btâˆ¥2 â‰¤Î»minBâ‹†
w/4.
Application: Confident linear regression
By directly composing the linear correlation test
in Lemma I.1 with the transformer construction in Corollary 5 (using an argument similar as the
proof of Proposition I.4), and using the power of the linear correlation test Proposition I.5, we imme-
diately obtain the following result, which outputs a prediction from (approximately) least squares if
bÏˆ := Î¨lin(D) = 1, and abstains from predicting if bÏˆ = 0. This can be viewed as a form of â€œconfident
linear regressionâ€, where the model predicts only if it thinks the linear signal is strong enough.
Proposition I.6 (Confident linear regression). For any Bw > 0, 0 < Bâ‹†
w â‰¤Bâ‹†w, 0 â‰¤Î»min â‰¤Î»max,
Îµ â‰¤BxBw/10, 0 < Î± â‰¤Î² with Îº := Î²/Î±, there exists a L-layer attention-only transformer with
L â‰¤O

Îº log BxBw
Îµ

,
max
â„“âˆˆ[L] M (â„“) â‰¤O(1),
|||Î¸||| â‰¤O

R + 1
Î² + Î»2
min(Bâ‹†
w)2

(with
R
:=
max {BxBw, By, 1})
such
that
the
following
holds.
Let
N
â‰¥
e
O

max {K4, Î»maxK2Ïƒ2
(Bâ‹†
w)2Î»2
min } Â· d

. Suppose the input format is (3) with dimension D â‰¥2d + 4. Let ICL
instance (D, xN+1) be drawn from any distribution P satisfying Assumption D. Then the transformer
outputs a 2-dimensional prediction (within the test token ehN+1)
(byN+1, bÏˆ) âˆˆR Ã— {0, 1}
such that the following holds:
1. If âˆ¥wâ‹†
Pâˆ¥2 â‰¥Bâ‹†
w, then with probability at least 1âˆ’Î´ over D, we have |byN+1âˆ’âŸ¨bwLS, xN+1âŸ©| â‰¤Îµ,
and bÏˆ = 1 if D is in addition well-conditioned for least squares (in the sense of (5) with Î» = 0).
2. If âˆ¥wâ‹†
Pâˆ¥2 â‰¤
Î»min
10Î»max Bâ‹†
w, then with probability at least 1 âˆ’Î´ over D, we have byN+1 = 0 and
bÏˆ = 0.
J
Proof of Theorem 12: Noisy linear model with mixed noise levels
For each fixed k âˆˆ[K], we consider the following data generating model Pk, where we first sample
P = Pwâ‹†,Ïƒk âˆ¼Ï€ from wâ‹†âˆ¼N(0, Id/d), and then sample data {(xi, yi)}iâˆˆ[N+1]
iid
âˆ¼Pwâ‹†,Ïƒk as
Pwâ‹†,Ïƒk : xi âˆ¼N(0, Id),
yi = âŸ¨xi, wâ‹†âŸ©+ Îµi,
Îµi âˆ¼N(0, Ïƒ2
k).
Also, recall that the Bayes optimal estimator on Pk is given by byBayes
N+1 =
D
wÎ»k
ridge(D), xN+1
E
with
ridge Î»k = Ïƒ2
kd/N, and the Bayes risk on Pk is given by
BayesRiskk := infA Ek
 1
2(A(D)(xN+1) âˆ’yN+1)2
= Ek
h
1
2
 byBayes
N+1 âˆ’yN+1
2i
.
71

Recall that in Section 4.1.1, we consider a mixture law PÏ€ that generates data from Pk with k âˆ¼Î›. It
is clear that we have (pushing infA into Ekâˆ¼Î› does not increase the value) we have
BayesRiskÏ€ â‰¥Ekâˆ¼Î›[BayesRiskk],
i.e., the Bayes risk can only be greater if we consider a mixture of models. In other words, if a
transformer can achieve near-Bayes ICL on each meta-task Pk, then it can perform near-Bayes ICL
on any meta-task Ï€ which is a mixture of Pk with k âˆ¼Î›. Therefore, to prove Theorem 12, it suffices
to show the following (strengthened) result.
Theorem J.1 (Formal version of Theorem 12). Suppose that N â‰¥0.1d and we write Ïƒmax =
maxk{Ïƒk, 1}, Ïƒmin = mink{Ïƒk, 1}. Suppose in input format (3) we have D â‰¥Î˜(Kd). Then there
exists a transformer Î¸ with
L â‰¤O
 Ïƒâˆ’2
min log(N/Ïƒmin)

,
max
â„“âˆˆ[L] M (â„“) â‰¤O (K) ,
max
â„“âˆˆ[L] D(â„“) â‰¤O(K2),
|||Î¸||| â‰¤O (ÏƒmaxKd log(N)) ,
such that for any k âˆˆ[K], it holds that
Ek
1
2(yN+1 âˆ’byN+1)2

â‰¤BayesRiskk + e
O
 
Ïƒ2
max
Ïƒ2/3
min
log K
N
1/3
!
if we choose Nval := |Dval| â‰N 2/3[log K]1/3.
The core of the proof of Theorem J.1 is to show that any estimator bw that achieves small validation
loss bLval must achieve small population loss.
Throughout the rest of this section, recall that we define Ntrain = |Dtrain| , Nval = |Dval|, Itrain = {i :
(xi, yi) âˆˆDtrain}, Ival = {i : (xi, yi) âˆˆDval}, and Xtrain = [xi]iâˆˆItrain.
J.1
Proof of Theorem J.1
Fix parameters Î´, Îµ, Î³ > 0 and a large universal constant C0. Let us set
Î± = max
n
0, 1/2 âˆ’
p
d/Ntrain
o2
,
Î² = 25,
Bâ‹†
w = 1 + C0
r
log(N)
d
,
Bw = C0(Bâ‹†
w + Ïƒmax/Ïƒmin),
Bx = C0
p
d log(N),
By = C0(Bâ‹†
w + Ïƒmax)
p
log(N),
Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):
EÏ€ = {âˆ¥wâ‹†âˆ¥2 â‰¤Bâ‹†
w, âˆ¥Îµâˆ¥2 â‰¤2Ïƒmax
âˆš
N},
Ew = {Î± â‰¤Î»min(XâŠ¤
trainXtrain/Ntrain) â‰¤Î»max(XâŠ¤
trainXtrain/Ntrain) â‰¤Î²},
Eb,train = {âˆ€(xi, yi) âˆˆDtrain, âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,val = {âˆ€(xi, yi) âˆˆDval, âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,N+1 = {âˆ¥xN+1âˆ¥2 â‰¤Bx, |yN+1| â‰¤By}.
For the good event E := EÏ€ âˆ©Ew âˆ©Eb,train âˆ©Eb,test âˆ©Eb,N+1, we can show that P(Ec) â‰¤O
 N âˆ’10
.
Further, by the proof of Lemma F.1 (see e.g. (34)), we know that maxkâˆˆ[K]
wÎ»k
ridge(Dtrain)

2 â‰¤
Bw/2 holds under the good event E.
For the ridge Î»k =
dÏƒ2
k
Ntrain and parameters (Î±, Î², Î³, Îµ), we consider the transformer Î¸ constructed in
Theorem I.2, with a clipped prediction byN+1 = g
ready(TFÎ¸(H)).
In the following, we upper bound the quantity Ek(byN+1 âˆ’yN+1)2 for any fixed k. Similar to the
proof of Corollary 6 (Appendix F.4), we decompose
Ek(byN+1 âˆ’yN+1)2 = Ek

1{E}(byN+1 âˆ’yN+1)2
+ Ek

1{Ec}(byN+1 âˆ’yN+1)2
,
and we analyze these two parts separately.
72

Part I.
Recall that by our construction, when E holds, we have byN+1 = clipBy(âŸ¨bw, xN+1âŸ©) and
the statements of Theorem I.2 hold for bw. Thus, we have
Ek

1{E}(byN+1 âˆ’yN+1)2
= Ek
h
1{E}(clipBy(âŸ¨xN+1, bwâŸ©) âˆ’yN+1)2i
â‰¤Ek

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2
.
Let us consider the following risk functional
Lval,wâ‹†(w) = E(x,y)âˆ¼Pwâ‹†,Ïƒk
h
1
2(âŸ¨w, xâŸ©âˆ’y)2i
= 1
2

âˆ¥w âˆ’wâ‹†âˆ¥2
2 + Ïƒ2
k

.
Then, under the good event E0 := EÏ€ âˆ©Ew âˆ©Eb,train âˆ©Eb,test of (wâ‹†, D),
Ek

1{E}(âŸ¨xN+1, bwâŸ©âˆ’yN+1)2 wâ‹†, D

= Ek

1{E}(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 wâ‹†, D

â‰¤Ek

(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2 wâ‹†, D

= E(x,y)âˆ¼Pwâ‹†,Ïƒk

(âŸ¨xN+1, bw(D)âŸ©âˆ’yN+1)2
= Lval,wâ‹†(bw(D)).
By our construction, under the good event E0, we have
Lval,wâ‹†(bw(D)) â‰¤Lval,wâ‹†(bwk(Dtrain)) + max
lâˆˆ[K]
bLval(bwl(Dtrain)) âˆ’Lval,wâ‹†(bwl(Dtrain))
 + Î³,
where
bwl(Dtrain)) âˆ’wÎ»l
ridge(Dtrain)

2 â‰¤Îµ for each l âˆˆ[K]. Clearly,
2Ek[1{E0}Lval,wâ‹†(bwk(Dtrain))] = Ek
h
1{E0}

âˆ¥bwk(Dtrain) âˆ’wâ‹†âˆ¥2
2 + Ïƒ2
k
i
â‰¤Ek
h
1{E0}
wÎ»k
ridge(Dtrain) âˆ’wâ‹†
2
2 + 2Îµ
wÎ»k
ridge(Dtrain) âˆ’wâ‹†

2 + Îµ2i
+ Ïƒ2
k
â‰¤Ek
hwÎ»k
ridge(Dtrain) âˆ’wâ‹†
2
2 + 2Îµ
wÎ»k
ridge(Dtrain) âˆ’wâ‹†

2 + Îµ2i
+ Ïƒ2
k
â‰¤2Riskk,train + 2Îµ
p
2Riskk,train + Îµ2,
where we denote 2Riskk,train = Ek
wÎ»k
ridge(Dtrain) âˆ’wâ‹†
2
2 + Ïƒ2
k, and we also note that Riskk,train â‰¤
1 + Ïƒ2
k by definition. By Lemma J.1, we have
Riskk,train â‰¤BayesRiskk + O

(Ïƒ2
k + 1)Nval
N

.
We next deal with the term Îµval := maxlâˆˆ[K]
bLval(bwl(Dtrain)) âˆ’Lval,wâ‹†(bwl(Dtrain))
. Note that for
the good event Etrain := EÏ€ âˆ©Ew âˆ©Eb,train of (wâ‹†, Dtrain), we have
Ek[1{E0}Îµval] â‰¤Ek[1{Etrain}Îµval] â‰¤Ewâ‹†,Dtrainâˆ¼Pk[1{Etrain} Â· EDval [Îµval| wâ‹†, Dtrain]].
Thus, Lemma J.2 yields
Ek[1{E0}Îµval] â‰¤O
 B2
w

Â·
"r
log K
Nval
+ log K
Nval
#
.
Therefore, we can conclude that
Ek

1{E}(byN+1 âˆ’yN+1)2
â‰¤2BayesRiskk + O
 
ÎµÏƒmax + Îµ2 + Ïƒ2
maxNval
N
+ B2
w
r
log K
Nval
+ B2
w log K
Nval
!
.
Therefore, we can choose (Îµ, Nval) so that Nval â‰¤N/2 as
Nval = max
( B2
w
Ïƒ2max
N
2/3
log1/3(K), log K
)
,
Îµ = Ïƒmax
N
.
It is worth noting that such choice of Nval is feasible as long as N â‰³
B4
w
Ïƒ4
max log K. Under such choice,
we obtain
1
2Ek

1{E}(byN+1 âˆ’yN+1)2
â‰¤BayesRiskk + O
 
Ïƒ4/3
maxB2/3
w
log K
N
1/3!
.
73

Part II.
Similar to the proof of Corollary 6, we have
E

1{Ec}(byN+1 âˆ’yN+1)2
â‰¤O
 
B2
y
N 5
!
â‰¤O
Ïƒ2
max
N 4

.
Conclusion.
Combining the both cases, we obtain
Ek
 1
2(yN+1 âˆ’byN+1)2
â‰¤BayesRiskk + O

Ïƒ4/3
maxB2/3
w

log K
N
1/3
â‰¤BayesRiskk + O

Ïƒ2
max
Ïƒ2/3
min

log K
N
1/3
+ Ïƒ4/3
max
log2/3(N) log1/3(K)
d2/3N1/3

â‰¤BayesRiskk + e
O

Ïƒ2
max
Ïƒ2/3
min

log K
N
1/3
,
where we plug in our choice of By. The bounds on M (â„“), D(â„“) and |||Î¸||| follows immediately from
Theorem I.2. This completes the proof.
J.2
Derivation of the exact Bayes predictor
Let (D, xN+1, yN+1) be (N + 1) observations from the data generating model Ï€ considered in Sec-
tion 4.1.1. On observing (D, xN+1), the Bayes predictor of yN+1 is given by its posterior mean:
EÏ€[yN+1|D, xN+1] = EÏ€[âŸ¨xN+1, wâ‹†âŸ©+ ÎµN+1|D, xN+1] = âŸ¨xN+1, EÏ€[wâ‹†|D]âŸ©.
It thus remains to derive EÏ€[wâ‹†|D]. Recall that our data generating model is given by k âˆ¼Î›, By
Bayesâ€™ rule, we have
EÏ€[wâ‹†|D] =
X
kâ€²âˆˆ[K]
PÏ€(k = kâ€²|D) Â· EÏ€[wâ‹†|D, k = kâ€²].
(46)
On k = kâ€², the data is generated from the noisy linear model wâ‹†âˆ¼N(0, Id/d), and y = Xwâ‹†+ Îµ
where Îµi
iid
âˆ¼N(0, Ïƒ2
kâ€²). It is a standard result that EÏ€[wâ‹†|D, k = kâ€²] is given by the ridge estimator
EÏ€[wâ‹†|D, k = kâ€²] =
 XâŠ¤X + dÏƒ2
kâ€²
âˆ’1
|
{z
}
bÎ£âˆ’1
kâ€²
XâŠ¤y =: bwkâ€²
=
XâŠ¤X
N
+ dÏƒ2
kâ€²
N
âˆ’1 XâŠ¤y
N
.
(Note that the sample covariance within bÎ£kâ€² is not normalized by N, which is not to be confused
with remaining parts within the paper.) Therefore, the posterior mean (46) is exactly a weighted
combination of K ridge regression estimators, each with regularization dÏƒ2
k/N.
It remains to derive the mixing weights PÏ€(k = kâ€²|D) for all kâ€² âˆˆ[K]. By Bayesâ€™ rule, we have
PÏ€(k = kâ€²|D) âˆkâ€² PÏ€(k = kâ€²) Â·
Z
wâ‹†
p(wâ‹†) Â· pkâ€²,wâ‹†(D|wâ‹†)dwâ‹†
âˆÎ›kâ€² Â·
Z
w
1
(2Ï€d)d/2(2Ï€Ïƒ2
kâ€²)N/2 exp
 
âˆ’dâˆ¥wâˆ¥2
2
2
âˆ’âˆ¥Xw âˆ’yâˆ¥2
2
2Ïƒ2
kâ€²
!
dw
âˆÎ›kâ€² Â·
Z
w
1
(2Ï€Ïƒ2
kâ€²)N/2 exp
 
âˆ’1
2wâŠ¤
XâŠ¤X
Ïƒ2
kâ€²
+ dId

w +

w, XâŠ¤y
Ïƒ2
kâ€²

âˆ’âˆ¥yâˆ¥2
2
2Ïƒ2
kâ€²
!
dw
âˆÎ›kâ€² Â·
Z
w
1
(2Ï€Ïƒ2
kâ€²)N/2 exp

âˆ’1
2Ïƒ2
kâ€² (w âˆ’bwkâ€²)âŠ¤bÎ£kâ€²(w âˆ’bwkâ€²) âˆ’
1
2Ïƒ2
kâ€²

âˆ¥yâˆ¥2
2 âˆ’yâŠ¤XbÎ£âˆ’1
kâ€² XâŠ¤y

dw
âˆÎ›kâ€² Â· det(bÎ£kâ€²/Ïƒ2
kâ€²)âˆ’1/2
ÏƒN
kâ€²
exp

âˆ’1
2Ïƒ2
kâ€²

âˆ¥yâˆ¥2
2 âˆ’yâŠ¤XbÎ£âˆ’1
kâ€² XâŠ¤y

74

âˆÎ›kâ€² Â·
1
ÏƒNâˆ’d
kâ€²
det(XâŠ¤X + dÏƒ2
kâ€²Id)1/2 exp

âˆ’1
2Ïƒ2
kâ€²

âˆ¥yâˆ¥2
2 âˆ’âŸ¨y, Xbwkâ€²âŸ©

.
Note that such mixing weights involve the determinant of the matrix bÎ£kâ€² = XâŠ¤X + dÏƒ2
kâ€²Id, which
depends on the data X in a non-trivial fashion; Any transformer has to approximate these weights if
their mechanism is to directly approximate the exact Bayesian predictor (46).
J.3
Useful lemmas
Lemma J.1. For 2Riskk,train = Ek
wÎ»k
ridge(Dtrain) âˆ’wâ‹†
2
2 + Ïƒ2
k, there exists universal constant C
such that
Riskk,train â‰¤BayesRiskk + C(Ïƒ2
k + 1)Nval
N .
Proof. Recall that under Pk, we have
wâ‹†âˆ¼N(0, Id/d),
yi = âŸ¨xi, wâ‹†âŸ©+ Îµi,
Îµi âˆ¼N(0, Ïƒ2).
We denote yt = [yi]iâˆˆItrain, then by definition wÎ»k
ridge(Dtrain) = (XâŠ¤
trainXtrain +dÏƒ2
k)âˆ’1Xtrainyt (with
Î»k = dÏƒ2
k/Ntrain). Thus, a simple calculation yields
2Riskk,train = Ek
wÎ»k
ridge(Dtrain) âˆ’wâ‹†
2
2 + Ïƒ2
k = Ïƒ2
kEtr
 (XâŠ¤
trainXtrain + dÏƒ2
k)âˆ’1
+ Ïƒ2
k,
and analogously, 2BayesRiskk = Ïƒ2
kEtr
 (XâŠ¤X + dÏƒ2
kId)âˆ’1
+ Ïƒ2
k. Therefore,
2Riskk,train âˆ’2BayesRiskk = Ïƒ2
kEtr
 (XâŠ¤
trainXtrain + dÏƒ2
kId)âˆ’1
âˆ’Ïƒ2
kEtr
 (XâŠ¤X + dÏƒ2
kId)âˆ’1
â‰¤Ïƒ2
kNvalEk[Î»min(Î£)âˆ’1],
where in the above inequality we denote Î£ := XâŠ¤
trainXtrain + dÏƒ2
kId and use the following fact:
tr
 Î£âˆ’1
âˆ’tr
 (Î£ + XâŠ¤
v Xv)âˆ’1
= tr

Î£âˆ’1/2(Id âˆ’(Id + Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1/2)âˆ’1)Î£âˆ’1/2
= tr

Î£âˆ’1/2(Id + Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1/2)âˆ’1Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1
=
D
(Id + Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1/2)âˆ’1Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1/2, Î£âˆ’1E
â‰¤rank(Î£âˆ’1/2XâŠ¤
v XvÎ£âˆ’1/2)Î»max(Î£âˆ’1) â‰¤NvalÎ»min(Î£)âˆ’1.
Case 1. We first suppose that Ntrain â‰¤16d. Then by definition Î£ âª°dÏƒ2
kId, and hence
Ïƒ2
kNvalEk[Î»min(Î£)âˆ’1] â‰¤Ïƒ2
kNval
dÏƒ2
k
â‰¤16Nval
Ntrain
â‰¤32Nval
N
.
Case 2. When Ntrain â‰¥9d, then we consider the event Et := {Î»min(XâŠ¤
trainXtrain/Ntrain) â‰¥
1
16}. By
Lemma B.2 we have P(Ec
t ) â‰¤exp(âˆ’Ntrain/8). Therefore,
Ïƒ2
kNvalEk[Î»min(Î£)âˆ’1] = Ïƒ2
kNvalEk[1{Et}Î»min(Î£)âˆ’1] + Ïƒ2
kNvalEk[1{Ec
t }Î»min(Î£)âˆ’1]
â‰¤16Ïƒ2
kNval
Ntrain
Â· P(Et) + Nval
d
Â· P(Ec
t )
â‰¤32Ïƒ2
kNval
N
+ Nval
d
Â· exp(âˆ’N/16) = O
(Ïƒ2
k + 1)Nval
N

.
Combining these two cases finishes the proof.
Lemma J.2. Condition on the event Etrain, we have
EDvalâˆ¼Pk|wâ‹†,Dtrain

max
lâˆˆ[K]
bLval(bwl) âˆ’Lval,wâ‹†(bwl)


â‰¤CB2
w
ï£®
ï£°log(2K)
Nval
+
s
log(2K)
Nval
ï£¹
ï£»,
where we denote bwl = bwl(Dtrain).
75

Proof. We only need to work with a fixed pair of (wâ‹†, Dtrain) such that Etrain holds. Hence, in the
following we only consider the randomness of Dval conditional on such a (wâ‹†, Dtrain).
Recall that for any w,
bLval(w) =
1
2 |Dval|
X
(xi,yi)âˆˆDval
(âŸ¨xi, wâŸ©âˆ’yi)2,
and we have EDv[bLval(w)] = Lval,wâ‹†(w). For each i âˆˆIval,
yi âˆ’âŸ¨xi, bwlâŸ©= Îµi âˆ’âŸ¨xi, wâ‹†âˆ’bwlâŸ©âˆ¼SG(Ïƒ2
k + âˆ¥wâ‹†âˆ’bwlâˆ¥2).
Note that under Etrain, we have bwl âˆˆB2(Bw) for all l âˆˆ[K], and hence Ïƒ2
k + âˆ¥wâ‹†âˆ’bwlâˆ¥2 â‰¤5B2
w.
We then have (yi âˆ’âŸ¨xi, bwlâŸ©)2â€™s are (conditional) i.i.d random variables in SE(CB4
w). Then, by
Bernsteinâ€™s inequality, we have
PDval
bLval(bwl) âˆ’Lval,wâ‹†(bwl)
 â‰¥t

â‰¤2 exp

âˆ’cNval min
 t2
B2w
,
t
Bw

,
where c is a universal constant. Applying the union bound, we obtain
PDval

max
lâˆˆ[K]
bLval(bwl) âˆ’Lval,wâ‹†(bwl)
 â‰¥t

â‰¤K exp

âˆ’cNval min
 t2
B2w
,
t
Bw

.
Taking integration completes the proof.
J.4
Generalized linear models with adaptive link function selection
Suppose that (gk : R â†’R)kâˆˆ[K] is a set of link functions such that gk is non-decreasing and
C2-smooth for each k âˆˆ[K]. We consider the input format we introduce in Section 4.1 with
|Dtrain| = âŒˆN/2âŒ‰, |Dval| = âŒŠN/2âŒ‹.
Theorem J.2 (GLMs with adaptive link function selection). For any fixed set of parameters defined
in Assumption B, as long as N â‰¥O (d), there exists a transformer Î¸ with L â‰¤O (log(N)) layers,
input dimension D = Î˜ (dK) and maxâ„“âˆˆ[L] M (â„“) â‰¤e
O
 d3N

, such that the following holds.
For any kâ‹†âˆˆ[K] and any distribution P that is a generalized linear model of the link function gkâ‹†
and some parameter Î², if Assumption B holds for each pair (P, gk), then
E(D,xN+1,yN+1)âˆ¼P

(byN+1 âˆ’yN+1)2
â‰¤E(x,y)âˆ¼P

(gkâ‹†(âŸ¨x, Î²âŸ©) âˆ’y)2
+ O
 
d
N +
r
log(K)
N
!
,
or equivalently, E(D,xN+1)âˆ¼P[(byN+1 âˆ’E[yN+1|xN+1])2] â‰¤O

d/N +
p
log(K)/N

.
Proof. For each k âˆˆ[K], we consider optimizing the following training loss:
w(k)
GLM := arg min
w
bL(k)
train(w) :=
1
Ntrain
X
(xi,yi)âˆˆDtrain
â„“k(âŸ¨xi, wâŸ©, yi),
where â„“k(t, y) := âˆ’yt+
R t
0 gk(s)ds is the convex (integral) loss associated with gk (as in Section 3.1).
Also, for each predictor f : Rd â†’R, we consider the squared validation loss bLval:
bLval(f) :=
1
2Nval
X
(xi,yi)âˆˆDval
(f(xi) âˆ’yi)2.
Fix a large universal constant C0. Let us set
Î± = ÂµgÂµx/8,
Î² = 8LgKx,
Bx = C0Kx
p
d log(N),
By = C0Ky
p
log(N),
76

Then, we define good events similarly to the proof of Corollary 6 (Appendix F.4):
Ew =
n
âˆ€k âˆˆ[K], âˆ€w âˆˆB2(Bw), Î± â‰¤Î»min(âˆ‡2bL(k)
train(w)) â‰¤Î»max(âˆ‡2bL(k)
train(w)) â‰¤Î²,
o
,
Er =
n
âˆ€k âˆˆ[K],
w(k)
GLM

2 â‰¤Bw/2
o
,
Eb,train = {âˆ€(xi, yi) âˆˆDtrain, âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,val = {âˆ€(xi, yi) âˆˆDval, âˆ¥xiâˆ¥2 â‰¤Bx, |yi| â‰¤By},
Eb,N+1 = {âˆ¥xN+1âˆ¥2 â‰¤Bx, |yN+1| â‰¤By}.
Similar to the proof of Theorem G.2 (Appendix G.2), we know the good event E := Ew âˆ©Er âˆ©
Eb,train âˆ©Eb,test âˆ©Eb,N+1 holds with high probability: P(Ec) â‰¤O
 N âˆ’10
.
Similar to the proof of Theorem I.2, we can show that there exists a transformer Î¸ with prediction
byN+1 = g
ready(TFÎ¸(H)) (clipped by By), such that (for any P) the following holds under E:
(a) For each k âˆˆ[K], fk = Ak(Dtrain) is a predictor such that
fk(xi) âˆ’gk(âŸ¨xi, w(k)
GLMâŸ©)
 â‰¤Îµ
for all i âˆˆ[N + 1] (where Îµ is chosen as in Appendix G.2).
(b) byN+1 = clipBy( bf(xN+1)), where bf = ATF(D) is an aggregated predictor given by bf =
P
k Î»kfk, such that (Î»k) is a distribution supported on k âˆˆ[K] such that bLval(fk) â‰¤
minkâ€²âˆˆ[K] bLval(fkâ€²) + Î³.
Similar to the proof of Theorem G.2, for E0 := Ew âˆ©Er âˆ©Eb,train âˆ©Eb,test, we have
E(D,xN+1,yN+1)âˆ¼P(byN+1 âˆ’yN+1)2 â‰¤EDâˆ¼P
h
1{E0}Lval( bf)
i
+ O
 
B2
y
N 5
!
,
where we denote Lval(f) := E(x,y)âˆ¼P
h
1{âˆ¥xâˆ¥2 â‰¤Bx}(f(x) âˆ’y)2i
for each predictor f. By the
definition of bf, we then have (under E0)
Lval( bf) â‰¤Lval(fkâ‹†) + max
l
bLval(fl) âˆ’Lval(fl)
 + Î³.
For the first term, repeating the argument in the proof of Theorem G.2 directly yields that for
Etrain := Ew âˆ©Er âˆ©Eb,train,
EDtrainâˆ¼P[1{Etrain}Lval(fkâ‹†)] â‰¤E(x,y)âˆ¼P(gkâ‹†(âŸ¨x, Î²âŸ©) âˆ’y)2 + O (d/Ntrain) .
For the second term, similar to Lemma J.2, we can show that conditional on Dtrain such that Etrain
holds, it holds
EDvalâˆ¼P|Dtrain

1{E0} max
l
bLval(fl) âˆ’Lval(fl)


â‰¤O
 K2
y

Â·
 r
log K
Nval
+ log K
Nval
!
.
Combining these inequalities and suitably choosing Î³ complete the proof.
K
Analysis of pretraining
Thus far, we have established the existence of transformers for performing various ICL tasks with
good in-context statistical performance. We now analyze the sample complexity of pretraining these
transformers from a finite number of training ICL instances.
K.1
Generalization guarantee for pretraining
Setup
At pretraining time, each training ICL instance has form Z := (H, yN+1), where H :=
H(D, xN+1) âˆˆRDÃ—(N+1) denote the input sequence formatted as in (3). We consider the square
loss between the in-context prediction and the ground truth label:
â„“icl(Î¸; Z) := 1
2

yN+1 âˆ’clipBy
 ready
|
{z
}
g
ready
(TFR
Î¸ (H))
2
.
77

Above, clipBy(t) := max {min {t, By}, âˆ’By} is the standard clipping operator onto [âˆ’By, By],
and TFR
Î¸ the transformer architecture as in Definition 3 with clipping operators after each layer: let
H(0) = clipR(H),
H(â„“) = clipR

MLPÎ¸(â„“)
mlp

AttnÎ¸(â„“)
attn

H(â„“âˆ’1)
for all â„“âˆˆ[L],
clipR(H) := [Projâˆ¥hâˆ¥2â‰¤R(hi)]i.
The clipping operator is used to control the Lipschitz constant of TFÎ¸ with respect to Î¸, and we
typically choose a sufficiently large clipping radius R so that it does not modify the behavior of the
transformer on any input sequence of our concern.
We draw ICL instances Z := (H, yN+1) = (D, (xN+1, yN+1)) from a (meta-)distribution denoted as
Ï€, which first sample an in-context data distribution P âˆ¼Ï€, then sample iid examples (xi, yi)N+1
i=1
iid
âˆ¼
PâŠ—(N+1) and form D = {(xi, yi)}iâˆˆ[N]. Our pretraining loss is the average ICL loss on n pretraining
instances Z(1:n) iid
âˆ¼Ï€, and we consider the corresponding test ICL loss on a new test instance:
bLicl(Î¸) := 1
n
n
X
j=1
â„“icl(Î¸; Zj),
Licl(Î¸) := EPâˆ¼Ï€,Zâˆ¼PâŠ—(N+1)[â„“icl(Î¸; Z)].
Our pretraining algorithm is to solve a standard constrained empirical risk minimization (ERM)
problem over transformers with L layers, M heads, and norm bound B (recall the definition of the
|||Â·||| norm in (2)):
bÎ¸ :=
arg min
Î¸âˆˆÎ˜L,M,Dâ€²,B
bLicl(Î¸),
Î˜L,M,Dâ€²,B :=

Î¸ = (Î¸(1:L)
attn , Î¸(1:L)
mlp ) : max
â„“âˆˆ[L] M (â„“) â‰¤M, max
â„“âˆˆ[L] D(â„“) â‰¤Dâ€², |||Î¸||| â‰¤B

.
(TF-ERM)
Generalization guarantee
By standard uniform concentration analysis via chaining arguments
(Proposition B.4; see also [87, Chapter 5] for similar arguments), we have the following excess loss
guarantee for (TF-ERM). The proof can be found in Appendix L.2.
Theorem K.1 (Generalization for pretraining). With probability at least 1 âˆ’Î¾ (over the pretraining
instances {Zj}jâˆˆ[n]), the solution bÎ¸ to (TF-ERM) satisfies
Licl(bÎ¸) â‰¤
inf
Î¸âˆˆÎ˜L,M,Dâ€²,B
Licl(Î¸) + O
 
B2
y
r
L2(MD2 + DDâ€²)Î¹ + log(1/Î¾)
n
!
,
where Î¹ = log(2 + max {B, R, By}) is a log factor.
K.2
Examples of pretraining for in-context regression problems
In Theorem K.1, the comparator infÎ¸âˆˆÎ˜L,M,Dâ€²,B Licl(Î¸) is simply the smallest expected ICL loss
for ICL instances drawn from Ï€, among all transformers within the norm ball Î˜L,M,Dâ€²,B. Using
our constructions in Section 3 & 4, we show that this comparator loss is small on various (meta-
)distribution Ï€â€™s, by which we obtain end-to-end guarantees for pretraining transformers with small
ICL loss at test time. Here we showcase this argument on several representative regression problems.
Linear regression
For any in-context data distribution P, let wâ‹†
P := EP[xxâŠ¤]âˆ’1EP[xy] denote
the best linear predictor for P. We show that with mild choices of L, M, B, the learned transformer
can perform in-context linear regression with near-optimal statistical power, in that on the sampled
P âˆ¼Ï€ and ICL instance {(xi, yi)}iâˆˆ[N+1]
iid
âˆ¼P, it competes with the best linear predictor wâ‹†
P for
this particular P. The proof follows directly by on combining Corollary 5 with Theorem K.1, and
can be found in Appendix L.3.
Theorem K.2 (Pretraining transformers for in-context linear regression). Suppose P âˆ¼Ï€ is almost
surely well-posed for in-context linear regression (Assumption A) with the canonical parameters.
Then, for N â‰¥e
O(d), with probability at least 1 âˆ’Î¾ (over the training instances Z(1:n)), the solution
78

bÎ¸ of (TF-ERM) with L = O(Îº log(ÎºN/Ïƒ)) layers, M = 3 heads, Dâ€² = 0 (attention-only), and
B = O(
âˆš
Îºd) achieves small excess ICL risk over wâ‹†
P:
Licl(bÎ¸) âˆ’EPâˆ¼Ï€E(x,y)âˆ¼P
1
2(y âˆ’âŸ¨wâ‹†
P, xâŸ©)2

â‰¤e
O
 r
Îº2d2 + log(1/Î¾)
n
+ dÏƒ2
N
!
,
where e
O(Â·) only hides polylogarithmic factors in Îº, N, 1/Ïƒ.
To our best knowledge, Theorem K.2 offers the first end-to-end result for pretraining a transformer
to perform in-context linear regression with explicit excess loss bounds. The e
O(
p
Îº2d2/n) term
originates from the generalization of pretraining (Theorem K.1), where as the e
O(dÏƒ2/N) term
agrees with the standard fast rate for the excess loss of linear regression [38]. Further, as long as
n â‰¥e
O(Îº2N/Ïƒ2), the excess risk achieves the optimal rate e
O(dÏƒ2/N) (up to log factors).
Additional examples
By similar arguments as in the proof of Theorem K.2, we can directly turn
most of our other expressivity results into results on the pretrained transformers. Here we present
three such additional examples (proofs in Appendix L.4-L.6). The first example is for the sparse
linear regression problem considered in Theorem 8.
Theorem K.3 (Pretraining transformers for in-context sparse linear regression). Suppose each P âˆ¼Ï€
is almost surely an instance of the sparse linear model specified in Theorem 8 with parameters Bâ‹†
w
and Ïƒ. Suppose N â‰¥e
O(s log((d âˆ¨N)/Ïƒ)) and let Îº := Bâ‹†
w/Ïƒ.
Then with probability at least 1 âˆ’Î¾ (over the training instances Z(1:n)), the solution bÎ¸ of (TF-ERM)
with L = e
O(Îº2(1 + d/N)) layers, M = 2 heads, Dâ€² = 2d, and B = e
O(poly(d, Bâ‹†
w, Ïƒ)) achieves
small excess ICL risk:
Licl(bÎ¸) âˆ’Ïƒ2 â‰¤e
O
 r
Îº4d2(1 + d/N)2 + log(1/Î¾)
n
+ Ïƒ2 s log d
N
!
,
where e
O(Â·) only hides polylogarithmic factors in d, N, 1/Ïƒ.
Our next example is for the problem of noisy linear regression with mixed noise levels considered
in Theorem 12 and Theorem J.1. There, the constructed transformer uses the post-ICL validation
mechanism to perform ridge regression with an adaptive regulariation strength depending on the
particular input sequence.
Theorem K.4 (Pretraining transformers for in-context noisy linear regression with algorithm selec-
tion). Suppose Ï€ is the data generating model (noisy linear model with mixed noise levels) considered
in Theorem J.1, with Ïƒmax â‰¤O(1). Let N â‰¥d/10.
Then, with probability at least 1 âˆ’Î¾ (over the training instances Z(1:n)), the solution bÎ¸ of (TF-
ERM) with input dimension D = Î˜(dK), L = O(Ïƒâˆ’2
min log(N/Ïƒmin)) layers, M = O(K) heads,
Dâ€² = O(K2), and B = O(poly(K, Ïƒâˆ’1
min, d, N)) achieves small excess ICL risk:
Licl(bÎ¸) âˆ’BayesRiskÏ€ â‰¤e
O
ï£«
ï£­
s
Ïƒâˆ’4
minK3d2 + log(1/Î¾)
n
+ Ïƒ2
max
Ïƒ2/3
min
log K
N
1/3
ï£¶
ï£¸,
where e
O(Â·) only hides polylogarithmic factors in d, N, K, 1/Ïƒmin.
Our final example is for in-context logistic regression. For simplicity we consider the realizable case.
Theorem K.5 (Pretraining transformers for in-context logistic regression; square loss guarantee).
Suppose for P âˆ¼Ï€, P is almost surely a realizable logistic model (i.e. P = Plog
Î² with âˆ¥Î²âˆ¥2 â‰¤Bâ‹†
w as
in Corollary G.1). Suppose that Bâ‹†
w = O (1) and N â‰¥O (d).
Then, with probability at least 1 âˆ’Î¾ (over the training instances Z(1:n)), the solution bÎ¸ of (TF-ERM)
with L = O(log(N)) layers, M = e
O
 d3N

heads, Dâ€² = 0, and B = O(poly(d, N)) achieves
small excess ICL risk:
Licl(bÎ¸) âˆ’EPlog
Î² âˆ¼Ï€E(x,y)âˆ¼Plog
Î²
1
2(y âˆ’Ïƒlog(âŸ¨Î², xâŸ©))2

â‰¤e
O
 r
d5N + log(1/Î¾)
n
+ d
N
!
,
where e
O(Â·) only hides polylogarithmic factors in d, N.
79

Remark on generality of transformer
All results above are established by the expressivity results
in Section 3 & 4 for transformers to implement various ICL procedures (such as least squares, Lasso,
GLM, and ridge regression with in-context algorithm selection), combined with the generalization
bound (Theorem K.1). However, the transformer itself was not specified to encode any actual structure
about the problem at hand in any result above, other than having sufficiently large number of layers,
number of heads, and weight norms, which illustrates the flexibility of the transformer architecture.
L
Proofs for Section K
L.1
Lipschitzness of transformers
For any p âˆˆ[1, âˆ], let âˆ¥Hâˆ¥2,p := (PN
i=1 âˆ¥hiâˆ¥p
2)1/p denote the column-wise (2, p)-norm of H. For
any radius R > 0, we denote HR := {H : âˆ¥Hâˆ¥2,âˆâ‰¤R} be the ball of radius R under norm âˆ¥Â·âˆ¥2,âˆ.
Lemma L.1. For a single MLP layer Î¸mlp = (W1, W2), we introduce its norm (as in (2))
Î¸mlp
 = âˆ¥W1âˆ¥op + âˆ¥W2âˆ¥op .
For any fixed hidden dimension Dâ€², we consider
Î˜mlp,B :=

Î¸mlp :
Î¸mlp
 â‰¤B
	
.
Then for H âˆˆHR, Î¸mlp âˆˆÎ˜mlp,B, the function (Î¸mlp, H) 7â†’MLPÎ¸mlp(H) is (BR)-Lipschitz w.r.t.
Î¸mlp and (1 + B2)-Lipschitz w.r.t. H.
Proof. Recall that by our definition, for the parameter Î¸mlp = (W1, W2) âˆˆÎ˜mlp,B and the input
H = [hi] âˆˆRDÃ—N, the output MLPÎ¸mlp(H) = H + W2Ïƒ(W1H) = [hi + W2Ïƒ(W1hi)]i.
Therefore, for Î¸â€²
mlp = (Wâ€²
1, Wâ€²
2) âˆˆÎ˜mlp,B, we have
MLPÎ¸mlp(H) âˆ’MLPÎ¸â€²
mlp(H)

2,âˆ
= max
i
âˆ¥W2Ïƒ(W1hi) âˆ’Wâ€²
2Ïƒ(Wâ€²
1hi)âˆ¥2
= max
i
âˆ¥(W2 âˆ’Wâ€²
2)Ïƒ(W1hi) + Wâ€²
2(Ïƒ(W1hi) âˆ’Ïƒ(Wâ€²
1hi))âˆ¥2
â‰¤max
i
âˆ¥W2 âˆ’Wâ€²
2âˆ¥op âˆ¥Ïƒ(W1hi)âˆ¥2 + âˆ¥Wâ€²
2âˆ¥op âˆ¥Ïƒ(W1hi) âˆ’Ïƒ(Wâ€²
1hi)âˆ¥2
â‰¤max
i
âˆ¥W2 âˆ’Wâ€²
2âˆ¥op âˆ¥W1hiâˆ¥2 + âˆ¥Wâ€²
2âˆ¥op âˆ¥W1hi âˆ’Wâ€²
1hiâˆ¥2
â‰¤BR âˆ¥W2 âˆ’Wâ€²
2âˆ¥op + BR âˆ¥W1 âˆ’Wâ€²
1âˆ¥op ,
where the second inequality follows from the 1-Lipschitznees of Ïƒ = [Â·]+. Similarly, for Hâ€² = [hâ€²
i] âˆˆ
RDÃ—N,
MLPÎ¸mlp(H) âˆ’MLPÎ¸mlp(Hâ€²)

2,âˆ= max
i
âˆ¥hi + W1Ïƒ(W2hi) âˆ’hâ€²
i âˆ’W1Ïƒ(W2hâ€²
i)âˆ¥2
â‰¤âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ+ max
i
âˆ¥W1(Ïƒ(W2hi) âˆ’Ïƒ(W2hâ€²
i))âˆ¥2
â‰¤âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ+ max
i
B âˆ¥Ïƒ(W2hi) âˆ’Ïƒ(W2hâ€²
i)âˆ¥2
â‰¤âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ+ B2 âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ.
Lemma L.2. For a single attention layer Î¸attn = {(Vm, Qm, Km)}mâˆˆ[M] âŠ‚RDÃ—D, we introduce
its norm (as in (2))
|||Î¸attn||| := max
mâˆˆ[M]
n
âˆ¥Qmâˆ¥op , âˆ¥Kmâˆ¥op
o
+
M
X
m=1
âˆ¥Vmâˆ¥op .
For any fixed dimension D, we consider
Î˜attn,B := {Î¸attn : |||Î¸attn||| â‰¤B}.
Then for H âˆˆHR, Î¸attn âˆˆÎ˜attn,B, the function (Î¸attn, H) 7â†’AttnÎ¸attn(H) is (B2R3)-Lipschitz
w.r.t. Î¸attn and (1 + B3R2)-Lipschitz w.r.t. H.
80

Proof. Recall that by our definition, for the parameter Î¸attn = {(Vm, Qm, Km)}mâˆˆ[M] âˆˆÎ˜attn,B
and the input H = [hi] âˆˆRDÃ—N, the output AttnÎ¸attn(H) = [ehi] is given by
ehi = hi +
M
X
m=1
1
N
N
X
j=1
Ïƒ(âŸ¨Qmhi, KmhjâŸ©) Â· Vmhj.
Now, for Î¸â€²
attn = {(Vâ€²
m, Qâ€²
m, Kâ€²
m)}mâˆˆ[M], we consider
ehâ€²
i =

AttnÎ¸â€²
attn(H)

i = hi +
M
X
m=1
1
N
N
X
j=1
Ïƒ(âŸ¨Qâ€²
mhi, Kâ€²
mhjâŸ©) Â· Vâ€²
mhj,
âˆ€i âˆˆ[N].
Clearly
AttnÎ¸attn(H) âˆ’AttnÎ¸â€²
attn(H)

2,âˆ= maxi
ehi âˆ’ehâ€²
i

2. For any i âˆˆ[N], we have
ehi âˆ’ehâ€²
i

2 =

M
X
m=1
1
N
N
X
j=1
[Ïƒ(âŸ¨Qmhi, KmhjâŸ©)Vmhj âˆ’Ïƒ(âŸ¨Qâ€²
mhi, Kâ€²
mhjâŸ©)Vâ€²
mhj]

2
â‰¤
M
X
m=1
1
N
N
X
j=1
âˆ¥Ïƒ(âŸ¨Qmhi, KmhjâŸ©)Vm âˆ’Ïƒ(âŸ¨Qâ€²
mhi, Kâ€²
mhjâŸ©)Vâ€²
mâˆ¥op âˆ¥hjâˆ¥2
â‰¤
M
X
m=1
1
N
N
X
j=1
âˆ¥hjâˆ¥2
nÏƒ(âŸ¨Qmhi, KmhjâŸ©)
 Â· âˆ¥Vm âˆ’Vâ€²
mâˆ¥op
+
Ïƒ(âŸ¨Qmhi, KmhjâŸ©) âˆ’Ïƒ(âŸ¨Qâ€²
mhi, KmhjâŸ©)
 Â· âˆ¥Vâ€²
mâˆ¥op
+
Ïƒ(âŸ¨Qâ€²
mhi, KmhjâŸ©) âˆ’Ïƒ(âŸ¨Qâ€²
mhi, Kâ€²
mhjâŸ©)
 Â· âˆ¥Vâ€²
mâˆ¥op
o
â‰¤
M
X
m=1
1
N
N
X
j=1
R
n
B2R2 Â· âˆ¥Vm âˆ’Vâ€²
mâˆ¥op + âˆ¥Qmhi âˆ’Qâ€²
mhiâˆ¥2 Â· âˆ¥Kmhjâˆ¥2 Â· âˆ¥Vâ€²
mâˆ¥op
+ âˆ¥Qâ€²
mhiâˆ¥2 Â· âˆ¥Kmhj âˆ’Kâ€²
mhjâˆ¥2 Â· âˆ¥Vâ€²
mâˆ¥op
o
â‰¤
M
X
m=1
R
n
B2R2 âˆ¥Vm âˆ’Vâ€²
mâˆ¥op + BR2 âˆ¥Qm âˆ’Qâ€²
mâˆ¥op Â· âˆ¥Vâ€²
mâˆ¥op + BR2 âˆ¥Km âˆ’Kâ€²
mâˆ¥op Â· âˆ¥Vâ€²
mâˆ¥op
o
â‰¤B2R3n
M
X
m=1
âˆ¥Vm âˆ’Vâ€²
mâˆ¥op + max
m âˆ¥Qm âˆ’Qâ€²
mâˆ¥op + max
m âˆ¥Km âˆ’Kâ€²
mâˆ¥op
o
= B2R3|||Î¸attn âˆ’Î¸â€²
attn|||,
where the second inequality uses the definition of operator norm, the third inequality follows from
the triangle inequality, the forth inequality is because âˆ¥Qmhiâˆ¥2 â‰¤BR, âˆ¥Kmhjâˆ¥2 â‰¤BR, and Ïƒ is
1-Lipschitz. This completes the proof the Lipschitzness w.r.t. Î¸attn.
Similarly, we consider Hâ€² = [hâ€²
i], and
ehâ€²
i =

AttnÎ¸â€²
attn(H)

i = hâ€²
i +
M
X
m=1
1
N
N
X
j=1
Ïƒ
 
Qmhâ€²
i, Kmhâ€²
j

Â· Vmhâ€²
j,
âˆ€i âˆˆ[N].
By definition, we can similarly bound


ehâ€²
i âˆ’hâ€²
i

âˆ’

ehi âˆ’hi

2
=

M
X
m=1
1
N
N
X
j=1

Ïƒ(âŸ¨Qmhi, KmhjâŸ©)Vmhj âˆ’Ïƒ
 
Qmhâ€²
i, Kmhâ€²
j

Vmhâ€²
j


2
â‰¤
M
X
m=1
1
N
N
X
j=1
âˆ¥Vmâˆ¥op
Ïƒ(âŸ¨Qmhi, KmhjâŸ©)hj âˆ’Ïƒ
 
Qmhâ€²
i, Kmhâ€²
j

hâ€²
j

2
81

â‰¤
M
X
m=1
1
N
N
X
j=1
âˆ¥Vmâˆ¥op
nÏƒ(âŸ¨Qmhi, KmhjâŸ©)
 Â·
hj âˆ’hâ€²
j

2
+
Ïƒ(âŸ¨Qmhi, KmhjâŸ©) âˆ’Ïƒ(âŸ¨Qmhâ€²
i, KmhjâŸ©)
 Â·
hâ€²
j

2
+
Ïƒ(âŸ¨Qmhâ€²
i, KmhjâŸ©) âˆ’Ïƒ
 
Qmhâ€²
i, Kmhâ€²
j
 Â·
hâ€²
j

2
o
â‰¤
M
X
m=1
1
N
N
X
j=1
âˆ¥Vmâˆ¥op Â· 3 âˆ¥Qmâˆ¥op âˆ¥Kmâˆ¥op R2 hj âˆ’hâ€²
j

2
â‰¤R2 âˆ¥H âˆ’Hâ€²âˆ¥2,âˆÂ· 3 max
mâˆˆ[M] âˆ¥Qmâˆ¥op âˆ¥Kmâˆ¥op Â·
M
X
m=1
âˆ¥Vmâˆ¥op
â‰¤B3R2 âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ,
where the last inequality uses |||Î¸attn||| â‰¤B and the AM-GM inequality. This completes the proof the
Lipschitzness w.r.t. H.
Corollary L.1. For a fixed number of heads M and hidden dimension Dâ€², we consider
Î˜TF,1,B =

Î¸ = (Î¸attn, Î¸mlp) : M heads, hidden dimension Dâ€², |||Î¸||| â‰¤B
	
.
Then for the function TFR given by
TFR : (Î¸, H) 7â†’clipR
 MLPÎ¸mlp(AttnÎ¸attn(H))

,
Î¸ âˆˆÎ˜TF,1,B, H âˆˆHR
TFR is BÎ˜-Lipschitz w.r.t Î¸ and LH-Lipschitz w.r.t. H, where BÎ˜ := BR(1 + BR2 + B3R2) and
BH := (1 + B2)(1 + B2R3).
Proof. For any Î¸ = (Î¸attn, Î¸mlp), H âˆˆHR, and Î¸â€² = (Î¸â€²
attn, Î¸â€²
mlp), we have
âˆ¥TFÎ¸(H) âˆ’TFÎ¸â€²(H)âˆ¥2,âˆâ‰¤
MLPÎ¸mlp(AttnÎ¸attn(H)) âˆ’MLPÎ¸mlp
 AttnÎ¸â€²
attn(H)

2,âˆ
+
MLPÎ¸mlp
 AttnÎ¸â€²
attn(H)

âˆ’MLPÎ¸â€²
mlp
 AttnÎ¸â€²
attn(H)

2,âˆ
â‰¤(1 + B2)
AttnÎ¸attn(H) âˆ’AttnÎ¸â€²
attn(H)

2,âˆ+ BR
Î¸mlp âˆ’Î¸â€²
mlp

â‰¤(1 + B2)B2R3|||Î¸attn âˆ’Î¸â€²
attn||| + BR
Î¸mlp âˆ’Î¸â€²
mlp

â‰¤BÎ˜|||Î¸ âˆ’Î¸â€²|||,
where the second inequality follows from Lemma L.2 and Lemma L.1 and the fact that
âˆ¥AttnÎ¸attn(H)âˆ¥2,âˆâ‰¤R := R + B3R3 for all H âˆˆHR.
Furthermore, for Hâ€² âˆˆHR, we have
âˆ¥TFÎ¸(H) âˆ’TFÎ¸(Hâ€²)âˆ¥2,âˆâ‰¤(1 + B2) âˆ¥AttnÎ¸attn(H) âˆ’AttnÎ¸attn(Hâ€²)âˆ¥2,âˆ
â‰¤(1 + B2)(1 + B3R2) âˆ¥H âˆ’Hâ€²âˆ¥2,âˆ,
which also follows from Lemma L.2 and Lemma L.1.
Proposition L.1 (Lipschitzness of transformers). For a fixed number of heads M and hidden
dimension Dâ€², we consider
Î˜TF,L,B =
n
Î¸ = (Î¸(1:L)
attn , Î¸(1:L)
mlp ) : M (â„“) = M, D(â„“) = Dâ€², |||Î¸||| â‰¤B
o
.
Then the function TFR is (LBLâˆ’1
H
BÎ˜)-Lipschitz w.r.t Î¸ âˆˆÎ˜TF,L,B for any fixed H.
Proof. For Î¸ = Î¸(1:L) âˆˆÎ˜TF,L,B, eÎ¸ = eÎ¸(1:L) âˆˆÎ˜TF,L,B, we have
TFR
Î¸(H) âˆ’TFR
eÎ¸(H)

2,âˆ
82

â‰¤
L
X
â„“=1
TFR
Î¸(â„“+1:L)

TFR
Î¸(â„“)

TFR
eÎ¸(1:â„“âˆ’1)(H)

âˆ’TFR
Î¸(â„“+1:L)

TFR
eÎ¸(â„“)

TFR
eÎ¸(1:â„“âˆ’1)(H)

2,âˆ
â‰¤
L
X
â„“=1
BLâˆ’â„“
Î˜
TFR
Î¸(â„“)

TFR
eÎ¸(1:â„“âˆ’1)(H)

âˆ’TFR
eÎ¸(â„“)

TFR
eÎ¸(1:â„“âˆ’1)(H)

2,âˆ
â‰¤
L
X
â„“=1
BLâˆ’â„“
H
BÎ˜ Â·


Î¸(â„“) âˆ’eÎ¸(â„“)

 â‰¤LBLâˆ’1
H
BÎ˜ Â·


Î¸ âˆ’eÎ¸


,
where the second inequality follows from Corollary L.1, and the last inequality is because BH â‰¥
1.
L.2
Proof of Theorem K.1
In this section, we prove a slightly more general result by considering the general ICL loss
â„“icl(Î¸; Z) := â„“( g
ready(TFR
Î¸(H)), yN+1).
We assume that the loss function â„“satisfies sup |â„“| â‰¤B0
â„“and sup |âˆ‚1â„“| â‰¤B1
â„“. For the special case
â„“(s, t) = 1
2(s âˆ’t)2, we can take B0
â„“= 4B2
y, B1
â„“= 2By.
We then consider
XÎ¸ := 1
n
n
X
j=1
â„“icl(Î¸; Zj) âˆ’EZ[â„“icl(Î¸; Z)],
where Z(1:n) are i.i.d copies of Z âˆ¼P, P âˆ¼Ï€. It remains to apply Proposition B.4 to the random
process {XÎ¸}. We verify the preconditions:
(a) By [87, Example 5.8], it holds that log N(Î´; B|||Â·|||(r), |||Â·|||) â‰¤L(3MD2 + 2DDâ€²) log(1 + 2r/Î´),
where B|||Â·|||(r) is any ball of radius r under norm |||Â·|||.
(b) |â„“icl(Î¸; Z)| â‰¤B0
â„“and hence B0
â„“-sub-Gaussian.
(c)
â„“icl(Î¸; Z) âˆ’â„“icl(eÎ¸; Z)
 â‰¤B1
â„“Â· (LBLâˆ’1
H
BÎ˜) Â·


Î¸ âˆ’eÎ¸


, by Proposition L.1.
Therefore, we can apply the uniform concentration result in Proposition B.4 to obtain that, with
probability at least 1 âˆ’Î¾,
sup
Î¸
|XÎ¸| â‰¤CB0
â„“
r
L(MD2 + DDâ€²)Î¹ + log(1/Î¾)
n
,
where Î¹ = log(2 + B Â· LBLâˆ’1
H
BÎ˜B1
â„“/B0
â„“) â‰¤20L log(2 + max{B, R, B1
â„“/B0
â„“}). Recalling that
Licl(bÎ¸) â‰¤inf
Î¸ Licl(Î¸) + 2 sup
Î¸
|XÎ¸|
completes the proof.
L.3
Proof of Theorem K.2
By Corollary 5, there exists a transformer TFÎ¸ such that for every P satisfying Assumption A with
canonical parameters (and thus in expectation over P âˆ¼Ï€) and every N â‰¥e
O(d), it outputs prediction
byN+1 = g
ready(TFÎ¸(H)) such that
Licl(Î¸) = EPâˆ¼Ï€,(D,xN+1,yN+1âˆ¼P)
1
2(byN+1 âˆ’yN+1)2

â‰¤EPâˆ¼Ï€[LP(wâ‹†
P)] + O
dÏƒ2
N

,
where we recall that LP(wâ‹†
P) := 1
2E(x,y)âˆ¼P

(y âˆ’âŸ¨wâ‹†
P, xâŸ©)2
. By inspecting the proof, the same
result holds if we change TFÎ¸ to the clipped version TFR
Î¸ if we choose R2 = O(B2
x+B2
y+B2
w+1) =
O(d + Îº), so that on the good event Ecov âˆ©Ew considered therein, all intermediate outputs within
83

TFÎ¸ has âˆ¥Â·âˆ¥2,âˆâ‰¤R and thus the clipping does not modify the transformer output on Ecov âˆ©Ew.
Further, recall by (33) that Î¸ has size bounds
L â‰¤O

Îº log NÎº
Ïƒ

,
max
â„“âˆˆ[L] M (â„“) â‰¤3,
|||Î¸||| â‰¤O(
âˆš
Îºd).
We can thus apply Theorem K.1 to obtain that the solution bÎ¸ to (TF-ERM) with the above choice of
(L, M, B) and Dâ€² = 0 (attention-only) satisfies the following with probability at least 1 âˆ’Î¾:
Licl(bÎ¸) â‰¤
inf
Î¸â€²âˆˆÎ˜L,M,Dâ€²,B
Licl(Î¸â€²) + O
 r
L2MD2Î¹ + log(1/Î¾)
n
!
â‰¤Licl(Î¸) + e
O
 r
L2MD2 + log(1/Î¾)
n
!
â‰¤e
O
 r
Îº2d2 + log(1/Î¾)
n
+ dÏƒ2
N
!
.
Above, Î¹ = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
L.4
Proof of Theorem K.3
We invoke Theorem 8 (using the construction in Theorem 7 with a different choice of L) with the
following parameters:
L = e
O
 (Bâ‹†
w)2/Ïƒ2 Ã— (1 + d/N)

= e
O
 Îº2(1 + d/N)

,
M = Î˜(1),
Dâ€² = 2d,
Bx = e
O(
âˆš
d),
By = e
O(Bâ‹†
w + Ïƒ),
Î´ =

Ïƒ2
1
B2yN
2
,
|||Î¸||| â‰¤B = O
 R + (1 + Î»N)Î²âˆ’1
â‰¤O

R + Ïƒ
p
log d

â‰¤e
O(poly(d, Bâ‹†
w, Ïƒ)),
where e
O(Â·) hides polylogarithmic factors in d, N, Bâ‹†
w, Îº.
Then, Theorem 8 shows that there exists a transformer Î¸ with L layers, maxâ„“âˆˆ[L] M (â„“) â‰¤M heads,
Dâ€² hidden dimension for the MLP layers, and |||Î¸||| â‰¤B such that, on almost surely every P âˆ¼Ï€, it
returns a prediction byN+1 such that, on the good event E0 considered therein (over D âˆ¼P) which
satisfies P(E0) â‰¥1 âˆ’Î´,
E(xN+1,yN+1)âˆ¼P
h
(byN+1 âˆ’yN+1)2i
â‰¤Ïƒ2[1 + O(s log(d/Î´)/N)].
By inspecting the proof, the same result holds if we change TFÎ¸ to the clipped version TFR
Î¸ if we
choose R2 = O(B2
x + B2
y + (Bâ‹†
w)2 + 1) = O(d + (Bâ‹†
w)2 + Ïƒ2), so that on the good event E0
considered therein, all intermediate outputs within TFÎ¸ has âˆ¥Â·âˆ¥2,âˆâ‰¤R and thus the clipping does
not modify the transformer output on the good event. On the bad event Ec
0, using the same argument
as in the proof of Theorem 8, we have
ED,(xN+1,yN+1)âˆ¼P

1{Ec
0}(byN+1 âˆ’yN+1)2
â‰¤
q
PD(Ec
0) Â·
 8EyN+1âˆ¼P

B4
y + y4
N+1
1/2 â‰¤e
O
Ïƒ2
N

.
Combining the above two bounds and further taking expectation over P âˆ¼Ï€ gives
Licl(Î¸) = EPâˆ¼Ï€,(D,xN+1,yN+1)âˆ¼P
1
2(byN+1 âˆ’yN+1)2

â‰¤Ïƒ2 + e
O
 Ïƒ2s log d/N

.
We can thus apply Theorem K.1 to obtain that the solution bÎ¸ to (TF-ERM) with the above choice of
(L, M, B, Dâ€²) satisfies the following with probability at least 1 âˆ’Î¾:
Licl(bÎ¸) â‰¤
inf
Î¸â€²âˆˆÎ˜L,M,Dâ€²,B
Licl(Î¸â€²) + O
 r
L2(MD2 + DDâ€²)Î¹ + log(1/Î¾)
n
!
â‰¤Licl(Î¸) + e
O
 r
L2(MD2 + DDâ€²) + log(1/Î¾)
n
!
â‰¤Ïƒ2 + e
O
 r
Îº4d2(1 + d/N)2 + log(1/Î¾)
n
+ Ïƒ2 s log d
N
!
.
Above, Î¹ = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
84

L.5
Proof of Theorem K.4
We invoke Theorem 12 and Theorem J.1, which shows that (recalling the input dimension D =
Î˜(Kd)) there exists a transformer Î¸ with the following size bounds:
L â‰¤O
 Ïƒâˆ’2
min log(N/Ïƒmin)

,
max
â„“âˆˆ[L] M (â„“) â‰¤M = O (K) ,
max
â„“âˆˆ[L] D(â„“) â‰¤Dâ€² = O(K2),
|||Î¸||| â‰¤O (ÏƒmaxKd log(N)) ,
such that it outputs byN+1 that satisfies
EÏ€
1
2(yN+1 âˆ’byN+1)2

â‰¤BayesRiskÏ€ + e
O
 
Ïƒ2
max
Ïƒ2/3
min
log K
N
1/3
!
.
By inspecting the proof, the same result holds if we change TFÎ¸ to the clipped version TFR
Î¸ if we
choose R2 = O(B2
x + B2
y + (Bâ‹†
w)2 + 1) = O(d + Ïƒ2
max), so that on the good event considered
therein, all intermediate outputs within TFÎ¸ has âˆ¥Â·âˆ¥2,âˆâ‰¤R and thus the clipping does not modify
the transformer output on the good event. Using this clipping radius, we obtain
Licl(Î¸) = EPâˆ¼Ï€,(D,xN+1,yN+1)âˆ¼P
1
2(byN+1 âˆ’yN+1)2

â‰¤BayesRiskÏ€ + e
O
 
Ïƒ2
max
Ïƒ2/3
min
log K
N
1/3
!
.
We can thus apply Theorem K.1 to obtain that the solution bÎ¸ to (TF-ERM) with the above choice of
(L, M, B, Dâ€²) satisfies the following with probability at least 1 âˆ’Î¾:
Licl(bÎ¸) â‰¤
inf
Î¸â€²âˆˆÎ˜L,M,Dâ€²,B
Licl(Î¸â€²) + O
 r
L2(MD2 + DDâ€²)Î¹ + log(1/Î¾)
n
!
â‰¤Licl(Î¸) + e
O
 r
L2(MD2 + DDâ€²) + log(1/Î¾)
n
!
â‰¤BayesRiskÏ€ + e
O
ï£«
ï£­
s
Ïƒâˆ’4
minK3d2 + log(1/Î¾)
n
+ Ïƒ2
max
Ïƒ2/3
min
log K
N
1/3
ï£¶
ï£¸.
Above, Î¹ = O(log(1 + max {By, R, B})) = e
O(1). This finishes the proof.
L.6
Proof of Theorem K.5
The proof follows from similar arguments as of Theorem K.3 and Theorem K.4, where we plug in the
size bounds (number of layers, heads, and weight norms) from Theorem G.2 and Corollary G.1.
M
Experimental details and additional studies
M.1
Additional details for Section 6
Architecture and optimization
We train a 12-layer encoder-only transformer, where each layer
consists of an attention layer as in Definition 1 with M = 8 heads, hidden dimension D = 64, and
ReLU activation (normalized by the sequence length), as well as an MLP layer as in Definition 2
hidden dimension Dâ€² = 64. We add Layer Normalization [3] after each attention and MLP layer
to help optimization, as in standard implementations [84]. We append linear read-in layer and
linear read-out layer before and after the transformer respectively, both applying a same affine
transform to all tokens in the sequence and are trainable. The read-in layer maps any input vector
to a D-dimensional hidden state, and the read-out layer maps a D-dimensional hidden state to a
1-dimensional scalar.
Each training sequence corresponds to a single ICL instance with N in-context training examples
{(xi, yi)}N
i=1 âŠ‚Rd Ã— R and test input xN+1 âˆˆRd. The input to the transformer is formatted as
85

in (3) where each token has dimension d + 1 (no zero-paddings). The transformer is trained by
minimizing the following loss with fresh mini-batches:
L(Î¸) = EPâˆ¼Ï€,(H,yN+1)âˆ¼P[â„“P(ready(TFÎ¸(H)), yN+1)],
(47)
where the loss function â„“P : R2 â†’R may depend on the training data distribution P in general;
we use the square loss when P is regression data, and the logistic loss when P is classification data.
We use the Adam optimizer with a fixed learning rate 10âˆ’4, which we find works well for all our
experiments. Throughout all our experiments except for the sparse linear regression experiment
in Figure 3a, we train the model for 300K steps, where each step consists of a (fresh) minibatch with
batch size 64 in the base mode, and K minibatches each with batch size 64 in the mixture mode.
For the sparse linear regression experiment, we find that minimizing the training objective (47) alone
was not enough, e.g. for the learned transformer to achieve better loss than the least squares algorithm
(which achieves much higher test loss than the Lasso; cf. Figure 3a). To help optimization, we
augment (47) with another loss that encourages the second-to-last hidden states to recover the true
(sparse) coefficient wâ‹†:
Lfit-w(Î¸) = 1
N0
N0
X
j=1
EP=Pwâ‹†âˆ¼Ï€,(H,yN+1)âˆ¼P
"
h
TF(1:Lâˆ’1)
Î¸
(H)
i
j,(Dâˆ’d+1):D âˆ’wâ‹†

2
2
#
.
(48)
Specifically, the above loss encourages the first N0 â‰¤N tokens within the second-to-last layer to
be close to wâ‹†. We choose N0 = 5 (recall that the total number of tokens is N = 10 and sequence
length is N + 1 = 11 for this experiment). We minimize the loss L(Î¸) + Î»Lfit-w(Î¸) with Î» = 0.1
for 2M steps for this task.
Evaluation
All evaluations are done on the trained transformer with 6400 test instances. We use
the square loss for regression tasks, and the classification error (1âˆ’accuracy) between the true label
yN+1 âˆˆ{0, 1} and the predicted label 1{byN+1 â‰¥1/2}. We report the means in all experiments,
as well as their standard deviations (using one-std error bars) in Figure 2a, 2b, 5a, 5b. In Figure
2c, 3b, 3c 5c, all standard deviations are sufficiently small (not significantly exceeding the width of
the markers), thus we did not show error bars in those plots.
Baseline algorithms
We implement various baseline machine learning algorithms to compare with
the learned transformers. A superset of the algorithms is shown in Figure 3a:
â€¢ Least squares, Logistic regression: Standard algorithms for linear regression and linear
classification, respectively. Note that least squares is also a valid algorithm for classification.
â€¢ Averaging: The simple algorithm which computes the linear predictor bw = 1
N
PN
i=1 yixi and
predicts byN+1 = âŸ¨bw, xN+1âŸ©;
â€¢ 3-NN: 3-Nearest Neighbors.
â€¢ Ridge: Standard ridge regression as in (ICRidge). We specifically consider two Î»â€™s (denoted
as lam_1 and lam_2): Î»1, Î»2 = (0.005, 0.125). These are the Bayes-optimal regularization
strengths for the noise levels (Ïƒ1, Ïƒ2) = (0.1, 0.5) respectively under the noisy linear model
(cf. Corollary 6), using the formula Î»â‹†= dÏƒ2/N, with (d, N) = (20, 40).
â€¢ Lasso: Standard Lasso as in (ICLasso) with Î» âˆˆ{1, 0.1, 0.01, 0.001}.
In Figure 2c, the ridge_analytical curve plots the expected risk of ridge regression under the
noisy linear model over 20 geometrically spaced values of Î»â€™s in between (Î»1, Î»2), using analytical
formulae (with Monte Carlo simulations). The Bayes_err_{1,2} indicate the expected risks of Î»1
on task 1 (with noise Ïƒ1) and Î»2 on task 2 (with noise Ïƒ2), respectively.
M.2
Decoder-based architecture
ICL capabilities have also been demonstrated in the literature for decoder-based architectures [31,
2, 47]. There, the transformer can do in-context predictions at every token xi using past tokens
{(xj, yj)}jâ‰¤iâˆ’1 as training examples. Here we show that such architectures is also able to perform
in-context algorithm selection at every token; For results for this architecture on â€œbaseâ€ ICL tasks
(such as those considered in Figure 3a), we refer the readers to Garg et al. [31].
86

(a) Linear regression
0
10
20
30
40
in-context examples
0.0
0.2
0.4
0.6
0.8
1.0
square loss
TF_alg_select
TF_reg
TF_cls
Least Squares
(b) Linear classification
0
10
20
30
40
in-context examples
0.15
0.20
0.25
0.30
0.35
0.40
error
TF_alg_select
TF_reg
TF_cls
Logistic Regression
(c) Reg vs. cls at token 40
0
1
2
3
4
regression_square_loss
0.20
0.25
0.30
0.35
classification_error
TF_alg_select
TF_reg
TF_cls
Least Squares
Averaging
3-NN
Figure 5: In-context algorithm selection abilities of transformers between linear regression and linear clas-
sification. (a,b) On these two tasks, a single transformer TF_alg_select simultaneously approaches
the performance of the strongest baseline algorithm Least Squares on linear regression and Logistic
Regression on linear classification. (c) At token 40 (using example {0, . . . , 39} for training), TF_alg_select
matches the performance of the best baseline algorithm for both tasks. (a,b,c) Note that transformers pretrained
on a single task (TF_reg, TF_cls) perform near-optimally on their pretraining task but suboptimally on the
other task.
Setup
Our setup is the same as the two â€œmixtureâ€ modes (linear model + linear classification
model, and noisy linear models with two different noise levels) as in Section 6, except that the
architecture is GPT-2 following Garg et al. [31], and the input format is changed to (11) (so that the
input sequence has 2N + 1 tokens) without positional encodings. For every i âˆˆ[N + 1], we extract
the prediction byi using a linear read-out function applied on output token 2i âˆ’1, and the (learnable)
linear read-out function is the same across all tokens, similar as in Appendix M.1. The rest of the
setup (optimization, training, and evaluation) is the same as in Section 6 & M.1. Note that we also
train on the objective (47) for all tokens averaged, instead of for the last test token as in Section 6.
Result
Figure 2 shows the results for noisy linear models with two different noise levels, and Figure
5 shows the results for linear model + linear classification model. We observe that at every token,
In both cases, TF_alg_select nearly matches the strongest baseline for both tasks simultaneously,
whereas transformers trained on a single task perform suboptimally on the other task. Further, this
phenomenon consistently shows up at every token. For example, in Figure 2a & 2b, TF_alg_select
matches ridge regression with the optimal Î» on all tokens i âˆˆ{1, . . . , N} (N = 40). In Figure
5a & 5b, TF_alg_select matches least squares on the regression task and logistic regression on
the classification task on all tokens i âˆˆ[N]. This demonstrates the in-context algorithm selection
capabilities of standard decoder-based transformer architectures.
M.3
Computational resource
All our experiments are performed on 8 Nvidia Tesla A100 GPUs (40GB memory). The total GPU
time is approximately 5 days (on 8 GPUs), with the largest individual training run taking about a
single day on a single GPU.
87

