Clifford Group Equivariant Neural Networks
David Ruhe
AI4Science Lab, AMLab, API
University of Amsterdam
david.ruhe@gmail.com
Johannes Brandstetter
Microsoft Research
AI4Science
brandstetter@ml.jku.at
Patrick Forré
AI4Science Lab, AMLab
University of Amsterdam
p.d.forre@uva.nl
Abstract
We introduce Clifford Group Equivariant Neural Networks: a novel approach
for constructing O(n)- and E(n)-equivariant models. We identify and study the
Clifford group: a subgroup inside the Clifford algebra tailored to achieve several fa-
vorable properties. Primarily, the group’s action forms an orthogonal automorphism
that extends beyond the typical vector space to the entire Clifford algebra while
respecting the multivector grading. This leads to several non-equivalent subrepre-
sentations corresponding to the multivector decomposition. Furthermore, we prove
that the action respects not just the vector space structure of the Clifford algebra but
also its multiplicative structure, i.e., the geometric product. These findings imply
that every polynomial in multivectors, including their grade projections, constitutes
an equivariant map with respect to the Clifford group, allowing us to parameter-
ize equivariant neural network layers. An advantage worth mentioning is that
we obtain expressive layers that can elegantly generalize to inner-product spaces
of any dimension. We demonstrate, notably from a single core implementation,
state-of-the-art performance on several distinct tasks, including a three-dimensional
n-body experiment, a four-dimensional Lorentz-equivariant high-energy physics
experiment, and a five-dimensional convex hull experiment.

1
Introduction
Incorporating group equivariance to ensure symmetry constraints in neural networks has been a highly
fruitful line of research [CW16, WGTB17, CGKW18, KT18, WC19, WGW+18, Est20, BBCV21,
WFVW21, CLW22]. Besides translation and permutation equivariance [ZKR+17, SGT+08], rotation
equivariance proved to be vitally important for many graph-structured problems as encountered in,
e.g., the natural sciences. Applications of such methods include modeling the dynamics of complex
physical systems or motion trajectories [KFW+18, BHP+22]; studying or generating molecules,
proteins, and crystals [RDRL14, GLB+16, CTS+17, SSK+18, ZCD+20, TVS+21, AGB22]; and
point cloud analysis [WSK+15, UPH+19]. Note that many of these focus on three-dimensional
problems involving rotation, reflection, or translation equivariance by considering the groups O(3),
SO(3), E(3), or SE(3).
Such equivariant neural networks can be broadly divided into three categories: approaches that
scalarize geometric quantities, methods employing regular group representations, and those utilizing
irreducible representations, often of O(3) [HRXH22]. Scalarization methods operate exclusively on
Code is available at https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

1
|{z}
Scalars
e1 + e2 + e3
|
{z
}
Vectors
e12 + e13 + e23
|
{z
}
Bivectors
e123
| {z }
Trivectors
ρ(w)
⟳
ρ(w)
⟳
ϕ
ϕ
Figure 1: CGENNs (represented with ϕ) are able to operate on multivectors (elements of the Clifford
algebra) in an O(n)- or E(n)-equivariant way. Specifically, when an action ρ(w) of the Clifford
group, representing an orthogonal transformation such as a rotation, is applied to the data, the model’s
representations corotate. Multivectors can be decomposed into scalar, vector, bivector, trivector, and
even higher-order components. These elements can represent geometric quantities such as (oriented)
areas or volumes. The action ρ(w) is designed to respect these structures when acting on them.
scalar features or manipulate higher-order geometric quantities such as vectors via scalar multiplica-
tion [SSK+18, CCG18, KGG20, KKN20, SHW21, JES+21, GBG21, SUG21, DLD+21, HHR+22,
TF22]. They can be limited by the fact that they do not extract all directional information. Regular
representation methods construct equivariant maps through an integral over the respective group
[CW16, KT18]. For continuous Lie groups, however, this integral is intractable and requires coarse
approximation [FSIW20, Bek19]. Methods of the third category employ the irreducible repre-
sentations of O(3) (the Wigner-D matrices) and operate in a steerable spherical harmonics basis
[TSK+18, AHK19, FWFW20, BHP+22, BMS+22]. This basis allows a decomposition into type-l
vector subspaces that transform under Dl: the type-l matrix representation of O(3) [Len90, FA+91].
Through tensor products decomposed using Clebsch-Gordan coefficients (Clebsch-Gordan tensor
product), vectors (of different types) interact equivariantly. These tensor products can be parameter-
ized using learnable weights. Key limitations of such methods include the necessity for an alternative
basis, along with acquiring the Clebsch-Gordan coefficients, which, although they are known for
unitary groups of any dimension [KR67], are not trivial to obtain [AKHvD11].
We propose Clifford Group Equivariant Neural Networks (CGENNs): an equivariant parameterization
of neural networks based on Clifford algebras. Inside the algebra, we identify the Clifford group
and its action, termed the (adjusted) twisted conjugation, which has several advantageous properties.
Unlike classical approaches that represent these groups on their corresponding vector spaces, we
carefully extend the action to the entire Clifford algebra. There, it automatically acts as an orthogonal
automorphism that respects the multivector grading, enabling nontrivial subrepresentations that
operate on the algebra subspaces. Furthermore, the twisted conjugation respects the Clifford algebra’s
multiplicative structure, i.e. the geometric product, allowing us to bypass the need for explicit tensor
product representations. As a result, we obtain two remarkable properties. First, all polynomials in
multivectors generate Clifford group equivariant maps from the Clifford algebra to itself. Additionally,
grade projections are equivariant, allowing for a denser parameterization of such polynomials. We
then demonstrate how to construct parameterizable neural network layers using these properties.
Our method comes with several advantages. First, instead of operating on alternative basis repre-
sentations such as the spherical harmonics, CGENNs (similarly to scalarization methods) directly
transform data in a vector basis. Second, multivector representations allow a (geometrically mean-
ingful) product structure while maintaining a finite dimensionality as opposed to tensor product
representations. Through geometric products, we can transform vector-valued information, resulting
in a more accurate and nuanced interpretation of the underlying structures compared to scalarization
methods. Further, we can represent exotic geometric objects such as pseudovectors, encountered
in certain physics problems, which transform in a nonstandard manner. Third, our method readily
generalizes to orthogonal groups regardless of the dimension or metric signature of the space, thereby
attaining O(n)- or E(n)-equivariance. These advantages are demonstrated on equivariance bench-
2

marks of different dimensionality. Note that specialized tools were developed for several of these
tasks, while CGENNs can be applied more generally.
2
The Clifford Algebra
Clifford algebras (also known as geometric algebras) are powerful mathematical objects with ap-
plications in various areas of science and engineering. For a complete formal construction, we
refer the reader to Appendix D. Let V be a finite-dimensional vector space over a field F equipped
with a quadratic form q : V →F. The Clifford algebra Cl(V, q) is the unitary, associative, non-
commutative algebra generated by V such that for every v ∈V the relation v2 = q(v) holds, i.e.,
vectors square to scalars. This simple property solely generates a unique mathematical theory
that underpins many applications. Note that every element x of the Clifford algebra Cl(V, q) is a
linear combination of (formal, non-commutative) products of vectors modulo the condition that
every appearing square v2 gets identified with the scalar square q(v): x = P
i∈I ci · vi,1 · · · vi,ki
2.
Here, the index set I is finite, ci ∈F, k ∈N0, vi,j ∈V . The Clifford algebra’s associated bi-
linear form b(v1, v2) := 1
2 (q(v1 + v2) −q(v1) −q(v2)) yields the fundamental Clifford identity:
v1v2 + v2v1 = 2b(v1, v2) for v1, v2 ∈V (Lemma D.3). In this context, the quantity v1v2 repre-
sents the geometric product, which is aptly named for its ability to compute geometric properties
and facilitate various transformations. Note that when v1, v2 are orthogonal (e.g., for orthogonal
basis vectors), b(v1, v2) = 0, in which case v1v2 = −v2v1. The dimensionality of the algebra
is 2n, where n := dim V (Theorem D.15). Let e1, . . . , en be an orthogonal basis of V . The tu-
ple (eA)A⊆[n], [n] := {1, . . . , n}, is an orthogonal basis for Cl(V, q), where for all such A the
product eA := Q<
i∈A ei is taken in increasing order (Theorem D.26). We will see below that we
can decompose the algebra into vector subspaces Cl(m)(V, q), m = 0, . . . , n, called grades, where
dim Cl(m)(V, q) =
  n
m

. Elements of grade m = 0 and m = 1 are scalars (Cl(0)(V, q) = F) and
vectors (Cl(1)(V, q) = V ), respectively, while elements of grade m = 2 and m = 3 are referred to as
bivectors and trivectors. Similar terms are used for elements of even higher grade. These higher-order
grades can represent (oriented) points, areas, and volumes, as depicted in Figure 1.
Clifford algebras provide tools that allow for meaningful algebraic representation and manipulation
of geometric quantities, including areas and volumes [RDK21, DM02, DFM09]. In addition, they
offer generalizations such as extensions of the exterior and Grassmannian algebras, along with the
natural inclusion of complex numbers and Hamilton’s quaternions [Gra62, Ham66]. Applications
of Clifford algebras can be found in robotics [BCRLZE06, HZBC08], computer graphics [WCL05,
BTH22], signal processing [Hit12, BMQQS+21], and animation [HP10, CC12]. In the context of
machine learning, Clifford algebras and hypercomplex numbers have been employed to improve the
performance of algorithms in various tasks. For example, [MFW21] learn an equivariant embedding
using geometric neurons used for classification tasks. Further, [Spe21] introduce geometric algebra
attention networks for point cloud problems in physics, chemistry, and biology. More recently,
[BBWG22] introduce Clifford neural layers and Clifford Fourier transforms for accelerating solving
partial differential equations. [RGdK+23] continue this direction, strengthening the geometric
inductive bias by the introduction of geometric templates. Concurrently with this work, [BDHBC23]
develop the geometric algebra transformer. Further, [LK14, PRM+18, ZXXC18] introduce complex-
valued and quaternion-valued networks. Finally, [KIdHN23] define normalizing flows on the group
of unit quaternions for sampling molecular crystal structures.
3
Theoretical Results
In order to construct equivariant multivector-valued neural networks, we outline our theoretical
results. We first introduce the following theorem regarding the multivector grading of the Clifford
algebra, which is well-known in case the algebra’s metric is non-degenerate. Although the general
case, including a potentially degenerate metric, appears to be accepted, we were unable to find a
self-contained proof during our studies. Hence, we include it here for completeness.
Theorem 3.1 (The multivector grading of the Clifford algebra). Let e1, . . . , en and b1, . . . , bn be
two orthogonal bases of (V, q). Then the following sub-vector spaces Cl(m)(V, q) of Cl(V, q),
2If ki = 0 the product of vectors vi,1 · · · vi,ki is empty, and, in this case, we mean the unit 1 in Cl(V, q).
3

m = 0, . . . , n, are independent of the choice of the orthogonal basis, i.e.,
Cl(m)(V, q) := span {eA | A ⊆[n], |A| = m}
!= span {bA | A ⊆[n], |A| = m} .
(1)
The proof can be found in Theorem D.27. Intuitively, this means that the claims made in the following
(and the supplementary material) are not dependent on the chosen frame of reference, even in the
degenerate setting. We now declare that the Clifford algebra Cl(V, q) decomposes into an orthogonal
direct sum of the vector subspaces Cl(m)(V, q). To this end, we need to extend the bilinear form b
from V to Cl(V, q). For elements x1, x2, x ∈Cl(V, q), the extended bilinear form ¯b and the extended
quadratic form ¯q are given via the projection onto the zero-component (explained below), where
β : Cl(V, q) →Cl(V, q) denotes the main anti-involution of Cl(V, q)3 :
¯b : Cl(V, q) × Cl(V, q) →F,
¯b(x1, x2) := (β(x1)x2)(0),
¯q(x) := ¯b(x, x).
(2)
Note that by construction, both ¯b and ¯q reduce to their original versions when restricted to V . Using
the extended quadratic form, the tuple (Cl(V, q), ¯q) turns into a quadratic vector space in itself. As a
corollary (see Corollary D.30), the Clifford algebra has an orthogonal-sum decomposition w.r.t. the
extended bilinear form ¯b:
Cl(V, q) =
n
M
m=0
Cl(m)(V, q),
dim Cl(m)(V, q) =
n
m

.
(3)
This result implies that we can always write x ∈Cl(V, q) as x = x(0) + x(1) + · · · + x(n), where
x(m) ∈Cl(m)(V, q) denotes the grade-m part of x. Selecting a grade defines an orthogonal projection:
(_)(m) : Cl(V, q) →Cl(m)(V, q),
x 7→x(m),
m = 0, . . . , n.
(4)
Let us further introduce the notation Cl[0](V, q) := Ln
m even Cl(m)(V, q), whose elements are of even
parity, and Cl[1](V, q) := Ln
m odd Cl(m)(V, q) for those of odd parity. We use x = x[0] + x[1] to
denote the parity decomposition of a multivector.
3.1
The Clifford Group and its Clifford Algebra Representations
Let Cl×(V, q) denote the group of invertible elements of the Clifford algebra, i.e., the set of those ele-
ments w ∈Cl(V, q) that have an inverse w−1 ∈Cl(V, q): w−1w = ww−1 = 1. For w ∈Cl×(V, q),
we then define the (adjusted) twisted conjugation as follows:
ρ(w) : Cl(V, q) →Cl(V, q),
ρ(w)(x) := wx[0]w−1 + α(w)x[1]w−1,
(5)
where α is the main involution of Cl(V, q), which is given by α(w) := w[0] −w[1]. This map
ρ(w) : Cl(V, q) →Cl(V, q), notably not just from V →V , will be essential for constructing
equivariant neural networks operating on the Clifford algebra. In general, ρ and similar maps defined
in the literature do not always posses the required properties (see Motivation E.1). However, when
our ρ is restricted to a carefully chosen subgroup of Cl×(V, q), many desirable characteristics emerge.
This subgroup will be called the Clifford group4 of Cl(V, q) and we define it as:
Γ(V, q) :=
n
w ∈Cl×(V, q) ∩

Cl[0](V, q) ∪Cl[1](V, q)
  ∀v ∈V, ρ(w)(v) ∈V
o
.
(6)
In words, Γ(V, q) contains all invertible (parity) homogeneous elements that preserve vectors (m = 1
elements) via ρ. The special Clifford group is defined as Γ[0](V, q) := Γ(V, q) ∩Cl[0](V, q).
Regarding the twisted conjugation, ρ(w) was ensured to reduce to a reflection when restricted
to V – a property that we will conveniently use in the upcoming section. Specifically, when
w, x ∈Cl(1)(V, q) = V , w ∈Cl×(V, q), then ρ(w) reflects x in the hyperplane normal to w:
ρ(w)(x) = −wxw−1
!= x −2 b(w, x)
b(w, w)w.
(7)
Next, we collect several advantageous identities of ρ in the following theorem, which we elaborate
on afterwards. For proofs, consider Lemma E.8, Theorem E.10, and Theorem E.29.
3Recall that any x ∈Cl(V, q) can be written as a linear combination of vector products. β is the map that
reverses the order: β
 P
i∈I ci · vi,1 · · · vi,ki

:= P
i∈I ci · vi,ki · · · vi,1. For details, see Definition D.18.
4We elaborate shortly on the term Clifford group in contrast to similar definitions in Remark E.14.
4

Cl(V, q) × · · · × Cl(V, q)
ℓtimes
z
}|
{
Cl(V, q) × · · · × Cl(V, q)
Cl(V, q)
Cl(V, q)
ρ(w)
ρ(w)
ρ(w)
F
ρ(w)
F
Cl(V, q)
Cl(V, q)
Cl(m)(V, q)
Cl(m)(V, q)
ρ(w)
ρ(w)
(_)(m)
(_)(m)
Figure 2: Commutative diagrams expressing Clifford group equivariance with respect to the main
operations: polynomials F (left) and grade projections (_)(m) (right).
Theorem 3.2. Let w1, w2, w ∈Γ(V, q), x1, x2, x ∈Cl(V, q), c ∈F, m ∈{0, . . . , n}. ρ then
satisfies:
1. Additivity: ρ(w)(x1 + x2) = ρ(w)(x1) + ρ(w)(x2),
2. Multiplicativity: ρ(w)(x1x2) = ρ(w)(x1)ρ(w)(x2),
and:
ρ(w)(c) = c,
3. Invertibility: ρ(w−1)(x) = ρ(w)−1(x),
4. Composition: ρ(w2) (ρ(w1)(x)) = ρ(w2w1)(x),
and:
ρ(c)(x) = x for c ̸= 0,
5. Orthogonality: ¯b (ρ(w)(x1), ρ(w)(x2)) = ¯b (x1, x2).
The first two properties state that ρ(w) is not only additive, but even multiplicative regarding the
geometric product. The third states that ρ(w) is invertible, making it an algebra automorphism
of Cl(V, q). The fourth property then states that ρ : Γ(V, q) →AutAlg(Cl(V, q)) is also a group
homomorphism to the group of all algebra automorphisms. In other words, Cl(V, q) is a linear
representation of Γ(V, q) and, moreover, it is also an algebra representation of Γ(V, q). Finally, the
last point shows that each ρ(w) generates an orthogonal map with respect to the extended bilinear
form ¯b. These properties yield the following results (see also Theorem E.16, Corollary E.18, Figure 2).
Corollary 3.3 (All grade projections are Clifford group equivariant). For w ∈Γ(V, q), x ∈Cl(V, q)
and m = 0, . . . , n we have the following equivariance property:
ρ(w)(x(m)) = (ρ(w)(x))(m) .
(8)
In particular, for x ∈Cl(m)(V, q) we also have ρ(w)(x) ∈Cl(m)(V, q).
This implies that the grade projections: Cl(V, q) →Cl(m)(V, q) are Γ(V, q)-equivariant maps, and,
that each Cl(m)(V, q) constitutes an orthogonal representation of Γ(V, q). The latter means that ρ
induces a group homomorphisms from the Clifford group to the group of all orthogonal invertible
linear transformations of Cl(m)(V, q):
ρ(m) : Γ(V, q) →O

Cl(m)(V, q), ¯q

,
ρ(m)(w) := ρ(w)|Cl(m)(V,q).
(9)
Corollary 3.4 (All polynomials are Clifford group equivariant). Let F ∈F[T1, . . . , Tℓ] be a
polynomial in ℓvariables with coefficients in F, w ∈Γ(V, q).
Further, consider ℓelements
x1, . . . , xℓ∈Cl(V, q). Then we have the following equivariance property:
ρ(w) (F(x1, . . . , xℓ)) = F(ρ(w)(x1), . . . , ρ(w)(xℓ)).
(10)
To prove that ρ(w) distributes over any polynomial, we use both its additivity and multiplicativity
regarding the geometric product.
By noting that one can learn the coefficients of such polynomials, we can build flexible parameteriza-
tions of Clifford group equivariant layers for quadratic vector spaces of any dimension or metric. We
involve grade projections to achieve denser parameterizations. More details regarding neural network
constructions follow in Section 4.
5

3.2
Orthogonal Representations
To relate to the orthogonal group O(V, q), the set of invertible linear maps Φ : V →V preserving q,
first note that Equation (5) shows that for every w ∈F× we always have: ρ(w) = idCl(V,q). So the
action ρ on Cl(V, q) can already be defined on the quotient group Γ(V, q)/F×. Moreover, we have:
Theorem 3.5 (See also Remark E.31 and Corollary E.32 ). If (V, q) is non-degenerate5 then ρ induces
a well-defined group isomorphism:
¯ρ(1) : Γ(V, q)/F×
∼
−→O(V, q),
[w] 7→ρ(w)|V .
(11)
The above implies that O(V, q) acts on whole Cl(V, q) in a well-defined way. Concretely, if
x ∈Cl(V, q) is of the form x = P
i∈I ci · vi,1 · · · vi,ki with vi,j ∈V , ci ∈F and Φ ∈O(V, q)
is given by ¯ρ(1)([w]) = Φ with w ∈Γ(V, q), then we have:
ρ(w)(x) =
X
i∈I
ci · ρ(w)(vi,1) · · · ρ(w)(vi,ki) =
X
i∈I
ci · Φ(vi,1) · · · Φ(vi,ki).
(12)
This means that Φ acts on a multivector x by applying an orthogonal transformation to all
its vector components vi,j, as illustrated in Figure 1.
Theorem 3.5 implies that that a map
f : Cl(V, q)ℓin →Cl(V, q)ℓout is equivariant to the Clifford group Γ(V, q) if and only if it is
equivariant to the orthogonal group O(V, q) when both use the actions on Cl(V, q) described
above (for each component). To leverage these results for constructing O(V, q)-equivariant neural
networks acting on vector features, i.e., x ∈V ℓin, we refer to Section 4.
To prove Theorem 3.5, we invoke Cartan-Dieudonné (Theorem C.13), stating that every orthogonal
transformation decomposes into compositions of reflections, and recall that ρ reduces to a reflection
when restricted to V , see Theorem E.25. Theorem 3.5 also allows one to provide explicit descriptions
of the element of the Clifford group, see Corollary E.27.
Finally, it is worth noting that our method is also Pin and Spin group-equivariant6. These groups,
sharing properties with the Clifford group, are also often studied in relation to the orthogonal group.
4
Methodology
We restrict our layers to use F := R. Our method is most similar to steerable methods such as
[BHP+22]. However, unlike these works, we do not require an alternative basis representation
based on spherical harmonics, nor do we need to worry about Clebsch-Gordan coefficients. Instead,
we consider simply a steerable vector basis for V , which then automatically induces a steerable
multivector basis for Cl(V, q) and its transformation kernels. By steerability, we mean that this
basis can be transformed in a predictable way under an action from the Clifford group, which acts
orthogonally on both V and Cl(V, q) (see Figure 1).
We present layers yielding Clifford group-equivariant optimizable transformations. All the main ideas
are based on Corollary 3.3 and Corollary 3.4. It is worth mentioning that the methods presented here
form a first exploration of applying our theoretical results, making future optimizations rather likely.
Linear Layers
Let x1, . . . , xℓ∈Cl(V, q) be a tuple of multivectors expressed in a steerable basis,
where ℓrepresents the number of input channels. Using the fact that a polynomial restricted to the
first order constitutes a linear map, we can construct a linear layer by setting
y(k)
cout := T lin
ϕcout (x1, . . . , xℓ)(k) :=
ℓ
X
cin=1
ϕcoutcink x(k)
cin ,
(13)
where ϕcoutcink ∈R are optimizable coefficients and cin, cout denote the input and output channel,
respectively. As such, Tϕ : Cl(V, q)ℓ→Cl(V, q) is a linear transformation in each algebra subspace
5Here we restrict (V, q) to be non-degenerate as we never consider degenerate quadratic forms in our
experiments. However, in the supplementary material we generalize this also to the degenerate case by carefully
taking the radical subspace R of (V, q) into account on both sides of the isomorphism.
6We discuss in Appendix E.6 a general definition of these groups.
6

k. Recall that this is possible due to the result that ρ(w) respects the multivector subspaces. This
computes a transformation for the output channel cout; the map can be repeated (using different sets of
parameters) for other output channels, similar to classical neural network linear layers. For y(0)
cout ∈R
(the scalar part of the Clifford algebra), we can additionally learn an invariant bias parameter.
Geometric Product Layers
A core strength of our method comes from the fact that we can also
parameterize interaction terms. In this work, we only consider layers up to second order. Higher-order
interactions are indirectly modeled via multiple successive layers. As an example, we take the pair
x1, x2. Their interaction terms take the form

x(i)
1 x(j)
2
(k)
, i, j, k = 0, . . . , n; where we again make
use of the fact that ρ(w) respects grade projections. As such, all the grade-k terms resulting from the
interaction of x1 and x2 are parameterized with
Pϕ(x1, x2)(k) :=
n
X
i=0
n
X
j=0
ϕijk

x(i)
1 x(j)
2
(k)
,
(14)
where Pϕ : Cl(V, q) × Cl(V, q) →Cl(V, q). This means that we get (n + 1)3 parameters for every
geometric product between a pair of multivectors7. Parameterizing and computing all second-order
terms amounts to ℓ2 such operations, which can be computationally expensive given a reasonable
number of channels ℓ. Instead, we first apply a linear map to obtain y1, . . . , yℓ∈Cl(V, q). Through
this map, the mixing (i.e., the terms that will get multiplied) gets learned. That is, we only get ℓ
pairs (x1, y1), . . . , (xℓ, yℓ) from which we then compute z(k)
cout := Pϕcout (xcin, ycin)(k). Note that here
we have cin = cout, i.e., the number of channels does not change. Hence, we refer to this layer as
the element-wise geometric product layer. We can obtain a more expressive (yet more expensive)
parameterization by linearly combining such products by computing
z(k)
cout := T prod
ϕcout(x1, . . . , xℓ, y1, . . . , yℓ)(k) :=
ℓ
X
cin=1
Pϕcoutcin(xcin, ycin)(k),
(15)
which we call the fully-connected geometric product layer. Computational feasibility and experimental
verification should determine which parameterization is preferred.
Normalization and Nonlinearities
Since our layers involve quadratic and potentially higher-order
interaction terms, we need to ensure numerical stability. In order to do so, we use a normalization
operating on each multivector subspace before computing geometric products by putting
x(m) 7→
x(m)
σ(am)
 ¯q(x(m)) −1

+ 1,
(16)
where x(m) ∈Cl(m)(V, q). Here, σ denotes the logistic sigmoid function, and am ∈R is a learned
scalar. The denominator interpolates between 1 and the quadratic form ¯q
 x(m)
, normalizing the
magnitude of x(m). This ensures that the geometric products do not cause numerical instabilities
without losing information about the magnitude of x(m), where a learned scalar interpolates between
both regimes. Note that by Theorem 3.2, ¯q(x(m)) is invariant under the action of the Clifford group,
rendering Equation (16) an equivariant map.
Next, we use the layer-wide normalization scheme proposed by [RGdK+23], which, since it is also
based on the extended quadratic form, is also equivariant with respect to the twisted conjugation.
Regarding nonlinearities, we use a slightly adjusted version of the units proposed by [RGdK+23].
Since the scalar subspace Cl(0)(V, q) is always invariant with respect to the twisted conjugation,
we can apply x(m) 7→ReLU
 x(m)
when m = 0 and x(m) 7→σϕ
 ¯q
 x(m)
x(m) otherwise. We
can replace ReLU with any common scalar activation function. Here, σϕ represents a potentially
parameterized nonlinear function. Usually, however, we restrict it to be the sigmoid function. Since
we modify x(m) with an invariant scalar quantity, we retain equivariance. Such gating activations are
commonly used in the equivariance literature [WGW+18, GS22].
7In practice, we use fewer parameters due to the sparsity of the geometric product, implying that many
interactions will invariably be zero, thereby making their parameterization redundant.
7

103
104
O(3) Signed Volumes
10
1
10
3
10
5
10
7
103
104
O(5) Convex Hulls
0
10
20
30
102
103
104
O(5) Regression
10
4
10
2
100
102
E(n)-MLP
VN
GVP
MLP
MLP+Aug
EMLP SO(5)
EMLP O(5)
CGENN (Ours)
Figure 3: Left: Test mean-squared errors on the O(3) signed volume
task as functions of the number of training data. Note that due to
identical performance, some baselines are not clearly visible. Right:
same, but for the O(5) convex hull task.
Figure
4:
Test
mean-
squared-errors on the O(5)
regression task.
Embedding Data in the Clifford Algebra
In this work, we consider data only from the vector
space V or the scalars R, although generally one might also encounter, e.g., bivector data. That is,
we have some scalar features h1, . . . , hk ∈R and some vector features hk+1, . . . , hℓ∈V . Typical
examples of scalar features include properties like mass, charge, temperature, and so on. Additionally,
one-hot encoded categorical features are also included because {0, 1} ⊂R and they also transform
trivially. Vector features include positions, velocities, and the like. Then, using the identifications
Cl(0)(V, q) ∼= R and Cl(1)(V, q) ∼= V , we can embed the data into the scalar and vector subspaces of
the Clifford algebra to obtain Clifford features x1, . . . , xℓ∈Cl(V, q).
Similarly, we can predict scalar- or vector-valued data as output of our model by grade-projecting onto
the scalar or vector parts, respectively. We can then directly compare these quantities with ground-
truth vector- or scalar-valued data through a loss function and use standard automatic differentiation
techniques to optimize the model. Note that invariant predictions are obtained by predicting scalar
quantities.
5
Experiments
Here, we show that CGENNs excel across tasks, attaining top performance in several unique contexts.
Parameter budgets as well as training setups are kept as similar as possible to the baseline references.
All further experimental details can be found in the public code release.
5.1
Estimating Volumetric Quantities
O(3) Experiment: Signed Volumes
This task highlights the fact that equivariant architectures
based on scalarization are not able to extract some essential geometric properties from input data.
In a synthetic setting, we simulate a dataset consisting of random three-dimensional tetrahedra. A
main advantage of our method is that it can extract covariant quantities including (among others)
signed volumes, which we demonstrate in this task. Signed volumes are geometrically significant
because they capture the orientation of geometric objects in multidimensional spaces. For instance,
in computer graphics, they can determine whether a 3D object is facing towards or away from the
camera, enabling proper rendering. The input to the network is the point cloud and the loss function
is the mean-squared error between the signed volume and its true value. Note that we are predicting
a covariant (as opposed to invariant) scalar quantity (also known as a pseudoscalar) under O(3)
transformations using a positive-definite (Euclidean) metric. The results are displayed in the left
part of Figure 3. We compare against a standard multilayer perceptron (MLP), an MLP version of
the E(n)-GNN [SHW21] which uses neural networks to update positions with scalar multiplication,
Vector Neurons (VN) [DLD+21], and Geometric Vector Perceptrons (GVP) [JES+21]. We see that
the scalarization methods fail to access the features necessary for this task, as evidenced by their test
loss not improving even with more available data. The multilayer perceptron, although a universal
approximator, lacks the correct inductive biases. Our model, however, has the correct inductive biases
(e.g., the equivariance property) and can also access the signed volume. Note that we do not take
8

the permutation invariance of this task into account, as we are interested in comparing our standard
feed-forward architectures against similar baselines.
O(5) Experiment: Convex Hulls
We go a step further and consider a five-dimensional Euclidean
space, showcasing our model’s ability to generalize to high dimensions. We also make the experiment
more challenging by including more points and estimating the volume of the convex hull generated by
these points – a task that requires sophisticated algorithms in the classical case. Note that some points
may live inside the hull and do not contribute to the volume. We use the same network architectures
as before (but now embedded in a five-dimensional space) and present the results in Figure 3. We
report the error bars for CGENNs, representing three times the standard deviation of the results of
eight runs with varying seeds. Volume (unsigned) is an invariant quantity, enabling the baseline
methods to approximate its value. However, we still see that CGENNs outperform the other methods,
the only exception being the low-data regime of only 256 available data points. We attribute this to
our method being slightly more flexible, making it slightly more prone to overfitting. To mitigate this
issue, future work could explore regularization techniques or other methods to reduce overfitting in
low-data scenarios.
5.2
O(5) Experiment: Regression
We compare against the methods presented by [FWW21] who propose an O(5)-invariant regression
problem. The task is to estimate the function f(x1, x2) := sin(∥x1∥) −∥x2∥3/2 +
x⊤
1 x2
∥x1∥∥x2∥, where
the five-dimensional vectors x1, x2 are sampled from a standard Gaussian distribution in order to
simulate train, test, and validation datasets. The results are shown in Figure 4. We used baselines from
[FWW21] including an MLP, MLP with augmentation (MLP+Aug), and the O(5)- & SO(5)-MLP
architectures. We maintain the same number of parameters for all data regimes. For extremely small
datasets (30 and 100 samples), we observe some overfitting tendencies that can be countered with
regularization (e.g., weight decay) or using a smaller model. For higher data regimes (300 samples
and onward), CGENNs start to significantly outperform the baselines.
5.3
E(3) Experiment: n-Body System
Method
MSE (↓)
SE(3)-Tr.
0.0244
TFN
0.0155
NMP
0.0107
Radial Field
0.0104
EGNN
0.0070
SEGNN
0.0043
CGENN
0.0039 ± 0.0001
Table 1: Mean-squared error (MSE)
on the n-body system experiment.
The n-body experiment [KFW+18] serves as a benchmark
for assessing the performance of equivariant (graph) neural
networks in simulating physical systems [HRXH22]. In this
experiment, the dynamics of n = 5 charged particles in a
three-dimensional space are simulated. Given the initial posi-
tions and velocities of these particles, the task is to accurately
estimate their positions after 1 000 timesteps. To address this
challenge, we construct a graph neural network (GNN) using
the Clifford equivariant layers introduced in the previous sec-
tion. We use a standard message-passing algorithm [GSR+17]
where the message and update networks are CGENNs. So
long as the message aggregator is equivariant, the end-to-end
model also maintains equivariance. The input to the network
consists of the mean-subtracted positions of the particles (to achieve translation invariance) and
their velocities. The model’s output is the estimated displacement, which is to the input to achieve
translation-equivariant estimated target positions. We include the invariant charges as part of the
input and their products as edge attributes. We compare against the steerable SE(3)-Transformers
[FWFW20], Tensor Field Networks [TSK+18], and SEGNN [BHP+22]. Scalarization baselines
include Radial Field [KKN20] and EGNN [SHW21]. Finally, NMP [GSR+17] is not an E(3)-
equivariant method. The number of parameters in our model is maintained similar to the EGNN and
SEGNN baselines to ensure a fair comparison.
Results of our experiment are presented in Table 1, where we also present for CGENN three times
the standard deviation of three identical runs with different seeds. Our approach clearly outperforms
earlier methods and is significantly better than [BHP+22], thereby surpassing the baselines. This
experiment again demonstrates the advantage of leveraging covariant information in addition to scalar
quantities, as it allows for a more accurate representation of the underlying physics and leads to better
predictions.
9

Model
Accuracy (↑)
AUC (↑)
1/ϵB (↑)
(ϵS = 0.5)
1/ϵB (↑)
(ϵS = 0.3)
ResNeXt [XGD+17]
0.936
0.9837
302
1147
P-CNN [CMS17]
0.930
0.9803
201
759
PFN [KMT19]
0.932
0.9819
247
888
ParticleNet [QG20]
0.940
0.9858
397
1615
EGNN [SHW21]
0.922
0.9760
148
540
LGN [BAO+20]
0.929
0.9640
124
435
LorentzNet [GMZ+22]
0.942
0.9868
498
2195
CGENN
0.942
0.9869
500
2172
Table 2: Performance comparison between our proposed method and alternative algorithms on the
top tagging experiment. We present the accuracy, Area Under the Receiver Operating Characteristic
Curve (AUC), and background rejection 1/ϵB and at signal efficiencies of ϵS = 0.3 and ϵS = 0.5.
5.4
O(1, 3) Experiment: Top Tagging
Jet tagging in collider physics is a technique used to identify and categorize high-energy jets produced
in particle collisions, as measured by, e.g., CERN’s ATLAS detector [BTB+08]. By combining
information from various parts of the detector, it is possible to trace back these jets’ origins [KNS+19,
ATL17]. The current experiment seeks to tag jets arising from the heaviest particles of the standard
model: the ‘top quarks’ [IQWW09]. A jet tag should be invariant with respect to the global reference
frame, which can transform under Lorentz boosts due to the relativistic nature of the particles. A
Lorentz boost is a transformation that relates the space and time coordinates of an event as seen
from two inertial reference frames. The defining characteristic of these transformations is that they
preserve the Minkowski metric, which is given by γ(ct, x, y, z) := (ct)2 −x2 −y2 −z2. Note the
difference with the standard positive definite Euclidean metric, as used in the previous experiments.
The set of all such transformations is captured by the orthogonal group O(1, 3); therefore, our method
is fully compatible with modeling this problem.
We evaluate our model on a top tagging benchmark published by [KPTR19]. It contains 1.2M training
entries, 400k validation entries, and 400k testing entries. For each jet, the energy-momentum 4-vectors
are available for up to 200 constituent particles, making this a much larger-scale experiment than the
ones presented earlier. Again, we employ a standard message passing graph neural network [GSR+17]
using CGENNs as message and update networks. The baselines include ResNeXt [XGD+17], P-
CNN [CMS17], PFN [KMT19], ParticleNet [QG20], LGN [BAO+20], EGNN [SHW21], and the
more recent LorentzNet [GMZ+22]. Among these, LGN is a steerable method, whereas EGNN
and LorentzNet are scalarization methods. The other methods are not Lorentz-equivariant. Among
the performance metrics, there are classification accuracy, Area Under the Receiver Operating
Characteristic Curve (AUC), and the background rejection rate 1/ϵB at signal efficiencies of ϵS = 0.3
and ϵS = 0.5, where ϵB and ϵS are the false positive and true positive rates, respectively. We observe
that LorentzNet, a method that uses invariant quantities, is an extremely competitive baseline that was
optimized for this task. Despite this, CGENNs are able to match its performance while maintaining
the same core implementation.
6
Conclusion
We presented a novel approach for constructing O(n)- and E(n)-equivariant neural networks based on
Clifford algebras. After establishing the required theoretical results, we proposed parameterizations of
nonlinear multivector-valued maps that exhibit versatility and applicability across scenarios varying
in dimension. This was achieved by the core insight that polynomials in multivectors are O(n)-
equivariant functions. Theoretical results were empirically substantiated in three distinct experiments,
outperforming or matching baselines that were sometimes specifically designed for these tasks.
CGENNs induce a (non-prohibitive) degree of computational overhead similar to other steerable
methods. On the plus side, we believe that improved code implementations such as custom GPU
kernels or alternative parameterizations to the current ones can significantly alleviate this issue,
potentially also resulting in improved performances on benchmark datasets. This work provides solid
theoretical and experimental foundations for such developments.
10

References
[AGB22]
Simon Axelrod and Rafael Gómez-Bombarelli, Geom, energy-annotated molecular
conformations for property prediction and molecular generation, Scientific Data,
Springer Science and Business Media LLC, 2022.
[AHK19]
Brandon M. Anderson, Truong-Son Hy, and Risi Kondor, Cormorant: Covariant
Molecular Neural Networks., Conference on Neural Information Processing Systems
(NeurIPS), 2019, pp. 14510–14519.
[AKHvD11]
Arne Alex, Matthias Kalus, Alan Huckleberry, and Jan von Delft, A numerical algo-
rithm for the explicit calculation of su (n) and sl (n, c) clebsch–gordan coefficients,
Journal of Mathematical Physics 52 (2011), no. 2, 023507.
[Art57]
Emil Artin, Geometric Algebra, Interscience Tracts in Pure and Applied Mathematics,
no. 3, Interscience Publishers, Inc., 1957.
[ATL17]
ATLAS, Jet energy scale measurements and their systematic uncertainties in
proton-proton collisions at √s = 13 TeV with the ATLAS detector, arXiv preprint
arXiv:1703.09665, 2017.
[BAO+20]
Alexander Bogatskiy, Brandon M. Anderson, Jan T. Offermann, Marwah Roussi,
David W. Miller, and Risi Kondor, Lorentz Group Equivariant Neural Network
for Particle Physics, International Conference on Machine Learning (ICML), 2020,
pp. 992–1002.
[BBCV21]
M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velivckovi’c, Geometric Deep
Learning: Grids, Groups, Graphs, Geodesics, and Gauges, arXiv, 2021.
[BBWG22]
Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta,
Clifford neural layers for pde modeling, arXiv preprint arXiv:2209.04934, 2022.
[BCRLZE06]
Eduardo Bayro-Corrochano, Leo Reyes-Lozano, and Julio Zamora-Esquivel, Con-
formal geometric algebra for robotic vision, Journal of Mathematical Imaging and
Vision 24 (2006), 55–81.
[BDHBC23]
Johann Brehmer, Pim De Haan, Sönke Behrends, and Taco Cohen, Geometric algebra
transformers, arXiv preprint arXiv:2305.18415, 2023.
[BEdL+16]
Henri Bal, Dick Epema, Cees de Laat, Rob van Nieuwpoort, John Romein, Frank
Seinstra, Cees Snoek, and Harry Wijshoff, A medium-scale distributed system for
computer science research: Infrastructure for the long term, IEEE Computer 49
(2016), no. 5, 54–63.
[Bek19]
Erik J Bekkers, B-spline cnns on lie groups, arXiv preprint arXiv:1909.12057, 2019.
[BHP+22]
Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J. Bekkers, and Max
Welling, Geometric and Physical Quantities improve E(3) Equivariant Message
Passing., International Conference on Learning Representations (ICLR), 2022.
[Bie20]
Lukas Biewald, Experiment tracking with weights and biases, 2020, Software avail-
able from wandb.com.
[BMQQS+21] Uzair Aslam Bhatti, Zhou Ming-Quan, Huo Qing-Song, Sajid Ali, Aamir Hussain,
Yan Yuhuan, Zhaoyuan Yu, Linwang Yuan, and Saqib Ali Nawaz, Advanced color
edge detection using clifford algebra in satellite images, IEEE Photonics Journal 13
(2021), no. 2, 1–20.
[BMS+22]
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa,
Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris Kozinsky, E(3)-
equivariant graph neural networks for data-efficient and accurate interatomic poten-
tials, Nature Communications, 2022.
[BTB+08]
JM Butterworth, J Thion, U Bratzler, PN Ratoff, RB Nickerson, JM Seixas,
I Grabowska-Bold, F Meisel, S Lokwitz, et al., The atlas experiment at the cern large
hadron collider, Jinst 3 (2008), S08003.
[BTH22]
Stephane Breuils, Kanta Tachibana, and Eckhard Hitzer, New Applications of Clif-
ford’s Geometric Algebra, Advances in Applied Clifford Algebras, Springer Science
and Business Media LLC, 2022.
11

[CC12]
John Stephen Roy Chisholm and Alan K Common, Clifford algebras and their
applications in mathematical physics, vol. 183, Springer Science & Business Media,
2012.
[CCG18]
Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger, Spherenet: Learn-
ing Spherical Representations for Detection and Classification in Omnidirectional
Images., European Conference on Computer Vision (ECCV), Springer International
Publishing, 2018, pp. 525–541.
[CGKW18]
Taco S. Cohen, Mario Geiger, Jonas Köhler, and Max Welling, Spherical CNNs.,
International Conference on Learning Representations (ICLR), 2018.
[CLW22]
Gabriele Cesa, Leon Lang, and Maurice Weiler, A Program to Build E(N)-Equivariant
Steerable CNNs., International Conference on Learning Representations (ICLR),
2022.
[CMS17]
CMS, Boosted jet identification using particle candidates and deep neural networks,
Detector Performance Figures: CMS-DP-17-049, 2017.
[Cru80]
Albert Crumeyrolle, Algèbres de clifford dégénérées et revêtements des groupes con-
formes affines orthogonaux et symplectiques, Annales De L Institut Henri Poincare-
physique Theorique 33 (1980), 235–249.
[Cru90]
Albert Crumeyrolle, Orthogonal and Symplectic Clifford Algebras: Spinor Structures,
vol. 57, Springer Science & Business Media, 1990.
[CTS+17]
Stefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda, Igor Poltavsky, Kristof T.
Schütt, and Klaus-Robert Müller, Machine learning of accurate energy-conserving
molecular force fields, Science Advances, American Association for the Advance-
ment of Science (AAAS), 2017.
[CW16]
Taco Cohen and Max Welling, Group Equivariant Convolutional Networks., Interna-
tional Conference on Machine Learning (ICML), 2016, pp. 2990–2999.
[DFM09]
Leo Dorst, Daniel Fontijne, and Stephen Mann, Geometric algebra for computer sci-
ence (revised edition): An object-oriented approach to geometry, Morgan Kaufmann,
2009.
[DKL10]
Tekin Dereli, ¸Sahin Koçak, and Murat Limoncu, Degenerate Spin Groups as Semi-
Direct Products, Advances in Applied Clifford Algebras 20 (2010), no. 3-4, 565–573.
[DLD+21]
Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and
Leonidas J. Guibas, Vector Neurons: A General Framework for SO(3)-Equivariant
Networks., IEEE International Conference on Computer Vision (ICCV), IEEE, 2021,
pp. 12180–12189.
[DM02]
Leo Dorst and Stephen Mann, Geometric algebra: a computational framework for
geometrical applications, IEEE Computer Graphics and Applications 22 (2002),
no. 3, 24–31.
[Est20]
Carlos Esteves, Theoretical aspects of group equivariant neural networks, arXiv,
2020.
[FA+91]
William T Freeman, Edward H Adelson, et al., The design and use of steerable filters,
IEEE Transactions on Pattern analysis and machine intelligence 13 (1991), no. 9,
891–906.
[FSIW20]
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson, General-
izing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary
Continuous Data., International Conference on Machine Learning (ICML), 2020,
pp. 3165–3176.
[FWFW20]
Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling, Se(3)-
Transformers: 3d Roto-Translation Equivariant Attention Networks., Conference on
Neural Information Processing Systems (NeurIPS), 2020.
[FWW21]
Marc Finzi, Max Welling, and Andrew Gordon Wilson, A practical method for
constructing equivariant multilayer perceptrons for arbitrary matrix groups, Interna-
tional Conference on Machine Learning, PMLR, 2021, pp. 3318–3328.
12

[GBG21]
Johannes Gasteiger, Florian Becker, and Stephan Günnemann, Gemnet: Universal Di-
rectional Graph Neural Networks for Molecules., Conference on Neural Information
Processing Systems (NeurIPS), 2021, pp. 6790–6802.
[GLB+16]
Richard Gowers, Max Linke, Jonathan Barnoud, Tyler Reddy, Manuel Melo, Sean
Seyler, Jan Doma´nski, David Dotson, Sébastien Buchoux, Ian Kenney, and Oliver
Beckstein, Mdanalysis: A Python Package for the Rapid Analysis of Molecular
Dynamics Simulations, Proceedings of the Python in Science Conference, SciPy,
2016.
[GMZ+22]
Shiqi Gong, Qi Meng, Jue Zhang, Huilin Qu, Congqiao Li, Sitian Qian, Weitao
Du, Zhi-Ming Ma, and Tie-Yan Liu, An efficient Lorentz equivariant graph neural
network for jet tagging, Journal of High Energy Physics, Springer Science and
Business Media LLC, 2022.
[Gra62]
Hermann Grassmann, Die ausdehnungslehre, vol. 1, Enslin, 1862.
[GS22]
Mario Geiger and Tess Smidt, e3nn: Euclidean neural networks, 2022.
[GSR+17]
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E
Dahl, Neural message passing for quantum chemistry, International conference on
machine learning, PMLR, 2017, pp. 1263–1272.
[Ham66]
William Rowan Hamilton, Elements of quaternions, Longmans, Green, & Company,
1866.
[HHR+22]
Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang,
Equivariant Graph Mechanics Networks with Constraints., International Conference
on Learning Representations (ICLR), 2022.
[Hit12]
Eckhard Hitzer, The clifford fourier transform in real clifford algebras, 19th Inter-
national Conference on the Application of Computer Science and Mathematics in
Architecture and Civil Engineering, 2012.
[HP10]
Eckhard Hitzer and Christian Perwass, Interactive 3d space group visualization with
clucalc and the clifford geometric algebra description of space groups, Advances in
applied Clifford algebras 20 (2010), 631–658.
[HRXH22]
Jiaqi Han, Yu Rong, Tingyang Xu, and Wenbing Huang, Geometrically equivariant
graph neural networks: A survey, arXiv, 2022.
[Hun07]
John D Hunter, Matplotlib: A 2d graphics environment, Computing in science &
engineering 9 (2007), no. 03, 90–95.
[HZBC08]
Dietmar Hildenbrand, Julio Zamora, and Eduardo Bayro-Corrochano, Inverse kine-
matics computation in computer graphics and robotics using conformal geometric
algebra, Advances in applied Clifford algebras 18 (2008), 699–713.
[IQWW09]
Joseph R Incandela, Arnulf Quadt, Wolfgang Wagner, and Daniel Wicke, Status and
prospects of top-quark physics, Progress in Particle and Nuclear Physics 63 (2009),
no. 2, 239–292.
[JES+21]
Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend,
and Ron O. Dror, Learning from Protein Structure with Geometric Vector Percep-
trons., International Conference on Learning Representations (ICLR), 2021.
[KFW+18]
Thomas N. Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard S.
Zemel, Neural Relational Inference for Interacting Systems., International Conference
on Machine Learning (ICML), 2018, pp. 2693–2702.
[KGG20]
Johannes Klicpera, Janek Groß, and Stephan Günnemann, Directional Message Pass-
ing for Molecular Graphs., International Conference on Learning Representations
(ICLR), 2020.
[KIdHN23]
Jonas Köhler, Michele Invernizzi, Pim de Haan, and Frank Noé, Rigid body flows for
sampling molecular crystal structures, arXiv preprint arXiv:2301.11355, 2023.
[KKN20]
Jonas Köhler, Leon Klein, and Frank Noé, Equivariant Flows: Exact Likelihood
Generative Learning for Symmetric Densities., International Conference on Machine
Learning (ICML), 2020, pp. 5361–5370.
13

[KMT19]
Patrick T. Komiske, Eric M. Metodiev, and Jesse Thaler, Energy flow networks: deep
sets for particle jets, Journal of High Energy Physics, Springer Science and Business
Media LLC, 2019.
[KNS+19]
Roman Kogler, Benjamin Nachman, Alexander Schmidt, Lily Asquith, Emma
Winkels, Mario Campanelli, Chris Delitzsch, Philip Harris, Andreas Hinzmann,
Deepak Kar, et al., Jet substructure at the large hadron collider, Reviews of Modern
Physics 91 (2019), no. 4, 045003.
[KPTR19]
Gregor Kasieczka, Tilman Plehn, Jennifer Thompson, and Michael Russel, Top quark
tagging reference dataset, March 2019.
[KR67]
LM Kaplan and M Resnikoff, Matrix products and the explicit 3, 6, 9, and 12-j
coefficients of the regular representation of su (n), Journal of Mathematical Physics 8
(1967), no. 11, 2194–2205.
[KT18]
Risi Kondor and Shubhendu Trivedi, On the Generalization of Equivariance and
Convolution in Neural Networks to the Action of Compact Groups., International
Conference on Machine Learning (ICML), 2018, pp. 2752–2760.
[Len90]
Reiner Lenz, Group theoretical methods in image processing, vol. 413, Springer,
1990.
[LK14]
Sergey Levine and Vladlen Koltun, Learning complex neural network policies with
trajectory optimization, International Conference on Machine Learning, PMLR, 2014,
pp. 829–837.
[LS09]
Douglas Lundholm and Lars Svensson, Clifford Algebra, Geometric Algebra, and
Applications, arXiv preprint arXiv:0907.5356, 2009.
[MFW21]
Pavlo Melnyk, Michael Felsberg, and Mårten Wadenbäck, Embed me if you can: A
geometric perceptron, Proceedings of the IEEE/CVF International Conference on
Computer Vision, 2021, pp. 1276–1284.
[PGM+19]
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al., Pytorch:
An imperative style, high-performance deep learning library, Advances in neural
information processing systems, vol. 32, 2019.
[PRM+18]
Titouan Parcollet, Mirco Ravanelli, Mohamed Morchid, Georges Linarès, Chiheb
Trabelsi, Renato De Mori, and Yoshua Bengio, Quaternion recurrent neural networks,
arXiv preprint arXiv:1806.04418, 2018.
[PVG+11]
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand
Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent
Dubourg, et al., Scikit-learn: Machine learning in python, the Journal of machine
Learning research 12 (2011), 2825–2830.
[QG20]
Huilin Qu and Loukas Gouskos, Jet tagging via particle clouds, Physical Review D,
American Physical Society (APS), 2020.
[RDK21]
Martin Roelfs and Steven De Keninck, Graded symmetry groups: plane and simple,
arXiv preprint arXiv:2107.03771, 2021.
[RDRL14]
Raghunathan Ramakrishnan, Pavlo O. Dral, Matthias Rupp, and O. Anatole von
Lilienfeld, Quantum chemistry structures and properties of 134 kilo molecules,
Scientific Data, Springer Science and Business Media LLC, 2014.
[RGdK+23]
David Ruhe, Jayesh K Gupta, Steven de Keninck, Max Welling, and Johannes
Brandstetter, Geometric clifford algebra networks, arXiv preprint arXiv:2302.06594,
2023.
[Ser12]
Jean-Pierre Serre, A Course in Arithmetic, vol. 7, Springer Science & Business Media,
2012.
[SGT+08]
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele
Monfardini, The graph neural network model, IEEE transactions on neural networks
20 (2008), no. 1, 61–80.
[SHW21]
Victor Garcia Satorras, E. Hoogeboom, and M. Welling, E(n) Equivariant Graph
Neural Networks, International Conference on Machine Learning, 2021.
14

[Spe21]
Matthew Spellings, Geometric algebra attention networks for small point clouds,
arXiv preprint arXiv:2110.02393, 2021.
[SSK+18]
K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Müller,
Schnet – A deep learning architecture for molecules and materials, The Journal of
Chemical Physics 148 (2018), no. 24, 241722.
[SUG21]
Kristof Schütt, Oliver T. Unke, and Michael Gastegger, Equivariant message pass-
ing for the prediction of tensorial properties and molecular spectra., International
Conference on Machine Learning (ICML), 2021, pp. 9377–9388.
[Syl52]
James Joseph Sylvester, XIX. A demonstration of the theorem that every homoge-
neous quadratic polynomial is reducible by real orthogonal substitutions to the form
of a sum of positive and negative squares, The London, Edinburgh, and Dublin
Philosophical Magazine and Journal of Science 4 (1852), no. 23, 138–142.
[TF22]
Philipp Thölke and G. D. Fabritiis, Torch MD-NET: Equivariant Transformers for
Neural Network based Molecular Potentials, ArXiv, vol. abs/2202.02541, 2022.
[TSK+18]
Nathaniel Thomas, T. Smidt, S. Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and
Patrick F. Riley, Tensor Field Networks: Rotation- and Translation-Equivariant
Neural Networks for 3d Point Clouds, arXiv, 2018.
[TVS+21]
Raphael J. L. Townshend, Martin Vögele, Patricia Suriana, Alexander Derry, Alexan-
der Powers, Yianni Laloudakis, Sidhika Balachandar, Bowen Jing, Brandon M. An-
derson, Stephan Eismann, Risi Kondor, Russ B. Altman, and Ron O. Dror, Atom3d:
Tasks on Molecules in Three Dimensions., Conference on Neural Information Pro-
cessing Systems (NeurIPS), 2021.
[UPH+19]
Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and
Sai-Kit Yeung, Revisiting Point Cloud Classification: A New Benchmark Dataset
and Classification Model on Real-World Data., IEEE International Conference on
Computer Vision (ICCV), 2019, pp. 1588–1597.
[VDWCV11]
Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux, The numpy array: a
structure for efficient numerical computation, Computing in science & engineering
13 (2011), no. 2, 22–30.
[VGO+20]
Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy,
David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
Bright, et al., Scipy 1.0: fundamental algorithms for scientific computing in python,
Nature methods 17 (2020), no. 3, 261–272.
[Was21]
Michael L Waskom, Seaborn: statistical data visualization, Journal of Open Source
Software 6 (2021), no. 60, 3021.
[WC19]
Maurice Weiler and Gabriele Cesa, General E(2)-Equivariant Steerable CNNs.,
Conference on Neural Information Processing Systems (NeurIPS), 2019, pp. 14334–
14345.
[WCL05]
Rich Wareham, Jonathan Cameron, and Joan Lasenby, Applications of conformal
geometric algebra in computer vision and graphics, Computer Algebra and Geomet-
ric Algebra with Applications: 6th International Workshop, IWMM 2004, Shanghai,
China, May 19-21, 2004 and International Workshop, GIAE 2004, Xian, China, May
24-28, 2004, Revised Selected Papers, Springer, 2005, pp. 329–349.
[WFVW21]
Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling, Coordinate inde-
pendent convolutional networks–isometry and gauge equivariant convolutions on
riemannian manifolds, arXiv preprint arXiv:2106.06020, 2021.
[WGTB17]
Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Bros-
tow, Harmonic Networks: Deep Translation and Rotation Equivariance., Computer
Vision and Pattern Recognition (CVPR), 2017, pp. 7168–7177.
[WGW+18]
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen, 3d
Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data.,
Conference on Neural Information Processing Systems (NeurIPS), 2018, pp. 10402–
10413.
15

[WSK+15]
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang,
and Jianxiong Xiao, 3d ShapeNets: A deep representation for volumetric shapes.,
Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1912–1920.
[XGD+17]
Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He, Aggre-
gated Residual Transformations for Deep Neural Networks., Computer Vision and
Pattern Recognition (CVPR), 2017, pp. 5987–5995.
[YCo20]
YCor, Analogue of the special orthogonal group for singular quadratic forms, Math-
Overflow, 2020, url:https://mathoverflow.net/q/378828 (version: 2020-12-13).
[ZCD+20]
C Lawrence Zitnick, Lowik Chanussot, Abhishek Das, Siddharth Goyal, Javier Heras-
Domingo, Caleb Ho, Weihua Hu, Thibaut Lavril, Aini Palizhati, Morgane Riviere,
and others, An introduction to electrocatalyst design using machine learning for
renewable energy storage, arXiv, 2020.
[ZKR+17]
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R
Salakhutdinov, and Alexander J Smola, Deep sets, Advances in neural information
processing systems, vol. 30, 2017.
[ZXXC18]
Xuanyu Zhu, Yi Xu, Hongteng Xu, and Changjian Chen, Quaternion convolutional
neural networks, Proceedings of the European Conference on Computer Vision
(ECCV), 2018, pp. 631–647.
16

Acknowledgments
We thank Leo Dorst and Steven de Keninck for insightful discussions about Clifford (geometric)
algebras and their applications as well as pointing us to the relevant literature. Further, we thank
Robert-Jan Schlimbach and Jayesh Gupta for their help with computational infrastructure and scaling
up the experiments. This work used the Dutch national e-infrastructure with the support of the SURF
Cooperative using grant no. EINF-5757 as well as the DAS-5 and DAS-6 clusters [BEdL+16].
Broader Impact Statement
The advantages of more effective equivariant graph neural networks could be vast, particularly in the
physical sciences. Improved equivariant neural networks could revolutionize fields like molecular
biology, astrophysics, and materials science by accurately predicting system behaviors under various
transformations. They could further enable more precise simulations of physical systems. Such new
knowledge could drive forward scientific discovery, enabling innovations in healthcare, materials
engineering, and our understanding of the universe. However, a significant drawback of relying
heavily on advanced equivariant neural networks in physical sciences is the potential for unchecked
errors or biases to propagate when these systems are not thoroughly cross-validated using various
methods. This could potentially lead to flawed scientific theories or dangerous real-world applications.
Reproducibility Statement
We have released the code to reproduce the experimental results, including hyperparameters, network
architectures, and so on, at https://github.com/DavidRuhe/clifford-group-equivariant-neural-networks.
Computational Resources
The volumetric quantities and regression experiments were carried out on 1×11 GB NVIDIA GeForce
GTX 1080 Ti and 1 × 11 GB NVIDIA GeForce GTX 2080 Ti instances. The n-body experiment ran
on 1 × 24 GB NVIDIA GeForce RTX 3090 and 1 × 24 GB NVIDIA RTX A5000 nodes. Finally, the
top tagging experiment was conducted on 4 × 40 GB NVIDIA Tesla A100 Ampere instances.
Licenses
We would like to thank the scientific software development community, without whom this work
would not be possible.
This work made use of Python (PSF License Agreement), NumPy [VDWCV11] (BSD-3 Clause),
PyTorch [PGM+19] (BSD-3 Clause), CUDA (proprietary license), Weights and Biases [Bie20] (MIT),
Scikit-Learn [PVG+11] (BSD-3 Clause), Seaborn [Was21] (BSD-3 Clause), Matplotlib [Hun07]
(PSF), [VGO+20] (BSD-3).
The top tagging dataset is licensed under CC-By Attribution 4.0 International.
17

Contents
1
Introduction
1
2
The Clifford Algebra
3
3
Theoretical Results
3
3.1
The Clifford Group and its Clifford Algebra Representations . . . . . . . . . . . . . . . . . .
4
3.2
Orthogonal Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
4
Methodology
6
5
Experiments
8
5.1
Estimating Volumetric Quantities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
5.2
O(5) Experiment: Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
5.3
E(3) Experiment: n-Body System
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
5.4
O(1, 3) Experiment: Top Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
6
Conclusion
10
References
11
Acknowledgements
17
Broader Impact Statement
17
Reproducibility Statement
17
Computational Resources
17
Licences
17
A Glossary
20
B
Supplementary Material: Introduction
22
C Quadratic Vector Spaces and the Orthogonal Group
23
D The Clifford Algebra and Typical Constructions
26
D.1 The Clifford Algebra and its Universal Property
. . . . . . . . . . . . . . . . . . . . . . . .
26
D.2 The Multivector Filtration and the Grade . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
D.3 The Parity Grading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
D.4 The Dimension of the Clifford Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
D.5
Extending the Quadratic Form to the Clifford Algebra
. . . . . . . . . . . . . . . . . . . . .
32
D.6 The Multivector Grading
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
D.7 The Radical Subalgebra of the Clifford Algebra . . . . . . . . . . . . . . . . . . . . . . . . .
44
D.8 The Center of the Clifford Algebra
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
D.9 The Twisted Center of the Clifford Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
18

E The Clifford Group and its Clifford Algebra Representations
49
E.1
Adjusting the Twisted Conjugation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
E.2
The Clifford Group
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
E.3
The Structure of the Clifford Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
E.4
Orthogonal Representations of the Clifford Group
. . . . . . . . . . . . . . . . . . . . . . .
60
E.5
The Spinor Norm and the Clifford Norm
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
E.6
The Pin Group and the Spin Group
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
19

A
Glossary
Notation
Meaning
O(n)
The orthogonal group acting on an n-dimensional vector space.
SO(n)
The special orthogonal group acting on an n-dimensional vector space.
E(n)
The Euclidean group acting on an n-dimensional vector space.
SE(n)
The special Euclidean group acting on an n-dimensional vector space.
GL(n)
The general linear group acting on an n-dimensional vector space.
O(V, q)
The orthogonal group of vector space V with respect to a quadratic form q.
OR(V, q)
The orthogonal group of vector space V with respect to a quadratic form q that
acts as the identity on R.
V
A vector space.
R
The radical vector subspace of V such that for any f ∈R, b(f, v) = 0 for all
v ∈V .
F
A field.
R
The real numbers.
N
The natural numbers.
Z
The integers.
[n]
The set of integers {1, . . . , n}.
q
A quadratic form, q : V →F.
b
A bilinear form, b : V × V →F.
Cl(V, q)
The Clifford algebra over a vector space V with quadratic form q.
Cl×(V, q)
The group of invertible Clifford algebra elements.
Cl[×](V, q)
The group of invertible parity-homogeneous Clifford algebra elements.
Cl[×](V, q) := {w ∈Cl×(V, q) | η(w) ∈{±1}}.
Cl[0](V, q)
The even subalgebra of the Clifford algebra. Cl[0](V, q) := Ln
m even Cl(m)(V, q).
Cl[1](V, q)
The odd part of the Clifford algebra. Cl[1](V, q) := Ln
m odd Cl(m)(V, q).
Cl(m)(V, q)
The grade-m subspace of the Clifford algebra.
(_)(m)
Grade projection, (_)(m) : Cl(V, q) →Cl(m)(V, q).
ζ
Projection onto the zero grade, ζ : Cl(V, q) →Cl(0)(V, q).
ei
A basis vector, ei ∈V .
fi
A basis vector of R.
eA
A Clifford basis element (product of basis vectors) with eA ∈Cl(V, q), A ⊆
{1, . . . , n}.
¯b
The extended bilinear form on the Clifford algebra, ¯b : Cl(V, q) × Cl(V, q) →F.
¯q
The extended quadratic form on the Clifford algebra, ¯q : Cl(V, q) →F.
α
Clifford main involution α : Cl(V, q) →Cl(V, q).
β
Main Clifford anti-involution, also known as reversion, β : Cl(V, q) →Cl(V, q).
γ
Clifford conjugation γ : Cl(V, q) →Cl(V, q).
η
Coboundary of α. For w ∈Cl×(V, q), η(w) ∈{±1} if and only if w is parity-
homogeneous.
ρ(w)
The (adjusted) twisted conjugation, used as the action of the Clifford group.
ρ(w) : Cl(V, q) →Cl(V, q), w ∈Cl×(V, q).
Γ(V, q)
The Clifford group of Cl(V, q).
20

Notation
Meaning
Γ[0](V, q)
The special Clifford group. It excludes orientation-reversing (odd) elements.
Γ[0](V, q) := Γ(V, q) ∩Cl[0](V, q).
V(R)
The radical subalgebra of Cl(V, q). I.e., those elements that have zero ¯q. It is
equal to the exterior algebra of R.
V[i](R)
The even or odd (i ∈{0, 1}) subalgebra of V(R).
V[i](R) := V(R) ∩
Cl[i](V, q).
V(≥1)(R)
The subalgebra of V(R) with grade greater than or equal to one. V(≥1)(R) :=
span{f1 . . . fk | k ≥1, fl ∈R}.
V×(R)
The group of invertible elements of V(R). V×(R) = F× + V(≥1)(R).
V[×](R)
The group of invertible elements of V(R) with even grades. V[×](R) := F× +
span{f1 . . . fk | k ≥2 even, fl ∈R}.
V∗(R)
Same as V×(R), but with F× set to 1.
V[∗](R)
Same as V[×](R), but with F× set to 1.
SN
Spinor norm, SN : Cl(V, q) →F.
CN
Clifford norm, CN : Cl(V, q) →F.
21

B
Supplementary Material: Introduction
Existing literature on Clifford algebra uses varying notations, conventions, and focuses on several
different applications. We gathered previous definitions and included independently derived results to
achieve the desired outcomes for this work and to provide proofs for the most general cases, including,
e.g., potential degeneracy of the metric. As such, this appendix acts as a comprehensive resource
on quadratic spaces, orthogonal groups, Clifford algebras, their constructions, and specific groups
represented within Clifford algebras.
We start in Appendix C with a primer to quadratic spaces and the orthogonal group. In particular,
we specify how the orthogonal group is defined on a quadratic space, and investigate its action.
We pay special attention to the case of spaces with degenerate quadratic forms, and what we call
“radical-preserving” orthogonal automorphisms. The section concludes with the presentation of the
Cartan-Dieudonné theorem.
In Appendix D, we introduce the definition of the Clifford algebra as a quotient of the tensor algebra.
We investigate several key properties of the Clifford algebra. Then, we introduce its parity grading,
which in turn is used to prove that the dimension of the algebra is 2n, where n is the dimension of the
vector space on which it is defined. This allows us to construct an algebra basis. Subsequently, we
extend the quadratic and bilinear forms of the vector space to their Clifford algebra counterparts. This
then leads to the construction of an orthogonal basis for the algebra. Furthermore, the multivector
grading of the algebra is shown to be basis independent, leading to the orthogonal sum decomposition
into the usual subspaces commonly referred to as scalars, vectors, bivectors, and so on. Finally, we
investigate some additional properties of the algebra, such as its center, its radical subalgebra, and its
twisted center.
In Appendix E we introduce, motivate, and adjust the twisted conjugation map. We show that
it comprises an algebra automorphism, thereby respecting the algebra’s vector space and also its
multiplicative properties. Moreover, it can serve as a representation of the Clifford algebra’s group of
invertible elements. We then identify what we call the Clifford group, whose action under the twisted
conjugation respects also the multivector decomposition. We investigate properties of the Clifford
group and its action. Specifically, we show that its action yields a radical-preserving orthogonal
automorphism. Moreover, it acts orthogonally on each individual subspace.
Finally, after introducing the Spinor and Clifford norm, the section concludes with the pursuit of a
general definition for the Pin and Spin group, which are used in practice more often than the Clifford
group. This, however, turns out to be somewhat problematic when generalizing to fields beyond R.
We motivate several choices, and outline their (dis)advantages. For R, we finally settle on definitions
that are compatible with existing literature.
22

C
Quadratic Vector Spaces and the Orthogonal Group
We provide a short introduction to quadratic spaces and define the orthogonal group of a (non-definite)
quadratic space. We use these definitions in our analysis of how the Clifford group relates to the
orthogonal group.
We will always denote with F a field of a characteristic different from 2, char(F) ̸= 2. Sometimes
we will specialize to the real numbers F = R. Let V be a vector space over F of finite dimension
dimF V = n. We will follow [Ser12].
Definition C.1 (Quadratic forms and quadratic vector spaces). A map q : V →F will be called a
quadratic form of V if for all c ∈F and v ∈V :
q(c · v) = c2 · q(v),
(17)
and if:
b(v1, v2) := 1
2 (q(v1 + v2) −q(v1) −q(v2)) ,
(18)
is a bilinear form over F in v1, v2 ∈V , i.e., it is separately F-linear in each of the arguments v1 and
v2 when the other one is fixed.
The tuple (V, q) will then be called a quadratic (vector) space.
Remark C.2.
1. Note that we explicitely do not make assumptions about the non-degeneracy
of b. Even the extreme case with constant q = 0 is allowed and of interest.
2. Further note, that b will automatically be a symmetric bilinear form.
Definition C.3 (The radical subspace). Now consider the quadratic space (V, q). We then call the
subspace:
R := {f ∈V | ∀v ∈V. b(f, v) = 0} ,
(19)
the radical subspace of (V, q).
Remark C.4.
1. The radical subspace of (V, q) is the biggest subspace of V where q is
degenerate. Note that this space is orthogonal to all other subspaces of V .
2. If W is any complementary subspace of R in V , so that V = R ⊕⊥W, then q restricted to
W is non-degenerate.
Definition C.5 (Orthogonal basis). A basis e1, . . . , en of V is called orthogonal basis of V if for all
i ̸= j we have:
b(ei, ej) = 0.
(20)
It is called an orthonormal basis if, in addition, q(ei) ∈{−1, 0, +1} for all i = 1, . . . , n.
Remark C.6. Note that every quadratic space (V, q) has an orthogonal basis by [Ser12] p. 30 Thm.
1, but not necessarily a orthonormal basis. However, if F = R then (V, q) has an orthonormal basis
by Sylvester’s law of inertia, see [Syl52].
Definition C.7 (The orthogonal group). For a quadratic space (V, q) we define the orthogonal group
of (V, q) as follows:
O(q) := O(V, q) :=

Φ : V →V
 Φ F-linear automorphism8, s.t. ∀v ∈V. q(Φ(v)) = q(v)
	
.
(21)
If (V, q) = R(p,q,r), i.e. if F = R and V = Rn and q has the signature (p, q, r) with p + q + r = n
then we define the group of orthogonal matrices of signature (p, q, r) as follows:
O(p, q, r) :=

O ∈GL(n)
 O⊤∆(p,q,r)O = ∆(p,q,r)
	
,
(22)
where we used the (n × n)-diagonal signature matrix:
∆(p,q,r) := diag(+1, . . . , +1
|
{z
}
p-times
, −1, . . . , −1
|
{z
}
q-times
, 0, . . . , 0
| {z }
r-times
).
(23)
23

Theorem C.8 (See [YCo20]). Let (V, q) be a finite dimensional quadratic space over a field F with
char(F) ̸= 2. Let R ⊆V be the radical subspace, r := dim R, and, W ⊆V a complementary
subspace, m := dim W. Then we get an isomorphism:
O(V, q) ∼=

O(W, q|W )
0m×r
M(r, m)
GL(r)

=

O
0m×r
M
G
  O ∈O(W, q|W ), M ∈M(r, m), G ∈GL(r)

,
(24)
where M(r, m) := Fr×m is the additive group of all (r × m)-matrices with coefficients in F and
where GL(r) is the multiplicative group of all invertible (r × r)-matrices with coefficients in F.
Proof. Let e1, . . . , em be an orthogonal basis for W and f1, . . . , fr be a basis for R, then the
associated bilinear form b of q has the following matrix representation:

Q
0
0
0

,
(25)
with an invertible diagonal (m × m)-matrix Q. For the matrix of any orthogonal automorphism Φ of
V we get the necessary and sufficient condition:

Q
0
0
0

!=

A⊤
C⊤
B⊤
D⊤
 
Q
0
0
0
 
A
B
C
D

(26)
=

A⊤QA
A⊤QB
B⊤QA
B⊤QB

.
(27)
This is equivalent to the conditions:
Q = A⊤QA,
0 = A⊤QB,
0 = B⊤QB.
(28)
This shows that A is the matrix of an orthogonal automorphism of W (w.r.t. b|W ), which, since Q is
invertible, is also invertible. The second equation then shows that necessarily B = 0. Since the whole
matrix needs to be invertible also D must be invertible. Furthermore, there are no constraints on C.
If all those conditions are satisfied the whole matrix satisfies the orthogonality constraints from
above.
Corollary C.9. For R(p,q,r) we get:
O(p, q, r) ∼=

O(p, q)
0(p+q)×r
M(r, p + q)
GL(r)

,
(29)
where O(p, q) := O(p, q, 0).
Remark C.10. Note that the composition Φ1◦Φ2 of orthogonal automorphisms of (V, q) corresponds
to the matrix multiplication as follows:

O1
0
M1
G1
 
O2
0
M2
G2

=

O1O2
0
M1O2 + G1M2
G1G2

.
(30)
Definition C.11 (Radical preserving orthogonal automorphisms). For a quadratic space (V, q) with
radical subspace R ⊆V we define the group of radical preserving orthogonal automorphisms as
follows:
OR(V, q) := {Φ ∈O(V, q) | Φ|R = idR} .
(31)
If (V, q) = R(p,q,r), i.e. if F = R and V = Rn and q has the signature (p, q, r) with p + q + r = n
then we define the group of radical preserving orthogonal matrices of signature (p, q, r) as follows:
OR(p, q, r) := {O ∈O(p, q, r) | bottom right corner of O = Ir} =

O(p, q)
0(p+q)×r
M(r, p + q)
Ir

,
(32)
where Ir is the (r × r)-identity matrix.
24

Remark C.12. Note that the matrix representation of Φ ∈OR(q) w.r.t. an orthogonal basis like in
the proof of theorem C.8 is of the form:

O
0m×r
M
Ir

,
(33)
with O ∈O(W, q|W ), M ∈M(r, m) and where Ir is the (r × r)-identity matrix.
The composition Φ1 ◦Φ2 of Φ1 and Φ2 ∈OR(q) is then given by the corresponding matrix
multiplication:

O1
0
M1
Ir
 
O2
0
M2
Ir

=

O1O2
0
M1O2 + M2
Ir

.
(34)
By observing the left column (the only part that does not transform trivially), we see that O(q|W )
acts on M(r, m) just by matrix multiplication from the right (in the corresponding basis):
(O1, M1) · (O2, M2) = (O1O2, M1O2 + M2).
(35)
This immediately shows that we can write OR(q) as the semi-direct product:
OR(q) ∼= O(q|W ) ⋉M(r, m).
(36)
In the special case of R(p,q,r) we get:
OR(p, q, r) ∼= O(p, q) ⋉M(r, p + q).
(37)
We conclude this chapter by citing the Theorem of Cartan and Dieudonné about the structure of
orthogonal groups in the non-degnerate case, but still for arbitrary fields F with char(F) ̸= 2.
Theorem C.13 (Theorem of Cartan-Dieudonné, see [Art57] Thm. 3.20). Let (V, q) be a non-
degenerate quadratic space of finite dimension dim V = n < ∞over a field F of char(F) ̸= 2. Then
every element g ∈O(V, q) can be written as:
g = r1 ◦· · · ◦rk,
(38)
with 1 ≤k ≤n, where ri are reflections w.r.t. non-singular hyperplanes.
25

D
The Clifford Algebra and Typical Constructions
In this section, we provide the required definitions, constructions, and derivations leading to the
results stated in the main paper. We start with a general introduction to the Clifford algebra.
D.1
The Clifford Algebra and its Universal Property
We follow [LS09, Cru90].
Let (V, q) be a finite dimensional quadratic space over a field F (also denoted an F-vector space) with
char(F) ̸= 2. We abbreviate the corresponding bilinear form b on vectors v1, v2 ∈V as follows:
b(v1, v2) := 1
2 (q(v1 + v2) −q(v1) −q(v2)) .
(39)
Definition D.1. To define the Clifford algebra Cl(V, q) we first consider the tensor algebra of V :
T(V ) :=
∞
M
m=0
V ⊗m = span {v1 ⊗· · · ⊗vm | m ≥0, vi ∈V } ,
(40)
V ⊗m := V ⊗· · · ⊗V
|
{z
}
m-times
,
V ⊗0 := F,
(41)
and the following two-sided ideal9:
I(q) :=

v ⊗v −q(v) · 1T(V )
 v ∈V

.
(42)
Then we define the Clifford algebra Cl(V, q) as the following quotient:
Cl(V, q) := T(V )/I(q).
(43)
In words, we identify the square of a vector with its quadratic form. We also denote the canonical
algebra quotient map as:
π : T(V ) →Cl(V, q).
(44)
Remark D.2.
1. It is not easy to see, but always true, that dim Cl(V, q) = 2n, where n :=
dim V , see Theorem D.15.
2. If e1, . . . , en is any basis of (V, q) then (eA)A⊆[n] is a basis for Cl(V, q), where we put for
a subset A ⊆[n] := {1, . . . , n}:
eA :=
<
Y
i∈A
ei,
e∅:= 1Cl(V,q).
(45)
where the product is taken in increasing order of the indices i ∈A, see Corollary D.16.
3. If e1, . . . , en is any orthogonal basis of (V, q), then one can even show that (eA)A⊆[n] is an
orthogonal basis for Cl(V, q) w.r.t. an extension of the bilinear form b from V to Cl(V, q),
see Theorem D.26.
Lemma D.3 (The fundamental identity). Note that for v1, v2 ∈V , we always have the fundamental
identity in Cl(V, q):
v1v2 + v2v1 = 2b(v1, v2).
(46)
Proof. By definition of the Clifford algebra we have the identities:
q(v1) + v1v2 + v2v1 + q(v2) = v1v1 + v1v2 + v2v1 + v2v2
(47)
= (v1 + v2)(v1 + v2)
(48)
= q(v1 + v2)
(49)
= b(v1 + v2, v1 + v2)
(50)
= b(v1, v1) + b(v1, v2) + b(v2, v1) + b(v2, v2)
(51)
= q(v1) + 2b(v1, v2) + q(v2).
(52)
Substracting q(v1) + q(v2) on both sides gives the claim.
9The ideal ensures that for all elements containing v ⊗v (i.e., linear combinations, multiplications on the left,
and multiplications on the right), v ⊗v is identified with q(v); e.g. x ⊗v ⊗v ⊗y ∼x ⊗(q(v) · 1T(V )) ⊗y for
every x, y ∈T(V ).
26

Further, the Clifford algebra Cl(V, q) is fully characterized by the following property.
Theorem D.4 (The universal property of the Clifford algebra). For every F-algebra10 (an algebra
over field F) A and every F-linear map f : V →A such that for all v ∈V we have:
f(v)2 = q(v) · 1A,
(53)
there exists a unique F-algebra homomorphism11 ¯f : Cl(V, q) →A such that ¯f(v) = f(v) for all
v ∈V .
More explicitely, if f satisfies equation 53 and x ∈Cl(V, q). Then we can take any representation of
x of the following form:
x = c0 +
X
i∈I
ci · vi,1 · · · vi,ki,
(54)
with finite index sets I and ki ∈N and coefficients c0, ci ∈F and vectors vi,j ∈V , j = 1, . . . , ki,
i ∈I, and, then we can compute ¯f(x) by the following formula (without ambiguity):
¯f(x) = c0 · 1A +
X
i∈I
ci · f(vi,1) · · · f(vi,ki).
(55)
In the following, we will often denote ¯f again with f without further indication.
D.2
The Multivector Filtration and the Grade
Note that the Clifford algebra is a filtered algebra.
Definition D.5. We define the multivector filtration12 of Cl(V, q) for grade m ∈N0 as follows:
Cl(≤m)(V, q) := π

T(≤m)(V )

,
T(≤m)(V ) :=
m
M
l=0
V ⊗l.
(56)
Remark D.6. Note that we really get a filtration on the space Cl(V, q):
F = Cl(≤0)(V, q) ⊆Cl(≤1)(V, q) ⊆Cl(≤2)(V, q) ⊆. . . ⊆Cl(≤n)(V, q) = Cl(V, q).
(57)
Furthermore, note that this is compatible with the algebra structure of Cl(V, q), i.e. for i, j ≥0 we
get:
x ∈Cl(≤i)(V, q)
∧
y ∈Cl(≤j)(V, q)
=⇒
xy ∈Cl(≤i+j)(V, q).
(58)
Together with the equality: π(T(≤m)(V )) = Cl(≤m)(V, q) for all m, we see that the natural map:
π : T(V ) →Cl(V, q),
(59)
is a surjective homomorphism of filtered algebras. Indeed, since Cl(V, q) is a well-defined algebra,
since we modded out a two-sided ideal, π is clearly a homomorphism of filtered algebras. As a
quotient map π is automatically surjective.
Definition D.7 (The grade of an element). For x ∈Cl(V, q) \ {0} we define its grade through the
following condition:
grd x := k
such that
x ∈Cl(≤k)(V, q) \ Cl(≤k−1)(V, q),
(60)
grd 0 := −∞.
(61)
10For the purpose of this text an algebra is always considered to be associative and unital (containing an
identity element), but not necessarily commutative.
11An algebra homomorphism is both linear and multiplicative.
12A filtration F is an indexed family (Ai)i∈I (I is an ordered index set) of subsets of an algebraic structure A
such that for i ≤j : Ai ⊆Aj.
27

D.3
The Parity Grading
In this section, we introduce the parity grading of the Clifford algebra. We will use the parity grading
to later construct the (adjusted) twisted conjugation map, which will be used as a group action on the
Clifford algebra.
Definition D.8 (The main involution). The linear map:
α : V →Cl(V, q),
α(v) := −v,
(62)
satisfies (−v)2 = v2 = q(v). The universal property of Cl(V, q) thus extends α to a unique algebra
homomorphism:
α : Cl(V, q) →Cl(V, q),
α
 
c0 +
X
i∈I
ci · vi,1 · · · vi,ki
!
(63)
= c0 +
X
i∈I
ci · α(vi,1) · · · α(vi,ki)
(64)
= c0 +
X
i∈I
(−1)ki · ci · vi,1 · · · vi,ki,
(65)
for any finite sum representation with vi,j ∈V and ci ∈F. This extension α will be called the main
involution13 of Cl(V, q).
Definition D.9 (Parity grading). The main involution α of Cl(V, q) now defines the parity grading of
Cl(V, q) via the following homogeneous parts:
Cl[0](V, q) := {x ∈Cl(V, q) | α(x) = x} ,
(66)
Cl[1](V, q) := {x ∈Cl(V, q) | α(x) = −x} .
(67)
With this we get the direct sum decomposition:
Cl(V, q) = Cl[0](V, q) ⊕Cl[1](V, q),
(68)
x = x[0] + x[1],
x[0] := 1
2(x + α(x)),
x[1] := 1
2(x −α(x)),
(69)
with the homogeneous parts x[0] ∈Cl[0](V, q) and x[1] ∈Cl[1](V, q).
We define the parity of an (homogeneous) element x ∈Cl(V, q) as follows:
prt(x) :=
(
0
if x ∈Cl[0](V, q),
1
if x ∈Cl[1](V, q).
(70)
Definition D.10 (Z/2Z-graded algebras). An F-algebra (A, +, ·) together with a direct sum decom-
position of sub-vector spaces:
A = A[0] ⊕A[1],
(71)
is called a Z/2Z-graded algebra if for every i, j ∈Z/2Z we always have:
x ∈A[i]
∧
y ∈A[j]
=⇒
x · y ∈A[i+j].
(72)
Note that [i + j] is meant here to be computed modulo 2. The Z/2Z-grade of an (homogeneous)
element x ∈A will also be called the parity of x:
prt(x) :=
0
if x ∈A[0],
1
if x ∈A[1].
(73)
The above requirement then implies for homogeneous elements x, y ∈A the relation:
prt(x · y) = prt(x) + prt(y)
mod 2.
(74)
We can now summarize the results of this section as follows:
Theorem D.11. The Clifford algebra Cl(V, q) is a Z/2Z-graded algebra in its parity grading.
13An involution is a map that is its own inverse.
28

D.4
The Dimension of the Clifford Algebra
In this subsection we determine the dimension of the Clifford algebra, which allows us to construct
bases for the Clifford algebra.
In the following, we again let F be any field of char(F) ̸= 2.
Definition/Lemma D.12 (The twisted tensor product of Z/2Z-graded F-algebras). Let A and B be
two Z/2Z-graded algebras over F. Then their twisted tensor product Aˆ⊗B is defined via the usual
tensor product A ⊗B of F-vector spaces, but where the product is defined on homogeneous elements
a1, a2 ∈A, b1, b2 ∈B via:
(a1 ˆ⊗b1) · (a2 ˆ⊗b2) := (−1)prt(b1)·prt(a2)(a1a2)ˆ⊗(b1b2).
(75)
This turns Aˆ⊗B also into a Z/2Z-graded algebra over F with the Z/2Z-grading:
(Aˆ⊗B)[0] :=

A[0] ⊗B[0]
⊕

A[1] ⊗B[1]
,
(76)
(Aˆ⊗B)[1] :=

A[0] ⊗B[1]
⊕

A[1] ⊗B[0]
.
(77)
In particular, if a ∈A, b ∈B are homogeneous elements then we have:
prtA ˆ⊗B(aˆ⊗b) = prtA(a) + prtB(b)
mod 2.
(78)
Proof. By definition we already know that Aˆ⊗B is an F-vector space. So we only need to investigate
the multiplication and Z/2Z-grading.
For homogenous elements a1, a2, a3 ∈A, b1, b2, b3 ∈B we have:
 (a1 ˆ⊗b1) · (a2 ˆ⊗b2)

· (a3 ˆ⊗b3)
(79)
= (−1)prt(b1)·prt(a2)  (a1a2)ˆ⊗(b1b2)

· (a3 ˆ⊗b3)
(80)
= (−1)prt(b1)·prt(a2) · (−1)prt(b1b2)·prt(a3)(a1a2a3)ˆ⊗(b1b2b3)
(81)
= (−1)prt(b1)·prt(a2)+prt(b1)·prt(a3)+prt(b2)·prt(a3)(a1a2a3)ˆ⊗(b1b2b3)
(82)
= (−1)prt(b1)·prt(a2a3) · (−1)prt(b2)·prt(a3)(a1a2a3)ˆ⊗(b1b2b3)
(83)
= (−1)prt(b2)·prt(a3)(a1 ˆ⊗b1) ·
 (a2a3)ˆ⊗(b2b3)

(84)
= (a1 ˆ⊗b1) ·
 (a2 ˆ⊗b2) · (a3 ˆ⊗b3)

.
(85)
This shows associativity of multiplication on homogeneous elements, which extends by linearity to
general elements. The distributive law is clear.
To check that we have a Z/2Z-grading, note that for homogeneous elements a1, a2 ∈A, b1, b2 ∈B
we have:
prtA ˆ⊗B
 (a1 ˆ⊗b1) · (a2 ˆ⊗b2)

= prtA ˆ⊗B

((−1)prt(a2)·prt(b1)a1a2)ˆ⊗(b1b2)

(86)
= prtA(a1a2) + prtB(b1b2)
(87)
= prtA(a1) + prtA(a2) + prtB(b1) + prtB(b2)
(88)
= prtA ˆ⊗B
 a1 ˆ⊗b1

+ prtA ˆ⊗B
 a2 ˆ⊗b2

mod 2.
(89)
The general case follows by linear combinations.
Remark D.13 (The universal property of the twisted tensor product of Z/2Z-graded F-algebras).
Let A1, A2, B be Z/2Z-graded F-algebras. Consider Z/2Z-graded F-algebra homomorphisms:
ψ1 : A1 →B,
ψ2 : A2 →B,
(90)
such that for all homogeneous elements a1 ∈A1 and a2 ∈A2 we have:
ψ1(a1) · ψ2(a2) = (−1)prt(a1)·prt(a2) · ψ2(a2) · ψ1(a1).
(91)
Then there exists a unique Z/2Z-graded F-algebra homomorphism:
ψ : A1 ˆ⊗A2 →B,
(92)
29

such that:
ψ ◦ϕ1 = ψ1,
ψ ◦ϕ2 = ψ2,
(93)
where ϕ1, ϕ2 are the following Z/2Z-graded F-algebra homomorphisms:
ϕ1 : A1 →A1 ˆ⊗A2,
a1 7→a1 ˆ⊗1,
(94)
ϕ2 : A2 →A1 ˆ⊗A2,
a2 7→1ˆ⊗a2,
(95)
which satisfy for all homogeneous elements a1 ∈A1 and a2 ∈A2:
ϕ1(a1) · ϕ2(a2) = (−1)prt(a1)·prt(a2) · ϕ2(a2) · ϕ1(a1).
(96)
Furthermore, A1 ˆ⊗A2 together with ϕ1, ϕ2 is uniquely characterized as a Z/2Z-graded F-algebra
by the above property (when considering all possible such B and ψ1, ψ2).
Proposition D.14. Let (V, q) be a finite dimensional quadratic vector space over F, char(F) ̸= 2,
with an orthogonal sum decomposition:
(V, q) = (V1, q1) ⊕(V2, q2).
(97)
Then the inclusion maps: Cl(Vi, qi) →Cl(V, q) induce an isomorphism of Z/2Z-graded F-algebras:
Cl(V, q) ∼= Cl(V1, q1)ˆ⊗Cl(V2, q2).
(98)
In particular:
dim Cl(V, q) = dim Cl(V1, q1) · dim Cl(V2, q2).
(99)
Proof. First consider the canonical inclusion maps, l = 1, 2:
ϕl : Cl(Vl, ql) →Cl(V, q),
xl 7→xl.
(100)
These satisfy for all (parity) homogeneous elements xl ∈Cl(Vl, ql), l = 1, 2, the condition:
ϕ1(x1)ϕ2(x2) = x1x2
!= (−1)prt(x1)·prt(x2) · x2x1 = (−1)prt(x1)·prt(x2) · ϕ2(x2)ϕ1(x1).
(101)
Indeed, we can by linearity reduce to the case that x1 and x2 are products of vectors v1 ∈V1 and
v2 ∈V2, resp. Since V1 and V2 are orthogonal to each other by assumption, for those elements we
get by the identities D.3:
v1v2 = −v2v1 + 2 b(v1, v2)
|
{z
}
=0
= −v2v1.
(102)
This shows the condition above for x1 and x2.
We then define the F-bilinear map:
Cl(V1, q1) × Cl(V2, q2) →Cl(V, q),
(x1, x2) 7→x1x2,
(103)
which thus factorizes through the F-linear map:
ϕ : Cl(V1, q1)ˆ⊗Cl(V2, q2) →Cl(V, q),
x1 ˆ⊗x2 7→x1x2.
(104)
We now show that ϕ also respects multiplication. For this let x1, y1 ∈Cl(V1, q1) and x2, y2 ∈
Cl(V2, q2) (parity) homogeneous elements. We then get:
ϕ
 (x1 ˆ⊗x2) · (y1 ˆ⊗y2)

(105)
= ϕ

(−1)prt(x2)·prt(y1) · (x1y1)ˆ⊗(x2y2)

(106)
= (−1)prt(x2)·prt(y1) · x1y1x2y2
(107)
= x1x2y1y2
(108)
= ϕ(x1 ˆ⊗x2) · ϕ(y1 ˆ⊗y2).
(109)
This shows the multiplicativity of ϕ on homogeneous elements. The general case follows by the
F-linearity of ϕ. Note that ϕ also respects the parity grading.
30

Now consider the Z/2Z-graded F-algebra Cl(V1, q1)ˆ⊗Cl(V2, q2) and Z/2Z-graded F-algebra ho-
momorphisms:
ψ1 : Cl(V1, q1) →Cl(V1, q1)ˆ⊗Cl(V2, q2),
x1 7→x1 ˆ⊗1,
(110)
ψ2 : Cl(V2, q2) →Cl(V1, q1)ˆ⊗Cl(V2, q2),
x2 7→1ˆ⊗x2.
(111)
Note that for all homogeneous elements xl ∈Cl(Vl, ql), l = 1, 2, we have:
ψ2(x2) · ψ1(x1) = (1ˆ⊗x2) · (x1 ˆ⊗1)
(112)
= (−1)prt(x1)·prt(x2) · x1 ˆ⊗x2
(113)
= (−1)prt(x1)·prt(x2) · (x1 ˆ⊗1) · (1ˆ⊗x2)
(114)
= (−1)prt(x1)·prt(x2) · ψ1(x1) · ψ2(x2).
(115)
We then define the F-linear map:
ψ : V = V1 ⊕V2 →Cl(V1, q1)ˆ⊗Cl(V2, q2),
v = v1 + v2 7→ψ1(v1) + ψ2(v2) =: ψ(v). (116)
Note that we have:
ψ(v)2 = (ψ1(v1) + ψ2(v2)) · (ψ1(v1) + ψ2(v2))
(117)
= ψ1(v1)2 + ψ2(v2)2 + ψ1(v1) · ψ2(v2) + ψ2(v2) · ψ1(v1)
(118)
= ψ1(v1)2 + ψ2(v2)2 + ψ1(v1) · ψ2(v2) + (−1)prt(v1)·prt(v2)ψ1(v1) · ψ2(v2)
|
{z
}
=0
(119)
= ψ1(v2
1) + ψ2(v2
2)
(120)
= q1(v1) · ψ1(1Cl(V1,q1)) + q2(v2) · ψ2(1Cl(V2,q2))
(121)
= q1(v1) · 1Cl(V1,q1) ˆ⊗Cl(V2,q2) + q2(v2) · 1Cl(V1,q1) ˆ⊗Cl(V2,q2)
(122)
= (q1(v1) + q2(v2)) · 1Cl(V1,q1) ˆ⊗Cl(V2,q2)
(123)
= q(v) · 1Cl(V1,q1) ˆ⊗Cl(V2,q2).
(124)
By the universal property of the Clifford algebra ψ uniquely extends to an F-algebra homomorphism:
ψ : Cl(V, q) →Cl(V1, q1)ˆ⊗Cl(V2, q2),
(125)
vi1 · · · vik 7→(ψ1(vi1,1) + ψ2(vi1,2)) · · · (ψ1(vik,1) + ψ2(vik,2)).
(126)
One can see from this, by explicit calculation, that ψ also respects the Z/2Z-grading. Furthermore,
we see that ψ ◦ϕl = ψl for l = 1, 2, and, also, ϕ ◦ψl = ϕl for l = 1, 2.
One easily sees that ϕ and ψ are inverse to each other and the claim follows.
Theorem D.15 (The dimension of the Clifford algebra). Let (V, q) be a finite dimensional quadratic
vector space over F, char(F) ̸= 2, n := dim V < ∞. Then we have:
dim Cl(V, q) = 2n.
(127)
Proof. Let e1, . . . , en be an orthogonal basis of (V, q) and Vi := span(ei) ⊆V , qi := q|Vi. Then
we get the orthogonal sum decomposition:
(V, q) = (V1, q1) ⊕· · · ⊕(Vn, qn),
(128)
and thus by Proposition D.14 the isomorphism of Z/2Z-graded algebras:
Cl(V, q) ∼= Cl(V1, q1)ˆ⊗· · · ˆ⊗Cl(Vn, qn),
(129)
and thus:
dim Cl(V, q) = dim Cl(V1, q1) · · · dim Cl(Vn, qn),
(130)
Since dim Vi = 1 and thus:
dim Cl(Vi, qi) = dim (F ⊕Vi) = 2,
(131)
for i = 1, . . . , n, we get the claim:
dim Cl(V, q) = 2n.
(132)
31

Corollary D.16 (Bases for the Clifford algebra). Let (V, q) be a finite dimensional quadratic vector
space over F, char(F) ̸= 2, n := dim V < ∞. Let e1, . . . , en be any basis of V , then (eA)A⊆[n] is
a basis for Cl(V, q), where we put for a subset A ⊆[n] := {1, . . . , n}:
eA :=
<
Y
i∈A
ei,
e∅:= 1Cl(V,q).
(133)
where the product is taken in increasing order of the indices i ∈A.
Proof. Since Cl(V, q) = T(V )/I(q) and
T(V ) = span {ei1 ⊗· · · ⊗eim | m ≥0, ij ∈[n], j ∈[m]} ,
(134)
we see that:
Cl(V, q) = span {ei1 · · · eim | m ≥0, ij ∈[n], j ∈[m]} .
(135)
By several applications of the fundamental identities D.3:
eikeil = −eileik + 2b(eik, eil),
eikeik = q(eik),
(136)
we can turn products ei1 · · · eim into sums of smaller products if some of the occuring indices agree,
say ik = il. Furthermore, we can also use those identities to turn the indices in increasing order. This
then shows:
Cl(V, q) = span {eA | A ⊆[n]} .
(137)
Since # {A ⊆[n]} = 2n and dim Cl(V, q) = 2n by Theorem D.15 we see that {eA | A ⊆[n]} must
already be a basis for Cl(V, q).
D.5
Extending the Quadratic Form to the Clifford Algebra
We provide the extension of the quadratic form from the vector space to the Clifford algebra, which
will lead to the constrution of an orthogonal basis of the Clifford algebra.
Definition D.17 (The opposite algebra of an algebra). Let (A, +, ·) be an algebra. The opposite
algebra (Aop, +, •) is defined to consist of the same underlying vector space (Aop, +) = (A, +), but
where the multiplication is reversed in comparison to A, i.e. for x, y ∈Aop we have:
x • y := y · x.
(138)
Note that this really turns (Aop, +, •) into an algebra.
Definition D.18 (The main anti-involution of the Clifford algebra). Consider the following linear
map:
β : V →Cl(V, q)op,
v 7→v,
(139)
which also satisfies v • v = vv = q(v). By the universal property of the Clifford algebra we get a
unique extension to an algebra homomorphism:
β : Cl(V, q) →Cl(V, q)op,
β
 
c0 +
X
i∈I
ci · vi,1 · · · vi,ki
!
(140)
= c0 +
X
i∈I
ci · vi,1 • · · · • vi,ki
(141)
= c0 +
X
i∈I
ci · vi,ki · · · vi,1,
(142)
for any finite sum representation with vi,j ∈V and ci ∈F. We call β the main anti-involution of
Cl(V, q).
Definition D.19 (The combined anti-involution of the Clifford algebra). The combined anti-involution
or Clifford conjugation of Cl(V, q) is defined to be the F-algebra homomorphism:
γ : Cl(V, q) →Cl(V, q)op,
γ(x) := β(α(x)).
(143)
More explicitely, it is given by the formula:
γ
 
c0 +
X
i∈I
ci · vi,1 · · · vi,ki
!
= c0 +
X
i∈I
(−1)ki · ci · vi,ki · · · vi,1,
(144)
for any finite sum representation with vi,j ∈V and ci ∈F.
32

Remark D.20. If e1, . . . , en ∈V is an orthogonal basis for (V, q). Then we have for A ⊆[n]:
α(eA) = (−1)|A|eA,
β(eA) = (−1)(
|A|
2 )eA,
γ(eA) = (−1)(
|A|+1
2 )eA.
(145)
Recall the definition of a trace of a linear map:
Definition D.21 (The trace of an endomorphism). Let Y be a vector space over a field F of dimension
dim Y = m < ∞and Φ : Y →Y a vector space endomorphism14. Let B = {b1, . . . , bm} be a basis
for Y and B∗= {b∗
1, . . . , b∗
m} be the corresponding dual basis of Y∗, defined via: b∗
j(bi) := δi,j.
Let A = (ai,j)i=1,...,m,
j=1,...,m
be the matrix representation of Φ w.r.t. B:
∀j ∈[m].
Φ(bj) =
m
X
i=1
ai,jbi.
(146)
Then the trace of Φ is defined via:
Tr(Φ) :=
m
X
j=1
aj,j =
m
X
j=1
b∗
j(Φ(bj)) ∈F.
(147)
It is a well known fact that Tr(Φ) is not dependent on the initial choice of the basis B. Furthermore,
Tr is a well-defined F-linear map (homomorphism of vector spaces):
Tr : EndF(Y) →F,
Φ 7→Tr(Φ).
(148)
We now want to define the projection of x ∈Cl(V, q) onto its zero component x(0) ∈F in a basis
independent way.
Definition D.22 (The projection onto the zero component). We define the F-linear map:
ζ : Cl(V, q) →F,
ζ(x) := 2−n Tr(x),
(149)
where n := dim V and Tr(x) := Tr(Lx), where Lx is the endomorphism of Cl(V, q) given by left
multiplication with x:
Lx : Cl(V, q) →Cl(V, q),
y 7→Lx(y) := xy.
(150)
We call ζ the projection onto the zero component. We also often write for x ∈Cl(V, q):
x(0) := ζ(x).
(151)
The name is justified by following property:
Lemma D.23. Let e1, . . . , en be a fixed orthogonal basis of (V, q). Then we know that (eA)A⊆[n] is
a basis for Cl(V, q). So we can write every x ∈Cl(V, q) as:
x =
X
A⊆[n]
xA · eA,
(152)
with xA ∈F, A ⊆[n]. The claim is now that we have:
ζ(x)
!= x∅.
(153)
Proof. By the linearity of the trace we only need to investigate Tr(eA) for A ⊆[n]. For A = ∅, we
have e∅= 1 and we get:
Tr(1) =
X
B⊆[n]
e∗
B(1 · eB) =
X
B⊆[n]
1 = 2n,
(154)
14A map from a mathematical object space to itself.
33

which shows: ζ(1) = 2−n Tr(1) = 1. Now, consider A ⊆[n] with A ̸= ∅. Let △denote the
symmetric difference of two sets. Further, we can write ± to refrain from distinguishing between
signs, which will not affect the result.
Tr(eA) =
X
B⊆[n]
e∗
B(eAeB)
(155)
=
X
B⊆[n]
±
Y
i∈A∩B
q(ei) · e∗
B(eA△B)
(156)
=
X
B⊆[n]
±
Y
i∈A∩B
q(ei) · δB,A△B
(157)
=
X
B⊆[n]
±
Y
i∈A∩B
q(ei) · δ∅,A
(158)
= 0,
(159)
where the third equality follows from the fact that B = A△B holds if and only if A = ∅, regardless
of B. However, A = ∅was ruled out by assumption, so then in the last equality we always have
δ∅,A = 0. So for A ̸= ∅we have: ζ(eA) = 2−n Tr(eA) = 0. Altogether we get:
ζ(eA) = δA,∅=
1,
if A = ∅,
0,
else.
(160)
With this and linearity we get:
ζ(x) = ζ

X
A⊆[n]
xA · eA

=
X
A⊆[n]
xA · ζ(eA) =
X
A⊆[n]
xA · δA,∅= x∅.
(161)
This shows the claim.
Definition D.24 (The bilinear form on the Clifford algebra). For our quadratic F-vector space (V, q)
with corresponding bilinear form b and corresponding Clifford algebra Cl(V, q) we now define the
following F-bilinear form on Cl(V, q):
¯b : Cl(V, q) × Cl(V, q) →F,
¯b(x, y) := ζ(β(x)y).
(162)
We also define the corresponding quadratic form on Cl(V, q) via:
¯q : Cl(V, q) →F,
¯q(x) := ¯b(x, x) = ζ(β(x)x).
(163)
We will see below that ¯b and ¯q will agree with b and q, resp., when they are restricted to V . From
that point on we will denote ¯b just by b, and ¯q with q, resp., without (much) ambiguity.
Lemma D.25. For v, w ∈V we have:
¯b(v, w) = b(v, w).
(164)
Proof. We pick an orthogonal basis e1, . . . , en for V and write:
v =
n
X
i=1
ai · ei,
w =
n
X
j=1
cj · ej.
(165)
34

We then get by linearity:
¯b(v, w) =
n
X
i=1
n
X
j=1
aicj · ¯b(ei, ej)
(166)
=
n
X
i=1
n
X
j=1
aicj · ζ(β(ei)ej)
(167)
=
n
X
i=1
n
X
j=1
aicj · ζ(eiej)
(168)
=
X
i̸=j
aicj · ζ(eiej)
| {z }
=0
+
X
i=j
aicj · ζ(eiej)
(169)
=
n
X
i=1
aici · q(ei) · ζ(1)
|{z}
=1
(170)
=
n
X
i=1
n
X
j=1
aicj ·
q(ei)·δi,j=
z
}|
{
b(ei, ej)
(171)
= b


n
X
i=1
ai · ei,
n
X
j=1
cj · ej


(172)
= b(v, w).
(173)
This shows the claim.
Theorem D.26. Let e1, . . . , en be an orthogonal basis for (V, q) then (eA)A⊆[n] is an orthogonal
basis for Cl(V, q) w.r.t. the induced bilinear form ¯b. Furthermore, for x, y ∈Cl(V, q) of the form:
x =
X
A⊆[n]
xA · eA,
y =
X
A⊆[n]
yA · eA,
(174)
with xA, yA ∈F we get:
¯b(x, y) =
X
A⊆[n]
xA · yA ·
Y
i∈A
q(ei),
¯q(x) =
X
A⊆[n]
x2
A ·
Y
i∈A
q(ei).
(175)
Note that: ¯q(e∅) = ¯q(1) = 1.
Proof. We already know that (eA)A⊆[n] is a basis for Cl(V, q). So we only need to check the
orthogonality condition. First note that for eC = ei1 · · · eir we get:
¯q(eC) = ζ(β(eC)eC) = ζ(eir · · · ei1 · ei1 · · · eir) = q(ei1) · · · q(eir).
(176)
Now let A, B ⊆[n] with A ̸= B, i.e. with A△B ̸= ∅. We then get:
¯b(eA, eB) = ζ(β(eA)eB) = ±
Y
i∈A∩B
q(ei) · ζ(eA△B) = 0.
(177)
35

This shows that (eA)A⊆[n] is an orthogonal basis for Cl(V, q). For x, y ∈Cl(V, q) from above we
get:
¯b(x, y) = ¯b

X
A⊆[n]
xA · eA,
X
B⊆[n]
yB · eB


(178)
=
X
A⊆[n]
X
B⊆[n]
xA · yB · ¯b(eA, eB)
(179)
=
X
A⊆[n]
X
B⊆[n]
xA · yB · ¯q(eA) · δA,B
(180)
=
X
A⊆[n]
xA · yA · ¯q(eA)
(181)
=
X
A⊆[n]
xA · yA ·
Y
i∈A
q(ei).
(182)
This shows the claim.
D.6
The Multivector Grading
Now that we have an orthogonal basis for the algebra, we show that the Clifford algebra allows a
vector space grading that is independent of the chosen orthogonal basis.
Let (V, q) be a quadratic space over a field F with char(F) ̸= 2 and dim V = n < ∞.
In the following we present a technical proof that works for fields F with char(F) ̸= 2. An alternative,
simpler and more structured proof, but for the more restrictive case of char(F) = 0, can be found in
Theorem D.37 later.
Theorem D.27 (The multivector grading of the Clifford algebra). Let e1, . . . , en be an orthogonal
basis of (V, q). Then for every m = 0, . . . , n we define the following sub-vector space of Cl(V, q):
Cl(m)(V, q) := span {ei1 · · · eim | 1 ≤i1 < · · · < im ≤n}
(183)
= span {eA | A ⊆[n], |A| = m} ,
(184)
where Cl(0)(V, q) := F.
Then the sub-vector spaces Cl(m)(V, q), m = 0, . . . , n, are independent of the choice of the orthogo-
nal basis, i.e. if b1, . . . , bn is another orthogonal basis of (V, q), then:
Cl(m)(V, q) = span {bi1 · · · bim | 1 ≤i1 < · · · < im ≤n} .
(185)
Proof. First note that by the orthogonality and the fundamental relation of the Clifford algebra we
have for all i ̸= j:
eiej = −ejei,
bibj = −bjbi.
(186)
We now abbreviate:
B(m) := span {bj1 · · · bjm | 1 ≤j1 < · · · < jm ≤n} ,
(187)
and note that:
Cl(m)(V, q) = span {ei1 · · · eim | 1 ≤i1 < · · · < im ≤n}
(188)
= span {ei1 · · · eim | 1 ≤i1, . . . , im ≤n, ∀s ̸= t. is ̸= it} .
(189)
We want to show that for 1 ≤j1 < · · · < jm ≤n we have that:
bj1 · · · bjm ∈Cl(m)(V, q).
(190)
Since we have two bases we can write an orthogonal change of basis:
bj =
n
X
i=1
ai,jei ∈V = Cl(1)(V, q).
(191)
36

Using this, we can now write the above product as the sum of two terms:
bj1 · · · bjm =
X
i1,...,im
ai1,j1 · · · aim,jm · ei1 · · · eim
(192)
=
X
i1,...,im
∀s̸=t. is̸=it
ai1,j1 · · · aim,jm · ei1 · · · eim +
X
i1,...,im
∃s̸=t. is=it
ai1,j1 · · · aim,jm · ei1 · · · eim.
(193)
Our claim is equivalent to the vanishing of the second term. Note that the above equation for bj
already shows the claim for m = 1. The case m = 0 is trivial.
We now prove the claim for m = 2 by hand before doing induction after. Recall that j1 ̸= j2:
bj1bj2 =
X
i1,i2
i1̸=i2
ai1,j1ai2,j2 · ei1ei2 +
n
X
i=1
ai,j1ai,j2 · eiei
(194)
=
X
i1,i2
i1̸=i2
ai1,j1ai2,j2 · ei1ei2 +
n
X
i=1
ai,j1 ai,j2 · q(ei)
|
{z
}
=b(ei,bj2)
(195)
=
X
i1,i2
i1̸=i2
ai1,j1ai2,j2 · ei1ei2 + b(bj1, bj2)
|
{z
}
=0
(196)
=
X
i1,i2
i1̸=i2
ai1,j1ai2,j2 · ei1ei2
(197)
∈Cl(2)(V, q).
(198)
This shows the claim for m = 2.
By way of induction we now assume that we have shown the claim until some m ≥2, i.e. we have:
bj1 · · · bjm =
X
i1,...,im
∀k̸=l. ik̸=il
ai1,j1 · · · aim,jmei1 · · · eim ∈Cl(m)(V, q).
(199)
Now consider another bj with j := jm+1 ̸= jk, k = 1, . . . , m. We then get:
37

bj1 · · · bjmbj =




X
i1,...,im
∀k̸=l. ik̸=il
ai1,j1 · · · aim,jmei1 · · · eim




 n
X
i=1
ai,jei
!
(200)
=
n
X
i=1
X
i1,...,im
∀k̸=l. ik̸=il
ai1,j1 · · · aim,jmai,jei1 · · · eimei
(201)
=
n
X
i=1
X
i1,...,im
∀k̸=l. ik̸=il
i/∈{i1,...,im}
ai1,j1 · · · aim,jmai,jei1 · · · eimei
(202)
+
n
X
i=1
X
i1,...,im
∀k̸=l. ik̸=il
i∈{i1,...,im}
ai1,j1 · · · aim,jmai,jei1 · · · eimei
(203)
=
X
i1,...,im,im+1
∀k̸=l. ik̸=il
ai1,j1 · · · aim,jmaim+1,jm+1ei1 · · · eimeim+1
(204)
+
n
X
i=1
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · aim,jmai,jei1 · · · eimei.
(205)
38

We have to show that the last term vanishes. The last term can be written as:
n
X
i=1
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · aim,jmai,j · ei1 · · · eimei
(206)
=
n
X
i=1
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · ais,js · · · aim,jmai,j · ei1 · · · eis · · · eimei
(207)
=
n
X
i=1
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · ais,js · · · aim,jmai,j · ei1 · · ·
eis · · · eimeisei · (−1)m−s
(208)
=
n
X
i=1
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · ais,js · · · aim,jmai,j · ei1 · · ·
eis · · · eim · (−1)m−sq(ei)
(209)
=
m
X
s=1
n
X
i=1
X
i1,...,im
∀k̸=l. ik̸=il
is=i
ai1,j1 · · · ais,js · · · aim,jmai,j · ei1 · · ·
eis · · · eim · (−1)m−sq(ei)
(210)
=
m
X
s=1
X
i1,...,im
∀k̸=l. ik̸=il
ai1,j1 · · · ais,js · · · aim,jmais,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eis)
(211)
=
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
n
X
is=1
is /∈{i1,.,
is,.,im}
ai1,j1 · · · ais,js · · · aim,jmais,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eis)
(212)
=
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
n
X
is=1
ai1,j1 · · · ais,js · · · aim,jmais,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eis) (213)
−
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
X
is∈{i1,.,
is,.,im}
ai1,j1 · · · ais,js · · · aim,jmais,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eis)
(214)
=
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
ai1,j1 · · ·
ais,js · · · aim,jm · ei1 · · ·
eis · · · eim · (−1)m−s
n
X
is=1
ais,jsais,jq(eis)
|
{z
}
=b(bj,bjs)=0
(215)
−
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
X
is∈{i1,.,
is,.,im}
ai1,j1 · · · ais,js · · · aim,jmais,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eis)
(216)
= −
m
X
s=1
X
i1,...,
is,...,im
∀k̸=l. ik̸=il
X
i∈{i1,.,
is,.,im}
ai1,j1 · · · ai,js · · · aim,jmai,j · ei1 · · ·
eis · · · eim · (−1)m−sq(ei)
(217)
= −
m
X
s=1
m
X
t=1
t̸=s
X
i1,.,
is,.,it,.,im
∀k̸=l. ik̸=il
ai1,j1 · · · ait,js · · · aim,jmait,j · ei1 · · ·
eis · · · eim · (−1)m−sq(eit).
(218)
39

Note that the positional index t can occure before or after the positional index s. Depending of its
position the elements eit will appear before or after the element eis in the product. We look at both
cases separately and suppress the dots in between for readability.
First consider t > s:
m
X
s=1
m
X
t=1
t>s
X
i1,.,
is,.,it,.,im
∀k̸=l. ik̸=il
ai1,j1 · ait,js · ait,jt · aim,jmait,j · ei1 ·
eis · eit · eim · (−1)m−sq(eit) (219)
=
m
X
s=1
m
X
t=1
t>s
X
i1,.,
is,.,it,.,im
∀k̸=l. ik̸=il
ai1,j1 · ait,js · ait,jt · aim,jmait,j · (−1)t−2eit · ei1 ·
eis ·
eit · eim · (−1)m−sq(eit)
(220)
=
m
X
s=1
m
X
t=1
t>s
n
X
it=1
ait,js · ait,jt · ait,j(−1)m−sq(eit)(−1)teit · πs,t(it)
(221)
with
πs,t(i) :=
X
i1,.,
is,.,
it,.,im
∀k̸=l. ik̸=il
∀k. ik̸=i
ai1,j1 ·
ait,js ·
ait,jt · aim,jm · ei1 ·
eis ·
eit · eim
(222)
=
m
X
s=1
m
X
t=1
t>s
n
X
i=1
(−1)m+s+tai,js · ai,jt · ai,j · q(ei) · ei · πs,t(i)
(223)
=
m
X
s=1
m
X
t=1
t>s
y(s, t),
(224)
with
y(s, t) :=
n
X
i=1
(−1)m+s+tai,js · ai,jt · ai,j · q(ei) · ei · πs,t(i).
(225)
It is important to note that for all s ̸= t we have:
y(s, t) = y(t, s).
(226)
40

Now consider t < s:
m
X
s=1
m
X
t=1
t<s
X
i1,.,it,.,
is,.,im
∀k̸=l. ik̸=il
ai1,j1 · ait,jt · ait,js · aim,jmait,j · ei1 · eit ·
eis · eim · (−1)m−sq(eit) (227)
=
m
X
s=1
m
X
t=1
t<s
X
i1,.,it,.,
is,.,im
∀k̸=l. ik̸=il
ai1,j1 · ait,jt · ait,js · aim,jmait,j · (−1)t−1eit · ei1 ·
eit ·
eis · eim · (−1)m−sq(eit)
(228)
=
m
X
s=1
m
X
t=1
t<s
n
X
it=1
ait,js · ait,jt · ait,j(−1)m−sq(eit)(−1)t−1eit · πs,t(it)
(229)
=
m
X
s=1
m
X
t=1
t<s
n
X
i=1
(−1)m+s+t+1ai,js · ai,jt · ai,j · q(ei) · ei · πs,t(i)
(230)
= −
m
X
s=1
m
X
t=1
t<s
y(s, t)
(231)
= −
m
X
t=1
m
X
s=1
s<t
y(t, s)
(232)
= −
m
X
t=1
m
X
s=1
s<t
y(s, t)
(233)
= −
m
X
s=1
m
X
t=1
t>s
y(s, t).
(234)
In total we see that both terms appear with a different sign and thus cancel out. This shows the
claim.
Corollary D.28. Let e1, . . . , en and b1, . . . , bn be two orthogonal bases of (V, q) with basis transition
matrix C = (ci,j)i∈[n],j∈[n]:
∀j ∈[n].
bj =
X
i∈[n]
ci,j · ei.
(235)
Then C is invertible and we have the following matrix relations:
diag(q(b1), . . . , q(bn)) = C⊤diag(q(e1), . . . , q(en))C.
(236)
Furthermore, for every subset J ⊆[n] we have the formula:
bJ =
X
I⊆[n]
|I|=|J|
det CI,J · eI,
(237)
with the submatrix: CI,J = (ci,j)i∈I,j∈J.
41

Proof. For j, l ∈[n] we have:
diag(q(b1), . . . , q(bn))j,l = q(bj) · δj,l
(238)
= b(bj, bl)
(239)
=
n
X
i=1
n
X
k=1
ci,j · ck,l · b(ei, ek)
(240)
=
n
X
i=1
n
X
k=1
ci,j · ck,l · q(ei) · δi,k
(241)
=
 C⊤diag(q(e1), . . . , q(en))C

j,l .
(242)
This shows the matrix identity:
diag(q(b1), . . . , q(bn)) = C⊤diag(q(e1), . . . , q(en))C.
(243)
Furthermore, we have the following identites:
bJ = bj1 · · · bjm
(244)
=

X
i1∈[n]
ci1,j1 · ei1

· · ·

X
im∈[n]
cim,jm · eim


(245)
=
X
i1∈[n],...,im∈[n]
(ci1,j1 · · · cim,jm) · (ei1 · · · eim)
(246)
D.27
=
X
i1∈[n],...,im∈[n]
|{i1,...,im}|=m
(ci1,j1 · · · cim,jm) · (ei1 · · · eim)
(247)
=
X
i1∈[n],...,im∈[n]
|{i1,...,im}|=m
sgn(i1, . . . , im) · (ci1,j1 · · · cim,jm) · e{i1,...,im}
(248)
=
X
i1,...,im∈[n]
i1<···<im
 X
σ∈Sm
sgn(σ) · ciσ(1),j1 · · · ciσ(m),jm
!
· e{i1,...,im}
(249)
=
X
I⊆[n]
|I|=m
det CI,J · eI.
(250)
This shows the claim.
Notation D.29. For m /∈{0, . . . , n} it is sometimes convenient to put:
Cl(m)(V, q) := 0.
(251)
Corollary D.30. We have the following orthogonal sum decomposition (w.r.t. ¯q) of the Clifford
algebra Cl(V, q) into its F-vector spaces of multivector components:
Cl(V, q) =
n
M
m=0
Cl(m)(V, q),
(252)
which is independent of the choice of orthogonal basis of (V, q). Also note that for all m = 0, . . . , n:
dim Cl(m)(V, q) =
n
m

.
(253)
Definition D.31. We call an element x ∈Cl(m)(V, q) an m-multivector or an element of Cl(V, q) of
pure grade m. For x ∈Cl(V, q) we have a decomposition:
x = x(0) + x(1) + · · · + x(n),
(254)
with x(m) ∈Cl(m)(V, q), m = 0, . . . , n. We call x(m) the grade-m-component of x.
42

Remark D.32. Note that the multivector grading of Cl(V, q) is only a grading of F-vector spaces,
but not of F-algebras. The reason is that multiplication can make the grade drop. For instance, for
v ∈V = Cl(1)(V, q) we have vv = q(v) ∈Cl(0)(V, q), while a grading for algebras would require
that vv ∈Cl(2)(V, q), which is here not the case.
Remark D.33 (Parity grading and multivector filtation in terms of multivector grading).
1. We clearly have:
Cl[0](V, q) =
M
m=0,...,n
m even
Cl(m)(V, q),
Cl[1](V, q) =
M
m=1,...,n
m odd
Cl(m)(V, q).
(255)
2. It is also clear that for general m we have:
Cl(≤m)(V, q) =
m
M
l=0
Cl(m)(V, q)
⊆
Cl(V, q).
(256)
A simpler proof of Theorem D.27 can be obtained if we assume that char(F) = 0. We would then
argue as follows.
Definition D.34 (Antisymmetrization). For m ∈N and x1, . . . , xm ∈Cl(V, q) we define their
antisymmetization as:
[x1; . . . ; xm] := 1
m!
X
σ∈Sm
sgn(σ) · xσ(1) · · · xσ(m),
(257)
where Sm denotes the group of all permuations of [m]. Note that, due to the division by m! we need
that char(F) = 0 if we want to accommodate arbitrary m ∈N.
Lemma D.35. Let e1, . . . , en be an orthogonal basis of (V, q) and x1, . . . , xm ∈Cl(V, q). Then we
have:
1. [x1; . . . ; xm] is linear in each of its arguments (if the other arguments are fixed).
2. [x1; . . . ; xk; . . . ; xl; . . . ; xm] = −[x1; . . . ; xl; . . . ; xk; . . . ; xm].
3. [x1; . . . ; xk; . . . ; xl; . . . ; xm] = 0 if xk = xl.
4. [ei1; . . . ; eim] = ei1 · · · eim if | {i1, . . . , im} | = m (i.e. if all indices are different).
Definition D.36 (Multivector grading - alternative, basis independent definition). For m = 0, . . . , n
we (re-)define:
Cl(m)(V, q) := span {[v1; . . . ; vm] | v1, . . . , vm ∈V } .
(258)
Theorem D.37. Let e1, . . . , en be an orthogonal basis of (V, q), char(F) = 0. Then for every
m = 0, . . . , n we have the equality:
Cl(m)(V, q) = span {eA | A ⊆[n], |A| = m} .
(259)
Note that the rhs is seemingly dependent of the choice of the orthogonal basis while the lhs is defined
in a basis independent way.
Proof. We can write every vk ∈V as a linear combination of its basis vectors:
vk =
X
jk∈[n]
ck,jk · ejk.
(260)
43

With this we get:
[v1; . . . ; vm] =
X
j1,...,jm∈[n]
Y
k∈[m]
ck,jk · [ej1; . . . ; ejm]
(261)
=
X
j1,...,jm∈[n]
|{j1,...,jm}|=m
Y
k∈[m]
ck,jk · [ej1; . . . ; ejm]
(262)
=
X
j1,...,jm∈[n]
|{j1,...,jm}|=m
Y
k∈[m]
ck,jk · ej1 · · · ejm
(263)
=
X
j1,...,jm∈[n]
|{j1,...,jm}|=m
±
Y
k∈[m]
ck,jk · e{j1,...,jm}
(264)
∈span {eA | A ⊆[n], |A| = m} .
(265)
This shows the inclusion:
Cl(m)(V, q) ⊆span {eA | A ⊆[n], |A| = m} .
(266)
The reverse inclusion is also clear as:
eA = [ej1; . . . ; ejm] ∈Cl(m)(V, q),
(267)
where A = {j1, . . . , jm} and |A| = m. This shows the equality of both sets.
D.7
The Radical Subalgebra of the Clifford Algebra
Again, let (V, q) be a quadratic vector space of finite dimensions dim V = n < ∞over a field F of
char(F) ̸= 2. Let b the corresponding bilinear form of q.
Notation D.38. We denote the group of the invertible elements of Cl(V, q):
Cl×(V, q) := {x ∈Cl(V, q) | ∃y ∈Cl(V, q). xy = yx = 1} .
(268)
Let R ⊆V be V ’s radical subspace. Recall:
R := {f ∈V | ∀v ∈V. b(f, v) = 0} .
(269)
Definition D.39 (The radical subalgebra). We define the radical subalgebra of Cl(V, q) to be:
^
(R) := span {1, f1 · · · fk | k ∈N0, fl ∈R, l = 1, . . . , k} ⊆Cl(V, q).
(270)
Note that q|R = 0 and that V(R) coincides with Cl(R, q|R).
Notation D.40. We make the following further abbreviations:
^[i]
(R) :=
^
(R) ∩Cl[i](V, q),
(271)
^(≥1)
(R) := span {f1 · · · fk | k ≥1, fl ∈R, l = 1, . . . , k} ,
(272)
^×
(R) := F× +
^(≥1)
(R),
(273)
^[×]
(R) := F× + span {f1 · · · fk | k ≥2 even, fl ∈R, l = 1, . . . , k} ,
(274)
^∗
(R) := 1 +
^(≥1)
(R),
(275)
^[∗]
(R) := 1 + span {f1 · · · fk | k ≥2 even, fl ∈R, l = 1, . . . , k} .
(276)
Here, F× denotes the set of invertible elements of F.
Lemma D.41.
1. For every h ∈V(≥1)(R) there exists a k ≥0 such that:
hk+1 = 0.
(277)
In particular, no h ∈V(≥1)(R) is ever invertible.
44

2. Every y ∈V×(R) = V(R) \ V(≥1)(R) is invertible. Its inverse is given by:
y−1 = c−1  1 −h + h2 −· · · + (−1)khk
,
(278)
where we write: y = c · (1 + h) with c ∈F×, h ∈V(≥1)(R), and k is such that hk+1 = 0.
3. In particular, we get:
^×
(R) =
^
(R) ∩Cl×(V, q).
(279)
Proof. Items 1 and 3 are clear. For item 2 we refer to Example E.21.
Lemma D.42 (Twisted commutation relationships).
1. For every f ∈R and v ∈V we have
the anticommutation relationship:
fv = −vf + 2 b(f, v)
| {z }
=0
= −vf.
(280)
2. For every f ∈R and x ∈Cl(V, q) we get the following twisted commutation relationship:
α(x)f = (x[0] −x[1])f = f(x[0] + x[1]) = fx.
(281)
3. For every y ∈V(R) and every x ∈Cl[0](V, q) we get:
xy = yx.
(282)
4. For every y ∈V(R) and v ∈V we get:
α(y)v = (y[0] −y[1])v = v(y[0] + y[1]) = vy.
(283)
5. For every y ∈V[0](R) (of even parity) and every x ∈Cl(V, q) we get:
yx = xy.
(284)
Remark D.43. A direct consequence from Lemma D.42 is that the even parity parts: V[0](R),
V[×](R) and V[∗](R) all lie in the center of Cl(V, q), which we denote by Z(Cl(V, q)):
^[∗]
(R) ⊆
^[×]
(R) ⊆
^[0]
(R) ⊆Z(Cl(V, q)),
(285)
i.e. every y ∈V[0](R) (of even parity) commutes with every x ∈Cl(V, q). For more detail we refer
to Theorem D.47.
In the following we will study the center of the Clifford algebra Z(Cl(V, q)) more carefully.
D.8
The Center of the Clifford Algebra
In the following final subsections, we study additional properties of the Clifford algebra that will aid
us in studying group representations and actions on the algebra in the upcoming sections.
Let (V, q) be a quadratic space over a field F with char(F) ̸= 2 and dim V = n < ∞.
Definition D.44 (The center of an algebra). The center of an algebra A is defined to be:
Z(A) := {z ∈A | ∀x ∈A. xz = zx} .
(286)
Lemma D.45. Let e1, . . . , en be an orthogonal basis of (V, q). For A ⊆[n] := {1, . . . , n} let
eA := Q<
i∈A ei be the product in Cl(V, q) in increasing index order, e∅:= 1. Then we get for two
subsets A, B ⊆[n]:
eAeB = (−1)|A|·|B|−|A∩B| · eBeA.
(287)
45

In particular, for j /∈A we get:
eAej = (−1)|A| · ejeA,
(288)
and:
eAej −ejeA = ((−1)|A| −1) · ejeA = (−1)t((−1)|A|+1 + 1)eA ˙∪{j},
(289)
where t is the position of j in the ordered set A ˙∪{j}.
For i ∈A we get:
eAei = (−1)|A|−1 · eieA = (−1)|A|−sq(ei)eA\{i},
(290)
and:
eAei −eieA = (−1)s((−1)|A| + 1)q(ei)eA\{i},
(291)
where s is the position of i in the ordered set A.
Proof. Let B := {j1, . . . , j|B|} ⊆[n].
eAeB =
<
Y
i∈A
ei
<
Y
j∈B
ej
(292)
= (−1)|A|−1[j1∈A]ej1
<
Y
i∈A
ei
<
Y
j∈B\j1
ej
(293)
= (−1)|B||A|−P
j∈B 1[j∈A]eBeA
(294)
= (−1)|B||A|−|A∩B|eBeA
(295)
For the other two identities, we similarly make use of the fundamental Clifford identity.
Lemma D.46. Let (V, q) be a quadratic space with dim V = n < ∞. Let e1, . . . , en be an
orthogonal basis for (V, q). For x ∈Cl(V, q) we have the equivalence:
x ∈Z(Cl(V, q))
⇐⇒
∀i ∈[n].
xei = eix.
(296)
Proof. This is clear as Cl(V, q) is generated by ek1 · · · ekl.
Theorem D.47 (The center of the Clifford algebra). Let (V, q) be a quadratic space with dim V =
n < ∞, char F ̸= 2, and let R ⊆V be the radical subspace of (V, q). Then for the center of Cl(V, q)
we have the following cases:
1. If n is odd then:
Z(Cl(V, q)) =
^[0]
(R) ⊕Cl(n)(V, q).
(297)
2. If n is even then:
Z(Cl(V, q)) =
^[0]
(R).
(298)
In all cases we have:
^[0]
(R) ⊆Z(Cl(V, q)).
(299)
Proof. Let e1, . . . , en be an orthogonal basis for (V, q). The statement can then equivalently be
expressed as:
46

1. If n is odd or q = 0 (on all vectors), then:
Z(Cl(V, q)) = span

1, eA, e[n]
 |A| even , ∀i ∈A. q(ei) = 0
	
(300)
=
^[0]
(R) + Cl(n)(V, q).
(301)
2. If n is even and q ̸= 0 (on some vector), then:
Z(Cl(V, q)) = span {1, eA | |A| even , ∀i ∈A. q(ei) = 0}
(302)
=
^[0]
(R).
(303)
Note that if n is even and q = 0 then both points would give the same answer, as then V = R and
thus:
Cl(n)(V, q) =
^(n)
(R) ⊆
^[0]
(R).
(304)
We first only consider basis elements eA with A ⊆[n] and check when we have eA ∈Z(Cl(V, q)).
We now consider three cases:
1. A = ∅. We always have: e∅= 1 ∈Z(Cl(V, q)).
2. A = [n]. For every i ∈[n] we have:
e[n]ei = (−1)n−1 · eie[n] = ±q(ei) · e[n]\{i}.
(305)
So e[n] ∈Z(Cl(V, q)) iff either n is odd or q = 0 (on all ei).
3. ∅̸= A ⊊[n]. Note that for i ∈A we have:
eAei = (−1)|A|−1 · eieA = ±q(ei) · eA\{i},
(306)
while for j /∈A we have:
eAej = (−1)|A| · ejeA.
(307)
The latter implies that for eA ∈Z(Cl(V, q)) to hold, |A| necessarily needs to be even. Since
in that case (−1)|A|−1 = −1, we necessarily need by the former case that for every i ∈A,
q(ei) = 0.
Now consider a linear combination x = P
A⊆[n] cA · eA ∈Cl(V, q) with cA ∈F. Then abbreviate:
Z := span {eA | A ⊆[n], eA ∈Z(Cl(V, q))} ⊆Z(Cl(V, q)).
(308)
We need to show that x ∈Z. We thus write:
x = y + z,
y :=
X
A⊆[n]
eA /∈Z
cA · eA,
z :=
X
A⊆[n]
eA∈Z
cA · eA ∈Z.
(309)
So with x, z ∈Z(Cl(V, q)) also y ∈Z(Cl(V, q)) and we are left to show that y = 0.
First note that, since e∅= 1 ∈Z, there is no e∅-component in y.
We now have for every i ∈[n]:
0 = yei −eiy
(310)
=
X
A⊆[n]
eA /∈Z
cA · (eAei −eieA)
(311)
=
X
A⊆[n]
eA /∈Z
i∈A
cA · (eAei −eieA) +
X
A⊆[n]
eA /∈Z
i/∈A
cA · (eAei −eieA)
(312)
=
X
A⊆[n]
eA /∈Z
i∈A
±((−1)|A| + 1)q(ei)cA · eA\{i} +
X
A⊆[n]
eA /∈Z
i/∈A
±((−1)|A|+1 + 1)cA · eA ˙∪{i}.
(313)
47

Note that for A, B ⊆[n] with A ̸= B we always have:
A \ {i} ̸= B \ {i} ,
if i ∈A, i ∈B,
(314)
A \ {i} ̸= B ˙∪{i} ,
if i ∈A, i /∈B,
(315)
A ˙∪{i} ̸= B \ {i} ,
if i /∈A, i ∈B,
(316)
A ˙∪{i} ̸= B ˙∪{i} ,
if i /∈A, i /∈B.
(317)
So the above representation for yei −eiy is already given in basis form. By their linear independence
we then get that for every A ⊆[n] with eA /∈Z and every i ∈[n]:
0 = ((−1)|A| + 1)q(ei)cA,
for i ∈A,
(318)
0 = ((−1)|A|+1 + 1)cA,
for i /∈A.
(319)
First consider the case e[n] /∈Z. By the previous result we then know that n is even and q ̸= 0. So
there exists ei with q(ei) ̸= 0. So the above condition for i ∈[n] then reads:
0 = 2q(ei)c[n],
for i ∈[n],
(320)
which implies c[n] = 0 as q(ei) ̸= 0. So y does not have a e[n]-component.
Similarly, for A ⊆[n] with A ̸= [n] and eA /∈Z and |A| odd there exists i /∈A and the above
condition reads:
0 = 2cA,
for i /∈A,
(321)
which implies cA = 0. So y does not have any eA-components with odd |A|.
Now let A ⊆[n] with A ̸= [n] and eA /∈Z and |A| even. Then by our previous analysis we know
that there exists i ∈A with q(ei) ̸= 0. Otherwise eA ∈Z. So the above condition reads:
0 = 2q(ei)cA,
for i ∈A,
(322)
which implies cA = 0 as q(ei) ̸= 0. This shows that y does not have any eA-component with even
|A|.
Overall, this shows that y = 0 and thus the claim.
D.9
The Twisted Center of the Clifford Algebra
Notation D.48. Let e1, . . . , en be an orthogonal basis of (V, q). For A ⊆[n] := {1, . . . , n} let
eA := Q<
i∈A ei be the product in Cl(V, q) in increasing index order, e∅:= 1. Then (eA)A⊆[n] forms
a basis for Cl(V, q).
Definition D.49 (The twisted center of a Z/2Z-graded algebra). We define the twisted center of a
Z/2Z-graded algebra A as the following subset:
K(A) :=
n
y ∈A
 ∀x ∈A. yx[0] + (y[0] −y[1])x[1] = xy
o
.
(323)
Theorem D.50. We have the following identification of the twisted center with the radical subalgebra
of the Clifford algebra Cl(V, q) and the set:
K(Cl(V, q)) =
^
(R) = {y ∈Cl(V, q) | ∀v ∈V. α(y)v = vy} .
(324)
Proof. Let y ∈V(R) then by Lemma D.42 we get:
yx[0] + α(y)x[1] = x[0]y + x[1]y = xy.
(325)
This shows that:
y ∈K(Cl(V, q)),
(326)
and thus:
^
(R) ⊆K(Cl(V, q)).
(327)
48

Note that the following inclusion is clear as V ⊆Cl(V, q):
K(Cl(V, q)) ⊆{y ∈Cl(V, q) | ∀v ∈V. α(y)v = vy} .
(328)
For the final inclusion, let y = P
B⊆[n] cB · eB ∈Cl(V, q) such that for all v ∈V we have
α(y)v = vy. Then for all orthogonal basis vectors ei we get the requirement:
eiy = α(y)ei,
(329)
which always holds if q(ei) = 0, and is only a condition for q(ei) ̸= 0. For such ei we get:
ei
 X
B⊆[n]
|B| even
i∈B
cB · eB +
X
B⊆[n]
|B| even
i/∈B
cB · eB +
X
B⊆[n]
|B| odd
i∈B
cB · eB +
X
B⊆[n]
|B| odd
i/∈B
cB · eB
!
(330)
= eiy
(331)
= α(y)ei
(332)
= α
 X
B⊆[n]
|B| even
i∈B
cB · eB +
X
B⊆[n]
|B| even
i/∈B
cB · eB +
X
B⊆[n]
|B| odd
i∈B
cB · eB +
X
B⊆[n]
|B| odd
i/∈B
cB · eB
!
ei
(333)
=
 X
B⊆[n]
|B| even
i∈B
cB · eB +
X
B⊆[n]
|B| even
i/∈B
cB · eB −
X
B⊆[n]
|B| odd
i∈B
cB · eB −
X
B⊆[n]
|B| odd
i/∈B
cB · eB
!
ei
(334)
= ei
 
−
X
B⊆[n]
|B| even
i∈B
cB · eB +
X
B⊆[n]
|B| even
i/∈B
cB · eB −
X
B⊆[n]
|B| odd
i∈B
cB · eB +
X
B⊆[n]
|B| odd
i/∈B
cB · eB
!
.
(335)
Since ei with q(ei) ̸= 0 is invertible, we can cancel ei on both sides and make use of the linear
independence of (eB)B⊆[n] to get that:
cB = 0
if
i ∈B.
(336)
Since for given B this can be concluded from every ei with q(ei) ̸= 0 we can only have cB ̸= 0 if
q(ej) = 0 for all j ∈B. Note that elements ej with q(ej) = 0 that are part of an orthogonal basis
satisfy ej ∈R. This shows that:
y =
X
B⊆[n]
∀j∈B. q(ej)=0
cB · eB ∈span {eA | A ⊆[n], ∀i ∈A. ei ∈R} =
^
(R).
(337)
This shows the remaining inclusion:
{y ∈Cl(V, q) | ∀v ∈V. α(y)v = vy} ⊆
^
(R).
(338)
This shows the equality of all three sets.
E
The Clifford Group and its Clifford Algebra Representations
We saw that Cartan-Dieudonné (Theorem C.13) generates the orthogonal group of a (non-degenerate)
quadratic space by composing reflections. Considering this, we seek in the following a group
representation that acts on the entire Clifford algebra, but reduces to a reflection when restricted to
V . Further, we ensure that the action is an algebra homomorphism and will therefore respect the
geometric product.
49

E.1
Adjusting the Twisted Conjugation
Recall the notation Cl×(V, q) := {x ∈Cl(V, q) | ∃y ∈Cl(V, q). xy = yx = 1}.
Motivation E.1 (Generalizing reflection operations). For v, w ∈V with q(w) ̸= 0 the reflection of v
onto the hyperplane that is normal to w is given by the following formula, which we then simplify:
rw(v) = v −2 b(w, v)
b(w, w)w
(339)
= wwv/q(w) −2b(w, v)
q(w) w
(340)
= −w(−wv + 2b(w, v))/q(w)
(341)
= −wvw/q(w)
(342)
= −wvw−1.
(343)
So we have rw(v) = −wvw−1 for v, w ∈V with q(w) ̸= 0. We would like to generalize this to
an operation ρ for w from a subgroup of Γ ⊆Cl×(V, q) (as large as possible) onto all elements
x ∈Cl(V, q). More explicitely, we want for all w1, w2 ∈Γ and x1, x2 ∈Cl(V, q), v, w ∈V ,
q(w) ̸= 0:
ρ(w)(v) = −wvw−1 = rw(v),
(344)
(ρ(w2) ◦ρ(w1)) (x1) = ρ(w2w1)(x1),
(345)
ρ(w1)(x1 + x2) = ρ(w1)(x1) + ρ(w1)(x2),
(346)
ρ(w1)(x1x2) = ρ(w1)(x1)ρ(w1)(x2).
(347)
The second condition makes sure that Cl(V, q) will be a group representation of Γ, i.e. we get a group
homomorphism:
ρ : Γ →Aut(Cl(V, q)),
(348)
where Aut(Cl(V, q)) denotes the set of automorphisms Cl(V, q) →Cl(V, q).
In the literature, the following versions of reflection operations were studied:
ρ0(w) : x 7→wxw−1,
ρ1(w) : x 7→α(w)xw−1,
ρ2(w) : x 7→wxα(w)−1.
(349)
Since ρ0(w) is missing the minus sign it only generalizes compositions of reflections for elements
w = w1 · · · wk, wl ∈V , q(vl) ̸= 0, of even parity k. The map ρ1(w), on the other hand, takes
the minus sign into account and generalizes also to such elements w = w1 · · · wk of odd party
k = prt(w) as then: α(w) = (−1)prt(w)w. However, in contrast to ρ0(w), which is an algebra
homomorphism for all w ∈Cl×(V, q), the map ρ1(w) is not multiplicative in x, as can be seen with
v1, v2 ∈V and w ∈V with q(w) ̸= 0:
ρ1(w)(v1v2) = α(w)v1v2w−1
(350)
= (−1)prt(w)wv1w−1(−1)prt(w)(−1)prt(w)wv2w−1
(351)
= (−1)prt(w)ρ1(w)(v1)ρ1(w)(v2)
(352)
̸= ρ1(w)(v1)ρ1(w)(v2).
(353)
The lack of multiplicativity means that reflection and taking geometric product does not commute.
To fix this, it makes sense to first restrict ρ1(w) to V , where it coincides with rw and also still is mul-
tiplicative in w, and then study under which conditions on w it extends to an algebra homomorphism
Cl(V, q) →Cl(V, q).
More formally, by the universal property of the Clifford algebra, the obstruction for:
ρ(w) : V →Cl(V, q),
ρ(w)(v) := α(w)vw−1 = wη(w)vw−1,
η(w) := w−1α(w),
(354)
with general invertible w ∈Cl×(V, q), to extend to an F-algebra homomorphism:
ρ(w) : Cl(V, q) →Cl(V, q),
(355)
50

is the following:
∀v ∈V.
q(v)
!= (ρ(w)(v))2
(356)
= ρ(w)(v)ρ(w)(v)
(357)
= (wη(w)vw−1)(wη(w)vw−1)
(358)
= wη(w)vη(w)vw−1,
(359)
which reduces to:
∀v ∈V.
q(v)
!= η(w)vη(w)v.
(360)
The latter is, for instance, satisfied if η(w) commutes with every v ∈V and 1 = η(w)2. In particular,
the above requirement is satisfied for all w ∈Cl×(V, q) with η(w) ∈{±1}, which is equivalent to
α(w) = ±w, which means that w is a homogeneous element of Cl(V, q) in the parity grading. This
discussion motivates the following definitions and analysis.
Notation E.2 (The coboundary of α). The coboundary η of α is defined on w ∈Cl×(V, q) as follows:
η : Cl×(V, q) →Cl×(V, q),
η(w) := w−1α(w).
(361)
Remark E.3.
1. η is a crossed group homomorphism (aka 1-cocycle), i.e. for w1, w2 ∈
Cl×(V, q) we have:
η(w1w2) = η(w1)w2η(w2),
with
η(w1)w2 := w−1
2 η(w1)w2.
(362)
2. For w ∈Cl×(V, q) we have:
α(η(w)) = α(w)−1w = η(w)−1.
(363)
3. For w ∈Cl×(V, q) we have that w is an homogeneous element in Cl(V, q), in the sense of
parity, if and only if η(w) ∈{±1}.
Definition E.4 (The group of homogeneous invertible elements). With the introduced notation we
can define the group of all invertible elements of Cl(V, q) that are also homogeneous (in the sense of
parity) as:
Cl[×](V, q) :=

Cl×(V, q) ∩Cl[0](V, q)

∪

Cl×(V, q) ∩Cl[1](V, q)

(364)
=

w ∈Cl×(V, q)
 η(w) ∈{±1}
	
.
(365)
Notation E.5 (The main involution - revisited). We now make the following abbreviations:
α0 := id : Cl(V, q) →Cl(V, q),
α0(x) := x[0] + x[1] = x,
(366)
α1 := α : Cl(V, q) →Cl(V, q),
α1(x) := x[0] −x[1].
(367)
For w ∈Cl(V, q) we then have:
αprt(w) : Cl(V, q) →Cl(V, q),
αprt(w)(x) = x[0] + (−1)prt(w)x[1].
(368)
Note that αprt(w) is an F-algebra involution of Cl(V, q) that preserves the parity grading of Cl(V, q).
We also need the following slight variation αw, which in many, but not all cases, coincides with
αprt(w). For w ∈Cl×(V, q) we define the w-twisted map:
αw : Cl(V, q) →Cl(V, q),
αw(x) := x[0] + η(w)x[1].
(369)
Remark E.6. Note that for w ∈Cl[×](V, q) we have that η(w) = (−1)prt(w) and thus αw = αprt(w),
in which case αw is an F-algebra involution of Cl(V, q) that preserves the parity grading of Cl(V, q).
Definition E.7 (Adjusted twisted conjugation). For w ∈Cl×(V, q) we define the twisted conjugation:
ρ(w) : Cl(V, q) →Cl(V, q),
ρ(w)(x) := wx[0]w−1 + α(w)x[1]w−1.
(370)
= w

x[0] + η(w)x[1]
w−1
(371)
= wαw(x)w−1.
(372)
51

We now want to re-investigate the action of ρ on Cl(V, q).
Lemma E.8. For every w ∈Cl[×](V, q) the map:
ρ(w) : Cl(V, q) →Cl(V, q),
x 7→ρ(w)(x) = w

x[0] + η(w)x[1]
w−1,
(373)
is an F-algebra automorphism that preserves the parity grading of Cl(V, q). Its inverse it given by
ρ(w−1).
Proof. First note that αw agrees with the involution αprt(w) for w ∈Cl[×](V, q). Since αprt(w) is an
F-algebra automorphism that preserves the parity grading of Cl(V, q) so is αw for w ∈Cl[×](V, q).
Furthermore, the conjugation ρ0(w) : x 7→wxw−1 is an F-algebra automorphism, which preserves
the parity grading of Cl(V, q) if w ∈Cl[×](V, q). To see this, note that 0 = prt(1) = prt(ww−1) =
prt(w) + prt(w−1). As such, prt(w) = prt(w−1). Then, prt(wxw−1) = prt(w) + prt(x) +
prt(w−1) = 2 prt(w) + prt(x) = prt(x). Here, we use the fact that the Clifford algebra is Z/2Z-
graded.
So, their composition ρ(w) = ρ0(w) ◦αw is also an F-algebra automorphism that preserves the
parity grading Cl(V, q) for w ∈Cl[×](V, q).
As a direct corollary we get:
Corollary E.9. Let F(T0,1, . . . , T1,ℓ) ∈F[T0,1, . . . , T1,ℓ] be a polynomial in 2ℓvariables with
coefficients in F. Let x1, . . . , xℓ∈Cl(V, q) be ℓelements of the Clifford algebra and w ∈Cl[×](V, q)
be a homogeneous invertible element of Cl(V, q). Then we have the following equivariance property:
ρ(w)

F(x[0]
1 , . . . , x[i]
l , . . . , x[1]
ℓ)

= F(ρ(w)(x1)[0], . . . , ρ(w)(xl)[i], . . . , ρ(w)(xℓ)[1]).
(374)
Proof. This directly follows from Lemma E.8.
Futhermore, we get the following result:
Theorem E.10. The map:
ρ : Cl[×](V, q) →AutAlg,prt (Cl(V, q)) ,
w 7→ρ(w),
(375)
is a well-defined group homomorphism from the group of all homogeneous invertible elements of
Cl(V, q) to the group of F-algebra automorphisms of Cl(V, q) that preserve the parity grading of
Cl(V, q). In particular, Cl(V, q), Cl[0](V, q), Cl[1](V, q) are group representations of Cl[×](V, q) via
ρ.
Proof. By the previous Lemma E.8 we already know that ρ is a well-defined map. We only need to
check if it is a group homomorphism. Let w1, w2 ∈Cl[×](V, q) and x ∈Cl(V, q), then we get:
(ρ(w2) ◦ρ(w1)) (x) = ρ(w2)(ρ(w1)(x))
(376)
= ρ(w2)

w1αprt(w1)(x)w−1
1

(377)
= w2αprt(w2) 
w1αprt(w1)(x)w−1
1

w−1
2
(378)
= w2αprt(w2)(w1)αprt(w2)(αprt(w1)(x))αprt(w2)(w1)−1w−1
2
(379)
= w2(−1)prt(w2) prt(w1)w1αprt(w2)+prt(w1)(x)(−1)prt(w2) prt(w1)w−1
1 w−1
2
(380)
= w2w1αprt(w2)+prt(w1)(x)w−1
1 w−1
2
(381)
= (w2w1)αprt(w2w1)(x)(w2w1)−1
(382)
= ρ(w2w1)(x),
(383)
52

where we used the multiplicativity of αprt(w)(x):
αw(x)αw(y) =

(−1)prt(w)x[1] + x[0]
 
(−1)prt(w)y[1] + y[0]
(384)
= (−1)prt(w) 
x[0]y[1] + x[1]y[0]
+ x[1]y[1] + x[0]y[0]
(385)
= αw 
(xy)[1]
+ (xy)[0]
(386)
= αw(xy).
(387)
This implies:
ρ(w2) ◦ρ(w1) = ρ(w2w1),
(388)
which shows the claim.
Finally, we want to re-check that our newly defined ρ, despite its different appearance, still has the
proper interpretation of a reflection.
Remark E.11. Let w, v ∈V with q(w) ̸= 0. Then ρ(w)(v) is the reflection of v w.r.t. the hyperplane
that is normal to w:
ρ(w)(v) = wαw(v)w−1 = −wvw−1 = rw(v).
(389)
Remark E.12. The presented results in this subsection can be slightly generalized as follows. If
w ∈Cl×(V, q) such that η(w) ∈V(R) then by Theorem D.50 we get for all v ∈V :
vη(w) = α(η(w))v = η(w)−1v.
(390)
This implies that for all v ∈V :
η(w)vη(w)v = η(w)η(w)−1vv = q(v),
(391)
and thus for all v ∈V :
(α(w)vw−1)(α(w)vw−1) = q(v).
(392)
By the universal property of the Clifford algebra the map:
ρ(w) : V →Cl(V, q),
v 7→α(w)vw−1,
(393)
uniquely extends to an F-algebra homomorphism:
ρ(w) : Cl(V, q) →Cl(V, q),
(394)
with:
x = c0 +
X
i∈I
ci · vi,1 · · · vi,ki
(395)
7→c0 +
X
i∈I
ci · α(w)vi,1w−1 · · · α(w)vi,kiw−1
(396)
= w
 
c0 +
X
i∈I
ci · η(w)vi,1 · · · η(w)vi,ki
!
w−1
(397)
= w


c0 +
X
i∈I
ki even
ci · vi,1 · · · vi,ki + η(w) ·
X
i∈I
ki odd
ci · vi,1 · · · vi,ki


w−1
(398)
= w

x[0] + η(w) · x[1]
w−1.
(399)
To further ensure that the elements w ∈Cl×(V, q) with the above property form a group we
might need to further restrict to require that η(w) ∈V[×](R). At least in this case we get for
w1, w2 ∈Cl×(V, q) with η(w1), η(w2) ∈V[×](R) ⊆Z(Cl(V, q)), see Remark D.43, that:
η(w2w1) = w−1
1 η(w2)w1η(w1) = w−1
1 w1η(w2)η(w1) = η(w2)η(w1) ∈
^[×]
(R).
(400)
So the following set:
C :=

w ∈Cl×(V, q)
 η(w) ∈
^[×]
(R)

(401)
is a subgroup of Cl×(V, q) where every w ∈C defines an algebra homomorphism:
ρ(w) : Cl(V, q) →Cl(V, q),
ρ(w)(x) = w

x[0] + η(w)x[1]
w−1.
(402)
53

E.2
The Clifford Group
Motivation E.13. We have seen in the last section that if we choose homogeneous invertible elements
w ∈Cl[×](V, q) then the action ρ(w), the (adjusted) twisted conjugation, is an algebra automorphism
of Cl(V, q) that also preserves the parity grading of Cl(V, q), in particular, ρ(w) is linear and
multiplicative.
We now want to investigate under which conditions on w the algebra automorphism ρ(w) also
preserves the multivector grading:
Cl(V, q) = Cl(0)(V, q) ⊕Cl(1)(V, q) ⊕· · · ⊕Cl(m)(V, q) ⊕· · · ⊕Cl(n)(V, q).
(403)
If this was the case then each component Cl(m)(V, q) would give rise to a corresponding group
representation.
To preserve the multivector grading we at least need that for v ∈V = Cl(1)(V, q) we have that
also ρ(w)(v) ∈Cl(1)(V, q) = V . We will see that for w ∈Cl×(V, q) such that ρ(w) is an algebra
homomorphism, i.e. for w homogeneous and invertible, and such that ρ(w)(v) ∈V for all v ∈V ,
we already get the preservation of the whole multivector grading of Cl(V, q).
Again, let (V, q) be a quadratic space over a field F with char(F) ̸= 2 and dim V = n < ∞and
e1, . . . , en and orthogonal basis of (V, q).
Remark E.14. In the following, we elaborate on the term Clifford group in constrast to previous
literature. First, the unconstrained Clifford group is also referred to as the Lipschitz group or
Clifford-Lipschitz group in honor of its creator Rudolf Lipschitz [LS09]. Throughout its inventions,
several generalizations and versions have been proposed, often varying in details, leading to slightly
non-equivalent definitions. For instance, some authors utilize conjugation as an action, others apply
the the twisted conjugation, while yet another group employs the twisting on the other side of the
conjugation operation. Furthermore, some authors require that the elements are homogeneous in the
parity grading, where others do not. We settle for a definition that requires homogeneous invertible
elements of the Clifford algebra that act via our adjusted twisted conjugation such that elements
from the vector space V land also in V . The reason for our definition is that we want the adjusted
twisted conjugation to act on the whole Clifford algebra Cl(V, q), not just on the vector space V .
Furthermore, we want it to respect, besides the vector space structure of Cl(V, q), also the product
structure, leading to algebra homomorphisms. In addition, we also want that the action to respect the
extended bilinear form b, the orthogonal structure, and the multivector grading. These properties
might not (all) be ensured in other definitions with different nuances.
Also note that the name Clifford group might be confused with different groups with the same name in
other literature, e.g., with the group of unitary matrices that normalize the Pauli group or with the
finite group inside the Clifford algebra that is generated by an orthogonal basis via the geometric
product.
Definition E.15.
1. We denote the unconstrained Clifford group of Cl(V, q) as follows:
˜Γ(V, q) :=

w ∈Cl×(V, q)
 ∀v ∈V. ρ(w)(v) ∈V
	
.
(404)
2. We denote the Clifford group of Cl(V, q) as follows:
Γ(V, q) := Cl[×](V, q) ∩˜Γ(V, q)
(405)
=

w ∈Cl×(V, q)
 η(w) ∈{±1} ∧∀v ∈V. ρ(w)(v) ∈V
	
.
(406)
3. We define the special Clifford group as follows:
Γ[0](V, q) := ˜Γ(V, q) ∩Cl[0](V, q) = Γ(V, q) ∩Cl[0](V, q).
(407)
Theorem E.16. For w ∈Γ(V, q) and x ∈Cl(V, q) we have for all m = 0, . . . , n:
ρ(w)(x(m)) = ρ(w)(x)(m).
(408)
In particular, for x ∈Cl(m)(V, q) we also have ρ(w)(x) ∈Cl(m)(V, q).
54

Proof. We first claim that for w ∈Γ(V, q) the set of elements:
b1 := ρ(w)(e1), . . . , bn := ρ(w)(en),
(409)
forms an orthogonal basis of (V, q). Indeed, since ρ(w)(et) ∈V , by definition of Γ(V, q), the
orthogonality relation, i ̸= j:
0 = 2b(ei, ej) = eiej + ejei,
(410)
transforms under ρ(w) to:
0 = ρ(w)(0) = ρ(w) (eiej + ejei)
(411)
= ρ(w)(ei)ρ(w)(ej) + ρ(w)(ej)ρ(w)(ei)
(412)
= bibj + bjbi
(413)
= 2b(bi, bj).
(414)
This shows that b1, . . . , bn is an orthogonal system in V . Using ρ(w−1) we also see that the system
is linear independent and thus an orthognal basis of V . By the basis-independence of the multivector
grading Cl(m)(V, q), see Theorem D.27, we then get for:
x =
X
i1<···<im
ci1,...,im · ei1 · · · eim ∈Cl(m)(V, q),
(415)
the relation:
ρ(w)(x) =
X
i1<···<im
ci1,...,im · bi1 · · · bim ∈Cl(m)(V, q).
(416)
This shows the claim.
Corollary E.17. The map:
ρ : Γ(V, q) →AutAlg,grd (Cl(V, q)) ,
w 7→ρ(w),
(417)
is a well-defined group homomorphism from the Clifford group to the group of F-algebra automor-
phisms of Cl(V, q) that preserve the multivector grading of Cl(V, q). In particular, Cl(V, q) and
Cl(m)(V, q) for m = 0, . . . , n, are group representations of Γ(V, q) via ρ.
Corollary E.18. Let F(T1, . . . , Tℓ) ∈F[T1, . . . , Tℓ] be a polynomial in ℓvariables with coefficients
in F and let k ∈{0, . . . , n}. Further, consider ℓelements x1, . . . , xℓ∈Cl(V, q). Then for every
w ∈Γ(V, q) we get the equivariance property:
ρ(w)

F(x1, , . . . , xℓ)(k)
= F(ρ(w)(x1), . . . , ρ(w)(xℓ))(k),
(418)
where the superscript (k) indicates the projection onto the multivector grade-k-part of the whole
expression.
Example E.19. Let w ∈V with q(w) ̸= 0, then w ∈Γ(V, q).
Proof. It is clear that w is homogeneous in the parity grading, as η(w) = −1. For v ∈V we get:
ρ(w)(v) = rw(v) = v −2b(v, w)
q(w) w ∈V.
(419)
This shows w ∈Γ(V, q).
Example E.20. Let e, f ∈V with b(v, f) = 0 for all v ∈V , and put: γ := 1 + ef ∈Cl[0](V, q).
Then γ ∈Γ(V, q).
Proof. First, note that for all v ∈V we get:
fv = −vf + 2b(f, v) = −vf.
(420)
Next, we see that:
γ−1 = 1 −ef.
(421)
55

Indeed, we get:
(1 + ef)(1 −ef) = 1 + ef −ef −efef
(422)
= 1 + effe
(423)
= 1 + q(f)
|{z}
=0
q(e)
(424)
= 1.
(425)
Now consider v ∈V . We then have:
ρ(γ)(v) = α(γ)vγ−1
(426)
= (1 + ef)v(1 −ef)
(427)
= (1 + ef)(v −vef)
(428)
= v + efv −vef −efvef
(429)
= v −evf −vef −eveff
(430)
= v −(ev + ve)f −q(f)
|{z}
=0
eve
(431)
= v −2b(e, v)f
(432)
∈V.
(433)
This shows γ ∈Γ(V, q).
Example E.21. Let f1, . . . , fr ∈V be a basis of the radical subspace R of (V, q). In particular, we
have b(v, fj) = 0 for all v ∈V . Then we put:
g := 1 + h,
with
h
∈span {fk1 · · · fks | s ≥2 even, 1 ≤k1 < k2 < . . . ks ≤r} ⊆Cl(V, q).
(434)
Then we claim that g ∈Γ(V, q) and ρ(g) = idCl(V,q).
Proof. Since we restrict to even products it is clear that g ∈Cl[0](V, q). Furthermore, note that, since
h lives in the radical subalgebra, there exists a number k ≥1 such that:
hk+1 = 0.
(435)
Then we get that:
g−1 = 1 −h + h2 + · · · + (−1)khk.
(436)
Indeed, we get:
(1 + h)
 k
X
l=0
(−1)lhl
!
=
k
X
l=0
(−1)lhl +
k
X
l=0
(−1)lhl+1
(437)
= 1 +
k
X
l=1
(−1)lhl −
k
X
l=1
(−1)lhl + (−1)k hk+1
|{z}
=0
(438)
= 1.
(439)
Furthermore, g lies in the center Z(Cl(V, q)) of Cl(V, q) by Theorem D.47. Then for v ∈V we get:
ρ(g)(v) = α(g)vg−1
(440)
= gvg−1
(441)
= vgg−1
(442)
= v
(443)
∈V.
(444)
So, g ∈Γ(V, q) and acts as the identity on V , and thus on Cl(V, q), via ρ.
56

E.3
The Structure of the Clifford Group
We have identified the Clifford group and its action on the algebra. In particular, our adjusted twisted
conjugation preserves the parity and multivector grading, and reduces to a reflection when restricted
to V . We now want to further investigate how the Clifford and the Clifford groups act via the twisted
conjugation ρ on V and Cl(V, q). We again denote by R ⊆V the radical subspace of V w.r.t. q. We
follow and extend the analysis of [Cru80, Cru90, DKL10].
We first investigate the kernel of the twisted action.
Corollary E.22 (The kernel of the twisted conjugation).
1. We have the following identity for
the twisted conjugation:
ker
 ρ|Cl×(V,q)

:=

w ∈Cl×(V, q)
 ρ(w) = idCl(V,q)
	 !=
^×
(R).
(445)
2. For the twisted conjugation restricted to the unconstrained Clifford, group and to V the
kernel is given by:
ker

ρ : ˜Γ(V, q) →GL(V )

:=
n
w ∈˜Γ(V, q)
 ρ(w)|V = idV
o !=
^×
(R).
(446)
3. For the twisted conjugation restricted to the Clifford group, the kernel is given by:
ker (ρ : Γ(V, q) →AutAlg(Cl(V, q))) :=

w ∈Γ(V, q)
 ρ(w) = idCl(V,q)
	
(447)
!=
^[×]
(R).
(448)
Proof. This follows directly from the characterizing of the twisted center of Cl(V, q) by Theorem
D.50. Note that we have for w ∈Cl×(V, q):
ρ(w) = idCl(V,q) ⇐⇒∀x ∈Cl(V, q). ρ(w)(x) = x
(449)
⇐⇒∀x ∈Cl(V, q). wx[0]w−1 + α(w)x[1]w−1 = x
(450)
⇐⇒∀x ∈Cl(V, q). wx[0] + α(w)x[1] = xw
(451)
⇐⇒w ∈K(Cl(V, q)) =
^
(R).
(452)
From this follows that:
w ∈Cl×(V, q) ∩
^
(R) =
^×
(R).
(453)
The other points follow similarly with Theorem D.50.
For the last point also note:
Γ(V, q) = ˜Γ(V, q) ∩Cl[×](V, q),
^×
(R) ∩Cl[×](V, q) =
^[×]
(R).
(454)
This shows the claims.
Lemma E.23.
1. For every w ∈Cl×(V, q) we have:
ρ(w)|R = idR.
(455)
2. For every w ∈Cl[×](V, q) we have:
ρ(w)|V(R) = idV(R).
(456)
3. For every g ∈V×(R) we have:
ρ(g)|V = idV .
(457)
4. For every g ∈V[×](R) we have:
ρ(g)|Cl(V,q) = idCl(V,q).
(458)
57

Proof. This directly follows from the twisted commutation relationship, see Lemma D.42. For
w ∈Cl×(V, q) and f ∈R ⊆V we have:
ρ(w)(f) = α(w)fw−1 = fww−1 = f.
(459)
For w ∈Cl[×](V, q) the map ρ(w) is an algebra automorphism of Cl(V, q) and satisfies ρ(w)(f) = f
for all f ∈R. So for every y ∈V(R) we have:
ρ(w)(y) = ρ(w)
 X
i∈I
ci · fki · · · fli
!
(460)
=
X
i∈I
ci · ρ(w)(fki) · · · ρ(w)(fli)
(461)
=
X
i∈I
ci · fki · · · fli
(462)
= y.
(463)
For g ∈V×(R) and v ∈V we get:
ρ(g)(v) = α(g)vg−1 = vgg−1 = v.
(464)
For g ∈V[×](R) the map ρ(g) is an algebra automorphism of Cl(V, q) and satisfies ρ(g)(v) = v for
all v ∈V . As above we see that for x ∈Cl(V, q) we get:
ρ(g)(x) = ρ(g)
 X
i∈I
ci · vki · · · vli
!
(465)
=
X
i∈I
ci · ρ(g)(vki) · · · ρ(g)(vli)
(466)
=
X
i∈I
ci · vki · · · vli
(467)
= x.
(468)
This shows all the claims.
From Lemma E.23 we see that ρ(w)|V(R) = idV(R) for w ∈Cl[×](V, q). Together with Corollary
E.18 we arrive at a slightly more general version that allows one to parameterize polynomials not just
with coefficients from F, but also with elements from V(R), and still get the equivariance w.r.t. the
Clifford group Γ(V, q):
Corollary E.24. Let F(T1, . . . , Tℓ+s) ∈F[T1, . . . , Tℓ+s] be a polynomial in ℓ+ s variables with
coefficients in F and let k ∈{0, . . . , n}. Further, consider ℓelements x1, . . . , xℓ∈Cl(V, q) and s
elements y1, . . . , ys ∈V(R). Then for every w ∈Γ(V, q) we get the equivariance property:
ρ(w)

F(x1, , . . . , xℓ, y1, . . . , ys)(k)
= F(ρ(w)(x1), . . . , ρ(w)(xℓ), y1, . . . , ys)(k),
(469)
where the superscript (k) indicates the projection onto the multivector grade-k-part of the whole
expression.
We now investigate the image/range of the twisted conjugation.
Theorem E.25 (The range of the twisted conjugation). The image/range of the Clifford group Γ(V, q)
under the twisted conjugation restricted to V coincides with all orthogonal automorphisms of (V, q)
that restrict to the identity idR of the radical subspace R ⊆V of (V, q):
ran (ρ : Γ(V, q) →GL(V )) = OR(V, q).
(470)
Again, recall that the kernel is given by:
ker (ρ : Γ(V, q) →GL(V )) =
^[×]
(R).
(471)
58

Proof. We first show that the range of ρ when restricted to V will consistute an orthogonal automor-
phism of (V, q). For this let e1, . . . , en be an orthogonal basis of (V, q) and R the radical subspace
of (V, q), r = dim R. W.l.o.g. we can assume that e1, . . . , em, em+1, . . . , em+r with:
R = span {em+1, . . . , em+r} ,
E := span {e1, . . . , em} .
(472)
For w ∈Γ(V, q) we now apply ρ(w) ∈GL(V ) to the basis elements ei. Note that by definition of
Γ(V, q) we have ρ(w)(ei) ∈V . With this we get:
2b(ρ(w)(ei), ρ(w)(ej)) = ρ(w)(ei)ρ(w)(ej) + ρ(w)(ej)ρ(w)(ei)
(473)
= ρ(w) (eiej + ejei)
(474)
= ρ(w) (2b(ei, ej))
(475)
= 2b(ei, ej).
(476)
This shows for v = Pn
i=1 ai · ei:
q(ρ(w)(v)) = q(
n
X
i=1
ai · ρ(w)(ei))
(477)
=
n
X
i=1
a2
i · q(ρ(w)(ei))
(478)
=
n
X
i=1
a2
i · q(ei)
(479)
= q(
n
X
i=1
ai · ei)
(480)
= q(v).
(481)
Since ρ(w) is also a linear automorphism of V with inverse ρ(w−1) we see that:
ρ(w)|V ∈O(V, q).
(482)
By Lemma E.23 we also see that:
ρ(w)|R = idR.
(483)
Together this shows that:
ρ(w)|V ∈OR(V, q).
(484)
This shows the inclusion:
ran (ρ : Γ(V, q) →GL(V )) ⊆OR(V, q).
(485)
Recall the definition of the set of radical preserving orthogonal automorphisms:
OR(V, q) := {Φ ∈O(V, q) | Φ|R = idR} ∼=

O(E, q|E)
0m×r
M(r, m)
idR

(486)
∼= O(E, q|E) ⋉M(r, m).
(487)
So an element Φ ∈OR(V, q) can equivalently be written as:
Φ =

O
0
M
I

=

O
0
0
I

◦

I
0
M
I

(488)
=

O1
0
0
I

◦· · · ◦

Ok
0
0
I

◦

I
0
M1,1
I

◦· · · ◦

I
0
Mm,r
I

,
(489)
where O = O1 · · · Ok is a product of k ≤m reflection matrices Ol by the Theorem of Cartan-
Dieudonné C.13, and M = Pm
i=1
Pr
j=1 Mi,j where the matrix Mi,j = ci,j · Ii,j only has the entry
ci,j ∈F at (i, j) (and 0 otherwise). Now let wl ∈V be the normal vector of the reflection Ol with
q(wl) ̸= 0 for l = 1, . . . , k, and for i = 1, . . . , m and j = 1, . . . , r put:
γi,j := 1 + ci,j · eiem+j ∈Cl(V, q),
(490)
59

and further:
w := w1 · · · wk · γ1,1 · · · γm,r ∈Cl(V, q).
(491)
Note that by Examples E.19 and E.20 we have:
{w ∈V | q(w) ̸= 0} ⊆Γ(V, q),
(492)


γi := 1 + ei
r
X
j=1
ci,jem+j

ci,j ∈F, i = 1, . . . , m, j = 1, . . . , r


⊆Γ(V, q),
(493)
which implies that w ∈Γ(V, q). With this we get:
ρ(w) = ρ(w1) ◦· · · ◦ρ(wk) ◦ρ(γ1,1) ◦· · · ◦ρ(γm,r)
(494)
=

O1
0
0
I

◦· · · ◦

Ok
0
0
I

◦

I
0
M1,1
I

◦· · · ◦

I
0
Mm,r
I

(495)
= Φ.
(496)
This thus shows the surjectivity of the map:
ρ : Γ(V, q) →OR(V, q).
(497)
This shows the claim.
We can summarize our finding in the following statement.
Corollary E.26. We have the short exact sequence:
1 −→
^[×]
(R)
incl
−→Γ(V, q)
ρ
−→OR(V, q) −→1.
(498)
From the above Theorem we can now also derive the structure of the elements of the Clifford group.
Corollary E.27 (Elements of the Clifford group). Let (V, q) be a finite dimensional quadratic vector
space of dimension n := dim V < ∞over a fields F with char(F) ̸= 2. Let R ⊆V be the radical
vector subspace of (V, q) with dimension r := dim R. Put m := n −r ≥0. Let e1, . . . , en be an
orthogonal basis of (V, q) ordered in such a way that em+1, . . . em+r = en are the basis vectors
inside R, while e1, . . . , em are spanning a non-degenerate orthogonal subspace to R inside (V, q).
Then every element of the Clifford group w ∈Γ(V, q) is of the form:
w = c · v1 · · · vk · γ1 · · · γm · g,
(499)
with c ∈F×, k ∈N0, vl ∈V with q(vl) ̸= 0 for l = 1, . . . , k,
γi = 1 + ei
r
X
j=1
ci,jem+j,
(500)
with ci,j ∈F for i = 1, . . . , m, j = 1, . . . , r, and some g ∈V[∗](R).
E.4
Orthogonal Representations of the Clifford Group
Lemma E.28. Let e1, . . . , en be an orthogonal basis of (V, q) and w ∈Γ(V, q). If we put for j ∈[n]:
bj := ρ(w)(ej),
(501)
and for A ⊆[n]:
bA :=
<
Y
i∈A
bi = ρ(w)(eA),
(502)
then b1, . . . , bn is an orthogonal basis for (V, q) and both (eA)A∈[n] and (bA)A∈[n] are orthogonal
bases for (Cl(V, q), ¯q).
60

Proof. First note that b1, . . . , bn is a basis of V . Indeed, the releation:
0 =
n
X
i=1
ai · bi,
(503)
with ai ∈F implies:
0 = ρ(w)−1(0)
(504)
= ρ(w)−1
 n
X
i=1
ai · bi
!
(505)
=
n
X
i=1
ai · ρ(w)−1(bi)
(506)
=
n
X
i=1
ai · ei.
(507)
Since e1, . . . , en is linear independent we get ai = 0 for all i ∈[n]. So also b1, . . . , bn is linear
independent and thus constitute a basis of V .
To show that b1, . . . , bn is an orthogonal basis of (V, q) let i ̸= j and then consider the following:
2 · b(bi, bj) = bibj + bjbi
(508)
= ρ(w)(ei)ρ(w)(ej) + ρ(w)(ej)ρ(w)(ei)
(509)
= ρ(w) (eiej + ejei)
(510)
= ρ(w)(2 · b(ei, ej)
|
{z
}
=0
)
(511)
= 0.
(512)
This shows that b1, . . . , bn is an orthogonal basis of (V, q).
Theorem D.26 then shows that both (eA)A∈[n] and (bA)A∈[n] are orthogonal bases for (Cl(V, q), ¯q).
Theorem E.29. Let w ∈Γ(V, q) and x ∈Cl(V, q) then we get:
¯q(ρ(w)(x)) = ¯q(x).
(513)
In other words, ρ(w) ∈O(Cl(V, q), ¯q). Furthermore, we have:
ρ(w)|V(R) = idV(R).
(514)
In other words, ρ(w) ∈OV(R)(Cl(V, q), ¯q).
Proof. Let e1, . . . , en be an orthogonal basis for (V, q) and bj := ρ(w)(ej) for j ∈[n]. Then
by Lemma E.28 we know that b1, . . . , bn is an orthogonal basis of (V, q) and both (eA)A⊆[n] and
(bA)A⊆[n] are orthogonal basis for (Cl(V, q), ¯q). Now let x ∈Cl(V, q) and write it as:
x =
X
A⊆[n]
xA · eA,
ρ(w)(x) =
X
A⊆[n]
xA · ρ(w)(eA) =
X
A⊆[n]
xA · bA.
(515)
61

Then we get:
¯q(ρ(w)(x)) =
X
A⊆[n]
xA ·
Y
i∈A
q(bi)
(516)
=
X
A⊆[n]
xA ·
Y
i∈A
q(ρ(w)(ei))
(517)
=
X
A⊆[n]
xA ·
Y
i∈A
ρ(w)(ei)ρ(w)(ei)
(518)
=
X
A⊆[n]
xA ·
Y
i∈A
ρ(w)(e2
i )
(519)
=
X
A⊆[n]
xA ·
Y
i∈A
ρ(w)(q(ei) · 1)
(520)
=
X
A⊆[n]
xA ·
Y
i∈A
q(ei) · ρ(w)(1)
(521)
=
X
A⊆[n]
xA ·
Y
i∈A
q(ei)
(522)
= ¯q(x).
(523)
This shows the claim. The remaining point follows from Lemma E.23.
Similarly, and more detailed, we also get the following, using Theorem D.27, Corollary E.17 and
Lemma E.23:
Corollary E.30. For every w ∈Γ(V, q) and m = 0, . . . , n we have:
ρ(w)|Cl(m)(V,q) ∈OV(m)(R)(Cl(m)(V, q), ¯q).
(524)
In words, ρ(w), when restricted to the m-th homogeneous multivector component Cl(m)(V, q) of
Cl(V, q) acts as an orthogonal automorphism of Cl(m)(V, q) w.r.t. ¯q. Furthermore, it acts as the
identity when further restricted to the m-th homogeneous multivector component of the radical
subalgebra: V(m)(R) ⊆Cl(m)(V, q).
Remark E.31. For m ∈[n] we use ρ(m) to denote the group homomorphism ρ when restricted to
act on the subvector space Cl(m)(V, q):
ρ(m) : Γ(V, q) →OV(m)(R)(Cl(m)(V, q), ¯q),
ρ(m)(w) := ρ(w)|Cl(m)(V,q).
(525)
By Theorem D.50 or Corollary E.22 we see that V[×](R) always lies inside the kernel of ρ(m):
^[×]
(R) ⊆ker ρ(m) ⊆Γ(V, q).
(526)
So we get a well-defined group homorphism on the quotient:
¯ρ(m) : Γ(V, q)/
^[×]
(R) →OV(m)(R)(Cl(m)(V, q), ¯q),
¯ρ(m)([w]) := ρ(w)|Cl(m)(V,q).
(527)
Furthermore, by Theorem E.25 we have the isomorphism:
¯ρ(1) : Γ(V, q)/
^[×]
(R) ∼= OV(1)(R)(Cl(1)(V, q), ¯q) = OR(V, q),
¯ρ(1)([w]) = ρ(w)|V . (528)
Consequently, for all m ∈[n] we get the composition of group homorphisms:
˜ρ(m) : OR(V, q)
(¯ρ(1))−1
∼=
Γ(V, q)/
^[×]
(R)
¯ρ(m)
→OV(m)(R)(Cl(m)(V, q), ¯q),
(529)
˜ρ(m)(Φ) = ρ(w)|Cl(m)(V,q),
for any w ∈Γ(V, q) with ρ(1)(w) = Φ.
(530)
62

Similarly, we get an (injective) group homomorphism:
˜ρ : OR(V, q) →OV(R)(Cl(V, q), ¯q),
(531)
˜ρ(Φ) := ρ(w),
for any w ∈Γ(V, q) with ρ(1)(w) = Φ.
(532)
So, the group OR(V, q) acts on Cl(V, q) and all subvector spaces Cl(m)(V, q), m = 0, . . . , n, in the
same way as Γ(V, q) does via ρ, when using the surjective map ρ(1) to lift elements Φ ∈OR(V, q) to
elements w ∈Γ(V, q) with ρ(1)(w) = Φ.
Specifically, let x ∈Cl(V, q) be of the form x = P
i∈I ci · vi,1 · · · vi,ki with vi,j ∈V , ci ∈F and
Φ ∈OR(V, q) is given by Φ = ¯ρ(1)([w]) with w ∈Γ(V, q), then we have:
ρ(w)(x) =
X
i∈I
ci · ρ(w)(vi,1) · · · ρ(w)(vi,ki) =
X
i∈I
ci · Φ(vi,1) · · · Φ(vi,ki).
(533)
This means that the action ˜ρ of OR(V, q) transforms a multivector x by transforming all its vector
components vi,j by Φ, acting through the usual orthogonal transformation on vectors.
As such, we have the following equivariance property with respect to OR(V, q).
Corollary E.32. Let F(T1, . . . , Tℓ+s) ∈F[T1, . . . , Tℓ+s] be a polynomial in ℓ+ s variables with
coefficients in F and let k ∈{0, . . . , n}. Further, consider ℓelements x1, . . . , xℓ∈Cl(V, q) and s
elements y1, . . . , ys ∈V(R). Then for every Φ ∈OR(V, q) we get the equivariance property:
˜ρ(Φ)

F(x1, , . . . , xℓ, y1, . . . , ys)(k)
= F(˜ρ(Φ)(x1), . . . , ˜ρ(Φ)(xℓ), y1, . . . , ys)(k),
(534)
where the superscript (k) indicates the projection onto the multivector grade-k-part of the whole
expression.
E.5
The Spinor Norm and the Clifford Norm
In this subsection we shortly introduce the three slightly different versions of a norm that appear in
the literature: the Spinor norm, the Clifford norm and the extended quadratic form. We are interested
under which conditions do they have multiplicative behaviour.
Definition E.33 (The Spinor norm and the Clifford norm). We define the Spinor norm and the Clifford
norm of Cl(V, q) as the maps:
SN : Cl(V, q) →Cl(V, q),
SN(x) := β(x)x,
(535)
CN : Cl(V, q) →Cl(V, q),
CN(x) := γ(x)x.
(536)
Also recall the extended quadratic form:
¯q : Cl(V, q) →F,
¯q(x) = ζ(β(x)x) = ζ(SN(x)).
(537)
As a first preliminary Lemma we need to study when the projection onto the zero-component is
multiplicative:
Lemma E.34. Let x ∈V(R) and y ∈Cl(V, q) then we have:
ζ(xy) = ζ(x) ζ(y).
(538)
As a result, the projection onto the zero component induces an F-algebra homomorphism:
ζ :
^
(R) →F,
y 7→ζ(y).
(539)
Proof. We now use the notations from D.40 and Lemma D.41. We distinguish two cases: x ∈
V(≥1)(R) and:
x ∈
^
(R) \
^(≥1)
(R) =
^×
(R) = F× +
^(≥1)
(R.
(540)
In the first case, we have: ζ(xy) = 0 = ζ(x) ζ(y), as multiplying with x ∈V(≥1)(R) can only
increase the grade of occurring terms or make them vanish.
In the second case, we can write x = a + f with a ∈F× and f ∈V(≥1)(R). Clearly, ζ(x) = a. We
then get by linearity and the first case:
ζ(xy) = ζ(ay + fy) = a ζ(y) + ζ(fy) = ζ(x) ζ(y) + 0.
(541)
This shows the claim.
63

Lemma E.35. Let x1, x2 ∈Cl(V, q).
1. If SN(x1) ∈Z(Cl(V, q)) then we have:
SN(x1x2) = SN(x1) SN(x2).
(542)
2. If CN(x1) ∈Z(Cl(V, q)) then we have:
CN(x1x2) = CN(x1) CN(x2).
(543)
3. If SN(x1) ∈V[0](R) or q = 0 then we have:
¯q(x1x2) = ¯q(x1)¯q(x2).
(544)
Proof. SN(x1) ∈Z(Cl(V, q)) implies:
SN(x1x2) = β(x1x2)x1x2
(545)
= β(x2)β(x1)x1x2
(546)
= β(x2) SN(x1)x2
(547)
SN(x1)∈Z(Cl(V,q))
=
SN(x1)β(x2)x2
(548)
= SN(x1) SN(x2).
(549)
Similarly for CN.
Together with Lemma E.34 and SN(x1) ∈V[0](R) ⊆V(R) ∩Z(Cl(V, q)) we get:
¯q(x1x2) = ζ (SN(x1x2))
(550)
= ζ (SN(x1) SN(x2))
(551)
= ζ(SN(x1)) ζ(SN(x2))
(552)
= ¯q(x1)¯q(x2).
(553)
This shows the claim.
Lemma E.36. Consider the following subset of Cl(V, q):
Γ[−](V, q) :=
n
x ∈Cl[0](V, q) ∪Cl[1](V, q)
 ∀v ∈V ∃v′ ∈V. α(x)v = v′x
o
.
(554)
Then Γ[−](V, q) is closed under multiplication and for every x ∈Γ[−](V, q) we have:
SN(x) ∈
^[0]
(R),
CN(x) ∈
^[0]
(R).
(555)
Proof. For x, y ∈Γ[−](V, q) we also have that xy is homogeneous. Furthermore, we get for v ∈V :
α(xy)v = α(x)α(y)v
(556)
= α(x)v′y
(557)
= ˜vxy,
(558)
for some v′, ˜v ∈V . So, xy ∈Γ[−](V, q) and Γ[−](V, q) is closed under multiplication.
With the above conditions on x we get for every v ∈V :
α(SN(x))v = α(β(x))α(x)v
(559)
= α(β(x))v′x
(560)
= α(β(x))(−α(β(v′)))x
(561)
= −α(β(x)β(v′))x
(562)
= −α(β(v′x))x
(563)
= −α(β(α(x)v))x
(564)
= −β(α(α(x)v))x
(565)
= −β(xα(v))x
(566)
= β(xv)x
(567)
= β(v)β(x)x
(568)
= v SN(x).
(569)
64

This implies by Theorem D.50 that:
SN(x) ∈
^
(R).
(570)
Since for homogeneous x ∈Cl(V, q) we have: prt(β(x)) = prt(x) and thus: SN(x) = β(x)x ∈
Cl[0](V, q). This implies:
SN(x) ∈
^
(R) ∩Cl[0](V, q) =
^[0]
(R).
(571)
This shows the claim.
Theorem E.37 (Multiplicativity of the three different norms). Both, the Spinor norm and the Clifford
norm, when restricted to the Clifford group, are well-defined group homomorphisms:
SN : Γ(V, q) →
^[×]
(R),
w 7→SN(w) = β(w)w,
(572)
CN : Γ(V, q) →
^[×]
(R),
w 7→CN(w) = γ(w)w.
(573)
Furthermore, the extended quadratic form ¯q of Cl(V, q) restricted to the Clifford group is a well-
defined group homomorphism:
¯q : Γ(V, q) →F×,
w 7→¯q(w) = ζ(β(w)w).
(574)
Proof. This directly follows from Lemma E.35 and Lemma E.36. Note that Γ(V, q) ⊆Γ[−](V, q).
Example E.38.
1. For a ∈F we get:
SN(a) = β(a)a = a2.
(575)
This also shows: ¯q(a) = a2¯q(1) = a2.
2. For w ∈V we have:
SN(w) = β(w)w = w2 = q(w).
(576)
This also shows: ¯q(w) = q(w).
3. For γ = 1 + ef with e ∈V and f ∈R we have:
SN(γ) = (1 + fe)(1 + ef)
(577)
= 1 + fe + ef + feef
(578)
= 1 + 2b(e, f) + q(f) · q(e)
(579)
= 1.
(580)
This also shows: ¯q(γ) = 1.
4. For g = 1 + h ∈V∗(R) we get:
SN(g) = (1 + β(h))(1 + h)
(581)
= 1 + β(h) + h + β(h)h,
(582)
and thus: ¯q(g) = 1.
65

E.6
The Pin Group and the Spin Group
We have investigated the Clifford group and its action on the algebra through the twisted conjugation.
In many fields of study, the Clifford group is further restricted to the Pin or Spin group. There can
arise a few issues regarding the exact definition of these groups, especially when also considering
fields other than the reals. We elaborate on these concerns here and leave the general definition for
future discussion.
Motivation E.39 (The problem of generalizing the definition of the Spin group). For a positive
definite quadratic form q on the real vector space V = Rn with n ≥3 the Spin group Spin(n) is
defined via the kernel of the Spinor norm (=extended quadratic form on Cl(V, q)) restricted to the
special Clifford group Γ[0](V, q):
Spin(n) := ker

¯q : Γ[0](V, q) →R×
=
n
w ∈Γ[0](V, q)
 ¯q(w) = 1
o
= ¯q|−1
Γ[0](V,q)(1).
(583)
Spin(n) is thus a normal subgroup of the special Clifford group Γ[0](V, q), and, as it turns out, a
double cover of the special orthogonal group SO(n) via the twisted conjugation ρ. The latter can be
summarized by the short exact sequence:
1 −→{±1}
incl
−→Spin(n)
ρ
−→SO(n) −→1.
(584)
We intend to generalize this in several directions: 1. from Spin to Pin group, 2. from Rn to vector
spaces V over general fields F with char(F) ̸= 2, 3. from non-degenerate to degenerate quadratic
forms q, 4. from positive (semi-)definite to non-definite quadratic forms q. This comes with several
challenges and ambiguities.
If we want to generalize the above to define the Pin group we would allow for elements not just
of (pure) even parity w ∈Γ[0](V, q). Here the question arises if one should generalize to the
unconstrained Clifford group ˜Γ(V, q) or the (homogeneous) Clifford group Γ(V, q). As discussed
before, to ensure that the (adjusted) twisted conjugation ρ is a well-defined algebra automorphism of
Cl(V, q) the parity homogeneity assumptions is crucial. Furthermore, the elements of ˜Γ(V, q) that
lead to non-trivial orthogonal automorphisms, e.g. v ∈V with q(v) ̸= 0 and γ = 1 + ef with e ∈V ,
f ∈R, are already homogeneous. So it is arguably safe and reasonable to restrict to the Clifford
group Γ(V, q) and define the Pin group Pin(V, q) as some subquotient of Γ(V, q).
In the non-definite (but still non-degenerate, real) case R(p,q), p, q ≥1, the special orthogonal
group SO(p, q) contains combinations of reflections r0 ◦r1 where the corresponding normal vectors
v0, v1 ∈V satisfy q(v0) = 1 and q(v1) = −1. Their product v0v1 would lie in the special Clifford
group Γ[0](V, q). However, their spinor norm would be different from 1:
¯q(v0v1) = q(v0)q(v1) = −1 ̸= 1.
(585)
Here now the question arises if we would like to preserve the former definition of Spin(p, q) as
¯q|−1
Γ[0](V,q)(1) and exclude v0v1 from Spin(p, q), or, if we adjust the definition of Spin(p, q) and
include v0v1. The former definition has the effect that Spin(p, q) does, in general, not map surjectively
onto SO(p, q) anymore, and we would only get a short exact sequence:
1 −→{±1}
incl
−→Spin(p, q)
ρ
−→SO(p, q)
¯¯q
−→R×/(R×)2
|
{z
}
∼
={±1}
.
(586)
The alternative would be to define Spin(p, q) as ¯q|−1
Γ[0](V,q)(±1). This would allow for v0v1 ∈
Spin(p, q) and lead to the short exact sequence:
1 −→µ4(R)
| {z }
={±1}
incl
−→Spin(p, q)
ρ
−→SO(p, q) −→1,
(587)
which exactly recovers the former behaviour for Spin(n), and, which makes Spin(p, q) a double
cover of SO(p, q).
However, for other fields F, char(F) ̸= 2, and non-degenerate (V, q), dim V ≥3, one would get,
with the last definition ¯q|−1
Γ[0](V,q)(±1), the exact sequence:
1 −→µ4(F)
incl
−→Spin(V, q)
ρ
−→SO(V, q)
¯¯q
−→F×/(F×)2,
(588)
66

and one would need to live with the fact that: a) µ4(F) :=

x ∈F×  x4 = 1
	
could contain a
number of elements k different from 2, rendering ρ a (k : 1)-map, in contrast to a (2 : 1)-map, and,
again, b) the non-surjectivity of ρ. The latter comes from the fact that a combination of reflections
r0 ◦r1 where the corresponding normal vectors v0, v1 ∈V with q(v0), q(v1) ̸= 0 cannot, in general,
be normalized such that the q(vi)’s lie in {±1} by multiplying/dividing the vi’s with some scalars
ci ∈F×. Note that we get:
q(vi/ci) = q(vi)/c2
i .
(589)
This shows that we can only normalize the vi’s such that the q(vi)’s lie inside a fixed system of
representatives S ⊆F× of F×/(F×)2, which is thus of size:
#S = #
 F×/(F×)2
,
(590)
which can be different from 2 = # {±1}.
So, in this general setting, the first definition of Spin(V, q) as ¯q|−1
Γ[0](V,q)(1) would at least correct the
map ρ to be a (2 : 1)-map. We would get the following short exact seequence:
1 −→{±1}
incl
−→Spin(V, q)
ρ
−→SO(V, q)
¯¯q
−→F×/(F×)2.
(591)
The normalization argument around Equation 589 for general fields F now would also give us a third
option: we could, instead of restricting elements to have a fixed value q(vi) ∈S, which depends on
the choice of S, identify elements w1, w2 ∈Γ[0](V, q) if they differ by a scalar c ∈F×:
w1 = c · w2.
(592)
Then for their spinor norms (modulo (F×)2) we would get:
¯q(w1) = c2 · ¯q(w2),
[¯q(w1)] = [¯q(w2)] ∈F×/(F×)2.
(593)
So, the spinor norms of w1 and w2 would be represented by the same representative s ∈S, as desired.
However, this definition would just identify Spin(V, q) with SO(V, q) via ρ:
Spin(V, q) = Γ[0](V, q)/F× ρ∼= SO(V, q),
(594)
and nothing new would emerge from this. Note that the latter isomorphisms always holds for
non-degenerate (V, q) with dim V ≥3, char(F) ̸= 2, and can be expressed as the exact sequence:
1 −→F× incl
−→Γ[0](V, q)
ρ
−→SO(V, q) −→1.
(595)
A fourth option would be to mod out the scalar squares (F×)2 instead of F× and use Γ[0](V, q)/(F×)2
as the definition of Spin(V, q). This would lead to the exact sequence:
1 −→F×/(F×)2 incl
−→Spin(V, q)
ρ
−→SO(V, q) −→1,
(596)
which would again coincide with the real case of Spin(n) as then R×/(R×)2 ∼= {±1}. However, in
the general case, again, ρ is here a (k : 1)-map instead of a (2 : 1)-map with k := #
 F×/(F×)2
.
Furthermore, the spinor norm ¯q on Spin(V, q) would not be well-defined anymore, in its current form,
as different representatives of elements [w1] = [w2] ∈Γ[0](V, q)/(F×)2 would differ by a scalar
square: w1 = c2 · w2. Their spinor norms would thus differ by by a forth scalar power:
¯q(w1) = ¯q(c2 · w2) = c4 · ¯q(w2).
(597)
So, the spinor norm on Γ[0](V, q)/(F×)2 would only be well defined modulo (F×)4 ⊆(F×)2:
[¯q] : Γ[0](V, q)/(F×)2 →F×/(F×)4,
[w] 7→[¯q(w)].
(598)
Things become even more complicated in the degenerate case. At least we always have a short exact
sequence for m ≥3:
1 −→
^[×]
(R)
incl
−→Γ[0](V, q)
ρ
−→SOR(V, q) −→1,
(599)
67

where SOR(V, q) indicates the set of those special orthogonal automorphisms Φ of (V, q) with
Φ|R = idR, where R is the radical subspace of (V, q) with r := dim R, n := dim V , m := n −r.
Recall that we have:
^[×]
(R) = F× ·
^[∗]
(R),
(600)
^[∗]
(R) = 1 + span {f1 · · · fk | k ≥2 even, fl ∈R, l = 1, . . . , k} .
(601)
Note that for g ∈V[∗](R) we have ρ(g)|V = idV and ¯q(g) = 1. So the elements from V[∗](R) are
only blowing up the kernels of ρ and ¯q and can be considered redundant for our analysis. So one can
argue that one can mod out V[∗](R) in the above groups. We thus get a short exact sequence:
1 −→F× −→Γ[0](V, q)/
^[∗]
(R)
|
{z
}
=:˜Γ
[0](V,q)
ρ
−→SOR(V, q) −→1,
(602)
which now looks similar to the non-degenerate case. We can now consider the same 4 options for the
definition of the Spin group as before:
¯q|−1
˜Γ
[0](V,q)(1),
¯q|−1
˜Γ
[0](V,q)(±1),
˜Γ
[0](V, q)/F×,
˜Γ
[0](V, q)/(F×)2.
(603)
As before, the third option can easily be discarded. If we want to preserve generality, we have the
option to either preserve ¯q and the (2 : 1)-property of ρ and pick the first option, or, preserve the
surjectivity of ρ and take the fourth option. If we are only interested in the R-case, then the second
option preserves all properties. Note that in the R-case the groups of the second and forth option are
isomorphic as groups anyways:
¯q|−1
˜Γ
[0](V,q)(±1) ∼= ˜Γ
[0](V, q)/(R×)2.
(604)
The reason is that we always have that: ¯q(F×) = (F×)2, and, in the R-case, the left group already
contains the relevant elements to also map surjectively onto SOR(V, q) via ρ.
One could further discuss if one wanted to replace the extended quadratic form ¯q, which is given by
¯q(x) = ζ(β(x)x) ∈F×, by the other possible definition of the spinor norm SN, which is only given
by SN(x) = β(x)x ∈V[×](R). However, for all relevant elements of Γ[0](V, q) both definitions
agree, but the description of the set of the rather irrelevant elements, which satisfy SN(x) = 1
and ρ(x)|V = idV , becomes more complicated than the set V[×](R). As we mod those irrelevant
elements out anyways and we prefer to have our “norm map” to map to the scalars F× instead of to
V[×](R), it is safe and reasonable to work with the extended quadratic form ¯q in all definitions.
In this paper we are mostly interested in working with the orthogonal groups and thus are interested
in preserving the surjectivity of ρ. Since, for computational reasons, we usually restrict ourselves to
the R-case, it is easier to work with (a restricted set of) elements of a group than equivalence classes,
and, the spinor norm/extended quadratic form has computational meaning, we side with the second
definition in this paper, but only state it for the R-case below. We leave the general definition for
future discussion.
Definition E.40 (The real Pin group and the real Spin group). Let V be a finite dimensional R-vector
space V , dim V = n < ∞, and q a (possibly degenerate) quadratic form on V . We define the (real)
Pin group and (real) Spin group, resp., of (V, q) as the following subquotients of the Clifford group
Γ(V, q) and its even parity part Γ[0](V, q), resp.:
Pin(V, q) := {x ∈Γ(V, q) | ¯q(x) ∈{±1}} /
^[∗]
(R),
(605)
Spin(V, q) :=
n
x ∈Γ[0](V, q)
 ¯q(x) ∈{±1}
o
/
^[∗]
(R).
(606)
If (V, q) = R(p,q,r) is the standard quadratic R-vector space with signature (p, q, r) then we denote:
Pin(p, q, r) := Pin(R(p,q,r)),
(607)
Spin(p, q, r) := Spin(R(p,q,r)).
(608)
68

Corollary E.41. Let (V, q) be a finite dimensional quadratic vector space over R. Then the twisted
conjugation induces a well-defined and surjective group homomorphism onto the group of radical
preserving orthogonal automorphisms of (V, q):
ρ : Pin(V, q) →OR(V, q),
(609)
with kernel:
ker (ρ : Pin(V, q) →OR(V, q)) = {±1} .
(610)
Correspondingly, for the Spin(V, q) group. In short, we have short exact sequences:
1 −→{±1}
incl
−→Pin(V, q)
ρ
−→OR(V, q) −→1,
(611)
1 −→{±1}
incl
−→Spin(V, q)
ρ
−→SOR(V, q) −→1.
(612)
The examples E.19, E.20, E.21, E.38 allow us to describe the elements of the Pin and Spin group as
follows.
Corollary E.42 (The elements of the real Pin group and the real Spin group). Let (V, q) be a finite
dimensional quadratic vector space over R with signature (p, q, r). We get the following description
of the elements of the Pin group:
Pin(V, q) =

±v1 · · · vk · γ1 · · · γp+q · g
 k ∈N0, q(vl) ∈{±1} , g ∈
^[∗]
(R)

/
^[∗]
(R),
(613)
where γi = 1 + ei
Pr
j=1 ci,jep+q+j, ci,j ∈R, for i = 1, . . . , p + q and j = 1, . . . , r, and, vl ∈V
with q(vl) ∈{±1} with l = 1, . . . , l and k ∈N0. Note that {ep+q+j | j ∈[r]} is meant to span the
radical subspace R of (V, q).
We similarly can describe the Spin group as follows:
Spin(V, q) =

±v1 · · · vk · γ1 · · · γp+q · g
 k ∈2N0, q(vl) ∈{±1} , g ∈
^[∗]
(R)

/
^[∗]
(R),
(614)
with the same conditions as above, but where k needs to be an even number.
Corollary E.43. Note that we also get well-defined group representations:
ρ : Pin(V, q) →AutAlg,grd(Cl(V, q)) ∩OV(R)(Cl(V, q), ¯q),
(615)
with kernel ker ρ = {±1}.
In particular, Cl(V, q) and Cl(m)(V, q) for m = 0, . . . , n, are orthogonal group representations of
Pin(V, q) via ρ:
ρ : Pin(V, q) →OV(m)(R)(Cl(m)(V, q), ¯q).
(616)
Also, if F(T1, . . . , Tℓ+s) ∈R[T1, . . . , Tℓ+s] is a polynomial in ℓ+ s variables with coefficients in R
and x1, . . . , xℓ∈Cl(V, q), y1, . . . , ys ∈V(R), and k ∈{0, . . . , n}. Then for every w ∈Pin(V, q)
we have the equivariance property:
ρ(w)

F(x1, . . . , xℓ, y1, . . . , ys)(k)
= F(ρ(w)(x1), . . . , ρ(w)(xℓ), y1, . . . , ys)(k).
(617)
69

