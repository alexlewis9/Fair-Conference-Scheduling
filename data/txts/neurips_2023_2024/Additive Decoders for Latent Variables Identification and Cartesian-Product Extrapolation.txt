Additive Decoders for Latent Variables Identification
and Cartesian-Product Extrapolation
S´ebastien Lachapelle∗,1
Divyat Mahajan∗
Ioannis Mitliagkas†
Simon Lacoste-Julien†,1
Mila & DIRO, Universit´e de Montr´eal
1Samsung - SAIT AI Lab, Montreal
Abstract
We tackle the problems of latent variables identification and “out-of-support” im-
age generation in representation learning. We show that both are possible for a
class of decoders that we call additive, which are reminiscent of decoders used for
object-centric representation learning (OCRL) and well suited for images that can
be decomposed as a sum of object-specific images. We provide conditions under
which exactly solving the reconstruction problem using an additive decoder is guar-
anteed to identify the blocks of latent variables up to permutation and block-wise
invertible transformations. This guarantee relies only on very weak assumptions
about the distribution of the latent factors, which might present statistical depen-
dencies and have an almost arbitrarily shaped support. Our result provides a new
setting where nonlinear independent component analysis (ICA) is possible and
adds to our theoretical understanding of OCRL methods. We also show theoreti-
cally that additive decoders can generate novel images by recombining observed
factors of variations in novel ways, an ability we refer to as Cartesian-product
extrapolation. We show empirically that additivity is crucial for both identifiability
and extrapolation on simulated data.
1
Introduction
The integration of connectionist and symbolic approaches to artificial intelligence has been proposed
as a solution to the lack of robustness, transferability, systematic generalization and interpretability of
current deep learning algorithms [53, 4, 13, 25, 21] with justifications rooted in cognitive sciences
[20, 28, 43] and causality [57, 63]. However, the problem of extracting meaningful symbols grounded
in low-level observations, e.g. images, is still open. This problem is sometime referred to as
disentanglement [4, 48] or causal representation learning [63]. The question of identifiability in
representation learning, which originated in works on nonlinear independent component analysis
(ICA) [65, 31, 33, 36], has been the focus of many recent efforts [49, 66, 26, 47, 3, 9, 41]. The
mathematical results of these works provide rigorous explanations for when and why symbolic
representations can be extracted from low-level observations. In a similar spirit, Object-centric
representation learning (OCRL) aims to learn a representation in which the information about
different objects are encoded separately [19, 22, 11, 24, 18, 51, 14]. These approaches have shown
impressive results empirically, but the exact reason why they can perform this form of segmentation
without any supervision is poorly understood.
∗Equal contribution. † Canada CIFAR AI Chair.
Correspondence to: {lachaseb, divyat.mahajan}@mila.quebec
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

+
=
Figure 1: Left: Additive decoders model the additive structure of scenes composed of multiple
objects. Right: Additive decoders allow to generate novel images never seen during training via
Cartesian-product extrapolation (Corollary 3). Purple regions correspond to latents/observations seen
during training. The blue regions correspond to the Cartesian-product extension. The middle set is
the manifold of images of balls. In this example, the learner never saw both balls high, but these can
be generated nevertheless thanks to the additive nature of the scene. Details in Section 3.2.
1.1
Contributions
Our first contribution is an analysis of the identifiability of a class of decoders we call additive
(Definition 1). Essentially, a decoder f(z) acting on a latent vector z ∈Rdz to produce an observation
x is said to be additive if it can be written as f(z) = P
B∈B f (B)(zB) where B is a partition of
{1, . . . , dz}, f (B)(zB) are “block-specific” decoders and the zB are non-overlapping subvectors of
z. This class of decoder is particularly well suited for images x that can be expressed as a sum of
images corresponding to different objects (left of Figure 1). Unsurprisingly, this class of decoder
bears similarity with the decoding architectures used in OCRL (Section 2), which already showed
important successes at disentangling objects without any supervision. Our identifiability results
provide conditions under which exactly solving the reconstruction problem with an additive decoder
identifies the latent blocks zB up to permutation and block-wise transformations (Theorems 1 & 2).
We believe these results will be of interest to both the OCRL community, as they partly explain the
empirical success of these approaches, and to the nonlinear ICA and disentanglement community, as it
provides an important special case where identifiability holds. This result relies on the block-specific
decoders being “sufficiently nonlinear” (Assumption 2) and requires only very weak assumptions
on the distribution of the ground-truth latent factors of variations. In particular, these factors can be
statistically dependent and their support can be (almost) arbitrary.
Our second contribution is to show theoretically that additive decoders can generate images never seen
during training by recombining observed factors of variations in novel ways (Corollary 3). To describe
this ability, we coin the term “Cartesian-product extrapolation” (right of Figure 1). We believe the
type of identifiability analysis laid out in this work to understand “out-of-support” generation is novel
and could be applied to other function classes or learning algorithms such as DALLE-2 [59] and
Stable Diffusion [61] to understand their apparent creativity and hopefully improve it.
Both latent variables identification and Cartesian-product extrapolation are validated experimentally
on simulated data (Section 4). More specifically, we observe that additivity is crucial for both by
comparing against a non-additive decoder which fails to disentangle and extrapolate.
Notation. Scalars are denoted in lower-case and vectors in lower-case bold, e.g. x ∈R and x ∈Rn.
We maintain an analogous notation for scalar-valued and vector-valued functions, e.g. f and f. The
ith coordinate of the vector x is denoted by xi. The set containing the first n integers excluding 0 is
denoted by [n]. Given a subset of indices S ⊆[n], xS denotes the subvector consisting of entries
xi for i ∈S. Given a function f(xS) ∈Rm with input xS, the derivative of f w.r.t. xi is denoted
by Dif(xS) ∈Rm and the second derivative w.r.t. xi and xi′ is D2
i,i′f(xS) ∈Rm. See Table 2 in
appendix for more.
Code: Our code repository can be found at this link.
2
Background & Literature review
Identifiability of latent variable models. The problem of latent variables identification can be best
explained with a simple example. Suppose observations x ∈Rdx are generated i.i.d. by first sampling
a latent vector z ∈Rdz from a distribution Pz and feeding it into a decoder function f : Rdz →Rdx,
2

i.e. x = f(z). By choosing an alternative model defined as ˆf := f ◦v and ˆz := v−1(z) where
v : Rdz →Rdz is some bijective transformation, it is easy to see that the distributions of ˆx = ˆf(ˆz)
and x are the same since ˆf(ˆz) = f ◦v(v−1(z)) = f(z). The problem of identifiability is that, given
only the distribution over x, it is impossible to distinguish between the two models (f, z) and ( ˆf, ˆz).
This is problematic when one wants to discover interpretable factors of variations since z and ˆz could
be drastically different. There are essentially two strategies to go around this problem: (i) restricting
the hypothesis class of decoders ˆf [65, 26, 44, 54, 9, 73], and/or (ii) restricting/adding structure to the
distribution of ˆz [33, 50, 42, 47]. By doing so, the hope is that the only bijective mappings v keeping
ˆf and ˆz into their respective hypothesis classes will be trivial indeterminacies such as permutations
and element-wise rescalings. Our contribution, which is to restrict the decoder function ˆf to be
additive (Definition 1), falls into the first category. Other restricted function classes for f proposed
in the literature include post-nonlinear mixtures [65], local isometries [16, 15, 29], conformal and
orthogonal maps [26, 60, 9] as well as various restrictions on the sparsity of f [54, 73, 7, 71]. Methods
that do not restrict the decoder must instead restrict/structure the distribution of the latent factors
by assuming, e.g., sparse temporal dependencies [31, 38, 42, 40], conditionally independent latent
variables given an observed auxiliary variable [33, 36], that interventions targeting the latent factors
are observed [42, 47, 46, 8, 2, 3, 64, 10, 67, 72, 34], or that the support of the latents is a Cartesian-
product [68, 62]. In contrast, our result makes very mild assumptions about the distribution of the
latent factors, which can present statistical dependencies, have an almost arbitrarily shaped support
and does not require any interventions. Additionally, none of these works provide extrapolation
guarantees as we do in Section 3.2.
Relation to nonlinear ICA. Hyv¨arinen and Pajunen [32] showed that the standard nonlinear ICA
problem where the decoder f is nonlinear and the latent factors zi are statistically independent is
unidentifiable. This motivated various extensions of nonlinear ICA where more structure on the
factors is assumed [30, 31, 33, 36, 37, 27]. Our approach departs from the standard nonlinear ICA
problem along three axes: (i) we restrict the mixing function to be additive, (ii) the factors do not
have to be necessarily independent, and (iii) we can identify only the blocks zB as opposed to each
zi individually up to element-wise transformations, unless B = {{1}, ..., {dz}} (see Section 3.1).
Object-centric representation learning (OCRL). Lin et al. [45] classified OCRL methods in
two categories: scene mixture models [22, 23, 24, 51] & spatial-attention models [19, 12, 11, 18].
Additive decoders can be seen as an approximation to the decoding architectures used in the former
category, which typically consist of an object-specific decoder f (obj) acting on object-specific latent
blocks zB and “mixed” together via a masking mechanism m(B)(z) which selects which pixel
belongs to which object. More precisely,
f(z) =
X
B∈B
m(B)(z) ⊙f (obj)(zB) , where m(B)
k
(z) =
exp(ak(zB))
P
B′∈B exp(ak(zB′)) ,
(1)
and where B is a partition of [dz] made of equal-size blocks B and a : R|B| →Rdx outputs a score
that is normalized via a softmax operation to obtain the masks m(B)(z). Many of these works
also present some mechanism to select dynamically how many objects are present in the scene and
thus have a variable-size representation z, an important technical aspect we omit in our analysis.
Empirically, training these decoders based on some form of reconstruction objective, probabilistic
or not, yields latent blocks zB that represent the information of individual objects separately. We
believe our work constitutes a step towards providing a mathematically grounded explanation for
why these approaches can perform this form of disentanglement without supervision (Theorems 1 &
2). Many architectural innovations in scene mixture models concern the encoder, but our analysis
focuses solely on the structure of the decoder f(z), which is a shared aspect across multiple methods.
Generalization capabilities of object-centric representations were studied empirically by Dittadi et al.
[14] but did not cover Cartesian-product extrapolation (Corollary 3) on which we focus here.
Diagonal Hessian penalty [58]. Additive decoders are also closely related to the penalty introduced
by Peebles et al. [58] which consists in regularizing the Hessian of the decoder to be diagonal. In
Appendix A.2, we show that “additivity” and “diagonal Hessian” are equivalent properties. They
showed empirically that this penalty can induce disentanglement on datasets such as CLEVR [35],
which is a standard benchmark for OCRL, but did not provide any formal justification. Our work
provides a rigorous explanation for these successes and highlights the link between the diagonal
Hessian penalty and OCRL.
3

Compositional decoders [7]. Compositional decoders were recently introduced by Brady et al. [7] as
a model for OCRL methods with identifiability guarantees. A decoder f is said to be compositional
when its Jacobian Df satisfies the following property everywhere: For all i ∈[dz] and B ∈B,
DBfi(z) ̸= 0 =⇒DBcfi(z) = 0, where Bc := [dz] \ B. In other words, each xi can locally
depend solely on one block zB (this block can change for different z). In Appendix A.3, we show that
compositional C2 decoders are additive. Furthermore, Example 3 shows a decoder that is additive
but not compositional, which means that additive C2 decoders are strictly more expressive than
compositional C2 decoders. Another important distinction with our work is that we consider more
general supports for z and provide a novel extrapolation analysis. That being said, our identifiability
result does not supersede theirs since they assume only C1 decoders while our theory assumes C2.
Extrapolation. Du and Mordatch [17] studied empirically how one can combine energy-based
models for what they call compositional generalization, which is similar to our notion of Cartesian-
product extrapolation, but suppose access to datasets in which only one latent factor varies and do
not provide any theory. Webb et al. [70] studied extrapolation empirically and proposed a novel
benchmark which does not have an additive structure. Besserve et al. [5] proposed a theoretical
framework in which out-of-distribution samples are obtained by applying a transformation to a single
hidden layer inside the decoder network. Krueger et al. [39] introduced a domain generalization
method which is trained to be robust to tasks falling outside the convex hull of training distributions.
Extrapolation in text-conditioned image generation was recently discussed by Wang et al. [69].
3
Additive decoders for disentanglement & extrapolation
Our theoretical results assume the existence of some data-generating process describing how the
observations x are generated and, importantly, what are the “natural” factors of variations.
Assumption 1 (Data-generating process). The set of possible observations is given by a lower
dimensional manifold f(Ztest) embedded in Rdx where Ztest is an open set of Rdz and f : Ztest →
Rdx is a C2-diffeomorphism onto its image. We will refer to f as the ground-truth decoder. At
training time, the observations are i.i.d. samples given by x = f(z) where z is distributed according
to the probability measure Ptrain
z
with support Ztrain ⊆Ztest. Throughout, we assume that Ztrain is
regularly closed (Definition 6).
Intuitively, the ground-truth decoder f is effectively relating the “natural factors of variations” z to
the observations x in a one-to-one fashion. The map f is a C2-diffeomorphism onto its image, which
means that it is C2 (has continuous second derivative) and that its inverse (restricted to the image
of f) is also C2. Analogous assumptions are very common in the literature on nonlinear ICA and
disentanglement [33, 36, 42, 1]. Mansouri et al. [52] pointed out that the injectivity of f is violated
when images show two objects that are indistinguishable, an important practical case that is not
covered by our theory.
We emphasize the distinction between Ztrain, which corresponds to the observations seen during
training, and Ztest, which corresponds to the set of all possible images. The case where Ztrain ̸= Ztest
will be of particular interest when discussing extrapolation in Section 3.2. The “regularly closed”
condition on Ztrain is mild, as it is satisfied as soon as the distribution of z has a density w.r.t. the
Lebesgue measure on Rdz. It is violated, for example, when z is a discrete random vector. Figure 2
illustrates this assumption with simple examples.
Objective. Our analysis is based on the simple objective of reconstructing the observations x by
learning an encoder ˆg : Rdx →Rdz and a decoder ˆf : Rdz →Rdx. Note that we assumed implicitly
that the dimensionality of the learned representation matches the dimensionality of the ground-truth.
We define the set of latent codes the encoder can output when evaluated on the training distribution:
ˆZtrain := ˆg(f(Ztrain)) .
(2)
When the images of the ground-truth and learned decoders match, i.e. f(Ztrain) = ˆf( ˆZtrain), which
happens when the reconstruction task is solved exactly, one can define the map v : ˆZtrain →Ztrain as
v := f −1 ◦ˆf .
(3)
This function is going to be crucial throughout the work, especially to define B-disentanglement
(Definition 3), as it relates the learned representation to the ground-truth representation.
4

Before introducing our formal definition of additive decoders, we introduce the following notation:
Given a set Z ⊆Rdz and a subset of indices B ⊆[dz], let us define ZB to be the projection of Z
onto dimensions labelled by the index set B. More formally,
ZB := {zB | z ∈Z} ⊆R|B| .
(4)
Intuitively, we will say that a decoder is additive when its output is the summation of the outputs of
“object-specific” decoders that depend only on each latent block zB. This captures the idea that an
image can be seen as the juxatoposition of multiple images which individually correspond to objects
in the scene or natural factors of variations (left of Figure 1).
Definition 1 (Additive functions). Let B be a partition of [dz]1. A function f : Z →Rdx is said to
be additive if there exist functions f (B) : ZB →Rdx for all B ∈B such that
∀z ∈Z, f(z) =
X
B∈B
f (B)(zB) .
(5)
This additivity property will be central to our analysis as it will be the driving force of identifiability
(Theorem 1 & 2) and Cartesian-product extrapolation (Corollary 3).
Remark 1. Suppose we have x = σ(P
B∈B f (B)(zB)) where σ is a known bijective function. For
example, if σ(y) := exp(y) (component-wise), the decoder can be thought of as being multiplicative.
Our results still apply since we can simply transform the data doing ˜x := σ−1(x) to recover the
additive form ˜x = P
B∈B f (B)(zB).
Differences with OCRL in practice. We point out that, although the additive decoders make intuitive
sense for OCRL, they are not expressive enough to represent the “masked decoders” typically used in
practice (Equation (1)). The lack of additivity stems from the normalization in the masks m(B)(z).
We hypothesize that studying the simpler additive decoders might still reveal interesting phenomena
present in modern OCRL approaches due to their resemblance. Another difference is that, in practice,
the same object-specific decoder f (obj) is applied to every latent block zB. Our theory allows for
these functions to be different, but also applies when functions are the same. Additionally, this
parameter sharing across f (B) enables modern methods to have a variable number of objects across
samples, an important practical point our theory does not cover.
3.1
Identifiability analysis
We now study the identifiability of additive decoders and show how they can yield disentanglement.
Our definition of disentanglement will rely on partition-respecting permutations:
Definition 2 (Partition-respecting permutations). Let B be a partition of {1, ..., dz}. A permutation
π over {1, ..., dz} respects B if, for all B ∈B, π(B) ∈B.
Essentially, a permutation that respects B is one which can permute blocks of B and permute elements
within a block, but cannot “mix” blocks together. We now introduce B-disentanglement.
Definition 3 (B-disentanglement). A learned decoder ˆf : Rdz →Rdx is said to be B-disentangled
w.r.t. the ground-truth decoder f when f(Ztrain) = ˆf( ˆZtrain) and the mapping v := f −1 ◦ˆf is a
diffeomorphism from ˆZtrain to Ztrain satisfying the following property: there exists a permutation π
respecting B such that, for all B ∈B, there exists a function ¯vπ(B) : ˆZtrain
B
→Ztrain
π(B) such that, for
all z ∈ˆZtrain, vπ(B)(z) = ¯vπ(B)(zB). In other words, vπ(B)(z) depends only on zB.
Thus, B-disentanglement means that the blocks of latent dimensions zB are disentangled from one
another, but that variables within a given block might remain entangled. Note that, unless the partition
is B = {{1}, ..., {dz}}, this corresponds to a weaker form of disentanglement than what is typically
seeked in nonlinear ICA, i.e. recovering each variable individually.
Example 1. To illustrate B-disentanglement, imagine a scene consisting of two balls moving around
in 2D where the “ground-truth” representation is given by z = (x1, y1, x2, y2) where zB1 = (x1, y1)
and zB2 = (x2, y2) are the coordinates of each ball (here, B := {{1, 2}, {3, 4}}). In that case, a
learned representation is B-disentangled when the balls are disentangled from one another. However,
the basis in which the position of each ball is represented might differ in both representations.
1Without loss of generality, we assume that the partition B is contiguous, i.e. each B ∈B can be written as
B = {i + 1, i + 2, . . . , i + |B|}.
5

Our first result (Theorem 1) shows a weaker form of disentanglement we call local B-disentanglement.
This means the Jacobian matrix of v, Dv, has a “block-permutation” structure everywhere.
Definition 4 (Local B-disentanglement). A learned decoder ˆf : Rdz →Rdx is said to be locally
B-disentangled w.r.t. the ground-truth decoder f when f(Ztrain) = ˆf( ˆZtrain) and the mapping
v := f −1 ◦ˆf is a diffeomorphism from ˆZtrain to Ztrain with a mapping v : ˆZtrain →Ztrain satisfying
the following property: for all z ∈ˆZtrain, there exists a permutation π respecting B such that, for all
B ∈B, the columns of Dvπ(B)(z) ∈R|B|×dz outside block B are zero.
In Appendix A.4, we provide three examples where local disentanglement holds but not global disen-
tanglement. The first one illustrates how having a disconnected support can allow for a permutation π
(from Definition 4) that changes between disconnected regions of the support. The last two examples
show how, even if the permutation stays the same throughout the support, we can still violate global
disentanglement, even with a connected support.
We now state the main identifiability result of this work which provides conditions to guarantee local
disentanglement. We will then see how to go from local to global disentanglement in the subsequent
Theorem 2. For pedagogical reasons, we delay the formalization of the sufficient nonlinearity
Assumption 2 on which the result crucially relies.
Theorem 1 (Local disentanglement via additive decoders). Suppose that the data-generating process
satisfies Assumption 1, that the learned decoder ˆf : Rdz →Rdx is a C2-diffeomorphism, that the
encoder ˆg : Rdx →Rdz is continuous, that both f and ˆf are additive (Definition 1) and that f
is sufficiently nonlinear as formalized by Assumption 2. Then, if ˆf and ˆg solve the reconstruction
problem on the training distribution, i.e. Etrain||x −ˆf(ˆg(x))||2 = 0, we have that ˆf is locally
B-disentangled w.r.t. f (Definition 4) .
The proof of Theorem 1, which can be found in Appendix A.5, is inspired from Hyv¨arinen et al. [33].
The essential differences are that (i) they leverage the additivity of the conditional log-density of z
given an auxiliary variable u (i.e. conditional independence) instead of the additivity of the decoder
function f, (ii) we extend their proof techniques to allow for “block” disentanglement, i.e. when B
is not the trivial partition {{1}, . . . , {dz}}, (iii) the asssumption “sufficient variability” of the prior
p(z | u) of Hyv¨arinen et al. [33] is replaced by an analogous assumption of “sufficient nonlinearity”
of the decoder f (Assumption 2), and (iv) we consider much more general supports Ztrain which
makes the jump from local to global disentanglement less direct in our case.
The identifiability-expressivity trade-off. The level of granularity of the partition B controls the
trade-off between identifiability and expressivity: the finer the partition, the tighter the identifiability
guarantee but the less expressive is the function class. The optimal level of granularity is going to
dependent on the application at hand. Whether B could be learned from data is left for future work.
Sufficient nonlinearity. The following assumption is key in proving Theorem 2, as it requires that
the ground-truth decoder is “sufficiently nonlinear”. This is reminiscent of the “sufficient variability”
assumptions found in the nonlinear ICA litterature, which usually concerns the distribution of the
latent variable z as opposed to the decoder f [30, 31, 33, 36, 37, 42, 73]. We clarify this link in
Appendix A.6 and provide intuitions why sufficient nonlinearity can be satisfied when dx ≫dz.
Assumption 2 (Sufficient nonlinearity of f). Let q := dz + P
B∈B
|B|(|B|+1)
2
. For all z ∈Ztrain, f
is such that the following matrix has linearly independent columns (i.e. full column-rank):
W (z) :=
h
Dif (B)(zB)
i
i∈B
h
D2
i,i′f (B)(zB)
i
(i,i′)∈B2
≤

B∈B
∈Rdx×q ,
(6)
where B2
≤:= B2 ∩{(i, i′) | i′ ≤i}. Note this implies dx ≥q.
The following example shows that Theorem 1 does not apply if the ground-truth decoder f is linear. If
that was the case, it would contradict the well known fact that linear ICA with independent Gaussian
factors is unidentifiable.
Example 2 (Importance of Assumption 2). Suppose x = f(z) = Az where A ∈Rdx×dz is full
rank. Take ˆf(z) := AV z and ˆg(x) := V −1A†x where V ∈Rdz×dz is invertible and A† is
the left pseudo inverse of A. By construction, we have that E[x −ˆf(ˆg(x))] = 0 and f and ˆf are
6

B-additive because f(z) = P
B∈B A·,BzB and ˆf(z) = P
B∈B(AV )·,BzB. However, we still have
that v(z) := f −1 ◦ˆf(z) = V z where V does not necessarily have a block-permutation structure,
i.e. no disentanglement. The reason we cannot apply Theorem 1 here is because Assumption 2 is
not satisfied. Indeed, the second derivatives of f (B)(zB) := A·,BzB are all zero and hence W (z)
cannot have full column-rank.
Example 3 (A sufficiently nonlinear f). In Appendix A.7 we show numerically that the function
f(z) := [z1, z2
1, z3
1, z4
1]⊤+ [(z2 + 1), (z2 + 1)2, (z2 + 1)3, (z2 + 1)4]⊤
(7)
is a diffeomorphism from the square [−1, 0] × [0, 1] to its image that satisfies Assumption 2.
Example 4 (Smooth balls dataset is sufficiently nonlinear). In Appendix A.7 we present a simple
synthetic dataset consisting of images of two colored balls moving up and down. We also verify
numerically that its underlying ground-truth decoder f is sufficiently nonlinear.
3.1.1
From local to global disentanglement
The following result provides additional assumptions to guarantee global disentanglement (Defini-
tion 3) as opposed to only local disentanglement (Definition 4). See Appendix A.8 for its proof.
Theorem 2 (From local to global disentanglement). Suppose that all the assumptions of Theorem 1
hold. Additionally, assume Ztrain is path-connected (Definition 8) and that the block-specific decoders
f (B) and ˆf (B) are injective for all blocks B ∈B. Then, if ˆf and ˆg solve the reconstruction
problem on the training distribution, i.e. Etrain||x −ˆf(ˆg(x))||2 = 0, we have that ˆf is (globally)
B-disentangled w.r.t. f (Definition 3) and, for all B ∈B,
ˆf (B)(zB) = f (π(B))(¯vπ(B)(zB)) + c(B), for all zB ∈ˆZtrain
B
,
(8)
where the functions ¯vπ(B) are from Defintion 3 and the vectors c(B) ∈Rdx are constants such that
P
B∈B c(B) = 0. We also have that the functions ¯vπ(B) : ˆZtrain
B
→Ztrain
π(B) are C2-diffeomorphisms
and have the following form:
¯vπ(B)(zB) = (f π(B))−1( ˆf (B)(zB) −c(B)), for all zB ∈ˆZtrain
B
.
(9)
Equation (8) in the above result shows that each block-specific learned decoder ˆf (B) is “imitating”
a block-specific ground-truth decoder f π(B). Indeed, the “object-specific” image outputted by the
decoder ˆf (B) evaluated at some zB ∈ˆZtrain
B
is the same as the image outputted by f (B) evaluated at
v(zB) ∈Ztrain
B
, up to an additive constant vector c(B). These constants cancel each other out when
taking the sum of the block-specific decoders.
Regularly 
closed
Not regularly 
closed
Path-connected
Not path-connected
Figure 2: Illustrating regularly closed
sets (Definition 6) and path-connected
sets (Definition 8). Theorem 2 requires
Ztrain to satisfy both properties.
Equation (9) provides an explicit form for the function
¯vπ(B), which is essentially the learned block-specific de-
coder composed with the inverse of the ground-truth block-
specific decoder.
Additional assumptions to go from local to global. As-
suming that the support of Ptrain
z
, Ztrain, is path-connected
(see Definition 8 in appendix) is useful since it prevents
the permutation π of Definition 4 from changing between
two disconnected regions of ˆZtrain. See Figure 2 for an
illustration. In Appendix A.9, we discuss the additional
assumption that each f (B) must be injective and show
that, in general, it is not equivalent to the assumption that
P
B∈B f (B) is injective.
3.2
Cartesian-product extrapolation
In this section, we show how a learned additive decoder can be used to generate images x that are
“out of support” in the sense that x ̸∈f(Ztrain), but that are still on the manifold of “reasonable”
images, i.e. x ∈f(Ztest). To characterize the set of images the learned decoder can generate, we
will rely on the notion of “cartesian-product extension”, which we define next.
7

Definition 5 (Cartesian-product extension). Given a
set Z ⊆Rdz and partition B of [dz], we define the
Cartesian-product extension of Z as
CPEB(Z) :=
Y
B∈B
ZB , where ZB := {zB | z ∈Z}.
It is indeed an extension of Z since Z ⊆Q
B∈B ZB.
[                            ]
[                           ]
[                           ]
[                            ]
Figure 3: Illustration of Definition 5.
Let us define ¯v : CPEB( ˆZtrain) →CPEB(Ztrain) to be the natural extension of the function v :
ˆZtrain →Ztrain. More explicitly, ¯v is the “concatenation” of the functions ¯vB given in Definition 3:
¯v(z)⊤:= [¯vB1(zπ−1(B1))⊤· · · ¯vBℓ(zπ−1(Bℓ))⊤] ,
(10)
where ℓis the number of blocks in B. This map is a diffeomorphism because each ¯vπ(B) is a
diffeomorphism from ˆZtrain
B
to Ztrain
π(B) by Theorem 2.
We already know that ˆf(z) = f ◦¯v(z) for all z ∈ˆZtrain. The following result shows that this
equality holds in fact on the larger set CPEB( ˆZtrain), the Cartesian-product extension of ˆZtrain. See
right of Figure 1 for an illustration of the following corollary.
Corollary 3 (Cartesian-product extrapolation). Suppose the assumptions of Theorem 2 holds. Then,
for all z ∈CPEB( ˆZtrain),
X
B∈B
ˆf (B)(zB) =
X
B∈B
f (π(B))(¯vπ(B)(zB)) .
(11)
Furthermore, if CPEB(Ztrain) ⊆Ztest, then ˆf(CPEB( ˆZtrain)) ⊆f(Ztest).
Equation (11) tells us that the learned decoder ˆf “imitates” the ground-truth f not just over ˆZtrain,
but also over its Cartesian-product extension. This is important since it guarantees that we can
generate observations never seen during training as follows: Choose a latent vector znew that is in
the Cartesian-product extension of ˆZtrain, but not in ˆZtrain itself, i.e. znew ∈CPEB( ˆZtrain) \ ˆZtrain.
Then, evaluate the learned decoder on znew to get xnew := ˆf(znew). By Corollary 3, we know that
xnew = f ◦¯v(znew), i.e. it is the observation one would have obtain by evaluating the ground-truth
decoder f on the point ¯v(znew) ∈CPEB(Ztrain). In addition, this xnew has never been seen during
training since ¯v(znew) ̸∈¯v( ˆZtrain) = Ztrain. The experiment of Figure 4 illustrates this procedure.
About the extra assumption “CPEB(Ztrain) ⊆Ztest”. Recall that, in Assumption 1, we interpreted
f(Ztest) to be the set of “reasonable” observations x, of which we only observe a subset f(Ztrain).
Under this interpretation, Ztest is the set of reasonable values for the vector z and the additional
assumption that CPEB(Ztrain) ⊆Ztest in Corollary 3 requires that the Cartesian-product extension
of Ztrain consists only of reasonable values of z. From this assumption, we can easily conclude that
ˆf(CPEB( ˆZtrain)) ⊆f(Ztest), which can be interpreted as: “The novel observations xnew obtained
via Cartesian-product extrapolation are reasonable”. Appendix A.11 describes an example where the
assumption is violated, i.e. CPEB(Ztrain) ̸⊆Ztest. The practical implication of this is that the new
observations xnew obtained via Cartesian-product extrapolation might not always be reasonable.
Disentanglement is not enough for extrapolation. To the best of our knowledge, Corollary 3 is the
first result that formalizes how disentanglement can induce extrapolation. We believe it illustrates the
fact that disentanglement alone is not sufficient to enable extrapolation and that one needs to restrict
the hypothesis class of decoders in some way. Indeed, given a learned decoder ˆf that is disentangled
w.r.t. f on the training support Ztrain, one cannot guarantee both decoders will “agree” outside the
training domain without further restricting ˆf and f. This work has focused on “additivity”, but we
believe other types of restriction could correspond to other types of extrapolation.
4
Experiments
We now present empirical validations of the theoretical results presented earlier. To achieve this,
we compare the ability of additive and non-additive decoders to both identify ground-truth latent
factors (Theorems 1 & 2) and extrapolate (Corollary 3) when trained to solve the reconstruction task
on simple images (64 × 64 × 3) consisting of two balls moving in space [2]. See Appendix B.1
8

ScalarLatents
BlockLatents
BlockLatents
(independent z)
(dependent z)
Decoders
RMSE
LMSSpear
RMSEOOS
LMSOOS
Spear
RMSE
LMSTree
RMSE
LMSTree
Non-add.
.06 ±.002
70.6±5.21
.18±.012
73.7±4.64
.02±.001
53.9±7.58
.02±.001
78.1±2.92
Additive
.06±.002
91.5±3.57
.11±.018
89.5±5.02
.03±.012
92.2±4.91
.01±.002
99.9±0.02
Table 1: Reporting reconstruction mean squared error (RMSE ↓) and the Latent Matching Score
(LMS ↑) for the three datasets considered: ScalarLatents and BlockLatents with independent and
dependent latents. Runs were repeated with 10 random initializations. RMSEOOS and LMSOOS
Spear are
the same metric but evaluated out of support (see Appendix B.3 for details). While the standard error
is high, the differences are still clear as can be seen in their box plot version in Appendix B.4.
for training details. We consider two datasets: one where the two ball positions can only vary
along the y-axis (ScalarLatents) and one where the positions can vary along both the x and y axes
(BlockLatents).
ScalarLatents: The ground-truth latent vector z ∈R2 is such that z1 and z2 corresponds to
the height (y-coordinate) of the first and second ball, respectively. Thus the partition is simply
B = {{1}, {2}} (each object has only one latent factor). This simple setting is interesting to study
since the low dimensionality of the latent space (dz = 2) allows for exhaustive visualizations like
Figure 4. To study Cartesian-product extrapolation (Corollary 3), we sample z from a distribution
with a L-shaped support given by Ztrain := [0, 1] × [0, 1] \ [0.5, 1] × [0.5, 1], so that the training set
does not contain images where both balls appear in the upper half of the image (see Appendix B.2).
BlockLatents: The ground-truth latent vector z ∈R4 is such that z{1,2} and z{3,4} correspond to the
x, y position of the first and second ball, respectively (the partition is simply B = {{1, 2}, {3, 4}},
i.e. each object has two latent factors). Thus, this more challenging setting illustrates “block-
disentanglement”. The latent z is sampled uniformly from the hypercube [0, 1]4 but the images
presenting occlusion (when a ball is behind another) are rejected from the dataset. We discuss how
additive decoders cannot model images presenting occlusion in Appendix A.12. We also present an
additional version of this dataset where we sample from the hypercube [0, 1]4 with dependencies. See
Appendix B.2 for more details about data generation.
Evaluation metrics: To evaluate disentanglement, we compute a matrix of scores (sB,B′) ∈Rℓ×ℓ
where ℓis the number of blocks in B and sB,B′ is a score measuring how well we can predict the
ground-truth block zB from the learned latent block ˆzB′ = ˆgB′(x) outputted by the encoder. The
final Latent Matching Score (LMS) is computed as LMS = arg maxπ∈SB
1
ℓ
P
B∈B sB,π(B), where
SB is the set of permutations respecting B (Definition 2). When B := {{1}, . . . , {dz}} and the
score used is the absolute value of the correlation, LMS is simply the mean correlation coefficient
(MCC), which is widely used in the nonlinear ICA literature [30, 31, 33, 36, 42]. Because our theory
guarantees recovery of the latents only up to invertible and potentially nonlinear transformations,
we use the Spearman correlation, which can capture nonlinear relationships unlike the Pearson
correlation. We denote this score by LMSSpear and will use it in the dataset ScalarLatents. For
the BlockLatents dataset, we cannot use Spearman correlation (because zB are two dimensional).
Instead, we take the score sB,B′ to be the R2 score of a regression tree. We denote this score by
LMStree. There are subtleties to take care of when one wants to evaluate LMStree on a non-additive
model due to the fact that the learned representation does not have a natural partition B. We must thus
search over partitions. We discuss this and provide further details on the metrics in Appendix B.3.
4.1
Results
Additivity is important for disentanglement. Table 1 shows that the additive decoder obtains a much
higher LMSSpear & LMSTree than its non-additive counterpart on all three datasets considered, even if
both decoders have very small reconstruction errors. This is corroborated by the visualizations of
Figures 4 & 5. Appendix B.5 additionally shows object-specific reconstructions for the BlockLatents
dataset. We emphasize that disentanglement is possible even when the latent factors are dependent
(or causally related), as shown on the ScalarLatents dataset (L-shaped support implies dependencies)
and on the BlockLatents dataset with dependencies (Table 1). Note that prior works have relied on
interventions [3, 2, 8] or Cartesian-product supports [68, 62] to deal with dependencies.
9

(a) Additive decoder
(b) Non-additive decoder
Figure 4: Figure (a) shows latent representation outputted by the encoder ˆg(x) over the training
dataset, and the corresponding reconstructed images of the additive decoder with median LMSSpear
among runs performed on the ScalarLatents dataset. Figure (b) shows the same thing for the
non-additive decoder. The color gradient corresponds to the value of one of the ground-truth factor,
the red dots correspond to factors used to generate the images and the yellow dashed square highlights
extrapolated images.
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(a) Additive Decoder
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(b) Non-Additive Decoder
Figure 5: Latent responses for the case of independent latents in the BlockLatent dataset. In each
plot, we report the latent factors predicted from multiple images where one ball moves along only
one axis at a time. For the additive case, at most two latents change, as it should, while more than
two latents change for the non-additive case. See Appendix B.5 for details.
Additivity is important for Cartesian-product extrapolation. Figure 4 illustrates that the additive
decoder can generate images that are outside the training domain (both balls in upper half of the
image) while its non-additive counterpart cannot. Furthermore, Table 1 also corroborates this showing
that the “out-of-support” (OOS) reconstruction MSE and LMSSpear (evaluated only on the samples
never seen during training) are significantly better for the additive than for the non-additive decoder.
Importance of connected support. Theorem 2 required that the support of the latent factors, Ztrain,
was path-connected. Appendix B.6 shows experiments where this assumption is violated, which
yields lower LMSSpear for the additive decoder, thus highlighting the importance of this assumption.
5
Conclusion
We provided an in-depth identifiability analysis of additive decoders, which bears resemblance
to standard decoders used in OCRL, and introduced a novel theoretical framework showing how
this architecture can generate reasonable images never seen during training via “Cartesian-product
extrapolation”. We validated empirically both of these results and confirmed that additivity was
indeed crucial. By studying rigorously how disentanglement can induce extrapolation, our work
highlighted the necessity of restricting the decoder to extrapolate and set the stage for future works to
explore disentanglement and extrapolation in other function classes such as masked decoders typically
used in OCRL. We postulate that the type of identifiability analysis introduced in this work has the
potential of expanding our understanding of creativity in generative models, ultimately resulting in
representations that generalize better.
10

Acknowledgements
This research was partially supported by the Canada CIFAR AI Chair Program, by an IVADO
excellence PhD scholarship and by Samsung Electronics Co., Ldt. The experiments were in part
enabled by computational resources provided by Calcul Qu´ebec (calculquebec.ca) and the Digital
Research Alliance of Canada (alliancecan.ca). Simon Lacoste-Julien is a CIFAR Associate
Fellow in the Learning in Machines & Brains program.
References
[1] K. Ahuja, J. Hartford, and Y. Bengio. Properties from mechanisms: an equivariance per-
spective on identifiable representation learning. In International Conference on Learning
Representations, 2022.
[2] K. Ahuja, J. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse
perturbations, 2022.
[3] K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. Interventional causal representation learning.
In Proceedings of the 40th International Conference on Machine Learning, 2023.
[4] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 2013.
[5] M. Besserve, R. Sun, D. Janzing, and B. Sch¨olkopf. A theory of independent mechanisms for
extrapolation in generative models. In Proceedings of the 35th AAAI Conference on Artificial
Intelligence (AAAI), 2021.
[6] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke,
J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of
Python+NumPy programs, 2018. URL http://github.com/google/jax.
[7] J. Brady, R. S. Zimmermann, Y. Sharma, B. Sch¨olkopf, J. von K¨ugelgen, and W. Brendel.
Provably learning object-centric representations. In International Conference on Machine
Learning, 2023.
[8] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation
learning. In Advances in Neural Information Processing Systems, 2022.
[9] S. Buchholz, M. Besserve, and B. Sch¨olkopf. Function classes for identifiable nonlinear
independent component analysis. In Advances in Neural Information Processing Systems, 2022.
[10] S. Buchholz, G. Rajendran, E. Rosenfeld, B. Aragam, B. Sch¨olkopf, and P. Ravikumar. Learning
linear causal representations from interventions under general nonlinear mixing, 2023.
[11] C. P. Burgess, L. Matthey, N. Watters, R. Kabra, I. Higgins, M. Botvinick, and A. Lerchner.
Monet: Unsupervised scene decomposition and representation, 2019.
[12] E. Crawford and J. Pineau. Spatially invariant unsupervised object detection with convolutional
neural networks. Proceedings of the AAAI Conference on Artificial Intelligence, 2019.
[13] A. S. d’Avila Garcez and L. Lamb. Neurosymbolic AI: The 3rd wave. ArXiv, abs/2012.05876,
2020.
[14] A. Dittadi, S. S. Papa, M. De Vita, B. Sch¨olkopf, O. Winther, and F. Locatello. Generalization
and robustness implications in object-centric learning. In Proceedings of the 39th International
Conference on Machine Learning, 2022.
[15] D. Donoho and C. Grimes. Image manifolds which are isometric to euclidean space. Journal of
Mathematical Imaging and Vision, 2003.
[16] D. L. Donoho and C. Grimes. Hessian eigenmaps: Locally linear embedding techniques for
high-dimensional data. Proceedings of the National Academy of Sciences, 2003.
11

[17] Y. Du and I. Mordatch. Implicit generation and modeling with energy based models. In
Advances in Neural Information Processing Systems, 2019.
[18] M. Engelcke, A. R. Kosiorek, O. P. Jones, and I. Posner. Genesis: Generative scene inference
and sampling with object-centric latent representations. In International Conference on Learning
Representations, 2020.
[19] S. M. A. Eslami, N. Heess, T. Weber, Y. Tassa, D. Szepesvari, K. Kavukcuoglu, and G. E.
Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, 2016.
[20] J. A. Fodor and Z. W. Pylyshyn. Connectionism and cognitive architecture: A critical analysis.
Cognition, 1988.
[21] A. Goyal and Y. Bengio. Inductive biases for deep learning of higher-level cognition. Proc. R.
Soc. A 478: 20210068, 2022.
[22] K. Greff, A. Rasmus, M. Berglund, T. Hao, H. Valpola, and J. Schmidhuber. Tagger: Deep
unsupervised perceptual grouping. In Advances in Neural Information Processing Systems,
2016.
[23] K. Greff, S. van Steenkiste, and J. Schmidhuber. Neural expectation maximization. In Advances
in Neural Information Processing Systems, 2017.
[24] K. Greff, R. L. Kaufman, R. Kabra, N. Watters, C. Burgess, D. Zoran, L. Matthey, M. Botvinick,
and A. Lerchner. Multi-object representation learning with iterative variational inference. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
[25] K. Greff, S. van Steenkiste, and J. Schmidhuber. On the binding problem in artificial neural
networks. ArXiv, abs/2012.05208, 2020.
[26] L. Gresele, J. V. K¨ugelgen, V. Stimper, B. Sch¨olkopf, and M. Besserve. Independent mechanism
analysis, a new concept? In Advances in Neural Information Processing Systems, 2021.
[27] H. H¨alv¨a, S. L. Corff, L. Leh´ericy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentan-
gling identifiable features from noisy data with structured nonlinear ICA. In A. Beygelzimer,
Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Information Processing
Systems, 2021.
[28] S. Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 1990.
[29] D. Horan, E. Richardson, and Y. Weiss. When is unsupervised disentanglement possible? In
Advances in Neural Information Processing Systems, 2021.
[30] A. Hyv¨arinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ica. In Advances in Neural Information Processing Systems, 2016.
[31] A. Hyv¨arinen and H. Morioka. Nonlinear ICA of Temporally Dependent Stationary Sources. In
Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.
[32] A. Hyv¨arinen and P. Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural Networks, 1999.
[33] A. Hyv¨arinen, H. Sasaki, and R. E. Turner.
Nonlinear ica using auxiliary variables and
generalized contrastive learning. In AISTATS. PMLR, 2019.
[34] Y. Jiang and B. Aragam. Learning nonparametric latent causal graphs with unknown interven-
tions, 2023.
[35] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. B. Girshick.
Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
12

[36] I. Khemakhem, D. Kingma, R. Monti, and A. Hyv¨arinen. Variational autoencoders and nonlinear
ica: A unifying framework. In Proceedings of the Twenty Third International Conference on
Artificial Intelligence and Statistics, 2020.
[37] I. Khemakhem, R. Monti, D. Kingma, and A. Hyv¨arinen. Ice-beem: Identifiable conditional
energy-based deep models based on nonlinear ica. In Advances in Neural Information Processing
Systems, 2020.
[38] D. A. Klindt, L. Schott, Y. Sharma, I. Ustyuzhaninov, W. Brendel, M. Bethge, and D. M.
Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In 9th
International Conference on Learning Representations, 2021.
[39] D. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. Le Priol, and
A. Courville. Out-of-distribution generalization via risk extrapolation (rex). In Proceedings of
the 38th International Conference on Machine Learning, 2021.
[40] S. Lachapelle and S. Lacoste-Julien. Partial disentanglement via mechanism sparsity. In UAI
2022 Workshop on Causal Representation Learning, 2022.
[41] S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, and Q. Bertrand.
Synergies between disentanglement and sparsity: a multi-task learning perspective, 2022.
[42] S. Lachapelle, P. Rodriguez Lopez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and
S. Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for
nonlinear ICA. In First Conference on Causal Learning and Reasoning, 2022.
[43] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn
and think like people. Behavioral and Brain Sciences, 2017.
[44] F. Leeb, G. Lanzillotta, Y. Annadani, M. Besserve, S. Bauer, and B. Sch¨olkopf. Structure by
architecture: Disentangled representations without regularization, 2021.
[45] Z. Lin, Y. Wu, S. V. Peri, W. Sun, G. Singh, F. Deng, J. Jiang, and S. Ahn. Space: Unsupervised
object-oriented scene representation via spatial attention and decomposition. In International
Conference on Learning Representations, 2020.
[46] P. Lippe, S. Magliacane, S. L¨owe, Y. M. Asano, T. Cohen, and E. Gavves. iCITRIS: Causal
representation learning for instantaneous temporal effects. In UAI 2022 Workshop on Causal
Representation Learning, 2022.
[47] P. Lippe, S. Magliacane, S. L¨owe, Y. M. Asano, T. Cohen, and E. Gavves. CITRIS: Causal
identifiability from temporal intervened sequences, 2022.
[48] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Sch¨olkopf, and O. Bachem. Chal-
lenging common assumptions in the unsupervised learning of disentangled representations. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
[49] F. Locatello, B. Poole, G. Raetsch, B. Sch¨olkopf, O. Bachem, and M. Tschannen. Weakly-
supervised disentanglement without compromises. In Proceedings of the 37th International
Conference on Machine Learning, 2020.
[50] F. Locatello, M. Tschannen, S. Bauer, G. R¨atsch, B. Sch¨olkopf, and O. Bachem. Disentangling
factors of variations using few labels. In International Conference on Learning Representations,
2020.
[51] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Doso-
vitskiy, and T. Kipf.
Object-centric learning with slot attention.
In Advances in Neural
Information Processing Systems, 2020.
[52] A. Mansouri, J. Hartford, K. Ahuja, and Y. Bengio. Object-centric causal representation learning.
In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations, 2022.
[53] G. F. Marcus. The algebraic mind : integrating connectionism and cognitive science, 2001.
13

[54] G. E. Moran, D. Sridhar, Y. Wang, and D. Blei. Identifiable deep generative models via sparse
decoding. Transactions on Machine Learning Research, 2022.
[55] J. Munkres. Analysis On Manifolds. Basic Books, 1991.
[56] J. R. Munkres. Topology. Prentice Hall, Inc., 2 edition, 2000.
[57] J. Pearl. The seven tools of causal inference, with reflections on machine learning. Commun.
ACM, 2019.
[58] W. Peebles, J. Peebles, J.-Y. Zhu, A. A. Efros, and A. Torralba. The hessian penalty: A weak
prior for unsupervised disentanglement. In Proceedings of European Conference on Computer
Vision (ECCV), 2020.
[59] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image
generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
[60] P. Reizinger, L. Gresele, J. Brady, J. V. K¨ugelgen, D. Zietlow, B. Sch¨olkopf, G. Martius,
W. Brendel, and M. Besserve. Embrace the gap: VAEs perform independent mechanism
analysis. In Advances in Neural Information Processing Systems, 2022.
[61] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis
with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2022.
[62] K. Roth, M. Ibrahim, Z. Akata, P. Vincent, and D. Bouchacourt. Disentanglement of correlated
factors via hausdorff factorized support. In The Eleventh International Conference on Learning
Representations, 2023.
[63] B. Sch¨olkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio.
Toward causal representation learning. Proceedings of the IEEE - Advances in Machine Learning
and Deep Neural Networks, 2021.
[64] C. Squires, A. Seigal, S. Bhate, and C. Uhler. Linear causal disentanglement via interventions.
In Proceedings of the 40th International Conference on Machine Learning, 2023.
[65] A. Taleb and C. Jutten. Source separation in post-nonlinear mixtures. IEEE Transactions on
Signal Processing, 1999.
[66] J. Von K¨ugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Sch¨olkopf, M. Besserve, and F. Lo-
catello. Self-supervised learning with data augmentations provably isolates content from style.
In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.
[67] J. von K¨ugelgen, M. Besserve, W. Liang, L. Gresele, A. Keki´c, E. Bareinboim, D. M. Blei,
and B. Sch¨olkopf. Nonparametric identifiability of causal representations from unknown
interventions, 2023.
[68] Y. Wang and M. I. Jordan. Desiderata for representation learning: A causal perspective, 2022.
[69] Z. Wang, L. Gui, J. Negrea, and V. Veitch. Concept algebra for text-controlled vision models,
2023.
[70] T. W. Webb, Z. Dulberg, S. M. Frankland, A. A. Petrov, R. C. O’Reilly, and J. D. Cohen.
Learning representations that support extrapolation. In Proceedings of the 37th International
Conference on Machine Learning, 2020.
[71] Q. Xi and B. Bloem-Reddy. Indeterminacy in generative models: Characterization and strong
identifiability. In Proceedings of The 26th International Conference on Artificial Intelligence
and Statistics, 2023.
[72] J. Zhang, C. Squires, K. Greenewald, A. Srivastava, K. Shanmugam, and C. Uhler. Identifiability
guarantees for causal disentanglement from soft interventions, 2023.
[73] Y. Zheng, I. Ng, and K. Zhang. On the identifiability of nonlinear ICA: Sparsity and beyond. In
Advances in Neural Information Processing Systems, 2022.
14

Appendix
Table of Contents
A Identifiability and Extrapolation Analysis
16
A.1
Useful definitions and lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.2
Relationship between additive decoders and the diagonal Hessian penalty . . . .
18
A.3
Additive decoders form a superset of compositional decoders [7] . . . . . . . . .
19
A.4
Examples of local but non-global disentanglement . . . . . . . . . . . . . . . .
20
A.5
Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
A.6
Sufficient nonlinearity v.s. sufficient variability in nonlinear ICA with auxiliary
variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
A.7
Examples of sufficiently nonlinear additive decoders . . . . . . . . . . . . . . .
26
A.8
Proof of Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
A.9
Injectivity of object-specific decoders v.s. injectivity of their sum
. . . . . . . .
30
A.10 Proof of Corollary 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
A.11 Will all extrapolated images make sense? . . . . . . . . . . . . . . . . . . . . .
32
A.12 Additive decoders cannot model occlusion
. . . . . . . . . . . . . . . . . . . .
32
B
Experiments
32
B.1
Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
B.2
Datasets Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
B.3
Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
B.4
Boxplots for main experiments (Table 1)
. . . . . . . . . . . . . . . . . . . . .
34
B.5
Additional Results: BlockLatents Dataset . . . . . . . . . . . . . . . . . . . . .
35
B.6
Disconnected Support Experiments . . . . . . . . . . . . . . . . . . . . . . . .
37
B.7
Additional Results: ScalarLatents Dataset . . . . . . . . . . . . . . . . . . . . .
38
15

Table 2: Table of Notation.
Calligraphic & indexing conventions
[n]
:=
{1, 2, . . . , n}
x
Scalar (random or not, depending on context)
x
Vector (random or not, depending on context)
X
Matrix
X
Set/Support
f
Scalar-valued function
f
Vector-valued function
f

A
Restriction of f to the set A
Df, Df
Jacobian of f and f
D2f
Hessian of f
B ⊆[n]
Subset of indices
|B|
Cardinality of the set B
xB
Vector formed with the ith coordinates of x, for all i ∈B
XB,B′
Matrix formed with the entries (i, j) ∈B × B′ of X.
Given X ⊆Rn, XB
:=
{xB | x ∈X} (projection of X)
Recurrent notation
x ∈Rdx
Observation
z ∈Rdz
Vector of latent factors of variations
Z ⊆Rdz
Support of z
f
Ground-truth decoder function
ˆf
Learned decoder function
B
A partition of [dz] (assumed contiguous w.l.o.g.)
B ∈B
A block of the partition B
B(i) ∈B
The unique block of B that contains i
π : [dz] →[dz]
A permutation
SB
:=
S
B∈B B2
Sc
B
:=
[dz]2 \ SB
Rdz×dz
SB
:=
{M ∈Rdz×dz | (i, j) ̸∈SB =⇒Mi,j = 0}
General topology
X
Closure of the subset X ⊆Rn in the standard topology of Rn
X ◦
Interior of the subset X ⊆Rn in the standard topology of Rn
A
Identifiability and Extrapolation Analysis
A.1
Useful definitions and lemmas
We start by recalling some notions of general topology that are going to be used later on. For a proper
introduction to these concepts, see for example Munkres [56].
Definition 6 (Regularly closed sets). A set Z ⊆Rdz is regularly closed if Z = Z◦, i.e. if it is equal
to the closure of its interior (in the standard topology of Rn).
Definition 7 (Connected sets). A set Z ⊆Rdz is connected if it cannot be written as a union of
non-empty and disjoint open sets (in the subspace topology).
Definition 8 (Path-connected sets). A set Z ⊆Rdz is path-connected if for all pair of points
z0, z1 ∈Z, there exists a continuous map ϕ : [0, 1] →Z such that ϕ(0) = z0 and ϕ(1) = z1.
Such a map is called a path between z0 and z1.

Definition 9 (Homeomorphism). Let A and B be subsets of Rn equipped with the subspace topology.
A function f : A →B is an homeomorphism if it is bijective, continuous and its inverse is continuous.
The following technical lemma will be useful in the proof of Theorem 1. For it, we will need
additional notation: Let S ⊆A ⊆Rn. We already saw that S refers to the closure S in the Rn
topology. We will denote by clA(S) the closure of S in the subspace topology of A induced by Rn,
which is not necessarily the same as S. In fact, both can be related via clA = S ∩A (see Munkres
[56, Theorem 17.4, p.95]).
Lemma 4. Let A, B ⊆Rn and suppose there exists an homeomorphism f : A →B. If A is
regularly closed in Rn, we have that B ⊆B◦.
Proof. Note that f

A◦is a continuous injective function from the open set A◦to f(A◦). By the
“invariance of domain” theorem [56, p.381], we have that f(A◦) must be open in Rn. Of course,
we have that f(A◦) ⊆B, and thus f(A◦) ⊆B◦(the interior of B is the largest open set contained
in B). Analogously, f −1
B◦is a continuous injective function from the open set B◦to f −1(B◦).
Again, by “invariance of domain”, f −1(B◦) must be open in Rn and thus f −1(B◦) ⊆A◦. We can
conclude that f(A◦) = B◦.
We can conclude as follow:
B = f(A) = f(A◦) = f(A◦∩A) = f(clA(A◦)) ⊆clB(f(A◦)) = clB(B◦) = B◦∩B ⊆B◦,
where the first inclusion holds by continuity of f [56, Thm.18.1 p.104].
This lemma is taken from [42].
Lemma 5 (Sparsity pattern of an invertible matrix contains a permutation). Let L ∈Rm×m be an
invertible matrix. Then, there exists a permutation σ such that Li,σ(i) ̸= 0 for all i.
Proof. Since the matrix L is invertible, its determinant is non-zero, i.e.
det(L) :=
X
π∈Sm
sign(π)
m
Y
i=1
Li,π(i) ̸= 0 ,
(12)
where Sm is the set of m-permutations. This equation implies that at least one term of the sum is
non-zero, meaning there exists π ∈Sm such that for all i ∈[m], Li,π(i) ̸= 0.
Definition 10 (Aligned subspaces of Rm×n). Given a subset S ⊆{1, ..., m} × {1, ..., n}, we define
Rm×n
S
:= {M ∈Rm×n | (i, j) ̸∈S =⇒Mi,j = 0} .
(13)
Definition 11 (Useful sets). Given a partition B of [d], we define
SB :=
[
B∈B
B2
Sc
B := {1, . . . , dz}2 \ SB
(14)
Definition 12 (Ck-diffeomorphism). Let A ⊆Rn and B ⊆Rm. A map f : A →B is said to be a
Ck-diffeomorphism if it is bijective, C2 and has a C2 inverse.
Remark 2. Differentiability is typically defined for functions that have an open domain in Rn.
However, in the definition above, the set A might not be open in Rn and B might not be open in Rm.
In the case of an arbitrary domain A, it is customary to say that a function f : A ⊆Rn →Rm is Ck
if there exists a Ck function g defined on an open set U ⊆Rn that contains A such that g

A = f
(i.e. g extends f). With this definition, we have that a composition of Ck functions is Ck, as usual.
See for example p.199 of Munkres [55].
The following lemma allows us to unambiguously define the k first derivatives of a Ck function
f : A →Rm on the set A◦.
Lemma 6. Let A ⊆Rn and f : A →Rm be a Ck function. Then, its k first derivatives is uniquely
defined on A◦in the sense that they do not depend on the specific choice of Ck extension.
17

Proof. Let g : U →Rn and h : V →Rn be two Ck extensions of f to U ⊆Rn and V ⊆Rn both
open in Rn. By definition,
g(x) = f(x) = h(x), ∀x ∈A .
(15)
The usual derivative is uniquely defined on the interior of the domain, so that
Dg(x) = Df(x) = Dh(x), ∀x ∈A◦.
(16)
Consider a point x0 ∈A◦. By definition of closure, there exists a sequence {xk}∞
k=1 ⊆A◦s.t.
limk→∞xk = x0. We thus have that
lim
k→∞Dg(xk) = lim
k→∞Dh(xk)
(17)
Dg(x0) = Dh(x0) ,
(18)
where we used the fact that the derivatives of g and h are continuous to go to the second line. Thus,
all the Ck extensions of f must have equal derivatives on A◦. This means we can unambiguously
define the derivative of f everywhere on A◦to be equal to the derivative of one of its Ck extensions.
Since f is Ck, its derivative Df is Ck−1, we can thus apply the same argument to get that the second
derivative of f is uniquely defined on A◦◦. It can be shown that A◦◦= A◦. One can thus apply the
same argument recursively to show that the first k derivatives of f are uniquely defined on A◦.
Definition 13 (Ck-diffeomorphism onto its image). Let A ⊆Rn. A map f : A →Rm is said
to be a Ck-diffeomorphism onto its image if the restriction f to its image ˜f : A →f(A) is a
Ck-diffeomorphism.
Remark 3. If S ⊆A ⊆Rn and f : A →Rm is a Ck-diffeomorphism on its image, then the
restriction of f to S, i.e. f

S, is also a Ck diffeomorphism on its image. That is because f

S is
clearly bijective, is Ck (simply take the Ck extension of f) and so is its inverse (simply take the Ck
extension of f −1).
A.2
Relationship between additive decoders and the diagonal Hessian penalty
Proposition 7 (Equivalence between additivity and diagonal Hessian). Let f : Rdz →Rdx be a C2
function. Then,
∀z ∈Rdz, f(z) = P
B∈B f (B)(zB)
where f (B) : R|B| →Rdx is C2.
⇐⇒
∀k ∈[dx], z ∈Rdz, D2fk(z) is
block diagonal with blocks in B.
(19)
Proof. We start by showing the “ =⇒” direction. Let B and B′ be two distinct blocks of B. Let
i ∈B and i′ ∈B′. We can compute the derivative of fk w.r.t. zi:
Difk(z) =
X
¯
B∈B
Dif ( ¯
B)
k
(z ¯
B) = Dif (B)
k
(zB) ,
(20)
where the last equality holds because i ∈B and not in any other block ¯B. Furthermore,
D2
i,i′fk(z) = D2
i,i′f (B)
k
(zB) = 0 ,
(21)
where the last equality holds because i′ ̸∈B. This shows that D2fk(z) is block diagonal.
We now show the “ ⇐= ” direction. Fix k ∈[dx], B ∈B. We know that D2
B,Bcfk(z) = 0 for
all z ∈Rdz. Fix z ∈Rdz. Consider a continuously differentiable path ϕ : [0, 1] →R|Bc| such
that ϕ(0) = 0 and ϕ(1) = zBc. As D2
B,Bcfk(z) is a continuous function of z, we can use the
fundamental theorem of calculus for line integrals to get that
DBfk(zB, zBc) −DBfk(zB, 0) =
Z 1
0
D2
B,Bcfk(zB, ϕ(t))
|
{z
}
=0
ϕ′(t)dt = 0 ,
(22)
(where D2
B,Bcfk(zB, ϕ(t))ϕ′(t) denotes a matrix-vector product) which implies that
DBfk(z) = DBfk(zB, 0) .
(23)
18

And the above equality holds for all B ∈B and all z ∈Rdz.
Choose an arbitrary z ∈Rdz. Consider a continously differentiable path ψ : [0, 1] →Rdz such that
ψ(0) = 0 and ψ(1) = z. By applying the fundamental theorem of calculus for line integrals once
more, we have that
fk(z) −fk(0) =
Z 1
0
Dfk(ψ(t))ψ′(t)dt
(24)
=
Z 1
0
X
B∈B
DBfk(ψ(t))ψ′
B(t)dt
(25)
=
X
B∈B
Z 1
0
DBfk(ψ(t))ψ′
B(t)dt
(26)
=
X
B∈B
Z 1
0
DBfk(ψB(t), 0)ψ′
B(t)dt ,
(27)
where the last equality holds by (23). We can further apply the fundamental theorem of calculus for
line integrals to each term
R 1
0 DBfk(ψB(t), 0)ψ′
B(t)dt to get
fk(z) −fk(0) =
X
B∈B
(fk(zB, 0) −fk(0, 0))
(28)
=⇒fk(z) = fk(0) +
X
B∈B
(fk(zB, 0) −fk(0))
(29)
=
X
B∈B

fk(zB, 0) −|B| −1
|B|
fk(0)

|
{z
}
f (B)
k
(zB):=
.
(30)
and since z was arbitrary, the above holds for all z ∈Rdz. Note that the functions f (B)
k
(zB) must be
C2 because fk is C2. This concludes the proof.
A.3
Additive decoders form a superset of compositional decoders [7]
Compositional decoders were introduced by Brady et al. [7] as a suitable class of functions to perform
object-centric representation learning with identifiability guarantees. They are also interested in
block-disentanglement, but, contrarily to our work, they assume that the latent vector z is fully
supported, i.e. Z = Rdz. We now rewrite the definition of compositional decoders in the notation
used in this work:
Definition 14 (Compositional decoders, adapted from [7]). Given a partition B, a differentiable
decoder f : Rdz →Rdx is said to be compositional w.r.t. B whenever the Jacobian Df(z) is such
that for all i ∈[dz], B ∈B, z ∈Rdz, we have
DBfi(z) ̸= 0 =⇒DBcfi(z) = 0 ,
where Bc is the complement of B ∈B.
In other words, each line of the Jacobian can have nonzero values only in one block B ∈B. Note
that this nonzero block can change with different values of z.
The next result shows that additive decoders form a superset of C2 compositional decoders (Brady
et al. [7] assumed only C1). Note that additive decoders are strictly more expressive than C2
compositional decoders because some additive functions are not compositional, like Example 3 for
instance.
Proposition 8 (Compositional implies additive). Given a partition B, if f : Rdz →Rdx is composi-
tional (Definition 14) and C2, then it is also additive (Definition 1).
Proof. Choose any i ∈[dx]. Our strategy will be to show that D2fi is block diagonal everywhere on
Rdz and use Proposition 7 to conclude that fi is additive.
19

Choose an arbitrary z0 ∈Rdz.
By compositionality, there exists a block B ∈B such that
DBcfi(z0) = 0. We consider two cases separately:
Case 1 Assume DBfi(z0) ̸= 0. By continuity of DBfi, there exists an open neighborhood of z0, U,
s.t. for all z ∈U, DBfi(z) ̸= 0. By compositionality, this means that, for all z ∈U, DBcfi(z) = 0.
When a function is zero on an open set, its derivative must also be zero, hence DDBcfi(z0) = 0.
Because f is C2, the Hessian is symmetric so that we also have DBcDfi(z0) = 0. We can thus
conclude that the Hessian D2fi(z0) is such that all entries are zero except possibly for D2fi(z0)B,B.
Hence, D2fi(z0) is block diagonal with blocks in B.
Case 2: Assume DBfi(z0) = 0. This means the whole row of the Jacobian is zero, i.e. Dfi(z0) = 0.
By continuity of Dfi, we have that the set V := (Dfi)−1({0}) is closed. Thus this set decomposes
as V = V ◦∪∂V where V ◦and ∂V are the interior and boundary of V , respectively.
Case 2.1: Suppose z0 ∈V ◦. Then we can take a derivative so that D2fi(z0) = 0, which of course
means that D2fi(z0) is diagonal.
Case 2.2: Suppose z0 ∈∂V . By the definition of boundary, for all open set U containing z0, U
intersects with the complement of V , i.e. (Dfi)−1(Rdz \ {0}). This means we can construct a
sequence {zk}∞
k=1 ⊆V c which converges to z0. By Case 1, we have that for all k ≥1, D2fi(zk) is
block diagonal. This means that limk→∞D2fi(zk) is block diagonal. Moreover, by continuity of
D2fi, we have that limk→∞D2fi(zk) = D2fi(z0). Hence D2fi(z0) is block diagonal.
We showed that for all z0 ∈Rdz, D2fi(z0) is block diagonal. Hence, f is additive by Proposition 7.
A.4
Examples of local but non-global disentanglement
In this section, we provide examples of mapping v : ˆZtrain →Ztrain that satisfy the local disentangle-
ment property of Definition 4, but not the global disentanglement property of Definition 3. Note that
these notions are defined for pairs of decoders f and ˆf, but here we construct directly the function v
which is usually defined as f −1 ◦ˆf. However, given v we can always define f and ˆf to be such
that f −1 ◦ˆf = v: Simply take f(z) := [z1, . . . , zdz, 0, . . . , 0]⊤∈Rdx and ˆf := f ◦v. This
construction however yields a decoder f that is not sufficiently nonlinear (Assumption 2). Clearly the
mappings v that we provide in the following examples cannot be written as compositions of decoders
f −1 ◦ˆf where f and ˆf satisfy all assumptions of Theorem 2, as this would contradict the theorem.
In Examples 5 & 6, the path-connected assumption of Theorem 2 is violated. In Example 7, it is less
obvious to see which assumptions would be violated.
Example 5 (Disconnected support with changing permutation). Let v : ˆZ →R2 s.t. ˆZ = ˆZ(1) ∪
ˆZ(2) ⊆R2 where ˆZ(1) = {z ∈R2 | z1 ≤0 and z2 ≤0} and ˆZ(2) = {z ∈R2 | z1 ≥1 and z2 ≥
1}. Assume
v(z) :=
(
(z1, z2),
if z ∈ˆZ(1)
(z2, z1),
if z ∈ˆZ(2) .
(31)
Step 1: v is a diffeomorphism. Note that v is its own inverse. Indeed,
v(v(z)) =
(
v(z1, z2) = (z1, z2),
if z ∈ˆZ(1)
v(z2, z1) = (z1, z2),
if z ∈ˆZ(2) .
Thus, v is bijective on its image. Clearly, v is C2, thus v−1 = v is also C2. Hence, v is a
C2-diffeomorphism.
Step 2: v is locally disentangled. The Jacobian of v is given by
Dv(z) :=







1
0
0
1

,
if z ∈ˆZ(1)
0
1
1
0

,
if z ∈ˆZ(2)
,
(32)
20

Figure 6: Illustration of ˆZ = ˆZ(b) ∪ˆZ(o) in Example 7 where ˆZ(b) is the blue region and ˆZ(o) is the
orange region. The two black dots correspond to (−1/2, −1/2) and (1/2, −1/2), where the function
v2(z1, z2) is evaluated to show that it is not constant in z1.
which is everywhere a permutation matrix, hence v is locally disentangled.
Step 3: v is not globally disentangled. That is because v1(z1, z2) depends on both z1 and z2.
Indeed, if z2 = 0, we have that v1(−1, 0) = −1 ̸= 0 = v1(0, 0). Also, if z1 = 1, we have that
v1(1, 1) = 1 ̸= 2 = v1(1, 2).
Example 6 (Disconnected support with fixed permutation). Let v : ˆZ →R2 s.t.
ˆZ = ˆZ(1) ∪
ˆZ(2) ⊆R2 where ˆZ(1) = {z ∈R2 | z2 ≤0} and ˆZ(2) = {z ∈R2 | z2 ≥1}. Assume
v(z) := z + 1(z ∈ˆZ(2)).
Step 1: v is a diffeomorphism. The image of v is the union of the following two sets: Z(1) :=
v( ˆZ(1)) = ˆZ(1) and Z(2) := v( ˆZ(2)) = {z ∈R2 | z2 ≥2}. Consider the map w : Z(1) ∪Z(2) →
ˆZ defined as w(z) := z −1(z ∈Z(2)). We now show that w is the inverse of v:
w(v(z)) = v(z) −1(v(z) ∈Z(2))
(33)
= z + 1(z ∈ˆZ(2)) −1(z + 1(z ∈ˆZ(2)) ∈Z(2)) .
(34)
If z ∈ˆZ(2), we have
w(v(z)) = z + 1 −1(z + 1 ∈Z(2))
(35)
= z + 1 −1(z ∈ˆZ(2)) = z .
(36)
If z ∈ˆZ(1), we have
w(v(z)) = z −1(z ∈Z(2)) = z .
(37)
A similar argument can be made to show that v(w(z)) = z. Thus w is the inverse of v. Both v and
its inverse w are C2, thus v is a C2-diffeomorphism on its image.
Step 2: v is locally disentangled. This is clear since Dv(z) = I everywhere.
Step 3: v is not globally disentangled. Indeed, the function v1(z1, z2) = z1 + 1(z ∈ˆZ(2)) is not
constant in z2.
Example 7 (Connected support). Let v : ˆZ →R2 s.t. ˆZ = ˆZ(b) ∪ˆZ(o) where ˆZ(b) and ˆZ(o) are
respectively the blue and orange regions of Figure 6. Both regions contain their boundaries. The
function v is defined as follows:
v1(z) := z1
(38)
v2(z) :=
(
(z2+1)2+1
2
,
if z ∈ˆZ(b)
ez2,
if z ∈ˆZ(o) .
(39)
Step 1: v is a diffeomorphism. Clearly, v1 is C2. To show that v2 also is, we must verify that v2(z)
is C2 at the frontier between ˆZ(b) and ˆZ(o), i.e. when z ∈[1/4, 1] × {0}.
v2(z) is continuous since
(z2 + 1)2 + 1
2

z2=0
= 1 = ez2|z2=0 .
(40)
21

v2(z) is C1 since
 
(z2 + 1)2 + 1
2
′
z2=0
= (z2 + 1)|z2=0 = 1 = ez2|z2=0 = (ez2)′|z2=0 .
(41)
v2(z) is C2 since
 
(z2 + 1)2 + 1
2
′′
z2=0
= 1|z2=0 = 1 = ez2|z2=0 = (ez2)′′|z2=0 .
(42)
We will now find an explicit expression for the inverse of v. Define
w1(z) := z1
(43)
w2(z) :=
(√2z2 −1 −1,
if z ∈v( ˆZ(b))
log(z2),
if z ∈v( ˆZ(o)) .
(44)
It is straightforward to see that w(v(z)) = z for all z ∈ˆZ. One can also show that w is C2 at the
boundary between both regions v( ˆZ(b)) and v( ˆZ(o)), i.e. when z ∈[1/4, 1] × {1}.
Since both v and its inverse w are C2, v is a C2-diffeomorphism.
Step 2: v is locally disentangled. The Jacobian of v is
Dv(z) :=







1
0
0
z2 + 1

, if z ∈ˆZ(b)
1
0
0
ez2

, if z ∈ˆZ(o)
,
(45)
which is a permutation-scaling matrix everywhere on ˆZ. Thus local disentanglement holds.
Step 3: v is not globally disentangled. However, v2(z1, z2) is not constant in z1. Indeed,
v2(−1
2, −1
2) = (z2 + 1)2 + 1
2

z2=−1/2
= 5
8 ̸= e−1/2 = v2(1
2, −1
2) .
(46)
Thus global disentanglement does not hold.
A.5
Proof of Theorem 1
Proposition 9. Suppose that the data-generating process satisfies Assumption 1, that the learned
decoder ˆf : Rdz →Rdx is a C2-diffeomorphism onto its image and that the encoder ˆg : Rdx →Rdz
is continuous. Then, if ˆf and ˆg solve the reconstruction problem on the training distribution, i.e.
Etrain||x −ˆf(ˆg(x))||2 = 0, we have that f(Ztrain) = ˆf( ˆZtrain) and the map v := f −1 ◦ˆf is a
C2-diffeomorphism from ˆZtrain to Ztrain.
Proof. First note that
Etrain||x −ˆf(ˆg(x))||2 = Etrain||f(z) −ˆf(ˆg(f(z)))||2 = 0 ,
(47)
which implies that, for Ptrain
z
-almost every z ∈Ztrain,
f(z) = ˆf(ˆg(f(z))) .
But since the functions on both sides of the equations are continuous, the equality holds for all
z ∈Ztrain. This implies that f(Ztrain) = ˆf ◦ˆg ◦f(Ztrain) = ˆf( ˆZtrain).
By Remark 3, the restrictions f : Ztrain →f(Ztrain) and ˆf :
ˆZtrain →
ˆf( ˆZtrain) are C2-
diffeomorphisms and, because f(Ztrain) = ˆf( ˆZtrain), their composition v := f −1 ◦ˆf : ˆZtrain →
Ztrain is a well defined C2-diffeomorphism (since C2-diffeomorphisms are closed under composi-
tion).
22

Theorem 1 (Local disentanglement via additive decoders). Suppose that the data-generating process
satisfies Assumption 1, that the learned decoder ˆf : Rdz →Rdx is a C2-diffeomorphism, that the
encoder ˆg : Rdx →Rdz is continuous, that both f and ˆf are additive (Definition 1) and that f
is sufficiently nonlinear as formalized by Assumption 2. Then, if ˆf and ˆg solve the reconstruction
problem on the training distribution, i.e. Etrain||x −ˆf(ˆg(x))||2 = 0, we have that ˆf is locally
B-disentangled w.r.t. f (Definition 4) .
Proof. We can apply Proposition 9 and have that the map v := f −1 ◦ˆf is a C2-diffeomorphism
from ˆZtrain to Ztrain. This allows one to write
f ◦v(z) = ˆf(z) ∀z ∈ˆZtrain
(48)
X
B∈B
f (B)(vB(z)) =
X
B∈B
ˆf (B)(zB) ∀z ∈ˆZtrain .
(49)
Since Ztrain is regularly closed and is diffeomorphic to ˆZtrain, by Lemma 4, we must have that
ˆZtrain ⊆( ˆZtrain)◦. Moreover, the left and right hand side of (49) are C2, which means they have
uniquely defined first and second derivatives on ( ˆZtrain)◦by Lemma 6. This means the derivatives
are uniquely defined on ˆZtrain.
Let z ∈ˆZtrain. Choose some J ∈B and some j ∈J. Differentiate both sides of the above equation
with respect to zj, which yields:
X
B∈B
X
i∈B
Dif (B)(vB(z))Djvi(z) = Dj ˆf (J)(zJ) .
(50)
Choose J′ ∈B \ {J} and j′ ∈J′. Differentiating the above w.r.t. zj′ yields
X
B∈B
X
i∈B
"
Dif (B)(vB(z))D2
j,j′vi(z) +
X
i′∈B
D2
i,i′f (B)(vB(z))Dj′vi′(z)Djvi(z)
#
= 0
X
B∈B
 X
i∈B
h
Dif (B)(vB(z))D2
j,j′vi(z) + D2
i,if (B)(vB(z))Dj′vi(z)Djvi(z)
i
+
X
(i,i′)∈B2
<
D2
i,i′f (B)(vB(z))(Dj′vi′(z)Djvi(z) + Dj′vi(z)Djvi′(z))

= 0 ,
(51)
where B2
< := B2 ∩{(i, i′) | i′ < i}. For the sake of notational conciseness, we are going to refer to
SB and Sc
B as S and Sc (Definition 11). Also, define
S< :=
[
B∈B
B2
< .
(52)
Let us define the vectors
∀i ∈{1, ...dz}, ⃗ai(z) := (D2
j,j′vi(z))(j,j′)∈Sc
(53)
∀i ∈{1, ...dz}, ⃗bi(z) := (Dj′vi(z)Djvi(z))(j,j′)∈Sc
(54)
∀B ∈B, ∀(i, i′) ∈B2
<, ⃗ci,i′(z) := (Dj′vi′(z)Djvi(z) + Dj′vi(z)Djvi′(z))(j,j′)∈Sc
(55)
This allows us to rewrite, for all k ∈{1, ..., dx}
X
B∈B

X
i∈B
h
Dif (B)
k
(vB(z))⃗ai(z) + D2
i,if (B)
k
(vB(z))⃗bi(z)
i
+
X
(i,i′)∈B2
<
D2
i,i′f (B)
k
(vB(z))⃗ci,i′(z)

= 0 .
(56)
We define
w(z, k) := ((Dif (B)
k
(zB))i∈B, (D2
i,if (B)
k
(zB))i∈B, (D2
i,i′f (B)
k
(zB))(i,i′)∈B2
<)B∈B
(57)
M(z) := [[⃗ai(z)]i∈B, [⃗bi(z)]i∈B, [⃗ci,i′(z)](i,i′)∈B2
<]B∈B ,
(58)
23

which allows us to write, for all k ∈{1, ..., dz}
M(z)w(v(z), k) = 0 .
(59)
We can now recognize that the matrix W (v(z)) of Assumption 2 is given by
W (v(z))⊤= [w(v(z), 1) . . . w(v(z), dx)]
(60)
which allows us to write
M(z)W (v(z))⊤= 0
(61)
W (v(z))M(z)⊤= 0
(62)
Since W (v(z)) has full column-rank (by Assumption 2 and the fact that v(z) ∈Ztrain), there exists
q rows that are linearly independent. Let K be the index set of these rows. This means W (v(z))K,·
is an invertible matrix. We can thus write
W (v(z))K,·M(z)⊤= 0
(63)
(W (v(z))K,·)−1W (v(z))K,·M(z)⊤= (W (v(z))K,·)−10
(64)
M(z)⊤= 0 ,
(65)
which means, in particular, that, ∀i ∈{1, . . . , dz},⃗bi(z) = 0, i.e.,
∀i ∈{1, . . . , dz}, ∀(j, j′) ∈Sc, Djvi(z)Dj′vi(z) = 0
(66)
Since the v is a diffeomorphism, its Jacobian matrix Dv(z) is invertible everywhere. By Lemma 5,
this means there exists a permutation π such that, for all j, Djvπ(j)(z) ̸= 0. This and (66) imply that
∀(j, j′) ∈Sc, Djvπ(j′)(z) Dj′vπ(j′)(z)
|
{z
}
̸=0
= 0,
(67)
=⇒∀(j, j′) ∈Sc, Djvπ(j′)(z) = 0 .
(68)
To show that Dv(z) is a B-block permutation matrix, the only thing left to show is that π respects
B. For this, we use the fact that, ∀B ∈B, ∀(i, i′) ∈B2
<, ⃗ci,i′(z) = 0 (recall M(z) = 0). Because
⃗ci,i′(z) = ⃗ci′,i(z), we can write
∀(i, i′) ∈S s.t. i ̸= i′, ∀(j, j′) ∈Sc, Dj′vi′(z)Djvi(z) + Dj′vi(z)Djvi′(z) = 0 .
(69)
We now show that if (j, j′) ∈Sc (indices belong to different blocks), then (π(j), π(j′)) ∈Sc
(they also belong to different blocks). Assume this is false, i.e. there exists (j0, j′
0) ∈Sc such that
(π(j0), π(j′
0)) ∈S. Then we can apply (69) (with i := π(j0) and i′ := π(j′
0)) and get
Dj′
0vπ(j′
0)(z)Dj0vπ(j0)(z)
|
{z
}
̸=0
+Dj′
0vπ(j0)(z)Dj0vπ(j′
0)(z) = 0 ,
(70)
where the left term in the sum is different of 0 because of the definition of π. This implies that
Dj′
0vπ(j0)(z)Dj0vπ(j′
0)(z) ̸= 0 ,
(71)
otherwise (70) cannot hold. But (71) contradicts (68). Thus, we have that,
(j, j′) ∈Sc =⇒(π(j), π(j′)) ∈Sc .
(72)
The contraposed is
(π(j), π(j′)) ∈S =⇒(j, j′) ∈S
(73)
(j, j′) ∈S =⇒(π−1(j), π−1(j′)) ∈S .
(74)
From the above, it is clear that π−1 respects B which implies that π respects B (Lemma 10). Thus
Dv(z) is a B-block permutation matrix.
24

Lemma 10 (B-respecting permutations form a group). Let B be a partition of {1, . . . , dz} and let π
and ¯π be a permutation of {1, . . . , dz} that respect B. The following holds:
1. The identity permutation e respects B.
2. The composition π ◦¯π respects B.
3. The inverse permutation π−1 respects B.
Proof. The first statement is trivial, since for all B ∈B, e(B) = B ∈B.
The second statement follows since for all B ∈B, ¯π(B) ∈B and thus π(¯π(B)) ∈B.
We now prove the third statement. Let B ∈B. Since π is surjective and respects B, there exists a
B′ ∈B such that π(B′) = B. Thus, π−1(B) = π−1(π(B′)) = B′ ∈B.
A.6
Sufficient nonlinearity v.s. sufficient variability in nonlinear ICA with auxiliary variables
In Section 3.1, we introduced the “sufficient nonlinearity” condition (Assumption 2) and highlighted
its resemblance to the “sufficient variability” assumptions often found in the nonlinear ICA liter-
ature [30, 31, 33, 36, 37, 42, 73]. We now clarify this connection. To make the discussion more
concrete, we consider the sufficient variability assumption found in Hyv¨arinen et al. [33]. In this
work, the latent variable z is assumed to be distributed according to
p(z | u) :=
dz
Y
i=1
pi(zi | u) .
(75)
In other words, the latent factors zi are mutually conditionally independent given an observed
auxiliary variable u. Define
w(z, u) :=
  ∂
∂zi
log pi(zi | u)

i∈[dz]
 ∂2
∂z2
i
log pi(zi | u)

i∈[dz]
!
∈R2dz .
(76)
We now recall the assumption of sufficient variability of Hyv¨arinen et al. [33]:
Assumption 3 (Assumption of variability from Hyv¨arinen et al. [33, Theorem 1]). For any z ∈Rdz,
there exists 2dz + 1 values of u, denoted by u(0), u(1), . . . , u(2dz) such that the 2dz vectors
w(z, u(1)) −w(z, u(0)), . . . , w(z, u(2dz)) −w(z, u(0))
(77)
are linearly independent.
To emphasize the resemblance with our assumption of sufficient nonlinearity, we rewrite it in
the special case where the partition B := {{1}, . . . , {dz}}. Note that, in that case, q := dz +
P
B∈B
|B|(|B|+1)
2
= 2dz.
Assumption 4 (Sufficient nonlinearity (trivial partition)). For all z ∈Ztrain, f is such that the
following matrix has independent columns (i.e. full column-rank):
W (z) :=
h
Dif (i)(zi)
i
i∈[dz]
h
D2
i,if (i)(zi)
i
i∈[dz]

∈Rdx×2dz .
(78)
One can already see the resemblance between Assumptions 3 & 4, e.g. both have something to do
with first and second derivatives. To make the connection even more explicit, define w(z, k) to be the
kth row of W (z) (do not conflate with w(z, u)). Also, recall the basic fact from linear algebra that
the column-rank is always equal to the row-rank. This means that W (z) is full column-rank if and
only if there exists k1, ..., k2dz ∈[dx] such that the vectors w(z, k1), . . . , w(z, k2dz) are linearly
independent. It is then easy to see the correspondance between w(z, k) and w(z, u) −w(z, u(0))
(from Assumption 3) and between the pixel index k ∈[dx] and the auxiliary variable u.
We now look at why Assumption 2 is likely to be satisfied when dx >> dz. Informally, one can
see that when dx is much larger than 2dz, the matrix W (z) has much more rows than columns and
thus it becomes more likely that we will find 2dz rows that are linearly independent, thus satisfying
Assumption 2.
25

Figure 7: Numerical verification that f : [−1, 0] × [0, 1] →R4 from Example 8 is injective (left),
has a full rank Jacobian (middle) and satisfies Assumption 2 (right). The left figure shows that f
is injective on the square [−1, 0] × [0, 1] since one can recover z uniquely by knowing the values
of f1(z) and f2(z), i.e. knowing the level sets. The middle figure reports the det(Df(z)⊤Df(z))
(columns of the Jacobian are normalized to have norm 1) and shows that it is nonzero in the square
[−1, 0] × [0, 1], which means the Jacobian is full rank. The right figure shows the determinant of
the matrix W (z) (from Assumption 2, but with normalized columns), we can see that it is nonzero
everywhere on the square [−1, 0] × [0, 1]. We normalized the columns of Df and W so that the
determinant is between 0 and 1.
A.7
Examples of sufficiently nonlinear additive decoders
Example 8 (A sufficiently nonlinear f - Example 3 continued). Consider the additive function
f(z) :=


z1
z2
1
z3
1
z4
1

+


(z2 + 1)
(z2 + 1)2
(z2 + 1)3
(z2 + 1)4

.
(79)
We will provide a numerical verification that this function is a diffeomorphism from the square
[−1, 0] × [0, 1] to its image that satisfies Assumption 2.
The Jacobian of f is given by
Df(z) =


1
1
2z1
2(z2 + 1)
3z2
1
3(z2 + 1)2
4z3
1
4(z2 + 1)3

,
(80)
and the matrix W (z) from Assumption 2 is given by
W (z) =


1
0
1
0
2z1
2
2(z2 + 1)
2
3z2
1
6z1
3(z2 + 1)2
6(z2 + 1)
4z3
1
12z2
1
4(z2 + 1)3
12(z2 + 1)2

.
(81)
Figure 7 presents a numerical verification that f is injective, has a full rank Jacobian and satisfies
Assumption 2. Injective f with full rank Jacobian is enough to conclude that f is a diffeomorphism
onto its image.
Example 9 (Smooth balls dataset is sufficiently nonlinear - Example 4 continued). We implemented
a ground-truth additive decoder f : [0, 5]2 →R64∗64∗3 which maps to 64x64 RGB images consisting
of two colored balls where z1 and z2 control their respective heights (Figure 8a). The analytical
form of f can be found in our code base. The decoder f is implemented in JAX [6] which allows for
its automatic differentiation to compute Df and D2f (Figures 8b & 8c). This allows us to verify
numerically that f is sufficiently nonlinear (Assumption 2). Recall that this assumption requires that
W (z) (defined in Assumption 2) has independent columns everywhere. To test this, we compute
Vol(z) :=
p
| det(W (z)⊤W (z))| over a grid of values of z and verify that Vol(z) > 0 everywhere
(Figure 8d). Note that Vol(z) corresponds to the 4D volume of the parallelepiped embedded in
R64∗64∗3 spanned by the four columns of W (z). This volume is > 0 if and only if the columns
are linearly independent. Note that we normalize the columns of W (z) so that they have a norm
26

of one. It follows that Vol(z) is between 0 and 1 where 1 means the vectors are orthogonal, i.e.
maximally independent. The minimal value of Vol(z) over the domain of f is ≈0.97, indicating that
Assumption 2 holds.
(a)
(b)
(c)
(d)
(e)
Figure 8: Figure (a) shows an image the synthetic dataset of Example 9. Figure (b) shows the
derivative of the image w.r.t. z1 (the height of the left ball) where the color intensity of each pixel
corresponds to the Euclidean norm along the RGB axis. Figure (c) similarly shows the second
derivative of the image w.r.t. z1. Figure (d) is a contour plot of the function
p
| det(W (z)⊤W (z))|
where W (z) is defined in Assumption 2 (here columns are normalized to have unit norm). The
smallest value of
p
| det(W (z)⊤W (z))| across domain is ≈0.97, indicating that Assumption 2 is
satisfied. See Example 9 and code for details. Figure 8e is a higher resolution rendering of the red
region of Figure 8d (to make sure there is no singularity there).
A.8
Proof of Theorem 2
We start with a simple definition:
Definition 15 (B-block permutation matrices). A matrix A ∈Rd×d is a B-block permutation matrix if
it is invertible and can be written as A = CPπ where Pπ is the matrix representing the B-respecting
permutation π (Pπei = eπ(i)) and C ∈Rd×d
SB
(See Definitions 10 & 11).
The following technical lemma leverages continuity and path-connectedness to show that the block-
permutation structure must remain the same across the whole domain. It can be skipped at first
read.
Lemma 11. Let C be a connected topological space and let M : C →Rd×d be a continuous function.
Suppose that, for all c ∈C, M(c) is an invertible B-block permutation matrix (Definition 15). Then,
there exists a B-respecting permutation π such that for all c ∈C and all distinct B, B′ ∈B,
M(c)π(B′),B = 0.
Proof. The reason this result is not trivial, is that, even if M(c) is a B-block permutation for all c,
the permutation might change for different c. The goal of this lemma is to show that, if C is connected
and the map M(·) is continuous, then one can find a single permutation that works for all c ∈C.
First, since C is connected and M is continuous, its image, M(C), must be connected (by [56,
Theorem 23.5]).
27

Second, from the hypothesis of the lemma, we know that
M(C) ⊆A :=


[
π∈S(B)
Rd×d
SB Pπ

\ {singular matrices} ,
(82)
where S(B) is the set of B-respecting permutations and Rd×d
SB Pπ = {MPπ | M ∈Rd×d
SB }. We can
rewrite the set A above as
A =
[
π∈S(B)
 Rd×d
SB Pπ \ {singular matrices}

,
(83)
We now define an equivalence relation ∼over B-respecting permutation: π ∼π′ iff for all B ∈B,
π(B) = π′(B). In other words, two B-respecting permutations are equivalent if they send every
block to the same block (note that they can permute elements of a given block differently). We notice
that
π ∼π′ =⇒Rd×d
SB Pπ = Rd×d
SB Pπ′ .
(84)
Let S(B)/ ∼be the set of equivalence classes induce by ∼and let Π stand for one such equivalence
class. Thanks to (84), we can define, for all Π ∈S(B)/ ∼, the following set:
VΠ := Rd×d
SB Pπ \ {singular matrices}, for some π ∈Π ,
(85)
where the specific choice of π ∈Π is arbitrary (any π′ ∈Π would yield the same definition, by (84)).
This construction allows us to write
A =
[
Π∈S(B)/∼
VΠ ,
(86)
We now show that {VΠ}Π∈S(B)/∼forms a partition of A. Choose two distinct equivalence classes of
permutations Π and Π′ and let π ∈Π and π′ ∈Π′ be representatives. We note that
Rd×d
SB Pπ ∩Rd×d
SB Pπ′ ⊆{singular matrices} ,
(87)
since any matrix that is both in Rd×d
SB Pπ and Rd×d
SB Pπ′ must have at least one row filled with zeros.
This implies that
VΠ ∩VΠ′ = ∅,
(88)
which shows that {VΠ}Π∈S(B)/∼is indeed a partition of A.
Each VΠ is closed in A (wrt the relative topology) since
VΠ = Rd×d
SB Pπ \ {singular matrices} = A ∩Rd×d
SB Pπ
|
{z
}
closed in Rd×d
.
(89)
Moreover, VΠ is open in A, since
VΠ = A \
[
Π′̸=Π
VΠ′
|
{z
}
closed in A
.
(90)
Thus, for any Π ∈S(B)/ ∼, the sets VΠ and S
Π′̸=Π VΠ′ forms a separation (see [56, Section 23]).
Since M(C) is a connected subset of A, it must lie completely in VΠ or S
Π′̸=Π VΠ′, by [56, Lemma
23.2]. Since this is true for all Π, it must follow that there exists a Π∗such that M(C) ⊆VΠ∗, which
completes the proof.
Theorem 2 (From local to global disentanglement). Suppose that all the assumptions of Theorem 1
hold. Additionally, assume Ztrain is path-connected (Definition 8) and that the block-specific decoders
f (B) and ˆf (B) are injective for all blocks B ∈B. Then, if ˆf and ˆg solve the reconstruction
problem on the training distribution, i.e. Etrain||x −ˆf(ˆg(x))||2 = 0, we have that ˆf is (globally)
B-disentangled w.r.t. f (Definition 3) and, for all B ∈B,
ˆf (B)(zB) = f (π(B))(¯vπ(B)(zB)) + c(B), for all zB ∈ˆZtrain
B
,
(8)
28

where the functions ¯vπ(B) are from Defintion 3 and the vectors c(B) ∈Rdx are constants such that
P
B∈B c(B) = 0. We also have that the functions ¯vπ(B) : ˆZtrain
B
→Ztrain
π(B) are C2-diffeomorphisms
and have the following form:
¯vπ(B)(zB) = (f π(B))−1( ˆf (B)(zB) −c(B)), for all zB ∈ˆZtrain
B
.
(9)
Proof. Step 1 - Showing the permutation π does not change for different z. Theorem 1 showed
local B-disentanglement, i.e. for all z ∈ˆZtrain, Dv(z) has a B-block permutation structure. The
first step towards showing global disentanglement is to show that this block structure is the same for
all z ∈ˆZtrain (a priori, π could be different for different z). Since v is C2, its Jacobian Dv(z) is
continuous. Since Ztrain is path-connected, ˆZtrain must also be since both sets are diffeomorphic. By
Lemma 11, this means the B-block permutation structure of Dv(z) is the same for all z ∈ˆZtrain
(implicitly using the fact that path-connected implies connected). In other words, there exists a
permutation π respecting B such that, for all z ∈ˆZtrain and all distinct B, B′ ∈B, DBvπ(B′)(z) = 0.
Step 2 - Linking object-specific decoders. We now show that, for all B ∈B, ˆf (B)(zB) =
f (π(B))(vπ(B)(z)) + c(B) for all z ∈ˆZtrain. To do this, we rewrite (50) as
D ˆf (J)(zJ) =
X
B∈B
Df (B)(vB(z))DJvB(z) ,
(91)
but because B ̸= π(J) =⇒DJvB(z) = 0 (block-permutation structure), we get
D ˆf (J)(zJ) = Df (π(J))(vπ(J)(z))DJvπ(J)(z) .
(92)
The above holds for all J ∈B. We simply change J by B in the following equation.
D ˆf (B)(zB) = Df (π(B))(vπ(B)(z))DBvπ(B)(z) .
(93)
Now notice that the r.h.s. of the above equation is equal to D(f (π(B)) ◦vπ(B)). We can thus write
D ˆf (B)(zB) = D(f (π(B)) ◦vπ(B))(z) , for all z ∈ˆZtrain .
(94)
Now choose distinct z, z0 ∈ˆZtrain. Since Ztrain is path-connected, ˆZtrain also is since they are
diffeomorphic. Hence, there exists a continuously differentiable function ϕ : [0, 1] →ˆZtrain such
that ϕ(0) = z0 and ϕ(1) = z. We can now use (94) together with the gradient theorem, a.k.a. the
fundamental theorem of calculus for line integrals, to show the following
Z 1
0
D ˆf (B)(ϕB(z)) · ϕB(t)dt =
Z 1
0
D(f (π(B)) ◦vπ(B))(ϕ(z)) · ϕ(t)dt
(95)
ˆf (B)(zB) −ˆf (B)(z0
B) = f (π(B)) ◦vπ(B)(z) −f (π(B)) ◦vπ(B)(z0)
(96)
ˆf (B)(zB) = f (π(B)) ◦vπ(B)(z) + ( ˆf (B)(z0
B) −f (π(B)) ◦vπ(B)(z0))
|
{z
}
constant in z
(97)
ˆf (B)(zB) = f (π(B)) ◦vπ(B)(z) + c(B) ,
(98)
which holds for all z ∈ˆZtrain.
We now show that P
B∈B c(B) = 0. Take some z0 ∈ˆZtrain. Equations (49) & (98) tell us that
X
B∈B
f (B)(vB(z0)) =
X
B∈B
ˆf (B)(z0
B)
(99)
=
X
B∈B
f (π(B))(vπ(B)(z0)) +
X
B∈B
c(B)
(100)
=
X
B∈B
f (B)(vB(z0)) +
X
B∈B
c(B)
(101)
=⇒0 =
X
B∈B
c(B)
(102)
29

Step 3 - From local to global disentanglement. By assumption, the functions f (B) : Ztrain
B
→Rdx
are injective. This will allow us to show that vπ(B)(z) depends only on zB. We proceed by
contradiction. Suppose there exists (zB, zBc) ∈ˆZtrain and z0
Bc such that (zB, z0
Bc) ∈ˆZtrain and
vπ(B)(zB, zBc) ̸= vπ(B)(zB, z0
Bc). This means
f (π(B)) ◦vπ(B)(zB, zBc) + c(B) = ˆf (B)(zB) = f (π(B)) ◦vπ(B)(zB, z0
Bc) + c(B)
f (π(B))(vπ(B)(zB, zB)) = f (π(B))(vπ(B)(zB, z0
B))
which is a contradiction with the fact that f (π(B)) is injective. Hence, vπ(B)(z) depends only on zB.
We also get an explicit form for vπ(B):
(f π(B))−1( ˆf (B)(zB) −c(B)) = vπ(B)(z) for all z ∈Ztrain .
(103)
We define the map ¯vπ(B)(zB) := (f π(B))−1( ˆf (B)(zB) −c(B)) which is from ˆZtrain
B
to Ztrain
π(B). This
allows us to rewrite (98) as
ˆf (B)(zB) = f (π(B)) ◦¯vπ(B)(zB) + c(B) , for all zB ∈Ztrain
B
.
(104)
Because ˆf (B) is also injective, we must have that ¯vπ(B) : ˆZtrain
B
→Ztrain
π(B) is injective as well.
We now show that ¯vπ(B) is surjective. Choose some zπ(B) ∈Ztrain
π(B). We can always find zπ(B)c
such that (zπ(B), zπ(B)c) ∈Ztrain. Because v : ˆZtrain →Ztrain is surjective (it is a diffeomorphism),
there exists a z0 ∈ˆZtrain such that v(z0) = (zπ(B), zπ(B)c). By (103), we have that
¯vπ(B)(z0
B) = vπ(B)(z0) .
(105)
which means ¯vπ(B)(z0
B) = zπ(B).
We thus have that ¯vπ(B) is bijective. It is a diffeomorphism because
det D¯vπ(B)(zB) = det DBvπ(B)(z) ̸= 0 ∀z ∈ˆZtrain
(106)
where the first equality holds by (103) and the second holds because v is a diffeomorphism and has
block-permutation structure, which means it has a nonzero determinant everywhere on ˆZtrain and is
equal to the product of the determinants of its blocks, which implies each block DBvπ(B) must have
nonzero determinant everywhere.
Since ¯vπ(B) :
ˆZtrain
B
→Ztrain
π(B) bijective and has invertible Jacobian everywhere, it must be a
diffeomorphism.
A.9
Injectivity of object-specific decoders v.s. injectivity of their sum
We want to explore the relationship between the injectivity of individual object-specific decoders
f (B) and the injectivity of their sum, i.e. P
B∈B f (B).
We first show the simple fact that having each f (B) injective is not sufficient to have P
B∈B f (B)
injective. Take f (B)(zB) = W (B)zB where W (B) ∈Rdx×|B| has full column-rank for all B ∈B.
We have that
X
B∈B
f (B)(zB) =
X
B∈B
W (B)zB = [W (B1) · · · W (Bℓ)]z ,
(107)
where it is clear that the matrix [W (B1) · · · W (Bℓ)] ∈Rdx×dz is not necessarily injective even if
each W (B) is. This is the case, for instance, if all W (B) have the same image.
We now provide conditions such that P
B∈B f (B) injective implies each f (B) injective. We start
with a simple lemma:
Lemma 12. If g ◦h is injective, then h is injective.
Proof. By contradiction, assume that h is not injective. Then, there exists distinct x1, x2 ∈Dom(h)
such that h(x1) = h(x2). This implies g ◦h(x1) = g ◦h(x2), which violates injectivity of g ◦h.
30

The following Lemma provides a condition on the domain of the function P
B∈B f (B), Ztrain, so that
its injectivity implies injectivity of the functions f (B).
Lemma 13. Assume that, for all B ∈B and for all distinct zB, z′
B ∈Ztrain
B
, there exists zBc such
that (zB, zBc), (z′
B, zBc) ∈Ztrain. Then, whenever P
B∈B f (B) is injective, each f (B) must be
injective.
Proof. Notice that f(z) := P
B∈B f (B)(zB) can be written as f := SumBlocks ◦¯f(z) where
¯f(z) :=


f (B1)(zB1)
...
f (Bℓ)(zBℓ)

, and SumBlocks(x(B1), . . . , x(Bℓ)) :=
X
B∈B
x(B)
(108)
Since f is injective, by Lemma 12 ¯f must be injective.
We now show that each f (B) must also be injective. Take zB, z′
B ∈Ztrain
B
such that f (B)(zB) =
f (B)(z′
B). By assumption, we know there exists a zBc s.t. (zB, zBc) and (z′
B, zBc) are in Ztrain.
By construction, we have that ¯f((zB, zBc)) = ¯f((z′
B, zBc)). By injectivity of ¯f, we have that
(zB, zBc) ̸= (z′
B, zBc), which implies zB ̸= z′
B, i.e. f (B) is injective.
A.10
Proof of Corollary 3
Corollary 3 (Cartesian-product extrapolation). Suppose the assumptions of Theorem 2 holds. Then,
for all z ∈CPEB( ˆZtrain),
X
B∈B
ˆf (B)(zB) =
X
B∈B
f (π(B))(¯vπ(B)(zB)) .
(11)
Furthermore, if CPEB(Ztrain) ⊆Ztest, then ˆf(CPEB( ˆZtrain)) ⊆f(Ztest).
Proof. Pick z ∈CPE( ˆZtrain). By definition, this means that, for all B ∈B, zB ∈ˆZtrain
B
. We thus
have that, for all B ∈B,
ˆf (B)(zB) = f (π(B)) ◦¯vπ(B)(zB) + c(B) .
(109)
We can thus sum over B to obtain
X
B∈B
ˆf (B)(zB) =
X
B∈B
f (π(B)) ◦¯vπ(B)(zB) +
X
B∈B
c(B)
|
{z
}
=0
.
(110)
Since z ∈CPE( ˆZtrain) was arbitrary, we have
for all z ∈CPE( ˆZtrain),
X
B∈B
ˆf (B)(zB) =
X
B∈B
f (π(B)) ◦¯vπ(B)(zB)
(111)
ˆf(z) = f ◦¯v(z) ,
(112)
where ¯v : CPEB( ˆZtrain) →CPEB(Ztrain) is defined as
¯v(z) :=


¯vB1(zπ−1(B1))
...
¯vBℓ(zπ−1(Bℓ))

,
(113)
The map ¯v is a diffeomorphism since each ¯vπ(B) is a diffeomorphism from ˆZtrain
B
to Ztrain
π(B).
By (112) we get
ˆf(CPEB( ˆZtrain)) = f ◦¯v(CPEB( ˆZtrain)) ,
(114)
and since the map ¯v is surjective we have ¯v(CPEB( ˆZtrain)) = CPEB(Ztrain) and thus
ˆf(CPEB( ˆZtrain)) = f(CPEB(Ztrain)) .
(115)
Hence if CPEB(Ztrain) ⊆Ztest, then f(CPEB(Ztrain)) ⊆f(Ztest).
31

A.11
Will all extrapolated images make sense?
Here is a minimal example where the assumption CPEB(Ztrain) ̸⊆Ztest is violated.
Example 10 (Violation of CPEB(Ztrain) ̸⊆Ztest). Imagine z = (z1, z2) where z1 and z2 are the x-
positions of two distinct balls. It does not make sense to have two balls occupying the same location in
space and thus whenever z1 = z2 we have (z1, z2) ̸∈Ztest. But if (1, 2) and (2, 1) are both in Ztrain,
it implies that (1, 1) and (2, 2) are in CPE(Ztrain), which is a violation of CPEB(Ztrain) ⊆Ztest.
A.12
Additive decoders cannot model occlusion
We now explain why additive decoders cannot model occlusion. Occlusion occurs when an object
is partially hidden behind another one. Intuitively, the issue is the following: Consider two images
consisting of two objects, A and B (each image shows both objects). In both images, the position of
object A is the same and in exactly one of the images, object B partially occludes object A. Since the
position of object A did not change, its corresponding latent block zA is also unchanged between
both images. However, the pixels occupied by object A do change between both images because of
occlusion. The issue is that, because of additivity, zA and zB cannot interact to make some pixels
that belonged to object A “disappear” to be replaced by pixels of object B. In practice, object-centric
representation learning methods rely a masking mechanism which allows interactions between zA
and zB (See Equation 1 in Section 2). This highlights the importance of studying this class of
decoders in future work.
B
Experiments
B.1
Training Details
Loss Function.
We use the standard reconstruction objective of mean squared error loss between
the ground truth data and the reconstructed/generated data.
Hyperparameters.
For both the ScalarLatents and the BlockLatents dataset, we used the Adam
optimizer with the hyperparameters defined below. Note that we maintain consistent hyperparameters
across both the Additive decoder and the Non-Additive decoder method.
ScalarLatents Dataset.
• Batch Size: 64
• Learning Rate: 1 × 10−3
• Weight Decay: 5 × 10−4
• Total Epochs: 4000
BlockLatents Dataset.
• Batch Size: 1024
• Learning Rate: 1 × 10−3
• Weight Decay: 5 × 10−4
• Total Epochs: 6000
Model Architecture.
We use the following architectures for Encoder and Decoder across both the
datasets (ScalarLatents, BlockLatents). Note that for the ScalarLatents dataset we train with latent
dimension dz = 2, and for the BlockLatents dataset we train with latent dimension dz = 4, which
corresponds to the dimensionalities of the ground-truth data generating process for both datasets.
Encoder Architecture:
• RestNet-18 Architecture till the penultimate layer (512 dimensional feature output)
• Stack of 5 fully-connected layer blocks, with each block consisting of Linear Layer (
dimensions: 512 × 512), Batch Normalization layer, and Leaky ReLU activation (negative
slope: 0.01).
32

• Final Linear Layer (dimension: 512 × dz) followed by Batch Normalization Layer to output
the latent representation.
Decoder Architecture (Non-additive):
• Fully connected layer block with input as latent representation, consisting of Linear Layer
(dimension: dz × 512), Batch Normalization layer, and Leaky ReLU activation (negative
slope: 0.01).
• Stack of 5 fully-connected layer blocks, with each block consisting of Linear Layer (
dimensions: 512 × 512), Batch Normalization layer, and Leaky ReLU activation (negative
slope: 0.01).
• Series of DeConvolutional layers, where each DeConvolutional layer is follwed by Leaky
ReLU (negative slope: 0.01) activation.
– DeConvolution Layer (cin: 64, cout: 64, kernel: 4; stride: 2; padding: 1)
– DeConvolution Layer (cin: 64, cout: 32, kernel: 4; stride: 2; padding: 1)
– DeConvolution Layer (cin: 32, cout: 32, kernel: 4; stride: 2; padding: 1)
– DeConvolution Layer (cin: 32, cout: 3, kernel: 4; stride: 2; padding: 1)
Decoder Architecture (Additive):
Recall that an additive decoder has the form f(z)
=
P
B∈B f (B)(zB). Each f (B) has the same architecture as the one presented above for the non-
additive case, but the input has dimensionality |B| (which is 1 or 2, depending on the dataset). Note
that we do not share parameters among the functions f (B).
B.2
Datasets Details
We use the moving balls environment from Ahuja et al. [2] with images of dimension 64 × 64 × 3,
with latent vector (z) representing the position coordinates of each balls. We consider only two balls.
The rendered images have pixels in the range [0, 255].
ScalarLatents Dataset.
We fix the x-coordinate of each ball to 0.25 and 0.75. The only factors
varying are the y-coordinates of both balls. Thus, z ∈R2 and B = {{1}, {2}} where z1 and
z2 designate the y-coordinates of both balls. We sample the y-coordinate of the first ball from a
continuous uniform distribution as follows: z1 ∼Uniform(0, 1). Then we sample the y-coordinate of
the second ball as per the following scheme:
z2 ∼
Uniform(0, 1)
if z1 ≤0.5
Uniform(0, 0.5)
else
Hence, this leads to the L-shaped latent support, i.e., Ztrain := [0, 1] × [0, 1] \ [0.5, 1] × [0.5, 1].
We use 50k samples for the test dataset, while we use 20k samples for the train dataset along with 5k
samples (25% of the train sample size) for the validation dataset.
BlockLatents Dataset.
For this dataset, we allow the balls to move in both the x, y directions, so
that z ∈R4 and B = {{1, 2}, {3, 4}}. For the case of independent latents, we sample each latent
component independently and identically distributed according to a uniform distribution over (0, 1),
i.e. zi ∼Uniform(0, 1). We rejected the images that present occlusion, i.e. when one ball hides
another one.2
For the case of dependent latents, we sample the latents corresponding to the first ball similarly
from the same continuous uniform distribution, i.e, z1, z2 ∼Uniform (0, 1). However, the latents of
the second ball are a function of the latents of the first ball, as described in what follows:
z3 ∼
Uniform(0, 0.5)
if 1.25 × (z2
1 + z2
2) ≥1.0
Uniform(0.5, 1)
if 1.25 × (z2
1 + z2
2) < 1.0
2Note that, in the independent latents case, the latents are not actually independent because of the rejection
step which prevents occlusion from happening.
33

z4 ∼
Uniform(0.5, 1)
if 1.25 × (z2
1 + z2
2) ≥1.0
Uniform(0, 0.5)
if 1.25 × (z2
1 + z2
2) < 1.0
Intuitively, this means the second ball will be placed in either the top-left or the bottom-right quadrant
based on the position of the first ball. We also exclude from the dataset the images presenting
occlusion.
Note that our dependent BlockLatent setup is same as the non-linear SCM case from Ahuja et al. [3].
We use 50k samples for both the train and the test dataset, along with 12.5k samples (25% of the
train sample size) for the validation dataset.
Disconnected Support Dataset.
For this dataset, we have setup similar to the ScalarLatents
dataset; we fix the x-coordinates of both balls to 0.25 and 0.75 and only vary the y-coordinates so
that z ∈R2. We sample the y-coordinate of the first ball (z1) from Uniform(0, 1). Then we sample
the y-coordinate of the second ball (z2) from either of the following continuous uniform distribution
with equal probability; Uniform(0, 0.25) and Uniform(0.75, 1). This leads to a disconnected support
given by Ztrain := [0, 1] × [0, 1] \ [0.25, 0.75] × [0.25, 0.75].
We use 50k samples for the test dataset, while we use 20k samples for the train dataset along with 5k
samples (25% of the train sample size) for the validation dataset.
B.3
Evaluation Metrics
Recall that, to evaluate disentanglement, we compute a matrix of scores (sB,B′) ∈Rℓ×ℓwhere ℓis
the number of blocks in B and sB,B′ is a score measuring how well we can predict the ground-truth
block zB from the learned latent block ˆzB′ = ˆgB′(x) outputted by the encoder. The final Latent
Matching Score (LMS) is computed as LMS = arg maxπ∈SB
1
ℓ
P
B∈B sB,π(B), where SB is the
set of permutations respecting B (Definition 2). These scores are always computed on the test set.
Metric LMSSpear:
As mentioned in the main paper, this metric is used for the ScalarLatents dataset
where each block is 1-dimensional. Hence, this metric is almost the same as the mean correlation
coefficient (MCC), which is widely used in the nonlinear ICA literature [30, 31, 33, 36, 42], with
the only difference that we use Spearman correlation instead of Pearson correlation as a score sB,B′.
The Spearman correlation can capture nonlinear monotonous relations, unlike Pearson which can
only capture linear dependencies. We favor Spearman over Pearson because our identifiability result
(Theorem 2) guarantees we can recover the latents only up to permutation and element-wise invertible
transformations, which can be nonlinear.
Metric LMStree:
This metric is used for the BlockLatents dataset. For this metric, we take sB,B′
to be the R2 score of a Regression Tree with maximal depth of 10. For this, we used the class
sklearn.tree.DecisionTreeRegressor from the sklearn library. We learn the parameters of
the Decision Tree using the train dataset and then use it to evaluate LMStree metric on the test dataset.
For the additive decoder, it is easy to compute this metric since the additive structure already gives a
natural partition B which matches the ground-truth. However, for the non-additive decoder, there is
no natural partition and thus we cannot compute LMStree directly. To go around this problem, for the
non-additive decoder, we compute LMStree for all possible partitions of dz latent variables into blocks
of size |B| = 2 (assuming all blocks have the same dimension), and report the best LMStree. This
procedure is tractable in our experiments due to the small dimensionality of the problem we consider.
B.4
Boxplots for main experiments (Table 1)
Since the standard error in the main results (Table 1) was high, we provide boxplots in Figures
9 & 10 to have a better visibility on what is causing this. We observe that the high standard error
for the Additive approach was due to bad performance for a few bad random initializations for the
ScalarLatents dataset; while we have nearly perfect latent identification for the others. Figure 14e
shows the latent space learned by the worst case seed, which somehow learned a disconnected
support even if the ground-truth support was connected. Similarly, for the case of Independent
BlockLatents, there are only a couple of bad random initializations and the rest of the cases have
perfect identification.
34

Add. 
 (In-Supp)
Non-Add. 
 (In-Supp)
Add. 
 (Out-Supp)
Non-Add. 
 (Out-Supp)
10
1
6 × 10
2
2 × 10
1
Reconstruction MSE (Log Scale)
Add. 
 (In-Supp)
Non-Add. 
 (In-Supp)
Add. 
 (Out-Supp)
Non-Add. 
 (Out-Supp)
50
75
100
LMS-Spearman (MCC)
Figure 9: Reconstruction mean squared error (MSE) (↓) and Latent Matching Score (LMS) (↑) over
10 different random initializations for ScalarLatents dataset.
Additive
Non-Additive
10
2
10
1
Reconstruction MSE (Log Scale)
Additive
Non-Additive
25
50
75
100
LMS-Tree
(a) Independent Latent Case
Additive
Non-Additive
10
2
2 × 10
2
Reconstruction MSE (Log Scale)
Additive
Non-Additive
60
80
100
LMS-Tree
(b) Dependent Latent Case
Figure 10: Reconstruction mean squared error (MSE) (↓) and Latent Matching Score (LMS) (↑) for
10 different initializations for BlockLatents dataset.
B.5
Additional Results: BlockLatents Dataset
To get a qualitative understanding of latent identification in the BlockLatents dataset, we plot the
response of each predicted latent as we change a particular ground-truth latent factor. We describe
the following cases of changing the ground-truth latents:
• Ball 1 moving along x-axis: We sample 10 equally spaced points for z1 from [0, 1]; while
keeping other latents fixed as follows: z2 = 0.25, z3 = 0.50, z4 = 0.75. We will never
have occlusion since the balls are separated along the y-axis z4 −z2 > 0.
• Ball 2 moving along x-axis: We sample 10 equally spaced points for z3 from [0, 1]; while
keeping other latents fixed as follows: z1 = 0.50, z2 = 0.25, z4 = 0.75. We will never
have occlusion since the balls are separated along the y-axis z4 −z2 > 0.
• Ball 1 moving along y-axis: We sample 10 equally spaced points for z2 from [0, 1]; while
keeping other latents fixed as follows: z1 = 0.25, z3 = 0.75, z4 = 0.50. We will never
have occlusion since the balls are separated along the x-axis z3 −z1 > 0.
• Ball 2 moving along y-axis: We sample 10 equally spaced points for z4 from [0, 1]; while
keeping other latents fixed as follows: z1 = 0.25, z2 = 0.50, z3 = 0.75. We will never
have occlusion since the balls are separated along the x-axis z3 −z1 > 0.
35

0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(a) Additive Decoder (Best) (LMSTree : 99.9)
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(b) Non-Additive Decoder (Best) (LMSTree : 83.9)
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(c) Additive Decoder (Median) (LMSTree : 99.8)
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(d) Non-Additive Decoder (Median) (LMSTree : 58.6)
0.25
0.50
0.75
Ball 1 moving along x axis
4
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(e) Additive Decoder (Worst) (LMSTree : 54.1)
0.25
0.50
0.75
Ball 1 moving along x axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along x axis
2
0
2
0.25
0.50
0.75
Ball 1 moving along y axis
2
0
2
Predicted Latents
0.25
0.50
0.75
Ball 2 moving along y axis
2
0
2
Latent 1
Latent 2
Latent 3
Latent 4
(f) Non-Additive Decoder (Worst) (LMSTree : 24.6)
Figure 11: Latent responses for the cases with the best/median/worst LMSTree among runs performed
on the BlockLatent dataset with independent latents. In each plot, we report the latent factors
predicted from multiple images where one ball moves along only one axis at a time.
Figure 5 in the main paper presents the latent responses plot for the median LMStree case among
random initializations. In Figure 11, we provide the results for the case of best and the worst LMStree
among random seeds. We find that Additive Decoder fails for only for the worst case random seed,
while Non-Additive Decoder fails for all the cases.
Additionally, we provide the object-specific reconstructions for the Additive Decoder in Figure 12.
This helps us better understand the failure of Additive Decoder for the worst case random seed
(Figure 12c), where the issue arises due to bad reconstruction error.
36

(a) Additive Decoder (Best)
(b) Additive Decoder (Median)
(c) Additive Decoder (Worst)
Figure 12: Object-specific renderings with the best/median/worst LMStree among runs performed
on the BlockLatents dataset with independent latents. In each plot, the first row is the original image,
the second row is the reconstruction and the third and fourth rows are the output of the object-specific
decoders. In the best and median cases, each object-specific decoder corresponds to one and only one
object, e.g. the third row of the best case always corresponds to the red ball. However, in the worst
case, there are issues with reconstruction as only one of the balls is generated. Note that the visual
artefacts are due to the additive constant indeterminacy we saw in Theorem 2, which cancel each
other as is suggested by the absence of artefacts in the reconstruction.
B.6
Disconnected Support Experiments
Since path-connected latent support is an important assumption for latent identification with additive
decoders (Theorem 2), we provide results for the case where the assumption is not satisfied. We
experiment with the Disconnected Support dataset (Section B.2) and find that we obtain much worse
LMSSpear as compared to the case of training with L-shaped support in the ScalarLatents dataset.
Over 10 different random initializations, we find mean LMSSpear performance of 69.5 with standard
error of 6.69.
For better qualitative understanding, we provide visualization of the latent support and the extrapolated
images for the median LMSSpear among 10 random seeds in Figure 13. Somewhat surprisingly, the
representation appears to be aligned in the sense that the first predicted latent corresponds to the blue
ball while the second predicted latent correspond to the red ball. Also surprisingly, extrapolation
37

Figure 13: Learned latent space, ˆZtrain, and the corresponding reconstructed images of the additive
decoder with the median LMSSpear among runs performed on the Disconnected Support dataset.
The red dots correspond to latent factors used to generate the images.
occurs (we can see images of both balls high). That being said, we observe that the relationship
between the predicted latent 2 (ˆz2) and y-coordinate of second (red) ball is not monotonic, which
explains why the Spearman correlation is so low (Spearman correlation scores are high when there is
a monotonic relationship between both variables).
B.7
Additional Results: ScalarLatents Dataset
To get a qualitative understanding of extrapolation, we plot the latent support on the test dataset and
sample a grid of equally spaced points from the support of each predicted latent on the test dataset.
The grid represents the cartesian-product of the support of predicted latents and would contain novel
combinations of latents that were unseen during training. We show the reconstructed images for each
point from the cartesian-product grid to see whether the model is able to reconstruct well the novel
latent combinations.
Figure 4 in the main paper presents visualizations of the latent support and the extrapolated images
for the median LMSSpear case among random seeds. In Figure 14, we provide the results for the case
of best and the worst LMSSpear among random seeds. We find that even for the best case (Figure 14b),
Non-Additive Decoder does not generate good quality extrapolated images, while Additive Decoder
generates extrapoalted images for the best and median case. The worst-case run for the Additive
Decoder has disconnected support, which explains why it is not able to extrapolate.
38

(a) Additive Decoder (Best) (LMSSpear : 99.9)
(b) Non-Additive Decoder (Best) (LMSSpear : 99.9)
(c) Additive Decoder (Median) (LMSSpear : 99.9)
(d) Non-Additive Decoder (Median) (LMSSpear : 76.1)
(e) Additive Decoder (Worst) (LMSSpear : 69.5)
(f) Non-Additive Decoder (Worst) (LMSSpear : 39.8)
Figure 14: Figure (a, c, e) shows the learned latent space, ˆZtrain, and the corresponding reconstructed
images of the additive decoder with the best/median/worst LMSSpear among runs performed on
the ScalarLatents dataset. Figure (b, d, f) shows the same thing for the non-additive decoder. The
red dots correspond to latent factors used to generate the images and the yellow square highlights
extrapolated images.
39

