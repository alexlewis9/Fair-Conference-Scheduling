User-Level Differential Privacy
With Few Examples Per User
Badih Ghazi
Google Research
Mountain View, CA, US
badihghazi@gmail.com
Pritish Kamath
Google Research
Mountain View, CA, US
pritish@alum.mit.edu
Ravi Kumar
Google Research
Mountain View, CA, US
ravi.k53@gmail.com
Pasin Manurangsi
Google Research
Bangkok, Thailand
pasin@google.com
Raghu Meka
UCLA
Los Angeles, CA, US
raghum@cs.ucla.edu
Chiyuan Zhang
Google Research
Mountain View, CA, US
chiyuan@google.com
Abstract
Previous work on user-level differential privacy (DP) [GKM21, BGH+23] obtained
generic algorithms that work for various learning tasks. However, their focus was
on the example-rich regime, where the users have so many examples that each user
could themselves solve the problem. In this work we consider the example-scarce
regime, where each user has only a few examples, and obtain the following results:
• For approximate-DP, we give a generic transformation of any item-level DP
algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a
(multiplicative) savings of Oε,δ(√m) in terms of the number of users required
for achieving the same utility, where m is the number of examples per user. This
algorithm, while recovering most known bounds for specific problems, also gives
new bounds, e.g., for PAC learning.
• For pure-DP, we present a simple technique for adapting the exponential mech-
anism [MT07] to the user-level setting. This gives new bounds for a variety
of tasks, such as private PAC learning, hypothesis selection, and distribution
learning. For some of these problems, we show that our bounds are near-optimal.
1
Introduction
Differential privacy (DP) [DMNS06, DKM+06] has become a widely popular notion of privacy
quantifying a model’s leakage of personal user information. It has seen a variety of industrial
and governmental deployments including [Gre16, App17, DKY17, Abo18, RE19, TM20]. Many
fundamental private machine learning algorithms in literature have been devised under the (implicit)
assumption that each user only contributes a single example to the training dataset, which we will
refer to as item-level DP. In practice, each user often provides multiple examples to the training
data. In this user-level DP setting, the output of the algorithm is required to remain (approximately)
statistically indistinguishable even when all the items belonging to a single user are substituted
(formal definitions below). Given such a strong (and preferable) privacy requirement, it is highly
non-trivial for the algorithm to take advantage of the increased number of examples. Several recent
studies have explored this question and showed—for a wide variety of tasks—that, while challenging,
this is possible [LSY+20, LSA+21, GKM21, ILPS22, BGH+23].
To discuss further, let us define some notation. We use n to denote the number of users and m
to denote the number of examples per user. We use Z to denote the universe of possible samples.
We write x = (x1,1, . . . , x1,m, x2,1, . . . , xn,m) ∈Znm to denote the input to our algorithm, and
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

xi := (xi,1, . . . , xi,m) for i ∈[n] to denote the ith user’s examples. Furthermore, we write x−i to
denote the input with the ith user’s data xi removed. Similarly, for S ⊆[n], we write x−S to denote
the input with all the examples of users in S removed.
Definition 1 (User-Level Neighbors). Two inputs x, x′ are user-level neighbors, denoted by x ≍x′,
iff x−i = x′
−i for some i ∈[n].
Definition 2 (User-Level and Item-Level DP). For ε, δ > 0, we say that a randomized algorithm1
A : Z →O is (ε, δ)-user-level DP iff, for any x ≍x′ and any S ⊆O, we have Pr[A(x) ∈S] ≤
eε Pr[A(x′) ∈S] + δ. When m = 1, we say that A is (ε, δ)-item-level DP.
The case δ = 0 is referred to as pure-DP, whereas the case δ > 0 is referred to as approximate-DP.
To formulate statistical tasks studied in our work, we consider the setting where there is an unknown
distribution D over Z and the input x ∼Dnm consists of nm i.i.d. samples drawn from D. A task T
(including the desired accuracy) is defined by ΨT(D), which is the set of “correct” answers. For a
parameter γ, we say that the algorithm A is γ-useful if Prx∼Dnm[A(x) ∈ΨT(D)] ≥γ for all valid
D. Furthermore, we say that nm is the sample complexity of the algorithm, whereas n is its user
complexity. For m ∈N and ε, δ > 0, let nT
m(ε, δ; γ) denote the smallest user complexity of any
(ε, δ)-user-level DP algorithm that is γ-useful for T. When γ is not stated, it is assumed to be 2/3.
Generic Algorithms Based on Stability. So far, there have been two main themes of research on
user-level DP learning. The first aims to provide generic algorithms that work with many tasks. Ghazi
et al. [GKM21] observed that any pseudo-globally stable (aka reproducible [ILPS22]) algorithm can
be turned into a user-level DP algorithm. Roughly speaking, pseudo-global stability requires that
the algorithm, given a random input dataset and a random string, returns a canonical output—which
may depend on the random string—with a large probability (e.g., 0.5). They then show that the
current best generic PAC learning item-level DP algorithms from [GGKM21] (which are based on
yet another notion of stability) can be made into pseudo-globally stable algorithms. This transforms
the aforementioned item-level DP algorithms into user-level DP algorithms.
In a recent breakthrough, Bun et al. [BGH+23] significantly expanded this transformation by showing
that any item-level DP algorithm can be compiled into a pseudo-globally stable algorithm—albeit
with some overhead in the sample complexity. Combining with [GKM21], they then get a generic
transformation of any item-level DP algorithm to a user-level DP algorithm. Such results are of
(roughly) the following form: If there is an item-level DP algorithm for some task with sample
complexity n, then there is a user-level DP algorithm with user complexity O(log(1/δ)/ε) as long as
m ≥eΩε,δ(n2). In other words, if each user has sufficiently many samples, the algorithm needs very
few users to learn under the user-level DP constraint. However, the requirement on the number of
examples per user can be prohibitive: due to the nature of the reduction, it requires each user to have
enough samples to learn by themselves. This leads to the following natural question:2
Is there a generic transformation from item-level to user-level DP algorithms for small m?
Algorithms for Specific Tasks & the √m Savings.
Meanwhile, borrowing a page from its
item-level DP counterpart, another active research theme has been to study specific tasks and
provide (tight) upper and lower bounds on their sample complexity for user-level DP. Problems such
as mean estimation, stochastic convex optimization (SCO), and discrete distribution learning are
well-understood under user-level DP (for approximate-DP); see, e.g., [LSY+20, LSA+21, NME22,
GKK+23]. A pattern has emerged through this line of studies: (the privacy-dependent part of)
the user complexity in the user-level DP setting is often roughly 1/√m smaller than that of the
item-level DP setting. For example, for the task of discrete distribution learning on domain [k] up to
total variation distance of α > 0 (denoted by DDk(α)), the user complexity for the user-level DP
is [DHS15, ASZ21, NME22]
nDDk(α)
m
(ε, δ) = ˜Θδ

k
εα√m +
k
α2m

.
1For simplicity, we will only consider the case where the output space O is finite throughout this work to
avoid measure-theoretic issues. This assumption is often without loss of generality since even the output in
continuous problems can be discretized in such a way that the “additional error” is negligible.
2Of course, there is a generic transformation where each user throws away all but one example; here, we are
looking for one that can reduce the user complexity compared to the item-level setting.
2

We can see that the first privacy-dependent term decreases by a factor of √m whereas the latter
privacy-independent term decreases by a factor of m. A similar phenomenon also occurs in the other
problems discussed above. As such, it is intriguing to understand this question further:
Is there a common explanation for these √m saving factors for the privacy-dependent term?
1.1
Our Contributions
Approximate-DP.
We answer both questions by showing a generic way to transform any item-level
DP algorithm into a user-level DP algorithm such that the latter saves roughly √m in terms of the
number of users required. Since the formal expression is quite involved, we state below a simplified
version, which assumes that the sample complexity can be written as the sum of two terms, the
privacy-independent term (i.e., nT
stat) and the privacy-dependent term that grows linearly in 1/ε and
polylogarithmically in 1/δ (i.e.,

log(1/δ)O(1)
ε

· nT
priv). This is the most common form of sample
complexity of DP learning discovered so far in the literature—in particular, this covers all the tasks
we have discussed previously. The corollary below shows that the privacy-dependent term decreases
by a factor of roughly √m whereas the privacy-independent term decreases by a factor of m. The
full version of the statement can be found in Theorem 10.
Corollary 3 (Main Theorem–Simplified). Let T be any task and γ > 0 be a parameter. Suppose that
for any sufficiently small ε, δ > 0, there is an (ε, δ)-item-level DP algorithm that is γ-useful for T
with sample complexity eO

log(1/δ)c
ε
· nT
priv + nT
stat

for some constant c. Then, for any sufficiently
small ε, δ > 0, there exists an (ε, δ)-user-level DP algorithm that is (γ −o(1))-useful for T and has
user complexity
eO

1
√m · log(1/δ)c+3/2
ε2
· nT
priv + 1
m · nT
stat + log(1/δ)
ε

.
Our result provides a powerful and generic tool in learning with user-level DP, as it can translate
any item-level DP bound to user-level DP bound. Specifically, up to polylogarithmic terms and the
dependency on ε, the corollary above recovers all the aforementioned user complexity bounds for
mean estimation, SCO, and discrete distribution learning. In addition, it also presents new sample
complexity bounds for the moderate m regime for PAC learning.
While our result is very general, it—perhaps inevitably—comes at a cost: the algorithm is computa-
tionally inefficient and the dependence on ε (and δ) is not tight. We discuss this—along with other
open questions—in more detail in Section 5.
Pure-DP.
Surprisingly, the pure-DP setting is less explored compared to approximate-DP in the
context of learning with user-level DP; the sample complexity of several of the problems mentioned
above (e.g., discrete distribution learning and SCO) is well-understood under approximate-DP but not
so under pure-DP. While we do not provide a generic transformation from item-level DP algorithms
to user-level DP algorithms for pure-DP, we give simple modifications of the popular pure-DP
exponential mechanism [MT07] that allow it to be used in the user-level DP setting. Consequently,
we obtain improved bounds for several problems. Due to space constraints, in the main body of the
paper, we will only discuss PAC learning. Results for other problems, such as hypothesis testing and
distribution learning can be found in Appendix E.
In PAC learning, Z = X × {0, 1} and there is a concept class C ⊆{0, 1}X . An error of c : Z →
{0, 1} w.r.t. D is defined as errD(c) := Pr(x,y)∼D[c(x) ̸= y]. The task PAC(C; α) is to, for any
distribution D that has zero error on some concept c∗∈C (aka realizable by C), output c : Z →{0, 1}
such that errD(c) ≤α. We give a tight characterization of the user complexity for PAC learning3:
Theorem 4. Let C be any concept class with probabilistic representation dimension d (i.e.
PRDim(C) = d). Then, for any sufficiently small α, ε > 0 and for all m ∈N, we have
nPAC(C;α)
m
(ε, δ = 0) = ˜Θ
  d
ε +
d
εαm

.
Previous work [GKM21, BGH+23] achieves the tight sample complexity only for large m (≫d2).
3PRDim is defined in Definition 20.
3

1.1.1
Technical Overview
In this section, we give a rough overview of our proofs. We will be intentionally vague; all definitions
and results will be formalized later in the paper.
Approximate-DP. At a high-level, our proof proceeds roughly as follows. First, we show that any
(ε, δ)-item-level DP A with high probability satisfies a local version of user-level DP for which we
only compare4 A(x−i) for different i, but with ε that is increased by a factor of roughly √m (see
Definition 16 and Theorem 17). If this were to hold not only for x−i but also for all user-level
neighbors x′ of x, then we would have been done since the (ε, δ)-item-level DP algorithm would
have also yielded (ε√m, δ)-user-level DP (which would then imply a result in favor of Corollary 3).
However, this is not true in general and is where the second ingredient of our algorithm comes in: We
use a propose-test-release style algorithm [DL09] to check whether the input is close to a “bad” input
for which the aforementioned local condition does not hold. If so, then we output ⊥; otherwise, we
run the item-level DP algorithm. Note that the test passes w.h.p. and thus we get the desired utility.
This second step is similar to recent work of Kohli and Laskowski [KL21] and Ghazi et al. [GKK+23],
who used a similar algorithm for additive noise mechanisms (namely Laplace and Gaussian mech-
anisms). The main difference is that instead of testing for local sensitivity of the function as in
[KL21, GKK+23], we directly test for the privacy loss of the algorithm A.
As for the first step, we prove this via a connection to a notion of generalization called sample perfect
generalization [CLN+16] (see Definition 12). Roughly speaking, this notion measures the privacy
loss when we run an (ε, δ)-item-level DP algorithm on two x, x′ drawn independently from Dn. We
show that w.h.p. A(x) ≈ε′,δ′ A(x′) for ε′ = O(
p
n log(1/δ) · ε). Although ostensibly unrelated to
the local version of user-level DP that we discussed above, it turns out that these are indeed related:
if we view the algorithm that fixes examples of all but one user, then applying the sample perfect
generalization bound on just the input of the last user (which consists of only m samples) shows that
the privacy loss when changing the last user is O(
p
m log(1/δ) · ε) w.h.p. It turns out that we can
turn this argument into a formal bound for our local notion of user-level DP (see Section 3.2.1.)
We remark that our results on sample perfect generalization also resolve an open question of Cum-
mings et al. [CLN+16]; we defer this discussion to Appendix C.1.
Finally, we also note that, while Bun et al. [BGH+23] also uses the sample perfect generalization no-
tion, their conversion from item-level DP algorithms to user-level DP algorithms require constructing
pseudo-globally stable algorithms with sample complexity m. This is impossible when m is small,
which is the reason why their reduction works only in the example-rich setting.
Pure-DP. Recall that the exponential mechanism [MT07] works as follows. Suppose there is a
set H of candidates we would like to select from, together with the scoring functions (scrH)H∈H,
where scrH : Zn →R has sensitivity at most ∆. The algorithm then outputs H with probability
proportional to exp (−ε·scrH(x)/2∆). The error guarantee of the algorithm scales with the sensitivity
∆(see Theorem 9.) It is often the case that scrH depends on the sum of individual terms involving
each item. Our approach is to clip the contribution from each user so that the sensitivity is small. We
show that selecting the clipping threshold appropriately is sufficient obtain new user-level pure-DP
(and in some cases optimal) algorithms.
2
Preliminaries
Let [n] = {1, . . . , n}, let [x]+ = max(0, x), and let eO(f) denote O(f logc f) for some constant c.
Let supp(A) denote the support of distribution A. We recall some standard tools from DP [Vad17].
For two distributions A, B, let dε(A ∥B) = P
x∈supp(A)[A(x) −eεB(x)]+ denote the eε-hockey
stick divergence. For brevity, we write A ≈ε,δ B to denote dε(A ∥B) ≤δ and dε(B ∥A) ≤δ.
Lemma 5 (“Triangle Inequality”). If A ≈ε′,δ′ B and B ≈ε,δ C, then A ≈ε+ε′,eεδ′+eε′δ C.
Lemma 6 (Group Privacy). Let A be any (ε, δ)-item-level DP algorithm and x, x′ be k-neighbors5,
then A(x) ≈ε′,δ′ A(x′) where ε′ = kε, δ′ = ekε−1
eε−1 δ ≤kekεδ.
4In Theorem 17, we need to compare A(x−S) for S’s of small sizes as well.
5I.e., there exists x = x0, x1, . . . , xk = x′ such that xi−1, xi are item-level neighbors for all i ∈[k].
4

Theorem 7 (Amplification-by-Subsampling [BBG18]). Let n, n′ ∈N be such that n′ ≥n. For any
(ε, δ)-item-level DP algorithm A with sample complexity n, let A′ denote the algorithm with sample
complexity n′ that randomly chooses n out of the n′ input samples and then runs A on the subsample.
Then A′ is (ε′, δ′)-DP where ε′ = ln (1 + η(eε −1)) , δ′ = ηδ for η = n/n′.
We also need the definition of the shifted and truncated discrete Laplace distribution:
Definition 8 (Shifted Truncated Discrete Laplace Distribution). For any ε, δ > 0, let κ = κ(ε, δ) :=
1 + ⌈ln(1/δ)/ε⌉and let TDLap(ε, δ) be the distribution supported on {0, . . . , 2κ} with probability
mass function at x being proportional to exp (−ε · |x −κ|).
Exponential Mechanism (EM). In the (generalized) selection problem (SELECT), we are given a
candidate set H and scoring functions scrH for all H ∈H whose sensitivities are at most ∆. We say
that an algorithm A is (α, β)-accurate iff PrH∼A(x)[scrH(x) ≤minH′∈H scrH′(x) + α] ≥1 −β.
Theorem 9 ([MT07]). There is an ε-DP (O(∆· log(|H|/β)/ε), β)-accurate algorithm for SELECT.
3
Approximate-DP
In this section, we prove our main result on approximate-DP user-level learning, which is stated in
Theorem 10 below. Throughout the paper we say that a parameter is “sufficiently small” if it is at
most an implicit absolute constant.
Theorem 10 (Main Theorem). Let T be any task and γ > 0 be a parameter. Then, for any sufficiently
small ε, δ > 0 and m ∈N, there exists ε′ =
ε2
log(1/δ)√
m log(m/δ) and δ′ = Θ

δ ·
ε
m log(1/δ)

such
that
nT
m(ε, δ; γ −o(1)) ≤eO

log(1/δ)
ε
+ 1
m · nT
1 (ε′, δ′; γ)

.
Note that Corollary 3 follows directly from Theorem 10 by plugging in the expression nT
1 (ε′, δ′; γ) =
eO

log(1/δ′)c
ε′
· nT
priv + nT
stat

into the bound.
While it might be somewhat challenging to interpret the bound in Theorem 10, given that the ε values
on the left and the right hand sides are not the same, a simple subsampling argument (similar to one
in [BGH+23]) allows us to make the LHS and RHS have the same ε. In this case, we have that the
user complexity is reduced by a factor of roughly 1/√m, as stated below. We remark that the δ
values are still different on the two sides; however, if we assume that the dependence on 1/δ is only
polylogarithmic, this results in at most a small gap of at most polylogarithmic factor in 1/δ.
Theorem 11 (Main Theorem–Same ε Version). Let T be any task and γ > 0 be a parameter. Then,
for any sufficiently small ε, δ > 0, and m ∈N, there exists δ′ = Θ

δ ·
ε
m log(1/δ)

such that
nT
m(ε, δ; γ −o(1)) ≤eO
log(1/δ)
ε
+
1
√m · log(1/δ)1.5
ε
· nT
1 (ε, δ′; γ)

.
Even though Theorem 11 might be easier to interpret, we note that it does not result in as sharp a
bound as Theorem 10 in some scenarios. For example, if we use Theorem 11 under the assumption
in Corollary 3, then the privacy-independent part would be
1
√m · log(1/δ)1.5
ε
· nT
stat, instead of 1
m · nT
stat
implied by Theorem 10. (On the other hand, the privacy-dependent part is the same.)
The remainder of this section is devoted to the proof of Theorem 10. The high-level structure follows
the overview in Section 1.1.1: first we show in Section 3.1 that any item-level DP algorithm satisfies
sample perfect generalization. Section 3.2.1 then relates this to a local version of user-level DP. We
then describe the full algorithm and its guarantees in Section 3.2.2. Since we fix a task T throughout
this section, we will henceforth discard the superscript T for brevity.
3.1
DP Implies Sample Perfect Generalization
We begin with the definition of sample perfect generalization algorithms.
5

Definition 12 ([CLN+16]). For β, ε, δ > 0, an algorithm A : Zn →O is said to be (β, ε, δ)-sample
perfectly generalizing iff, for any distribution D (over Z), Prx,x′∼Dn[A(x) ≈ε,δ A(x′)] ≥1 −β.
The
main
result
of
this
subsection
is
that
any
(ε, δ)-item-level
DP
algorithm
is
(β, O(ε
p
n log(1/βδ)), O(nδ))-sample perfectly generalizing:
Theorem 13 (DP ⇒Sample Perfect Generalization). Suppose that A : Zn →O is (ε, δ)-item-level
DP, and assume that ε, δ, β, ε
p
n log(1/βδ) > 0 are sufficiently small. Then, A is (β, ε′, δ′)-sample
perfectly generalizing, where ε′ ≤O(ε
p
n log(1/βδ)) and δ′ = O(nδ).
3.1.1
Bounding the Expected Divergence
As a first step, we upper bound the expectation of the hockey stick divergence dε′(A(x) ∥A(x′)):
Lemma 14. Suppose that A : Zn →O is an (ε, δ)-item-level DP algorithm. Further, assume that
ε, δ, and ε
p
n log(1/δ) > 0 are sufficiently small. Then, for ε′ = O(ε
p
n log(1/δ)), we have
Ex,x′∼Dn[dε′(A(x) ∥A(x′))] ≤O(nδ).
This proof follows an idea from [CLN+16], who prove a similar statement for pure-DP algorithms
(i.e., δ = 0). For every output o ∈O, they consider the function go : Zn →R defined by
go(x) := ln(Pr[A(x) = o]). The observation here is that, in the pure-DP case, this function is
ε-Lipschitz (i.e., changing a single coordinate changes its value by at most ε). They then apply
McDiarmid’s inequality on go to show that the function values are well-concentrated with a variance
of O(√n · ε). This suffices to prove the above statement for the case where the algorithm is pure-DP
(but the final bound still contains δ). To adapt this proof to the approximate-DP case, we prove a
“robust” version of McDiarmid’s inequality that works even for the case where the Lipschitz property
is violated on some pairs of neighbors (Lemma 32). This inequality is shown by arguing that we
may change the function “slightly” to make it multiplicative-Lipschitz, which then allows us to apply
standard techniques. Due to space constraints, we defer the full proof, which is technically involved,
to Appendix C.3.
3.1.2
Boosting the Probability: Proof of Theorem 13
While Lemma 14 bounds the expectation of the hockey stick divergence, it is insufficient to get
arbitrarily high probability bound (i.e., for any β as in Theorem 13); for example, using Markov’s
inequality would only be able to handle β > δ′. However, we require very small β in a subsequent
step of the proofs (in particular Theorem 17). Fortunately, we observe that it is easy to “boost” the β
to be arbitrarily small, at an additive cost of ε
p
n log(1/β) in the privacy loss.
To do so, we will need the following concentration result of Talagrand6 for product measures:
Theorem 15 ([Tal95]). For any distribution D over Z, E ⊆Zn, and t > 0, if Prx∼Dn[x ∈E] ≥1/2,
then Prx∼Dn[x ∈E≤t] ≥1 −0.5 exp
 −t2/4n

, where E≤t := {x′ ∈Zn | ∃x ∈E, ∥x′ −x∥0 ≤t}.
Proof of Theorem 13. We consider two cases based on whether β ≥2−n or not. Let us start with the
case that β ≥2−n. From Lemma 14, for ε′′ = O(ε
p
n log(1/δ)), we have
Ex,x′∼Dn[dε′′(A(x) ∥A(x′))] ≤O(nδ) =: δ′′.
Let E ⊆Zn × Zn denote the set of (˜x, ˜x′) such that A(˜x) ≈ε′′,4δ′′ A(˜x′). By Markov’s inequality,
we have that Prx,x′∼Dn[dε′′(A(˜x) ∥A(˜x′)) ≤4δ′′] ≥3/4 and Prx,x′∼Dn[dε′′(A(˜x′) ∥A(˜x)) ≤
4δ′′] ≥3/4 and hence Prx,x′∼Dn[(x, x′) ∈E] ≥1/2. By Theorem 15, for t = O(
p
n log(1/β)),
we have
Pr
x,x′∼Dn[(x, x′) ∈E≤t] ≥1 −β.
Next, consider any (x, x′) ∈E≤t. Since there exists (˜x, ˜x′) ∈E such that ∥x −˜x∥0, ∥x′ −
˜x′∥0 ≤t, we may apply Lemma 6 to conclude that dεt(A(x) ∥A(˜x)), dεt(A(˜x′) ∥A(x′)) ≤
6This is a substantially simplified version compared to the one in [Tal95], but is sufficient for our proof.
6

O(tδ). Furthermore, since dε′′(A(˜x) ∥A(˜x′)) ≤4δ′′, we may apply Lemma 5 to conclude that
dε′(A(x) ∥A(x′)) ≤δ′ where ε′ = ε′′ + 2tε = O(ε
p
n log(1/βδ)) and δ′ = O(δ′′ + tδ) = O(nδ).
Similarly, we also have dε′(A(x′ ∥A(x)) ≤δ′. In other words, A(x) ≈ε′,δ′ A(x′). Combining this
and the above inequality implies that A is (β, ε′, δ′)-sample perfectly generalizing as desired.
For the case β < 2−n, we may immediately apply group privacy (Lemma 6). Since any x, x′ ∼Dn
satisfies ∥x −x′∥0 ≤n, we have A(x) ≈ε′,δ′ A(x′) for ε′ = nε and δ′ = O(nδ). This implies
that A is (β, ε′, δ′)-sample perfectly generalizing (in fact, (0, ε′, δ′)-sample perfectly generalizing),
which concludes our proof.
3.2
From Sample Perfect Generalization to User-Level DP
Next, we will use the sample perfect generalization result from the previous subsection to show that
our algorithm satisfies a certain definition of a local version of user-level DP. We then finally turn this
intuition into an algorithm and prove Theorem 10.
3.2.1
Achieving Local-Deletion DP
We start by defining local-deletion DP (for user-level DP). As alluded to earlier, this definition only
considers x−i for different i’s. We also define the multi-deletion version where we consider x−S for
all subsets S of small size.
Definition 16 (Local-Deletion DP). An algorithm A is (ε, δ)-local-deletion DP (abbreviated LDDP)
at input x if A(x−i) ≈ε,δ A(x−i′) for all i, i′ ∈[n]. Furthermore, for r ∈N, an algorithm A is
(r, ε, δ)-local-deletion DP at x if A(x−S) ≈ε,δ A(x−S′) for all S, S′ ⊆[n] such that |S| = |S′| = r.
Below we show that, with high probability (over the input x), any item-level DP algorithm satisfies
LDDP with the privacy loss increased by roughly r√m. The proof uses the crucial observation that
relates sample perfect generalization to user-level DP.
Theorem 17. Let n, r ∈N with r ≤n, and let A : X (n−r)m →O be any (ε′, δ′)-item-level DP
algorithm. Further, assume that ε′, δ′, ε′r
p
m log(n/βδ′) > 0 are sufficiently small. Then, for any
distribution D,
Pr
x∼Dnm[A is (r, ε, δ)-LDDP at x] ≥1 −β,
where ε = O(ε′r
p
m log(n/βδ′)) and δ = O(rmδ′).
Proof. Let β′ = β/(
n
r)
2. Consider any fixed S, S′ ⊆[n] such that |S| = |S′| = r. Let T =
S ∪S′, t = |T|, U = S′ ∖S, U ′ = S ∖S′, q = |U| = |U ′|, and let A′
x−T denote the algorithm
defined by A′
x−T (y) = A(x−T ∪y).
Thus, for any fixed x−T ∈Z(n−t)m, Theorem 13 implies that
Pr
xU,xU′∼Dqm
h
A′
x−T (xU) ̸≈ε,δ A′
x−T (xU ′)
i
≤β′,
(1)
where ε = O(ε′p
qm log(1/β′δ′)) ≤O(ε′r
p
m log(n/βδ′)) and δ = O(rmδ′).
Notice that A′
x−T (xU) = A(x−S) and A′
x−T (xU ′) = A(x−S′). Thus, we have
Pr
x∼Dnm[A(x−S) ̸≈ε,δ A(x−S′)] = Ex−T ∼D(n−t)m

Pr
xU,xU′∼Dqm
h
A′
x−T (xU) ̸≈ε,δ A′
x−T (xU ′)
i
(1)
≤Ex−T ∼D(n−t)m [β′] = β′.
Hence, by taking a union bound over all possible S, S′ ⊆[n] such that |S| = |S′| = r, we have
Prx∼Dnm[A is not (r, ε, δ)-LDDP at x] ≤β as desired.
3.2.2
From LDDP to DP via Propose-Test-Release
Our user-level DP algorithm is present in Algorithm 1. Roughly speaking, we try to find a small
subset S such that x−S is LDDP with appropriate parameters. (Throughout, we assume that n ≥4κ
7

where κ is from Definition 8.) The target size of S is noised using the discrete Laplace distribution.
As stated in Section 1.1.1, the algorithm is very similar to that of [GKK+23], with the main difference
being that (i) X R1
stable is defined based on LDDP instead of the local deletion sensitivity and (ii) our
output is A(x−T ), whereas their output is the function value with Gaussian noise added. It is not
hard to adapt their proof to show that our algorithm is (ε, δ)-user-level DP, as stated below. (Full
proof is deferred to Appendix C.4.)
Lemma 18 (Privacy Guarantee). DelStabε,δ,A is (ε, δ)-user-level DP.
Algorithm 1 DelStabε,δ,A(x)
1: Input: Dataset x ∈X nm
2: Parameters: Privacy parameters ε, δ, Algorithm A : X (n−4κ)m →O
3: ε ←ε
3, δ ←
δ
e2ε+eε+2, κ ←κ(ε, δ).
4: Sample R1 ∼TDLap(ε, δ)
{See Definition 8}
5: X R1
stable ←

S ⊆[n] : |S| = R1, A is (4κ −R1, ε, δ)-LDDP at x−S
	
{See Definition 16}
6: if |X R1
stable| = ∅then
7:
return ⊥
8: Choose S ∈X R1
stable uniformly at random
9: Choose T ⊇S of size 4κ uniformly at random
10: return A(x−T )
Next, we prove the utility guarantee under the assumption that A is item-level DP:
Lemma 19 (Utility Guarantee). Let ε, δ > 0 be sufficiently small. Suppose that A is (ε′, δ′)-item-level
DP such that ε′ = Θ

ε
κ√
m log(nm/δ)

and δ′ = Θ
  δ
κm

. If A is γ-useful for any task T with
(n −4κ)m samples, then DelStabε,δ,A is (γ −o(1))-useful for T (with nm samples).
Proof. Consider another algorithm A0 defined in the same way as DelStabε,δ,A except that on Line 5,
we simply let X R1
stable ←
 [n]
R1

. Note that A0(x) is equivalent to randomly selecting n −4κ users and
running A on it. Therefore, we have Prx∼Dnm[A0(x) ∈ΨT] = Prx∼D(n−4κ)m[A(x) ∈ΨT] ≥γ
Meanwhile, A0 and DelStabε,δ,A are exactly the same as long as X R1
stable(x) =
 [n]
R1

in the latter. In
other words, we have
Pr
x∼Dnm[DelStabε,δ,A(x) ∈ΨT]
≥
Pr
x∼Dnm[A0(x) ∈ΨT] −
Pr
x∼Dnm
h
X R1
stable(x) ̸=
 [n]
R1
i
≥γ −
Pr
x∼Dnm[A is not (4κ, ε, δ)-LDDP at x]
≥γ −o(1),
where in the last inequality we apply Theorem 17 with, e.g., β = 0.1/nm.
Putting Things Together.
Theorem 10 is now “almost” an immediate consequence of Lemmas 18
and 19 (using κ = O (log(1/δ)/ε)). The only challenge here is that ε′ required in Lemma 19 depends
polylogarithmically on n. Nonetheless, we show below via a simple subsampling argument that this
only adds polylogarithmic overhead to the user/sample complexity.
Proof of Theorem 10. Let ε′
=
ε′(n), δ′ be as in Lemma 19.
Furthermore, let ε′′
=
ε2
log(1/δ)√
m log(m/δ). Notice that ε′(n)/ε′′ ≤(log n)O(1). Let N ∈N be the smallest number
such that N is divisible by m and

eε′′ −1

· n1(ε′′,δ′)
N
≤eε′(N/m+4κ) −1,
Observe that N ≤eO (mκ + n1(ε′′, δ′)).
From the definition of n1, there exists an (ε′′, δ′)-item-level DP algorithm A0 that is γ-useful for
T with sample complexity n1(ε′′, δ′). We start by constructing an algorithm A on N samples as
follows: randomly sample (without replacement) n1(ε′′, δ′) out of N items, then run A0 on this
8

subsample. By amplification-by-subsampling (Theorem 7), we have that A is (ε′(n), δ′)-item-level
DP for n = N/m + 4κ. Thus, by Lemma 18 and Lemma 19, we have that DelStabε,δ,A is (ε, δ)-DP
and (γ −o(1))-useful for T. Its user complexity is
n = N/m + 4κ = eO
 
log(1/δ)
ε
+ 1
m · n1
 
ε2
log(1/δ)
p
m log(m/δ)
, δ′
!!
.
4
Pure-DP: PAC Learning
In this section we prove Theorem 4.
Let z denote the input and for i ∈[n], let zi =
((xi,1, yi,1), . . . , (xi,m, yi,m)) denote the input to the ith user. A concept class is a set of func-
tions of the form X →{0, 1}. For T ⊆X × {0, 1}, we say that it is realizable by c : X →{0, 1} iff
c(x) = y for all (x, y) ∈T. Recall the definition of PRDim:
Definition 20 ([BNS19]). A distribution P on concept classes is an (α, β)-probabilistic representa-
tion (abbreviated as (α, β)-PR) of a concept class C if for every c ∈C and for every distribution DX
on X, with probability 1 −β over H ∼P, there exists h ∈H such that Prx∼DX [c(x) ̸= h(x)] ≤α.
The (α, β)-PR dimension of C is defined as PRDimα,β(C) := min(α,β)-PR P of C size(P), where
size(P) := maxH∈supp(P) log |H|. Furthermore, let the probabilistic representation dimension of C
be PRDim(C) := PRDim1/4,1/4(C).
Lemma 21 ([BNS19]). For any C and α ∈(0, 1/2), PRDimα,1/4(C) ≤eO (PRDim(C) · log(1/α)).
Proof of Theorem 4. The lower bound ˜Ω(d/ε) was shown in [GKM21] (for any value of m). The
lower bound ˜Ω
 d
εαm

follows from the lower bound of ˜Ω
  d
εα

on the sample complexity in the
item-level setting by [BNS19] and the observation that any (ε, δ)-user-level DP algorithm is also
(ε, δ)-item-level DP with the same sample complexity. Thus, we may focus on the upper bound.
It suffices to only prove the upper bound eO
  d
ε +
d
εαm

assuming7 m ≤1/α. Let P denote a
(0.01α, 1/4)-PR of C; by Lemma 21, there exists such P with size(P) ≤eO (d · log(1/α)). Assume
that the number n of users is at least κ·size(P)
εαm
= eO
 d
εαm

, where κ is a sufficiently large constant.
Our algorithm works as follows: Sample H ∼P and then run the ε-DP EM (Theorem 9) on candidate
set H with the scoring function scrh(z) := P
i∈[n] 1[zi is not realizable by h].
Observe that the sensitivity of scrh is one. Indeed, this is where our “clipping” has been applied: a
standard item-level step would sum up the error across all the samples, resulting in the sensitivity as
large as m, while our scoring function above “clips” the contribution of each user to just one.
A simple concentration argument (deferred to Appendix D) gives us the following:
Lemma 22. With probability 0.9 (over the input), the following hold for all h ∈H:
(i) If errD(h) ≤0.01α, then scrh(z) ≤0.05αnm.
(ii) If errD(h) > α, then scrh(z) > 0.1αnm.
We now assume that the event in Lemma 22 holds and finish the proof. Since a P is a (0.01α, 1/4)-
PR of C, with probability 3/4, we have that there exists h∗∈H such that errD(h∗) ≤0.01α.
From Lemma 22(i), we have scrh∗(z) ≤0.05αnm. Thus, by Theorem 9, with probability 2/3, the
algorithm outputs h that satisfies scrh(z) ≤0.05αnm + O (size(P)/ε), which, for a sufficiently
large κ, is at most 0.1αnm. By Lemma 22(ii), this implies that errD(h) ≤α as desired.
5
Conclusion and Open Questions
We presented generic techniques to transform item-level DP algorithms to user-level DP algorithms.
7If m is larger, then we can simply disregard the extra examples (as the first term dominates the bound).
9

There are several open questions. Although our upper bounds are demonstrated to be tight for many
tasks discussed in this work, it is not hard to see that they are not always tight8; thus, an important
research direction is to get a better understanding of the tight user complexity bounds for different
learning tasks.
On a more technical front, our transformation for approximate-DP (Theorem 10) has an item-level
privacy loss (roughly) of the form
ε2
√
m log(1/δ)3 while it seems plausible that
ε
√
m log(1/δ) can be
achieved; an obvious open question is to give such an improvement, or show that it is impossible. We
note that the extra factor of log(1/δ)/ε comes from the fact that Algorithm 1 uses LDDP with distance
O(κ) = O (log(1/δ)/ε). It is unclear how one could avoid this, given that the propose-test-release
framework requires checking input at distance O (log(1/δ)/ε).
Another limitation of our algorithms is their computational inefficiency, as their running time grows
linearly with the size of the possible outputs. In fact, the approximate-DP algorithm could even be
slower than this since we need to check whether A is LDDP at a certain input x (which involves
computing dε(A−S ∥A−S′)). Nevertheless, given the generality of our results, it is unlikely that a
generic efficient algorithm exists with matching user complexity; for example, even the algorithm
for user-level DP-SCO in [GKK+23] has a worst case running time that is not polynomial. Thus, it
remains an interesting direction to devise more efficient algorithms for specific tasks.
Finally, while our work has focused on the central model of DP, where the algorithm gets the access
to the raw input data, other models of DP have also been studied in the context of user-level DP,
such as the local DP model [GKM21, ALS23]. Here again, [GKM21, BGH+23] give a generic
transformation that sometimes drastically reduces the user complexity in the example-rich case.
Another interesting research direction is to devise a refined transformation for the example-sparse
case (like one presented in our paper for the central model) but for the local model.
8E.g., for PAC learning, Theorem 4 provides a better user complexity than applying the approximate-DP
transformation (Theorem 10) to item-level approximate-DP learners for a large regime of parameters and concept
classes; the former has user complexity that decreases with 1/m whereas the latter decreases with 1/√m.
10

References
[Abo18] John M Abowd. The US Census Bureau adopts differential privacy. In KDD, pages
2867–2867, 2018.
[ALS23] Jayadev Acharya, Yuhan Liu, and Ziteng Sun. Discrete distribution estimation under
user-level local differential privacy. In AISTATS, 2023.
[App17] Apple Differential Privacy Team. Learning with privacy at scale. Apple Machine
Learning Journal, 2017.
[ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private Assouad, Fano,
and Le Cam. In ALT, pages 48–78, 2021.
[BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling:
Tight analyses via couplings and divergences. In NeurIPS, pages 6280–6290, 2018.
[BF16] Raef Bassily and Yoav Freund.
Typicality-based stability and privacy.
CoRR,
abs/1604.03336, 2016.
[BGH+23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann
Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between
replicability, privacy, and adaptive generalization. In STOC, 2023.
[BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis
selection. In NeurIPS, pages 156–167, 2019.
[BNS19] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity
of pure private learners. JMLR, 20:146:1–146:33, 2019.
[CLN+16] Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, and Zhiwei Steven Wu.
Adaptive learning with robust generalization guarantees. In COLT, pages 772–814,
2016.
[DHS15] Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning
of structured discrete distributions. In NIPS, pages 2566–2574, 2015.
[DKM+06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT,
pages 486–503, 2006.
[DKY17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data
privately. In NeurIPS, pages 3571–3580, 2017.
[DL09] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC, pages
371–380, 2009.
[DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise
to sensitivity in private data analysis. In TCC, pages 265–284, 2006.
[DMR20] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The minimax learning rates
of normal and Ising undirected graphical models. Electronic Journal of Statistics,
14(1):2338 – 2361, 2020.
[GGKM21] Badih Ghazi, Noah Golowich, Ravi Kumar, and Pasin Manurangsi. Sample-efficient
proper PAC learning with approximate differential privacy. In STOC, 2021.
[GKK+23] Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Raghu Meka, and
Chiyuan Zhang. On user-level private convex optimization. In ICML, 2023.
[GKM21] Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private
learning via correlated sampling. In NeurIPS, pages 20172–20184, 2021.
[Gre16] Andy Greenberg. Apple’s “differential privacy” is about collecting your data – but not
your data. Wired, June, 13, 2016.
11

[ILPS22] Russell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell. Reproducibility in
learning. In STOC, pages 818–831, 2022.
[KL21] Nitin Kohli and Paul Laskowski. Differential privacy for black-box statistical analyses.
In TPDP, 2021.
[KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning
high-dimensional distributions. In COLT, pages 1853–1902, 2019.
[Kut02] Samuel Kutin. Extensions to McDiarmid’s inequality when differences are bounded
with high probability. Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep.
TR-2002-04, 2002.
[LSA+21] Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri,
and Ananda Theertha Suresh. Learning with user-level privacy. In NeurIPS, pages
12466–12479, 2021.
[LSY+20] Yuhan Liu, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Michael Riley.
Learning discrete distributions: user vs item-level privacy. In NeurIPS, 2020.
[MT07] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS,
pages 94–103, 2007.
[NME22] Shyam Narayanan, Vahab S. Mirrokni, and Hossein Esfandiari. Tight and robust private
mean estimation with few users. In ICML, pages 16383–16412, 2022.
[RE19] Carey Radebaugh and Ulfar Erlingsson. Introducing TensorFlow Privacy: Learning
with Differential Privacy for Training Data, March 2019. blog.tensorflow.org.
[Tal95] Michel Talagrand. Concentration of measure and isoperimetric inequalities in product
spaces. Publications Mathématiques de l’Institut des Hautes Etudes Scientifiques,
81:73–205, 1995.
[TM20] Davide Testuggine and Ilya Mironov. PyTorch Differential Privacy Series Part 1: DP-
SGD Algorithm Explained, August 2020. medium.com.
[Vad17] Salil P. Vadhan. The complexity of differential privacy. In Tutorials on the Foundations
of Cryptography, pages 347–450. Springer International Publishing, 2017.
[War16] Lutz Warnke. On the method of typical bounded differences. Comb. Probab. Comput.,
25(2):269–299, 2016.
12

A
Summary of Results
Our results for pure-DP are summarized in Table 1.
Problem
Item-Level DP
User-Level DP
(Previous Work)
(Previous Work)
(Our Work)
PAC Learning
˜Θ
  d
εα

˜Θ
  d
ε

when m ≥˜Θ

d2
ε2α2

˜Θ
  d
ε +
d
εαm

[BNS19]
[GKM21, BGH+23]
Theorem 4
Agnostic PAC
Learning
˜Θ
  d
α2 +
d
εα

-
eO

d
ε +
d
εα√m +
d
α2m

[BNS19]
Theorem 40
Learning Discrete
Distribution
˜Θ
  k
εα +
k
α2

-
˜Θ

k
ε +
k
εα√m +
k
α2m

[BKSW19, ASZ21]
Theorem 43
Learning Product
Distribution
˜Θ
  kd
εα + kd
α2

-
˜Θ

kd
ε +
kd
εα√m +
kd
α2m

[BKSW19, ASZ21]
Theorem 44
Learning Gaussian
(Known Covariance)
˜Θ
  d
εα +
d
α2

-
˜Θ

d
ε +
d
εα√m +
d
α2m

[BKSW19, ASZ21]
Theorem 45
Learning Gaussian
(Bounded Cov.)
˜Θ

d2
εα + d2
α2

-
˜Θ

d2
ε +
d2
εα√m +
d2
α2m

[BKSW19, KLSU19]
Theorem 46
Table 1: Summary of results on pure-DP. For (agnostic) PAC learning, d denotes the probabilistic represen-
tation dimension (Definition 20) of the concept class. To the best of our knowledge, none of the distribution
learning problems has been studied explicitly under pure user-level DP in previous work, although some bounds
can be derived using previous techniques for approximate-DP.
For approximate-DP, our transformation (Theorem 10) is very general and can be applied to any
statistical tasks. Due to this, we do not attempt to exhaustively list its corollaries. Rather, we give
only a few examples of consequences of Theorem 10 in Table 2. We note that in all cases, we either
improve upon previous results or match them up to log(1/δ)O(1)
ε
factor for all values of m.
Problem
Item-Level DP
User-Level DP
(Previous Work)
(Previous Work)
(Our Work)
PAC Learning
eO

d6
L
εα2

eO
  1
ε

when m ≥˜Θ

d12
L
ε2α4

eO

1
ε +
d6
L
ε2α2√m

[GGKM21]
[GGKM21, BGH+23]
Theorem 10
Learning Discrete
Distribution
˜Θ
  k
εα +
k
α2

˜Θ

1
ε +
k
εα√m +
k
α2m

eO

1
ε +
k
ε2α√m +
k
α2m

[BKSW19, ASZ21]
[NME22]
Theorem 10
Learning Product
Distribution
eO
  kd
εα + kd
α2

-
eO

1
ε +
kd
ε2α√m +
kd
α2m

[BKSW19]
Theorem 10
Table 2: Example results on approximate-DP. Each of our results is a combination of Theorem 10 and the
previously known item-level DP bound. For PAC learning, dL denotes the Littlestone’s dimension of the concept
class. We assume that δ ≪ε and use eO, ˜Θ to also hide polylogarithmic factors in 1/δ.
B
Additional Preliminaries
In this section, we list some additional material that is required for the remaining proofs.
13

For a, b ∈R such that a ≤b, let clipa,b : R →R denote the function
clipa,b(x) =



a
if x < a,
b
if x > b,
x
otherwise.
For two distributions A, B, we use dtv(A, B) := 1
2
P
x∈supp(A)∪supp(B) |A(x) −B(x)| to denote
their total variation distance, dKL(A ∥B) := P
x∈supp(A) A(x) · log(A(x)/B(x)) to denote their
Kullback–Leibler divergence, and dχ2(A ∥B) := P
x∈supp(A)
(A(x)−B(x))2
B(x)
to denote the χ2-
divergence. We use Ak to denote the distribution of k independent samples drawn from A. The
following three well-known facts will be useful in our proofs.
Lemma 23. For any two distributions A and B,
(i) (Pinsker’s inequality) dtv(A, B) ≤
q
1
2 · dKL(A ∥B).
(ii) dKL(Ak ∥Bk) = k · dKL(A ∥B) for k ∈N.
(iii) dKL(A ∥B) ≤dχ2(A ∥B).
We use ∥u∥0 to denote the Hamming “norm”9 of u, i.e., the number of non-zero coordinates of u.
The following is a well-known fact (aka constant-rate constant-distance error correcting codes),
which will be useful in our lower bound proofs.
Theorem 24. For d ∈N, there are u1, . . . , uW ∈{0, 1}d for W = 2Ω(d) such that ∥ui −uj∥0 ≥
Ω(d) for all distinct i, j ∈[W].
B.1
Concentration Inequalities
Next, we list a few concentration inequalities in convenient forms for subsequent proofs.
Chernoff bound.
We start with the standard Chernoff bound.
Lemma 25 (Chernoff Bound). Let X1, . . . , Xn denote i.i.d. Bernoulli random variables with success
probability p. Then, for all δ > 0, we have
Pr[X1 + · · · + Xn ≤(1 −δ)pn] ≤exp
−δ2pn
2

,
and,
Pr[X1 + · · · + Xn ≥(1 + δ)pn] ≤exp
−δ2pn
2 + δ

.
McDiarmid’s inequality.
For a distribution D on Z, and a function g : Zn →R, let µDn(g) :=
Ex∼Dn[g(x)]. We say that g is a c-bounded difference function iff |g(x) −g(x′)| ≤c for all
x, x′ ∈Zn that differ on a single coordinate (i.e., ∥x −x′∥0 = 1).
Lemma 26 (McDiarmid’s inequality). Let g : Zn →R be any c-bounded difference function. Then,
for all t > 0
Pr[|g(x) −µDn(g)| > t] ≤2 exp
−2t2
nc2

.
We will also use the following consequence of Azuma’s inequality.
Lemma 27. Let C1, . . . , Cn be sequence of random variables. Suppose that |Ci| ≤α almost surely
and that γ ≤E[Ci | C1 = c1, . . . , Ci−1 = ci−1] ≤0 for any c1, . . . , ci−1. Then, for every G > 0,
we have Pr
h
nγ −α
√
nG ≤P
i∈[n] Ci ≤α
√
nG
i
≥1 −2 exp (−G/2).
9This is not actually a norm, but often called as such.
14

B.2
Observations on Item-level vs User-level DP
Here we list a couple of trivial observations on the user / sample complexity in the item vs user-level
DP settings.
Observation 28. Let T be any task and γ > 0 be a parameter. Then, for any ε, δ ≥0, and m ∈N, it
holds that
1. nT
m(ε, δ; γ) ≤nT
1 (ε, δ; γ), and
2. nT
m(ε, δ; γ) ≥⌈nT
1 (ε, δ; γ)/m⌉.
Proof. Both parts follow by simple reductions as given below.
1. Let A be an (ε, δ)-item-level DP γ-useful algorithm for T. Then, in the user-level DP algorithm,
each user can simply throw away all-but-one example and run A. Clearly, this algorithm is
(ε, δ)-user-level DP and is γ-useful for T. The user complexity remains the same.
2. Let A be an (ε, δ)-user-level DP γ-useful algorithm for T with user complexity nT
m(ε, δ; γ). Then,
in the item-level DP algorithm, if there are N = m · nT
m(ε, δ; γ) users, we can just run A by
grouping each m examples together into a “super-user”. Clearly, this algorithm is also (ε, δ)-item-
level DP and has user complexity m·nT
m(ε, δ; γ). Thus, we have m·nT
m(ε, δ; γ) ≤nT
1 (ε, δ; γ).
C
Missing Details from Section 3
C.1
DP Implies Perfect Generalization
We start by recalling the definition of perfectly generalizing algorithms.
Definition 29 (Perfect generalization [CLN+16, BF16]). For β, ε, δ > 0, an algorithm A : Zn →O
is said to be (β, ε, δ)-perfectly generalizing iff, for any distribution D (over Z), there exists a
distribution SIMD such that
Pr
x∼Dn[A(x) ≈ε,δ SIMD] ≥1 −β.
It is known that perfect generalization and sample perfect generalization (Definition 12) are equivalent:
Lemma 30 ([CLN+16, BGH+23]). Any (β, ε, δ)-perfectly generalizing algorithm is also (β, 2ε, 3δ)-
sample perfectly generalizing.
Any (β, ε, δ)-sample perfectly generalizing algorithm is also
(√β, ε, δ + √β)-perfectly generalizing; furthermore, this holds even when we set SIMD to be
the distribution of A(x) where x ∼Dm.
Thus, by plugging the above into Theorem 13, we immediately arrive at the following theorem, which
shows that any (ε, δ)-DP algorithm is (β, eO(ε√n), O(nδ))-perfectly generalizing. This resolves an
open question of [CLN+16].
Theorem 31 (DP implies perfect generalization). Suppose that A is an (ε, δ)-item-level DP algorithm.
Further, assume that ε, δ, β, and ε
p
n log(1/βδ) are sufficiently small. Then, A is (β, ε′, δ′)-perfectly
generalizing, where ε′ ≤O(ε
p
n log(1/βδ)) and δ′ = O(nδ).
Furthermore, this holds even when we set SIMD to be the distribution of A(x) where x ∼Dn.
We remark that Bun et al. [BGH+23] showed that any (ε, δ)-DP algorithm can be “repackaged” to an
(β, ε′, δ′)-perfectly generalizing algorithm with similar parameters as our theorem above. However,
unlike our result, their proof does not show that the original algorithm is perfectly generalizing.
C.2
Achieving the Same ε via Subsampling: Proof of Theorem 11
Proof of Theorem 11. Let ε′′ =
ε2
log(1/δ)√
m log(m/δ).
We claim that n1 (ε′′, δ′) ≤n1(ε, δ′) ·
O(ε/ε′′); the lemma follows from this claim by simply plugging this bound into Theorem 10.
To see that this claim holds, by definition of n1, there exists an (ε, δ′)-item-level DP algorithm A0
that is γ-useful for T with sample complexity n1(ε, δ′). Let N ∈N be a smallest number such that
(eε −1) n1(ε, δ′)
N
≤eε′′ −1.
15

Notice that we have N ≤n1(ε, δ′) · O(ε/ε′′). Let A be an algorithm on N samples defined
as follows: randomly sample (without replacement) n1(ε, δ′) out of N samples, then run A0 on
this subsample. By the amplification-by-subsampling theorem (Theorem 7), we have that A is
(ε′′, δ′)-item-level DP. It is also obvious that the output distribution of this algorithm is the same
as that of A0 on n1(ε, δ′) examples, and this algorithm is thus also γ-useful for T. Thus, we have
n1 (ε′′, δ′) ≤N ≤n1(ε, δ′) · O(ε/ε′′) as desired.
C.3
Proof of Lemma 14
C.3.1
“Robust” McDiarmid Inequality for Almost-Multiplicative-Lipschitz function
We derive the following concentration inequality, which may be viewed as a strengthened McDi-
armid’s inequality (Lemma 26) for the multiplicative-Lipschitz case. To state this, let us define an
additional notation: for x ∈Zn, ˜xi ∈Z, we write ˜x(i) to denote x but with xi replaced by ˜xi.
Lemma 32. Let f : Zn →R≥0 be any function and D any distribution on Z. Assume that ε, δ > 0,
and ε
p
n log(1/δ) > 0 are sufficiently small. Then, for ε′ = O(ε
p
n log(1/δ)), we have
Ex,x′∼Dn[[f(x) −eε′ · f(x′)]+] ≤O(µDn(f) · δ) + O
 n
X
i=1
Ex,˜x∼Dn
h
f(x) −eε · f(˜x(i))
i
+
!
.
When the second term in the RHS (i.e., O
Pn
i=1 Ex,˜x∼Dn 
f(x) −eε · f(˜x(i))

+

) is zero, our
lemma is (essentially) the same as McDiarmid’s inequality for multiplicative-Lipschitz functions, as
proved, e.g., in [CLN+16]. However, our bound is more robust, in the sense that it can handle the
case where the multiplicative-Lipschitzness condition fails sometimes; this is crucial for the proof of
Lemma 14 as we only assume approximate-DP.
We note that, while there are (additive) McDiarmid’s inequalities that does not require Lipschitzness
everywhere [Kut02, War16], we are not aware of any version that works for us due to the regime
of parameters we are in and the assumptions we have. This is why we prove Lemma 32 from first
principles. The remainder of this subsection is devoted to the proof of Lemma 32.
Scalar Random Variable Adjustment for Bounded Ratio.
For the rest of this section, we write
z ≈ε z′ where z, z′ ∈R≥0 to indicate that z ∈[e−εz′, eεz′]. We extend the notion similarly to
R≥0-valued random variables: for a R≥0-valued random variable Z and z ∈R≥0, we write Z ≈ε z′
if z ≈ε z′ for each z ∈supp(Z). Furthermore, for (possibly dependent) R≥0-valued random
variables Z, Z′, we write Z ≈ε Z′ to denote z ≈ε z′ for all (z, z′) ∈supp((Z, Z′)).
Suppose Z is a R≥0-valued random variable such that any two values z, ˜z ∈supp(Z) satisfies
z ≈ε ˜z. Then, we also immediately have that their mean µ is in this range and thus Z ≈ε µ. We start
by showing a “robust” version of this statement, which asserts that, even if the former condition fails
sometimes, we can still “move” the random variable a little bit so that the second condition holds
(albeit with weaker 2ε bound). The exact statement is presented below.
Lemma 33. Let ε, µ∗≥0. Let Z be any R≥0-valued random variable and µ := E[Z]. Then, there
exists a random variable Z′, which is a post-processing of Z, such that
(i) Z′ ≈2ε µ∗.
(ii) E[Z′] = µ∗.
(iii) E(Z,Z′)[|Z −Z′|] ≤|µ −µ∗| + 2(1 + e−ε) · EZ, ˜
Z
h
Z −eε · ˜Z
i
+

, where (Z, Z′) is the
canonical coupling between Z, Z′ (from post-processing) and ˜Z is an i.i.d. copy of Z.
Proof. Let ℓ= e−ε · µ, r = eε · µ. We start by defining ˆZ := clipℓ,r(Z). This also gives a canonical
coupling between ˆZ and Z, which then yields
E(Z, ˆ
Z)[|Z −ˆZ|] = EZ

|Z −clipℓ,r(Z)|

= EZ [[Z −r]+ + [ℓ−Z]+]
= EZ

[Z −eε · µ]+

+ EZ
h
e−ε · µ −Z

+
i
16

= EZ
h
Z −eε · E ˜
Z[ ˜Z]
i
+

+ EZ
h
e−ε · E ˜
Z[ ˜Z] −Z
i
+

≤EZ, ˜
Z
h
Z −eε · ˜Z
i
+

+ EZ, ˜
Z
h
e−ε · ˜Z −Z
i
+

= (1 + e−ε) · EZ, ˜
Z
h
Z −eε ˜Z
i
+

,
(2)
where the inequality follows from the convexity of [·]+.
Define ˆµ := E ˆ
Z[ ˆZ]. Note that we have
|ˆµ −µ| ≤E(Z, ˆ
Z)[|Z −ˆZ|].
(3)
Next, consider two cases based on µ, µ∗.
• Case I: µ > eε · µ∗or µ < e−ε · µ∗. We assume w.l.o.g. that µ > eε · µ∗; the case µ < e−ε · µ∗
can be argued similarly. In this case, simply set Z′ = µ∗always. Obviously, items (i) and (ii) hold.
Moreover, we have
E(Z,Z′)[|Z −Z′|] = EZ[|Z −µ∗|] ≤E(Z, ˆ
Z)[| ˆZ −µ∗| + | ˆZ −Z|]
(⋆)
= E ˆ
Z[ ˆZ −µ∗] + E(Z, ˆ
Z)[| ˆZ −Z|]
= ˆµ −µ∗+ E(Z, ˆ
Z)[| ˆZ −Z|]
(3)
≤µ −µ∗+ 2E(Z, ˆ
Z)[| ˆZ −Z|],
where (⋆) follows from ˆZ ≥e−εµ ≥µ∗. Combining the above inequality with (2) yields the
desired bound in item (iii).
• Case II: µ∗≈ε µ. In this case, we already have supp( ˆZ) ⊆[e−ε · µ, eε · µ] ⊆[e−2εµ∗, e2εµ∗].
We assume w.l.o.g. that ˆµ ≥µ∗; the case ˆµ ≤µ∗can be handled similarly. In this case, let
f : [ℓ, r] →R be defined by f(τ) := E ˆ
Z[clipℓ,τ( ˆZ)]. Notice that f is continuous, f(ℓ) = ℓ≤µ∗
and f(r) = ˆµ ≥µ∗. Thus, there exists τ ∗such that f(τ ∗) = µ∗. We then define Z′ by
Z′ := clipℓ,τ ∗( ˆZ). This immediately satisfies items (i) and (ii). Furthermore, this gives a natural
coupling between Z′, ˆZ (and also Z) such that
E( ˆ
Z,Z′)[| ˆZ −Z′|] = E( ˆ
Z,Z′)[ ˆZ −Z′] = ˆµ −µ∗≤|µ −µ∗| + |ˆµ −µ|.
Combining this with (2) and (3) yields the desired bound in item (iii).
Concentration for Bounded-Ratio Martingales.
Since we will be applying Lemma 27 on the
logarithm of the ratios of random variables, it is crucial to bound its expectation. We present a bound
below. Note that similar statements have been used before in DP literature; we present the (simple)
proof here for completeness.
Lemma 34. Let Z be any R>0-valued random variable such that E[Z] = 1. For any sufficiently
small ε > 0, if Z ≈ε 1, then −8ε2 ≤E [ln Z] ≤0.
Proof. By the concavity of ln(·), we get E [ln Z] ≤ln E[Z] = 0.
Note also that for ε ≤1, we have ln(x) ≥x −1 −2(x −1)2 for all x ∈[e−ε, eε]. This implies that
E[ln Z] ≥E[Z −1 −2(Z −1)2] = −2E[(Z −1)2] ≥−2(eε −1)2 ≥−8ε2.
We are now ready to give a concentration inequality for martingales with bounded ratios. Note that
the lemma below is stated in an “expected deviation from [e−ε′ · µ, eε′ · µ]” form since it will be
more convenient to use in a subsequent step.
Lemma 35. Assume that ε, δ > 0, and ε
p
n log(1/δ) > 0 are sufficiently small. Let Y1, . . . , Yn be a
R≥0-valued martingale such that Yi ≈ε Yi−1 almost surely, and µ := E[Y1]. Then, we have
E[[Yn −eε′ · µ]+ + [µ −eε′ · Yn]+] ≤O(µ · δ),
where ε′ = O(ε
p
n log(1/δ)).
17

Proof. Let τ = 100 log(1/δ). Consider C1, . . . , Cn defined by Ci := ln(Yi/Yi−1), where we use
the convention Y0 := µ. By our assumption, we have |Ci| ≤ε. Furthermore, Lemma 34 implies
that −8ε2 ≤E[Yi | Yi−1 = yi−1, . . . , Y1 = y1] ≤0 for all yi−1, . . . , y1. Let γ := −8ε2. Applying
Lemma 27, the following holds for all G > 0:
Pr

nγ −ε
√
nG ≤
X
i∈[k]
Ci ≤ε
√
nG

≥1 −2 exp (−G/2) .
This is equivalent to
Pr
h
enγ−ε
√
nG · µ ≤Yk ≤eε
√
nG · µ
i
≥1 −2 exp (−G/2) .
(4)
Thus, we have
E
h
[µ −eε√n·τ · Yn]+
i
≤µ · Pr[Yn ≤e−ε√n·τ · µ] ≤µ · Pr[Yn ≤enγ−ε√n·τ/2µ]
(4)
≤µ · δ,
where the second inequality follows from the assumption that ε
p
n log(1/δ) is sufficiently small.
Furthermore, we have
E
h
[Yn −eε√n·τ · µ]+
i
=
Z ∞
0
Pr[Yn −eε√n·τ · µ ≥y]dy
= µ ·
Z ∞
0
Pr[Yn −eε√n·τ · µ ≥µ · y]dy
= µ ·
Z ∞
eε√n·τ Pr[Yn ≥µ · y] dy
= µ ·
Z ∞
τ
Pr[Yn ≥eε
√
nG · µ] ·
√nε
2
√
G
exp(ε
√
Gn) dG
(4)
≤µ ·
Z ∞
τ
2 exp(−G/2) ·
√nε
2
√
G
exp(ε
√
Gn) dG
≤µ ·
Z ∞
τ
2 exp(−G/4) dG
= µ · 8 exp(−τ/4)
≤µ · δ,
where the second and third inequalities are from our choice of τ.
Combining the previous two inequalities yields the desired bound for ε′ = ε√nτ.
Putting Things Together: Proof of Lemma 32.
Our overall strategy is similar to the proof for the
always-bounded case, which is to set up a martingale from suffix averages and then apply Lemma 35.
The main modification is that, instead of using this argument directly on f (and its suffix averages),
we use it on a different function, which is the result of applying Lemma 33 to f.
Proof of Lemma 32. For convenience, let µ := µDn(f). Let us first extend the function f to include
prefixes (i.e., f : Z≤n →R≥0) naturally as follows:
f(x1, . . . , xi) = EXi+1,...,Xn∼D[f(x1, . . . , xi, Xi+1, . . . , Xn)].
Next, we construct a function g : Z≤n →R≥0 as follows:
• g(∅) = f(∅) (which is equal to µ)
• For i = 1, . . . , n:
– For all (x1, . . . , xi−1) ∈Zi−1:
* Apply Lemma 33 with µ∗:= g(x1, . . . , xi−1) and Z = f(x1, . . . , xi−1, Xi), where
Xi ∼D to get a random variable Z′.
* The coupling between (Z, Z′) is naturally also a coupling between (xi, Z′) for xi ∈Z.
Let g(x1, . . . , xi) = Z′ based on this coupling for all xi ∈Z.
18

For brevity, let ρ = 2(1 + e−ε). We have
Ex∼Dn[|g(x) −f(x)|]
= Ex∼DnE˜xn∼Dn[|g(x≤n−1, ˜xn) −f(x≤n−1, ˜xn)|]
≤Ex∼Dn 
|g(x≤n−1) −f(x≤n−1)| + ρ · E˜xn,˜x′n∼D [f(x≤n−1, ˜xn) −eε · f(x≤n−1, ˜x′
n)]+

= Ex≤n−1∼Dn−1 [|g(x≤n−1) −f(x≤n−1)|] + ρ · Ex,x′∼Dn [f(x≤n−1, xn) −eε · f(x≤n−1, x′
n)]+ ,
where the inequality is from Lemma 33(iii).
By repeatedly applying the above argument (to the first term), we arrive at
Ex∼Dn[|g(x) −f(x)|] ≤ρ ·
n
X
i=1
Ex,x′∼Dn [f(x≤i−1, xi) −eε · f(x≤i−1, x′
i)]+ .
Recall from the definition that f(x≤i−1, xi) = Ex>i∼Dn−i[f(x≤i−1, xi, x>i)] and f(x≤i−1, x′
i) =
Ex>i∼Dn−i[f(x≤i−1, x′
i, x>i)]. Thus, by the convexity of [·]+, we further have
Ex∼Dn[|g(x) −f(x)|] ≤ρ ·
n
X
i=1
Ex,˜x∼Dn
h
f(x) −eε · f(˜x(i))
i
+ .
(5)
Next, let Y0, Y1, . . . , Yn be a sequence of random variables defined by sampling x1, . . . , xn ∼Dn and
letting Yi := g(x1, . . . , xi). By Lemma 33(ii), we have that Y1, . . . , Yn is a martingale. Lemma 33(i)
further implies that Yi−1 ≈2ε Yi. Thus, we may apply Lemma 35 to arrive at
E[[Yn −eε′/2 · µ]+ + [µ −eε′/2 · Yn]+] ≤O(µδ),
for ε′ = O(ε
p
n log(1/δ)). Note that this is equivalent to
Ex∼Dn[[g(x) −eε′/2 · µ]+ + [µ −eε′/2 · g(x)]+] ≤O(µδ).
(6)
We can now derive the bound on the desired quantity as follows:
Ex,x′∼Dn
h
[f(x) −eε′f(x′)]+
i
= Ex,x′∼Dn
h
[(f(x) −g(x)) + (g(x) −eε′/2µ) + eε′/2(µ −eε′/2g(x′)) + eε′(g(x′) −f(x′))]+
i
≤E[f(x) −g(x)]+ + E[g(x) −eε′/2µ]+ + eε′/2E[µ −eε′/2g(x′)]+ + E[g(x′) −f(x′)]+
(5),(6)
≤
O(µδ) + O
 n
X
i=1
Ex,˜x∼Dn
h
f(x) −eε · f(˜x(i))
i
+
!
.
C.3.2
Putting Things Together: Proof of Lemma 14
Lemma 14 is now a simple consequence of Lemma 32.
Proof of Lemma 14. For every o ∈O and x ∈Zn, let f o(x) := Pr[A(x) = o]. Applying
Lemma 32 to f o, we get
Ex,x′∼Dn[[f o(x) −eε′ · f o(x′)]+]
≤O(µDn(f o) · δ) + O
 n
X
i=1
Ex,˜x∼Dn
h
f o(x) −eε · f o(˜x(i))
i
+
!
,
where ε′ = O(ε
p
n log(1/δ)).
Summing this up over all o ∈O, we get
Ex,x′∼Dn[dε′(A(x) ∥A(x′))] ≤O(δ) + O
 n
X
i=1
Ex,˜x∼Dn
h
dε(A(x) ∥A(˜x(i)))
i!
≤O(nδ),
where the second inequality follows from the assumption that A is (ε, δ)-item-level DP, which implies
dε(A(x) ∥A(˜x(i))) ≤δ.
19

C.4
Privacy Analysis of Algorithm 1
Here we prove the privacy guarantee (Lemma 18) of Algorithm 1. This mostly follows the proof for
the analogous algorithm in [GKK+23].
We start with the following observation.
Observation 36. For user-level neighboring datasets x, x′, and all r1 ∈{0, . . . , n −1}, if
X r1
stable(x′) ̸= ∅, then X r1+1
stable(x) ̸= ∅.
Proof. Let i ∈[n] be such that x−i = x′
−i, and let x′
−S′ be an element of X r1
stable(x′). Select S such
that |S| = r1 + 1 and S ⊇S′ ∪{i}. It is obvious from definition that xS ∈X r1+1
stable(x).
Proof of Lemma 18. Let x, x′ be user-level neighboring datasets, and let A be a shorthand for
DelStabε,δ,A. First, we follow the proof of [GKK+23, Theorem 3.3]:
Pr[A(x) =⊥] = P2κ
r1=0 1[A(x) =⊥| R1 = r1] · Pr[R1 = r1]
= P2κ
r1=0 1[X r1
stable(x) = ∅] · Pr[R1 = r1]
≤Pr[R1 = 0] + P2κ
r1=1 1[X r1
stable(x) = ∅] · Pr[R1 = r1]
≤δ + P2κ
r1=1 1[X r1
stable(x) = ∅] · eε · Pr[R1 = r1 −1]
≤δ + P2κ
r1=1 1[X r1−1
stable(x′) = ∅] · eε · Pr[R1 = r1 −1]
≤δ + eε · P2κ
r1=0 1[A(x′) =⊥| R1 = r1] · Pr[R1 = r1]
= δ + eε · Pr[A(x′) =⊥],
(7)
and for any U0 ⊆O, we have
Pr[A(x) ∈U0] ≤P2κ
r1=0 Pr[A(x) ∈U0 | R1 = r1] · Pr[R1 = r1]
= P2κ−1
r1=0 Pr[A(x) ∈U0 | R1 = r1] · Pr[R1 = r1] + Pr[R1 = 2κ]
≤δ + eε · P2κ−1
r1=0 Pr[A(x) ∈U0 | R1 = r1] · Pr[R1 = r1 + 1],
(8)
To bound the term Pr[A(x) ∈U0 | R1 = r1] for r1 < 2κ, observe that if it is non-zero, then it must
be that A(x) ̸=⊥or equivalently that X r1
stable(x) ̸= ∅. In this case, we have10
Pr[A(x) ∈U0 | R1 = r1] = ES∼X r1
stable(x),J∼(
S
n−4κ) [Pr[A(xJ) ∈U0]] .
Since X r1
stable(x) ̸= ∅, Observation 36 also implies that11 X r1+1
stable(x′) ̸= ∅. Thus, we have
Pr[A(x′) ∈U0 | R1 = r1 + 1] = ES′∼X r1+1
stable(x′),J′∼(
S′
n−4κ) [Pr[A(xJ′) ∈U0]] .
Consider an arbitrary pair S ∈X r1
stable(x), S′ ∈X r1+1
stable(x′). Let i ∈[n] be such that x−i = x′
−i.
Note that |(S ∩S
′) ∖{i}| ≥n −4κ. Let J∗be any subset of (S ∩S
′) of size n −4κ. From
S ∈X r1
stable(x), we have
Pr[A(xJ) ∈U0] ≤eε Pr[A(xJ∗) ∈U0] + δ,
for all J ∈
 S
n−4κ

, and, from S′ ∈X r1+1
stable(x′), we have
Pr[A(xJ∗) ∈U0] ≤eε Pr[A(xJ′) ∈U0] + δ,
for all J′ ∈
 S
′
n−4κ

.
Therefore, by arbitrary coupling of S, S′, we can conclude that
Pr[A(x) ∈U0 | R1 = r1] ≤e2ε Pr[A(x′) ∈U0 | R1 = r1 + 1] + (eε + 1)δ.
10Here we write S := [n] ∖S.
11Again we write S
′ := [n] ∖S′.
20

Plugging this back to (8), we get
Pr[A(x) ∈U0]
≤δ + eε ·
2κ−1
X
r1=0
 e2ε Pr[A(x′) ∈U0 | R1 = r1 + 1] + (eε + 1)δ

· Pr[R1 = r1 + 1]
≤(e2ε + eε + 1)δ + e3ε · P2κ
r1=0 Pr[A(x′) ∈U0 | R1 = r1] · Pr[R1 = r1]
= (e2ε + eε + 1)δ + e3ε · Pr[A(x′) ∈U0].
(9)
Now, consider any set U ⊆O ∪{⊥} of outcomes. Let UO = U ∩O and U⊥= U ∩{⊥}. Then, we
have
Pr[A(x) ∈U] = Pr[A(x) ∈UO] + Pr[A(x) ∈U⊥]
(9),(7)
≤
 (e2ε + eε + 1)δ + e3ε · Pr[A(x′) ∈UO]

+
 δ + eε · Pr[A(x′) = U⊥]

≤(e2ε + eε + 2)δ + e3ε Pr[A(x′) ∈U]
≤δ + eε · Pr[A(x′) ∈U].
Therefore, the algorithm is (ε, δ)-user-level DP as desired.
D
Missing Details from Section 4
Below we give the proof of Lemma 22.
Proof of Lemma 22. Consider any h ∈{0, 1}X . Since zi’s are drawn independently from D, we
have that 1[zi is not realizable by h]’s are independent Bernoulli random variables with success
probability
ph = 1 −(1 −errD(h))m.
Consider the two cases:
• If errD(h) ≤0.01α, then we have
ph ≤1 −(1 −0.01α)m ≤0.01αm.
Thus, applying Lemma 25, we can conclude that
Pr[scrh(z) ≥0.05αnm] ≤exp (−Θ (nαm)) = exp (−Θ (κ · size(P))) .
Thus, when we select κ to be sufficiently large, we have
Pr[scrh(z) ≥0.05αnm] ≤exp(−10 size(P)) ≤
0.01
exp(size(P)) ≤0.01
|H| .
• If errD(h) > α, then we have
ph ≥1 −(1 −α)m ≥1 −e−αm ≥0.2αm,
where in the second inequality we use the fact that αm ≤1.
Again, applying Lemma 25, we can conclude that
Pr[scrh(z) ≤0.2αnm] ≤exp (−Θ (nαm)) = exp (−Θ (κ · size(P))) .
Thus, when we select κ to be sufficiently large, we have
Pr[scrh(z) ≤0.2αnm] ≤exp(−10 size(P)) ≤
0.01
exp(size(P)) ≤0.01
|H| .
Finally, taking a union bound over all h ∈H yields the desired result.
It is also worth noting that the scoring function can be written in the form:
scrh(z) :=
X
i∈[n]
clip0,1 (|{(x, y) ∈zi | h(x) ̸= y}|) ,
which is similar to what we present below for other tasks.
Finally, we remark that the standard techniques can boost the success probability from 2/3 to an
arbitrary probability 1 −β, with a multiplicative factor of log(1/β) cost.
21

E
Additional Results for Pure-DP
E.1
Algorithm: Pairwise Score Exponential Mechanism
In this section, we give a user-level DP version of pairwise score exponential mechanism
(EM) [BKSW19]. In the proceeding subsections, we will show how to apply this in multiple
settings, including agnostic learning, private hypothesis selection, and distribution learning.
Suppose there is a candidate set H. Furthermore, for every pair H, H′ ∈H of candidates, there is a
“comparison” function ψH,H′ : Z →[−1, 1]. For every pair H, H′ of candidates and any x ∈Z∗, let
pscrψ
H,H′(x) =
X
x∈x
ψH,H′(x).
We then define scrψ for each candidate H as
scrψ
H(x) :=
max
H′∈H∖{H} pscrψ
H,H′(x).
Traditionally, the item-level DP algorithm for pairwise scoring function is to use EM on the above
scoring function scrψ
H (where x denote the entire input) [BKSW19]; it is not hard to see that the scrψ
H
has (item-level) sensitivity of at most 2.
User-Level DP Algorithm.
We now describe our user-level DP algorithm ClippedPairwiseEM.
The algorithm computes:
pscrψ,τ
H,H′(x) :=
X
i∈[n]
clip−τ,τ

pscrψ
H,H′(xi)

,
scrψ,τ
H (x) :=
max
H′∈H∖{H} pscrψ,τ
H,H′(x).
Then, it simply runs EM using the score scrψ,τ
H
with sensitivity ∆= 2τ.
To state the guarantee of the algorithm, we need several additional distributional-based definitions of
the score:
pscrψ
H,H′(D) := Ex∼D [ψH,H′(x)] ,
scrψ
H(D) :=
max
H′∈H∖{H} pscrψ
H,H′(D).
Note that these distributional scores are normalized, so they are in [−1, 1].
The guarantee of our algorithm can now be stated as follows:
Theorem 37. Let ε, α, β ∈(0, 1/2]. Assume further that there exists H∗such that scrψ
H∗(D) ≤0.1α.
There exists an ε-DP algorithm that with probability 1 −β (over the randomness of the algorithm
and the input x ∼(Dm)n) outputs H ∈H such that scrψ
H(D) ≤0.5α as long as
n ≥˜Θ
log(|H|/β)
ε
+ log(|H|/β)
αε√m
+ log(|H|/β)
α2m

.
Proof. We use ClippedPairwiseEM with parameter τ = C

αm +
p
m log (1/α)

and assume that
n ≥C′ 
τ·log(|H|/β)
εαm
+ log(|H|/β)
α2m

= ˜Θ

log(|H|/β)
ε
+ log(|H|/β)
αε√m
+ log(|H|/β)
α2m

, where C, C′ are
sufficiently large constants. The privacy guarantee follows directly from that of EM, since the
(user-level) sensitivity is at most ∆(due to clipping).
To analyze the utility, we will consider following three events:
• E1: scrψ,τ
H (x) ≤scrψ,τ
H∗(x) + 0.1αnm.
• E2: for all H, H′ ∈H: if pscrψ
H,H′(D) ≤0.1α, then pscrψ
H,H′(x) ≤0.2αnm.
• E3: for all H, H′ ∈H: if pscrψ
H,H′(D) ≥0.5α, then pscrψ
H,H′(x) ≥0.4αnm.
22

When all events hold, E1 implies that scrψ,τ
H (x) ≤scrψ,τ
H∗(x) + 0.1αnm. Observe that E2 implies
that scrψ,τ
H∗(x) ≤0.2αnm. Combining these two inequalities, we get scrψ,τ
H (x) ≤0.3αnm; then, E3
implies that pscrψ
H,H′(D) ≤0.5α for all H′ ∈D. In turn, this means that scrψ
H(D) ≤0.5α.
Therefore, it suffices to show that each of Pr[E1], Pr[E2], Pr[E3] is at least 1 −β/3.
Bounding Pr[E1]. To bound the probability of E1, recall from Theorem 9 that, with probability
1 −β/3, the guarantee of EM ensures that
scrψ,τ
H (x) ≥scrψ,τ
H∗(x) + O
log(|H|/β)
ε

· ∆.
From our assumption on n, when C′ is sufficiently large, then we have O

log(|H|/β)
ε

· ∆=
O

log(|H|/β)
ε
· τ

≤0.1αnm as desired.
Bounding Pr[E2] and Pr[E3]. Since the proofs for both cases are analogous, we only show the full
argument for Pr[E3]. Let us fix H, H′ ∈H such that pscrψ
H,H′(D) ≥0.5α. We may assume w.l.o.g.
that12 pscrψ
H,H′(D) = 0.5α := µ.
Notice that pscrψ,τ
H,H′(x) is a 2-bounded difference function. As a result, McDiarmid’s inequality
(Lemma 26) implies that
Pr
x∼Dnm[|pscrψ,τ
H,H′(x) −µDnm(pscrψ,τ
H,H′)| > 0.05αnm] ≤2 exp
 −0.025α2nm

≤
β
3|H|2 , (10)
where the second inequality is due to our choice of n when C′ is sufficiently large.
To compute µDnm(pscrψ,τ
H,H′), observe further that
µDnm(pscrψ,τ
H,H′) = n · µDm(clip−τ,τ ◦pscrψ
H,H′).
(11)
Now observe once again that pscrψ
H,H′ : Zm →R is also a 2-bounded difference function and
µDm(pscrψ
H,H′) = µm. Therefore, McDiarmid’s inequality (Lemma 26) yields
Pr
x∼Dm[pscrψ
H,H′(x) > τ] ≤2 exp
 −0.5(τ −µm)2/m

≤10−4α2,
(12)
where the second inequality is due to our choice of τ when C is sufficiently large.
Finally, note that
µDm(clip−τ,τ ◦pscrψ
H,H′)
= Ex∼Dm[clip−τ,τ(pscrψ
H,H′(x))]
≥Ex∼Dm[clip−∞,τ(pscrψ
H,H′(x))]
= Ex∼Dm[pscrψ
H,H′(x)] + Ex∼Dm[(τ −pscrψ
H,H′(x)) · 1[pscrψ
H,H′(x) > τ]]
≥µm −Ex∼Dm[pscrψ
H,H′(x) · 1[pscrψ
H,H′(x) > τ]]
≥µm −
q
Ex∼Dm[pscrψ
H,H′(x)2] ·
q
Ex∼Dm[1[pscrψ
H,H′(x) > τ]]
≥µm −
√
m2 ·
r
Pr
x∼Dm[pscrψ
H,H′(x) > τ]
(12)
≥µm −0.01αm = 0.49αm,
(13)
where the third-to-last inequality follows from Cauchy–Schwarz.
12Otherwise, we may keep increasing ψH,H′(z) for different values of z until pscrψ
H,H′(D) = 0.5α; this
operation does not decrease the probability that pscrψ
H,H′(x) < 0.4αnm.
23

Combining (10), (11), and (13), we can conclude that
Pr
x∼Dnm[pscrψ,τ
H,H′(x) < 0.4αnm] ≤
β
3|H|2 .
Taking a union bound over H, H′ ∈H yields Pr[E3] ≥1 −β/3 as desired.
E.2
Lower Bound: User-level DP Fano’s Inequality
As we often demonstrate below that our bounds are (nearly) tight, it will be useful to have a generic
method for providing such a lower bound. Here we observe that it is simple to extend the DP Fano’s
inequality of Acharya et al. [ASZ21] to the user-level DP setting. We start by recalling Acharya et
al.’s (item-level) DP Fano’s inequality13:
Theorem 38 (Item-Level DP Fano’s Inequality [ASZ21]). Let T be any task. Suppose that there
exist D1, . . . , DW such that, for all distinct i, j ∈[W],
• ΨT(Di) ∩ΨT(Dj) = ∅,
• dKL(Di, Dj) ≤β, and
• dtv(Di, Dj) ≤γ.
Then, nT
1 (ε, δ = 0) ≥Ω

log W
γε
+ log W
β

.
The bound for the user-level case can be derived as follows.
Lemma 39 (User-Level DP Fano’s Inequality). Let T be any task.
Suppose that there exist
D1, . . . , DW such that, for all distinct i, j ∈[W],
• ΨT(Di) ∩ΨT(Dj) = ∅, and,
• dKL(Di, Dj) ≤β.
Then, nT
m(ε, δ = 0) ≥Ω

log W
ε
+ log W
ε√mβ + log W
mβ

.
Proof. Let D′
i = (Di)m for all i ∈[W]; note that dKL(D′
i ∥D′
j) ≤mβ. By Pinsker’s in-
equality (Lemma 23(i)), dtv(D′
i ∥D′
j) ≤O(√mβ); furthermore, we also have the trivial bound
dtv(D′
i ∥D′
j) ≤1. Finally, define a task T′ by ΨT′(D′
i) := ΨT(Di) for all i ∈[W]. Now, applying
Theorem 38,
nT′
1 (ε, δ = 0) ≥Ω

log W
min{1, √mβ} · ε + log W
mβ

= Ω
log W
ε
+ log W
ε√mβ + log W
mβ

.
Finally, observing that nT
m(ε, δ) = nT′
1 (ε, δ) yields our final bound.
E.3
Applications
Our user-level DP EM with pairwise score given above has a wide variety of applications. We now
give a few examples.
E.3.1
Agnostic PAC Learning
We start with agnostic PAC learning. The setting is exactly the same as in PAC learning (described
in the introduction; see Theorem 4) except that we do not assume that D is realizable by some
concept in C. Instead, the task agn(C; α) seeks an output c : X →{0, 1} such that errD(c) ≤
minc′∈C errD(c′) + α.
Theorem 40. Let C be any concept class with probabilistic representation dimension d (i.e.,
PRDim(C) = d) and let dVC be its VC dimension. Then, for any sufficiently small α, ε > 0
and for all m ∈N,
nagn(C;α)
m
(ε, δ = 0) ≤eO
d
ε +
d
αε√m +
d
α2m

,
13The particular version we use is a slight restatement of [ASZ21, Corollary 4].
24

and
nagn(C;α)
m
(ε, δ = 0) ≥˜Ω
d
ε +
dVC
αε√m + dVC
α2m

.
Observe that our bounds are (up to polylogarithmic factors) the same except for the d-vs-dVC in the
last two terms. We remark that the ratio d/dVC is not always bounded; e.g., for thresholds, dVC = 2
while d = Θ(log |Z|). A more careful argument can show that d in the last two terms in the upper
bound can be replaced by the (maximum) VC dimension of the probabilistic representation (which is
potentially smaller); this actually closes the gap in the particular case of thresholds. However, in the
general case, the VC dimension of C and the VC dimension of its probabilistic represetation are not
necessarily equal. It remains an interesting open question to close this gap.
We now prove Theorem 40. The algorithm is similar to that in the proof of Theorem 4, except that
we use the pairwise scoring function to compare the errors of the two hypotheses.
Proof of Theorem 40. Algorithm. For any two hypotheses c, c′ : X →{0, 1}, let the comparison
function between two concepts be ψc,c′((x, y)) = 1[c(x) = y] −1[c′(x) = y].
Let P denote a (0.01α, 1/4)-PR of C; by Lemma 21, there exists such P with size(P) ≤
eO (d · log(1/α)). Our algorithm works as follows: Sample H ∼P and then run the ε-DP pair-
wise scoring EM (Theorem 37) on candidate set H with the comparison function ψc,c′ defined above.
The privacy guarantee follows from that of Theorem 37.
As for the utility, first observe that
pscrψ
c,c′(D) = errD(c) −errD(c′).
(14)
This means that, if we let h∗∈argminh∈H errD(h), then scrψ
h∗(D) ≤0. Thus, Theorem 37 ensures
that, w.p. 0.99, if n ≥˜Θ

size(P)
ε
+ size(P)
αε√m + size(P)
α2m

= ˜Θ

d
ε +
d
αε√m +
d
α2m

, then the output
h satisfies scrψ
h(D) ≤0.5α. By (14), this also means that errD(h) ≤0.5α + errD(h∗). Finally, by
the guarantee of P, we have that errD(h∗) ≤0.01α + minc′∈C errD(c′) with probability 3/4. By a
union bound, we can conclude that w.p. more than 2/3, we have errD(h) < α + minc′∈C errD(c′).
Lower Bound. The lower bound Ω(d/ε) was shown in [GKM21] (and holds even for realizable
PAC learning). To derive the lower bound Ω

dVC
αε√m + dVC
α2m

, we will use DP Fano’s inequality
(Lemma 39). To do so, recall that C having VC dimension dVC means that there exist {x1, . . . , xdVC}
that is shattered by Z. From Theorem 24, there exist u1, . . . , uW ∈{0, 1}dVC where W = 2Ω(dVC)
such that ∥ui −uj∥0 ≥κ · dVC for some constant κ. For any sufficiently small α < κ/8, we can
define the distribution Di for i ∈[W] by
Di((xℓ, y)) =
1
2dVC
·

1 + 4α
κ · (21[(ui)ℓ= y] −1)

,
for all ℓ∈[W] and y ∈{0, 1}. Next, notice that, for any i ∈[W], we may select ci such that
errD(ci) = 1
2
 1 −4α
κ

.
Furthermore, for any i ̸= j ∈[W] and any hypothesis h : X →{0, 1}, we have
errDi(h) + errDj(h) ≥

1 −4α
κ

+ 4α
κ · κ · dVC
dVC
=

1 −4α
κ

+ 4α.
This means that errDi(h) > 1
2
 1 −4α
κ

+ 2α or errDj(h) > 1
2
 1 −4α
κ

+ 2α. In other words, we
have Ψagn(C;α)(Di) ∩Ψagn(C;α)(Dj) = ∅. Finally, we have
dKL(Di ∥Dj) ≤dχ2(Di ∥Dj)
=
X
ℓ∈[dVC],y∈{0,1}
(Di((x, y)) −Dj((x, y)))2
Dj((x, y))
≤
X
ℓ∈[dVC],y∈{0,1}
O(α/dVC)2
1/4dVC
25

=
X
ℓ∈[dVC]
O(α2/dVC)
≤O(α2).
Plugging this into Lemma 39, we get that nagn(C;α)
m
≥Ω

dVC
εα√m + dVC
mα2

as desired.
E.3.2
Private Hypothesis Selection
In Hypothesis Selection problem, we are given a set P of hypotheses, where each P ∈P is a
distribution over some domain Z. The goal is, for the underlying distribution D, to output P∗∈P
that is the closest (in TV distance) to D. We state below the guarantee one can get from our approach:
Theorem 41. Let α, β ∈(0, 0.1). Then, for any ε > 0, there exists an ε-user-level DP algorithm for
Hypothesis Selection that, when each user has m samples and minP′∈P dtv(P′, D) ≤0.1α, with
probability 1 −β, outputs P ∈P such that dtv(P, D) ≤α as long as
n ≥˜Θ
log(|P|/β)
ε
+ log(|P|/β)
αε√m
+ log(|P|/β)
α2m

.
For m = 1, the above theorem essentially match the item-level DP sample complexity bound
from [BKSW19]. The proof also proceeds similarly as theirs, except that we use the user-level DP
EM rather than the item-level DP one.
Proof Theorem 41. We use the so-called Scheffé score similar to [BKSW19]: For every P, P′ ∈P,
we define WP,P′ as the set {z ∈Z | P(z) > P′(z)} and let
ψP,P′(z) = P(WP,P′) −1[z ∈WP,P′].
We then use the ε-user-level DP pairwise scoring EM (Theorem 37). The privacy guarantee follows
immediately from the theorem.
For the utility analysis, let P∗∈argminP∈Pdtv(P, D).
Recall from our assumption that
dtv(P∗, D) ≤0.1α. Furthermore, observe that
pscrψ
P∗,P(D) = P∗(WP∗,P) −D(WP∗,P) ≤dtv(D, P∗) ≤0.1α.
Moreover, for every P ∈P, we have
pscrψ
P,P∗(D) = P(WP,P∗) −D(WP,P∗)
= (P(WP,P∗) −P∗(WP,P∗)) −(D(WP,P∗) −P∗(WP,P∗))
= dtv(P, P∗) −(D(WP,P∗) −P∗(WP,P∗))
≥dtv(P, P∗) −dtv(P∗, D)
≥dtv(P, P∗) −0.1α.
Thus, applying the utility guarantee from Theorem 37, we can conclude that, with probability 1 −β,
the algorithm outputs P such that dtv(P, D) ≤0.5α + 0.1α < α as desired.
E.3.3
Distribution Learning
Private hypothesis selection has a number of applications, arguably the most prominent one being
distribution learning. In distribution learning, there is a family P of distributions. The underlying
distribution D comes from this family. The task dlearn(P; α) is to output a distribution Q such that
dtv(Q, D) ≤α, where α > 0 denotes the accuracy parameter.
For convenient, let us defined packing and covering of a family of distributions:
• A family Q of distributions is an α-cover of a family P of distributions under distance
measure d iff, for all P ∈P, there exists Q ∈Q such that d(Q, P) ≤α.
• A family Q of distributions is an α-packing of a family P of distributions under distance
measure d iff, Q ⊆P and for all distinct Q, Q′ ∈Q, we have d(Q, Q′) ≥α.
26

The size of the cover / packing is defined as size(Q) := log(|Q|). The diameter of Q under distance
d is defined as maxQ,Q′∈Q d(Q, Q′).
The following convenient lemma directly follows from Theorem 41 and Lemma 39.
Theorem 42 (Distribution Learning—Arbitrary Family). Let P be any familiy of distributions, and
let α, β, ε > 0 be sufficiently small. If there exists an 0.1α-cover of P under the TV distance of size
L, then
ndlearn(P;α)
m
(ε, δ = 0) ≤eO
L
ε +
L
αε√m +
L
α2m

.
Furthermore, if there exists a family Q of size L that is an 3α-packing of P under the TV distance
and has diameter at most β under the KL-divergence, then
ndlearn(P;α)
m
(ε, δ = 0) ≥˜Ω
L
ε +
L
ε√mβ + L
mβ

.
In theorems below, we abuse the notations ˜Θ, eO and also use them to suppress terms that are
polylogarithmic in 1/α, d, R, and κ.
Discrete Distributions.
First is for the task of discrete distribution learning on domain [k] (denoted
by DDk(α)), which has been studied before in [LSY+20]. In this case, P consists of all distributions
over the domain [k]. We have the following theorem:
Theorem 43 (Distribution Learning—Discrete Distributions). For any sufficiently small α, ε > 0
and for all m ∈N, we have
nDDk(α)
m
(ε, δ = 0) = ˜Θ

k
ε +
k
αε√m +
k
α2m

.
Proof. Upper bound. It is simple to see that there exists an 0.1α-cover of the family of size
O(k · log(k/α)): We simply let the cover contain all distributions whose probability mass at each
point is a multiple of ⌊10k/α⌋. Plugging this into Theorem 42 yields the desired upper bound.
Lower bound. As for the lower bound, we may use a construction similar to that of [ASZ21] (and
also similar to that in the proof of Theorem 40). Assume w.l.o.g. that k is even. From Theorem 24,
there exist u1, . . . , uW ∈{0, 1}k/2 where W = 2Ω(k) such that ∥ui −uj∥0 ≥κ·k for some constant
κ. For any sufficiently small α < κ/16, we can define the distribution Di for i ∈[W] by
Di(2ℓ−1) = 1
2k

1 + 8α
κ · 1[(ui)ℓ= 0]

Di(2ℓ) = 1
2k

1 −8α
κ · 1[(ui)ℓ= 0]

,
for all ℓ∈[k/2]. By a similar calculation as in the proof of Theorem 40, we have that it is an
(3α)-packing under TV distance and its diameter under KL-divergence is at most O(α2). Plugging
this into Theorem 42 then gives the lower bound.
Interestingly, the user complexity matches those achieved via approximate-DP algorithms from
[LSY+20, NME22], except for the first term (i.e., k/ε) whereas the best known (ε, δ)-DP algorithm
of [NME22] works even with log(1/δ)/ε instead. In other words, when m is intermediate, the user
complexity between the pure-DP and approximate-DP cases are the same. Only for large m that
approximate-DP helps.
Product Distributions.
In this case, P is a product distribution over the domain [k]d (denoted by
PD(k, d; α)). We have the following theorem:
Theorem 44 (Distribution Learning—Product Distributions). For any sufficiently small α, ε > 0 and
for all m ∈N, we have
nPD(k,d;α)
m
(ε, δ = 0) = ˜Θ
kd
ε +
kd
αε√m + kd
α2m

.
27

Proof. Upper Bound. Similar to before, we simply take Q to be a 0.1α-cover (under TV distance) of
the product distributions, which is known to have size at most O(kd · log(kd/α)) [BKSW19, Lemma
6.2].
Lower Bound. Acharya et al. [ASZ21, Proof of Theorem 11] showed that there exists a family Q
of product distributions over [k]d that is a (3α)-packing under TV distance and its diameter under
KL-divergence is at most O(α2) of size Ω(kd). Theorem 42 then gives the lower bound.
Gaussian Distributions: Known Covariance.
Next, we consider Gaussian distributions N(µ, Σ),
where µ ∈Rd, Σ ∈Rd×d, under the assumption that ∥µ∥≤R and Σ = I14. Let Gauss(R, d; α)
denote this task, where α is again the (TV) accuracy. For this problem, we have:
Theorem 45 (Distribution Learning—Gaussian Distributions with Known Covariance). For any
sufficiently small α, ε > 0 and for all m ∈N, we have
nGauss(R,d;α)
m
(ε, δ = 0) = ˜Θ
d
ε +
d
αε√m +
d
α2m

.
Proof. Upper Bound. It is known that this family of distribution admits a (0.1α)-cover (under TV
distance) of size O(d · log(dR/α)) [BKSW19, Lemma 6.7]. This, together with Theorem 42, gives
the upper bound.
Lower Bound. Again, Acharya et al. [ASZ21, Proof of Theorem 12] showed that there exists a
family Q of isotropic Gaussian distributions with ∥µ∥≤O(
p
log(1/α)) that is a (3α)-packing
under TV distance and its diameter under KL-divergence is at most O(α2) of size Ω(d). Theorem 42
then gives the lower bound.
We note that this problem has already been (implicitly) studied under user-level approximate-DP
in [NME22],15 who showed that nGauss(R,d;α)
m
(ε, δ > 0) ≤eO

d
αε√m +
d
α2m

. Again, it is perhaps
surprising that our pure-DP bound nearly matches this result, except that we have the extra first term
(i.e., d/ε).
Gaussian Distributions: Unknown Bounded Covariance.
Finally, we consider the case where
the covariance is also unknown but is assumed to satisfied I ⪯Σ ⪯κI. We use Gauss(R, d, κ; α)
to denote this task.
Theorem 46 (Distribution Learning—Gaussian Distributions with Unknown Bounded Covariance).
Let κ > 1 be a constant. For any sufficiently small α, ε > 0 and for all m ∈N, we have
nGauss(R,d,κ;α)
m
(ε, δ = 0) = ˜Θκ
d2
ε +
d2
αε√m +
d2
α2m

.
Proof. Upper Bound. This follows from Theorem 42 and a known upper bound of O(d2 log(dκ/α)+
d · log(dR/α)) on the size of a (0.1α)-cover (under TV distance) of the family [BKSW19, Lemma
6.8].
Lower Bound. Devroye et al. [DMR20] showed16 that, for κ ≥1 + O(α), there exists a 3α-packing
under TV distance whose diameter under the KL-divergence is at most O(α2) of size Ω(d2). This,
together with Theorem 42, gives the lower bound.
We remark that it is simple to see that Gaussian with unbounded mean or unbounded covariance
cannot be learned with pure (even user-level) DP using a finite number of examples.
14Or equivalently that Σ is known; since such an assumption is sufficient to rotate the example to isotropic
positions.
15Actually, Narayanan et al. [NME22] studied the mean estimation problem, but it is not hard to see that an
algorithm for mean estimation also provides an algorithm for learning Gaussian distributions.
16Specifically, this is shown in [DMR20, Proposition 3.1]; their notation “m” denote the number of non-zero
(non-diagonal) entries of the covariance matrix which can be set to Ω(d2) for our purpose.
28

