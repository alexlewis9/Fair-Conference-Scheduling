ChaosBench: A Multi-Channel, Physics-Based
Benchmark for Subseasonal-to-Seasonal Climate
Prediction
Juan Nathaniel1,*, Yongquan Qu1, Tung Nguyen2, Sungduk Yu3,5, Julius Busecke1,4,
Aditya Grover2, Pierre Gentine1
1Columbia University, 2UCLA, 3UCI, 4LDEO, 5 Intel Labs
Abstract
Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for
disaster preparedness and robust decision making amidst climate change. Yet,
forecasting beyond the weather timescale is challenging because it deals with
problems other than initial condition, including boundary interaction, butterfly
effect, and our inherent lack of physical understanding. At present, existing
benchmarks tend to have shorter forecasting range of up-to 15 days, do not include
a wide range of operational baselines, and lack physics-based constraints for
explainability. Thus, we propose ChaosBench, a challenging benchmark to extend
the predictability range of data-driven weather emulators to S2S timescale. First,
ChaosBench is comprised of variables beyond the typical surface-atmospheric
ERA5 to also include ocean, ice, and land reanalysis products that span over 45
years to allow for full Earth system emulation that respects boundary conditions.
We also propose physics-based, in addition to deterministic and probabilistic
metrics, to ensure a physically-consistent ensemble that accounts for butterfly
effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from
four national weather agencies as baselines to our data-driven counterpart such
as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we
find methods originally developed for weather-scale applications fail on S2S task:
their performance simply collapse to an unskilled climatology. Nonetheless, we
outline and demonstrate several strategies that can extend the predictability range of
existing weather emulators, including the use of ensembles, robust control of error
propagation, and the use of physics-informed models. Our benchmark, datasets,
and instructions are available at https://leap-stc.github.io/ChaosBench.
1
Introduction
Although critical for economic planning, disaster preparedness, and policy-making, subseasonal-to-
seasonal (S2S) prediction is lagging behind the more established field of short/medium-range weather,
or long-range climate predictions. For instance, many natural hazards tend to manifest in the S2S
scale, including the slow-onset of droughts that lead to wildfire [1, 2], heavy precipitations that lead to
flooding [3], and persistent weather anomalies that lead to extremes [4]. So far, current approaches to
weather and climate prediction are heavily reliant on physics-based models in the form of Numerical
Weather Prediction (NWP). Many NWPs are based on the discretization of governing equations that
describe thermodynamics, fluid flows, etc. However, these models are expensive to run especially
in high-resolution setting. For example, there are massive computational overheads to perform
numerical integration at fine spatiotemporal resolutions that are operationally useful [5]. Furthermore,
*Corresponding author: jn2808@columbia.edu
38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.
arXiv:2402.00712v5  [cs.CV]  20 Nov 2024

Data-driven Model
Physics-based constraints
Deterministic/probabilistic
constraints
...
44 days lead-time
Inputs 
(Observations)
124 variables
45 years
Targets
(Simulation/Prediction)
Physics-based Model
Figure 1: We propose ChaosBench, a large-scale, fully-coupled, physics-based benchmark for
subseasonal-to-seasonal (S2S) climate prediction. It is framed as a high-dimensional sequential
regression task that consists of 45+ years, multi-system observations for validating physics-based
and data-driven models, and training the latter. Physics-based forecasts are generated from four
national weather agencies with 44-day lead-time and serve as baselines to data-driven forecasts. Our
benchmark is one of the first to incorporate physics-based metrics to ensure physically-consistent and
explainable models. The blurred image at ∆t = 44 represents a challenge of long-term forecasting.
their relative inaccessibility to non-experts is a major roadblock to the broader community. As a
result, there is a growing interest to apply data-driven models to emulate NWPs, as they tend to
have faster inference speed, are less resource-hungry, and more accessible [6, 7, 8, 9, 10, 11, 12].
Nevertheless, many data-driven benchmarks have so far been focused on the short (1-5 days), medium
(5-15 days), and long (years-decades) forecasting ranges. In this work, we include S2S as a more
challenging task that requires different emulation strategies: being in between two extremes, it is
doubly sensitive to (1) initial conditions (IC) as in the case for short/medium-range weather, and (2)
boundary conditions (BC) as in the case for long-range climate [13, 14, 15, 16].
We propose ChaosBench to bridge these gaps (Figure 1). It is comprised of variables beyond the
typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that
span over 45 years to allow for full Earth system emulation that respects boundary processes. We
also provide 44-day ahead physics-based control (deterministic) and perturbed (ensemble) forecasts
from four national weather agencies over the last 8 years as baselines. In addition, we introduce
physics-based and incorporate probabilistic, in addition to deterministic metrics, for a more physically-
consistent ensemble that accounts for butterfly effect. As far as we know, ChaosBench is one of the
first to systematically evaluate several state-of-the-art data-driven models including ViT/ClimaX [17],
PanguWeather [18], GraphCast [7], and FourCastNetV2 [9] on S2S predictability.
In this work, we demonstrate that existing physics-based and data-driven models are indistinguishable
from unskilled climatology as the forecasting range approaches the S2S timescale. The high spectral
divergence observed in many state-of-the-art models suggests the lost of predictive accuracy of
multi-scale structures. This leads to significant blurring and a tendency towards smoother predictions.
For one, such averaging is of little use when one attempts to identify extreme events requiring
high-fidelity forecasts on the S2S scale (e.g., regional droughts, hurricanes, etc). Also, performing
comparably worse than climatology renders them operationally unusable. This highlights the urgent
need for a robust and unified data-driven S2S intercomparison project.
2
Related Work
In recent years, several benchmarks have been introduced to push the field of data-driven weather and
climate prediction [19, 20, 21, 22, 23, 24, 25, 26, 27]. We analyze the limitations of existing works,
and propose how ChaosBench fills in these gaps (see Table 1, more justifications in Appendix C).
Gap in forecast lead-time. Many existing benchmarks are built for short/medium-range weather (up
to 15 days) [22, 19, 20], and long-term climate (annual to decadal scale) [26]. As discussed earlier,
these problems tend to be easier due to the lack of combined sensitivities to IC and BC [13, 14].
Limited spatiotemporal extent. Many S2S benchmarks tend to focus on regional forecasts, such as
the US [23, 24]. In addition, the temporal extent of observation with common interval is more varied,
2

Table 1: Comparison with other benchmark datasets: ChaosBench (ours) is evaluated on the largest set
of global variables, benchmarked against large number of operational NWPs (four national agencies
in the US, Europe, UK, and Asia), and incorporates both physics-based and probabilistic metrics for
a more physically-consistent S2S ensemble forecast.
Datasets
# input
variables
# target
variables
forecast lead
(days)
physics-based
metrics
probabilistic
metrics
spatial extent
WeatherBench [22]
110
110
15
✓
✓
global
SubseasonalRodeo [23]
<30
2
44
✗
✗
western US
SubseasonalClimateUSA [24]
<30
2
44
✗
✓
contiguous US
CliMetLab [25]
<30
2
44
✗
✓
global
ChaosBench (ours)
124
124
44
✓
✓
global
with some less than 20 years [19, 25]. ChaosBench has the most extensive overlapping temporal
coverage yet, extending to 45+ years of inputs covering multiple reanalysis products beyond ERA5.
Limited diversity of baseline models. Having a large set of physics-based forecasts as baselines is
key to reducing bias and diversifying the target goal-posts. Previous benchmarks are mostly focused
on increasing the number of data-driven models for baselines [22, 23]. In contrast, ChaosBench
also places weights on expanding the diversity of physics-based models, including those operated by
leading national weather agencies in the US, Europe, UK, and Asia.
Lack of physics-based constraints. So far, limited number of benchmarks have explicitly incorpo-
rated physical principles to improve or constrain forecasts. ChaosBench introduces physics-based
metrics that can be used for comparison (scalar) and integrated into ML pipeline (differentiable).
3
ChaosBench
3.1
Observations
We discuss the components of ChaosBench, including the global reanalysis products of surface-
atmosphere (ERA5), sea-ice (ORAS5), and terrestrial (LRA5), as well as simulations from physics-
based models. The spatiotemporal resolutions of the former are matched with the latter’s daily
forecasts at 1.5◦to allow for consistent evaluation and integration e.g., hybrid physics-based emulator.
However, we provide a one-liner script to process higher e.g., 0.25◦resolution input in Section B.4.
ERA5 Reanalysis provides a comprehensive record of the global atmosphere combining physics
and observations for correction [28].
We processed their hourly data from 1979 to present
and selected measurements at the 00UTC step.
The variables include temperature (t), spe-
cific humidity (q), geopotential height (z), and 3D wind speed (u, v, w) at 10 pressure levels:
1000, 925, 850, 700, 500, 300, 200, 100, 50, 10 hpa, totalling 60 variables (full list in D.1.1).
ORAS5 or the Ocean Reanalysis System 5 provides an extensive record of sea-ice variables that
incorporate multiple depth levels [29]. Since the public data is available on a monthly basis, we
replicate them for daily compatibility with temporal extent from 1979 to present, for a total of 21
variables, including sst and ssh (full list in D.1.2).
LRA5 or ERA5-Land Reanalysis provides a detailed record of variables governing global terrestrial
processes with specific corrections tailored for land surface applications such as flood forecasting [30]
or carbon fluxes [21, 31]. We processed hourly data from 1979 to present and selected measurements
at the 00UTC step, for a total of 43 variables, including t2m, u10, v10, and tp (full list in D.1.3).
3.2
Simulations
We briefly describe the forecast generation process from physics-based models (Figure 2), including
details on forecast frequency and the number of ensemble members. More details are provided in
3

 t = 1
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Normalized humidity@700-hpa label, forecast, and resid-
ual at the first (t = 1) and final (t = 44) step with ClimaX
Wavenumber, k
100
100
101
101
102
Number of days ahead
0
10
20
30
40
Power, S(k)
100
102
103
105
107
(b) Power spectrum S(k) vs. wavenumber k
plot as a function of prediction step of nor-
malized humidity@700-hpa with ClimaX
Figure 3: Motivating problem: as we perform longer rollouts, the (a) residual error becomes larger
and prediction becomes blurry. This behavior is captured in the Fourier frequency domain where the
(b) power spectra S(k) at low wavenumber k (i.e., low frequency signal) remains consistent at long
rollouts, but not for higher k (i.e., high frequency signal). This phenomenon explains why long-term
forecasts excel at capturing large-scale pattern but not fine-grained details i.e., smooth.
Appendix D.2. The list of available variables for physics-based forecast are similar to ERA5 but
missing {q10,q50,q100} and w /∈{w500} for a total of 48 variables. In all, we process control
(deterministic) and perturbed (ensemble) forecasts from 2016 to present [32].
Atmosphere (60)
Ocean (21)
Terrestrial (43)
Coupled
Physics
Numerics
Physics-based Model
Initialization/
Perturbation
Data
assimilation
Numerical
Dynamics
Others
Lead-time = T
Forecasts
Cryosphere (21)
Figure 2: Physics-based simulations that cou-
ple different parts of the Earth system along
with their operational choices such as data
assimilation. The brackets are the number of
variables provided in ChaosBench.
UKMO. The UK Meteorological Office uses
the Global Seasonal Forecast System Version 6
(GloSea6) model [33] to generate daily 3+1 ensem-
ble/control forecasts for 60-day lead time.
NCEP. The National Centers for Environmental Pre-
diction uses the Climate Forecast System 2 (CFSv2)
model [34] to generate daily 15+1 ensemble/control
forecast for 45-day lead time.
CMA. The China Meteorological Administration
uses the Beijing Climate Center (BCC) fully-coupled
BCC-CSM2-HR model [35] to generate 3+1 ensem-
ble/control forecasts at 3-day interval for 60-day lead
time.
ECMWF. The European Centre for Medium-Range
Weather Forecasts uses the operational Integrated
Forecasting System (IFS) that includes advanced data
assimilation strategies and global numerical model
of the Earth system [36]. In particular, we use the
CY41R1 version of the IFS to generate 50+1 ensem-
ble/control forecasts twice weekly for 46-day lead
time.
3.3
Auxiliary
In addition to baseline forecasts from physics-based
and data-driven models, we provide additional aux-
iliary data and baselines. This includes climatology, the long-term weather-state statistics, and
persistence, which uses initial observation for subsequent rollouts.
4

4
Benchmark Metrics
We provide an assortment of metrics, which we divide into deterministic, probabilistic, and several
proposed physics-based criteria, for increased explainability. For each metric, unless otherwise noted,
we apply a weighting scheme at each latitude θi as defined by Equation 1.
w(θi) =
cos(θi)
1
|θ|
P|θ|
a=1 cos(θa)
(1)
where θ is the set of all latitudes in our data, and |θ| is its cardinality. We denote the input at time
t as Xt ∈Rh×w×p, where h, w, p represent the height (i.e., latitude), width (i.e., longitude), and
parameter (e.g., temperature) with its associated vertical level (e.g., 1000-hpa or surface). In addition,
we denote {Yt, ˆYt} ∈Rh×w×p as the ground-truth label and prediction respectively. Finally, we
denote each element of latitude and longitude as θi ∈θ and γj ∈γ.
4.1
Deterministic Metrics
We provide popular deterministic metrics in the machine learning and climate science literature alike,
including RMSE, Bias, ACC, and MS-SSIM.
Root Mean Squared Error (RMSE) is useful to penalize outliers, which are especially critical for
weather and climate applications such as extreme event prediction (Equation S1).
Bias assists us to identify misspecification and systematic errors present in the model (Equation S2).
Anomaly Correlation Coefficient (ACC) measures the correlation between predicted and observed
anomalies. This metric is especially useful in weather and climate applications, where deviations
from the norm (e.g., temperature anomalies) often reveal interesting insights (Equation S3).
Multi-Scale Structural Similarity (MS-SSIM) [37] compares structural similarity between forecast
and ground-truth label across scales (refer to Appendix F.1.4 for more details). This is especially
useful in weather systems because they occur at multiple scales, from large systems like cyclones, to
smaller features like localized rain thunderstorms.
4.2
Physics Metrics
As illustrated in Figure 3, we find that in general, data-driven forecasts tend to become blurry
(Figure 3a) due to power divergence in the spectral domain (Figure 3b + S10). This motivates
us to propose two physics-based metrics that measure the deviation or difference between the
power spectra of prediction ˆS(k) and target S(k), where k ∈K, and K is the set of all scalar
wavenumbers from 2D Fourier transform. Focusing on high-frequency components, we introduce
Kq = {k ∈K | k ≥Q(q)}, where Q is the quantile function of K and q ∈[0, 1]. We set q = 0
or q = 0.9 for training and evaluation respectively. We denote Sq = {S(k) | k ∈Kq} as the
corresponding power spectra on Kq, and we normalize the distribution to S′(k) such that it sums up
to 1. Similarly we use ˆS′(k) to denote the normalized power for predictions.
Spectral Divergence (SpecDiv) follows principles from Kullback–Leibler (KL) divergence [38]
where we compute the expectation of the log ratio between target S′(k) and prediction ˆS′(k) spectra,
and is defined in Equation 2 (see Listing S1 for PYTORCH psuedocode).
MSpecDiv =
X
k
S′(k) · log(S′(k)/ ˆS′(k))
(2)
Spectral Residual (SpecRes) follows principles from RMSE and adapted from [39] where we
compute the root of the expected squared residual, and is defined in Equation 3 (see Listing S2 for
PYTORCH psuedocode).
MSpecRes =
q
Ek[( ˆS′(k) −S′(k))2]
(3)
5

0
10
20
30
40
Number of days ahead
1
2
3
4
5
6
RMSE [K]
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
RMSE [gpm]
×102
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
2.5
3.0
RMSE [10
3kgkg
1]
×10
3
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(b) ACC (↑is better)
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
SSIM
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(c) MS-SSIM (↑is better)
Figure 4: Evaluation results between baseline climatology (black line) and physics-based con-
trol/deterministic forecasts. At longer forecasting horizon, most physics-based control/deterministic
forecasts perform worse than climatology.
The expectations are calculated over Kq. For both physics-based metrics, the value will be zero if
the power spectra of the forecast is identical to the target, but will increase as discrepancy emerges.
Essentially, both metrics measure how well the forecasts preserve signals across the frequency
spectrum.
4.3
Probabilistic Metrics
In addition to the probabilistic version of RMSE, Bias, ACC, MS-SSIM, SpecDiv, and SpecRes
where we take their expectation with respect to the ensemble members (Equations S14-S19), we also
use several probabilistic metrics to evaluate ensemble forecasts critical for long-range S2S prediction.
Continuous Ranked Probability Score (CRPS) evaluates the accuracy of the ensemble distribution
against the target. Low CRPS values require forecasts to be reliable, where the predicted uncertainty
aligns with the actual uncertainty, and a smaller uncertainty is preferable (Equation S20).
6

0
10
20
30
40
Number of days ahead
1
2
3
4
5
6
RMSE [K]
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
RMSE [gpm]
×102
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
2.5
3.0
RMSE [10
3kgkg
1]
×10
3
q-700
Climatology
PW
GC
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
q-700
Climatology
PW
GC
(b) ACC (↑is better)
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
SSIM
q-700
Climatology
PW
GC
(c) MS-SSIM (↑is better)
Figure 5: Evaluation results between baseline climatology (black line) and data-driven models includ-
ing PanguWeather (PW), GraphCast (GC), and FourCastNetV2 (FCN2). We find that deterministic
ML models perform worse than climatology on S2S timescale. Note: FCN2 lacks q-700.
Continuous Ranked Probability Skill Score (CRPSS) evaluates the skill of probabilistic forecast
relative to climatology variability; CRPSS > 0 suggests skillfulness and vice versa (Equation S21).
Spread quantifies the uncertainty in ensemble forecasts by measuring the variability among ensemble
members, which helps to understand the range of possible outcomes and confidence (Equation S22).
Spread/Skill Ratio balances the ensemble spread with the forecast skill (e.g., RMSE); ideally, a
well-calibrated ensemble should have a spread that matches the forecast skill (Equation S23).
5
Benchmark Results
Throughout this section, we report headline results on ˆX ∈{t-850, z-500, q-700}, following Weather-
bench v2 [40]. The full benchmark scores are available at https://leap-stc.github.io/ChaosBench. We
primarily use four state-of-the-art models for comparison including ViT/ClimaX [17], PanguWeather
[18], GraphCast, and FourCastNetV2 [9] [7]. However, whenever ablation is performed, we use
popular baselines including Lagged Autoencoder [41], ResNet [42], UNet [22], and FNO [43] trained
7

Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
q-700
(a) SpecDiv (↓is better) for physics-based models
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
t-850
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
z-500
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
q-700
(b) SpecDiv (↓is better) for data-driven models
Figure 6: Spectral divergence between (a) physics-based, and (b) data-driven models. Overall,
we observe that the latter perform worse than their physics-based counterpart (barring NCEP) on
time-averaged spectral divergence. Note: FCN2 lacks q-700.
on 1979-2015 data and validated on 2016-2021 data. All evaluations presented here are done on the
held-out 2022 data. The full implementation details are discussed in Appendix E.
Table 2: Performance metrics for SoTAs with different training strategies, at ∆t = 44
Metrics
Variables
Reference
Autoregressive
Direct
Climatology
PW
GC
FCN2
ViT/ClimaX
RMSE ↓
t-850 (K)
3.39
5.85
5.87
5.11
3.56
z-500 (gpm)
81.0
120.9
136.0
112.4
83.1
q-700 (×10−3)
1.62
2.35
2.28
-
1.66
MS-SSIM ↑
t-850
0.85
0.70
0.70
0.74
0.83
z-500
0.82
0.68
0.66
0.72
0.81
q-700
0.62
0.43
0.45
-
0.59
SpecDiv ↓
t-850
0.01
0.25
0.05
0.28
0.20
z-500
0.01
0.33
0.03
0.11
0.13
q-700
0.03
0.23
0.27
-
0.28
Collapse in Predictive Skill. As shown in Figure 4 (+ S2), control forecasts from various operational
centers perform worse than climatology at the S2S scale beyond 15 days. A similar phenomenon of
skill collapse is evident in data-driven models, as depicted in Figure 5 (+ S3). Unlike their physics-
based counterparts, these forecasts exhibit significantly higher spectral divergence as evidenced in
Figure 6, indicating low predictive skill for multi-scale structures over long rollouts. This leads to
the blurring artifacts previously discussed. The pervasive lack of predictive skill underscores the
notoriously difficult challenge of S2S forecasting and highlights huge potential for improvement.
8

0
10
20
30
40
Number of days ahead
0.75
0.80
0.85
0.90
0.95
1.00
RMSE [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.7
0.8
0.9
1.0
1.1
1.2
RMSE [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.75
0.80
0.85
0.90
0.95
1.00
RMSE [ens/det]
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(a) RMSE: ensemble improves deterministic forecasts if ratio < 1
0
10
20
30
40
Number of days ahead
1.00
1.02
1.04
1.06
1.08
1.10
1.12
SSIM [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
1.00
1.02
1.04
1.06
1.08
1.10
1.12
1.14
SSIM [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
1.0
1.1
1.2
1.3
SSIM [ens/det]
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(b) MS-SSIM: ensemble improves deterministic forecasts if ratio > 1
Figure 7: Metrics ratio e.g., RMSEens/RMSEdet between ensemble and deterministic forecasts, where
the former improves the latter by accounting for IC uncertainty that can lead to trajectory divergences.
Note: n represents the number of ensemble members.
Ensemble Forecasts Account for IC Uncertainty. Despite the underperformance of deterministic
models, many studies have highlighted the potential of ensemble forecasts to account for trajectory
divergences caused by IC uncertainties [44, 45, 46], also known as the butterfly effect [15]. Figure
S4 shows that the performance of ensembles across physics-based models improves relative to their
deterministic counterparts. For instance, when we take the metrics ratio between ensemble and
deterministic forecasts as in Figure 7 (+ S5), the ratio of RMSE decreases with lead time, while
the ratio of MS-SSIM improves over time with little significant changes in SpecDiv. The extent of
improvement also appears to be affected by the number of ensemble members i.e., higher ensemble
size n appears to improve skillfulness. We also note similar insights from data-driven ensembling
strategy as discussed in Section G.3. This highlights the importance of building a well-dispersed
ensemble that accounts for long-range divergences for improved S2S predictability.
Minimizing Error Propagation Promotes Stability. Different training and inference strategies
have been proposed to improve the accuracy and stability of data-driven weather emulators. Chief
among these are the autoregressive and direct approaches [47]. The former iteratively cycles through
small interval to reach the target lead-time i.e., ∆t = Nδt where N ∈Z+ is the number of such
compositions, while the latter directly outputs ∆t. As summarized in Table 2, we find models trained
directly (e.g., ViT/ClimaX) have better performance than those used autoregressively (e.g., PW, GC,
FCN2). This suggests that error propagation is a significant source of error, and controlling for
stability is key to extend the predictability range of weather emulators. Once stability is achieved, the
remaining sources of errors including uncertainties in observation and/or modeling framework can be
improved through more data, better model, or both through data assimilation for instance [48].
Physical Constraints Yield Improved Performance. We find models that explicitly incorporate
physical knowledge (e.g., learning spectral signals beyond pixel information) have better performance
across metrics, such as FNO, as summarized in Table S8 given identical parameter budget of 106. This
phenomena is unsurprising and has been repeatedly demonstrated in many real-world applications of
physics-informed deep learning, for instance.
9

0
10
20
30
40
Number of days ahead
0.4
0.2
0.0
0.2
0.4
0.6
CRPSS
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.4
0.2
0.0
0.2
0.4
0.6
0.8
CRPSS
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.6
0.4
0.2
0.0
0.2
0.4
0.6
CRPSS
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
Figure 8: Probabilistic evaluation on ensemble forecasts indicating current skill limits of 15-20 days;
CRPSS > 0 suggests skills better than climatology variability. Note: n represents the number of
ensemble members.
Current Limits of S2S Predictability. Given our best models, we evaluate the extent of predictability
in order to base our next steps. As illustrated in Figure 8 (+ S6), we find that ECMWF high-resolution
ensemble, dubbed as the gold standard, still has the best performance in terms of CRPSS (vs ERA5
climatology), with a predictability range of around 15-20 days ahead before its skill collapses
to climatology (i.e., CRPSS →0). However, the resurgence of data-driven models are rapidly
transforming the field as they are able to efficiently distil knowledge and automatically discover
emergent patterns from large-scale, high-dimensional dataset, instead of first reducing them to
physical functions with limited set of variables requiring constant calibration as is traditionally done
in NWPs. The challenge, therefore, is to extend the predictability range of weather system as a
representation of large-scale chaos, and we welcome the machine learning communities to take part
in this open effort.
6
Conclusion
We present ChaosBench, a challenging benchmark to extend the predictability range of weather
emulators into the S2S timescale where many processes with significant socioeconomic repercussions
tend to occur, including extreme events. In addition to providing diverse datasets beyond ERA5 for a
full Earth system emulation, we also perform extensive benchmarking on state-of-the-art data-driven
and physics-based models alike. Through various ablation, we systematically find that skillfulness
can be extended by ensemble forecasting, controlling for exponential error growth, and incorporating
physical knowledge in our modeling approaches.
Future Work. Our input datasets have relatively coarse spatiotemporal resolution to match that
of physics-based S2S forecasts. Nevertheless, we make the data processing pipeline open-source,
allowing users to easily process inputs of the desired resolution (see Section B.4 for more details).
We are planning for a multi-source reanalysis products (e.g., MERRA-2 [49]), leveraging diverse
dataset strengths, such as the assimilation of different set of observations. As always, we welcome
any contribution from the open-source community to solve this important yet understudied problem.
And any comments, feedback, and/or future feature requests can be directed to the corresponding
author or through the Github issue tracker at https://github.com/leap-stc/ChaosBench.
Acknowledgments and Disclosure of Funding
We would like to thank Matthew Wilson, Tom Andersson, and Dale Durran for the insightful
discussion during the earlier version of the manuscript. The authors also acknowledge funding,
computing, and storage resources from the NSF Science and Technology Center (STC) Learning
the Earth with Artificial Intelligence and Physics (LEAP) (Award #2019625) and the Department of
Energy (DOE) Advanced Scientific Computing Research (ASCR) program (DE-SC0022255). AG
would like to acknowledge support from Google and Schmidt Sciences. Last but definitely not least,
we acknowledge the comprehensive S2S database emerging from the joint initiative of the World
Weather Research Programme (WWRP) and the World Climate Research Programme (WCRP). The
original S2S database is hosted at ECMWF as an extension of the TIGGE database.
10

References
[1] Angeline G Pendergrass, Gerald A Meehl, Roger Pulwarty, Mike Hobbins, Andrew Hoell, Amir
AghaKouchak, Céline JW Bonfils, Ailie JE Gallant, Martin Hoerling, David Hoffmann, et al.
Flash droughts present a new challenge for subseasonal-to-seasonal prediction. Nature Climate
Change, 10(3):191–199, 2020.
[2] Jatan Buch, A Park Williams, Caroline S Juang, Winslow D Hansen, and Pierre Gentine.
Smlfire1. 0: a stochastic machine learning (sml) model for wildfire activity in the western united
states. Geoscientific Model Development, 16(12):3407–3433, 2023.
[3] Sara Shamekh, Kara D Lamb, Yu Huang, and Pierre Gentine. Implicit learning of convective
organization explains precipitation stochasticity. Proceedings of the National Academy of
Sciences, 120(20):e2216158120, 2023.
[4] Lucas R Vargas Zeppetello, David S Battisti, and Marcia B Baker. The physics of heat waves:
What causes extremely high summertime temperatures? Journal of Climate, 35(7):2231–2251,
2022.
[5] Tapio Schneider, Swadhin Behera, Giulio Boccaletti, Clara Deser, Kerry Emanuel, Raffaele
Ferrari, L Ruby Leung, Ning Lin, Thomas Müller, Antonio Navarra, et al. Harnessing ai and
computing to advance climate modelling and prediction. Nature Climate Change, 13(9):887–
889, 2023.
[6] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Accurate
medium-range global weather forecasting with 3d neural networks. Nature, 619(7970):533–538,
2023.
[7] Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato,
Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, et al. Graphcast:
Learning skillful medium-range global weather forecasting. arXiv preprint arXiv:2212.12794,
2022.
[8] S Karthik Mukkavilli, Daniel Salles Civitarese, Johannes Schmude, Johannes Jakubik, Anne
Jones, Nam Nguyen, Christopher Phillips, Sujit Roy, Shraddha Singh, Campbell Watson, et al.
Ai foundation models for weather and climate: Applications, design, and implementation. arXiv
preprint arXiv:2309.10808, 2023.
[9] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al.
Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural
operators. arXiv preprint arXiv:2202.11214, 2022.
[10] Yongquan Qu and Xiaoming Shi. Can a machine learning–enabled numerical model help
extend effective forecast range through consistently trained subgrid-scale models? Artificial
Intelligence for the Earth Systems, 2(1):e220050, 2023.
[11] Yongquan Qu, Mohamed Aziz Bhouri, and Pierre Gentine. Joint parameter and parameterization
inference with uncertainty quantification through differentiable programming. In ICLR 2024
Workshop on AI4DifferentialEquations In Science.
[12] Sungduk Yu, Walter M Hannah, Liran Peng, Mohamed Aziz Bhouri, Ritwik Gupta, Jerry Lin,
Björn Lütjens, Justus C Will, Tom Beucler, Bryce E Harrop, et al. Climsim: An open large-scale
dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators.
arXiv preprint arXiv:2306.08754, 2023.
[13] Nikki C Privé and Ronald M Errico. The role of model and initial condition error in numerical
weather forecasting investigated with an observing system simulation experiment. Tellus A:
Dynamic Meteorology and Oceanography, 65(1):21740, 2013.
[14] Wanli Wu, Amanda H Lynch, and Aaron Rivers. Estimating the uncertainty in a regional climate
model related to initial and lateral boundary conditions. Journal of climate, 18(7):917–933,
2005.
[15] Edward N Lorenz. Deterministic nonperiodic flow. Journal of atmospheric sciences, 20(2):130–
141, 1963.
[16] Nathaniel Cresswell-Clay, Bowen Liu, Dale Durran, Andy Liu, Zachary I Espinosa, Raul
Moreno, and Matthias Karlbauer. A deep learning earth system model for stable and efficient
simulation of the current climate. arXiv preprint arXiv:2409.16247, 2024.
11

[17] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover.
Climax: A foundation model for weather and climate. arXiv preprint arXiv:2301.10343, 2023.
[18] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian. Pangu-weather:
A 3d high-resolution model for fast and accurate global weather forecast. arXiv preprint
arXiv:2211.02556, 2022.
[19] Karthik Kashinath, Mayur Mudigonda, Sol Kim, Lukas Kapp-Schwoerer, Andre Graubner,
Ege Karaismailoglu, Leo Von Kleist, Thorsten Kurth, Annette Greiner, Ankur Mahesh, et al.
Climatenet: An expert-labeled open dataset and deep learning architecture for enabling high-
precision analyses of extreme weather. Geoscientific Model Development, 14(1):107–124,
2021.
[20] Evan Racah, Christopher Beckham, Tegan Maharaj, Samira Ebrahimi Kahou, Mr Prabhat,
and Chris Pal. Extremeweather: A large-scale climate dataset for semi-supervised detection,
localization, and understanding of extreme weather events. Advances in neural information
processing systems, 30, 2017.
[21] Juan Nathaniel, Jiangong Liu, and Pierre Gentine. Metaflux: Meta-learning global carbon fluxes
from sparse spatiotemporal observations. Scientific Data, 10(1):440, 2023.
[22] Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and
Nils Thuerey. Weatherbench: a benchmark data set for data-driven weather forecasting. Journal
of Advances in Modeling Earth Systems, 12(11):e2020MS002203, 2020.
[23] Jessica Hwang, Paulo Orenstein, Judah Cohen, Karl Pfeiffer, and Lester Mackey. Improving
subseasonal forecasting in the western us with machine learning. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages
2325–2335, 2019.
[24] Soukayna Mouatadid, Paulo Orenstein, Genevieve Elaine Flaspohler, Miruna Oprescu, Judah
Cohen, Franklyn Wang, Sean Edward Knight, Maria Geogdzhayeva, Samuel James Levang,
Ernest Fraenkel, et al. Subseasonalclimateusa: A dataset for subseasonal forecasting and bench-
marking. In Thirty-seventh Conference on Neural Information Processing Systems Datasets
and Benchmarks Track, 2023.
[25] Frederic Vitart, Andrew W Robertson, Aaron Spring, Florian Pinault, Rok Roškar, W Cao,
S Bech, A Bienkowski, N Caltabiano, E De Coning, et al. Outcomes of the wmo prize challenge
to improve subseasonal to seasonal predictions using artificial intelligence. Bulletin of the
American Meteorological Society, 103(12):E2878–E2886, 2022.
[26] Duncan Watson-Parris, Yuhan Rao, Dirk Olivié, Øyvind Seland, Peer Nowack, Gustau Camps-
Valls, Philip Stier, Shahine Bouabid, Maura Dewey, Emilie Fons, et al. Climatebench v1. 0: A
benchmark for data-driven climate projections. Journal of Advances in Modeling Earth Systems,
14(10):e2021MS002954, 2022.
[27] Julia Kaltenborn, Charlotte Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz,
Chandni Nagda, Jakob Runge, Peer Nowack, and David Rolnick. Climateset: A large-scale
climate model dataset for machine learning. Advances in Neural Information Processing
Systems, 36:21757–21792, 2023.
[28] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-
Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al. The era5 global
reanalysis. Quarterly Journal of the Royal Meteorological Society, 146(730):1999–2049, 2020.
[29] Hao Zuo, Magdalena Alonso Balmaseda, Steffen Tietsche, Kristian Mogensen, and Michael
Mayer. The ecmwf operational ensemble reanalysis–analysis system for ocean and sea ice: a
description of the system and assessment. Ocean science, 15(3):779–808, 2019.
[30] Joaquín Muñoz-Sabater, Emanuel Dutra, Anna Agustí-Panareda, Clément Albergel, Gabriele
Arduini, Gianpaolo Balsamo, Souhail Boussetta, Margarita Choulga, Shaun Harrigan, Hans
Hersbach, et al. Era5-land: A state-of-the-art global reanalysis dataset for land applications.
Earth system science data, 13(9):4349–4383, 2021.
[31] Juan Nathaniel, Gabrielle Nyirjesy, Campbell D Watson, Conrad M Albrecht, and Levente J
Klein. Above ground carbon biomass estimate with physics-informed deep network. In IGARSS
2023-2023 IEEE International Geoscience and Remote Sensing Symposium, pages 1297–1300.
IEEE, 2023.
12

[32] Frederic Vitart, Constantin Ardilouze, Axel Bonet, Anca Brookshaw, M Chen, C Codorean,
M Déqué, L Ferranti, E Fucile, M Fuentes, et al. The subseasonal to seasonal (s2s) prediction
project database. Bulletin of the American Meteorological Society, 98(1):163–173, 2017.
[33] KD Williams, CM Harris, A Bodas-Salcedo, J Camp, RE Comer, D Copsey, D Fereday,
T Graham, R Hill, T Hinton, et al. The met office global coupled model 2.0 (gc2) configuration.
Geoscientific Model Development, 88(55):1509–1524, 2015.
[34] Suranjana Saha, Shrinivas Moorthi, Xingren Wu, Jiande Wang, Sudhir Nadiga, Patrick Tripp,
David Behringer, Yu-Tai Hou, Hui-ya Chuang, Mark Iredell, et al. The ncep climate forecast
system version 2. Journal of climate, 27(6):2185–2208, 2014.
[35] Tongwen Wu, Yixiong Lu, Yongjie Fang, Xiaoge Xin, Laurent Li, Weiping Li, Weihua Jie, Jie
Zhang, Yiming Liu, Li Zhang, et al. The beijing climate center climate system model (bcc-csm):
The main progress from cmip5 to cmip6. Geoscientific Model Development, 12(4):1573–1600,
2019.
[36] IFS DOCUMENTATION. Part v: The ensemble prediction.
[37] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for im-
age quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems &
Computers, 2003, volume 2, pages 1398–1402. Ieee, 2003.
[38] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of
mathematical statistics, 22(1):79–86, 1951.
[39] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani,
Dirk Pflüger, and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine
learning. Advances in Neural Information Processing Systems, 35:1596–1611, 2022.
[40] Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter Battaglia, Tyler Russel,
Alvaro Sanchez-Gonzalez, Vivian Yang, Rob Carver, Shreya Agrawal, et al. Weatherbench
2: A benchmark for the next generation of data-driven global weather models. arXiv preprint
arXiv:2308.15560, 2023.
[41] Bethany Lusch, J Nathan Kutz, and Steven L Brunton. Deep learning for universal linear
embeddings of nonlinear dynamics. Nature communications, 9(1):4950, 2018.
[42] Stephan Rasp and Nils Thuerey. Data-driven medium-range weather prediction with a resnet
pretrained on climate simulations: A new model for weatherbench. Journal of Advances in
Modeling Earth Systems, 13(2):e2020MS002405, 2021.
[43] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya,
Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differen-
tial equations. arXiv preprint arXiv:2010.08895, 2020.
[44] Cecil E Leith. Theoretical skill of monte carlo forecasts. Monthly weather review, 102(6):409–
418, 1974.
[45] Jonathan A Weyn, Dale R Durran, Rich Caruana, and Nathaniel Cresswell-Clay. Sub-seasonal
forecasting with a large ensemble of deep-learning weather prediction models. Journal of
Advances in Modeling Earth Systems, 13(7):e2021MS002502, 2021.
[46] Lei Chen, Xiaohui Zhong, Hao Li, Jie Wu, Bo Lu, Deliang Chen, Shang-Ping Xie, Libo Wu,
Qingchen Chao, Chensen Lin, et al. A machine learning model that outperforms conventional
global subseasonal forecast models. Nature Communications, 15(1):6425, 2024.
[47] Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Sandeep Madireddy, Romit Maulik,
Veerabhadra Kotamarthi, Ian Foster, and Aditya Grover. Scaling transformer neural networks
for skillful and reliable medium-range weather forecasting. arXiv preprint arXiv:2312.03876,
2023.
[48] Yongquan Qu, Juan Nathaniel, Shuolin Li, and Pierre Gentine. Deep generative data assimilation
in multimodal setting. arXiv preprint arXiv:2404.06665, 2024.
[49] Ronald Gelaro, Will McCarty, Max J Suárez, Ricardo Todling, Andrea Molod, Lawrence
Takacs, Cynthia A Randles, Anton Darmenov, Michael G Bosilovich, Rolf Reichle, et al. The
modern-era retrospective analysis for research and applications, version 2 (merra-2). Journal of
climate, 30(14):5419–5454, 2017.
13

[50] Soukayna Mouatadid, Paulo Orenstein, Genevieve Flaspohler, Judah Cohen, Miruna Oprescu,
Ernest Fraenkel, and Lester Mackey. Adaptive bias correction for improved subseasonal
forecasting. Nature Communications, 14(1):3482, 2023.
[51] David Storkey, Adam T Blaker, Pierre Mathiot, Alex Megann, Yevgeny Aksenov, Edward W
Blockley, Daley Calvert, Tim Graham, Helene T Hewitt, Patrick Hyder, et al. Uk global ocean
go6 and go7: A traceable hierarchy of model resolutions. Geoscientific Model Development,
11(8):3187–3213, 2018.
[52] Pierre Mathiot, Adrian Jenkins, Christopher Harris, and Gurvan Madec. Explicit representation
and parametrised impacts of under ice shelf seas in the z coordinate ocean model nemo 3.6.
Geoscientific Model Development, 10(7):2849–2874, 2017.
[53] Kristian Mogensen, Magdalena Alonso Balmaseda, Anthony Weaver, et al. The nemovar ocean
data assimilation system as implemented in the ecmwf ocean analysis for system 4. 2012.
[54] Hiroyuki Tsujino, L Shogo Urakawa, Stephen M Griffies, Gokhan Danabasoglu, Alistair J
Adcroft, Arthur E Amaral, Thomas Arsouze, Mats Bentsen, Raffaele Bernardello, Claus W
Böning, et al. Evaluation of global ocean–sea-ice model simulations based on the experimental
protocols of the ocean model intercomparison project phase 2 (omip-2). Geoscientific Model
Development, 13(8):3643–3708, 2020.
[55] Martin J Best, M Pryor, DB Clark, Gabriel G Rooney, R Essery, CB Ménard, JM Edwards,
MA Hendry, A Porson, N Gedney, et al. The joint uk land environment simulator (jules), model
description–part 1: energy and water fluxes. Geoscientific Model Development, 4(3):677–699,
2011.
[56] Shinya Kobayashi, Yukinari Ota, Yayoi Harada, Ayataka Ebita, Masami Moriya, Hirokatsu
Onoda, Kazutoshi Onogi, Hirotaka Kamahori, Chiaki Kobayashi, Hirokazu Endo, et al. The
jra-55 reanalysis: General specifications and basic characteristics. Journal of the Meteorological
Society of Japan. Ser. II, 93(1):5–48, 2015.
[57] Yuhong Tian, Curtis E Woodcock, Yujie Wang, Jeff L Privette, Nikolay V Shabanov, Liming
Zhou, Yu Zhang, Wolfgang Buermann, Jiarui Dong, Brita Veikkanen, et al. Multiscale anal-
ysis and validation of the modis lai product: I. uncertainty assessment. Remote Sensing of
Environment, 83(3):414–430, 2002.
[58] Thomas R Loveland, Bradley C Reed, Jesslyn F Brown, Donald O Ohlen, Zhiliang Zhu, LWMJ
Yang, and James W Merchant. Development of a global land cover characteristics database and
igbp discover from 1 km avhrr data. International journal of remote sensing, 21(6-7):1303–1330,
2000.
[59] WR Wieder, J Boehnert, GB Bonan, and M Langseth. Regridded harmonized world soil
database v1. 2. ORNL DAAC, 2014.
[60] Akio Arakawa. Computational design for long-term numerical integration of the equations of
fluid motion: Two-dimensional incompressible flow. part i. Journal of computational physics,
135(2):103–114, 1997.
[61] Suranjana Saha, Shrinivas Moorthi, Hua-Lu Pan, Xingren Wu, Jie Wang, Sudhir Nadiga, Patrick
Tripp, Robert Kistler, John Woollen, David Behringer, et al. Ncep climate forecast system
reanalysis (cfsr) monthly products, january 1979 to december 2010. 2010.
[62] Stephen M Griffies, Matthew J Harrison, Ronald C Pacanowski, Anthony Rosati, et al. A
technical guide to mom4. GFDL Ocean Group Tech. Rep, 5(5):371, 2004.
[63] MB Ek, KE Mitchell, Ying Lin, Eric Rogers, Pablo Grunmann, Victor Koren, George Gayno,
and JD Tarpley. Implementation of noah land surface model advances in the national centers for
environmental prediction operational mesoscale eta model. Journal of Geophysical Research:
Atmospheres, 108(D22), 2003.
[64] Jesse Meng, Rongqian Yang, Helin Wei, Michael Ek, George Gayno, Pingping Xie, and Kenneth
Mitchell. The land surface analysis in the ncep climate forecast system reanalysis. Journal of
Hydrometeorology, 13(5):1621–1630, 2012.
[65] Leonard Zobler. A world soil file grobal climate modeling. NASA Tech. memo, 32, 1986.
[66] Mariano Hortal and AJ Simmons. Use of reduced gaussian grids in spectral models. Monthly
Weather Review, 119(4):1057–1074, 1991.
14

[67] Tongwen Wu, Lianchun Song, Weiping Li, Zaizhi Wang, Hua Zhang, Xiaoge Xin, Yanwu
Zhang, Li Zhang, Jianglong Li, Fanghua Wu, et al. An overview of bcc climate system model
development and application for climate change studies. Journal of Meteorological Research,
28:34–56, 2014.
[68] Peter J Lawrence and Thomas N Chase. Representing a new modis consistent land surface
in the community land model (clm 3.0). Journal of Geophysical Research: Biogeosciences,
112(G1), 2007.
[69] Peter Janssen, Jean-Raymond Bidlot, Saleh Abdalla, and Hans Hersbach. Progress in ocean
wave forecasting at ECMWF. ECMWF Reading, UK, 2005.
[70] M Th Van Genuchten. A closed-form equation for predicting the hydraulic conductivity of
unsaturated soils. Soil science society of America journal, 44(5):892–898, 1980.
[71] Sylvie Malardel, Nils Wedi, Willem Deconinck, Michail Diamantakis, Christian Kühnlein,
George Mozdzynski, Mats Hamrud, and Piotr Smolarkiewicz. A new grid for the ifs. ECMWF
newsletter, 146(23-28):321, 2016.
[72] Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Chandratreya, Qiang Du, and Hod
Lipson. Automated discovery of fundamental variables hidden in experimental data. Nature
Computational Science, 2(7):433–442, 2022.
[73] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
[74] Dan Hendrycks and Kevin Gimpel.
Gaussian error linear units (gelus).
arXiv preprint
arXiv:1606.08415, 2016.
[75] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
15

ChaosBench: A Multi-Channel, Physics-Based Benchmark for
Subseasonal-to-Seasonal Climate Prediction
Supplementary Material
Juan Nathaniel1,*, Yongquan Qu1, Tung Nguyen2, Sungduk Yu3,5, Julius Busecke1,4,
Aditya Grover2, Pierre Gentine1
1Columbia University, 2UCLA, 3UCI, 4LDEO, 5 Intel Labs
A
Accountability and Reproducibility Statement
ChaosBench is published under the open source GNU General Public License. Further development
and potential updates discussed in the limitations section will take place on the ChaosBench page.
Furthermore, we are committed to maintaining and preserving the ChaosBench benchmark. Ongoing
maintenance also includes tracking and resolving issues identified by the broader community after
release. User feedback will be closely monitored via the GitHub issue tracker. All assets are hosted
on GitHub and HuggingFace, which guarantees reliable and stable storage.
Dataset: All our dataset, present and future (e.g., with more years, multi-resolution support, etc) are
available at https://huggingface.co/datasets/LEAP/ChaosBench.
Model Checkpoints: All of our model checkpoints used for the purposes of ablation in this work are
available at https://huggingface.co/datasets/LEAP/ChaosBench/tree/main/logs.
Code:
Our code and its future extension based on community feedback is accessible at
https://github.com/leap-stc/ChaosBench.
Documentation: Finally, our main webpage will keep track of all important updates and latest
documentation, and is accessible at https://leap-stc.github.io/ChaosBench.
*Corresponding author: jn2808@columbia.edu
1

B
Getting Started
Here, we provide a detailed description on how to prepare the necessary data, perform
training, and benchmark your own model.
However, we refer users to our webpage
https://leap-stc.github.io/ChaosBench for the most updated how-to guides.
The
following
sections
assume
successful
cloning
of
our
Github
repository
https://github.com/leap-stc/ChaosBench.
If you find any problems, feel free to con-
tact us or raise an issue.
B.1
Data Preparation
First, navigate to the repository directory and install the necessary dependencies.
$ cd ChaosBench
$ pip
install -r requirements.txt
Second, download the dataset using the following commands.
$ cd data/
$ wget
https :// huggingface.co/datasets/LEAP/ChaosBench/resolve/main/
process.sh
$ chmod +x process.sh
Third, process the following required and optional dataset.
# Required
for inputs and
climatology (e.g., normalization )
$ ./ process.sh era5
$ ./ process.sh lra5
$ ./ process.sh oras5
$ ./ process.sh climatology
# Optional: control (deterministic ) forecasts
$ ./ process.sh ukmo
$ ./ process.sh ncep
$ ./ process.sh cma
$ ./ process.sh ecmwf
# Optional: perturbed (ensemble) forecasts
$ ./ process.sh ukmo_ensemble
$ ./ process.sh ncep_ensemble
$ ./ process.sh cma_ensemble
$ ./ process.sh ecmwf_ensemble
# Optional: SoTa (deterministic ) forecasts
$ ./ process.sh panguweather
$ ./ process.sh graphcast
$ ./ process.sh fourcastnetv2
2

B.2
Training
We will cover how training can generally be performed, followed by how one can switch between
different training strategies by manipulating the config .yaml file.
First, define your model class.
# An example
can be found for e.g. <YOUR_MODEL > == fno
$ touch
chaosbench/models/<YOUR_MODEL >.py
Second, import and initialize your model in the main chaosbench/models/model.py file, given
the pseudocode below.
# Examples
for lagged_ae , fno , resnet , unet are
provided
import
lightning.pytorch as pl
from
chaosbench.models
import
YOUR_MODEL
class
S2SBenchmarkModel (pl. LightningModule ):
def
__init__(
self ,
...
):
super(S2SBenchmarkModel , self).__init__ ()
# Initialize
your
model
self.model = YOUR_MODEL.BEST_MODEL (...)
# The rest of model
construction
logic
Third, run the train.py script. We recommend using GPUs for training.
# The _s2s
suffix
identifies data -driven
models
$ python
train.py --config_filepath
chaosbench/configs/<YOUR_MODEL >
_s2s.yaml
3

Now you will notice that there is a .yaml file. We define the definition of each field, allowing for
greater control over different training strategies.
# The .yaml file
always has two
sections: model_args
and
data_args
model_args:
model_name: <str >
# Name of your
model e.g., ’unet_s2s ’
input_size: <int >
# Input size , default: 60 (ERA5)
output_size: <int >
# Output size , default: 60 (ERA5)
learning_rate: <float >
# Learning
rate
num_workers: <int >
# Number of workers
epochs: <int >
# Number of epochs
t_max: <int >
# Learning
rate
scheduler
only_headline: <bool >
# Only
optimized
for config. HEADLINE_VARS
data_args:
batch_size: <int >
# Batch
size
train_years: [...]
# Train
years e.g., [1979 , ...]
val_years: [...]
# Val years e.g., [2016 , ...]
n_step: <int , 1>
# Number of autoregressive
training
steps
lead_time: <int , 1>
# N-day ahead
forecast (for direct
scheme)
land_vars: [...]
# Extra
LRA5 vars e.g., [’t2m ’, ...]
ocean_vars: [...]
# Extra
ORAS5
vars e.g., [’sosstsst ’, ...]
Note,
1. If only_headline is set to True, then the model is optimized only for a subset of variables
defined in config.HEADLINE_VARS (default: False).
2. If n_step is set to values greater than 1, the models will train over n-autoregressive steps
(default: 1).
3. If lead_time is set to values greater than 1, the models will be able to forecast n-days
ahead. For example, in our direct forecasts, if lead_time is set to 4, our model will predict
the states 4 days into the future (default: 1).
4. If land_vars and/or ocean_vars are set with entries from the acronyms in Tables S3 and
S2, these will be used as additional inputs and targets, on top of ERA5 variables (default: []).
4

B.3
Evaluation
Once training is done, we can perform evaluation depending on the use case. We recommend using
GPUs for evaluation.
First, if we have an autoregressive model, we can simply run:
# Evaluating
autoregressive
model , e.g.,
# --model_name ’unet_s2s ’
# --eval_years
2022
# --version_num 0
## Checkpoint
versions
autogenerated in logs/
# --lra5 ’t2m ’ ’tp ’
## Additional
LRA5 vars to be evaluated
# --oras5 ’sosstsst ’
## Additional
ORAS5
vars to be evaluated
$ python
eval_iter.py --model_name <str > --eval_years <int > --
version_num <int > --lra5
[...]
--oras5
[...]
Second, if we have a collection of models trained specifically for unique lead_time, we can run:
# Evaluating
direct
model
with the
default
sequence of
# lead_time = [1, 5, 10, 15, 20, 25, 30, 35, 40, 44] e.g.,
# --model_name ’unet_s2s ’
# --eval_years
2022
# --version_nums 0 4 5 6 7 8 9 10 11 12
# --lra5 ’t2m ’ ’tp ’
## Additional
LRA5 vars to be evaluated
# --oras5 ’sosstsst ’
## Additional
ORAS5
vars to be evaluated
$ python
eval_direct.py --model_name <str > --eval_years <int > --
version_nums
[...]
--lra5
[...]
--oras5
[...]
Third, if we have a probabilistic model that generates ensemble forecasts (e.g., one checkpoint
represents one ensemble member) and are supposed to be evaluated with additional probabilistic
metrics, we can run:
# Evaluating
ensembles
with
additional
probabilistic
metrics e.g.,
# --model_name ’unet_ensemble_s2s ’
# --eval_years
2022
# --version_nums 0 1 2
## One
ensemble
member per
version
# --lra5 ’t2m ’ ’tp ’
## Additional
LRA5 vars to be evaluated
# --oras5 ’sosstsst ’
## Additional
ORAS5
vars to be evaluated
$ python
eval_ensemble.py --model_name <str > --eval_years <int > --
version_nums
[...]
--lra5
[...]
--oras5
[...]
5

B.4
Optional: Processing Multi-Resolution Input
We open-source the data processing script to allow users to process the inputs given different
resolution (highest is 0.25-degree):
# Process
inputs
with e.g., 0.25 - degree
resolution
$ python
scripts/process_atmos .py --resolution
0.25 # ERA5
$ python
scripts/process_ocean .py --resolution
0.25 # ORAS5
$ python
scripts/process_land.py
--resolution
0.25 # LRA5
6

C
Related Work
Here we discuss the criteria used to compare different S2S benchmark. This list is by no means
exhaustive and there exists many ways to interpret the different contribution, strength, and scope of
each. We refer interested reader to the respective benchmark paper and website.
On Input Variables. The number of input channels indicates the number of unique variables used
for training data-driven models. For instance, in the case of SubseasonalClimateUSA, these include
tmin, tmax, tmean, precip_agg, precip_mean, SST, SIC, z-10, z100, z500, z850, u-250, u-925, v-250,
v-925, surface_P, RH, SSP, precipitable water, PE, DEM, KG, MJO-phase, MJO-amp, ENSO-I,
despite them having similar (25) variables across data sources.
On Target Variables and Agencies. Similarly, the number of target channels represent the variables
these benchmarks are aiming for. This is closely related to the number of benchmark agencies, which
refers to the number of physics-based simulations used as target, rather than inputs. In the case for
SubseasonalClimateUSA, for instance, the number of target channels correspond to two: precipitation
and surface temperature, while the number of benchmark agencies is also two: CFSv2 (NCEP) and
IFS (ECMWF), despite them using multiple other simulations generated from agencies but as inputs;
though evaluated on all in their follow-up work [50] despite not initially described in the dataset
paper.
On Physics Metrics. The flag for physics-based metrics indicates whether these benchmarks
incorporate not just physical explanation, but also formulate them as scalar and differentiable metrics
for future optimization problem.
On Probabilistic Metrics. The flag for probabilistic metrics indicates whether these benchmarks
incorporate probabilistic (e.g., CRPS, CRPSS, Spread, SSR), in addition to deterministic metrics.
On Spatial Extent. Furthermore, the spatial extent indicates the extent of the target benchmark,
rather than of the input dataset. This is because some of the more challenging S2S forecasting task is
to get the correct global space-time correlation, and having a full global coverage provides a more
complete evaluation.
7

D
ChaosBench
D.1
Observations from Reanalysis Products
D.1.1
ERA5
The following table indicates the 48 variables that are inferred by physics-based models. Note that
the Input ERA5 observations contains ALL fields, including the unchecked boxes:
Table S1: List of ERA5 reanalysis variables
Parameters/Levels (hPa)
1000
925
850
700
500
300
200
100
50
10
Geopotential height, z (gpm)
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Specific humidity, q (kg kg−1)
✓
✓
✓
✓
✓
✓
✓
Temperature, t (K)
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
U component of wind, u (ms−1)
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
V component of wind, v (ms−1)
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
Vertical velocity, w (Pas−1)
✓
D.1.2
ORAS5
The variables for ORAS5 consist of the following as described in Table S2.
Table S2: List of ORAS5 reanalysis variables
Acronyms
Long Name
Units
iicethic
sea ice thickness
m
iicevelu
sea ice zonal velocity
ms−1
iicevelv
sea ice meridional velocity
ms−1
ileadfra
sea ice concentration
(0-1)
so14chgt
depth of 14◦isotherm
m
so17chgt
depth of 17◦isotherm
m
so20chgt
depth of 20◦isotherm
m
so26chgt
depth of 26◦isotherm
m
so28chgt
depth of 28◦isotherm
m
sohefldo
net downward heat flux
Wm−2
sohtc300
heat content at upper 300m
Jm−2
sohtc700
heat content at upper 700m
Jm−2
sohtcbtm
heat content for total water column
Jm−2
sometauy
meridonial wind stress
Nm−2
somxl010
mixed layer depth 0.01
m
somxl030
mixed layer depth 0.03
m
sosaline
salinity
PSU
sossheig
sea surface height
m
sosstsst
sea surface temperature
◦C
sowaflup
net upward water flux
kg/m2/s
sozotaux
zonal wind stress
Nm−2
8

D.1.3
LRA5
The variables for LRA5 consist of the following as described in Table S3.
Table S3: List of LRA5 reanalysis variables
Acronyms
Long Name
Units
asn
snow albedo
(0 - 1)
d2m
2-meter dewpoint temperature
K
e
total evaporation
m of water equivalent
es
snow evaporation
m of water equivalent
evabs
evaporation from bare soil
m of water equivalent
evaow
evaporation from open water
m of water equivalent
evatc
evaporation from top of canopy
m of water equivalent
evavt
evaporation from vegetation transpiration
m of water equivalent
fal
forecaste albedo
(0 - 1)
lai_hv
leaf area index, high vegetation
m2m−2
lai_lv
leaf area index, low vegetation
m2m−2
pev
potential evaporation
m
ro
runoff
m
rsn
snow density
kgm−3
sd
snow depth
m of water equivalent
sde
snow depth water equivalent
m
sf
snowfall
m of water equivalent
skt
skin temperature
K
slhf
surface latent heat flux
Jm−2
smlt
snowmelt
m of water equivalent
snowc
snowcover
%
sp
surface pressure
Pa
src
skin reservoir content
m of water equivalent
sro
surface runoff
m
sshf
surface sensible heat flux
Jm−2
ssr
net solar radiation
Jm−2
ssrd
download solar radiation
Jm−2
ssro
sub-surface runoff
m
stl1
soil temperature level 1
K
stl2
soil temperature level 2
K
stl3
soil temperature level 3
K
stl4
soil temperature level 4
K
str
net thermal radiation
Jm−2
strd
downward thermal radiation
Jm−2
swvl1
volumetric soil water layer 1
m3m−3
swvl2
volumetric soil water layer 2
m3m−3
swvl3
volumetric soil water layer 3
m3m−3
swvl4
volumetric soil water layer 4
m3m−3
t2m
2-meter temperature
K
tp
total precipitation
m
tsn
temperature of snow layer
K
u10
10-meter u-wind
ms−1
v10
10-meter v-wind
ms−1
9

D.2
Physics-Based Simulations
In this section, we describe in detail the physics-based models used as baselines in ChaosBench.
Wherever possible, we discuss specific strategies regarding coupling to the ocean, sea ice, wave, land,
initialization and perturbation strategies, specifications of initial/boundary conditions, as well as other
numerical considerations to generate forecast.
D.2.1
The UK Meteorological Office (UKMO) [33]
• Initialization and Ensemble. The UKMO model employs the lagged initialization strategy
to generate an ensemble of forecasts (4 in this case) at different initialization time to
improve prediction stability.
• Coupling with ocean is performed with the Global Ocean 6.0 model [51], based on
NEMO3.6 [52] with 0.25 degree horizontal resolution and 75 vertical pressure levels. The
ocean model is initialized and calibrated using Nonlinear Evolutionary Model VARiation
(NEMOVAR) [53], a specific data assimilation strategy that uses temperature, salinity
profiles, altimeter-derived sea level anomalies to calibrate forecasts. Frequency of coupling
is 1-hourly.
• Coupling with sea ice is performed with the Global Sea Ice 8.1 (CICE5.1.2) model [54],
and again initialized from NEMOVAR.
• Coupling with wave model is not yet operational.
• Coupling with land surface is performed with the Joint UK Land Environment Simulator
(JULES) [55]. Soil moisture, soil temperature, and snow are initialized using JULES and
forced using the the Japanese 55-year Reanalysis (JRA-55) data [56]. The land surface
model is paramaterized by land cover type from a combination of satellite (e.g., MODIS
LAI [57]) and radiometer data (e.g., AVHRR [58]). In addition, another parameterization in
the form of soil characteristics is derived from the Harmonized World Soil Database [59].
• Model grid uses the Arakawa C-grid [60] to solve partial differential equations on a
spherical surface. In particular, the velocity components (such as zonal and meridional
wind) are defined at the center of each face of the grid cells (in the case of a rectilinear grid)
or along cell edges (in the case of a curvilinear grid). The scalar quantities such as pressure
or temperature are computed at the corners of the grid cells.
• Large-scale dynamics uses the Semi-Lagrangian approach. It does not strictly follow fluid
parcels (i.e., Lagrangian), but it does calculate the value of a field, such as temperature (i.e.,
Eulerian) by tracing back along the trajectory that a fluid parcel would have taken to reach a
specific point at the current time step. This backward trajectory is used to find the origin of
the fluid parcel and determine its properties, which are then used to update the model fields.
This hybrid approach is therefore termed Semi-Lagrangian.
D.2.2
National Centers for Environmental Prediction (NCEP) [61]
• Initialization and Ensemble. The NCEP model adds small perturbation to the atmospheric,
oceanic and land analysis at each cycle across 4 ensemble to reduce sensitivity to initial
conditions.
• Coupling with ocean is performed with the GFDL Modular Ocean Model version
4 (MOM4) model that has a spatial resolution of 0.5-degree and 0.25-degree in the
longitude-latitude directions [62]. There are 40 vertical pressure levels.
10

• Coupling with sea ice is also performed with the GFDL Sea Ice Simulator (SIS), which
models the thermodynamics and overall dynamics of sea ice [62].
• Coupling with wave model is not yet operational.
• Coupling with land surface is performed with 4-layer Noah Land surface model 2.7.1
[63]. Soil moisture, soil temperature, and snow are initialized using Noah and forced using
the Climate Forecast System [61] and the Global Land Data Assimilation System [64]
reanalysis data. The land surface model is parameterized by land cover type AVHRR data.
In addition, another paramaterization in the form of soil characteristics is derived from the
world soil climate database [65].
• Model grid uses the Gaussian grid [66], where the longitude (x-axis) are evenly spaced
while the latitudes (y-axis) are not. Instead, they are determined by the roots of the
associated Legendre polynomials, which correspond to the Gaussian quadrature points
for the sphere. This ensures that the actual area represented by each grid cell is more uniform.
• Large-scale dynamics uses the Spectral approach. It solves partial differential equations
by transforming them from the physical space into the spectral domain. In the latter case,
the equations are transformed into a series of coefficients that represent the amplitude of
waves across scales. The transformations are usually done using Fourier series for periodic
domains or spherical harmonics when dealing with the whole Earth’s surface [66]. This
method is especially beneficial for smooth functions and for representing large-scale wave
phenomena, such as the Rossby waves, which are important for understanding weather and
climate.
D.2.3
China Meteorological Administration (CMA) [35]
• Initialization and Ensemble. The CMA model uses the lagged average forecasting (LAF)
method across 4 ensemble members to ensure that the mean forecast is less sensitive to
initial conditions.
• Coupling with ocean is performed with the GFDL MOM4 model, which has 40 vertical
pressure levels [62]. Frequency of coupling is 2-hourly.
• Coupling with sea ice is performed with the GFDL Sea Ice Simulator (SIS), similar to that
used by NCEP [62].
• Coupling with wave model is not yet operational.
• Coupling with land surface is performed with the Atmosphere-Vegetation Interaction
Model version 2 (AVIM2) model [67] and the NCAR NCAR Community Land Model
version 3.0 (CLMv3) [68]. Soil moisture, soil temperature, and snow are not initialized
directly using reanalysis data, as used by other land surface models. Rather, air-sea-land-ice
coupled model is forced by near-surface atmospheric and ocean reanalysis in a long-term
integration, and the land initial conditions are produced as a by-product. As a result, the
parameterization of land cover type is done by this process, while soil characteristics is
derived from the Harmonized World Soil Database [59].
• Model grid uses the Gaussian grid [66], similar to that used by the NCEP.
• Large-scale dynamics uses a mixture of Spectral approach for the vorticity, temperature,
and surface pressure, as well as Semi-Lagrangian for specific humidity and cloud waters
other tracers.
11

D.2.4
European Center for Medium-Range Weather Forecasts (ECMWF) [36]
• Initialization and Ensemble. The operational IFS forecast is generated through Singular
Vectors (SV) method: it creates a variety of initial conditions by adjusting certain parameters
slightly, thus generating different starting points.
• Coupling with ocean is performed with NEMO3.4.1 with 1-degree resolution and 42
vertical pressure levels. Frequency of coupling is 3-hourly.
• Coupling with sea ice is not operational for this model’s version (but it is in the newer
generation, though the forecast start-date is much later than 2016). As a result, sea ice initial
conditions are persisted up to day 15 and then relaxed to climatology up to day 45.
• Coupling with wave model is performed with ECMWF wave model with 0.5-degree
resolution [69].
• Coupling with land surface is relatively more complex than the rest, and we refer readers
to their documentation. Regardless, it is based on Land Data Assimilation System (LDAS)
that combines heterogenous high-quality dataset from satellite to ground sensors, and
integrated with the operational IFS model. The parameterization for land cover type is
primarily based on MODIS collection 5 [57] and soil characteristics from the FAO dominant
soil texture class [70].
• Model grid uses the Cubic Octohedral grid [71], where the Earth’s surface is projected
onto a cube. Then, the cube is further subdivided to form an octahedron, where the faces
represent finer grid cells. This multi-scale gridding scheme allows for parallelization where
processes at different scales could be solved simultaneously.
• Large-scale dynamics uses a mixture of Spectral and Semi-Lagrangian approach, similar
to that used by CMA.
12

E
Data-Driven Baseline Models
In this section, we describe in detail implementation and hyperparemeter selections of our data-driven
models used as baselines to ChaosBench. Most of the choices are based on the original works that are
adapted to weather and climate applications using similar input dataset. All training are performed
using 2x NVIDIA A100 GPUs.
E.1
Lagged Autoencoder (AE)
We implement lagged AE from [72] with 5 encoder blocks and 5 decoder block, with
detailed specification in Table S4.
Each encoder block is comprised of MAXPOOL2D ◦
(CONV2D →BATCHNORM2D →RELU →CONV2D →BATCHNORM2D →RELU).
Similarly, the decoder block is comprised of CONVTRANSPOSE2D →BACTNORM2D →
RELU) L(CONVTRANSPOSE2D →BACTNORM2D →SIGMOID) ◦(CONV2D).
Table S4: Hyperparameters for Lagged AE
Hyperparameters
Values
Channels
[64, 128, 256, 512, 1024]
Encoder Kernel
3 × 3
Decoder Kernel
2 × 2
Max Pooling Window
2 × 2
Batch Normalization
TRUE
Optimizer
ADAMW [73]
Learning Rate
COSINEANNEALING(10−2 →10−3)
Batch Size
32
Epochs
500
Tmax
500
E.2
ResNet
We adapt ResNet implementation from [42] using ResNet-50 as feature extractor and 5
decoder blocks, following specification in Table S5.
Each decoder block is composed of
CONVTRANSPOSE2D →BACTNORM2D →LEAKYRELU.
Table S5: Hyperparameters for ResNet
Hyperparameters
Values
Backbone
RESNET-50
Decoder Channels
[1024, 512, 256, 128, 64]
Decoder Activation
LEAKYRELU(0.15)
Optimizer
ADAMW
Learning Rate
COSINEANNEALING(10−2 →10−3)
Batch Size
32
Epochs
500
Tmax
500
E.3
UNet
We adapt UNet implementation from [22] using 5 encoder and 5 decoder blocks, with skip connec-
tions, following specification in Table S6. The composition of the encoder and decoder components
are similar to those described for Lagged Autoencoder, with the addition of SKIP connection between
each corresponding contracting-expansive path.
13

Table S6: Hyperparameters for UNet
Hyperparameters
Values
Channels
[64, 128, 256, 512, 1024]
Activation
LEAKYRELU(0.15)
Encoder Kernel
3 × 3
Decoder Kernel
2 × 2
Max Pooling Window
2 × 2
Optimizer
ADAMW
Learning Rate
COSINEANNEALING(10−2 →10−3)
Batch Size
32
Epochs
500
Tmax
500
E.4
Fourier Neural Operator (FNO)
We adapt FNO implementation from [43], following specification in Table S7 and illustrated in
S1. We implement the encoder-decoder structure, where we (1) first transform our input Xt by
convolutional layers both in the Fourier (applying fast fourier transform; FFT) and physical domains,
before we concatenate both (applying inverse FFT for the former convolved features), and apply
non-linear GELU activation function [74]. We select only the first 4 main Fourier modes to make
the number of trainable parameters comparable with the other data-driven baseline models. The (2)
decoder block then applies deconvolutional operation to the latent features to generate output Yt.
Table S7: Hyperparameters for FNO
Hyperparameters
Values
Non-Spectral Channels
[64, 128, 256, 512, 1024]
Spectral Channel
[64, 128, 256, 512, 1024]
Activation
GELU
Fourier Modes
(4,4)
Optimizer
ADAMW
Learning Rate
COSINEANNEALING(10−2 →10−3)
Batch Size
32
Epochs
500
Tmax
500
Xt
FFT = H(Xt)
Spectral
Conv2D
IFFT = H-1H(Xt)
Conv2D
Wt
Activation:
GeLU
Encoder Block (x4)
Decoder Block (x4)
Yt
Figure S1: FNO architecture: (1) in the encoder block, we transform our input Xt by convolutional
layers both in the Fourier and physical domains, before we concatenate and apply non-linear GELU
activation function. The (2) decoder block is then applying deconvolutional operation to the latent
features to generate forecast Yt.
14

E.5
ClimaX
ClimaX is based on the ViT model [75] with variational positional embedding in variable-time space.
We use ClimaX model as is described and implemented in the original paper and is pre-trained using
CMIP6 [17]. We fine-tune the original pre-trained model given our training setup.
E.6
PanguWeather, FourCastNetV2, GraphCast
We
perform
inference
using
their
latest
checkpoints
using
the
API
provided
here:
https://github.com/ecmwf-lab/ai-models.
For this work, we process the forecasts at biweekly temporal resolution. In the codebase, we provide
the script for further flexibility, for instance:
# Process
biweekly , 1.5- degree
forecasts
for the year 2022
## Panguweather
$ python
scripts/process_sota.py --model_name
panguweather
--years
2022
## Graphcast
$ python
scripts/process_sota.py --model_name
graphcast
--years
2022
## FourCastNetV2
$ python
scripts/process_sota.py --model_name
fourcastnetv2
--years
2022
15

F
Evaluation Metrics
F.1
Deterministic Metrics
We describe in detail the four primary vision-based metrics used for this benchmark, including RMSE,
Bias, ACC, and MS-SSIM.
F.1.1
Root Mean-Squared Error (RMSE)
As described in the main text, we apply latitude-adjustment to RMSE computation.
MRMSE =
v
u
u
t
1
|θ||γ|
|θ|
X
i=1
|γ|
X
j=1
w(θi)( ˆYi,j −Yi,j)2
(S1)
F.1.2
Bias
Similarly, we apply latitude-adjustment to Bias computation.
MBias =
1
|θ||γ|
|θ|
X
i=1
|γ|
X
j=1
w(θi)( ˆYi,j −Yi,j)
(S2)
F.1.3
Anomaly Correlation Coefficient (ACC)
We remove the indexing for a more compact representation where the summation is performed
over each grid cell (i, j). The predicted and observed anomalies at each grid-cell are denoted by
A ˆYi,j = ˆYi,j −C and AYi,j = Yi,j −C, where C is the observational climatology. We apply
latitude-adjustment to ACC computation.
MACC =
P w(θ)[A ˆY · AY]
qP w(θ)A2
ˆY
P w(θ)A2
Y
(S3)
F.1.4
Multi-scale Structural Similarity Index Measure (MS-SSIM)
Let Y and ˆY be two images to be compared, and let µY, σ2
Y and σY ˆY be the mean of Y, the variance
of Y, and the covariance of Y and ˆY, respectively. The luminance, contrast and structure comparison
measures are defined as follows:
l(Y, ˆY) = 2µYµ ˆY + C1
µ2
Y + µ2
ˆY + C1
,
(S4)
c(Y, ˆY) = 2σYσ ˆY + C2
σ2
Y + σ2
ˆY + C2
,
(S5)
s(Y, ˆY) = σY ˆY + C3
σYσ ˆY + C3
,
(S6)
where C1, C2 and C3 are constants given by
C1 = (K1L)2, C2 = (K2L)2, and C3 = C2/2.
(S7)
L = 255 is the dynamic range of the gray scale images, and K1 ≪1 and K2 ≪1 are two small
constants. To compute the MS-SSIM metric across multiple scales, the images are successively
low-pass filtered and down-sampled by a factor of 2. We index the original image as scale 1, and the
desired highest scale as scale M. At each scale, the contrast comparison and structure comparison
16

are computed and denoted as cj(Y, ˆY) and sj(Y, ˆY) respectively. The luminance comparison is
only calculated at the last scale M, denoted by lM(Y, ˆY). Then, the MS-SSIM metric is defined by
MMS−SSIM = [lM(Y, ˆY)]αM ·
M
Y
j=1
[cj(Y, ˆY)]βj[sj(Y, ˆY)]γj
(S8)
where αM, βj and γj are parameters. We use the same set of parameters as in [37]: K1 = 0.01,
K2 = 0.03, M = 5, α5 = β5 = γ5 = 0.1333, β4 = γ4 = 0.2363, β3 = γ3 = 0.3001,
β2 = γ2 = 0.2856, β1 = γ = 0.0448. The predicted and ground truth images of physical variables
are re-scaled to 0-255 prior to the calculation of their MS-SSIM values.
F.2
Physics-Based Metrics
In this section, we describe in detail the definition and implementation of our physics-based metrics,
including PYTORCH psuedocode implementation.
Let Y be a 2D image of size h × w for a physical variables at a specific time, variable, and level. Let
f(x, y) be the intensity of the pixel at position (x, y). First, we compute the 2D Fourier transform of
the image by
F(kx, ky) =
w−1
X
x=0
h−1
X
y=0
f(x, y) · e−2πi(kxx/w+kyy/h),
(S9)
where kx and ky correspond to the wavenumber components in the horizontal and vertical directions,
respectively, and i is the imaginary unit. The power at each wavenumber component (kx, ky) is given
by the square of the magnitude spectrum of F(kx, ky), that is,
S(kx, ky) = |F(kx, ky)|2 = Re[F(kx, ky)]2 + Im[F(kx, ky)]2.
(S10)
The scalar wavenumber is defined as:
k =
q
k2x + k2y,
(S11)
which represents the magnitude of the spatial frequency vector, indicating how rapidly features change
spatially regardless of direction. Then, the energy distribution at a spatial frequency corresponding to
k is defined as
S(k) =
X
(kx,ky):√
k2x+k2y=k
S(kx, ky).
(S12)
Given the spatial energy frequency distribution for observations E(k) and predictions ˆS(k) , we
perform normalization for each over Kq, the set of wavenumbers corresponding to high-frequency
components of energy distribution, as defined in Equation S13. This is to ensure that the sum of the
component sums up to 1 which exhibits pdf-like property.
S′(k) =
S(k)
P
k∈Kq S(k),
ˆS′(k) =
ˆS(k)
P
k∈Kq ˆS(k)
,
k ∈Kq
(S13)
17

import
torch
import
torch.nn as nn
class
SpectralDiv(nn.Module):
"""
Compute
Spectral
divergence
given the top -k percentile
wavenumber
(higher k means
higher
frequency)
"""
def
__init__(
self ,
percentile =0.9,
input_shape =(121 ,240)
):
super(SpectralDiv , self).__init__ ()
self.percentile = percentile
# Compute
the
discrete
Fourier
Transform
sample
frequencies
for a signal of size
nx , ny = input_shape
kx = torch.fft.fftfreq(nx) * nx
ky = torch.fft.fftfreq(ny) * ny
kx , ky = torch.meshgrid(kx , ky)
# Construct
discretized k-bins
self.k = specify_k_bins (...)
# Get k-percentile
index
self. k_percentile_idx = int(len(self.k) * self.percentile)
def
forward(self , predictions , targets):
# Preprocess data , including
handling of missing
values , etc
predictions = preprocess_data (...)
targets = preprocess_data (...)
# Compute
along mini -batch
predictions , targets = torch.nanmean(predictions , dim =0),
torch.nanmean(targets , dim =0)
# Transform
prediction
and
targets
onto the
Fourier
space and
compute
the power
predictions_power = torch.fft.fft2(predictions)
predictions_power = torch.abs( predictions_power )**2
targets_power = torch.fft.fft2(targets)
targets_power = torch.abs( targets_power )**2
# Normalize as pdf
predictions_Sk = predictions_power / torch.nansum(
predictions_power )
targets_Sk = targets_power / torch.nansum( targets_power )
# Compute
spectral Sk divergence
div = torch.nansum(targets_Sk * torch.log(torch.clamp(
targets_Sk / predictions_Sk , min=1e-9)))
return div
Listing S1: Psuedocode for computing SpecDiv using PYTORCH
18

import
torch
import
torch.nn as nn
class
SpectralRes(nn.Module):
"""
Compute
Spectral
residual
given the top -k percentile
wavenumber (
higher k means
higher
frequency)
"""
def
__init__(
self ,
percentile =0.9,
input_shape =(121 ,240)
):
super(SpectralRes , self).__init__ ()
self.percentile = percentile
# Compute
the
discrete
Fourier
Transform
sample
frequencies
for a signal of size
nx , ny = input_shape
kx = torch.fft.fftfreq(nx) * nx
ky = torch.fft.fftfreq(ny) * ny
kx , ky = torch.meshgrid(kx , ky)
# Construct
discretized k-bins
self.k = specify_k_bins (...)
# Get k-percentile
index
self. k_percentile_idx = int(len(self.k) * self.percentile)
def
forward(self , predictions , targets):
# Preprocess data , including
handling of missing
values , etc
predictions = preprocess_data (...)
targets = preprocess_data (...)
# Compute
along mini -batch
predictions , targets = torch.nanmean(predictions , dim =0),
torch.nanmean(targets , dim =0)
# Transform
prediction
and
targets
onto the
Fourier
space and
compute
the power
predictions_power = torch.fft.fft2(predictions)
predictions_power = torch.abs( predictions_power )**2
targets_power = torch.fft.fft2(targets)
targets_power = torch.abs( targets_power )**2
# Normalize as pdf
predictions_Sk = predictions_power / torch.nansum(
predictions_power )
targets_Sk = targets_power / torch.nansum( targets_power )
# Compute
spectral Sk residual
res = torch.sqrt(torch.nanmean(torch.square( predictions_Sk -
targets_Sk)))
return res
Listing S2: Psuedocode for computing SpecRes using PYTORCH
19

F.3
Probabilistic Metrics
Here, we broadly define n ∈N as an ensemble member, and N ∈R the total number of ensemble
members.
F.3.1
Deterministic Extension
This includes the ensemble version of deterministic and physics-based metrics, including RMSE,
Bias, ACC, MS-SSIM, SpecDiv, and SpecRes.
Mens
RMSE = 1
N
N
X
n=1
Mn
RMSE
(S14)
Mens
Bias = 1
N
N
X
n=1
Mn
Bias
(S15)
Mens
ACC = 1
N
N
X
n=1
Mn
ACC
(S16)
Mens
MS−SSIM = 1
N
N
X
n=1
Mn
MS−SSIM
(S17)
Mens
SpecDiv = 1
N
N
X
n=1
Mn
SpecDiv
(S18)
Mens
SpecRes = 1
N
N
X
n=1
Mn
SpecRes
(S19)
F.3.2
CRPS
CRPS measures the accuracy of probabilistic forecasts by integrating the square of the difference
between the cumulative distribution function (CDF) of the forecast and the CDF of the observed
data over all possible outcomes. It can be thought of as probabilistic MAE, where a smaller value is
desirable and a deterministic forecast reduces to MAE. We first apply latitude-adjustments for the
forecasts and target fields.
MCRP S(F, x) =
Z ∞
−∞
(F(y) −H(y −x))2 dy
(S20)
where F(y) is the CDF of the forecast, H(y −x) is the Heaviside step function at the observed value
x, and y ranges over all possible outcomes.
F.3.3
CRPSS
CRPSS measures the skillfulness of an ensemble forecasts, with positive being skillful, zero unskilled,
and negative being worse than baseline climatology.
MCRP SS = 1 −
CRPSforecast
CRPSclimatology
(S21)
20

F.3.4
Spread
We apply latitude-adjusted spread of the ensemble members, and std is the standard deviation
operator.
MSpread =
1
|θ||γ|
|θ|
X
i=1
|γ|
X
j=1
std

{w(θi) ˆYn
i,j}N
n=1

(S22)
F.3.5
Spread/Skill Ratio (SSR)
We use ensemble RMSE as the skill in the SSR computation.
MSSR = MSpread
Mens
RMSE
(S23)
21

G
Extended Results
We provide extended results accompanying the main text.
0
10
20
30
40
Number of days ahead
1
2
3
4
5
6
RMSE [K]
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
RMSE [gpm]
×102
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
2.5
3.0
RMSE [10
3kgkg
1]
×10
3
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(b) ACC (↑is better)
0
10
20
30
40
Number of days ahead
2.0
1.5
1.0
0.5
0.0
0.5
1.0
BIAS [K]
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
40
30
20
10
0
10
20
30
40
BIAS [gpm]
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
4
2
0
2
4
BIAS [10
3kgkg
1]
×10
4
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(c) Bias (→0 is better)
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0
10
20
30
40
Number of days ahead
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
SSIM
q-700
Climatology
ECMWF
CMA
UKMO
NCEP
(d) MS-SSIM (↑is better)
Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
t-850
Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
z-500
Climatology
ECMWF
CMA
UKMO
NCEP
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
q-700
(e) SpecDiv (↓is better)
Figure S2: Evaluation results between baseline climatology (black line) and physics-based control
(deterministic) forecasts. At longer forecasting horizon, most physics-based deterministic forecasts
perform worse than climatology while maintaining structures as evidenced from their low SpecDiv
(barring NCEP).
22

0
10
20
30
40
Number of days ahead
1
2
3
4
5
6
RMSE [K]
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
RMSE [gpm]
×102
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
2.5
3.0
RMSE [10
3kgkg
1]
×10
3
q-700
Climatology
PW
GC
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.2
0.0
0.2
0.4
0.6
0.8
1.0
ACC
q-700
Climatology
PW
GC
(b) ACC (↑is better)
0
10
20
30
40
Number of days ahead
2.0
1.5
1.0
0.5
0.0
0.5
1.0
BIAS [K]
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
40
30
20
10
0
10
20
30
40
BIAS [gpm]
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
4
2
0
2
4
BIAS [10
3kgkg
1]
×10
4
q-700
Climatology
PW
GC
(c) Bias (→0 is better)
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
t-850
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
z-500
Climatology
PW
GC
FCN2
0
10
20
30
40
Number of days ahead
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
SSIM
q-700
Climatology
PW
GC
(d) MS-SSIM (↑is better)
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
t-850
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
z-500
Climatology
PW
GC
FCN2
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
q-700
(e) SpecDiv (↓is better)
Figure S3: Evaluation results between baseline climatology (black line) and data-driven models
including PanguWeather (PW), FourCastNetV2 (FCN2), and GraphCast (GC). Overall, we observe
that data-driven models perform significantly worse than climatology on S2S timescale. They also
perform poorly on physics-based metrics indicating the lack of predictive power on multi-scale
structures. Note: FCN2 lacks q-700 and climatology naturally has low SpecDiv (direct observations).
23

0
10
20
30
40
Number of days ahead
1.0
1.5
2.0
2.5
3.0
3.5
4.0
RMSE [K]
t-850
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.2
0.4
0.6
0.8
1.0
RMSE [gpm]
×102
z-500
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.75
1.00
1.25
1.50
1.75
2.00
RMSE [10
3kgkg
1]
×10
3
q-700
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
ACC
t-850
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
ACC
z-500
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
ACC
q-700
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(b) ACC (↑is better)
0
10
20
30
40
Number of days ahead
0.70
0.75
0.80
0.85
0.90
0.95
1.00
SSIM
t-850
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.80
0.85
0.90
0.95
1.00
SSIM
z-500
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.5
0.6
0.7
0.8
0.9
SSIM
q-700
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(c) MS-SSIM (↑is better)
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
t-850
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.0
0.1
0.2
0.3
0.4
0.5
SDIV
z-500
Climatology
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.00
0.05
0.10
0.15
SDIV
q-700
(d) SpecDiv (↓is better)
Figure S4: Evaluation results between baseline climatology (black line) and physics-based ensembles
from ECMWF, CMA, UKMO, NCEP. Overall, we observe that ensemble forecasts perform better
than their deterministic counterparts.
24

0
10
20
30
40
Number of days ahead
0.75
0.80
0.85
0.90
0.95
1.00
RMSE [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.7
0.8
0.9
1.0
1.1
1.2
RMSE [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.75
0.80
0.85
0.90
0.95
1.00
RMSE [ens/det]
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(a) RMSE: ensemble improves deterministic forecasts if ratio < 1
0
10
20
30
40
Number of days ahead
5
0
5
10
15
20
ACC [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
80
60
40
20
0
20
40
ACC [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
1.0
1.5
2.0
2.5
3.0
3.5
ACC [ens/det]
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(b) ACC: ensemble improves deterministic forecasts if ratio > 1
0
10
20
30
40
Number of days ahead
1.00
1.02
1.04
1.06
1.08
1.10
1.12
SSIM [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
1.00
1.02
1.04
1.06
1.08
1.10
1.12
1.14
SSIM [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
1.0
1.1
1.2
1.3
SSIM [ens/det]
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(c) MS-SSIM: ensemble improves deterministic forecasts if ratio > 1
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.0
0.2
0.4
0.6
0.8
1.0
SDIV [ens/det]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.00
0.25
0.50
0.75
1.00
SDIV [ens/det]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0.00
0.25
0.50
0.75
1.00
1.25
SDIV [ens/det]
q-700
(d) SpecDiv: ensemble improves deterministic forecasts if ratio < 1
Figure S5: Metrics ratio e.g., RMSEens/RMSEdet between ensemble and deterministic forecasts, where
the former improves the latter by accounting for IC uncertainty that can lead to long-range instability
and trajectory divergences. Note: n represents the number of ensemble members. The ratio for ACC
fluctuates as the scalar value approaches 0.
25

0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
2.5
CRPS [K]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
10
20
30
40
50
CRPS [gpm]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.4
0.6
0.8
1.0
1.2
1.4
CRPS [10
3kgkg
1]
×10
3
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(a) CRPS (↓is better)
0
10
20
30
40
Number of days ahead
0.4
0.2
0.0
0.2
0.4
0.6
CRPSS
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.4
0.2
0.0
0.2
0.4
0.6
0.8
CRPSS
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.6
0.4
0.2
0.0
0.2
0.4
0.6
CRPSS
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(b) CRPSS (> 0 is better)
0
10
20
30
40
Number of days ahead
0.0
0.5
1.0
1.5
2.0
2.5
3.0
SPREAD [K]
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0
10
20
30
40
50
60
SPREAD [gpm]
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.00
0.25
0.50
0.75
1.00
1.25
1.50
SPREAD [10
3kgkg
1]
×10
3
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(c) Spread
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
SSR
t-850
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
SSR
z-500
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
SSR
q-700
ECMWF (n=50)
CMA (n=3)
UKMO (n=3)
NCEP (n=15)
(d) SSR (> 0 is better)
Figure S6: Probabilistic evaluation on ensemble forecasts indicating current skill limits of 15 days.
Note: n represents the number of ensemble members.
26

G.1
Effects of Different Autoregressive Training Steps; lead_time
We showcased more results for autoregressive training strategy. In this case, we performed autoregres-
sive training using either 1 or 5 iterative steps (n_step; s). As illustrated in Figure S7, we observe
that incorporating temporal information improve the vision-based metrics even at longer forecasting
timesteps, with lower RMSE, higher MS-SSIM. However, the converse trend is true incorporating
temporal context makes S2S forecast worse off in some physics-based scores. The modified loss
function for training a model with multiple autoregressive steps is:
L = 1
|S|
s
X
i=1
L( ˆYt+siYt+si), ∀si ∈S
(S24)
Here S = {1, · · · , s} and s ∈N+ is the autoregressive steps. For this work, we set s = 5.
0
10
20
30
40
Number of days ahead
2
3
4
5
6
RMSE [K]
t-850
S=1
S=5
0
10
20
30
40
Number of days ahead
25
50
75
100
125
RMSE [gpm]
z-500
S=1
S=5
0
10
20
30
40
Number of days ahead
1.0
1.5
2.0
2.5
RMSE [10
3kgkg
1]
q-700
S=1
S=5
(a) RMSE (↓is better)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
SSIM
t-850
S=1
S=5
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
SSIM
z-500
S=1
S=5
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
SSIM
q-700
S=1
S=5
(b) MS-SSIM (↑is better)
0
10
20
30
40
Number of days ahead
0.4
0.6
0.8
SDIV
t-850
S=1
S=5
0
10
20
30
40
Number of days ahead
1.0
1.2
1.4
SDIV
z-500
S=1
S=5
0
10
20
30
40
Number of days ahead
0.5
1.0
1.5
2.0
SDIV
q-700
S=1
S=5
(c) SpecDiv (↓is better)
Figure S7: Ablation results for incorporating temporal information in an autoregressive scheme for
long-range forecast using UNet models. The x-axis represents the number of forecasting days for
t-850, z-500, q-700 representative tasks. Blue and orange lines illustrate autoregressive scheme with
s = 1 and s = 5 respectively. Overall we observe that incorporating temporal information improve
the vision-based metrics even at longer forecasting timesteps. However, the converse trend is true
where incorporating temporal context makes S2S forecast worse off in some physics-based scores.
27

G.2
Effects of Subset Optimization; headline_vars
In many cases, we seek to train data-driven models so that they are able to perform well on all states
by optimizing for the full state of the next forecasting timestep t + 1, that is,
ϕ∗= argmin
ϕ
L( ˆYt+1, Yt+1)
where L is any loss function. This task is especially useful for building emulators that act as surrogates
for the more expensive physics-based NWP models [17].
Although the first task is useful for learning the full complex interaction between variables, it is
relatively difficult due to the intrinsic high-dimensionality of the data. As a result, we introduce a
second task that allows for the optimization on a subset of variables of interest (Y′ ∈Y):
ϕ∗= argmin
ϕ
L( ˆY′
t+1, Y′
t+1)
Here Y′
t+1 = {t-850, z-500, q-700}, and we train them using 5 autoregressive steps i.e., n_step = 5.
Table S8: Long-range forecasting (∆t = 44) results on select metrics and target variables between
physics-based and data-driven models. Results are for Task 1 (full) and Task 2 (sparse). (*) Baseline
model that uses privileged information (observations) to make prediction.
RMSE ↓
MS-SSIM ↑
SpecDiv ↓
Models
T850
(K)
Z500
(gpm)
Q700
(×10−3)
T850
Z500
Q700
T850
Z500
Q700
Climatology*
3.39
81.0
1.62
0.85
0.82
0.62
0.01
0.01
0.03
Persistence*
5.88
127.8
2.47
0.71
0.69
0.41
0.02
0.03
0.05
UKMO
5.00
116.2
2.32
0.64
0.71
0.43
0.06
0.09
0.07
NCEP
4.90
116.7
2.30
0.75
0.71
0.43
0.53
0.55
0.10
CMA
5.08
118.7
2.49
0.75
0.72
0.45
0.05
0.04
0.06
ECMWF
4.72
115.1
2.30
0.75
0.72
0.44
0.06
0.07
0.06
Task 1: Full Dynamics Prediction
Lagged AE
5.55
122.4
2.03
0.74
0.71
0.47
0.18
2.44
0.21
ResNet
5.67
125.3
2.07
0.73
0.70
0.47
0.21
0.37
0.26
UNet
5.47
121.5
2.13
0.73
0.71
0.45
0.30
1.16
2.20
FNO
5.06
112.5
1.95
0.75
0.73
0.51
0.18
0.11
0.10
Task 2: Sparse Dynamics Prediction
Lagged AE
5.39
119.0
2.12
0.75
0.73
0.48
0.52
1.41
0.29
ResNet
5.80
124.1
2.18
0.74
0.72
0.46
0.33
1.22
0.09
UNet
5.57
120.2
2.18
0.74
0.71
0.45
1.20
1.08
0.07
FNO
4.73
101.8
1.91
0.79
0.76
0.52
0.18
0.23
0.21
Overall, we find models that attempt to preserve spectral structures (e.g., FNO) perform better on all
metrics, deterministic and physics-based. Also, Task 2 (sparse) appears to be easier than Task 1 (full).
Nonetheless, they are still performing worse than climatology.
28

G.3
Effects of Ensemble Forecasts
This section provides additional results for data-driven ensemble approach, and follow similar
evaluation process as the physics-based counterpart.
0
10
20
30
40
Number of days ahead
0.7
0.8
0.9
1.0
1.1
RMSE [ens/det]
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.70
0.75
0.80
0.85
0.90
0.95
RMSE [ens/det]
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.80
0.85
0.90
0.95
1.00
RMSE [ens/det]
q-700
UNet (n=5)
ResNet (n=5)
(a) RMSE: ensemble improves deterministic forecasts if ratio < 1
0
10
20
30
40
Number of days ahead
1.5
1.0
0.5
0.0
0.5
ACC [ens/det]
×102
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.0
0.2
0.4
0.6
0.8
1.0
1.2
ACC [ens/det]
×103
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
50
40
30
20
10
0
10
ACC [ens/det]
q-700
UNet (n=5)
ResNet (n=5)
(b) ACC: ensemble improves deterministic forecasts if ratio > 1
0
10
20
30
40
Number of days ahead
1.000
1.025
1.050
1.075
1.100
1.125
1.150
SSIM [ens/det]
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
1.00
1.05
1.10
1.15
1.20
SSIM [ens/det]
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
1.00
1.05
1.10
1.15
1.20
1.25
1.30
SSIM [ens/det]
q-700
UNet (n=5)
ResNet (n=5)
(c) MS-SSIM: ensemble improves deterministic forecasts if ratio > 1
UNet (n=5)
ResNet (n=5)
0.0
0.2
0.4
0.6
0.8
1.0
1.2
SDIV [ens/det]
t-850
UNet (n=5)
ResNet (n=5)
0.0
0.2
0.4
0.6
SDIV [ens/det]
z-500
UNet (n=5)
ResNet (n=5)
0.0
0.5
1.0
1.5
SDIV [ens/det]
q-700
(d) SpecDiv: ensemble improves deterministic forecasts if ratio < 1
Figure S8: Metrics ratio e.g., RMSEens/RMSEdet between ensemble and deterministic forecasts, where
the former improves the latter by accounting for IC uncertainty that can lead to long-range instability
and trajectory divergences. Note: n represents the number of ensemble members.
29

0
10
20
30
40
Number of days ahead
1.0
1.5
2.0
2.5
3.0
3.5
CRPS [K]
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
10
20
30
40
50
60
CRPS [gpm]
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.6
0.7
0.8
0.9
1.0
1.1
1.2
CRPS [10
3kgkg
1]
×10
3
q-700
UNet (n=5)
ResNet (n=5)
(a) CRPS (↓is better)
0
10
20
30
40
Number of days ahead
1.00
0.75
0.50
0.25
0.00
0.25
0.50
CRPSS
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.75
0.50
0.25
0.00
0.25
0.50
0.75
CRPSS
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.4
0.2
0.0
0.2
CRPSS
q-700
UNet (n=5)
ResNet (n=5)
(b) CRPSS (> 0 is better)
0
10
20
30
40
Number of days ahead
1
2
3
4
5
6
7
SPREAD [K]
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.2
0.4
0.6
0.8
1.0
SPREAD [gpm]
×102
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.25
0.50
0.75
1.00
1.25
1.50
1.75
SPREAD [10
3kgkg
1]
×10
3
q-700
UNet (n=5)
ResNet (n=5)
(c) Spread
0
10
20
30
40
Number of days ahead
0.4
0.6
0.8
1.0
1.2
1.4
SSR
t-850
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.4
0.6
0.8
1.0
SSR
z-500
UNet (n=5)
ResNet (n=5)
0
10
20
30
40
Number of days ahead
0.3
0.4
0.5
0.6
0.7
0.8
SSR
q-700
UNet (n=5)
ResNet (n=5)
(d) SSR (> 0 is better)
Figure S9: Probabilistic evaluation on ensemble forecasts. Note: n represents the number of ensemble
members.
30

G.4
Power Spectra
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
t-850
1-day ahead
44-day ahead
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
z-500
1-day ahead
44-day ahead
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
q-700
1-day ahead
44-day ahead
(a) Task 1
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
t-850
1-day ahead
44-day ahead
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
z-500
1-day ahead
44-day ahead
100
101
102
Wavenumber, k
100
102
104
106
Power, S(k)
q-700
1-day ahead
44-day ahead
(b) Task 2
Figure S10: Power spectra for ViT/ClimaX demonstrating energy decay/divergence especially for
high k as lead time grows.
31

G.5
Qualitative Evaluation
Task 1 (S=5)
 norm-t850
 t = 1
Task 1 (S=5)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (S=5)
 norm-t850
 t = 1
Task 2 (S=5)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S11: Normalized t@850-hpa qualitative results for UNet-autoregressive (S=5).
Task 1 (S=5)
 norm-z500
 t = 1
Task 1 (S=5)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (S=5)
 norm-z500
 t = 1
Task 2 (S=5)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S12: Normalized z@500-hpa qualitative results for UNet-autoregressive (S=5).
Task 1 (S=5)
 norm-q700
 t = 1
Task 1 (S=5)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (S=5)
 norm-q700
 t = 1
Task 2 (S=5)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S13: Normalized q@700-hpa qualitative results for UNet-autoregressive (S=5).
32

Task 1 (Direct)
 norm-t850
 t = 1
Task 1 (Direct)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-t850
 t = 1
Task 2 (Direct)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S14: Normalized t@850-hpa qualitative results for UNet-direct.
Task 1 (Direct)
 norm-z500
 t = 1
Task 1 (Direct)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-z500
 t = 1
Task 2 (Direct)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S15: Normalized z@500-hpa qualitative results for UNet-direct.
Task 1 (Direct)
 norm-q700
 t = 1
Task 1 (Direct)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-q700
 t = 1
Task 2 (Direct)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S16: Normalized q@700-hpa qualitative results for UNet-direct.
33

Task 1 (Direct)
 norm-t850
 t = 1
Task 1 (Direct)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-t850
 t = 1
Task 2 (Direct)
 norm-t850
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S17: Normalized t@850-hpa qualitative results for ClimaX-direct.
Task 1 (Direct)
 norm-z500
 t = 1
Task 1 (Direct)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-z500
 t = 1
Task 2 (Direct)
 norm-z500
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S18: Normalized z@500-hpa qualitative results for ClimaX-direct.
Task 1 (Direct)
 norm-q700
 t = 1
Task 1 (Direct)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(a) Task 1
Task 2 (Direct)
 norm-q700
 t = 1
Task 2 (Direct)
 norm-q700
 t = 44
2
0
2
2
0
2
1
0
1
2
0
2
2
0
2
1
0
1
Truth
Prediction
Residual
(b) Task 2
Figure S19: Normalized q@700-hpa qualitative results for ClimaX-direct.
34

