Optimal Learners for Realizable Regression:
PAC Learning and Online Learning
Idan Attias
Ben-Gurion University of the Negev
idanatti@post.bgu.ac.il
Steve Hanneke
Purdue University
steve.hanneke@gmail.com
Alkis Kalavasis
Yale University
alvertos.kalavasis@yale.edu
Amin Karbasi
Yale University, Google Research
amin.karbasi@yale.edu
Grigoris Velegkas
Yale University
grigoris.velegkas@yale.edu
Abstract
In this work, we aim to characterize the statistical complexity of realizable regres-
sion both in the PAC learning setting and the online learning setting.
Previous work had established the sufficiency of finiteness of the fat shattering di-
mension for PAC learnability and the necessity of finiteness of the scaled Natarajan
dimension, but little progress had been made towards a more complete character-
ization since the work of Simon (SICOMP â€™97). To this end, we first introduce
a minimax instance optimal learner for realizable regression and propose a novel
dimension that both qualitatively and quantitatively characterizes which classes of
real-valued predictors are learnable. We then identify a combinatorial dimension
related to the Graph dimension that characterizes ERM learnability in the realizable
setting. Finally, we establish a necessary condition for learnability based on a
combinatorial dimension related to the DS dimension, and conjecture that it may
also be sufficient in this context.
Additionally, in the context of online learning we provide a dimension that charac-
terizes the minimax instance optimal cumulative loss up to a constant factor and
design an optimal online learner for realizable regression, thus resolving an open
question raised by Daskalakis and Golowich in STOC â€™22.
1
Introduction
Real-valued regression is one of the most fundamental and well-studied problems in statistics and
data science [Vap99, GBC16, Bac21], with numerous applications in domains such as economics
and medicine [DG17]. However, despite its significance and applicability, theoretical understanding
of the statistical complexity of real-valued regression is still lacking.
Perhaps surprisingly, in the fundamental realizable Probably Approximately Correct (PAC) setting
[Val84] and the realizable online setting [Lit88, DG22], we do not know of any characterizing
dimension or optimal learners for the regression task. This comes in sharp contrast with binary and
multiclass classification, both in the offline and the online settings, where the situation is much more
clear [Lit88, HLW94, BEHW89, Han16, DSS14, BCD+22, Nat89, DSBDSS15, RST23]. Our goal
in this work is to make progress regarding the following important question:
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Which dimensions characterize PAC and online learnability for realizable real-valued regression?
Consider an instance space ğ’³, label space ğ’´= [0, 1] and hypothesis class â„‹âŠ†[0, 1]ğ’³. In this work,
we focus on the case of the regression framework with respect to the absolute loss â„“(ğ‘¥, ğ‘¦) â‰œ|ğ‘¥âˆ’ğ‘¦|
for any ğ‘¥, ğ‘¦âˆˆ[0, 1], following previous works [BLW94, Sim97, ABDCBH97, BL98, DG22]. Our
qualitative results hold for more general losses, namely any approximate pseudo-metric loss, which
includes common losses such as â„“ğ‘for any ğ‘â‰¥1. See the formal statements in Appendix H.
PAC/Offline Realizable Regression.
Let us first recall the definition of realizable real-valued
regression in the PAC setting. Informally, the learner is given i.i.d. labeled examples drawn from an
unknown distribution ğ’Ÿwith the promise that there exists a target hypothesis â„â‹†âˆˆâ„‹that perfectly
labels the data. The goal is to use this training set to design a predictor with small error on future
examples from the same distribution. Note that given a learner ğ´and sample ğ‘†âˆ¼ğ’Ÿğ‘›, we let ğ´(ğ‘†; ğ‘¥)
be its prediction on ğ‘¥âˆˆğ’³when the training set is ğ‘†.
Definition 1 (PAC Realizable Regression). Let â„“: [0, 1]2 â†’Râ‰¥0 be the absolute loss function.
Consider a class â„‹âŠ†[0, 1]ğ’³for some domain ğ’³. Let â„â‹†âˆˆâ„‹be an unknown target function and
let ğ’Ÿğ’³be an unknown distribution on ğ’³. A random sample ğ‘†of size ğ‘›consists of points ğ‘¥1, . . . , ğ‘¥ğ‘›
drawn i.i.d. from ğ’Ÿğ’³and the corresponding values â„â‹†(ğ‘¥1), . . . , â„â‹†(ğ‘¥ğ‘›) of the target function.
â€¢ An algorithm ğ´: (ğ’³Ã— [0, 1])ğ‘›â†’[0, 1]ğ’³is an ğ‘›-sample PAC learner for â„‹with respect
to â„“if, for all 0 < ğœ€, ğ›¿< 1, there exists ğ‘›= ğ‘›(ğœ€, ğ›¿) âˆˆN such that for any â„â‹†âˆˆâ„‹and any
domain distribution ğ’Ÿğ’³, it holds that Eğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥))] â‰¤ğœ€, with probability at
least 1 âˆ’ğ›¿over ğ‘†âˆ¼ğ’Ÿğ‘›
ğ’³.
â€¢ An algorithm ğ´: (ğ’³Ã— [0, 1])ğ‘›â†’[0, 1]ğ’³is an ğ‘›-sample cut-off PAC learner for â„‹with
respect to â„“if, for all 0 < ğœ€, ğ›¿, ğ›¾< 1, there exists ğ‘›= ğ‘›(ğœ€, ğ›¿, ğ›¾) âˆˆN such that for any
â„â‹†âˆˆâ„‹and any domain distribution ğ’Ÿğ’³, it holds that Prğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾] â‰¤
ğœ€, with probability at least 1 âˆ’ğ›¿over ğ‘†âˆ¼ğ’Ÿğ‘›
ğ’³.
We remark that these two PAC learning definitions are qualitatively equivalent (cf. Lemma 1). We
note that, throughout the paper, we implicitly assume, as e.g., in [HKS19], that all hypothesis classes
are admissible in the sense that they satisfy mild measure-theoretic conditions, such as those specified
in [DKLD84] (Section 10.3.1) or [Pol12] (Appendix C).
The question we would like to understand in this setting (and was raised by [Sim97]) follows:
Question 1. Can we characterize learnability and design minimax optimal PAC learners for realiz-
able regression?
Traditionally, in the context of statistical learning a minimax optimal learner ğ´â‹†is one
that, for every class â„‹, given an error parameter ğœ€and a confidence parameter ğ›¿requires
minğ´maxğ’Ÿâ„³ğ´(â„‹; ğœ€, ğ›¿, ğ’Ÿ) samples to achieve it, where â„³ğ´(â„‹; ğœ€, ğ›¿, ğ’Ÿ) is the number of samples
that some learner ğ´requires to achieve error ğœ€with confidence ğ›¿when the data-generating distribution
is ğ’Ÿ. There seems to be some inconsistency in the literature about realizable regression, where some
works present minimax optimal learners when the maximum is also taken over the hypothesis class
â„‹, i.e., minğ´maxğ’Ÿ,â„‹â„³ğ´(â„‹, ğœ€, ğ›¿, ğ’Ÿ). This is a weaker result compared to ours, since it shows that
there exists some hypothesis class for which these learners are optimal.
Our main results in this setting, together with the uniform convergence results from prior work, give
rise to an interesting landscape of realizable PAC learning that is depicted in Figure 1.
Online Realizable Regression.
We next shift our attention to the classical setting of online learning
where the learner interacts with the adversary over a sequence of ğ‘‡rounds: in every round the
adversary presents an example ğ‘¥ğ‘¡âˆˆğ’³, the learner predicts a label Ì‚ï¸€ğ‘¦ğ‘¡and then the adversary reveals
the correct label ğ‘¦â‹†
ğ‘¡. In this context, realizability means that there always exists some function
â„ğ‘¡âˆˆâ„‹that perfectly explains the examples and the labels that the adversary has chosen. In the
agnostic setting, the goal of the learner is to compete with the performance of the best function
in â„‹, i.e., achieve small regret. This setting was introduced by [Lit88] in the context of binary
classification, where they also characterized learnability in the realizable setting and provided an
optimal algorithm. Later, [BDPSS09] provided an almost optimal algorithm in the agnostic setting,
which suffered from an additional log ğ‘‡factor in its regret bound. This extra factor was later shaved
2

Figure 1: Landscape of Realizable PAC Regression: the â€œdeletedâ€ arrows mean that the implication
is not true. The equivalence between finite fat-shattering dimension and the uniform convergence
property is known even in the realizable case (see [SSSSS10]) and the fact that PAC learnability
requires finite scaled Natarajan dimension is proved in [Sim97]. The properties of the other three
dimensions (scaled Graph dimension, scaled One-Inclusion-Graph (OIG) dimension, and scaled
Daniely-Shalev Shwartz (DS) dimension) are shown in this work. We further conjecture that finite
scaled Natajaran dimension is not sufficient for PAC learning, while finite scaled DS does suffice.
Interestingly, we observe that the notions of uniform convergence, learnability by any ERM and PAC
learnability are separated in realizable regression.
by [ABED+21]. The study of multiclass online classification was initiated by [DSBDSS15], who
provided an optimal algorithm for the realizable setting and an algorithm that is suboptimal by
a factor of log ğ‘˜Â· log ğ‘‡in the agnostic setting, where ğ‘˜is the total number of labels. Recently,
[RST23] shaved off the log ğ‘˜factor. The problem of online regression differs significantly from that
of online classification, since the loss function is not binary. The agnostic online regression setting
has received a lot of attention and there is a series of works that provides optimal minimax guarantees
[RST10, RS14, RST15b, RST15a, BDR21].
To the best of our knowledge, the realizable setting has received much less attention. A notable excep-
tion is the work of [DG22] that focuses on realizable online regression using proper1 learners. They
provide an optimal regret bound with respect to the sequential fat-shattering dimension. However, as
they mention in their work (cf. Examples 1 and 2), this dimension does not characterize the optimal
cumulative loss bound.
Interestingly, Daskalakis and Golowich [DG22] leave the question of providing a dimension that
characterizes online realizable regression open. In our work, we resolve this question by providing
upper and lower bounds for the cumulative loss of the learner that are tight up to a constant factor of
2 using a novel combinatorial dimension that is related to (scaled) Littlestone trees (cf. Definition 13).
Formally, the setting of realizable online regression is defined as follows:
Definition 2 (Online Realizable Regression). Let â„“: [0, 1]2 â†’Râ‰¥0 be a loss function. Consider
a class â„‹âŠ†[0, 1]ğ’³for some domain ğ’³. The realizable online regression setting over ğ‘‡rounds
consists of the following interaction between the learner and the adversary:
â€¢ The adversary presents ğ‘¥ğ‘¡âˆˆğ’³.
â€¢ The learner predicts Ì‚ï¸€ğ‘¦ğ‘¡âˆˆ[0, 1], possibly using randomization.
â€¢ The adversary reveals the true label ğ‘¦â‹†
ğ‘¡âˆˆ[0, 1] with the constraint that âˆƒâ„â‹†
ğ‘¡âˆˆâ„‹, âˆ€ğœâ‰¤
ğ‘¡, â„(ğ‘¥ğœ) = ğ‘¦â‹†
ğœ.
â€¢ The learner suffers loss â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦â‹†
ğ‘¡).
The goal of the learner is to minimize its expected cumulative loss Cğ‘‡= E
[ï¸âˆ‘ï¸€
ğ‘¡âˆˆ[ğ‘‡] â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦â‹†
ğ‘¡)
]ï¸
.
We remark that in the definition of the cumulative loss Cğ‘‡, the expectation is over the randomness of
the algorithm (which is the only stochastic aspect of the online setting). As we explained before, the
main question we study in this setting is the following:
1A learner is proper when the predictions Ì‚ï¸€ğ‘¦ğ‘¡can be realized by some function â„ğ‘¡âˆˆâ„‹.
3

Question 2. Can we characterize the optimal cumulative loss and design optimal online learners for
realizable regression?
Our main result in this setting provides a dimension that characterizes the optimal cumulative loss up
to a factor of 2 and provides an algorithm that achieves this bound.
1.1
Related Work
Our work makes progress towards the characterization of realizable regression both in the offline
and online settings. Similar results are known for the more studied settings of binary and mul-
ticlass (PAC/online) classification [Lit88, HLW94, BEHW89, Han16, DSS14, BCD+22, Nat89,
DSBDSS15, RBR09, BLM20, RST23, ABED+21, HKLM22, KKMV23, BGH+23]. The funda-
mental works of [BLW94, Sim97, ABDCBH97, BL98] study the question of PAC learnability for the
regression task. However, none of them characterizes learnability in the realizable setting. [Sim97]
showed that finiteness of scaled Natarajan dimension is necessary for realizable PAC regression.
[BL98] employed the one-inclusion graph (OIG) algorithm to get a real-valued predictor whose
expected error is upper bounded by the ğ‘‰ğ›¾-dimension, whose finiteness is sufficient but not necessary
for realizable learnability in this setting. We refer to [KS23] for details about this dimension which
was introduced in [ABDCBH97]. [ABDCBH97] showed that finiteness of the fat shattering dimen-
sion at all scales is equivalent to PAC learnability in the agnostic setting. Nevertheless, this does
not hold in the realizable case as Example 1 demonstrates. Similarly, the work of [BLW94] shows
that fat shattering dimension characterizes the regression task when the labels are corrupted by noise.
Recently, the concurrent and independent work of [AACSZ23] provides high-probability bounds for
the one-inclusion graph algorithm in the realizable PAC regression setting using the ğ‘‰ğ›¾-dimension.
We underline that this dimension does not characterize realizable regression. For more general losses,
see [Men02, BBM05].
In the area of online regression, the work of [DG22] studies the realizable setting of online regression
with the absolute loss (as we do) and presents a randomized proper learning algorithm which achieves
a near-optimal cumulative loss in terms of the sequential fat-shattering dimension of the hypothesis
class. We emphasize that this dimension does not tightly capture this setting and, hence, does
not address the question that we study. In particular, the lower bound they provide is related to
the unimprovability of a bound concerning the sequential fat-shattering dimension and does not
tightly capture the complexity of the problem. In the more general agnostic case, regret bounds
have been obtained in the work of [RST15a], using the sequential fat-shattering dimension and
sequential covering numbers. These quantities are also not tight in the realizable setting. See
also [RS14, RST15b, RST17]. Moreover, regression oracles have been used in contextual bandits
problems, see [FR20, SLX22] for further details.
Our characterization (cf. Theorem 2) for the offline setting via the OIG algorithm [HLW94] is further
motivated by the work of [MHS22], where they propose a dimension of similar flavor for adversarially
robust PAC learnability. We mention that recently the OIG algorithm has received a lot of attention
from the statistical learning theory community [AACSZ22, BHM+21, KVK22, SMB22, AHHM22,
CP22]. Finally, for a small sample of works that deal with offline and online regression problems, see
[BDGR22, Gol21, AH22, HKS19, She17, MKFI22] and the references therein.
2
PAC Learnability for Realizable Regression
In this section, we present various combinatorial dimensions that provide necessary or sufficient
conditions for learnability of real-valued functions. All the definitions that we consider have a similar
flavor. We first define what it means for a class â„‹to â€œshatterâ€ a set of ğ‘›points and then we define the
dimension to be equal to the cardinality of the largest set that â„‹can shatter. Moreover, since we are
considering real-valued learning, these dimensions are parameterized by a scaling factor ğ›¾âˆˆ(0, 1)
which should be interpreted as the distance that we can get to the optimal function. We start with the
standard notion of projection of a class to a set of unlabeled examples.
Definition 3 (Projection of â„‹to ğ‘†). Given ğ‘†= {ğ‘¥1, . . . , ğ‘¥ğ‘›} âˆˆğ’³ğ‘›, the projection of â„‹âŠ†ğ’´ğ’³to
ğ‘†is â„‹|ğ‘†= {(â„(ğ‘¥1), . . . , â„(ğ‘¥ğ‘›)) : â„âˆˆâ„‹}.
Furthermore, we say that a labeled sample ğ‘†âˆˆ(ğ’³Ã— [0, 1])ğ‘›is realizable with respect to â„‹if there
exists â„âˆˆâ„‹such that â„(ğ‘¥ğ‘–) = ğ‘¦ğ‘–, âˆ€ğ‘–âˆˆ[ğ‘›].
4

2.1
ğ›¾-Fat Shattering Dimension
Perhaps the most well-known dimension in the real-valued learning setting is the fat shattering
dimension that was introduced in [KS94]. Its definition is inspired by the pseudo-dimension [Pol90]
and it is, essentially, a scaled version of it.
Definition 4 (ğ›¾-Fat Shattering Dimension [KS94]). Let â„‹âŠ†[0, 1]ğ’³. We say that a sample ğ‘†âˆˆğ’³ğ‘›
is ğ›¾-fat shattered by â„‹if there exist ğ‘ 1, . . . , ğ‘ ğ‘›âˆˆ[0, 1]ğ‘›such that for all ğ‘âˆˆ{0, 1}ğ‘›there exists
â„ğ‘âˆˆâ„‹such that:
â€¢ â„ğ‘(ğ‘¥ğ‘–) â‰¥ğ‘ ğ‘–+ ğ›¾, âˆ€ğ‘–âˆˆ[ğ‘›] such that ğ‘ğ‘–= 1.
â€¢ â„ğ‘(ğ‘¥ğ‘–) â‰¤ğ‘ ğ‘–âˆ’ğ›¾, âˆ€ğ‘–âˆˆ[ğ‘›] such that ğ‘ğ‘–= 0.
The ğ›¾-fat shattering dimension Dfat
ğ›¾
is defined to be the maximum size of a ğ›¾-fat shattered set.
In the realizable setting, finiteness of the fat-shattering dimension (at all scales) is sufficient for
learnability and it is equivalent to uniform convergence2 [BLW94]. However, the next example shows
that it is not a necessary condition for learnability. This comes in contrast to the agnostic case for
real-valued functions, in which learnability and uniform convergence are equivalent for all â„‹.
Example 1 (Realizable Learnability â‡Finite Fat-Shattering Dimension, see Section 6 in [BLW94]).
Consider a class â„‹âŠ†[0, 1]ğ’³where each hypothesis is uniquely identifiable by a single example, i.e.,
for any ğ‘¥âˆˆğ’³and any ğ‘“, ğ‘”âˆˆâ„‹we have that ğ‘“(ğ‘¥) Ì¸= ğ‘”(ğ‘¥), unless ğ‘“â‰¡ğ‘”. Concretely, for every ğ‘‘âˆˆ
N, let {ğ‘†ğ‘—}0â‰¤ğ‘—â‰¤ğ‘‘âˆ’1 be a partition of ğ’³and define â„‹ğ‘‘=
{ï¸€
â„ğ‘0,...,ğ‘ğ‘‘âˆ’1 : ğ‘ğ‘–âˆˆ{0, 1}, 0 â‰¤ğ‘–â‰¤ğ‘‘âˆ’1
}ï¸€
,
where
â„ğ‘0,...,ğ‘ğ‘‘âˆ’1(ğ‘¥) = 3
4
ğ‘‘âˆ’1
âˆ‘ï¸
ğ‘—=0
1ğ‘†ğ‘—(ğ‘¥)ğ‘ğ‘—+ 1
8
ğ‘‘âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘ğ‘˜2âˆ’ğ‘˜.
For any ğ›¾â‰¤1/4, Dfat
ğ›¾(â„‹ğ‘‘) = ğ‘‘, since for a set of points ğ‘¥0, . . . , ğ‘¥ğ‘‘âˆ’1 âˆˆğ’³such that each ğ‘¥ğ‘—belongs
to ğ‘†ğ‘—, 0 â‰¤ğ‘—â‰¤ğ‘‘âˆ’1, â„‹ğ‘‘contains all possible patterns of values above 3/4 and below 1/4. Indeed,
it is not hard to verify that for any ğ‘—âˆˆ{0, . . . , ğ‘‘âˆ’1} if we consider any â„â€² := â„ğ‘0,...,ğ‘ğ‘‘âˆ’1 âˆˆâ„‹ğ‘‘
with ğ‘ğ‘—= 1 we have â„â€²(ğ‘¥ğ‘—) â‰¥3/4. Similarly, if ğ‘ğ‘—= 0 then it holds that â„â€²(ğ‘¥ğ‘—) â‰¤1/4. Hence,
Dfat
ğ›¾(âˆªğ‘‘âˆˆNâ„‹ğ‘‘) = âˆ. Nevertheless, by just observing one example (ğ‘¥, â„â‹†(ğ‘¥)) any ERM learner finds
the exact labeling function â„â‹†.
We remark that this example already shows that the PAC learnability landscape of regression is quite
different from that of multiclass classification, where agnostic learning and realizable learning are
characterized by the same dimension [BCD+22].
To summarize this subsection, the fat-shattering dimension is a natural way to quantify how well the
function class can interpolate (with gap ğ›¾) some fixed function. Crucially, this interpolation contains
only inequalities (see Definition 4) and hence (at least intuitively) cannot be tight for the realizable
setting, where there exists some function that exactly labels the features. Example 1 gives a natural
example of a class with infinite fat-shattering dimension that can, nevertheless, be learned with a
single sample in the realizable setting.
2.2
ğ›¾-Natarajan Dimension
The ğ›¾-Natarajan dimension was introduced by [Sim97] and is inspired by the Natarajan dimension
[Nat89], which has been used to derive bounds in the multiclass classification setting. Before
explaining the ğ›¾-Natarajan dimension, let us begin by reminding to the reader the standard Natarajan
dimension. We say that a set ğ‘†= {ğ‘¥1, ..., ğ‘¥ğ‘›} of size ğ‘›is Natarajan-shattered by a concept class
â„‹âŠ†ğ’´ğ’³if there exist two functions ğ‘“, ğ‘”: ğ‘†â†’ğ’´so that ğ‘“(ğ‘¥ğ‘–) Ì¸= ğ‘”(ğ‘¥ğ‘–) for all ğ‘–âˆˆ[ğ‘›], and for all
ğ‘âˆˆ{0, 1}ğ‘›there exists â„âˆˆâ„‹such that â„(ğ‘¥ğ‘–) = ğ‘“(ğ‘¥ğ‘–) if ğ‘ğ‘–= 1 and â„(ğ‘¥ğ‘–) = ğ‘”(ğ‘¥ğ‘–) if ğ‘ğ‘–= 0. Note
that here we have equalities instead of inequalities (recall the fat-shattering case Definition 4).
From a geometric perspective (see [BCD+22]), this means that the space â„‹projected on the set
ğ‘†contains the set {ğ‘“(ğ‘¥1), ğ‘”(ğ‘¥1)} Ã— ... Ã— {ğ‘“(ğ‘¥ğ‘›), ğ‘”(ğ‘¥ğ‘›)}. This set is "isomorphic" to the Boolean
2Informally, uniform convergence means that for all distributions ğ’Ÿ, with high probability over the sample,
the error of all â„âˆˆâ„‹on the sample is close to their true population error.
5

hypercube of size ğ‘›by mapping ğ‘“(ğ‘¥ğ‘–) to 1 and ğ‘”(ğ‘¥ğ‘–) to 0 for all ğ‘–âˆˆ[ğ‘›]. This means that the
Natarajan dimension is essentially the size of the largest Boolean cube contained in â„‹.
[Sim97] defines the scaled analogue of the above dimension as follows.
Definition 5 (ğ›¾-Natarajan Dimension [Sim97]). Let â„‹âŠ†[0, 1]ğ’³. We say that a set ğ‘†âˆˆğ’³ğ‘›is
ğ›¾-Natarajan-shattered by â„‹if there exist ğ‘“, ğ‘”: [ğ‘›] â†’[0, 1] such that for every ğ‘–âˆˆ[ğ‘›] we have
â„“(ğ‘“(ğ‘–), ğ‘”(ğ‘–)) â‰¥2ğ›¾, and
â„‹|ğ‘†âŠ‡{ğ‘“(1), ğ‘”(1)} Ã— . . . Ã— {ğ‘“(ğ‘›), ğ‘”(ğ‘›)} .
The ğ›¾-Natarajan dimension DNat
ğ›¾
is defined to be the maximum size of a ğ›¾-Natarajan-shattered set.
Intuitively, one should think of the ğ›¾-Natarajan dimension as indicating the size of the largest Boolean
cube that is contained in â„‹. Essentially, every coordinate ğ‘–âˆˆ[ğ‘›] gets its own translation of the 0, 1
labels of the Boolean cube, with the requirement that these two labels are at least 2ğ›¾far from each
other. [Sim97] showed the following result, which states that finiteness of the Natarajan dimension at
all scales is a necessary condition for realizable PAC regression:
Informal Theorem 1 (Theorem 3.1 in [Sim97]). â„‹âŠ†[0, 1]ğ’³is PAC learnable in the realizable
regression setting only if DNat
ğ›¾
(â„‹) < âˆfor all ğ›¾âˆˆ(0, 1).
Concluding these two subsections, we have explained the main known general results about realizable
offline regression: (i) finiteness of fat-shattering is sufficient but not necessary for PAC learning and
(ii) finiteness of scaled Natarajan is necessary for PAC learning.
2.3
ğ›¾-Graph Dimension
We are now ready to introduce the ğ›¾-graph dimension, which is a relaxation of the definition of the
ğ›¾-Natarajan dimension. To the best of our knowledge, it has not appeared in the literature before. Its
definition is inspired by its non-scaled analogue in multiclass classification [Nat89, DSS14].
Definition 6 (ğ›¾-Graph Dimension). Let â„‹âŠ†[0, 1]ğ’³, â„“: R2 â†’[0, 1]. We say that a sample ğ‘†âˆˆğ’³ğ‘›
is ğ›¾-graph shattered by â„‹if there exists ğ‘“: [ğ‘›] :â†’[0, 1] such that for all ğ‘âˆˆ{0, 1}ğ‘›there exists
â„ğ‘âˆˆâ„‹such that:
â€¢ â„ğ‘(ğ‘¥ğ‘–) = ğ‘“(ğ‘–), âˆ€ğ‘–âˆˆ[ğ‘›] such that ğ‘ğ‘–= 0.
â€¢ â„“(â„ğ‘(ğ‘¥ğ‘–), ğ‘“(ğ‘–)) > ğ›¾, âˆ€ğ‘–âˆˆ[ğ‘›] such that ğ‘ğ‘–= 1.
The ğ›¾-graph dimension DG
ğ›¾is defined to be the maximum size of a ğ›¾-graph shattered set.
We mention that the asymmetry in the above definition is crucial. In particular, replacing the equality
â„ğ‘(ğ‘¥ğ‘–) = ğ‘“(ğ‘–) with â„“(â„ğ‘(ğ‘¥ğ‘–), ğ‘“(ğ‘–)) â‰¤ğ›¾fails to capture the properties of the graph dimension.
Intuitively, the equality in the definition reflects the assumption of realizability, i.e., the guarantee
that there exists a hypothesis â„â‹†that exactly fits the labels. Before stating our main result, we can
collect some useful observations about this new combinatorial measure. In particular, we provide
examples inspired by [DSS14, DSBDSS15] which show (i) that there exist gaps between different
ERM learners (see Example 3) and (ii) that any learning algorithm with a close to optimal sample
complexity must be improper (see Example 4).
Our first main result relates the scaled graph dimension with the learnability of any class â„‹âŠ†[0, 1]ğ’³
using a (worst case) ERM learner. This result is the scaled analogue of known multiclass results
[DSS14, DSBDSS15] but its proof for the upper bound follows a different path. For the formal
statement of our result and its full proof, we refer the reader to Appendix B.
Informal Theorem 2 (Informal, see Theorem 1). Any â„‹âŠ†[0, 1]ğ’³is PAC learnable in the realizable
regression setting by a worst-case ERM learner if and only if DG
ğ›¾(â„‹) < âˆfor all ğ›¾âˆˆ(0, 1).
Proof Sketch. The proof of the lower bound follows in a similar way as the lower bound regarding
learnability of binary hypothesis classes that have infinite VC dimension [VC71, BEHW89]. If â„‹
has infinite ğ›¾-graph dimension for some ğ›¾âˆˆ(0, 1), then for any ğ‘›âˆˆN we can find a sequence of
ğ‘›points ğ‘¥1, . . . , ğ‘¥ğ‘›that are ğ›¾-graph shattered by â„‹. Then, we can define a distribution ğ’Ÿthat puts
most of its mass on ğ‘¥1, so if the learner observes ğ‘›samples then with high probability it will not
observe at least half of the shattered points. By the definition of the ğ›¾-graph dimension this shows
6

that, with high probability, there exists at least one ERM learner that is ğ›¾-far on at least half of these
points. The result follows by taking ğ‘›â†’âˆ.
The most technically challenging part of our result is the upper bound. There are three main steps in
our approach. First, we argue that by introducing a â€œghostâ€ sample of size ğ‘›on top of the training
sample, which is also of size ğ‘›, we can bound the true error of any ERM learner on the distribution
by twice its error on the ghost sample. This requires a slightly more subtle treatment compared to the
argument of [BEHW89] for binary classification. The second step is to use a â€œrandom swapsâ€ type of
argument in order to bound the performance of the ERM learner on the ghost sample as follows: we
â€œmergeâ€ these two samples by considering any realizable sequence ğ‘†of 2ğ‘›points, we randomly swap
elements whose indices differ by ğ‘›, and then we consider an ERM learner who gets trained on the
first ğ‘›points. We can show that if such a learner does not make many mistakes on the unseen part of
the sequence, in expectation over the random swaps, then it has a small error on the true distribution.
The main advantage of this argument is that it allows us to bound the number of mistakes of such a
learner on the unseen part of the sequence without using any information about ğ’Ÿ. The last step of the
proof is where we diverge from the argument of [BEHW89]. In order to show that this expectation is
small, we first map â„‹to a partial concept class3 â„‹[AHHM22], then we project â„‹to the sequence
ğ‘†, and finally we map â„‹back to a total concept class by considering a disambiguation of it. Through
these steps, we can show that there will be at most (2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›)) many different functions in
this â€œprojectedâ€ class. Then, we can argue that, with high probability, any ERM learner who sees the
first half of the sequence makes, in expectation over the random swaps, a small number of mistakes on
the second half of the sequence. Finally, we can take a union bound over all the (2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›))
possible such learners to conclude the proof.
2.4
ğ›¾-One-Inclusion Graph Dimension
In this section, we provide a minimax optimal learner for realizable PAC regression. We first review
a fundamental construction which is a crucial ingredient in the design of our learner, namely the
one-inclusion (hyper)graph (OIG) algorithm AOIG for a class â„‹âŠ†ğ’´ğ’³[HLW94, RBR09, DSS14,
BCD+22]. This algorithm gets as input a training set (ğ‘¥1, ğ‘¦1), ..., (ğ‘¥ğ‘›, ğ‘¦ğ‘›) realizable by â„‹and an
additional example ğ‘¥. The goal is to predict the label of ğ‘¥. Let ğ‘†= {ğ‘¥1, ..., ğ‘¥ğ‘›, ğ‘¥}. The idea is to
construct the one-inclusion graph ğºOIG
â„‹|ğ‘†induced by the pair (ğ‘†, â„‹). The node set ğ‘‰of this graph
corresponds to the set â„‹|ğ‘†(projection of â„‹to ğ‘†) and, so, ğ‘‰âŠ†ğ’´[ğ‘›+1]. For the binary classification
case, two vertices are connected with an edge if they differ in exactly one element ğ‘¥of the ğ‘›+ 1
points in ğ‘†. For the case where ğ’´is discrete and |ğ’´| > 2, the hyperedge is generalized accordingly.
Definition 7 (One-Inclusion Hypergraph [HLW94, RBR09, BCD+22]). Consider the set [ğ‘›] and
a hypothesis class â„‹âŠ†ğ’´[ğ‘›]. We define a graph ğºOIG
â„‹
= (ğ‘‰, ğ¸) such that ğ‘‰= â„‹. Consider a
direction ğ‘–âˆˆ[ğ‘›] and a mapping ğ‘“: [ğ‘›] âˆ–{ğ‘–} â†’ğ’´. We introduce the hyperedge ğ‘’ğ‘–,ğ‘“= {â„âˆˆğ‘‰:
â„(ğ‘—) = ğ‘“(ğ‘—), âˆ€ğ‘—âˆˆ[ğ‘›] âˆ–{ğ‘–}}. We define the edge set of ğºOIG
â„‹
to be the collection
ğ¸= {ğ‘’ğ‘–,ğ‘“: ğ‘–âˆˆ[ğ‘›], ğ‘“: [ğ‘›] âˆ–{ğ‘–} â†’ğ’´, ğ‘’ğ‘–,ğ‘“Ì¸= âˆ…} .
In the regression setting, having created the one-inclusion graph with ğ’´= [0, 1], the goal is to orient
the edges; the crucial property is that â€œgoodâ€ orientations of this graph yield learning algorithms with
low error. An orientation is good if the maximum out-degree of the graph is small (cf. Definition 8).
Informally, if the maximum out-degree of any node is ğ‘€, then we can create a predictor whose
expected error rate is at most ğ‘€/(ğ‘›+ 1). Note that in the above discussion we have not addressed
the issue that we deal with regression tasks and not classification and, hence, some notion of scale is
required in the definition of the out-degree.
Intuitively, the set ğ‘†that induces the vertices â„‹|ğ‘†of the OIG consists of the features of the training
examples {ğ‘¥1, ..., ğ‘¥ğ‘›} and the test point ğ‘¥. Hence, each edge of the OIG should be thought of as
the set of all potential labels of the test point that are realizable by â„‹, so it corresponds to the set
of all possible meaningful predictions for ğ‘¥. An orientation ğœmaps every edge ğ‘’to a vertex ğ‘£âˆˆğ‘’
and, hence, it is equivalent to the prediction of a learning algorithm. We can now formally define the
notion of an orientation and the scaled out-degree.
3For an introduction to partial concept classes and their disambiguations we refer the reader to Appendix B.1.
7

Definition 8 (Orientation and Scaled Out-Degree). Let ğ›¾âˆˆ[0, 1], ğ‘›âˆˆN, â„‹âŠ†[0, 1][ğ‘›]. An
orientation of the one-inclusion graph ğºOIG
â„‹
= (ğ‘‰, ğ¸) is a mapping ğœ: ğ¸â†’ğ‘‰so that ğœ(ğ‘’) âˆˆğ‘’for
any ğ‘’âˆˆğ¸. Let ğœğ‘–(ğ‘’) âˆˆ[0, 1] denote the ğ‘–-th entry of the orientation.
For a vertex ğ‘£âˆˆğ‘‰, corresponding to some hypothesis â„âˆˆâ„‹(see Definition 7), let ğ‘£ğ‘–be the ğ‘–-th entry
of ğ‘£, which corresponds to â„(ğ‘–). The (scaled) out-degree of a vertex ğ‘£under ğœis outdeg(ğ‘£; ğœ, ğ›¾) =
|{ğ‘–âˆˆ[ğ‘›] : â„“(ğœğ‘–(ğ‘’ğ‘–,ğ‘£), ğ‘£ğ‘–) > ğ›¾}|. The maximum (scaled) out-degree of ğœis outdeg(ğœ, ğ›¾) =
maxğ‘£âˆˆğ‘‰outdeg(ğ‘£; ğœ, ğ›¾).
Finally, we introduce the following novel dimension in the context of real-valued regression. An
analogous dimension was proposed in the context of learning under adversarial robustness [MHS22].
Definition 9 (ğ›¾-OIG Dimension). Consider a class â„‹âŠ†[0, 1]ğ’³and let ğ›¾âˆˆ[0, 1]. We define the
ğ›¾-one-inclusion graph dimension DOIG
ğ›¾
of â„‹as follows:
DOIG
ğ›¾
(â„‹) = sup{ğ‘›âˆˆN : âˆƒğ‘†âˆˆğ’³ğ‘›such that âˆƒfinite subgraph ğº= (ğ‘‰, ğ¸) of ğºOIG
â„‹|ğ‘†= (ğ‘‰ğ‘›, ğ¸ğ‘›)
such that âˆ€orientations ğœ, âˆƒğ‘£âˆˆğ‘‰, where outdeg(ğ‘£; ğœ, ğ›¾) > ğ‘›/3} .
We define the dimension to be infinite if the supremum is not attained by a finite ğ‘›.
In words, it is the largest ğ‘›âˆˆN (potentially âˆ) such that there exists an (unlabeled) sequence ğ‘†of
length ğ‘›with the property that no matter how one orients some finite subgraph of the one-inclusion
graph, there is always some vertex for which at least 1/3 of its coordinates are ğ›¾-different from the
labels of the edges that are attached to this vertex. We remark that the hypothesis class in Example 1
has ğ›¾-OIG dimension equal to ğ‘‚(1). Moreover, a finite fat-shattering dimension of hypothesis class
implies a finite OIG dimension of roughly the same size (see Appendix C). We also mention that the
above dimension satisfies the â€œfinite characterâ€ property and the remaining criteria that dimensions
should satisfy according to [BDHM+19] (see Appendix F). As our main result in this section, we
show that any class â„‹is learnable if and only if this dimension is finite and we design an (almost)
optimal learner for it.
Informal Theorem 3 (Informal, see Theorem 2). Any â„‹âŠ†[0, 1]ğ’³is PAC learnable in the realizable
regression setting if and only if DOIG
ğ›¾
(â„‹) < âˆfor all ğ›¾âˆˆ(0, 1).
The formal statement and its full proof are postponed to Appendix C.
Proof Sketch. We start with the lower bound. As we explained before, orientations of the one-
inclusion graph are, in some sense, equivalent to learning algorithms. Therefore, if this dimension is
infinite for some ğ›¾> 0, then for any ğ‘›âˆˆN, there are no orientations with small maximum out-degree.
Thus, for any learner we can construct some distribution ğ’Ÿunder which it makes a prediction that is
ğ›¾-far from the correct one with constant probability, which means that â„‹is not PAC learnable.
Let us now describe the proof of the converse direction, which consists of several steps. First,
notice that the finiteness of this dimension provides good orientations for finite subgraphs of the one-
inclusion graph. Using the compactness theorem of first-order logic, we can extend them to a good
orientation of the whole, potentially infinite, one-inclusion graph. This step gives us a weak learner
with the following property: for any given ğ›¾there is some ğ‘›0 âˆˆN so that, with high probability
over the training set, when it is given ğ‘›0 examples as its training set it makes mistakes that are of
order at least ğ›¾on a randomly drawn point from ğ’Ÿwith probability at most 1/3. The next step is
to boost the performance of this weak learner. This is done using the â€œmedian-boostingâ€ technique
[KÃ©g03] (cf. Algorithm 2) which guarantees that after a small number of iterations we can create an
ensemble of weak learners such that, a prediction rule according to their (weighted) median will not
make any ğ›¾-mistakes on the training set. However, this is not sufficient to prove that the ensemble of
these learners has small loss on the distribution ğ’Ÿ. This is done by establishing sample compression
schemes that have small length. Essentially, such schemes consist of a compression function ğœ…which
takes as input a training set and outputs a subset of it, and a reconstruction function ğœŒwhich takes as
input the output of ğœ…and returns a predictor whose error on every point of the training set ğ‘†is at most
ğ›¾. Extending the arguments of [LW86] from the binary setting to the real-valued setting we show
that the existence of such a scheme whose compression function returns a set of â€œsmallâ€ cardinality
implies generalization properties of the underlying learning rule. Finally, we show that our weak
learner combined with the boosting procedure admit such a sample compression scheme.
Before proceeding to the next section, one could naturally ask whether there is a natural property of
the concept class that implies finiteness of the scaled OIG dimension. The work of [Men02] provides
8

a sufficient and natural condition that implies finiteness of our complexity measure. In particular,
Mendelson shows that classes that contain functions with bounded oscillation (as defined in [Men02])
have finite fat-shattering dimension. This implies that the class is learnable in the agnostic setting
and hence is also learnable in the realizable setting. As a result, the OIG-based dimension is also
finite. So, bounded oscillations are a general property that guarantees that the finiteness of OIG-based
dimension and fat-shattering dimension coincide. We also mention that deriving bounds for the
OIG-dimension for interesting families of functions is an important yet non-trivial question.
2.5
ğ›¾-DS Dimension
So far we have identified a dimension (cf. Definition 9) that characterizes the PAC learnability of
realizable regression. However, it might not be easy to calculate it in some settings. Our goal in
this section is to introduce a relaxation of this definition which we conjecture that also characterizes
learnability in this context. This new dimension is inspired by the DS dimension, a combinatorial
dimension defined by Daniely and Shalev-Shwartz in [DSS14]. In a recent breakthrough result,
[BCD+22] showed that the DS dimension characterizes multiclass learnability (with a possibly
unbounded number of labels and the 0-1 loss). We introduce a scaled version of the DS dimension.
To this end, we first define the notion of a scaled pseudo-cube.
Definition 10 (Scaled Pseudo-Cube). Let ğ›¾âˆˆ[0, 1]. A class â„‹âŠ†[0, 1]ğ‘‘is called a ğ›¾-pseudo-cube
of dimension ğ‘‘if it is non-empty, finite and, for any ğ‘“âˆˆâ„‹and direction ğ‘–âˆˆ[ğ‘‘], the hyper-edge
ğ‘’ğ‘–,ğ‘“= {ğ‘”âˆˆâ„‹: ğ‘”(ğ‘—) = ğ‘“(ğ‘—) âˆ€ğ‘—âˆˆ[ğ‘‘], ğ‘–Ì¸= ğ‘—} satisfies |ğ‘’ğ‘–,ğ‘“| > 1 and â„“(ğ‘”1(ğ‘–), ğ‘”2(ğ‘–)) > ğ›¾for any
ğ‘”1, ğ‘”2 âˆˆğ‘’ğ‘–,ğ‘“, ğ‘”1 Ì¸= ğ‘”2.
Pseudo-cubes can be seen as a relaxation of the notion of a Boolean cube (which should be intuitively
related with the Natarajan dimension) and were a crucial tool in the proof of [BCD+22]. In our
setting, scaled pseudo-cubes will give us the following combinatorial dimension.
Definition 11 (ğ›¾-DS Dimension). Let â„‹âŠ†[0, 1]ğ’³. A set ğ‘†âˆˆğ’³ğ‘›is ğ›¾-DS shattered if â„‹|ğ‘†contains
an ğ‘›-dimensional ğ›¾-pseudo-cube. The ğ›¾-DS dimension DDS
ğ›¾
is the maximum size of a ğ›¾-DS-shattered
set.
Extending the ideas from the multiclass classification setting, we show that the scaled-DS dimension
is necessary for realizable PAC regression. Simon (Section 6, [Sim97]) left as an open direction to
â€œobtain supplementary lower bounds [for realizable regression] (perhaps completely unrelated to the
combinatorial or Natarajan dimension)â€. Our next result is a novel contribution to this direction.
Informal Theorem 4 (Informal, see Theorem 3). Any â„‹âŠ†[0, 1]ğ’³is PAC learnable in the realizable
regression setting only if DDS
ğ›¾(â„‹) < âˆfor all ğ›¾âˆˆ(0, 1).
The proof is postponed to Appendix D. We believe that finiteness of DDS
ğ›¾(â„‹) is also a sufficient
condition for realizable regression. However, the approach of [BCD+22] that establishes a similar
result in the setting of multiclass classification does not extend trivially to the regression setting.
Conjecture 1 (Finite ğ›¾-DS is Sufficient). Let â„‹âŠ†[0, 1]ğ’³. If DDS
ğ›¾(â„‹) < âˆfor all ğ›¾âˆˆ(0, 1), then
â„‹is PAC learnable in the realizable regression setting.
3
Online Learnability for Realizable Regression
In this section we will provide our main result regarding realizable online regression. Littlestone
trees have been the workhorse of online classification problems [Lit88]. First, we provide a definition
for scaled Littlestone trees.
Definition 12 (Scaled Littlestone Tree). A scaled Littlestone tree of depth ğ‘‘â‰¤âˆis a complete
binary tree of depth ğ‘‘defined as a collection of nodes
â‹ƒï¸
0â‰¤â„“<ğ‘‘
{ï¸€
ğ‘¥ğ‘¢âˆˆğ’³: ğ‘¢âˆˆ{0, 1}â„“}ï¸€
= {ğ‘¥âˆ…} âˆª{ğ‘¥0, ğ‘¥1} âˆª{ğ‘¥00, ğ‘¥01, ğ‘¥10, ğ‘¥11} âˆª...
and real-valued gaps
â‹ƒï¸
0â‰¤â„“<ğ‘‘
{ï¸€
ğ›¾ğ‘¢âˆˆ[0, 1] : ğ‘¢âˆˆ{0, 1}â„“}ï¸€
= {ğ›¾âˆ…} âˆª{ğ›¾0, ğ›¾1} âˆª{ğ›¾00, ğ›¾01, ğ›¾10, ğ›¾11} âˆª...
9

such that for every path ğ‘¦âˆˆ{0, 1}ğ‘‘and finite ğ‘›< ğ‘‘, there exists â„âˆˆâ„‹so that â„(ğ‘¥ğ‘¦â‰¤â„“) = ğ‘ ğ‘¦â‰¤â„“+1
for 0 â‰¤â„“â‰¤ğ‘›, where ğ‘ ğ‘¦â‰¤â„“+1 is the label of the edge connecting the nodes ğ‘¥ğ‘¦â‰¤â„“and ğ‘¥ğ‘¦â‰¤â„“+1 and
â„“(ğ‘ ğ‘¦â‰¤â„“,0, ğ‘ ğ‘¦â‰¤â„“,1) = ğ›¾ğ‘¦â‰¤â„“.
In words, scaled Littlestone trees are complete binary trees whose nodes are points of ğ’³and the
two edges attached to every node are its potential classifications. An important quantity is the gap
between the two values of the edges. We define the online dimension Donl(â„‹) as follows.
Definition 13 (Online Dimension). Let â„‹âŠ†[0, 1]ğ’³.
Let ğ’¯ğ‘‘be the space of all scaled Lit-
tlestone trees of depth ğ‘‘(cf.
Definition 12) and ğ’¯
= â‹ƒï¸€âˆ
ğ‘‘=0 ğ’¯ğ‘‘.
For any scaled tree ğ‘‡=
â‹ƒï¸€
0â‰¤â„“â‰¤dep(ğ‘‡)
{ï¸€
(ğ‘¥ğ‘¢, ğ›¾ğ‘¢) âˆˆ(ğ’³, [0, 1]) : ğ‘¢âˆˆ{0, 1}â„“}ï¸€
, let ğ’«(ğ‘‡) = {ğ‘¦= (ğ‘¦0, ..., ğ‘¦dep(ğ‘‡)) : ğ‘¦ğ‘–âˆˆ
{0, 1}ğ‘–)} be the set of all paths in ğ‘‡. The dimension Donl(â„‹) is
Donl(â„‹) = sup
ğ‘‡âˆˆğ’¯
inf
ğ‘¦âˆˆğ’«(ğ‘‡)
dep(ğ‘‡)
âˆ‘ï¸
ğ‘–=0
ğ›¾ğ‘¦ğ‘–.
(1)
In words, this dimension considers the tree that has the maximum sum of label gaps over its path with
the smallest such sum, among all the trees of arbitrary depth. Note that we are taking the supremum
(infimum) in case there is no tree (path) that achieves the optimal value. Providing a characterization
and a learner with optimal cumulative loss Cğ‘‡for realizable online regression was left as an open
problem by [DG22]. We resolve this question (up to a factor of 2) by showing the following result.
Informal Theorem 5 (Informal, see Theorem 4). For any â„‹âŠ†[0, 1]ğ’³and ğœ€> 0, there exists a
deterministic learner with Câˆâ‰¤Donl(â„‹) + ğœ€and any, potentially randomized, learner satisfies
Câˆâ‰¥Donl(â„‹)/2 âˆ’ğœ€.
The formal statement and the full proof of our results can be found in Appendix E. First, we underline
that this result holds when there is no bound on the number of rounds that the learner and the
adversary interact. This follows the same spirit as the results in the realizable binary and multiclass
classification settings [Lit88, DSBDSS15]. The dimension that characterizes the minimax optimal
cumulative loss for any given ğ‘‡follows by taking the supremum in Definition 13 over trees whose
depth is at most ğ‘‡and the proof follows in an identical way (note that even with finite fixed depth ğ‘‡
the supremum is over infinitely many trees). Let us now give a sketch of our proofs.
Proof Sketch. The lower bound follows using similar arguments as in the classification setting: for
any ğœ€> 0, the adversary can create a scaled Littlestone tree ğ‘‡that achieves the sup inf bound, up to
an additive ğœ€. In the first round, the adversary presents the root of the tree ğ‘¥âˆ…. Then, no matter what the
learner picks the adversary can force error at least ğ›¾âˆ…/2. The game is repeated on the new subtree. The
proof of the upper bound presents the main technical challenge to establish Theorem 4. The strategy
of the learner can be found in Algorithm 3. The key insight in the proof is that, due to realizability,
we can show that in every round ğ‘¡there is some Ì‚ï¸€ğ‘¦ğ‘¡âˆˆ[0, 1] the learner can predict so that, no matter
what the adversary picks as the true label ğ‘¦â‹†
ğ‘¡, the online dimension of the class under the extra
restriction that â„(ğ‘¥ğ‘¡) = ğ‘¦â‹†
ğ‘¡, i.e, the updated version space ğ‘‰= {â„âˆˆâ„‹: â„(ğ‘¥ğœ) = ğ‘¦â‹†
ğœ, 1 â‰¤ğœâ‰¤ğ‘¡},
will decrease by â„“(ğ‘¦â‹†
ğ‘¡, Ì‚ï¸€ğ‘¦ğ‘¡). Thus, under this strategy of the learner, the adversary can only distribute
up to Donl(â„‹) across all the rounds of the interaction. We explain how the learner can find such a Ì‚ï¸€ğ‘¦ğ‘¡
and we handle technical issues that arise due to the fact that we are dealing with sup inf instead of
max min in the formal proof (cf. Appendix E).
4
Conclusion
In this work, we developed optimal learners for realizable regression in PAC learning and online
learning. Moreover, we identified combinatorial dimensions that characterize learnability in these
settings. We hope that our work can lead to simplified characterizations for these problems. We believe
that the main limitation of our work is that the OIG-based dimension we propose is more complicated
than the dimensions that have been proposed in the past, like the fat-shattering dimension (which,
as we explain, does not characterize learnability in the realizable regression setting). Nevertheless,
despite its complexity, this is the first dimension that characterizes learnability in the realizable
regression setting. More to that, our work leaves as an important next step to prove (or disprove) the
conjecture that the (combinatorial and simpler) ğ›¾-DS dimension is qualitatively equivalent to the
ğ›¾-OIG dimension. Another future direction, that is not directly related to this conjecture, is to better
understand the gap between the fat-shattering dimension and the OIG-based dimension.
10

Acknowledgements
Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR
(N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS).
Grigoris Velegkas is supported by TILOS, the Onassis Foundation, and the Bodossaki Foundation.
This work was done in part while some of the authors were visiting Archimedes AI Research Center.
References
[AACSZ22] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhivo-
tovskiy. The one-inclusion graph algorithm is not always optimal. arXiv preprint
arXiv:2212.09270, 2022. 4
[AACSZ23] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhiv-
otovskiy.
Optimal pac bounds without uniform convergence.
arXiv preprint
arXiv:2304.09167, 2023. 4
[AB99] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical founda-
tions, volume 9. Cambridge university press Cambridge, 1999. 18, 21
[ABDCBH97] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-
sensitive dimensions, uniform convergence, and learnability. Journal of the ACM
(JACM), 44(4):615â€“631, 1997. 2, 4
[ABDH+20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas
Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust
learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM),
67(6):1â€“42, 2020. 25
[ABED+21] Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, and Eylon
Yogev. Adversarial laws of large numbers and optimal regret in online classification.
In Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing,
pages 447â€“455, 2021. 3, 4
[AH22] Idan Attias and Steve Hanneke. Adversarially robust learning of real-valued functions.
arXiv preprint arXiv:2206.12977, 2022. 4, 25
[AHHM22] Noga Alon, Steve Hanneke, Ron Holzman, and Shay Moran. A theory of pac
learnability of partial concept classes. In 2021 IEEE 62nd Annual Symposium on
Foundations of Computer Science (FOCS), pages 658â€“671. IEEE, 2022. 4, 7, 17, 19,
25
[AHM22] Idan Attias, Steve Hanneke, and Yishay Mansour.
A characterization of semi-
supervised adversarially-robust pac learnability. arXiv preprint arXiv:2202.05420,
2022. 25
[Bac21] Francis Bach. Learning theory from first principles. Online version, 2021. 1
[BBM05] Peter Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complex-
ities. Annals of Statistics, 33(4):1497â€“1537, 2005. 4
[BCD+22] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A
characterization of multiclass learnability. In 2022 IEEE 63rd Annual Symposium on
Foundations of Computer Science (FOCS), pages 943â€“955. IEEE, 2022. 1, 4, 5, 7, 9,
25, 28
[BDGR22] Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed
online learning is as easy as statistical learning. In Conference on Learning Theory,
pages 1716â€“1786. PMLR, 2022. 4
[BDHM+19] Shai Ben-David, Pavel HrubeÅ¡, Shay Moran, Amir Shpilka, and Amir Yehudayoff.
Learnability can be undecidable. Nature Machine Intelligence, 1(1):44â€“48, 2019. 8,
30
11

[BDPSS09] Shai Ben-David, DÃ¡vid PÃ¡l, and Shai Shalev-Shwartz. Agnostic online learning. In
COLT, volume 3, page 1, 2009. 2
[BDR21] Adam Block, Yuval Dagan, and Alexander Rakhlin. Majorizing measures, sequential
complexities, and online learning. In Conference on Learning Theory, pages 587â€“590.
PMLR, 2021. 3
[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM),
36(4):929â€“965, 1989. 1, 4, 6, 7
[BGH+23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann
Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections
between replicability, privacy, and adaptive generalization. In Proceedings of the
55th Annual ACM Symposium on Theory of Computing, pages 520â€“527, 2023. 4
[BHM+21] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir
Yehudayoff. A theory of universal learning. In Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing, pages 532â€“541, 2021. 4
[BHMZ20] Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy. Proper
learning, helly number, and an optimal svm bound. In Conference on Learning
Theory, pages 582â€“609. PMLR, 2020. 25
[BL98] Peter L Bartlett and Philip M Long. Prediction, learning, uniform convergence, and
scale-sensitive dimensions. Journal of Computer and System Sciences, 56(2):174â€“
190, 1998. 2, 4
[BLM20] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification
and online prediction. In 2020 IEEE 61st Annual Symposium on Foundations of
Computer Science (FOCS), pages 389â€“402. IEEE, 2020. 4
[BLW94] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the
learnability of real-valued functions. In Proceedings of the seventh annual conference
on Computational learning theory, pages 299â€“310, 1994. 2, 4, 5
[CKW08] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple
sources. Journal of Machine Learning Research, 9(8), 2008. 31
[CP22] Moses Charikar and Chirag Pabbaraju. A characterization of list learnability. arXiv
preprint arXiv:2211.04956, 2022. 4
[DG17] Dheeru Dua and Casey Graff. UCI machine learning repository. 2017. 1
[DG22] Constantinos Daskalakis and Noah Golowich. Fast rates for nonparametric online
learning: from realizability to learning in games. In Proceedings of the 54th Annual
ACM SIGACT Symposium on Theory of Computing, pages 846â€“859, 2022. 1, 2, 3, 4,
10, 28
[DKLD84] Richard M Dudley, H Kunita, F Ledrappier, and RM Dudley. A course on empirical
processes. In Ecole dâ€™Ã©tÃ© de probabilitÃ©s de Saint-Flour XII-1982, pages 1â€“142.
Springer, 1984. 2
[DMY16] Ofir David, Shay Moran, and Amir Yehudayoff. Supervised learning through the lens
of compression. Advances in Neural Information Processing Systems, 29, 2016. 25,
26
[DSBDSS15] Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass
learnability and the erm principle. J. Mach. Learn. Res., 16(1):2377â€“2404, 2015. 1,
3, 4, 6, 10, 25, 30
[DSS14] Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems.
In Conference on Learning Theory, pages 287â€“316. PMLR, 2014. 1, 4, 6, 7, 9, 17,
25, 30
12

[FHMM23] Yuval Filmus, Steve Hanneke, Idan Mehalel, and Shay Moran. Optimal predic-
tion using expert advice and randomized littlestone dimension.
arXiv preprint
arXiv:2302.13849, 2023. 30
[FR20] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual
bandits with regression oracles. In International Conference on Machine Learning,
pages 3199â€“3210. PMLR, 2020. 4
[FW95] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the
vapnik-chervonenkis dimension. Machine learning, 21(3):269â€“304, 1995. 25
[GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press,
2016. 1
[GHST05] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. Pac-bayesian compression
bounds on the prediction error of learning algorithms for classification. Machine
Learning, 59:55â€“76, 2005. 25
[GKN14] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample
compression for nearest neighbors. Advances in Neural Information Processing
Systems, 27, 2014. 25
[Gol21] Noah Golowich. Differentially private nonparametric regression under a growth
condition. In Conference on Learning Theory, pages 2149â€“2192. PMLR, 2021. 4
[Han16] Steve Hanneke. The optimal sample complexity of pac learning. The Journal of
Machine Learning Research, 17(1):1319â€“1333, 2016. 1, 4
[HKLM22] Max Hopkins, Daniel M Kane, Shachar Lovett, and Gaurav Mahajan. Realizable
learning is all you need. In Conference on Learning Theory, pages 3015â€“3069.
PMLR, 2022. 4, 31
[HKS18] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Agnostic sample
compression for linear regression. arXiv preprint arXiv:1810.01864, 2018. 25
[HKS19] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample com-
pression for real-valued learners. In Algorithmic Learning Theory, pages 466â€“488.
PMLR, 2019. 2, 4, 25, 26
[HLW94] David Haussler, Nick Littlestone, and Manfred K Warmuth. Predicting {0, 1}-
functions on randomly drawn points. Information and Computation, 115(2):248â€“292,
1994. 1, 4, 7
[JÂ´S03] Tadeusz Januszkiewicz and Jacek Â´Swi Ë›atkowski. Hyperbolic coxeter groups of large
dimension. Commentarii Mathematici Helvetici, 78(3):555â€“583, 2003. 28
[KÃ©g03] BalÃ¡zs KÃ©gl. Robust regression by boosting the median. In Learning Theory and
Kernel Machines, pages 258â€“272. Springer, 2003. 8, 25
[KKMV23] Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. Statistical
indistinguishability of learning algorithms. arXiv preprint arXiv:2305.14311, 2023.
4
[KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of
probabilistic concepts. Journal of Computer and System Sciences, 48(3):464â€“497,
1994. 5
[KS23] Pieter Kleer and Hans Simon. Primal and dual combinatorial dimensions. Discrete
Applied Mathematics, 327:185â€“196, 2023. 4
[KSW17] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compres-
sion: Efficiency, consistency, infinite dimensions. Advances in Neural Information
Processing Systems, 30, 2017. 25
13

[KVK22] Alkis Kalavasis, Grigoris Velegkas, and Amin Karbasi. Multiclass learnability beyond
the pac framework: Universal rates and partial concept classes. arXiv preprint
arXiv:2210.02297, 2022. 4
[Lit88] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-
threshold algorithm. Machine learning, 2:285â€“318, 1988. 1, 2, 4, 9, 10
[LW86] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability.
1986. 8, 25, 26
[Men02] Shahar Mendelson. Improving the sample complexity using global data. IEEE
transactions on Information Theory, 48(7):1977â€“1991, 2002. 4, 8, 9
[MHS19] Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially
robustly learnable, but only improperly. In Conference on Learning Theory, pages
2512â€“2530. PMLR, 2019. 25
[MHS20] Omar Montasser, Steve Hanneke, and Nati Srebro. Reducing adversarially robust
learning to non-robust pac learning. Advances in Neural Information Processing
Systems, 33:14626â€“14637, 2020. 25
[MHS21] Omar Montasser, Steve Hanneke, and Nathan Srebro. Adversarially robust learning
with unknown perturbation sets. In Conference on Learning Theory, pages 3452â€“
3482. PMLR, 2021. 25
[MHS22] Omar Montasser, Steve Hanneke, and Nati Srebro. Adversarially robust learning: A
generic minimax optimal learner and characterization. Advances in Neural Informa-
tion Processing Systems, 35:37458â€“37470, 2022. 4, 8, 25
[MKFI22] Jason Milionis, Alkis Kalavasis, Dimitris Fotakis, and Stratis Ioannidis. Differen-
tially private regression with unbounded covariates. In International Conference on
Artificial Intelligence and Statistics, pages 3242â€“3273. PMLR, 2022. 4
[MY16] Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes.
Journal of the ACM (JACM), 63(3):1â€“10, 2016. 25, 27
[Nat89] Balas K Natarajan. On learning sets and functions. Machine Learning, 4(1):67â€“97,
1989. 1, 4, 5, 6
[Osa13] Damian L Osajda. A construction of hyperbolic coxeter groups. Commentarii
Mathematici Helvetici, 88(2):353â€“367, 2013. 28
[Pol90] David Pollard. Empirical processes: theory and applications. Ims, 1990. 5
[Pol12] David Pollard. Convergence of stochastic processes. Springer Science & Business
Media, 2012. 2
[RBR09] Benjamin IP Rubinstein, Peter L Bartlett, and J Hyam Rubinstein. Shifting: One-
inclusion mistake bounds and sample compression. Journal of Computer and System
Sciences, 75(1):37â€“59, 2009. 4, 7
[RS14] Alexander Rakhlin and Karthik Sridharan. Online non-parametric regression. In
Conference on Learning Theory, pages 1232â€“1264. PMLR, 2014. 3, 4
[RST10] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random
averages, combinatorial parameters, and learnability. Advances in Neural Information
Processing Systems, 23, 2010. 3
[RST15a] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via
sequential complexities. J. Mach. Learn. Res., 16(1):155â€“186, 2015. 3, 4, 30
[RST15b] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities
and uniform martingale laws of large numbers. Probability theory and related fields,
161:111â€“153, 2015. 3, 4, 30
14

[RST17] Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy,
minimax regret and minimax risk. 2017. 4
[RST23] Vinod Raman, Unique Subedi, and Ambuj Tewari. A characterization of online
multiclass learnability. arXiv preprint arXiv:2303.17716, 2023. 1, 3, 4
[She17] Or Sheffet. Differentially private ordinary least squares. In International Conference
on Machine Learning, pages 3105â€“3114. PMLR, 2017. 4
[Sim97] Hans Ulrich Simon. Bounds on the number of examples needed for learning functions.
SIAM Journal on Computing, 26(3):751â€“763, 1997. 2, 3, 4, 5, 6, 9
[SLX22] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler op-
timal algorithm for contextual bandits under realizability. Mathematics of Operations
Research, 47(3):1904â€“1931, 2022. 4
[SMB22] Han Shao, Omar Montasser, and Avrim Blum. A theory of pac learnability under
transformation invariances. arXiv preprint arXiv:2202.07552, 2022. 4
[SSSSS10] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnabil-
ity, stability and uniform convergence. The Journal of Machine Learning Research,
11:2635â€“2670, 2010. 3
[Val84] Leslie G Valiant.
A theory of the learnable.
Communications of the ACM,
27(11):1134â€“1142, 1984. 1
[Vap99] Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on
neural networks, 10(5):988â€“999, 1999. 1
[VC71] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability and its Applications, 16(2):264â€“
280, 1971. 6
[WHEY15] Yair Wiener, Steve Hanneke, and Ran El-Yaniv.
A compression technique for
analyzing disagreement-based active learning. J. Mach. Learn. Res., 16:713â€“745,
2015. 25
15

A
Notation and Definitions
We first overview standard definitions about sample complexity in PAC learning.
PAC Sample Complexity.
The realizable PAC sample complexity â„³(â„‹; ğœ€, ğ›¿) of â„‹is defined as
â„³(â„‹; ğœ€, ğ›¿) = inf
ğ´âˆˆğ’œâ„³ğ´(â„‹; ğœ€, ğ›¿) ,
(1)
where the infimum is over all possible learning algorithms and â„³ğ´(â„‹; ğœ€, ğ›¿) is the minimal integer
such that for any ğ‘šâ‰¥â„³ğ´(â„‹; ğœ€, ğ›¿), every distribution ğ’Ÿğ’³on ğ’³, and, true target â„â‹†âˆˆâ„‹, the
expected loss Eğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘‡)(ğ‘¥), â„â‹†(ğ‘¥))] of ğ´is at most ğœ€with probability 1 âˆ’ğ›¿over the training
set ğ‘‡= {(ğ‘¥, â„â‹†(ğ‘¥)) : ğ‘¥âˆˆğ‘†}, ğ‘†âˆ¼ğ’Ÿğ‘š
ğ’³.
PAC Cut-Off Sample Complexity.
We slightly overload the notation of the sample complexity
and we define
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) = inf
ğ´âˆˆğ’œâ„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾) ,
(2)
where the infimum is over all possible learning algorithms and â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾) is the minimal integer
such that for any ğ‘šâ‰¥â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾), every distribution ğ’Ÿğ’³on ğ’³, and, true target â„â‹†âˆˆâ„‹, the
expected cut-off loss Prğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘‡)(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾] of ğ´is at most ğœ€with probability 1 âˆ’ğ›¿over
the training set ğ‘‡= {(ğ‘¥, â„â‹†(ğ‘¥)) : ğ‘¥âˆˆğ‘†}, ğ‘†âˆ¼ğ’Ÿğ‘š
ğ’³.
Lemma 1 (Equivalence Between Sample Complexities). For every ğœ€, ğ›¿âˆˆ(0, 1)2 and every â„‹âŠ†
[0, 1]ğ’³, where ğ’³is the input domain, it holds that
â„³(â„‹; âˆšğœ€, ğ›¿, âˆšğœ€) â‰¤â„³(â„‹; ğœ€, ğ›¿) â‰¤â„³(â„‹; ğœ€/2, ğ›¿, ğœ€/2)
Proof. Let ğ´be a learning algorithm. We will prove the statement for each fixed ğ´and for each data-
generating distribution ğ’Ÿ, so the result follows by taking the infimum over the learning algorithms.
Assume that the cut-off sample complexity of ğ´is â„³ğ´(â„‹; ğœ€/2, ğ›¿, ğœ€/2). Then, with probability 1 âˆ’ğ›¿
over the training sample ğ‘†âˆ¼ğ’Ÿ, for its expected loss it holds that
E
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥))] â‰¤ğœ€
2 +
(ï¸
1 âˆ’ğœ€
2
)ï¸
Â· ğœ€
2 â‰¤ğœ€,
thus, â„³ğ´(â„‹; ğœ€, ğ›¿) â‰¤â„³ğ´(â„‹; ğœ€/2, ğ›¿, ğœ€/2).
The other direction follows by using Markovâ€™s inequality. In particular, if we have that with probability
at least 1 âˆ’ğ›¿over ğ‘†âˆ¼ğ’Ÿit holds that
E
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥))] â‰¤ğœ€,
then Markovâ€™s inequality gives us that
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥)) â‰¥âˆšğœ€] â‰¤âˆšğœ€,
which shows that â„³ğ´(â„‹; ğœ€, ğ›¿) â‰¥â„³ğ´(â„‹; âˆšğœ€, ğ›¿, âˆšğœ€).
ERM Sample Complexity.
In the special case where ğ’œis the class ERM of all possible ERM
algorithms, i.e., algorithms that return a hypothesis whose sample error is exactly 0, we define the
ERM sample complexity as the number of samples required by the worst-case ERM algorithm, i.e.,
â„³ERM(â„‹; ğœ€, ğ›¿) =
sup
ğ´âˆˆERM
â„³ğ´(â„‹; ğœ€, ğ›¿) ,
(3)
and its cut-off analogue as
â„³ERM(â„‹; ğœ€, ğ›¿, ğ›¾) =
sup
ğ´âˆˆERM
â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾) ,
(4)
B
ğ›¾-Graph Dimension and ERM Learnability
In this section, we show that ğ›¾-graph dimension determines the learnability of â„‹âŠ†[0, 1]ğ’³using any
ERM learner. We first revisit the notion of partial concept classes which will be useful for deriving
our algorithms.
16

B.1
Partial Concept Classes and A Naive Approach that Fails
[AHHM22] proposed an extension of the binary PAC model to handle partial concept classes, where
â„‹âŠ†{0, 1, â‹†}ğ’³, for some input domain ğ’³, where â„(ğ‘¥) = â‹†should be thought of as â„not knowing
the label of ğ‘¥âˆˆğ’³. The main motivation behind their work is that partial classes allow one to
conveniently express data-dependent assumptions. As an intuitive example, a halfspace with margin
is a partial function that is undefined inside the forbidden margin and is a well-defined halfspace
outside the margin boundaries. Instead of dealing with concept classes â„‹âŠ†ğ’´ğ’³where each concept
â„âˆˆâ„‹is a total function â„: ğ’³â†’ğ’´, we study partial concept classes â„‹âŠ†(ğ’´âˆª{â‹†})ğ’³, where
each concept â„is now a partial function and â„(ğ‘¥) = â‹†means that the function â„is undefined at ğ‘¥.
We define the support of â„as the set supp(â„) = {ğ‘¥âˆˆğ’³: â„(ğ‘¥) Ì¸= â‹†}. Similarly as in the case of
total classes, we say that a finite sequence ğ‘†= (ğ‘¥1, ğ‘¦1, . . . , ğ‘¥ğ‘›, ğ‘¦ğ‘›) is realizable with respect to â„‹if
there exists some â„* âˆˆâ„‹such that â„*(ğ‘¥ğ‘–) = ğ‘¦ğ‘–, âˆ€ğ‘–âˆˆ[ğ‘›].
An important notion related to partial concept classes is that of disambiguation.
Definition 14 (Disambiguation of Partial Concept Class [AHHM22]). Let ğ’³be an input domain.
A total concept class â„‹âŠ†{0, 1}ğ’³is a special type of a partial concept. Given some partial
concept class â„‹âŠ†{0, 1, â‹†}ğ’³we say that ğ»is a disambiguation of â„‹if for any finite sequence
ğ‘†âˆˆ(ğ’³Ã— {0, 1})* if ğ‘†is realizable with respect to â„‹, then ğ‘†is realizable with respect to â„‹.
Intuitively, by disambiguating a partial concept class we convert it to a total concept class without
reducing its â€œexpressivityâ€.
Let us first describe an approach to prove the upper bound, i.e., that if the scaled-graph dimension
is finite for all scales then the class is ERM learnable, that does not work. We could perform the
following transformation, inspired by the multiclass setting [DSS14]: for any â„âˆˆâ„‹âŠ†[0, 1]ğ’³, let us
consider the function Ìƒï¸€â„: ğ’³Ã— [0, 1] â†’{0, 1} with Ìƒï¸€â„(ğ‘¥, ğ‘¦) = 1 if and only if â„(ğ‘¥) = ğ‘¦, Ìƒï¸€â„(ğ‘¥, ğ‘¦) = 0
if and only if â„“(â„(ğ‘¥), ğ‘¦) > ğœ€and Ìƒï¸€â„(ğ‘¥, ğ‘¦) = â‹†otherwise. This induces a new binary partial hypothesis
class Ìƒï¸€
â„‹= {Ìƒï¸€â„: â„âˆˆâ„‹} âŠ†{0, 1, â‹†}ğ’³. We note that DG
ğœ€(â„‹) = VC( Ìƒï¸€
â„‹). However, we cannot use
ERM for the partial concept class since in general this approach fails. In particular, a sufficient
condition for applying ERM is that VC({supp(Ìƒï¸€â„) : Ìƒï¸€â„âˆˆÌƒï¸€
â„‹)} < âˆ.
Remark 1. Predicting â‹†in [AHHM22] implies a mistake for the setting of partial concept classes.
However, in our regression setting, â‹†is interpreted differently and corresponds to loss at most ğ›¾
which is desirable. In particular, the hard instance for proper learners in the partial concepts paper
(see Proposition 4 in [AHHM22]) is good in settings where predicting â‹†does not count as a mistake,
as in our regression case.
B.2
Main Result
We are now ready to state the main result of this section. We will prove the next statement.
Theorem 1. Let â„“be the absolute loss function. For every class â„‹âŠ†[0, 1]ğ’³and for any ğœ€, ğ›¿, ğ›¾âˆˆ
(0, 1)3, the sample complexity bound for realizable PAC regression by any ERM satisfies
â„¦
(ï¸ƒ
DG
ğ›¾(â„‹) + log(1/ğ›¿)
ğœ€
)ï¸ƒ
â‰¤â„³ERM(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¤ğ‘‚
(ï¸ƒ
DG
ğ›¾(â„‹) log(1/ğœ€) + log(1/ğ›¿)
ğœ€
)ï¸ƒ
.
In particular, any ERM algorithm A achieves
E
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥)] â‰¤
inf
ğ›¾âˆˆ[0,1] ğ›¾+ Ìƒï¸€Î˜
(ï¸ƒ
DG
ğ›¾(â„‹) + log(1/ğ›¿)
ğ‘›
)ï¸ƒ
,
with probability at least 1 âˆ’ğ›¿over ğ‘†of size ğ‘›.
Proof. We prove the upper bound and the lower bound of the statement separately.
Upper Bound for the ERM learner.
We deal with the cut-off loss problem with parameters
(ğœ€, ğ›¿, ğ›¾) âˆˆ(0, 1)3. Our proof is based on a technique that uses a â€œghostâ€ sample to establish
generalization guarantees of the algorithm. Let us denote
erğ’Ÿ,ğ›¾(â„) â‰œ
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(â„(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾] ,
(1)
17

and for a dataset ğ‘§âˆˆ(ğ’³Ã— [0, 1])ğ‘›,
Ì‚ï¸€erğ‘§,ğ›¾(â„) â‰œ1
|ğ‘§|
âˆ‘ï¸
(ğ‘¥,ğ‘¦)âˆˆğ‘§
I{â„“(â„(ğ‘¥), ğ‘¦) > ğ›¾} .
(2)
We will start by showing the next symmetrization lemma in our setting. Essentially, it bounds the
probability that there exists a bad ERM learner by the probability that there exists an ERM learner on
a sample ğ‘Ÿwhose performance on a hidden sample ğ‘ is bad. For a similar result, see Lemma 4.4 in
[AB99].
Lemma 2 (Symmetrization). Let ğœ€, ğ›¾âˆˆ(0, 1)2, ğ‘›> 0. Fix ğ‘= ğ’³Ã— [0, 1]. Let
ğ‘„ğœ€,ğ›¾= {ğ‘§âˆˆğ‘ğ‘›: âˆƒâ„âˆˆâ„‹: Ì‚ï¸€erğ‘§,ğ›¾(â„) = 0, erğ’Ÿ,ğ›¾(â„) > ğœ€}
(3)
and
ğ‘…ğœ€,ğ›¾= {(ğ‘Ÿ, ğ‘ ) âˆˆğ‘ğ‘›Ã— ğ‘ğ‘›: âˆƒâ„âˆˆâ„‹: erğ’Ÿ,ğ›¾(â„) > ğœ€, Ì‚ï¸€erğ‘Ÿ,ğ›¾(â„) = 0, Ì‚ï¸€erğ‘ ,ğ›¾(â„) â‰¥ğœ€/2} .
(4)
Then, for ğ‘›â‰¥ğ‘/ğœ€, where ğ‘is some absolute constant, we have that
ğ’Ÿğ‘›(ğ‘„ğœ€,ğ›¾) â‰¤2ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) .
Proof. We will show that ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) â‰¥ğ’Ÿğ‘›(ğ‘„ğœ€,ğ›¾)
2
. By the definition of ğ‘…ğœ€,ğ›¾we can write
ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) =
âˆ«ï¸
ğ‘„ğœ€,ğ›¾
ğ’Ÿğ‘›(ğ‘ : âˆƒâ„âˆˆâ„‹, erğ’Ÿ,ğ›¾(â„) > ğœ€, Ì‚ï¸€erğ‘Ÿ,ğ›¾(â„) = 0, Ì‚ï¸€erğ‘ ,ğ›¾(â„) â‰¥ğœ€/2)ğ‘‘ğ’Ÿğ‘›(ğ‘Ÿ) .
For ğ‘Ÿâˆˆğ‘„ğœ€,ğ›¾, fix â„ğ‘Ÿâˆˆâ„‹that satisfies Ì‚ï¸€erğ‘Ÿ,ğ›¾(â„ğ‘Ÿ) = 0, erğ’Ÿ,ğ›¾(â„) > ğœ€. It suffices to show that for â„ğ‘Ÿ
ğ’Ÿğ‘›(ğ‘ : Ì‚ï¸€erğ‘ ,ğ›¾(â„ğ‘Ÿ) â‰¥ğœ€/2) â‰¥1/2 .
Then, the proof of the lemma follows immediately. Since erğ’Ÿ,ğ›¾(â„ğ‘Ÿ) > ğœ€, we know that ğ‘›Â· Ì‚ï¸€erğ‘ ,ğ›¾(â„)
follows a binomial distribution with probability of success on every try at least ğœ€and ğ‘›number of
tries. Thus, the multiplicative version of Chernoffâ€™s bound gives us
ğ’Ÿğ‘›(ğ‘ : Ì‚ï¸€erğ‘ ,ğ›¾(â„ğ‘Ÿ) < ğœ€/2) â‰¤ğ‘’âˆ’ğ‘›Â·ğœ€
8 .
Thus, if ğ‘›= ğ‘/ğœ€, for some appropriate absolute constant ğ‘we see that
ğ’Ÿğ‘›(ğ‘ : Ì‚ï¸€erğ‘ ,ğ›¾(â„ğ‘Ÿ) < ğœ€/2) < 1/2 ,
which concludes the proof.
Next, we can use a random swap argument to upper bound ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) with a quantity that involves a
set of permutations over the sample of length 2ğ‘›. The main idea behind the proof is to try to leverage
the fact that each of the labeled examples is as likely to occur among the first ğ‘›examples or the last
ğ‘›examples.
Following [AB99], we denote by Î“ğ‘›the set of all permutations on {1, . . . , 2ğ‘›} that swap ğ‘–and
ğ‘›+ ğ‘–, for all ğ‘–that belongs to {1, . . . , ğ‘›} . In other words, for all ğœâˆˆÎ“ğ‘›and ğ‘–âˆˆ{1, . . . , ğ‘›} either
ğœ(ğ‘–) = ğ‘–, ğœ(ğ‘›+ ğ‘–) = ğ‘›+ ğ‘–or ğœ(ğ‘–) = ğ‘›+ ğ‘–, ğœ(ğ‘›+ ğ‘–) = ğ‘–. Thus, we can think of ğœas acting
on coordinates where it (potentially) swaps one element from the first half of the sample with the
corresponding element on the second half of the sample. For some ğ‘§âˆˆğ‘2ğ‘›we overload the notation
and denote ğœ(ğ‘§) the effect of applying ğœto the sample ğ‘§.
We are now ready to state the bound. Importantly, it shows that by (uniformly) randomly choosing
a permutation ğœâˆˆÎ“ğ‘›we can bound the probability that a sample falls into the bad set ğ‘…ğœ€,ğ›¾by a
quantity that does not depend on the distribution ğ’Ÿ.
Lemma 3 (Random Swaps; Adaptation of Lemma 4.5 in [AB99]). Fix ğ‘= ğ’³Ã— [0, 1]. Let ğ‘…ğœ€,ğ›¾be
any subset of ğ‘2ğ‘›and ğ’Ÿany probability distribution on ğ‘. Then
ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) =
E
ğ‘§âˆ¼ğ’Ÿ2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] â‰¤max
ğ‘§âˆˆğ‘2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] ,
where U(Î“ğ‘›) is the uniform distribution over the set of swapping permutations Î“ğ‘›.
18

Proof. First, notice that the bound
E
ğ‘§âˆ¼ğ’Ÿ2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] â‰¤max
ğ‘§âˆˆğ‘2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] ,
follows trivially and the maximum exists since Prğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] takes finitely many values
for any finite ğ‘›and all ğ‘§âˆˆğ‘2ğ‘›. Thus, the bulk of the proof is to show that
ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) =
E
ğ‘§âˆ¼ğ’Ÿ2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] .
First, notice that since example is drawn i.i.d., for any swapping permutation ğœâˆˆÎ“ğ‘›we have that
ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) = ğ’Ÿ2ğ‘›(ï¸€{ï¸€
ğ‘§âˆˆğ‘2ğ‘›: ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾
}ï¸€)ï¸€
.
(5)
Thus, the following holds
ğ’Ÿ2ğ‘›(ğ‘…ğœ€,ğ›¾) =
âˆ«ï¸
ğ‘2ğ‘›I{ğ‘§âˆˆğ‘…ğœ€,ğ›¾} ğ‘‘ğ’Ÿ2ğ‘›(ğ‘§)
=
1
|Î“ğ‘›|
âˆ‘ï¸
ğœâˆˆÎ“ğ‘›
âˆ«ï¸
ğ‘2ğ‘›I{ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾} ğ‘‘ğ’Ÿ2ğ‘›(ğ‘§)
=
âˆ«ï¸
ğ‘2ğ‘›
(ï¸ƒ
1
|Î“ğ‘›|
âˆ‘ï¸
ğœâˆˆÎ“ğ‘›
I{ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾}
)ï¸ƒ
ğ‘‘ğ’Ÿ2ğ‘›(ğ‘§)
=
E
ğ‘§âˆ¼ğ’Ÿ2ğ‘›
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] ,
where the first equation follows by definition, the second by Equation (5), the third because the
number of terms in the summation is finite, and the last one by definition.
As a last step we can bound the above RHS by using all possible patterns when â„‹is (roughly
speaking) projected in the sample ğ‘Ÿğ‘ âˆˆğ‘2ğ‘›.
Lemma 4 (Bounding the Bad Event). Fix ğ‘= ğ’³Ã— [0, 1]. Let ğ‘…ğœ€,ğ›¾âŠ†ğ‘2ğ‘›be the set
ğ‘…ğœ€,ğ›¾= {(ğ‘Ÿ, ğ‘ ) âˆˆğ‘ğ‘›Ã— ğ‘ğ‘›: âˆƒâ„âˆˆâ„‹: Ì‚ï¸€erğ‘Ÿ,ğ›¾(â„) = 0, Ì‚ï¸€erğ‘ ,ğ›¾(â„) â‰¥ğœ€/2} .
Then
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] â‰¤(2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›))2âˆ’ğ‘›ğœ€/2 .
Proof. Throughout the proof we fix ğ‘§= (ğ‘§1, ..., ğ‘§2ğ‘›) âˆˆğ‘2ğ‘›, where ğ‘§ğ‘–= (ğ‘¥ğ‘–, ğ‘¦ğ‘–) = (ğ‘¥ğ‘–, â„â‹†(ğ‘¥ğ‘–))
and let ğ‘†= {ğ‘¥1, ..., ğ‘¥2ğ‘›}. Consider the projection set â„‹|ğ‘†. We define a partial binary concept class
â„‹â€² âŠ†{0, 1, â‹†}2ğ‘›as follows:
â„‹â€² :=
â§
â¨
â©â„â€² âˆˆ{0, 1, â‹†}2ğ‘›: âˆƒâ„âˆˆâ„‹|ğ‘†: âˆ€ğ‘–âˆˆ[2ğ‘›]
â§
â¨
â©
â„(ğ‘¥ğ‘–) = ğ‘¦ğ‘–,
â„â€²(ğ‘–) = 0
â„“(â„(ğ‘¥ğ‘–), ğ‘¦ğ‘–) > ğ›¾,
â„â€²(ğ‘–) = 1
0 < â„“(â„(ğ‘¥ğ‘–), ğ‘¦ğ‘–) â‰¤ğ›¾,
â„â€²(ğ‘–) = â‹†
â«
â¬
â­
â«
â¬
â­.
Importantly, we note that, by definition, VC(â„‹â€²) â‰¤DG
ğ›¾(â„‹).
Currently, we have a partial binary concept class â„‹â€². As a next step, we would like to replace the
â‹†symbols and essentially reduce the problem to a total concept class. This procedure is called
disambiguation (cf. Definition 14). The next key lemma shows that there exists a compact (in terms
of cardinality) disambiguation of a VC partial concept class for finite instance domains.
Lemma 5 (Compact Disambiguations, see [AHHM22]). Let â„‹be a partial concept class on a
finite instance domain ğ’³with VC(â„‹) = ğ‘‘. Then there exists a disambiguation â„‹of â„‹with size
|â„‹| = |ğ’³|ğ‘‚(ğ‘‘log |ğ’³|).
This means that there exists a disambiguation â„‹â€² of â„‹â€² of size at most
(2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›)) .
19

Since this (total) binary concept class is finite, we can apply the following union bound argument.
We have that ğœ(ğ‘§) âˆˆğ‘…if and only if some â„âˆˆâ„‹satisfies
âˆ‘ï¸€ğ‘›
ğ‘–=1 I{â„“(â„(ğ‘¥ğœ(ğ‘–)), ğ‘¦ğœ(ğ‘–)) > ğ›¾}
ğ‘›
= 0,
âˆ‘ï¸€ğ‘›
ğ‘–=1 I{â„“(â„(ğ‘¥ğœ(ğ‘›+ğ‘–)), ğ‘¦ğœ(ğ‘›+ğ‘–)) > ğ›¾}
ğ‘›
â‰¥ğœ€/2 .
We can relate this event with an event about the disambiguated partial concept class â„‹â€² since the
number of 1â€™s can only increase. In particular, for any swapping permutation ğœof the 2ğ‘›points, if
there exists a function in â„‹that is correct on the first ğ‘›points and is off by at least ğ›¾on at least ğœ€ğ‘›/2
of the remaining ğ‘›points, then there is a function in the disambiguation â„‹â€² that is 0 on the first ğ‘›
points and is 1 on those same ğœ–ğ‘›/2 of the remaining points.
If we fix some ğœâˆˆÎ“ğ‘›, and some â„â€² âˆˆâ„‹â€² then â„â€² is a witness that ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾only if âˆ€ğ‘–âˆˆ[ğ‘›] we do
not have that â„â€²(ğ‘–) = 1, â„â€²(ğ‘–+ ğ‘›) = 1. Thus at least one of â„â€²(ğ‘–), â„â€²(ğ‘›+ ğ‘–) must be zero. Moreover,
at least ğ‘›ğœ€/2 entries must be non-zero. Thus, when we draw random swapping permutation the
probability that all the non-zero entries land on the second half of the sample sample is at most
2âˆ’ğ‘›ğœ€/2.
Crucially since the number of possible functions is at most |â„‹â€²| â‰¤(2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›)) a union
bound gives us that
Pr
ğœâˆ¼U(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] â‰¤(2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›)) Â· 2âˆ’ğ‘›ğœ€/2 .
Thus, since ğ‘§âˆˆğ‘2ğ‘›was arbitrary we have that
max
ğ‘§âˆˆğ‘2ğ‘›
Pr
ğœU(Î“ğ‘›)[ğœ(ğ‘§) âˆˆğ‘…ğœ€,ğ›¾] â‰¤(2ğ‘›)ğ‘‚(DG
ğ›¾(â„‹) log(2ğ‘›)) Â· 2âˆ’ğ‘›ğœ€/2 .
This concludes the proof.
Lower Bound for the ERM learner.
Our next goal is to show that
â„³ERM(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¥ğ¶0 Â· DG
ğ›¾(â„‹) + log(1/ğ›¿)
ğœ€
.
To this end, we will show that there exists an ERM learner satisfying this lower bound. This will
establish that the finiteness of ğ›¾-graph dimension for any ğ›¾âˆˆ(0, 1), is necessary for PAC learnability
using a worst-case ERM. It suffices to show that there exists a bad ERM algorithm that requires
at least ğ¶0
DG
ğ›¾(â„‹)+log(1/ğ›¿)
ğœ€
samples to cut-off PAC learn â„‹. First let us consider the case where
ğ‘‘= DG
ğ›¾(â„‹) < âˆand let ğ‘†= {ğ‘¥1, ...., ğ‘¥ğ‘‘} be a ğ›¾-graph-shattered set by â„‹with witness ğ‘“0.
Consider the ERM learner ğ’œthat works as follows: Upon seeing a sample ğ‘‡âŠ†ğ‘†consistent with ğ‘“0,
ğ’œreturns a function ğ’œ(ğ‘‡) that is equal to ğ‘“0 on elements of ğ‘‡and ğ›¾-far from ğ‘“0 on ğ‘†âˆ–ğ‘‡. Such a
function exists since ğ‘†is ğ›¾-graph-shattered with witness ğ‘“0. Let us take ğ›¿< 1/100 and ğœ€< 1/12.
Define a distribution over ğ‘†âŠ†ğ’³such that
Pr[ğ‘¥1] = 1 âˆ’2ğœ€,
Pr[ğ‘¥ğ‘–] = 2ğœ€/(ğ‘‘âˆ’1), âˆ€ğ‘–âˆˆ{2, ..., ğ‘‘} .
Let us set â„â‹†= ğ‘“0 and consider ğ‘šsamples {(ğ‘§ğ‘–, ğ‘“0(ğ‘§ğ‘–)}ğ‘–âˆˆ[ğ‘š]. Since we work in the scaled PAC
model, ğ’œwill make a ğ›¾-error on all examples from ğ‘†which are not in the sample (since in that
case the output will be ğ›¾-far from the true label). Let us take ğ‘šâ‰¤ğ‘‘âˆ’1
6ğœ€. Then, the sample will
include at most (ğ‘‘âˆ’1)/2 examples which are not ğ‘¥1 with probability 1/100, using Chernoffâ€™s bound.
Conditioned on that event, this implies that the ERM learner will make a ğ›¾-error with probability at
least
2ğœ€
ğ‘‘âˆ’1Â·(ğ‘‘âˆ’1âˆ’ğ‘‘âˆ’1
2 ) = ğœ€, over the random draw of the test point. Thus, â„³ğ’œ(â„‹; ğœ€, ğ›¿, ğ›¾) = â„¦( ğ‘‘âˆ’1
ğœ€).
Moreover, the probability that the sample will only contain ğ‘¥1 is (1âˆ’2ğœ€)ğ‘šâ‰¥ğ‘’âˆ’4ğœ€ğ‘šwhich is greater
that ğ›¿whenever ğ‘šâ‰¤log(1/ğ›¿)/(4ğœ€). This implies that the ğ›¾-cut-off ERM sample complexity is
lower bounded by
max
{ï¸‚ğ‘‘âˆ’1
6ğœ€, log(1/ğ›¿)
2ğœ€
}ï¸‚
= ğ¶0 Â· DG
ğ›¾(â„‹) + log(1/ğ›¿)
ğœ€
.
Thus â„³ERM(â„‹; ğœ€, ğ›¿, ğ›¾), satisfies the desired bound when the dimension is finite. Finally, it remains
to claim about the case where DG
ğ›¾(â„‹) = âˆfor the given ğ›¾. We consider a sequence of ğ›¾-graph-
shattered sets ğ‘†ğ‘›with |ğ‘†ğ‘›| = ğ‘›and repeat the claim for the finite case. This will yield that for any ğ‘›
the cut-off ERM sample complexity is lower bounded by â„¦((ğ‘›+ log(1/ğ›¿))/ğœ€) and this yields that
â„³ERM(â„‹; ğœ€, ğ›¿, ğ›¾) = âˆ.
20

However, as Example 4 shows, the optimal learner cannot be proper and as a result, this dimension
does not characterize PAC learnability for real-valued regression (there exist classes whose ğ›¾-graph
dimension is infinite but are PAC learnable in the realizable regression setting).
C
ğ›¾-OIG Dimension and Learnability
In this section we identify a dimension characterizing qualitatively and quantitatively what classes of
predictors â„‹âŠ†[0, 1]ğ’³are PAC learnable and we provide PAC learners that achieve (almost) optimal
sample complexity. In particular, we show the following result.
Theorem 2. Let â„“be the absolute loss function. For every class â„‹âŠ†[0, 1]ğ’³and for any ğœ€, ğ›¿, ğ›¾âˆˆ
(0, 1)3, the sample complexity bound for realizable PAC regression satisfies
â„¦
(ï¸ƒ
DOIG
2ğ›¾(â„‹)
ğœ€
)ï¸ƒ
â‰¤â„³(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¤ğ‘‚
(ï¸ƒ
DOIG
ğ›¾
(â„‹)
ğœ€
log2 DOIG
ğ›¾
(â„‹)
ğœ€
+ 1
ğœ€log 1
ğ›¿
)ï¸ƒ
.
In particular, there exists an algorithm ğ´such that
E
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥)] â‰¤
inf
ğ›¾âˆˆ[0,1] ğ›¾+ Ìƒï¸€Î˜
(ï¸ƒ
DOIG
ğ›¾
(â„‹) + log(1/ğ›¿)
ğ‘›
)ï¸ƒ
,
with probability at least 1 âˆ’ğ›¿over ğ‘†âˆ¼ğ’Ÿğ‘›.
A finite fat-shattering dimension implies a finite OIG dimension.
Let â„±âŠ†[0, 1]ğ’³be a function
class with finite ğ›¾-fat shattering dimension for any ğ›¾> 0. We show that DOIG
ğ›¾
(â„±) is upper bounded
(up to constants and log factors) by Dfat
ğ‘ğ›¾(â„±), for some ğ‘> 0, where the OIG dimension is defined
with respect to the â„“1 loss. Note that the opposite direction does not hold. Example 1 exhibits a
function class with an infinite fat-shattering dimension that can be learned with a single example,
and as a result, the OIG dimension has to be finite. On the one hand, we have an upper bound on the
sample complexity of ğ‘‚
(ï¸€1
ğœ–
(ï¸€
Dfat
Ëœğ‘ğœ€(â„±) log2 1
ğœ–+ log 1
ğ›¿
)ï¸€)ï¸€
, for any ğœ€, ğ›¿âˆˆ(0, 1). See sections 19.6 and
20.4 about the restricted model in [AB99]. On the other hand, we prove in Lemma 6 a lower bound
on the sample complexity of â„¦
(ï¸
DOIG
2ğœ€
(â„±)
ğœ–
)ï¸
, for any ğœ€âˆˆ(0, 1), and so DOIG
ğ›¾
(â„±) is upper bounded by
Dfat
ğ‘ğ›¾(â„±) up to constants and log factors.
C.1
Proof of the Lower Bound
Lemma 6. [Lower Bound of PAC Regression] Let ğ´be any learning algorithm and ğœ€, ğ›¿, ğ›¾âˆˆ(0, 1)3
such that ğ›¿< ğœ€. Then,
â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¥â„¦
(ï¸ƒ
DOIG
2ğ›¾(â„‹)
ğœ€
)ï¸ƒ
.
Proof. Let ğ‘›0 = DOIG
2ğ›¾(â„‹). Let ğ‘›âˆˆN, 1 < ğ‘›â‰¤ğ‘›0. We know that for each such ğ‘›there exists
some ğ‘†âˆˆğ’³ğ‘›such that the one-inclusion graph of â„‹|ğ‘†has the property that: there exists a finite
subgraph ğº= (ğ‘‰, ğ¸) of ğºOIG
â„‹|ğ‘†such that for any orientation ğœ: ğ¸â†’ğ‘‰of the subgraph, there exists
a vertex ğ‘£âˆˆğ‘‰with outdeg(ğ‘£; ğœ, 2ğ›¾) > DOIG
2ğ›¾(â„‹)/3.
Given the learning algorithm ğ´: (ğ’³Ã—[0, 1])â‹†Ã—ğ’³â†’[0, 1], we can describe an orientation ğœğ´of the
edges in ğ¸. For any vertex ğ‘£= (ğ‘£1, . . . , ğ‘£ğ‘›) âˆˆğ‘‰let ğ‘ƒğ‘£be the distribution over (ğ‘¥1, ğ‘£1), ...., (ğ‘¥ğ‘›, ğ‘£ğ‘›)
defined as
ğ‘ƒğ‘£((ğ‘¥1, ğ‘£1)) = 1 âˆ’ğœ€, ğ‘ƒğ‘£((ğ‘¥ğ‘¡, ğ‘£ğ‘¡)) =
ğœ€
ğ‘›âˆ’1, ğ‘¡âˆˆ{2, . . . , ğ‘›} .
Let ğ‘š= ğ‘›/(2ğœ€). For each vertex ğ‘£âˆˆğ‘‰and direction ğ‘¡âˆˆ[ğ‘›], consider the hyperedge ğ‘’ğ‘¡,ğ‘£. For each
ğ‘¢âˆˆğ‘’ğ‘¡,ğ‘£we define
ğ‘ğ‘¡(ğ‘¢) =
Pr
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘¢
[â„“(ğ´(ğ‘†; ğ‘¥ğ‘¡), ğ‘¢ğ‘¡) > ğ›¾|(ğ‘¥ğ‘¡, ğ‘¢ğ‘¡) /âˆˆğ‘†] ,
and let ğ¶ğ‘’ğ‘¡,ğ‘£= {ğ‘¢âˆˆğ‘’ğ‘¡,ğ‘£: ğ‘ğ‘¡(ğ‘¢) < 1/2}. If ğ¶ğ‘’ğ‘¡,ğ‘£= âˆ…, we orient the edge ğ‘’ğ‘¡,ğ‘£arbitrarily. Since for
all ğ‘¢, ğ‘£âˆˆğ‘’ğ‘¡,ğ‘£the distributions ğ‘ƒğ‘š
ğ‘¢, ğ‘ƒğ‘š
ğ‘£conditioned on the event that (ğ‘¥ğ‘¡, ğ‘¢ğ‘¡), (ğ‘¥ğ‘¡, ğ‘£ğ‘¡) respectively
21

are not in ğ‘†are the same, we can see that âˆ€ğ‘¢, ğ‘£âˆˆğ¶ğ‘’ğ‘¡,ğ‘£it holds that â„“(ğ‘¢ğ‘¡, ğ‘£ğ‘¡) â‰¤2ğ›¾. We orient the
edge ğ‘’ğ‘¡,ğ‘£using an arbitrary element of ğ¶ğ‘’ğ‘¡,ğ‘£.
Because of the previous discussion, we can bound from above the out-degree of all vertices ğ‘£âˆˆğ‘‰
with respect to the orientation ğœğ´as follows:
outdeg(ğ‘£; ğœğ´, 2ğ›¾) â‰¤
âˆ‘ï¸
ğ‘¡
I{ğ‘ğ‘¡(ğ‘£) â‰¥1/2} â‰¤1 + 2
ğ‘›
âˆ‘ï¸
ğ‘¡=2
Pr
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾|(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†] .
Notice that
Pr
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾|(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†] = Prğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£[{â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾} âˆ§{(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†}]
Prğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£[(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†]
,
and by the definition of ğ‘ƒğ‘£, we have that
Pr
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†] =
(ï¸‚
1 âˆ’
ğœ€
ğ‘›âˆ’1
)ï¸‚ğ‘š
â‰¥1 âˆ’
ğ‘›
2(ğ‘›âˆ’1) ,
since ğ‘š= ğ‘›/(2ğœ€). Combining the above, we get that
outdeg(ğ‘£; ğœğ´, 2ğ›¾) â‰¤1 + 2
(ï¸‚
1 âˆ’
ğ‘›
2(ğ‘›âˆ’1)
)ï¸‚
ğ‘›
âˆ‘ï¸
ğ‘¡=2
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[I{â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾} Â· I{(ğ‘¥ğ‘¡, ğ‘¦ğ‘¡) /âˆˆğ‘†}] ,
and so
outdeg(ğ‘£; ğœğ´, 2ğ›¾) â‰¤1 + 2
(ï¸‚
1 âˆ’
ğ‘›
2(ğ‘›âˆ’1)
)ï¸‚
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[ï¸ƒğ‘›
âˆ‘ï¸
ğ‘¡=2
I{â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾}
]ï¸ƒ
= 1 + 2
(ï¸‚
1 âˆ’
ğ‘›
2(ğ‘›âˆ’1)
)ï¸‚ğ‘›âˆ’1
ğœ€
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[ï¸ƒ
ğœ€
ğ‘›âˆ’1
ğ‘›
âˆ‘ï¸
ğ‘¡=2
I{â„“(ğ´(ğ‘†, ğ‘¥ğ‘¡), ğ‘¦ğ‘¡) > ğ›¾}
]ï¸ƒ
â‰¤1 + 2
(ï¸‚
1 âˆ’
ğ‘›
2(ğ‘›âˆ’1)
)ï¸‚ğ‘›âˆ’1
ğœ€
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[ï¸‚
E
(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘£[I{â„“(ğ´(ğ‘†; ğ‘¥), ğ‘¦) > ğ›¾}]
]ï¸‚
= 1 + 2
(ï¸‚
1 âˆ’
ğ‘›
2(ğ‘›âˆ’1)
)ï¸‚ğ‘›âˆ’1
ğœ€
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[ï¸‚
Pr
(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘£[â„“(ğ´(ğ‘†; ğ‘¥), ğ‘¦) > ğ›¾]
]ï¸‚
â‰¤1 + ğ‘›âˆ’2
ğœ€
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£
[ï¸‚
Pr
(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘£[â„“(ğ´(ğ‘†; ğ‘¥), ğ‘¦) > ğ›¾]
]ï¸‚
By picking â€œhardâ€ distribution ğ’Ÿ= ğ‘ƒğ‘£â‹†, where ğ‘£â‹†âˆˆarg maxğ‘£â€²âˆˆğ‘‰outdeg(ğ‘£â€²; ğœğ´, 2ğ›¾) we get that
that
E
ğ‘†âˆ¼ğ‘ƒğ‘š
ğ‘£â‹†
[ï¸‚
Pr
(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘£â‹†[â„“(ğ´(ğ‘†; ğ‘¥), ğ‘¦) > ğ›¾]
]ï¸‚
â‰¥(outdeg(ğ‘£â‹†; ğœğ´, 2ğ›¾) âˆ’1) Â·
ğœ€
ğ‘›âˆ’2
â‰¥ğœ€
6 ,
since outdeg(ğ‘£â‹†; ğœğ´, 2ğ›¾) > ğ‘›/3. By picking ğ‘›= ğ‘›0 we see that when the learner uses ğ‘š= ğ‘›0/ğœ€
samples then its expected error is at least ğœ€/6. Notice that when the learner uses ğ‘šâ€² = â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾)
samples we have that
E
ğ‘†âˆ¼ğ‘ƒğ‘šâ€²
ğ‘£â‹†
[ï¸‚
Pr
(ğ‘¥,ğ‘¦)âˆ¼ğ‘ƒğ‘£â‹†[â„“(ğ´(ğ‘†; ğ‘¥), ğ‘¦) > ğ›¾]
]ï¸‚
â‰¤ğ›¿+ (1 âˆ’ğ›¿)ğœ€â‰¤ğ›¿+ ğœ€â‰¤2ğœ€.
Thus, we see that for any algorithm ğ´
â„³ğ´(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¥â„¦
(ï¸ƒ
DOIG
2ğ›¾(â„‹)
ğœ€
)ï¸ƒ
,
hence
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¥â„¦
(ï¸ƒ
DOIG
2ğ›¾(â„‹)
ğœ€
)ï¸ƒ
.
22

C.2
Proof of the Upper Bound
Let us present the upper bound. For this proof, we need three tools: we will provide a weak learner
based on the scaled one-inclusion graph, a boosting algorithm for real-valued functions, and consistent
sample compression schemes for real-valued functions.
To this end, we introduce the one-inclusion graph (OIG) algorithm AOIG
ğ›¾
for realizable regression at
scale ğ›¾.
C.2.1
Scaled One-Inclusion Graph Algorithm and Weak Learning
First, we show that every scaled orientation ğœof the one-inclusion graph gives rise to a learner Ağœ
whose expected absolute loss is upper bounded by the maximum out-degree induced by ğœ.
Lemma 7 (From Orientations to Learners). Let ğ’Ÿğ’³be a distribution over ğ’³and â„â‹†âˆˆâ„‹âŠ†[0, 1]ğ’³,
let ğ‘›âˆˆN and ğ›¾âˆˆ(0, 1). Then, for any orientation ğœ: ğ¸ğ‘›â†’ğ‘‰ğ‘›of the scaled-one-inclusion graph
ğºOIG
â„‹
= (ğ‘‰ğ‘›, ğ¸ğ‘›), there exists a learner Ağœ: (ğ’³Ã— [0, 1])ğ‘›âˆ’1 â†’[0, 1]ğ’³, such that
E
ğ‘†âˆ¼ğ’Ÿğ‘›âˆ’1
ğ’³
[ï¸‚
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(Ağœ(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾]
]ï¸‚
â‰¤maxğ‘£âˆˆğ‘‰ğ‘›outdeg(ğ‘£; ğœ, ğ›¾)
ğ‘›
,
where Ağœis trained using a sample ğ‘†of size ğ‘›âˆ’1 realized by â„â‹†.
Algorithm 1 From orientation ğœto learner Ağœ
Input: An â„‹-realizable sample {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘›âˆ’1
ğ‘–=1 and a test point ğ‘¥âˆˆğ’³, ğ›¾âˆˆ(0, 1).
Output: A prediction Ağœ(ğ‘¥).
1. Create the one-inclusion graph ğºOIG
â„‹|(ğ‘¥1,...,ğ‘¥ğ‘›âˆ’1,ğ‘¥).
2. Consider the edge in direction ğ‘›defined by the realizable sample {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘›âˆ’1
ğ‘–=1 ; let
ğ‘’= {â„âˆˆâ„‹|(ğ‘¥1,...,ğ‘¥ğ‘›âˆ’1,ğ‘¥) : âˆ€ğ‘–âˆˆ[ğ‘›âˆ’1] â„(ğ‘–) = ğ‘¦ğ‘–} .
3. Return Ağœ(ğ‘¥) = ğœ(ğ‘’)(ğ‘›).
Proof. By the classical leave-one-out argument, we have that
E
ğ‘†âˆ¼ğ’Ÿğ‘›âˆ’1
ğ’³
[ï¸‚
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(Ağœ(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾]
]ï¸‚
=
E
(ğ‘†,(ğ‘¥,ğ‘¦))âˆ¼ğ’Ÿğ‘›[I{â„“(â„ğ‘†(ğ‘¥), ğ‘¦) > ğ›¾}] =
E
ğ‘†â€²âˆ¼ğ’Ÿğ‘›,ğ¼âˆ¼U([ğ‘›])[I{â„“(â„ğ‘†â€²
âˆ’ğ¼(ğ‘¥â€²
ğ¼), ğ‘¦â€²
ğ¼) > ğ›¾}] ,
where â„ğ‘†is the predictor Ağœusing the examples ğ‘†, and U([ğ‘›]) is the uniform distribution on
{1, . . . , ğ‘›}. Now for every fixed ğ‘†â€² we have that
E
ğ¼âˆ¼U([ğ‘›)][I{â„“(â„ğ‘†â€²
âˆ’ğ¼(ğ‘¥â€²
ğ¼), ğ‘¦â€²
ğ¼) > ğ›¾}] = 1
ğ‘›
âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]
I{â„“(ğœ(ğ‘’ğ‘–)(ğ‘–), ğ‘¦â€²
ğ‘–) > ğ›¾} = outdeg(ğ‘¦â€²; ğœ, ğ›¾)
ğ‘›
,
where ğ‘¦â€² is the node of the scaled OIG that corresponds to the true labeling of ğ‘†â€². By taking
expectation over ğ‘†â€² âˆ¼ğ’Ÿğ‘›we get that
E
ğ‘†â€²âˆ¼ğ’Ÿğ‘›,ğ¼âˆ¼U([ğ‘›])[I{â„“(â„ğ‘†â€²
âˆ’ğ¼(ğ‘¥â€²
ğ¼), ğ‘¦â€²
ğ¼) > ğ›¾}] â‰¤
E
ğ‘†â€²âˆ¼ğ’Ÿğ‘›
[ï¸‚outdeg(ğ‘¦â€²; ğœ, ğ›¾)
ğ‘›
]ï¸‚
â‰¤maxğ‘£âˆˆğ‘‰ğ‘›outdeg(ğ‘£; ğœ, ğ›¾)
ğ‘›
.
Equipped with the previous result, we are now ready to show that when the learner gets at least
DOIG
ğ›¾
(â„‹) samples as its training set, then its expected ğ›¾-cutoff loss is bounded away from 1/2.
Lemma 8 (Scaled OIG Guarantee (Weak Learner)). Let ğ’Ÿğ’³be a distribution over ğ’³and â„â‹†âˆˆâ„‹âŠ†
[0, 1]ğ’³, and ğ›¾âˆˆ(0, 1). Then, for all ğ‘›> DOIG
ğ›¾
(â„‹) there exists an orientation ğœâ‹†such that for the
prediction error of the one-inclusion graph algorithm AOIG
ğœâ‹†
: (ğ’³Ã— [0, 1])ğ‘›âˆ’1 Ã— ğ’³â†’[0, 1] , it holds
that
E
ğ‘†âˆ¼ğ’Ÿğ‘›âˆ’1
ğ’³
[ï¸‚
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³
[ï¸€
â„“(AOIG
ğœâ‹†(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾
]ï¸€]ï¸‚
â‰¤1/3 .
23

Proof. Fix ğ›¾âˆˆ(0, 1). Assume that ğ‘›> DOIG
ğ›¾
(â„‹) and let ğºOIG
â„‹|(ğ‘†,ğ‘¥) = (ğ‘‰ğ‘›, ğ¸ğ‘›) be the possibly
infinite scaled one-inclusion graph. By the definition of the ğ›¾-OIG dimension (see Definition 9), for
every finite subgraph ğº= (ğ‘‰, ğ¸) of ğºOIG
â„‹|(ğ‘†,ğ‘¥) there exists an orientation ğœ: ğ¸â†’ğ‘‰such that for
every vertex in ğºthe out-degree is at most ğ‘›/3, i.e.,
âˆ€ğ‘†âˆˆğ’³ğ‘›, âˆ€finite ğº= (ğ‘‰, ğ¸) of ğºOIG
â„‹|ğ‘†, âˆƒorientation ğœğ¸s.t. âˆ€ğ‘£âˆˆğ‘‰, it holds outdeg(ğ‘£; ğœğ¸, ğ›¾) â‰¤ğ‘›/3 .
First, we need to create an orientation of the whole (potentially infinite) one-inclusion graph.
We will create this orientation using the compactness theorem of first-order logic which states that a
set of formulas Î¦ is satisfiable if and only if it is finitely satisfiable, i.e., every finite subset Î¦â€² âŠ†Î¦
is satisfiable. Let ğºOIG
â„‹|ğ‘†= (ğ‘‰ğ‘›, ğ¸ğ‘›) be the (potentially infinite) one-inclusion graph of â„‹|ğ‘†. Let ğ’µ
be the set of pairs ğ‘§= (ğ‘£, ğ‘’) âˆˆğ‘‰ğ‘›Ã— ğ¸ğ‘›so that ğ‘£âˆˆğ‘’. Our goal is to assign binary values to each
ğ‘§âˆˆğ’µ. We define the following sets of formulas:
â€¢ For each ğ‘’âˆˆğ¸ğ‘›we let Î¦ğ‘’:= âˆƒexactly one ğ‘£âˆˆğ‘’: ğ‘§(ğ‘£, ğ‘’) = 1.
â€¢ For each ğ‘£âˆˆğ‘‰ğ‘›we let Î¦ğ‘£:= âˆƒat most ğ‘›/3 different ğ‘’ğ‘–,ğ‘“
âˆˆğ¸ğ‘›: ğ‘£âˆˆğ‘’ğ‘–,ğ‘“âˆ§
(âˆƒğ‘£â€² âˆˆğ‘’ğ‘–,ğ‘“: (ğ‘§(ğ‘£â€², ğ‘’) = 1 âˆ§â„“(ğ‘£â€²
ğ‘–, ğ‘£ğ‘–) > ğ›¾))
It is not hard to see that each Î¦ğ‘’, Î¦ğ‘£can be expressed in first-order logic. Then, we define
Î¦ :=
(ï¸ƒâ‹‚ï¸
ğ‘’âˆˆğ¸ğ‘›
Î¦ğ‘’
)ï¸ƒ
âˆ©
(ï¸ƒâ‹‚ï¸
ğ‘£âˆˆğ‘‰ğ‘›
Î¦ğ‘£
)ï¸ƒ
.
Notice that an orientation of the edges of ğºOIG
â„‹|ğ‘†is equivalent to picking an assignment of the elements
of ğ’µthat satisfies all the Î¦ğ‘’. Moreover, notice that for such an assignment, if all the Î¦ğ‘£are satisfied
then then maximum ğ›¾-scaled out-degree of ğºOIG
â„‹|ğ‘†is at most ğ‘›/3.
We will now show that Î¦ is finitely satisfiable. Let Î¦â€² be a finite subset of Î¦ and let ğ¸â€² âŠ†ğ¸ğ‘›, ğ‘‰â€² âŠ†
ğ‘‰ğ‘›, be the set of edges, vertices that appear in Î¦â€², respectively. If ğ‘‰â€² = âˆ…, then we can orient the
edges in ğ¸â€² arbitrarily and satisfy Î¦â€². Similarly, if ğ¸â€² = âˆ…we can let all the ğ‘§(ğ‘’, ğ‘£) = 0 and satisfy
all the Î¦ğ‘£, ğ‘£âˆˆğ‘‰â€². Thus, assume that both sets are non-empty. Consider the finite subgraph of ğºOIG
â„‹|ğ‘†
that is induced by ğ‘‰â€² and let ğ¸â€²â€² be the set of edges of this subgraph. For every edge ğ‘’âˆˆğ¸â€² âˆ–ğ¸â€²â€²4,
pick an arbitrary orientation, i.e, for exactly one ğ‘£âˆˆğ‘’set ğ‘§(ğ‘’, ğ‘£) = 1 and for the remaining ğ‘£â€² âˆˆğ‘’set
ğ‘§(ğ‘’, ğ‘£â€²) = 0. By the definition of DOIG
ğ›¾
(â„‹) there is an orientation ğœğ¸â€²â€² of the edges in ğ¸â€²â€² such that
âˆ€ğ‘£âˆˆğ‘‰â€²outdeg(ğ‘£; ğœğ¸â€²â€², ğ›¾) â‰¤ğ‘›/3. For every ğ‘’âˆˆğ¸â€²â€² pick the assignment of all the ğ‘§(ğ‘£, ğ‘’), ğ‘£âˆˆğ‘’,
according to the orientation ğœğ¸â€²â€². Thus, because of the maximum out-degree property of ğœğ¸â€²â€² we
described before, we can also see that all the Î¦ğ‘£, ğ‘£âˆˆğ‘‰â€², are satisfied. Hence, we have shown that Î¦
is finitely satisfiable, so it is satisfiable. This assignment on ğ‘§(ğ‘£, ğ‘’) induces an orientation ğœâ‹†under
which all the vertices of the one-inclusion graph have out-degree at most ğ‘›/3.
We will next use the orientation ğœâ‹†of ğºOIG
â„‹|ğ‘†= (ğ‘‰ğ‘›, ğ¸ğ‘›) to design a learner AOIG
ğœâ‹†
: (ğ’³Ã—[0, 1])ğ‘›âˆ’1Ã—
ğ’³â†’[0, 1], invoking Lemma 7. In particular, we get that, from Lemma 7 with the chosen orientation,
E
ğ‘†âˆ¼ğ’Ÿğ‘›âˆ’1
ğ’³
[ï¸‚
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³
[ï¸€
â„“(AOIG
ğœâ‹†(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾
]ï¸€]ï¸‚
â‰¤maxğ‘£âˆˆğ‘‰ğ‘›outdeg(ğ‘£; ğœâ‹†, ğ›¾)
ğ‘›
â‰¤1/3 ,
which concludes the proof.
C.2.2
Boosting Real-Valued Functions
Definition 15 (Weak Real-Valued Learner). Let â„“be a loss function. Let ğ›¾âˆˆ[0, 1], ğ›½âˆˆ(0, 1
2),
and â„‹âŠ†[0, 1]ğ’³. For a distribution ğ’Ÿğ’³over ğ’³and true target function â„â‹†âˆˆâ„‹, we say that
ğ‘“: ğ’³â†’[0, 1] is (ğ›¾, ğ›½)-weak learner with respect to ğ’Ÿğ’³and â„â‹†, if
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³
[ï¸
â„“(ğ‘“(ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾
]ï¸
< 1
2 âˆ’ğ›½.
4Since the edges in ğ¸â€²â€² are of finite length, we first need to map them to the appropriate edges in ğ¸â€².
24

Following [HKS19], we define the weighted median as
Median(ğ‘¦1, . . . , ğ‘¦ğ‘‡; ğ›¼1, . . . , ğ›¼ğ‘‡) = min
{ï¸ƒ
ğ‘¦ğ‘—:
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡I[ğ‘¦ğ‘—< ğ‘¦ğ‘¡]
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡
< 1
2
}ï¸ƒ
,
and the weighted quantiles, for ğœƒâˆˆ[0, 1/2], as
ğ‘„+
ğœƒ(ğ‘¦1, . . . , ğ‘¦ğ‘‡; ğ›¼1, . . . , ğ›¼ğ‘‡) = min
{ï¸ƒ
ğ‘¦ğ‘—:
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡I[ğ‘¦ğ‘—< ğ‘¦ğ‘¡]
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡
< 1
2 âˆ’ğœƒ
}ï¸ƒ
ğ‘„âˆ’
ğœƒ(ğ‘¦1, . . . , ğ‘¦ğ‘‡; ğ›¼1, . . . , ğ›¼ğ‘‡) = max
{ï¸ƒ
ğ‘¦ğ‘—:
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡I[ğ‘¦ğ‘—> ğ‘¦ğ‘¡]
âˆ‘ï¸€ğ‘‡
ğ‘¡=1 ğ›¼ğ‘¡
< 1
2 âˆ’ğœƒ
}ï¸ƒ
,
and
we
let
ğ‘„+
ğœƒ(ğ‘¥)
=
ğ‘„+
ğœƒ(â„1(ğ‘¥), . . . , â„ğ‘‡(ğ‘¥); ğ›¼1, . . . , ğ›¼ğ‘‡), ğ‘„âˆ’
ğœƒ(ğ‘¥)
=
ğ‘„âˆ’
ğœƒ(â„1(ğ‘¥), . . . , â„ğ‘‡(ğ‘¥); ğ›¼1, . . . , ğ›¼ğ‘‡),
where â„1, . . . , â„ğ‘‡, ğ›¼1, . . . , ğ›¼ğ‘‡
are the values returned
by Algorithm 2. The following guarantee holds for this procedure.
Lemma 9 (MedBoost guarantee [KÃ©g03]). Let â„“be the absolute loss and ğ‘†= {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘š
ğ‘–=1,
ğ‘‡= ğ‘‚
(ï¸€1
ğœƒ2 log(ğ‘š)
)ï¸€
. Let â„1, . . . , â„ğ‘‡and ğ›¼1, . . . , ğ›¼ğ‘‡be the functions and coefficients returned from
MedBoost. For any ğ‘–âˆˆ{1, . . . , ğ‘š} it holds that
max
{ï¸
â„“
(ï¸
ğ‘„+
ğœƒ/2(ğ‘¥ğ‘–), ğ‘¦ğ‘–
)ï¸
, â„“
(ï¸
ğ‘„âˆ’
ğœƒ/2(ğ‘¥ğ‘–), ğ‘¦ğ‘–
)ï¸}ï¸
â‰¤ğ›¾.
Algorithm 2 MedBoost [KÃ©g03]
Input: ğ‘†= {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘š
ğ‘–=1.
Parameters: ğ›¾, ğ›½, ğ‘‡.
Initialize ğ’«1 = Uniform(ğ‘†).
For ğ‘¡= 1, . . . , ğ‘‡:
1. Find a (ğ›¾, ğ›½)-weak learner â„ğ‘¡with respect to (ğ‘¥ğ‘–, ğ‘¦ğ‘–) âˆ¼ğ’«ğ‘¡, using a subset ğ‘†ğ‘¡âŠ†ğ‘†.
2. For ğ‘–= 1, . . . , ğ‘š:
(a) Set ğ‘¤(ğ‘¡)
ğ‘–
= 1 âˆ’2I
[ï¸€
â„“(â„ğ‘¡(ğ‘¥ğ‘–), ğ‘¦ğ‘–) > ğ›¾
]ï¸€
.
(b) Set ğ›¼ğ‘¡= 1
2 log
(ï¸‚
(1âˆ’ğ›¾) âˆ‘ï¸€ğ‘›
ğ‘–=1 ğ’«ğ‘¡(ğ‘¥ğ‘–,ğ‘¦ğ‘–)I
[ï¸
ğ‘¤(ğ‘¡)
ğ‘–
=1
]ï¸
(1+ğ›¾) âˆ‘ï¸€ğ‘›
ğ‘–=1 ğ’«ğ‘¡(ğ‘¥ğ‘–,ğ‘¦ğ‘–)I
[ï¸
ğ‘¤(ğ‘¡)
ğ‘–
=âˆ’1
]ï¸
)ï¸‚
.
(c)
â€¢ If ğ›¼ğ‘¡= âˆ: return ğ‘‡copies of â„ğ‘¡, (ğ›¼1 = 1, . . . , ğ›¼ğ‘‡= 1), and ğ‘†ğ‘¡.
â€¢ Else:
ğ‘ƒğ‘¡+1(ğ‘¥ğ‘–, ğ‘¦ğ‘–)
=
ğ‘ƒğ‘¡(ğ‘¥ğ‘–, ğ‘¦ğ‘–) exp(âˆ’ğ›¼ğ‘¡ğ‘¤ğ‘¡
ğ‘–) /ğ‘ğ‘¡,
where
ğ‘ğ‘¡
=
âˆ‘ï¸€ğ‘›
ğ‘—=1 ğ’«ğ‘¡(ğ‘¥ğ‘—, ğ‘¦ğ‘—) exp
(ï¸€
âˆ’ğ›¼ğ‘¡ğ‘¤ğ‘¡
ğ‘—
)ï¸€
.
Output: Functions â„1, . . . , â„ğ‘‡, coefficients ğ›¼1, . . . , ğ›¼ğ‘‡and sets ğ‘†1, . . . , ğ‘†ğ‘‡.
C.2.3
Generalization via Sample Compression Schemes
Sample compression scheme is a classic technique for proving generalization bounds, introduced
by [LW86, FW95]. These bounds proved to be useful in numerous learning settings, such as
binary classification [GHST05, MY16, BHMZ20], multiclass classification [DSBDSS15, DSS14,
DMY16, BCD+22], regression [HKS18, HKS19], active learning [WHEY15], density estimation
[ABDH+20], adversarially robust learning [MHS19, MHS20, MHS21, MHS22, AHM22, AH22],
learning with partial concepts [AHHM22], and showing Bayes-consistency for nearest-neighbor
methods [GKN14, KSW17]. As a matter of fact, compressibility and learnability are known to be
equivalent for general learning problems [DMY16]. Another remarkable result by [MY16] showed
that VC classes enjoy a sample compression that is independent of the sample size.
We start with a formal definition of a sample compression scheme.
Definition 16 (Sample compression scheme). A pair of functions (ğœ…, ğœŒ) is a sample compression
scheme of size â„“for class â„‹if for any ğ‘›âˆˆN, â„âˆˆâ„‹and sample ğ‘†= {(ğ‘¥ğ‘–, â„(ğ‘¥ğ‘–))}ğ‘›
ğ‘–=1, it holds
for the compression function that ğœ…(ğ‘†) âŠ†ğ‘†and |ğœ…(ğ‘†) | â‰¤â„“, and the reconstruction function
ğœŒ(ğœ…(ğ‘†)) = Ë†â„satisfies Ë†â„(ğ‘¥ğ‘–) = â„(ğ‘¥ğ‘–) for any ğ‘–âˆˆ[ğ‘›].
25

We show a generalization bound that scales with the sample compression size. The proof follows
from [LW86].
Lemma 10 (Sample compression scheme generalization bound). Fix a margin ğ›¾âˆˆ[0, 1]. For any
ğ‘˜âˆˆN and fixed function ğœ‘: (ğ’³Ã— [0, 1])ğ‘˜â†’[0, 1]ğ’³, for any distribution ğ’Ÿover ğ’³Ã— [0, 1] and
any ğ‘šâˆˆN, for ğ‘†= {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘–âˆˆ[ğ‘š] i.i.d. ğ’Ÿ-distributed random variables, if there exist indices
ğ‘–1, ..., ğ‘–ğ‘˜âˆˆ[ğ‘š] such that âˆ‘ï¸€
(ğ‘¥,ğ‘¦)âˆˆğ‘†I{â„“(ğœ‘((ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜))(ğ‘¥), ğ‘¦) > ğ›¾} = 0, then
E
(ğ‘¥,ğ‘¦)âˆ¼ğ’Ÿ[I{â„“(ğœ‘((ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜))(ğ‘¥), ğ‘¦) > ğ›¾}] â‰¤
1
ğ‘šâˆ’ğ‘˜(ğ‘˜log ğ‘š+ log(1/ğ›¿)) .
with probability at least 1 âˆ’ğ›¿over ğ‘†.
Proof. Let us define Ì‚ï¸€â„“ğ›¾(â„; ğ‘†)
=
1
|ğ‘†|
âˆ‘ï¸€
(ğ‘¥,ğ‘¦)âˆˆğ‘†I{â„“(â„(ğ‘¥), ğ‘¦)
>
ğ›¾} and â„“ğ›¾(â„; ğ’Ÿ)
=
E(ğ‘¥,ğ‘¦)âˆ¼ğ’Ÿ[I{â„“(â„(ğ‘¥), ğ‘¦) > ğ›¾}]. For any indices ğ‘–1, ..., ğ‘–ğ‘˜âˆˆ[ğ‘š], the probability of the bad event
Pr
ğ‘†âˆ¼ğ’Ÿğ‘š[Ì‚ï¸€â„“ğ›¾(ğœ‘((ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜)); ğ‘†) = 0 âˆ§â„“ğ›¾(ğœ‘((ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜)); ğ’Ÿ) > ğœ€]
is at most
E
[ï¸
I{â„“ğ›¾(ğœ‘({(ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–ğ‘—)}ğ‘—âˆˆ[ğ‘˜]); ğ’Ÿ) > ğœ€} Pr[Ì‚ï¸€â„“ğ›¾(ğœ‘({(ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–ğ‘—)}ğ‘—âˆˆ[ğ‘˜]); ğ‘†âˆ–{(ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–ğ‘—)}ğ‘—âˆˆ[ğ‘˜]) = 0|{(ğ‘¥ğ‘–ğ‘—, ğ‘¦ğ‘–ğ‘—)}ğ‘—âˆˆ[ğ‘˜]]
]ï¸
< (1 âˆ’ğœ€)ğ‘šâˆ’ğ‘˜
where the expectation is over (ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜) and the inner probability is over ğ‘†âˆ–
(ğ‘¥ğ‘–1, ğ‘¦ğ‘–1), ..., (ğ‘¥ğ‘–ğ‘˜, ğ‘¦ğ‘–ğ‘˜). Taking a union bound over all ğ‘šğ‘˜possible choices for the ğ‘˜indices, we get
that the bad event occurs with probability at most
ğ‘šğ‘˜exp(âˆ’ğœ€(ğ‘šâˆ’ğ‘˜)) â‰¤ğ›¿â‡’ğœ€=
1
ğ‘šâˆ’ğ‘˜(ğ‘˜log ğ‘š+ log(1/ğ›¿)) .
C.3
Putting it Together
We now have all the necessary ingredients in place to prove the upper bound of Theorem 2. First,
we use Lemma 8 on a sample of size ğ‘›0 = DOIG
ğ›¾
(â„‹) to obtain a learner which makes ğ›¾-errors with
probability at most 1/35. Then, we use the boosting algorithm we described (see Algorithm 2) to
obtain a learner that does not make any ğ›¾-mistakes on the training set. Notice that the boosting
algorithm on its own does not provide any guarantees about the generalization error of the procedure.
This is obtained through the sample compression result we described in Appendix C.2.3. Since we
run the boosting algorithm for a few rounds on a sample whose size is small, we can provide a sample
compression scheme following the approach of [DMY16, HKS19].
Lemma 11 (Upper Bound of PAC Regression). Let â„‹âŠ†[0, 1]ğ’³and ğœ€, ğ›¿, ğ›¾âˆˆ(0, 1)3. Then,
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¤ğ‘‚
(ï¸ƒ
DOIG
ğ›¾
(â„‹)
ğœ€
log2 DOIG
ğ›¾
(â„‹)
ğœ€
+ 1
ğœ€log 1
ğ›¿
)ï¸ƒ
.
Proof. Let ğ‘›be the number of samples ğ‘†= ((ğ‘¥1, ğ‘¦1), . . . , (ğ‘¥ğ‘›, ğ‘¦ğ‘›)) that are available to the learner,
ğ‘›0 = DOIG
ğ›¾
(â„‹) and let ğ´be the algorithm obtained from Lemma 8. We have that
E
ğ‘†âˆ¼ğ’Ÿğ‘›0âˆ’1
ğ’³
[ï¸‚
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†; ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾]
]ï¸‚
â‰¤1/3 .
This means that, for any distribution ğ’Ÿğ’³and any labeling function â„â‹†âˆˆâ„‹we can draw a sample
ğ‘†â‹†= ((ğ‘¥1, ğ‘¦1), . . . , (ğ‘¥ğ‘›0âˆ’1, ğ‘¦ğ‘›0âˆ’1)) with non-zero probability such that
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³[â„“(ğ´(ğ‘†â‹†; ğ‘¥), â„â‹†(ğ‘¥)) > ğ›¾] â‰¤1
3 .
5In expectation over the training set.
26

Notice that such a classifier is a (ğ›¾, 1/6)-weak learner (see Definition 15). Thus, by executing the
MedBoost algorithm (see Algorithm 2) for ğ‘‡= ğ‘‚(log ğ‘›) rounds we obtain a classifier Ë†â„: ğ’³â†’R
such that, â„“(Ë†â„(ğ‘¥ğ‘–), ğ‘¦ğ‘–) â‰¤ğ›¾, âˆ€ğ‘–âˆˆ[ğ‘›]. We underline that the subset ğ‘†ğ‘¡that is used in line 1 of
Algorithm 2 has size at most ğ‘›0, for all rounds ğ‘¡âˆˆ[ğ‘‡]. Thus, the total number of samples that is used
by MedBoost is at most ğ‘‚(ğ‘›0 log ğ‘›). Hence, following the approach of [MY16] we can encode the
classifiers produced by MedBoost as a compression set that consists of ğ‘˜= ğ‘‚(ğ‘›0 log ğ‘›) samples
that were used to train the classifiers along with ğ‘˜log ğ‘˜extra bits that indicate their order. Thus, using
generalization based on sample compression scheme as in Lemma 10, we have that with probability
at least 1 âˆ’ğ›¿over ğ‘†âˆ¼ğ’Ÿğ‘›,
E
(ğ‘¥,ğ‘¦)âˆ¼ğ’Ÿ
[ï¸
I
{ï¸
â„“(Ë†â„(ğ‘¥), ğ‘¦) > ğ›¾
}ï¸]ï¸
â‰¤
ğ¶
ğ‘›âˆ’ğ‘›0 log(ğ‘›)
(ï¸€
ğ‘›0 log2 ğ‘›+ log(1/ğ›¿)
)ï¸€
,
which means that for large enough ğ‘›,
E
(ğ‘¥,ğ‘¦)âˆ¼ğ’Ÿ
[ï¸
I
{ï¸
â„“(Ë†â„(ğ‘¥), ğ‘¦) > ğ›¾
}ï¸]ï¸
â‰¤ğ‘‚
(ï¸‚ğ‘›0 log2 ğ‘›
ğ‘›
+ log(1/ğ›¿)
ğ‘›
)ï¸‚
.
Thus,
Pr
(ğ‘¥,ğ‘¦)âˆ¼ğ’Ÿ
[ï¸
â„“(Ë†â„(ğ‘¥), ğ‘¦) > ğ›¾
]ï¸
â‰¤ğ‘‚
(ï¸‚ğ‘›0 log2 ğ‘›
ğ‘›
+ log(1/ğ›¿)
ğ‘›
)ï¸‚
.
Hence, we can see that
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¤ğ‘‚
(ï¸ƒ
DOIG
ğ›¾
(â„‹)
ğœ€
log2 DOIG
ğ›¾
(â„‹)
ğœ€
+ 1
ğœ€log 1
ğ›¿
)ï¸ƒ
.
D
ğ›¾-DS Dimension and Learnability
In this section, we will show that finiteness of ğ›¾-DS dimension is necessary for PAC learning in the
realizable case.
Theorem 3. Let â„‹âŠ†[0, 1]ğ’³, ğœ€, ğ›¿, ğ›¾âˆˆ(0, 1)3. Then,
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) â‰¥â„¦
(ï¸ƒ
DDS
2ğ›¾(â„‹) + log(1/ğ›¿)
ğœ€
)ï¸ƒ
.
Proof. Let ğ‘‘= DOIG
2ğ›¾(â„‹). Then, there exists some ğ‘†= (ğ‘¥1, . . . , ğ‘¥ğ‘‘) âˆˆğ’³ğ‘‘such that â„‹|ğ‘†contains
a 2ğ›¾-pseudo-cube, which we call â„‹â€². By the definition of the scaled pseudo-cube, âˆ€â„âˆˆâ„‹â€², ğ‘–âˆˆ[ğ‘‘],
there is exactly one â„â€² âˆˆâ„‹â€² such that â„(ğ‘¥ğ‘—) = â„â€²(ğ‘¥ğ‘—), ğ‘—Ì¸= ğ‘–, and â„“(â„(ğ‘¥ğ‘–), â„â€²(ğ‘¥ğ‘–)) > 2ğ›¾. We pick
the target function â„â‹†uniformly at random among the hypotheses of â„‹â€² and we set the marginal
distribution ğ’Ÿğ’³of ğ’Ÿas follows
Pr[ğ‘¥1] = 1 âˆ’2ğœ€,
Pr[ğ‘¥ğ‘–] = 2ğœ€/(ğ‘‘âˆ’1), âˆ€ğ‘–âˆˆ{2, ..., ğ‘‘} .
Consider ğ‘šsamples {(ğ‘§ğ‘–, â„â‹†(ğ‘§ğ‘–)}ğ‘–âˆˆ[ğ‘š] drawn i.i.d. from ğ’Ÿ. Let us take ğ‘šâ‰¤ğ‘‘âˆ’1
6ğœ€. Then, the sample
will include at most (ğ‘‘âˆ’1)/2 examples which are not ğ‘¥1 with probability 1/100, using Chernoffâ€™s
bound. Let us call this event ğ¸. Conditioned on ğ¸, the posterior distribution of the unobserved
points is uniform among the vertices of the ğ‘‘/2-dimensional 2ğ›¾-pseudo-cube. Thus, if the test
point ğ‘¥falls among the unobserved points, the learner will make a ğ›¾-mistake with probability at
least 1/2. To see that, let Ì‚ï¸€ğ‘¦be the prediction of the learner on ğ‘¥. Since every hyperedge has size at
least 2 and all the vertices that are on the hyperedge differ by at least 2ğ›¾in the direction of ğ‘¥, no
matter what Ì‚ï¸€ğ‘¦is the correct label ğ‘¦â‹†is at least ğ›¾-far from it. Since Pr[ğ¸] â‰¥1/100, we can see that
â„³ğ’œ(â„‹; ğœ€, ğ›¿, ğ›¾) = â„¦( ğ‘‘
ğœ€). Moreover, by the law of total probability there must exist a deterministic
choice of the target function â„â‹†, that could depend on ğ’œ, which satisfies the lower bound. For
the other part of the lower bound, notices the probability that the sample will only contain ğ‘¥1 is
27

(1 âˆ’2ğœ€)ğ‘šâ‰¥ğ‘’âˆ’4ğœ€ğ‘šwhich is greater that ğ›¿whenever ğ‘šâ‰¤log(1/ğ›¿)/(4ğœ€). This implies that the
ğ›¾-cut-off sample complexity is lower bounded by
max
{ï¸‚
ğ¶1 Â· ğ‘‘
ğœ€, ğ¶2 Â· log(1/ğ›¿)
ğœ€
}ï¸‚
= ğ¶0 Â· DDS
2ğ›¾(â„‹) + log(1/ğ›¿)
ğœ€
.
Thus â„³ğ’œ(â„‹; ğœ€, ğ›¿, ğ›¾), satisfies the desired bound when the dimension is finite. Finally, it remains
to claim about the case where DDS
2ğ›¾(â„‹) = âˆfor the given ğ›¾. We consider a sequence of 2ğ›¾-DS
shattered sets ğ‘†ğ‘›with |ğ‘†ğ‘›| = ğ‘›and repeat the claim for the finite case. This will yield that for any
ğ‘›the ğ›¾-cut-off sample complexity is lower bounded by â„¦((ğ‘›+ log(1/ğ›¿))/ğœ€) and this yields that
â„³(â„‹; ğœ€, ğ›¿, ğ›¾) = âˆ.
We further conjecture that this dimension is also sufficient for PAC learning.
Conjecture 2. A class â„‹âŠ†(0, 1)ğ’³is PAC learnable in the realizable regression setting with respect
to the absolute loss function if and only if DDS
ğ›¾(â„‹) < âˆfor any ğ›¾âˆˆ(0, 1).
We believe that there must exist a modification of the approach of [BCD+22] that will be helpful in
settling the above conjecture.
Conjecture 3. There exists â„‹âŠ†(0, 1)ğ’³for which DNat
ğ›¾
(â„‹) = 1 for all ğ›¾âˆˆ(0, 1) but DDS
ğ›¾(â„‹) < âˆ
for some ğ›¾âˆˆ(0, 1).
In particular, we believe that one can extend the construction of [BCD+22] (which uses various
tools from algebraic topology as a black-box) and obtain a hypothesis class â„‹âŠ†[0, 1]ğ’³that has
ğ›¾-Natarajan dimension 1 but is not PAC learnable (it will have infinite ğ›¾-DS dimension). This
construction though is not immediate and requires new ideas related to the works of [JÂ´S03, Osa13]
E
Online Realizable Regression
In this section, we present our results regarding online realizable regression. The next result resolves
an open question of [DG22]. It provides an online learner with optimal (off by a factor of 2)
cumulative loss in realizable regression.
Theorem 4 (Optimal Cumulative Loss). Let â„‹âŠ†[0, 1]ğ’³and ğœ€> 0. Then, there exists a deterministic
algorithm (Algorithm 3) whose cumulative loss in the realizable setting is bounded by Donl(â„‹) + ğœ€.
Conversely, for any ğœ€> 0, every deterministic algorithm in the realizable setting incurs loss at least
Donl(â„‹)/2 âˆ’ğœ€.
Algorithm 3 Scaled SOA
Parameters: {ğœ€ğ‘¡}ğ‘¡âˆˆN.
Initialize ğ‘‰(1) = â„‹.
For ğ‘¡= 1, . . .:
1. Receive ğ‘¥ğ‘¡âˆˆğ’³.
2. For every ğ‘¦âˆˆ[0, 1], let ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦) =
{ï¸€
â„âˆˆğ‘‰(ğ‘¡) : â„(ğ‘¥ğ‘¡) = ğ‘¦
}ï¸€
.
3. Let Ì‚ï¸€ğ‘¦ğ‘¡be an arbitrary label such that
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,Ì‚ï¸€ğ‘¦ğ‘¡)
)ï¸
â‰¥sup
ğ‘¦â€² Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â€²)
)ï¸
âˆ’ğœ€ğ‘¡.
4. Predict Ì‚ï¸€ğ‘¦ğ‘¡.
5. Receive the true label ğ‘¦â‹†
ğ‘¡and incur loss â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦â‹†
ğ‘¡).
6. Update ğ‘‰(ğ‘¡+1) =
{ï¸€
â„âˆˆğ‘‰(ğ‘¡) : â„(ğ‘¥ğ‘¡) = ğ‘¦â‹†
ğ‘¡
}ï¸€
.
Proof. Let us begin with the upper bound. Assume that Donl(â„‹) < âˆ. Suppose we are pre-
dicting on the ğ‘¡-th point in the sequence and let ğ‘‰(ğ‘¡) be the version space so far, i.e., ğ‘‰(ğ‘¡) =
28

{â„âˆˆâ„‹: âˆ€ğœâˆˆ[ğ‘¡âˆ’1], â„(ğ‘¥ğœ) = ğ‘¦ğœ}. Let ğ‘¥ğ‘¡be the next point to predict on. For each label ğ‘¦âˆˆR,
let ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦) = {â„âˆˆğ‘‰(ğ‘¡) : â„(ğ‘¥ğ‘¡) = ğ‘¦}. From the definition of the dimension Donl, we know that for
all ğ‘¦, ğ‘¦â€² âˆˆR such that ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦), ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â€²) Ì¸= âˆ…,
Donl(ğ‘‰ğ‘¡) â‰¥â„“(ğ‘¦, ğ‘¦â€²) + min
{ï¸
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦)
)ï¸
, Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â€²)
)ï¸}ï¸
.
Let Ì‚ï¸€ğ‘¦ğ‘¡be an arbitrary label with Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,Ì‚ï¸€ğ‘¦ğ‘¡)
)ï¸
â‰¥supğ‘¦â€² Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â€²)
)ï¸
âˆ’ğœ€ğ‘¡, where ğœ€ğ‘¡is some
sequence shrinking arbitrarily quickly in the number of rounds ğ‘¡. The learner predicts Ì‚ï¸€ğ‘¦ğ‘¡. Assume
that the adversary picks ğ‘¦â‹†
ğ‘¡as the true label and, so, the learner incurs loss â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦â‹†
ğ‘¡) at round ğ‘¡. Then,
the updated version space ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡) has
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸
â‰¤sup
ğ‘¦â€² Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â€²)
)ï¸
â‰¤Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,Ì‚ï¸€ğ‘¦ğ‘¡)
)ï¸
+ ğœ€ğ‘¡,
which implies
min
{ï¸
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,Ì‚ï¸€ğ‘¦ğ‘¡)
)ï¸
, Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸}ï¸
â‰¥Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥,ğ‘¦â‹†
ğ‘¡)
)ï¸
âˆ’ğœ€ğ‘¡.
This gives that
Donl(ğ‘‰(ğ‘¡)) â‰¥â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦*
ğ‘¡) + min
{ï¸
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,Ì‚ï¸€ğ‘¦ğ‘¡)
)ï¸
, Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸}ï¸
â‰¥â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦*
ğ‘¡) + Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸
âˆ’ğœ€ğ‘¡,
and, by re-arranging,
Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸
â‰¤Donl(ğ‘‰(ğ‘¡)) âˆ’â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦*
ğ‘¡) + ğœ€ğ‘¡.
(1)
So every round reduces the dimension by at least the magnitude of the loss (minus ğœ€ğ‘¡). Notice that
Donl(ğ‘‰(ğ‘¡+1)) = Donl (ï¸
ğ‘‰(ğ‘¡)
(ğ‘¥ğ‘¡,ğ‘¦â‹†
ğ‘¡)
)ï¸
. Thus, by choosing the {ğœ€ğ‘¡}ğ‘¡âˆˆN sequence such that
âˆ‘ï¸
ğ‘¡
ğœ€ğ‘¡â‰¤ğœ€â€² ,
and summing up Equation (1) over all ğ‘¡âˆˆN, we get a cumulative loss bound
âˆ‘ï¸
ğ‘¡
â„“(Ì‚ï¸€ğ‘¦ğ‘¡, ğ‘¦â‹†
ğ‘¡) â‰¤Donl(â„‹) + ğœ€â€² .
Hence, we see that by taking the limit as ğœ€â€² goes to 0 shows that the cumulative loss is upper bounded
by Donl(â„‹). This analysis shows that Algorithm 3 achieves the cumulative loss bound Donl(â„‹) + ğœ€â€²,
for arbitrarily small ğœ€â€² > 0.
Let us continue with the lower bound. For any ğœ€> 0, we are going to prove that any deterministic
learner must incur cumulative loss at least Donl(â„‹)/2 âˆ’ğœ€. By the definition of Donl(â„‹), for any
ğœ€> 0, there exists a tree ğ‘‡ğœ€such that, for every path ğ‘¦,
âˆ
âˆ‘ï¸
ğ‘–=1
ğ›¾ğ‘¦â‰¤ğ‘–â‰¥Donl(â„‹) âˆ’2ğœ€,
i.e., the sum of the gaps across the path is at least Donl(â„‹) âˆ’2ğœ€. The strategy of the adversary is
the following: in the first round, she presents the learner with the instance ğ‘¥1 = ğ‘¥âˆ…. Then, no matter
what label Ë†ğ‘¦1 the learner picks, the adversary can choose the label ğ‘¦â‹†
1 so that |Ë†ğ‘¦1 âˆ’ğ‘¦â‹†
1| â‰¥ğ›¾âˆ…/2. The
adversary can keep picking the instances ğ‘¥ğ‘¡based on the induced path of the choices of the true
labels {ğ‘¦â‹†
ğœ}ğœ<ğ‘¡and the loss of the learner in every round ğ‘¡is at least ğ›¾ğ‘¦â‰¤ğ‘¡/2. Thus, summing up over
all the rounds as ğ‘‡â†’âˆ, we see that the total loss of the learner is at least
Donl(â„‹)
2
âˆ’ğœ€.
29

Remark 2 (Randomized Online Learners). We highlight that, unlike the setting of realizable online
classification, in the case of realizable online regression randomization does not seem to help
the learner (see also [FHMM23]). In particular, the lower bound of Donl(â„‹)
2
âˆ’ğœ€holds even for
randomized learners. To see it, notice that for all distributions over ğ’Ÿover [0, 1] it holds that
max
ğ‘1,ğ‘2
{ï¸
E
ğ‘‹âˆ¼ğ’Ÿ[â„“(ğ‘‹, ğ‘1)], E
ğ‘‹âˆ¼ğ’Ÿ[â„“(ğ‘‹, ğ‘2)]
}ï¸
â‰¥â„“(ğ‘1, ğ‘2)/2 .
Example 2 (Sequential Complexity Measures). Sequential fat-shattering dimension and sequen-
tial covering numbers are two standard combinatorial measures for regression in online settings
[RST15b, RST15a]. Note that Example 1 can be learned with 1 sample even in the online realizable
setting. Hence, Example 1 shows that sequential fat-shattering dimension fails to characterize online
realizable regression (since this dimension is at least as large as fat-shattering dimension which
is infinite in this example). Moreover, we know that sequential covering numbers and sequential
fat-shattering dimension are of the same order of magnitude and so they are also infinite in the case
of Example 1.
F
Dimension and Finite Character Property
[BDHM+19] gave a formal definition of the notion of â€œdimensionâ€ or â€œcomplexity measureâ€, that all
previously proposed dimensions in statistical learning theory comply with. In addition to characteriz-
ing learnability, a dimension should satisfy the finite character property:
Definition 17 (Finite Character [BDHM+19]). A dimension characterizing learnability can be
abstracted as a function ğ¹that maps a class â„‹to N âˆª{âˆ} and satisfies the finite character property:
for every ğ‘‘âˆˆN and â„‹, the statement â€œğ¹(â„‹) â‰¥ğ‘‘â€ can be demonstrated by a finite set ğ‘‹âŠ†ğ’³
of domain points, and a finite set of hypotheses ğ»âŠ†â„‹. That is, â€œğ¹(â„‹) â‰¥ğ‘‘â€ is equivalent to
the existence of a bounded first order formula ğœ‘(ğ’³, â„‹) in which all the quantifiers are of the form:
âˆƒğ‘¥âˆˆğ’³, âˆ€ğ‘¥âˆˆğ’³or âˆƒâ„âˆˆâ„‹, âˆ€â„âˆˆâ„‹.
Claim 1. The scaled one-inclusion graph dimension DOIG
ğ›¾
(â„‹) satisfies the finite character property.
Proof. To demonstrate that DOIG(â„‹) â‰¥ğ‘‘, it suffices to find a set ğ‘†of ğ‘›domain points and present
a finite subgraph ğº= (ğ‘‰, ğ¸) of the one-inclusion hypergraph induced by ğ‘†where every orientation
ğœ: ğ¸â†’ğ‘‰has out-degree at least ğ‘›/3. Note that ğ‘‰is, by definition, a finite collection of datasets
realizable by â„‹and so this means that we can demonstrate that DOIG(â„‹) â‰¥ğ‘‘with a finite set of
domain points and a finite set of hypotheses.
G
Examples for Scaled Graph Dimension
These examples are adaptations from [DSS14, DSBDSS15].
Example 3 (Large Gap Between ERM Learners). For every ğ‘‘âˆˆN, consider a domain ğ’³ğ‘‘such that
|ğ’³ğ‘‘| = ğ‘‘and ğ’³ğ‘‘, ğ’³ğ‘‘â€² are disjoint for ğ‘‘Ì¸= ğ‘‘â€². For all ğ‘‘âˆˆN, let ğ‘ƒ(ğ’³ğ‘‘) denote the collection of all
finite and co-finite6 subsets of ğ’³ğ‘‘. Let us fix ğ›¾âˆˆ(0, 1). Consider a mapping ğ‘“: âˆªğ‘‘âˆˆNğ‘ƒ(ğ’³ğ‘‘) â†’
[0, 1] such that ğ‘“(ğ´ğ‘‘) âˆˆ(ğ›¾, 1) for all ğ‘‘âˆˆN, ğ´ğ‘‘âˆˆğ‘ƒ(ğ’³ğ‘‘), and ğ‘“(ğ´ğ‘‘) Ì¸= ğ‘“(ğ´â€²
ğ‘‘â€²) for all ğ´ğ‘‘Ì¸=
ğ´â€²
ğ‘‘â€², ğ´ğ‘‘âˆˆğ‘ƒ(ğ’³ğ‘‘), ğ´ğ‘‘â€² âˆˆğ‘ƒ(ğ’³ğ‘‘â€²). Such a mapping exists due to the density of the reals. For any
ğ‘‘âˆˆN, ğ´ğ‘‘âŠ†ğ’³ğ‘‘, let â„ğ´ğ‘‘(ğ‘¥) = ğ‘“(ğ´ğ‘‘) Â· 1{ğ‘¥âˆˆğ´ğ‘‘} and consider the scaled first Cantor class
â„‹ğ’³ğ‘‘,ğ›¾= {â„ğ´ğ‘‘: ğ´ğ‘‘âˆˆğ‘ƒ(ğ’³ğ‘‘)}. We claim that DNat
ğ›¾
(â„‹ğ’³ğ‘‘,ğ›¾) = 1 and that DG
ğ›¾(â„‹ğ’³ğ‘‘,ğ›¾) = |ğ’³ğ‘‘| = ğ‘‘
since one can use ğ‘“âˆ…for the ğ›¾-graph shattering. Consider the following two ERM learners for the
scaled first Cantor class â„‹ğ’³ğ‘‘,ğ›¾:
1. Whenever a sample of the form ğ‘†= {(ğ‘¥ğ‘–, 0)}ğ‘–âˆˆ[ğ‘›] is observed, the first algorithm outputs
â„âˆª{ğ‘¥ğ‘–}ğ‘
ğ‘–âˆˆ[ğ‘›] which minimizes the empirical error. If the sample contains a non-zero element,
the ERM learner identifies the correct hypothesis. The sample complexity of PAC learning is
â„¦(ğ‘‘).
2. The second algorithm either returns the all-zero function or identifies the correct hypothesis
if the sample contains a non-zero label. This is a good ERM learner ğ’œERM
good with sample
complexity ğ‘š(ğœ€, ğ›¿) = 1
ğœ€log
(ï¸€1
ğ›¿
)ï¸€
.
6A set ğ‘†âŠ†ğ’³ğ‘‘is co-finite if its complement ğ‘†ğ‘is finite.
30

The construction that illustrates the poor performance of the first learner is exactly the same as in
the proof of the lower bound of Theorem 1. The second part of the example is formally shown in
Claim 2, which follows.
Claim 2 (Good ERM Learner). Let ğœ€, ğ›¿âˆˆ(0, 1)2. Then, the good ERM learner of Example 3 has
sample complexity â„³(ğœ€, ğ›¿) = 1
ğœ€log
(ï¸€1
ğ›¿
)ï¸€
.
Proof. Let ğ‘‘âˆˆN, ğ’Ÿğ’³ğ‘‘be a distribution over ğ’³ğ‘‘and â„ğ´â‹†
ğ‘‘be the labeling function. Consider a sample
ğ‘†of length ğ‘š. If the learner observes a value that is different from 0 among the labels in ğ‘†, then
it will be able to infer â„ğ´â‹†
ğ‘‘and incur 0 error. On the other hand, if the learner returns the all zero
function its error can be bounded as
E
ğ‘¥âˆ¼ğ’Ÿğ’³ğ‘‘
[â„“(â„âˆ…(ğ‘¥), â„ğ´â‹†
ğ‘‘(ğ‘¥))] â‰¤
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³ğ‘‘
[ğ‘¥âˆˆğ´â‹†
ğ‘‘] .
Since in all the ğ‘š= 1
ğœ€log
(ï¸€1
ğ›¿
)ï¸€
draws of the training set ğ‘†there were no elements from ğ´â‹†
ğ‘‘we can
see that, with probability at least 1 âˆ’ğ›¿over the draws of ğ‘†it holds that
Pr
ğ‘¥âˆ¼ğ’Ÿğ’³ğ‘‘
[ğ‘¥âˆˆğ´â‹†
ğ‘‘] â‰¤ğœ€.
Thus, the algorithm satisfies the desired guarantees.
The next example shows that no proper algorithm can be optimal in the realizable regression setting.
Example 4 (No Optimal PAC Learner Can be Proper). Let ğ’³ğ‘‘contain ğ‘‘elements and let ğ›¾âˆˆ(0, 1).
Consider the subclass of the scaled first Cantor class (see Example 3) with â„‹â€²
ğ‘‘,ğ›¾= {â„ğ´: ğ´âˆˆ
ğ‘ƒ(ğ’³ğ‘‘), |ğ´| = âŒŠğ‘‘/2âŒ‹}. First, since this class is contained in the scaled first Cantor class, we can
employ the good ERM and learn it. However, this learner is improper since â„âˆ…/âˆˆâ„‹â€²
ğ‘‘,ğ›¾. Then, no
proper algorithm is able to PAC learn â„‹â€²
ğ‘‘,ğ›¾using ğ‘œ(ğ‘‘) examples.
Proof. Suppose that an adversary chooses â„ğ´âˆˆâ„‹â€²
ğ‘‘,ğ›¾uniformly at random and consider the distribu-
tion on ğ’³ğ‘‘which is uniform on the complement of ğ´, where |ğ´| = ğ‘‚(ğ‘‘). Note that the error of every
hypothesis â„ğµâˆˆâ„‹â€²
ğ‘‘,ğ›¾is at least ğ›¾|ğµâˆ–ğ´|/ğ‘‘. Therefore, to return a hypothesis with small error, the
algorithm must recover a set that is almost disjoint from ğ´and so recover ğ´. However the size of ğ´
implies that it cannot be done with ğ‘œ(ğ‘‘) examples.
Formally, fix ğ‘¥0 âˆˆğ’³ğ‘‘and ğœ€âˆˆ(0, 1). Let ğ´âŠ†ğ’³ğ‘‘âˆ–{ğ‘¥0} of size ğ‘‘/2. Let ğ’Ÿğ´be a distribution with
mass ğ’Ÿğ´((ğ‘¥0, â„ğ´(ğ‘¥0))) = 1 âˆ’16ğœ€and is uniform on the points {(ğ‘¥, â„ğ´(ğ‘¥)) : ğ‘¥âˆˆğ´ğ‘}, where ğ´ğ‘
is the complement of ğ´(without ğ‘¥0).
Consider a proper learning algorithm ğ’œ. We will show that there is some algorithm-dependent set ğ´,
so that when ğ’œis run on ğ’Ÿğ´with ğ‘š= ğ‘‚(ğ‘‘/ğœ€), it outputs a hypothesis with error at least ğ›¾with
constant probability.
Pick ğ´uniformly at random from all sets of size ğ‘‘/2 of ğ’³ğ‘‘âˆ–{ğ‘¥0}. Let ğ‘be the random variable
that counts the number of samples in the ğ‘šdraws from ğ’Ÿğ´that are not (ğ‘¥0, â„ğ´(ğ‘¥0)). Standard
concentration bounds imply that with probability at least 1/2, the number of points from (ğ’³ğ‘‘âˆ–
{ğ‘¥0}) âˆ–ğ´is at most ğ‘‘/4. Conditioning on this event, ğ´is a uniformly chosen random set of size ğ‘‘/2
that is chosen uniformly from all subsets of a set ğ’³â€² âŠ‚ğ’³ğ‘‘with |ğ’³â€²| â‰¥3ğ‘‘/4 (these points are not
present in the sample). Now assume that the learner returns a hypothesis â„ğµ, where ğµis a subset of
size ğ‘‘/2. Note that E[|ğµâˆ–ğ´|] â‰¥ğ‘‘/6. Hence there exists a set ğ´such that with probability 1/2, it
holds that |ğµâˆ–ğ´| â‰¥ğ‘‘/6. This means that ğ’œincurs a loss of at least ğ›¾on all points in ğµâˆ–ğ´and the
mass of each such point is â„¦(ğœ€/ğ‘‘). Hence, in total, the learner will incur a loss of order ğ›¾Â· ğœ€.
H
Extension to More General Loss Functions
Our results can be extended to loss functions that satisfy approximate pseudo-metric axioms (see
e.g., [HKLM22, CKW08]). The main difference from metric losses is that we allow an approximate
triangle inequality instead of a strict inequality. Many natural loss functions are captured by this
definition, such as the well-studied â„“ğ‘losses for the regression setting. Abstractly, in this context, the
31

label space7 is an abstract non-empty set ğ’´, equipped with a general loss function â„“: ğ’´2 â†’Râ‰¥0
satisfying the following property.
Definition 18 (Approximate Pseudo-Metric). For ğ‘â‰¥1, a loss function â„“: ğ’´2 â†’Râ‰¥0 is ğ‘-
approximate pseudo-metric if (i) â„“(ğ‘¥, ğ‘¥) = 0 for any ğ‘¥âˆˆğ’´, (ii) â„“(ğ‘¥, ğ‘¦) = â„“(ğ‘¦, ğ‘¥) for any ğ‘¥, ğ‘¦âˆˆğ’´,
and, (iii) â„“satisfies a ğ‘-approximate triangle inequality; for any ğ‘¦1, ğ‘¦2, ğ‘¦3 âˆˆğ’´, it holds that
â„“(ğ‘¦1, ğ‘¦2) â‰¤ğ‘(â„“(ğ‘¦1, ğ‘¦3) + â„“(ğ‘¦2, ğ‘¦3)).
Furthermore, note that all dimensions for â„‹, DG
ğ›¾(â„‹), DOIG
ğ›¾
(â„‹), DDS
ğ›¾(â„‹), and Donl
ğ›¾(â„‹) are defined
for loss functions satisfying Definition 18.
Next, we provide extensions of our main results for approximate pseudo-metric losses and provide
proof sketches for the extensions.
ERM Learnability for Approximate Pseudo-Metrics.
For ERM learnability and losses satisfying
Definition 18, we can obtain the next result.
Theorem 5. Let â„“be a loss function satisfying Definition 18. Then for every class â„‹âŠ†ğ’´ğ’³, â„‹is
learnable by any ERM in the realizable PAC regression setting under â„“if and only if DG
ğ›¾(â„‹) < âˆ
for all ğ›¾âˆˆ(0, 1).
The proof of the upper bound and the lower bound follow in the exact same way as with the absolute
loss.
PAC Learnability for Approximate Pseudo-Metrics.
As for PAC learning with approximate
pseudo-metric losses, we can derive the next statement.
Theorem 6. Let â„“be a loss function satisfying Definition 18. Then every class â„‹âŠ†ğ’´ğ’³is PAC
learnable in the realizable PAC regression setting under â„“if and only if DOIG
ğ›¾
(â„‹) < âˆfor any
ğ›¾âˆˆ(0, 1).
Proof Sketch. We can generalize the upper bound in Theorem 2 for the scaled OIG dimension as
follows. One of the ingredients of the proof for the absolute loss is to construct a sample compression
scheme through the median boosting algorithm (cf. Algorithm 2). While the multiplicative update
rule is defined for any loss function, the median aggregation is no longer the right aggregation
for arbitrary (approximate) pseudo-metrics. However, for each such loss function, there exists an
aggregation such that the output value of the ensemble is within some cutoff value from the true label
for each example in the training set, which means that we have a sample compression scheme for
some cutoff loss. In particular, we show that by using weak learners with cutoff parameter ğ›¾/(2ğ‘),
where ğ‘is the approximation level of the triangle inequality, the aggregation of the base learners can
be expressed as a sample compression scheme for cutoff loss with parameter ğ›¾.
Indeed, running the boosting algorithm with (ğ›¾/(2ğ‘), 1/6)-weak learners yields a set â„1, . . . , â„ğ‘
of weak predictors, with the property that for each training example (ğ‘¥, ğ‘¦), at least 2/3 of the
functions â„ğ‘–(as weighted by coefficients ğ›¼ğ‘–), 1 â‰¤ğ‘–â‰¤ğ‘, satisfy â„“(â„ğ‘–(ğ‘¥), ğ‘¦) â‰¤ğ›¾/(2ğ‘). For any
ğ‘¥, let Ë†â„(ğ‘¥) be a value in ğ’´such that at least 2/3 of â„ğ‘–(as weighted by ğ›¼ğ‘–), 1 â‰¤ğ‘–â‰¤ğ‘, satisfy
â„“(â„ğ‘–(ğ‘¥), Ë†â„(ğ‘¥)) â‰¤ğ›¾/(2ğ‘), if such a value exists, and otherwise Ë†â„(ğ‘¥) is an arbitrary value in ğ’´. In
particular, note that on the training examples (ğ‘¥, ğ‘¦), the label ğ‘¦satisfies this property, and hence Ë†â„(ğ‘¥)
is defined by the first case. Thus, for any training example, there exists â„ğ‘–(indeed, at least 2/3 of
them) such that both â„“(â„ğ‘–(ğ‘¥), ğ‘¦) â‰¤ğ›¾/(2ğ‘) and â„“(â„ğ‘–(ğ‘¥), Ë†â„(ğ‘¥)) â‰¤ğ›¾/(2ğ‘) are satisfied, and therefore
we have
â„“(Ë†â„(ğ‘¥), ğ‘¦) â‰¤ğ‘(â„“(Ë†â„(ğ‘¥), â„ğ‘–(ğ‘¥)) + â„“(â„ğ‘–(ğ‘¥), ğ‘¦)) â‰¤ğ›¾.
This function Ë†â„can be expressed as a sample compression scheme of size ğ‘‚
(ï¸
DOIG
ğ›¾/(2ğ‘)(â„‹) log(ğ‘š)
)ï¸
for cutoff loss with parameter ğ›¾: namely, it is purely defined by the â„ğ‘–functions, where each â„ğ‘–is
7We would like to mention that, in general, we do not require that the label space is bounded. In contrast, we
have to assume that the loss function takes values in a bounded space. This is actually necessary since having
an unbounded loss in the regression task would potentially make the learning task impossible. For instance,
having some fixed accuracy goal, one could construct a learning instance (distribution over labeled examples)
that would make estimation with that level of accuracy trivially impossible.
32

specified by ğ‘‚
(ï¸
DOIG
ğ›¾/(2ğ‘)(â„‹)
)ï¸
training examples, and we have ğ‘= ğ‘‚(log(ğ‘š)) such functions, and Ë†â„
satisfies â„“(Ë†â„(ğ‘¥), ğ‘¦) â‰¤ğ›¾for all ğ‘štraining examples (ğ‘¥, ğ‘¦). Thus, by standard generalization bounds
for sample compression, we get an upper bound that scales with Ëœğ‘‚
(ï¸
DOIG
ğ›¾/(2ğ‘)(â„‹) 1
ğ‘š
)ï¸
for the cutoff
loss with parameter ğ›¾, and hence by Markovâ€™s inequality, an upper bound
E[â„“(Ë†â„(ğ‘¥), ğ‘¦)] = Ëœğ‘‚
(ï¸‚
DOIG
ğ›¾/(2ğ‘)(â„‹) 1
ğ‘šğ›¾
)ï¸‚
.
We next deal with the lower bound. For the absolute loss, we scale the dimension by 2ğ›¾instead of ğ›¾
since for any two possible labels ğ‘¦1, ğ‘¦2 the learner can predict some intermediate point, and we want
to make sure that the prediction will be either ğ›¾far from ğ‘¦1 or ğ‘¦2. For an approximate pseudo-metric,
we should take instead 2ğ‘ğ›¾in order to ensure that the prediction is ğ›¾far, which means that the lower
bounds in Theorem 2 hold with a scale of 2ğ‘ğ›¾.
Online Learnability for Approximate Pseudo-Metrics.
Finally, we present the more general
statement for online learning.
Theorem 7. Let â„“be a loss function satisfying Definition 18 with parameter ğ‘â‰¥1. Let â„‹âŠ†ğ’´ğ’³and
ğœ€> 0. Then, there exists a deterministic algorithm whose cumulative loss in the realizable setting is
bounded by Donl(â„‹) + ğœ€. Conversely, for any ğœ€> 0, every deterministic algorithm in the realizable
setting incurs loss at least Donl(â„‹)/(2ğ‘) âˆ’ğœ€.
Proof Sketch. The upper bound of Theorem 4 works for any loss function. Recall the proof idea;
in every round ğ‘¡there is some Ì‚ï¸€ğ‘¦ğ‘¡âˆˆğ’´the learner can predict such that no matter what the
adversary picks as the true label ğ‘¦â‹†
ğ‘¡, the online dimension of the version space at round ğ‘¡, i.e,
ğ‘‰= {â„âˆˆâ„‹: â„(ğ‘¥ğœ) = ğ‘¦â‹†
ğœ, 1 â‰¤ğœâ‰¤ğ‘¡}, decreases by â„“(ğ‘¦â‹†
ğ‘¡, Ì‚ï¸€ğ‘¦ğ‘¡), minus some shrinking number ğœ–ğ‘¡
that we can choose as a parameter. Therefore we get that the sum of losses is bounded by the online
dimension and the sum of ğœ–ğ‘¡that we can choose to be arbitrarily small.
The lower bound for online learning in Theorem 4 would be Donl(â„‹)/(2ğ‘) âˆ’ğœ€, for any ğœ–> 0, since
the adversary can force a loss of ğ›¾ğ‘¦â‰¤ğ‘¡/(2ğ‘) in every round ğ‘¡, where ğ›¾ğ‘¦â‰¤ğ‘¡is the sum of the gaps
across the path ğ‘¦.
33

