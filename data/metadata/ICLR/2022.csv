authors,publisher,title,url,year,abstract,session,pdf_url,openreview_url,id,forum_content
"['Rose Wang', 'Esin Durmus', 'Noah Goodman', 'Tatsunori Hashimoto']",ICLR,Language modeling via stochastic processes,https://iclr.cc/virtual/2022/oral/5951,2022," Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines.",Oral 1: AI Applications,https://openreview.net/pdf?id=pMQwKL1yctf,https://openreview.net/forum?id=pMQwKL1yctf,pMQwKL1yctf,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'All reviewers found that the proposed LM with Brownian motion is interesting and novel. Several reviewers raised (minor) concerns about experiments, but have been generally resolved by the authors.'}, {'title': ""Thank you! We've updated the paper to include additional discussion on the method, visualization of latent trajectories, and fixes suggested by the reviewer. "", 'comment': 'We thank the reviewer for their feedback and questions! We’ve revised our paper to include additional discussion on the choice of latent dimension, visualizations of latent trajectories over coherent and incoherent text, and formatting/error edits raised by the reviewer. \n\n> The information about each of the ablations (ID, BM) could be explained better.\n\nThanks for the suggestion! We’ve revised the paper to reflect the reviewer’s recommendations: In Appendix E, we elaborate in more depth with how the ablations are implemented. \n\n> Inconsistency in the best TC method between different latent dimensions [...] I wonder if you have thoughts about this.\n\nThanks for raising this point! On tasks where the temporal structure isn’t clear (eg. ‘discourse structure’ on Wikisection, Table 1), the latent has a risk of picking spurious pieces of information, and this leads to uninformative latents. We think it’s an interesting open problem on how we might make the choice of latent dimension more robust! \n\n> Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best. Is there a reason behind this?\n\nThanks for catching this! That was an error on our side. We’ve updated the table to correct this mistake. \n\n> During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan?\n\nAlthough there is no explicit guarantee that the decoder uses the information from the latent plan during fine tuning, we do see promising empirical results which suggest the latent plan is useful in training. For example, we see in Table 9 that Time Control uses latent information in Table 9 where PPL is lower with latent plans than just with fine tuning GPT-2 without latent plans. \n\n> Overall the paper would have benefited from an intrinsic visualization of the latent space [...] by visualizing the planning trajectory difference between coherent and incoherent text.\n\nThanks for this suggestion! We’ve added a few latent trajectory examples over coherent and incoherent texts in Appendix J. For incoherent texts, we randomly mixed the sentence ordering of the original document. As the visualizations suggest, there is no information collapse of the embeddings. For completeness, we’ve also included trajectories of the ablated methods. '}, {'title': 'Continuation of response ', 'comment': '>  the BLEURT results do not appear to be anywhere in the paper ... The reported BLEU results are really low ... [2] report ROUGE results on ROCStories, which are much better. The missing BLEURT results would help contextualise model performance here. \n\nThanks for the suggestion and the reference! We agree with the reviewer’s intuitions with the low BLEU scores: the scores are low due to the open-endedness of the domain. We reran our text infilling experiments for all the methods and tracked BLEURT and ROUGE, as the reviewer suggested. The results are reported in Table 17. \n\nAs a clarification, our original work did not report BLEURT scores rather BertScore; those results were put in the Appendix (Table 17) due to space constraints. \n\nWe found that Time Control performs slightly better on BLEURT than the alternatives including ILM, a method specific for text infilling. \nIt performs slightly worse than ILM and LM as ranked by ROUGE (1, 2, L) scores, but does slightly better on the BLEU scores (Table 2). The ROUGE and BLEU results indicate a precision-recall tradeoff where TC generates infill-sentences with more overlap to the ROCStories reference, and the reference words appear more frequently in ILM/LM generation. \n\nNonetheless, the end arbiter of this task is a human (how coherent do the generations sound to a human?) and we care about at least matching ILM, a method developed specifically for text-infilling. So, it’s promising that our method performs better and/or competitively with ILM on human-based metrics (BLEURT and Human evaluations in Table 6).\n\n> “The human evaluation shows the model performs about as well as the ILM baseline from [5], which is ok I guess?”\n\nYes, our method is not specific to the task of text-infilling so it’s promising that it performs close to ILM, a method designed for the text infilling task! \n\n> “Can you group the tables a bit better, in thematic order?”\n\nThanks for letting us know. We’ve re-organized the tables and the new results suggested by reviewers in our revisions -- please let us know if the updated version improves readability! \n\n> “If the authors simply mean whether the model has learnt a notion of document structure, I think it would be better to be more explicit about this”\n\nThanks for pointing this out! We’ve updated the paper to be more explicit about the success and failure modes we ran into over the course of this work in Appendix M. \n\nRegarding the concern of ‘modelling document structure,’ we found that fine-tuned GPT2 was able to replicate certain aspects of a document corpus, such as section header ordering (~92% accurate). However, when going from document-level to section-level statistics, we noticed that GPT-2 seemed to be either undershooting or overshooting the section lengths; these are the results that were reported in the original submission. \n\nWe believe this is an interesting direction for future research to explore: Is the section length mismatch caused by a mismatch between the myopic next-token prediction LM objective and retaining higher-level statistics like section lengths? We’ll be more precise in our revisions about what we mean by global text dynamics. \n\n> “Do the authors just keep on conditioning the decoder on z_T, and force the model to generate from this? [...] Alternatively, do the authors resample z_{t+1} each time the model finishes generating a sentence?”\n\nThanks for raising this to our attention! We agree the paper should have been more clear on how we obtain latent plans in the forced long text generation setting. \nThe latent planning process is the following. \n\nLet’s have $S_{\\text{avg}(\\mathcal{D})}$ denote the average number of sentences in a document and $T_{\\text{avg}(\\mathcal{D})}$ denote the average number of tokens in a document.\n\nRather than planning a trajectory of length $S_{\\text{avg}(\\mathcal{D})}$ (the average number of sentences in a document) which is what is done in Section 4.3 for normal text generation, we scale the trajectory length to $c \\cdot S_{\\text{avg}(\\mathcal{D})}$. $c$ is determined by how many more tokens we need in order to fill up to GPT-2 maximum context length of 1024: $c = \\frac{1024 - T_{\\text{avg}(\\mathcal{D})}}{T_{\\text{avg}(\\mathcal{D})}}$. \n\nSo, to directly answer the reviewer’s question: we do not condition the decoder on z_T and do not resample new latents during generation. \n\nWe’ve updated the paper to include these explanations. \n\n> “A better experiment to run would be to simply ask the human annotators to score texts freely generated from GPT2 and TC for coherence, as a measure of how well TC can generate coherent text.”\n\nWe in fact do already ask human annotators to score the generation (rf. Table 7). In this setup, we remove the middle section of the generated output as the text is extremely long. See Figures 3-6 for examples of the full forced long text generation results.\n\n\n'}, {'title': ""Thank you! We've revised the paper to include additional experiments (ALBERT, ROUGE, BLEURT), discussions on BB dynamics, and clarification on decoding."", 'comment': ""We thank the reviewer for their feedback and questions! We’ve revised the paper to include results from additional experiments suggested by the reviewer (ALBERT baseline and ROUGE/BLEURT metrics), discussions on guaranteeing Brownian bridge latent dynamics, elaboration on the global text dynamics setting, and decoding setup in the forced long text generation setting.\n\n> There are some prior works on using structured probabilistic models ... which should also be cited.\n\nThank you for the suggested citations! We were not aware of these works. We’ve edited the related works section to reflect these updates.\n\n> It wasn't immediately clear that training the model on triples only is enough to guarantee general Brownian bridge dynamics for the entire text trajectory, I feel a note should be added to clarify this.\n\nWe agree with the reviewer that training on triplet samples may not guarantee Brownian bridge dynamics for the entire text trajectory. \nOn the empirical side, we found it easier to recover Brownian bridge dynamics via contrast triplets than the ELBO objective or with pairwise contrasts (rf. Appendix J and M.2). We’ve included a note on this in the paper revision. \n\n> My other quibble here is with how the model is presented … which leads to confusing things like the variance of the process \\sigma^2 being used in Equation 3 without prior introduction. \n\nWe thank the reviewer for their suggestions on presenting the model. We’ve updated the text to clarify that the variance of the process is the variance presented in Equation 1: we scale the distance in Equation 3 by the relative sentence location. Hopefully this clarifies the inference process for z_t. \n\n> I think the baselines should at least include an ALBERT model to show the performance upper bound on this problem. \n\nThank you for this suggestion! We reran the discourse coherence experiments for k=1, 5, 10 with ALBERT on the three domains presented in Table 1 (Wikisection, TM-2, TicketTalk).\n\nAs the reviewer hypothesized, we found that on k =1, ALBERT indeed performed above random chance on the conversation-based domains (55-60%) however performed randomly across all k-values on Wikisection. We believe this is due to the lack of discourse signal in fact-oriented domains like Wikipedia articles. We’ve included the k=1 results in Appendix L and updated Table 1 to include the k=5 and 10 results. \n\n> [I]t's questionable whether the model is really modelling 'local' dynamics at [k=5 or 10 in dialog settings].\n\nWe agree with the reviewer that k=5 and 10 results in a much simpler discourse prediction setting for dialog domains. Local dynamics defined with k=5 or 10 varies in discourse implications across domains. \n\nEven so, we found that in the k=1 prediction setting, most (if not all) of the baselines failed to achieve better than random chance. In investigating why this is the case for the dialog domains, we noticed that there were several sentences which contained simple, ambiguous responses, such as “OK” where sentence ordering is difficult to infer from. We hypothesize this is the reason why doing better than random is difficult on k=1. ALBERT is the only baseline that achieves slightly better than random performance, but it’s not significantly better (~55-60% test accuracy). \n\nNonetheless, we think these observations fit well with the intuition our work proposes: Neighboring sentences are close to each other and act like Brownian motion where ordering is difficult to infer, and goal-orientedness / discourse structure emerges on longer stretches of sentences in a document. \n\n""}, {'title': 'Continuation of response', 'comment': ""> Table 5 shows mixed results. More discussion and analysis here would be helpful.\n\nThanks for raising this concern! We assume by “mixed results” the reviewer is referring to the VAE ablation performing well on the Recipe domain, but less well on other domains.\n\nWe don’t have full insight as to why the VAE does better on the Recipe domain, but we looked into comparing the latent structure recovered by the VAE baseline and Time Control. What we found is the Time Control is best at extracting time over the course of the document (ie. its latents recover a Bridge process correlated with time), whereas the VAE doesn’t elicit strong temporal structure; see the newly added Appendix K. We hypothesize that temporal structure is not all you need to succeed in the Recipe domain.\n\n> “[p]lease make explicit whether the triplets have a notion of distance or not i.e. it is sensitive to different value of t depending on which sentence in the middle was sampled.”\n\nWe train the encoder to embed sentences such that the distances are scaled w.r.t the ground truth time value with the variance of the Brownian Bridge process, $\\frac{t(T-t)}{T}$ (rf. Equation 3). At inference time, the sentence index is not provided: A sentence is directly passed through the encoder without any other supervision. The triplet of sentences are sampled uniformly at random $(x_{t_a}, x_{t_b}, x_{t_c})$, then re-ordered such that $t_a < t_b < t_c$; this means that the variance does change across triplet samples. \n\nIn earlier iterations of the work, we did explore learning with pairwise contrasts with fixed t-distances, e.g. distances of length 1, 5, and 10 sentences. We observed that the resulting latent trajectories elicited different fits to Brownian bridge dynamics, and the quality in fit varied in t across domains; we’ve included some examples in Appendix M of the revised paper in case this is of future interest. \n\nThese observations seem to complement the discourse coherence results and how the difficulty of inferring discourse structure varies on the distance between sentences and the domain (eg. larger distances are easier in dialog settings). \n\nWe've updated the text to make this more clear. \n\n> “Are you summing over all the negative x_{t'}?”\n\nYes, we are summing over all negative $x_{t'}$. We’ve updated the text to make this more clear.\n\n""}, {'title': ""Thank you! We've updated the paper to include more details on ablation, goodness of latent model fit, and model sensitivity to sentence distances."", 'comment': 'We thank the reviewer for their feedback and questions! We’ve revised our paper to improve clarity on the ablation implementation, the goodness of fit with BB vs BM latent models, and the method’s sensitivity to the sentence distance sampled for contrasts. \n\n> "" the setting is fairly limited because this approach requires two contextual endpoints, the start and finish...For example, there are limited experiments with regard to controllable generation, or goal-oriented generation tasks.""\n\nThanks for raising this point! In earlier iterations of this work, we did test for other aspects of controllable text generation such as varying document lengths using shorter or longer latent plans. We did indeed find that shorter latent plans resulted in shorter text, and longer latent plans resulted in longer text (without suppressing the EOD token), although there wasn’t a 1-to-1 correspondence between the change in plan length and the change in text length across domains. For example, on the Wikisection, we reduced the latent plan length by 5 and 10 times fewer sentences and observed the average document length decreases from 691 tokens to 543 tokens to 542 tokens long (about 20% reduction). When we increase the latent plan length by 2 to 5 times, the average document length increases from 691 tokens to 814 tokens (~ 18% increase) to 842 tokens (~22% increase).\n\nWe do think an interesting extension of this work is to do multi-point/goal conditioning: In principle, Time Control can already do this because intermediate trajectories are also Brownian bridges. In this work, we focus on the two-endpoint case.\n\n> “The assumption that autoregressive generation follows a Brownian motion is strong and I would like to see some empirical evidence or theoretical argument supporting this.”\n\nThis is an interesting point -- in our original submission, we provide some empirical evidence in Table 9 which reports test PPL after fine tuning GPT2. We find that fine tuning GPT2 on Brownian bridge latent models results in slightly lower PPL on held-out examples than with Brownian motion latent models. This seems to suggest that using Brownian bridge latent models fit better than Brownian motion ones. \n\n> “A more careful instantiation of prior for VAE, or even learning a time sensitive prior would be a better implementation of the VAE baseline.”\n\nThank you for pointing this out! We agree with the reviewer that the paper should include more details on the VAE implementation and that the VAE prior matters in the bridge process model for our application. We will clarify in the text that the prior for $z_0$ is 0-centered and for $z_T$ 1-centered, just like in our contrastive learning setting. This would lead to the same time-sensitive prior for both settings. \n\nThe paper has been updated to address these concerns regarding clarity on the ablation implementation. \n\n> “[Regarding Table 1, t]he proposed approach is naturally suited for this metric/classifier because the encodings at different times are more or less linearly related with some stochasticity. However, this is not true for the other baselines, so I am not sure what is the takeaway message from this experiment.”\n\nThe takeaway message from this experiment is to compare against non-temporal baselines: How well of a model fit is the Brownian bridge latent model which assumes temporal dynamics to models that don’t assume temporal dynamics like BERT or SimCSE? Note that our latent model assumptions don’t necessarily have to be true: For example, on the recipe domain where all of the latent models do not perform better than random, these results seem to suggest that neither model assumption is a good way to model text dynamics. \n\n> “Also, more exposition on the Brownian motion baseline would be helpful. [...] On a related point, I don\'t get why BM for Table 2 would be the same as the brownian bridge.”\nThanks for raising these concerns. We will update the text to include more exposition to the Brownian motion baseline. \n\nThe text infilling setting uses the average of the prefix and suffix sentence embeddings ($\\frac{z_{\\text{prefix}} + z_{\\text{suffix}}}{2}$) for decoding the infill-sentence; one reason we do this is to compare the quality of interpolated latents which is of particular relevance for testing decoding quality between TC and ID (explicit vs implicit dynamics assumption). \n\nIn the Brownian Motion case, once we condition the intermediate latent with a target endpoint $ z_{\\text{suffix}}$, it becomes a Brownian Bridge by definition. On the other hand, throwing away the endpoint (aka the suffix) in the Brownian motion case felt like an unfair baseline. \n'}, {'title': ""Thank you! We've updated our paper to reflect the reviewer's recommendations."", 'comment': 'We thank the reviewer for their feedback and questions! We’ve revised our paper to include formatting changes recommended by the reviewer.  \n\n> Using Brownian bridge is a very simple and effective idea for text generation. My only concern is the range of its applicability: while it is far more natural than a simple random walk, Time Control only allows designating the first and last states for generation. However, in the actual situation, it is not always the case for the first (and sometimes, last) sentence should have a designated state. \n\nThanks for your feedback and kind words! We concur with the reviewer that an interesting future direction is ‘multi goal-conditioned generation,’ and we intend to investigate as an extension of our work.\n'}, {'summary_of_the_paper': 'This paper introduces a method to enhance the global coherence of text generated from Language models. The proposed method (Time Control). Under the assumption in the latent space of sentence embeddings, the incoherent text can be seen as ""Brownian motion"" in the latent space. In order to enforce a goal to the generated text authors by fixing a start and end to this Brownian motion the process of text generation can be modeled as a Brownian bridge. \n\nFrom this assumption, the authors drive a method that consists of three steps (1) training an encoder to map sentences to a latent plan defined as Brownian bridge (2) training a decoder to reconstruct sentences from the given context + the true encoded vector of the target sentence from planning latent space using the trained encoder (3) at inference time: given a start and endpoint, a target trajectory of vectors $z_0, ..., z_t, ..., z_T$ is sampled and use the decoder to generate a sentence based on this bridge. \n\nAuthors run several experiments to (1) evaluate the hypothesis that the encoder can capture local text dynamics using sentence order prediction task (2) evaluate the decoder to generate local incoherent text using the text-infilling task. (3) capture global text statistics by measuring the statistics (length of Wikipedia sections for city articles) of the generated text and compare them to the ground truth. (4) Evaluate the overall coherence of the long-generated text. Overall the results look convincing except for some caveats (see the areas of enhancement) \n', 'main_review': '**Pros**\n\n- The paper is well structured and easy to follow, the idea of modeling sentences to a Brownian bridge latent space is neat and generic enough to (1) allow for noise given its stochasticity (2) doesn\'t require explicit domain knowledge for planning. \n\n- Well Structured Experiments sections with 4 RQs and results that confirm each of the hypotheses\n\n- Reproducibility and Transparency in reporting of experiments in terms of available source code, dataset information, details about human evaluation, generation examples. \n\n**Areas of Enhancement & Questions to authors**\n\n- The information about each of the ablations (ID, BM) could be explained better. namely the section ""ablations"".  \n\n- There\'s a clear Inconsistency in the best TC method between different latent dimensions (8,6,32), in most of the experiments there\'s at least one of the 3 that is performing drastically worse than the other baselines, while there\'s overall no clear winner. I wonder if you have thoughts about this. \n\n- Table 5 the VAE(32) method performs the best overall in ""Wiki section"" although the TC (16) method has been highlighted as the best. Is there a reason behind this?\n\n- During the training of the decoder how do you make sure that the decoder uses the information given by the latent plan?\n\n- Overall the paper would have benefited from an intrinsic visualization of the latent space, to make sure for example that there\'s no  Information collapse of the embeddings when dealing with long sentences. This could be done by visualizing the planning trajectory difference between coherent and incoherent text.\n\n\n', 'summary_of_the_review': 'The paper introduces a simple method of preserving coherence in language modeling it builds on previous work that tried to implicitly model planning dynamics. The introduced solution is effective and general enough to not need domain-specific planning information. It is a good paper to accept overall. I advise the authors to clarify the information about the used baselines in a more clear manner. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper proposes a generation from a language model not only from an initial\nstate, but also using a goal state. Instead of Brownian motion, the authors\nemploy a draw from Brownian bridge by designating initial and end states,\ncalled Time Control.\nExperimental results show the proposed generation from Brownian bridge is more\nnatural and coherent for text-infilling task, and also preserves text \nstructures both by automatic evaluation and human evaluation.', 'main_review': 'This paper proposes a generation from a language model not only from an initial\nstate, but also using a goal state. Instead of Brownian motion, the authors\nemploy a draw from Brownian bridge by designating initial and end states,\ncalled Time Control.\nExperimental results show the proposed generation from Brownian bridge is more\nnatural and coherent for text-infilling task, and also preserves text \nstructures both by automatic evaluation and human evaluation.\n\nUsing Brownian bridge is a very simple and effective idea for text generation.\nMy only concern is the range of its applicability: while it is far more natural\nthan a simple random walk, Time Control only allows designating the first and\nlast states for generation. However, in the actual situation, it is not always\nthe case for the first (and sometimes, last) sentence should have a designated\nstate. First few sentences might constitute just an ice-break, and the actual \ncontent might start after that. \nMore generally, it is more desirable that we can condition the generation at\narbitrary time. In fact, I think that this can be done by a conditional draw\nfrom a Gaussian process. Since Brownian motion corresponds to using an \nexponential kernel of GP, sentence generation from conditional GP would be \nthe way for the future extention of this work.\nAnyway, this work will surely pave the way for such principled generations.\n\nMinor\n\n- Some tables are located within the main text. Tables and Figures should be\nplaced top or bottom of the paper for readability: please use \\begin{figure}[t]\nfor something like that.\n- Numerical results in Tables can be rendered in a smaller font (i.e. \\small).\nAlso I recommend to condense line spacing for Tables for readability, using \n\\usepackage{setspace} and begin{spacing}{0.9} ... end{spacing}, for example.\n', 'summary_of_the_review': 'Nice attempt for random generation from neural language models using the idea of Brownian bridge. This work will pave the way for more princpled random generation from language models.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper proposes to model the evolution of sentences in a document via a stochastic process; specifically a Brownian Bridge process. The paper start off by assuming that the generated sequences by autoregressive models like GPT-2 follow Brownian motion in that they tend to get incoherent and ""meander"" in the semantic space. This paper aims to reduce this random behavior by pinning the endpoints of the trajectory and model the generation by Brownian bridge process instead. The key intuition behind this process is that given two endpoints z0 and zT, the evolution of z along time t is a Gaussian with mean that is some linear combination of z0 and zT. This paper models text by training an encoder for sentences x that produces the embedding z by training over triplets (x0, xt, xT) where 0<t<T that encourages zt to follow Brownian bridge dynamics and uses contrastive loss with a negatively sampled x\'t for training.\n\nThe approach is tested for local coherence, long range order sensitivity, and generation of long sequences and is compared against ablative and external baselines. The proposed approach does lead to learning of embeddings that are obtainable via linear combination and this leads to improved performance on sensitivity to sentence order in documents and document generation. ', 'main_review': ""This paper has an interesting approach and tackles an important problem of streamlining sequence generation from autoregressive models. \n\nThe experiments show the value of learning a manifold over the latents that have a linear relationship with some stochastic perturbation. They provide evidence that learning in such a manner is promising in order to maintain coherence over long text generation.\n\nHowever, the setting is fairly limited because this approach requires two contextual endpoints, the start and finish. This is especially underwhelming given that the introduction states that this approach aims to perform \\emph{controllable goal-oriented} generation. In my view, the setting described and experimented with doesn't reflect this goal. For example, there are limited experiments with regard to controllable generation, or goal-oriented generation tasks.\n\nSecondly, the assumption that autoregressive generation follows a Brownian motion is strong and I would like to see some empirical evidence or theoretical argument supporting this. One simple experiment could be to actually try to fit a Brownian motion model to a bunch of sequences generated from GPT-2, and show that this fitted model is not suitable for naturally occurring text. \n\nExperiment wise, my biggest concern is the VAE baseline. The point of this baseline is to show that for the same setup of Brownian bridge process, contrastive learning is better than the VAE objective, but I feel that the VAE implementation as described in the appendix does not make the comparison fair. Due to lack of details in the paper, I am assuming that the priors p(z0) and p(zT) are standard gaussians. If this is not true, then a clarification would ease this concern of mine. But assuming this is true, the loss basically tries to match the encoder distributions q(z0) and q(zT) obtained by f_{\\theta}(x0) and f_{\\theta}(xT) to the standard Gaussian. What this means is that there is a pressure to make the 0 and T embeddings similar which is not at all what we want from this bridge process model. A more careful instantiation of prior for VAE, or even learning a time sensitive prior would be a better implementation of the VAE baseline.\n\nTable 1 is another concern. This experiment basically trains a linear classifier over the encodings to identify if they are in-or-out of order. The proposed approach is naturally suited for this metric/classifier because the encodings at different times are more or less linearly related with some stochasticity. However, this is not true for the other baselines, so I am not sure what is the takeaway message from this experiment.\n\nAlso, more exposition on the Brownian motion baseline would be helpful. The current description is not enough to get an idea about what exactly was done for generation and other experiments with this baseline. On a related point, I don't get why BM for Table 2 would be the same as the brownian bridge. Isn't it the case that Brownian motion baseline doesn't get to see \\emph{both} the endpoints? If I am mistaken about this, then more exposition is required here because I checked both the paper and the appendix carefully for this.\n\nTable 5 shows mixed results. More discussion and analysis here would be helpful.\n\nFor clarification: please make explicit whether the triplets have a notion of distance or not i.e. it is sensitive to different value of t depending on which sentence in the middle was sampled. From the context, I am assuming this is the case but clarification would be helpful. Also, notation in equation 2's denominator is confusing. Are you summing over all the negative x_{t'}? "", 'summary_of_the_review': 'Overall, I think this paper is well motivated and proposes a reasonable solution to improve coherence of model generated text. This is supported by ample experiments but I have serious concerns about some of the crucial experiments and baselines that I have detailed in my main review. Also, I think that the paper could be clearer about its contributions and implementation details.\n-----\nPost rebuttal: thanks to the authors for the detailed response addressing many of my concerns. My biggest concern about the prior in the VAE baseline is somewhat alleviated given that the the authors used different fixed priors for the two settings. While this could be improved by having learnable priors/better priors, I think the current setting makes the experiments reasonably sound. I have raised my score.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors propose to use a Brownian bridge process to model global coherence of a long piece of text. They show how to train such a model in an encoder-decoder style setup, using a contrastive loss to model the Brownian bridge dynamics. The authors then verify aspects of their model with a series of experiments to show that their model with an underlying generative process outperforms competing approaches on a variety of local and global coherence and generation tasks.', 'main_review': ""I really like the main modelling contribution of this paper. It is this reviewer's personal opinion that to do long-form text generation, it is not enough to generate token-by-token, but that some high-level planning is required, and the Brownian bridge process model (Time Control; TC) the authors propose is definitely a good candidate to model the latent drift of discourse (indeed, papers like [1] already used random walk-style models to explain properties of word vectors). There are some prior works on using structured probabilistic models, such as switching latent dynamical systems, for text generation [2], which should also be cited.\n\nThe motivation of the model present is clear, and the description of how the model is trained is generally clear enough to reimplement. It wasn't immediately clear that training the model on triples only is enough to guarantee general Brownian bridge dynamics for the entire text trajectory, I feel a note should be added to clarify this. My other quibble here is with how the model is presented: although the general probabilistic model is written down in Equation 1, the likelihood function (i.e. the functional form of p(z_t | x_0, x_t, x_T)) is not explicitly written down anywhere, which leads to confusing things like the variance of the process \\sigma^2 being used in Equation 3 without prior introduction. I feel like explicitly writing down the likelihood would make the equations in the paper flow much better. \n\nI feel the major weakness of this paper is with the experimental sections. For various reasons, I have objections to each of the experiments, which I will go through below:\n\n- The first experiment attempts to show that TC is a better model of local discourse coherence. The authors take two sentences from a document k steps apart, embeds them and them attempts to predict the sentence ordering from the embeddings. They say that for k=1 all models considered perform at chance level on all datasets, and only show results for k=5 and k=10. However, models trained using the k=1 objective (such as ALBERT [3] and StructBERT[4]) seem to be able to perform the task better than chance, so theoretically this should be possible. Therefore, I think the baselines should at least include an ALBERT model to show the performance upper bound on this problem. Further, k=5 (or even 10) starts meaning the sentences start becoming very far apart (10 dialogue turns is more than enough to complete some of the simple dialogue agent tasks!), so it's questionable whether the model is really modelling 'local' dynamics at this point.\n\n- The second experiment looks at text infilling on the ROCStories dataset, and use BLEU and BLEURT to automatically evaluate their models (although the BLEURT results do not appear to be anywhere in the paper). The reported BLEU results are really low, to the extent that it's unclear whether an improvement from 2 to 5 BLEU is really meaningful. Part of the issue is that BLEU measures precision, which penalises text generation where there are a variety of possible outputs; for this reason, [2] report ROUGE results on ROCStories, which are much better. The missing BLEURT results would help contextualise model performance here. The human evaluation shows the model performs about as well as the ILM baseline from [5], which is ok I guess? \n\n  In addition, the table ordering is incredibly confusing. Table 6, which shows the human evaluation for experiment 2, appears much later in the text, after tables for the later experiments. It took me a long time to find it. Can you group the tables a bit better, in thematic order?\n\n- The third experiment attempts to measure 'global text dynamics' by measuring length mismatch per section on Wikisections. It's unclear what notion of 'global text dynamics' the authors are referring to - there are many theories on discourse coherence of long text, and none of them easily map onto a simple measure of section length. If the authors simply mean whether the model has learnt a notion of document structure, I think it would be better to be more explicit about this: showing that fine-tuned GPT2 can't even replicate the structure of a homogenous document corpus is an interesting negative result. \n\n- The fourth experiment forces models to generate beyond the expected document length by suppressing generation of the EOD token. I'm really not a fan of this experiment, because I don't even expect TC to perform well on it. Do the authors just keep on conditioning the decoder on z_T, and force the model to generate from this? At this point, the model is just a standard autoregressive model, so the modelling contribution should have no effect. Alternatively, do the authors resample z_{t+1} each time the model finishes generating a sentence? In which case, how do the authors preserve the Brownian bridge dynamics, conditioning on hitting a target state z_T? There are a few methodological issues with this experiment. A better experiment to run would be to simply ask the human annotators to score texts freely generated from GPT2 and TC for coherence, as a measure of how well TC can generate coherent text.\n\nOverall, while the experimental section is weak, I really believe the core idea of directed Brownian dynamics for planning is a cool one, and deserves to be shared more widely. This is why I recommend acceptance.\n\nReferences: \n- [1]: RAND-WALK: A latent variable model approach to word embeddings, Sanjeev Arora et al. 2015\n- [2]: Generating Narrative Text in a Switching Dynamical System, Noah Weber et al. 2020\n- [3]: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Zhenzhong Lan et al. 2021\n- [4]: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding, Wei Wang et al. 2021\n- [5]: Enabling Language Models to Fill in the Blanks, Chris Donahue et al. 2020\n\n\n==================\n\nPost author response:\n\n> Nonetheless, we think these observations fit well with the intuition our work proposes: Neighboring sentences are close to each other and act like Brownian motion where ordering is difficult to infer, and goal-orientedness / discourse structure emerges on longer stretches of sentences in a document.\n\nI like this framing - currently it's implicit in the paper, but maybe it can be made more explicit that we expect the larger k results to be better, and that this verifies the Brownian bridge approach towards modelling text dynamics.\n\n> Nonetheless, the end arbiter of this task is a human (how coherent do the generations sound to a human?) and we care about at least matching ILM, a method developed specifically for text-infilling. So, it’s promising that our method performs better and/or competitively with ILM on human-based metrics (BLEURT and Human evaluations in Table 6).\n\nI think it should be made explicit then that ILM is in effect an upper bound for model performance, as it is a model trained specifically to do the task, and that matching the performance of ILM is actually a strong result for the TC model.\n\n> So, to directly answer the reviewer’s question: we do not condition the decoder on z_T and do not resample new latents during generation.\n\nThe model is thus primed to generate much longer text than it was typically exposed to? Thank you for the clarification.\n\n> We in fact do already ask human annotators to score the generation (rf. Table 7). In this setup, we remove the middle section of the generated output as the text is extremely long. See Figures 3-6 for examples of the full forced long text generation results.\n\nI believe the stronger (and more realistic) human evaluation is to not just evaluate the tail coherence on forced long text generation, but  instead directly sample from the model naturalistically and evaluate that output using human annotators. If TC better captures global coherence, this should be visible even in this setting.\n\nOverall, I would like to thank the authors for their response. Many of my concerns have been addressed, and I am happy to increase my score."", 'summary_of_the_review': 'Interesting modelling contribution to ensure global coherence of generated text. The proposed modelling approach could have wide applicability, which is why I recommend acceptance.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Language modeling via stochastic processes', 'authorids': ['~Rose_E_Wang1', '~Esin_Durmus1', '~Noah_Goodman1', '~Tatsunori_Hashimoto1'], 'authors': ['Rose E Wang', 'Esin Durmus', 'Noah Goodman', 'Tatsunori Hashimoto'], 'keywords': ['contrastive learning', 'language modelling', 'stochastic processes'], 'abstract': ""Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better).  Human evaluators also prefer TC's output 28.6% more than the baselines."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wang|language_modeling_via_stochastic_processes', 'pdf': '/pdf/ceeec650a60b1f87ad4dda26ecd02c9df0e3ed9d.pdf', 'one-sentence_summary': 'We introduce a language model that implicitly plans via a latent stochastic process.', 'supplementary_material': '/attachment/3c726fd45d501529d53d01a306dac4372f0175aa.zip', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/language-modeling-via-stochastic-processes/code)', '_bibtex': '@inproceedings{\nwang2022language,\ntitle={Language modeling via stochastic processes},\nauthor={Rose E Wang and Esin Durmus and Noah Goodman and Tatsunori Hashimoto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=pMQwKL1yctf}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Floris Geerts', 'Juan L. Reutter']",ICLR,Expressiveness and Approximation Properties of Graph Neural Networks,https://iclr.cc/virtual/2022/oral/6805,2022," Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=wIzUeM3TAU,https://openreview.net/forum?id=wIzUeM3TAU,wIzUeM3TAU,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper gives a new theoretical framework to characterize the expressive power of graph neural networks that describes GNN by tensor language (TL) and then makes it possible to analyze its expressive power through the lens of TL. The authors connect the expressive ability of TL to the color refinement algorithms and (vertex/graph) k-WL algorithms. By doing so, the several existing results can be recovered in a unifying manner. In addition to that, the function approximation ability is also investigated. \n\nThe paper gives a novel theoretical framework that gives a clear perspective to the problem of expressive power of GNN, which would be quite beneficial to the community and open up a new research direction. The reviewers have raised several questions on the paper, but the authors addressed all the concerns properly. Therefore, I recommend acceptance to ICLR2022.'}, {'title': 'Response to comment on revision', 'comment': 'We thank the reviewer for the very positive assessment of the original submission and the revision.'}, {'title': 'thank you for your answer', 'comment': 'I raised my score to 10.'}, {'title': 'Response to comment on revision', 'comment': 'We are pleased that the reviewer is satisfied with the revision. The additional suggested references will be included in the related work (see also answer to ""Answer to other frameworks"" comment).'}, {'title': 'Comment on suggested related work', 'comment': 'We thank the reviewer for the clarification and will include the suggested works in our related work, both when talking about expressive power and universality. We are aware of these works, which use a completely different approach to boost expressive power than what we consider. It would, in fact, be interesting to understand how the separation power of our TL language changes (and what kind of analysis can be made) when random vectors (for random vertex initialisation) are supported. \n'}, {'title': 'Response to comment on revision', 'comment': ""The reviewer's suggestions concerning related work, the use of other aggregation function and the inclusion of spectral GNNs were all very valuable for making the revision. We are pleased that the reviewer is now fully satisfied with the revision.""}, {'title': 'Response to Authors', 'comment': 'I am satisfied with the changes made to the paper. It is now much clearer what the contributions of the work are. I have replied to your request in the ""Answer to other frameworks"" comment, and have listed some relevant references on universality/approximation results for GNNs, based on the invidualization/randomization paradigm, to include in the paper. \n\nAll in all, my concerns have been well-addressed. I thus increase my score. '}, {'title': 'Reviewer Response', 'comment': 'I was mostly referring to a better discussion of matrix query languages and a presentation of the contributions made by TL, and this has been addressed. However, in terms of other frameworks and expressiveness studies, you should also include approaches based on indivualization/randomization to study/improve expressiveness in Section 2. \n\n[1] Sato et al. Random Features Strengthen Neural Networks. ICDM 2021.\n[2] Abboud et al. The Surprising Power of Graph Neural Networks with Random Node Initialization. IJCAI 2021. \n[3] Dasoulas et al. Coloring Graph Neural Networks for Node Disambiguation. IJCAI 2020.\n\nThese papers offer a means to improve on the expressiveness power of standard GNNs, with [2,3] providing universality results. '}, {'title': 'General Response to Authors', 'comment': 'Indeed the answers pointed out my concerns very well. Now the connection to the similar work is solid. Especially extending the analysis on higher power GNN such as CWN, GSN and MPNN which uses different aggregation are highly appreciated. I therefore increase the mark to 8. '}, {'title': 'Answer to reviewer 4', 'comment': 'We thank the reviewer for the detailed reviews and insightful comments which we will next address in turn.\n\n**Clarification of the order of $\\mathsf{WL}$-tests.** It is indeed confusing that some works on the separation power of $\\mathsf{GNN}$s use the ""folklore"" version of $k$-$\\mathsf{WL}$ while other works use the ""oblivious"" version $k$-$\\mathsf{OWL}$. We use the folklore version of $k$-$\\mathsf{WL}$ which, roughly speaking, corresponds to $(k+1)$-$\\mathsf{OWL}$. Colour refinement ($\\mathsf{CR}$) is not precisely the same as our $1$-$\\mathsf{WL}$, however, when it comes to vertex embeddings. We refer to Grohe (2021) for a detailed comparison between $k$-$\\mathsf{WL}$, $k$-$\\mathsf{OWL}$ and $\\mathsf{CR}$. \n\n*In the revision, we now clearly state in Section 2, that we use the ""folklore"" version of Cai et al. (1992).* Thanks for pointing this out.\n\n- Martin Grohe: The Logic of Graph Neural Networks. LICS, 2021.\n- Jin-yi Cai, Martin Fürer, Neil Immerman: An optimal lower bound on the number of variables for graph identifications. Comb. 12(4): 389-410 (1992).\n'}, {'title': 'Response to weak point 1 (novelty)', 'comment': '**Novelty.** The reviewer correctly points out that Baliclar et al. (2021) already connect $\\mathsf{GNN}$s to an, albeit different, matrix query language $\\mathsf{MATLANG}$ (Brijder et al. 2019), and use known connections between $\\mathsf{MATLANG}$ and $1$-$\\mathsf{WL}$ and $2$-$\\mathsf{WL}$, as established in Geerts (2021), to gain upper bounds on the separation power of ($1$st-order) $\\mathsf{GNN}$s. In this way, Baliclar et al. (2021) provide additional motivation for our work and we thank the reviewer for pointing out this reference.\n\n*In the revision, we now both include Baliclar et al (2021) as additional motivation for our work and also describe it in the related work section (both in the main paper and in Section A in the supplementary material.*\n\nIn our opinion, Baliclar et al. (2021)  does not reduce the novelty of our approach. In contrast to Baliclar et al. (2021), we use a new matrix query language ($\\mathsf{TL}$) which allows us to make new and important observations: the separation power of $\\mathsf{GNN}$s is related to the number of index variables (or more generally treewidth) and the summation depth of $\\mathsf{TL}$ expressions needed to define the layers of $\\mathsf{GNN}$s. In addition, whether guarded or unguarded expressions are needed, relates to whether $\\mathsf{CR}$ or $1$-$\\mathsf{WL}$ are required to assess the separation power of some $\\mathsf{GNN}$s. These connections are, to our knowledge, new and are not reported (and neither easily follow from) Baliclar et al. (2021) or previous works on matrix query languages. The simple quantitative measures (number variables, treewidth, summation depth) are the distinguishing and new aspects in our work compared to previous works.\n\nWe also want to clarify some statements in our paper with which the reviewer disagrees. With ""We do not have any general technique allowing us to expand these results for arbitrary $\\mathsf{GNN}$s"", we mean that no previous approach, whether based on matrix query languages or other any other method, allows for the analysis of the separation power of general $\\mathsf{GNN}$s. The $\\mathsf{GNN}$s considered in Baliclar et al. (2021) are restricted to those representable in $\\mathsf{MATLANG}$, which is a much less powerful and less general matrix query language than $\\mathsf{TL}$. Indeed, $\\mathsf{MATLANG}$ can be seen to be subsumed by $\\mathsf{TL}_3$. but  even when restricted to $\\mathsf{GNN}$s that are bounded by $2$-$\\mathsf{WL}$, it is not so immediate to cast, say $2$-$\\mathsf{FGNN}$s in $\\mathsf{MATLANG}$. As another example,  standard $\\mathsf{GNN}$s and $\\mathsf{GCN}$s can be represented in $\\mathsf{MATLANG}$, but when writing them in $\\mathsf{TL}$, one sees that a different summation depth is needed, and hence they are bounded by a different number of iterations of $\\mathsf{CR}$.  *In the revision we rephrased the sentence and now state ""We provide a tensor language-based technique to analyse the separation power of GNNs"".*\n\nSimilarly, when we say ""general matrix query languages are known, albeit not in the context of GNNs"", we mean again that, to our knowledge, no specification language geared towards $\\mathsf{GNN}$s has been proposed. Bacilcar et al. (2021) use an existing language ($\\mathsf{MATLANG}$) and Geerts et al. (2021b) propose more general matrix query languages in which computations can be done beyond what is needed for $\\mathsf{GNN}$s. Our language $\\mathsf{TL}$ is designed for $\\mathsf{GNN}$s and we can easily model classical $\\mathsf{GNN}$s, higher-order $\\mathsf{GNN}$s, and a variety of newer models in which classical $\\mathsf{GNN}$s are extended with additional graph structural information. *In the revision we reworked the related work entirely.*\n\nFinally, the statement ""In summary, our paper draws new and interesting connections between tensor languages, $\\mathsf{GNN}$ architectures and classic graph algorithms"" refers to the introduction of $\\mathsf{TL}$, a specification language for general $\\mathsf{GNN}$s, and the connection between the number of variables (treewidth) and summation depth and $k$-$\\mathsf{WL}$. All of this is, to our knowledge, new, and not covered by any previous works. Furthermore, $\\mathsf{TL}$ allows for generalising previously known approximation results. *We therefore kept this sentence in the revision.*\n\n- Muhammet Balcilar, Pierre Héroux, Benoit Gaüzère, Pascal Vasseur, Sébastien Adam, Paul Honeine: Breaking the limits of message passing graph neural networks. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021.\n- Robert Brijder, Floris Geerts, Jan Van den Bussche, Timmy Weerwag: On the Expressive Power of Query Languages for Matrices. ACM Trans. Database Syst. 44(4): 15:1-15:31 (2019).\n- Floris Geerts: On the Expressive Power of Linear Algebra on Graphs. Theory Comput. Syst. 65(1): 179-239 (2021).\n- Floris Geerts, Thomas Muñoz, Cristian Riveros, Domagoj Vrgoc: Expressive Power of Linear Algebra Query Languages. PODS 2021b\n'}, {'title': 'Response to weak point 2 (layer/iteration connection)', 'comment': '**Fine-grained analysis in terms of number of layers/iterations.** The reviewer finds the fine-grained analysis of the separation power in terms of the number of iterations of $k$-$\\mathsf{WL}$ not so relevant and trivial. We here respectfully disagree with the reviewer. While this may be immediate for $\\mathsf{CR}$ or maybe $1$-$\\mathsf{WL}$, for more complex architectures such as $\\mathsf{GCN}$s, $k$-$\\mathsf{IGN}$s, $\\mathsf{GSN}$s, etc., the connection between layers and iterations of $k$-$\\mathsf{WL}$ is quite intricate. \n\nWe solve this problem by connecting the number of rounds of $k$-$\\mathsf{WL}$ with the summation depth of $\\mathsf{TL}$ expressions used to model  $\\mathsf{GCN}$s, $k$-$\\mathsf{IGN}$s, $\\mathsf{GSN}$s, etc. A nice example of this are the $k$-$\\mathsf{IGN}$s, where the summation depth needed  for each layer precisely tells how many steps the $k$-$\\mathsf{WL}$ test needs to do, to simulate a single $k$-$\\mathsf{IGN}$ layer. This gives additional insights of what the equivariant layers can do. \n\nAs such, our contribution goes beyond simply connecting layers to iterations. Furthermore, the number of layers is an important parameter of $\\mathsf{GNN}$s in practice. No $\\mathsf{GNN}$ architectures can have as many layers as needed to compute the stable $k$-$\\mathsf{WL}$ colouring as this depends on the graph (size). Understanding the limitations of finite layered architectures is therefore desired. Finally, it is known that using more iterations of $k$-$\\mathsf{WL}$ adds separation power, similarly as using more variables adds to the separation power. Hence understanding the iterations needed allows us to compare different architectures, or even the same architecture with a different number of layers.'}, {'title': 'Response to weak point 3 (ease of using TL)', 'comment': '**Ease of writing $\\mathsf{GNN}$s in $\\mathsf{TL}$.** The reviewer finds that casting $\\mathsf{GNN}$s in $\\mathsf{TL}$ is not that straightforward as one needs to write down $\\mathsf{GNN}$s in tensor form. Indeed, the complexity of writing down $\\mathsf{GNN}$s in $\\mathsf{TL}$ is related to the complexity of the $\\mathsf{GNN}$s under consideration. For complex $\\mathsf{GNN}$s, defining $\\mathsf{TL}$ expressions will inherit the $\\mathsf{GNN}$s model complexity. In most cases that we encountered, however, writing down the $\\mathsf{TL}$ expression simply required writing down, almost verbatim, the definitions of $\\mathsf{GNN}$s as found in the literature. For $\\mathsf{GNN}$s that use tensors, for example $k$-$\\mathsf{FGNN}$s and $k$-$\\mathsf{IGN}s$, the definitions given in e.g., Maron et a.l (2019) are already using explicit tensor manipulations, which translate easily into $\\mathsf{TL}$. \n\nWe also want to reiterate that once the $\\mathsf{TL}$ expressions are obtained, no further analysis in terms of $\\mathsf{CR}$ or $k$-$\\mathsf{WL}$ is needed, as this follows from our results. We believe that writing down $\\mathsf{TL}$ expressions should be considerably easier than proving bounds in terms of $k$-$\\mathsf{WL}$ from scratch for each $\\mathsf{GNN}$ model. This is really where the advantage of $\\mathsf{TL}$ comes into play. Finally, when implementing $\\mathsf{GNN}$s in practice one often has to think in terms of explicit tensor manipulation.\n\n- Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman: Provably Powerful Graph Networks, Neurips 2019.'}, {'title': 'Response to weak point 3 (Other aggregation functions)', 'comment': '**Other aggregation functions.** The reviewer asks whether $\\mathsf{GNN}$s that use aggregation functions, other than summation, can be analysed as well. We thank the reviewer for raising this question. We can indeed deal with any other aggregation function but decided to only introduce summation in order not to overload the syntax of $\\mathsf{TL}$. Our results can be generalised when $\\mathsf{TL}$ is extended with a collection (say $\\Theta$) of aggregation functions and such that in the syntax of $\\mathsf{TL}$ one can write “$\\mathsf{aggr}(F,x,e)$”, with $F$ an aggregation function (such as max, stdv, min,...) in $\\Theta$. The notion of summation depth is then replaced by a notion of aggregation depth. It requires some additional notation and minor modification of the proof of Proposition C.3. Of course, the choice of aggregation functions matter when $\\mathsf{CR}$ or $k$-$\\mathsf{WL}$ lower bounds are required, as is well known from the seminal work by Xu et al. (2019). \n\n*In the revision, we now state in Section 3  that $\\mathsf{TL}$ can be generalised to include arbitrary aggregation functions that work on multisets of values and hint in a footnote how this can be done. In the supplementary material (Section C.5) we describe what changes are needed. In addition, we mention GraphSage using different aggregation functions and PNAs in Section 5, and give their analysis in Section D.1. as requested by the reviewer.*\n'}, {'title': 'Response to weak point 3 (ChebNet, spectral methods)', 'comment': '**Spectral GNNs.**  Another interesting question raised by the reviewer is whether spectral-based $\\mathsf{GNN}$s can be analysed as well. And indeed, special kinds of spectral $\\mathsf{GNN}$s can be analyzed. For example, ChebNet was analyzed in Balcilar et al. (2021) using connections to $\\mathsf{MATLANG}$. We can carry out a similar (and slightly refined analysis of ChebNet resulting in bounds by colour refinement) when the use of the maximal eigenvalue of the graph Laplacian does not play a role (see also Theorem 2 in Balcilar et al. (2021). Using the guarded fragment we get a bound in terms of colour refinement rather than $1$-$\\mathsf{WL}$, which makes a difference for vertex embeddings. As another example, we also show that CayleyNets can be analysed using tensor languages. This analysis seems to be new and nicely complements the spectral analysis of CayleyNets in Balcilar et al. (2021b).  We show that CayleyNets are bounded by $2$-$\\mathsf{WL}$.\n\n *In the revision, we now mention the Chebnet result in the paper and provide details in Section D.3, as requested by the reviewer. The analysis of CayleyNets can be found in Section D.3. as well.*\n\nIn general, It is an interesting avenue for further research to obtain a better understanding of connections between $\\mathsf{TL}_k$ and spectral properties. To our knowledge, the relation between $k$-$\\mathsf{WL}$ and spectral properties is rather unexplored, except that $2$-$\\mathsf{WL}$ equivalent graphs have the same spectrum (Dawar et al. 2019). Rattan et al. (2021) explore further connections between $k$-$\\mathsf{WL}$ and spectral properties, but how all this connects to $\\mathsf{TL}_k$ or $\\mathsf{GNN}$s needs further investigation.\n\n- Muhammet Balcilar, Pierre Héroux, Benoit Gaüzère, Pascal Vasseur, Sébastien Adam, Paul Honeine: Breaking the limits of message passing graph neural networks. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021.\n- Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, Paul Honeine: Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective, ICLR 2021b.\n- Anuj Dawar, Simone Severini, Octavio Zapata: Descriptive complexity of graph spectra. Ann. Pure Appl. Log. 170(9): 993-1007 (2019).\n- Gaurav Rattan, Tim Seppelt: Weisfeiler-Leman, Graph Spectra, and Random Walks. CoRR abs/2103.02972 (2021).\n'}, {'title': 'Response to weak point 4 (relevance of k-WL)', 'comment': '**Higher-order $\\mathsf{GNN}$s.** The reviewer raises a concern that, especially in the context of higher-order $\\mathsf{GNN}$s, our results may be primarily of theoretical interest. Furthermore, the reviewer indicates that $\\mathsf{MATLANG}$ suffices for most practical $\\mathsf{GNN}\\text{s}$. \n\nWe would like to point out that $\\mathsf{TL}$ and in particular the fragments $\\mathsf{TL}_k$ not only allow for the analysis of $k$-order $\\mathsf{GNN}$s, but also of  ""augmented\'\' $\\mathsf{GNN}$s in which complex graph structural information is used (e.g., the $\\mathsf{GSN}$s by Bouritsas et al. (2020), $\\mathcal F$-$\\mathsf{MPNN}$s by Barceló et al. (2021),  and Simplicial $\\mathsf{MPNN}$s by Bodnar et al. (2021)). The latter kind of $\\mathsf{GNN}$s are more practical than higher-order $\\mathsf{GNN}$s, yet cannot be (provably) analyzed by $\\mathsf{MATLANG}$ because they can exceed $2$-$\\mathsf{WL}$ in separation power.  Furthermore, $\\mathsf{TL}$ allows to relate summation depth to the  number of $k$-$\\mathsf{WL}$ iterations, which is less transparent (and again impossible for $k>3$) in $\\mathsf{MATLANG}$. \n\nIn summary, we respectfully disagree that $\\mathsf{MATLANG}$ suffices to analyse practical $\\mathsf{GNN}$s formalisms. *In the revision, we provide more details on these augmented $\\mathsf{GNN}$s in Section D.2. in the supplementary material and added a paragraph related to these architectures in Section 5 in the main paper.*\n\n- Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. In Graph Representation Learning and Beyond (GRL+) Workshop at the 37 th International Conference on Machine Learning, 2020.\n- Pablo Barceló, Floris Geerts, Juan L. Reutter, and Maksimilian Ryschkov. Graph neural networks  with local graph parameters.Neurips, 2021.\n- Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F. Montufar, Pietro Lio and Michael M. Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial networks. ICML 2021.'}, {'title': 'Response to weak point 5 (insights for GNNs)', 'comment': '**Insights for $\\mathsf{GNN}$s.** The reviewer would like to see more examples of how the gained insights can be used to construct more powerful $\\mathsf{GNN}$. These insights are, in some sense, implied by our main results:\n\n- To go beyond $\\mathsf{CR}$ or $1$-$\\mathsf{WL}$, one has to use $\\mathsf{GNN}$s that, in terms of $\\mathsf{TL}$, use more than two variables, or whose $\\mathsf{TL}$ expressions have a treewidth larger than one. That is, their layer definitions need to use linear algebraic computations only expressible in $\\mathsf{TL}_k$ for $k>2$. *In the revision, we further highlight this after Theorem 4.1 and in Section 5 when discussing treewidth (item 4).*\n - For vertex embeddings, if one wants to build $\\mathsf{GNN}$s that match $1$-$\\mathsf{WL}$ (rather than $\\mathsf{CR}$) in separation power, non-guarded $\\mathsf{TL}_2$ layers are needed. For example, this can be achieved by allowing global aggregation in layers as in $\\mathsf{eGIN}$s in Section 5. *We now further highlight this in Section 5 in the revision, when discussing $\\mathsf{eGIN}$s.*'}, {'title': 'Answer to expressibility of functions', 'comment': '**Expressing functions.** The reviewer suggests also to include some more holistic overview of the $\\mathsf{GNN}$ landscape in terms of expressible functions (as in Barceló et al (2020)). *In the revision, we now describe how our results can help in designing $\\mathsf{GNN}$s for certain functions  based on the limit of the separation power. (Section 4)*. To characterise what functions can be computed using $\\mathsf{GNN}s$ would require to lift the results in Barceló et. al. (2020) to $\\mathsf{TL}_k$. This in turn would require new interpolation theorems for bounded variable logics, which we are not aware of. \n\nWe would  like to note that our results can, to some extent, be understood as an extension of Barceló et al. (2020) notion of universality from logical classifiers to the general setting of arbitrary classifiers. Whereas they show that guarded modal logic, $\\mathsf{MPNN}$s and $\\mathsf{CR}$ are equivalent in expressive power when restricted to logical classifiers, we show that $\\mathsf{GTL}$, $\\mathsf{MPNN}$s and $\\mathsf{CR}$ are somewhat equivalent also in the general setting, in the sense that both $\\mathsf{MPNN}$s and $\\mathsf{GTL}$ formulas can approximate any graph classifier bounded by $\\mathsf{CR}$.\n \n* Pablo Barceló, Egor V. Kostylev, Mikaël Monet, Jorge Pérez, Juan L. Reutter, Juan Pablo Silva: The Logical Expressiveness of Graph Neural Networks. ICLR 2020.'}, {'title': 'Answer to presentation', 'comment': '**Presentation.**  We are aware that the presentation is sometimes dense. We tried our best to include concise yet correct definitions in the paper and the page limit necessarily limits to what extent things can be explained. *In the revised version, we did a pass over the paper and attempted to make the things more clear, with the focus on page 3 and page 7, as suggested by the reviewer.* For page 7, the reviewer would like to see more examples. We again are limited by the space limitation and as mentioned in the other comments *only found a place for one more example.*\n'}, {'title': 'Answer to ease of using tensor languages', 'comment': '**Ease of writing $\\mathsf{GNN}$s in $\\mathsf{TL}$.**  The reviewer mentions that it is not clear that writing $\\mathsf{GNN}$s in $\\mathsf{TL}$ is easier. We do propose $\\mathsf{TL}$  to be able to analyze $\\mathsf{GNN}$s and not as a new (or easier) specification language for $\\mathsf{GNN}$s. Where the value of our proposal comes from is that, once $\\mathsf{GNN}$s are represented in $\\mathsf{TL}$ , bounds on their separation power follow from our general results on $\\mathsf{TL}$ . We believe that writing $\\mathsf{GNN}$s in $\\mathsf{TL}$  is easier than proving bounds on the separation power directly for $\\mathsf{GNN}$s. So “easier” refers to showing bounds and not to writing $\\mathsf{GNN}$s.\n\nThe modelling of $\\mathsf{GNN}$s in terms of $\\mathsf{TL}$  is, however, unavoidably related to the complexity of the $\\mathsf{GNN}$s under consideration. For more complex $\\mathsf{GNN}$s, their description in the literature will be more involved, as will the $\\mathsf{TL}$  expressions needed. Due to space limitations, we only showed $\\mathsf{TL}$  expressions for simple $\\mathsf{GNN}$s, deferring more complex $\\mathsf{GNN}$s translations to the supplementary material. As can be seen there, the $\\mathsf{TL}$  translations are in most cases verbatim translations of the $\\mathsf{GNN}$ layer definitions. Exceptions are indeed models like $\\mathsf{GSN}$s or simplicial $\\mathsf{GNN}$s, where one basically has to express all additional computations in $\\mathsf{TL}$  in order to understand their separation power. \nIn response to the reviewer’s request for more examples, *in the revision, we could only find space for one more example, $2$-$\\mathsf{FGNN}$s, and the addition of a paragraph (Section 5) providing some general intuition on how $\\mathsf{GSN}$s, $\\mathcal{F}$-$\\mathsf{MPNN}$s, and Simplicial $\\mathsf{GNN}$s can be analysed. We also provide additional details in the supplementary material (Section D.2, “augmented GNNs”).*'}, {'title': 'Answer to other frameworks', 'comment': '**Other frameworks.** The reviewer mentions “other frameworks characterising $\\mathsf{GNN}$s”, but we are not aware of any other framework as general as ours. Of course, there is the message-passing neural network formalism used in e.g. Barcelo et al. 2020, but these vanilla $\\mathsf{MPNN}$s only allow analysing $\\mathsf{GNN}$s whose separation power is bounded by $\\mathsf{CR}$ (on vertex level) and $1$-$\\mathsf{WL}$ on the graph level (or, at most, $1$-$\\mathsf{WL}$ at the vertex level by using global readouts). \n*If the reviewer could point out more general or other frameworks, then we would be happy to include them in our related work.*\n\n* Pablo Barceló, Egor V. Kostylev, Mikaël Monet, Jorge Pérez, Juan L. Reutter, Juan Pablo Silva: The Logical Expressiveness of Graph Neural Networks. ICLR 2020.\n\n'}, {'title': 'Answer to Novelty', 'comment': 'We thank the reviewer for the positive review and suggestions of how to improve the paper. \n\n**Novelty.**  We wish to emphasise that it is the study of the separation power of a general tensor language (and its use for assessing the separation power of general $\\mathsf{GNN}$s) in terms of the number of index variables, treewidth and summation depth, that constitutes the novelty of our work. Such an analysis was not carried out for previous matrix query languages. To clarify the connections with previous matrix query languages, *in the revision, we have expanded the related work section in the main paper and provide a more detailed comparison with other matrix query languages in the supplementary material (Section A).*\n \nMore specifically, as is now mentioned in the related work section, $\\mathsf{TL}$ is inspired by the “$\\mathsf{sum}$-$\\mathsf{MATLANG}$” matrix query language in Geerts et al. (2021b) but specialised to $\\mathsf{GNN}$s and graphs. Furthermore,  $\\mathsf{TL}$ can be used to define arbitrary tensors, whereas $\\mathsf{sum}$-$\\mathsf{MATLANG}$ works on general matrices but is defined such that only scalars, vectors or matrices can be returned. Furthermore, $\\mathsf{TL}$ uses free vector variables, which combined with the notion of summation depth allows for a much more precise connection with logics, $\\mathsf{CR}$ and $k$-$\\mathsf{WL}$. Free variables are needed to represent vertex embeddings. Moreover, neither separation power nor connections to $\\mathsf{CR}$ and $k$-$\\mathsf{WL}$ were considered for $\\mathsf{sum}$-$\\mathsf{MATLANG}$. Indeed, for $\\mathsf{sum}$-$\\mathsf{MATLANG}$, only an equivalence to the positive fragment of the relational algebra (on real-valued relations) is known (Geerts et al. (2021b)). \n\nAn earlier matrix query language, $\\mathsf{MATLANG}$ (Brijder et al. (2019)) was connected to $1$-$\\mathsf{WL}$ and $2$-$\\mathsf{WL}$ in Geerts (2021), yet $\\mathsf{MATLANG}$ is designed in a different way by only allowing a specific set of matrix operations. $\\mathsf{MATLANG}$ can be shown to be included in $\\mathsf{TL}_3$, and again, $\\mathsf{MATLANG}$ can only output scalars, vectors or matrices. No notion of free variables or summation depth was considered in $\\mathsf{MATLANG}$. $\\mathsf{MATLANG}$ has, however, been used before to analyse $\\mathsf{GNN}$s (Balcilar et al. (2021))\n\nIn summary, $\\mathsf{TL}$ is different from previously proposed matrix query languages and furthermore, the connection to finite variable fragments of logics is novel as are the results on the separation power of $\\mathsf{TL}$ (Theorem 4.2, 4.4) . We also want to emphasise that the guarded fragment and its relation to colour refinement was not considered before in the matrix query language literature (Theorem 4.3), and neither was the bound on the number of indices with respect to the treewidth of expressions (Theorem 4.5). \n\n* Robert Brijder, Floris Geerts, Jan Van den Bussche, Timmy Weerwag: On the Expressive Power of Query Languages for Matrices. ACM Trans. Database Syst. 44(4): 15:1-15:31 (2019).\n* Floris Geerts: On the Expressive Power of Linear Algebra on Graphs. Theory Comput. Syst. 65(1): 179-239 (2021).\n* Floris Geerts, Thomas Muñoz, Cristian Riveros, Domagoj Vrgoc: Expressive Power of Linear Algebra Query Languages. PODS 2021.\n* Muhammet Balcilar, Pierre Héroux, Benoit Gaüzère, Pascal Vasseur, Sébastien Adam, Paul Honeine: Breaking the limits of message passing graph neural networks. ICML 2021.\n\n'}, {'title': 'Answer to Reviewer 2', 'comment': 'We thank the reviewer for the positive review and appreciate that the reviewer finds our approach interesting and our contributions significant. The list of typo’s provided by the reviewer are corrected in the revision, and we also clarified some sentences as suggested. We next address the comments related to function applications and lower bounds.\n\n**The role of functions in $\\mathsf{TL}$.**  The reviewer asks whether function applications do add to the separation power. This is a very good question. The upper bounds hold indeed for any set $\\Omega$ of functions, including  $\\Omega=\\emptyset$, so adding function applications *does not increase* the separation power. The lower bounds also hold for $\\Omega=\\emptyset$, as shown in the proof of Proposition C.5. This seems rather counter-intuitive, but the reason is that in $\\mathsf{TL}$ we do support addition, scalar multiplication and multiplication of expressions. These can be seen as specific function applications by themselves. Only addition and multiplication are needed to construct polynomials that can be used to interpolate between specific values, required to encode logical formulae (Proposition C.5). *In the revision (at the end of Section 4) we now include a comment on the impact of functions on the separation power.*\n \n**General lower bounds.** The reviewer raises the interesting question whether a general approach is possible for also obtaining lower bounds in terms of $\\mathsf{CR}$ or $k$-$\\mathsf{WL}$ tests. This seems challenging. One has to ensure that classes of $\\mathsf{GNN}$s can simulate $\\mathsf{CR}$ or $k$-$\\mathsf{WL}$ by means of injective functions on multisets (as e.g., in Xu et al. (2019), Morris et al. (2019)), or that the classes of $\\mathsf{GNN}$s are rich enough to compute homomorphism counts of treewidth $k$ patterns (Dell et al. (2018), Nguyen et al. (2020)). In some sense, these are just restatements of the requirement that $\\mathsf{CR}$ or $k$-$\\mathsf{WL}$ tests can be simulated, and still require a case by case analysis. We agree that having a more principled approach would be very valuable, but such an approach seems out of reach at the moment.\n\n* Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka: How Powerful are Graph Neural Networks? ICLR 2019.\n* Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe:Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. AAAI 2019.\n* Holger Dell, Martin Grohe, Gaurav Rattan: Lovász Meets Weisfeiler and Leman. ICALP 2018.\n* Hoang Nguyen, Takanori Maehara: Graph Homomorphism Convolution. ICML 2020.'}, {'title': 'Answer to Reviewer 1', 'comment': 'We thank the reviewer for the positive review and shared enthusiasm for our contributions. \n\nThe reviewer only raises one concern related to the relevance of our approximation results. More specifically, the reviewer argues for considering a *continuous setting*, motivated by the results in Wagstaff et al. (2019), rather than the discrete vertex label setting considered in our paper. In accordance, *we have revised the paper such that the vertices in graphs can hold real-valued labels, thereby lifting the discrete to the continuous setting.* Indeed, our proof techniques generalise easily to graphs with continuous vertex labels (labels in some $\\mathbb{R}^\\ell$). As can be seen in the main paper, no changes are required to the definition of our tensor language $\\mathsf{TL}$. Furthermore, we show how our proofs on the separation power of $\\mathsf{TL}$ (Section 4) can be generalised to the continuous setting in the supplementary material (Section C.6). For the approximation results (Section 6), we now assume that $\\mathcal{G}_s$ is a *compact set of graphs*, that is, we consider all graphs of size $n$ with vertex labels coming from a compact set $K$ in $\\mathbb{R}^\\ell$. As a consequence, we now consider the same setting as in Azizian et al. (2021), to which the reviewer also refers.\n\nAs such, we believe that the limitation mentioned by the reviewer is lifted entirely in the revision. The reviewer suggests an alternative proof technique for our approximation results (for the discrete case) based on $\\sigma$-algebras (Chen et al. (2019). Such an approach may indeed be feasible for the discrete case, but it is less clear how these techniques carry over the continuous setting which we now consider. By contrast, as in Azizian et al. (2021), our approach uniformly treats the discrete and continuous setting. One remark is that when looking at approximations by GNNs, for some of the architectures, lower bounds are only known for the discrete setting (e.g., GINs),  as also observed in Azizian et al. (2021).\n\n* Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, Michael A. Osborne: On the Limitations of Representing Functions on Sets. ICML 2019.\n* Waiss Azizian, Marc Lelarge: Expressive Power of Invariant and Equivariant Graph Neural Networks. ICLR 2021.\n* Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna: On the equivalence between graph isomorphism testing and function approximation with GNNs. NeurIPS 2019.\n'}, {'title': 'General response', 'comment': 'We thank the reviewers for their positive and detailed assessment of our paper. In the revision, we clarify the novelty of our approach in relation to previous work. We extend our approach in two major ways to accommodate for comments by reviewers 1 and 4. First, we now consider graphs with vertex real-valued vertex labels, such that our separation and approximation results apply to a continuous setting. Second, we now allow for arbitrary aggregation functions, thereby enabling the analysis of GNN that use aggregation functions different from summation. We also revised the main paper in accordance with other comments raised. For reasons of space, not all comments could be addressed in the main paper and had to be deferred to the supplementary material. Changes in the main paper are marked in blue. New sections and additional details in the supplementary material are marked with a vertical line in the margin.\n'}, {'summary_of_the_paper': 'This paper introduces a new approach to study the separation power and approximation properties of graph neural networks (GNN). The authors introduce the Tensor Languages (TL) and show the representation of GNN as TL expressions. In Section 4, they show the separation power of TL and relate it to color refinement algorithms like k-WL. In particular, they are able to characterize the separation power of such algorithms after a given number of iterations. These results allows them to compute in Section 5 the separation power of GNNs. They are able to recover all results in the literature in an unified way and prove some new results. In Section 6, they give consequences of their results in term of approximation of GNNs, recovering known results and proving new ones.\nThis is a theoretical paper without any experimental results.', 'main_review': ""Strengths: the paper is very clearly written and makes a clear new connection between programming language and deep learning. Formalizing GNN as a TL is a new toolbox that allows the authors to get an unified theoretical analysis of the separating power of GNNs. In particular, they are able to get new results characterizing the expressive power of GNNs as a function of the number of layers. This alone is a very nice contribution.\n\nWeakness: I think Section 6 on function approximation is less relevant. The main reason is that the authors consider the discrete topology on the set of graphs so that any function is continuous. But GNN are more restricted functions and are continuous as mapping acting on tensors. As a consequence, I think:\n1- the results presented in section 6 are correct but could probably be proved with simpler arguments similar to the ones used in Appendix A of 'On the equivalence between graph isomorphism testing and function approximation with GNNs' by Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna.\n2- the consequences of Theorem 6.1 and Corollary 6.2 are not clear for GNNs as GNNs cannot model any function taking a graph as argument. Indeed the importance of continuity is highlighted in section 3 of 'On the Limitations of Representing Functions on Sets' by Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, Michael Osborne.\nI think the authors should address this issue in Section 6.2 by explaining the possible limitation of their approach. Note that Azizian and Lelarge probably consider continuous functions of tensor representation of the graph for this reason."", 'summary_of_the_review': 'Very nice contribution making a new connection between programming language and GNN to study their expressive power.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper defined formal languages for describing tensor expressions. It showed that their expressive power is identical to (suitably parameterized) color refinement algorithms or (vertex/graph) WL algorithms. Then, by translating existing GNNs into the tensor languages, this paper provided upper bounds for the expressive power of GNNs, which recovered several existing results. In addition, this paper characterized the closure of a function class by functions whose expressive power is equal to or less than the functions (Theorem 6.1). As an application, this paper characterized the expressive power of several GNNs via the tensor language (Corollary 6.2, Proposition E.3).', 'main_review': '# Post-rebuttal Comments (11/29/2021)\n\nI appreciate the authors for their response. I am satisfied with their answers. So, I want to keep my score and vote for acceptance.\n\n# Initial Comments\n\n【Strength】\n- [1] This paper gives a general procedure for deriving the upper bound of expressive power for any GNN as long as we can translate it into the tensor language.\n- [2] Theorem 6.1 and Corollary 6.2 allow us to give lower bounds on the expressive power of GNNs as well.\n\n【Weakness】\n- [1] Compared to the upper bound, it is more difficult to show the lower bound of expressive power because the former needs individual proof for concrete GNNs.\n- [2] Those familiar with GNNs but not with the first-order logic may be hard to follow up the discussion of the paper.\n\n【Correctness】\n- [1] As far as I checked, I could not find any inappropriate points in the proofs.\n- [2] What I am wondering is that in Theorem 4.1, the equivalence relation $\\rho_1(\\mathsf{TL}_{k+1}(\\Omega))$ is independent of the choice of $\\Omega$. Considering the case of $\\Omega=\\emptyset$, does this result imply that the separation power of the tensor language does not increase even if we add expressive functions such as non-linear activation functions or MLPs?\n\n【Technical Novelty And Significance】\n- [1] As mentioned in this paper, several studies such as [Barcelo et al., 2020 ] have studied the expressive power of GNNs via first-order logic. However, while [Barcelo et al., 2020] related GNNs to the existing first-order logic (namely, $\\mathrm{FOC}_2$), this paper defined a new grammar, the tensor language $\\mathsf{TL}$, and related GNNs to the first-order logic via this language. In this sense, the approach of this paper is novel.\n- [2] This approach allows us to analyze expressive power independently of particular GNNs. In addition, it is relatively easy to derive the expressive power of a GNN because it is sufficient to translate the GNN into the tensor language. Therefore, I think this approach is significant.\n- [3] There is no easy and general way to obtain the guarantees for the lower bound of the expressive power compared with the upper bound. We have to check the sufficient condition of Corollary 6.2 for each GNN, which needs proof tailored to the GNN. If we can find a general approach to obtain the guarantees, it would increase the paper\'s significance. (However, it is also notable that this paper obtained lower bounds systematically to some extent, as mentioned in Proposition E.3.)\n- [4] The paper gives positive answers to the unresolved issues raised by existing studies. In this sense, this paper is significant.\n\n【Empirical Novelty And Significance】\n- [1] This paper does not have numerical experiments.\n\n【Detailed Comments】\n- [1] P.3: I think it is better to write the definition of irreflexivity as it is not well-known.\n- [2] P.3: Initially, for a graph $G$ and $\\mathbfit{v}\\in V^k_G$, ... → I am afraid this sentence is hard to understand, especially the part ""where, atp_k(G, v) is the atomic type..."". Could you reconsider the sentence?\n- [3] P.4: I think $S$ is undefined.\n- [4] P.6: The definition of $\\mathsf{C}_{k+1}$ does not appear in the main text (only available in Appendix).\n- [5] P.13: $\\pi_{\\sigma\\star G}\\textlbrackdbl \\varphi_1, \\sigma \\star \\nu \\textrbrackdbl \\cdot \\pi_{\\star G}\\textlbrackdbl \\varphi_2, \\sigma \\star \\nu \\textrbrackdbl$ → $\\star G$ should be $\\sigma\\star G$.\n- [6] P.15: e..g., → e.g., \n- [7] P.16: Here the unravelling is the (infinite tree ... → remove the parenthesis\n- [8] P.17: $\\pi_{H}\\textlbrackdbl \\varphi_1, \\mu[x_i \\to v] \\textrbrackdbl$ → $\\pi_{G}\\textlbrackdbl \\varphi_1, \\mu[x_i \\to v] \\textrbrackdbl$', 'summary_of_the_review': 'As far as I checked, I could not find any incorrect points in the proofs. I think this paper is technically novel and significant as it gave a model-agnostic approach for analyzing the expressive power of GNNs and solved several conjectures.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'N.A.', 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposes Tensor Language (TL), a language with which popular GNN models can be uniformly studied, so as to yield insights about their expressiveness and separation power. More specifically, the paper shows how TL corresponds to the WL hierarchy, and shows that summation depth in TL, as well as the number of index variables, correspond directly to iterations and tuple size for k-WL, respectively. Moreover, the paper establishes a direct correspondence between GNN update equations and expressions in TL, thus offering a simpler means to understand the expressive and separation power of GNNs, even considering their number of layers, by mapping their architectures to expressions in TL. TL , as well as its guarded fragment GTL, are then used to re-establish a set of known GNN results from the literature. \n\nBeyond expressive power, TL is also used to quantify the function approximation power of GNNs, and also shows that GNNs, characterized by a TL fragment, can learn functions with separation power upper-bounded by a refinement algorithm corresponding to this fragment. Finally, the TL framework is used to establish new results. In particular, it shows that k-IGN cannot achieve expressiveness beyond (k-1)-WL, as this model corresponds to tk iterations of (k-1)-WL, and also offers insights as to the expressiveness of k-IGN with a polynomial number of layers: Indeed, model power does not increase with more standard layers, e.g., those relying on the adjacency matrix, but rather, any increase in layers can only improve expressiveness by deriving GNN functions with e.g., increased treewidth.', 'main_review': 'The unified framework provided by TL is interesting, and allows a more uniform study of GNN models, and a simpler means to derive expressiveness bounds for new GNN models. The study of TL is also very well-grounded in the literature, as it includes most of the key works on GNN expressive power, and additionally confirms their findings. Furthermore, TL establishes a new set of results, thereby addressing some open questions in the field. In particular, it produces interesting insights about the effect of more layers, connecting this explicitly to treewidth and thus adding a nice nuance to this discussion. The results also appear intuitive and sound, though I did not check these thoroughly. Hence, the paper seems to be a valuable addition to the literature on GNNs.\n\nNonetheless, I find that the novelty of the approach is limited, particular relative to other frameworks characterizing GNNs. In particular, I cannot see the novelty proposed by TL relative to the matrix query languages mentioned in the paper. Therefore, I strongly suggest a more through comparison with related work. Furthermore, it is not clear how much simpler writing a TL expression for GNNs is, particularly with respect to non-standard GNNs involving, e.g, sub-structure counting. Hence, a more detailed presentation of specific model TL expressions, including some from the supplementary material, should be added into the paper to more clearly present the TL translation process. On a more minor note, the current writing style and notation are hard to follow. I refer specifically to Page 3, and the explanation of cr, gwl,vwl, etc. This section required multiple reads to be properly understood, and is quite dense. This is also true of other parts in the paper, e.g., Page 7. Therefore, a more simplified presentation, supported by examples, would be beneficial. Finally, the authors can also consider studying their framework with respect to classes of functions (or universality), as in Barcelo et al. [1], rather than just function approximation relative to separating power, to provide a more holistic overview of the GNN landscape. Alternatively, illustrations of functions with separation power limits can also benefit the presentation in this part of the paper. \n\n[1] Barcelo et al. The logical expressiveness of graph neural networks. ICLR 2020.', 'summary_of_the_review': ""The paper's main proposal, TL, offers a novel and simple means to characterize the power of GNNs irrespective of specific design choices, and helps derive some new results for GNNs. All in all, the paper makes good contributions, but some of its claims, namely the simplicity of TL translation, as well as its novelty, should be better explained."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""The paper proposed a novel way of evaluating the expressive power of any arbitrary GNNs in terms of WL test order using tensor language. The main motivation of this paper is that there is no straight forward expressive power evaluation tool so far. All existing methods need architecture specific proofs. Main challenge is the translating given GNN into tensor language. Later, they easily parametrize given GNN by two numbers which are the number of\xa0indices of tensor language ($k$ ) and number of summation depth ($t$). Then, they claimed that given GNN's expressive power is equal to $k-1$-WL test when it applies $t$ coloring rounds."", 'main_review': '## Strong Points\n\n1. Evaluation of the expressive power of GNN is indeed not straightforward and needs a lot of architecture specific proofs. Indeed, we need model agnostic way to evaluate expressive power of any GNN. \xa0Thus the main motivation of the paper is appropriate and might have a significant effect on the literature.\n\n2. Paper was written well, quite clear and understandable. It includes enough theoretical proofs and examples on how to find the expressive power of some well-known GNNs.\n\n## Clarifying the used WL test order\n\nClarifying the used WL test order would be great. In GNN papers, generally we call CR as 1-WL, then original 1-WL as 2-WL and so on and so forth. It seems that this paper follows (Cai et al 1992) notations. It means the GNN in (Maron et al.,2019b) , known to have 3-WL power in the original paper, has 2-WL power according to this paper. It would be better if this point is clearly mentioned.\n\n## Weak Points\n\n1. The way of evaluating expressive power by tensor language is not new at all. According to my knowledge, this kind of evaluation was recently proposed in [1]. They introduced Geerts\' matrix query language (MATLANG) for GNN world and showed how to use it in order to evaluate some well known GNN\'s expressive power by write down given GNN\'s forward calculations in matrix form. Then, they figured the expressive power out by determining which language can explain GNN\'s forward calculations. Thus, In Separation Power sections ""We do not have any general technique allowing us to expand these results for arbitrary GNNs"" and in Related Work section ""general matrix query languages are known,albeit not in the context of GNNs"" are basically not true.\xa0Also [1] falsified\xa0the claim that ""In summary, our paper draws new and interesting connections between tensor languages, GNN architectures and classic graph algorithms"". I strongly think the connection between the tensor language and WL-test order of GNN models is interesting and\xa0valuable, but not novel.\xa0Definitely the differences between similar recent works should be discussed.\xa0 \n\n2. Showing how many iterations of the WL-test equivalent of the given GNN is trivial. It is well-known that an additional layer in GNN is equivalent to WL-test\'s additional iteration (coloring rounds). So in expressive power analysis, we can assume that we always have enough layers as we assume the WL test continues till the stabilization of colors. Introducing that depth parameter as a contribution seems a little exaggerated. In my mind, just WL test order equivalence is enough. \n\n3. Even though the main claim is that by this paper, anyone can determine the GNN\'s expressive power easily, is not that straight forward. One needs to write down the GNN layer in tensor form. It seems possible while using summation and/or weighted summation for neighborhood aggregation. How about other aggregations? Can PNA ( GNN that uses different aggregation such as max, min, std) be evaluated by this framework? I have seen that in Appendix, there is an example for GraphSage\'s version which uses sum aggregation. But the main version of GraphSage uses max. \xa0There is no analysis for max aggregator of GraphSage. Is it possible to do it? Also some GNN such as Chebnet can be more powerful than 1-WL in some certain cases. But in general it is not less powerful than 1-WL. Can the proposed method determine this special case? Can Chebnet expressive power analysis be at least on the appendix?\n\n4. By Geerts\' matrix query language MATLANG, we can determine the expressive power up to 3-WL (or 2-WL in Cai et al 1992 notation). This paper extends this matrix language in order to determine the expressive power of GNN which goes beyond the 3-WL. However, due to the memory and cpu complexity it is not that practical and I have never seen any GNN in practice whose expressive power is more than 3-WL. So the main contribution of the paper beside what MATLANG provides, seems not necessary in practice at least for now. I am suspicious if we really need more powerful than 3-WL GNN. Thus maybe these theoretical analyses will never be used.\n\n5. One of the main practical advantages of this theoretical work would be to give us some insight on how to increase the expressive power of GNN. Even though it is mentioned in the abstract, I am not sure these insights are clear enough. Can we create a new GNN architecture to use these insights? I see that experimental work such that proposing a new GNN is not the main idea of the authors. But at least some clear examples on how to increase expressive power would be very valuable. \xa0\n\n\n### Reference\n[1] Muhammet Balcilar, Pierre Heroux, Benoit Gauzere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021\n', 'summary_of_the_review': 'I think theoretical sounds of the paper is quite strong and the main motivation is valid.\nI am slightly to lean to recommend acceptance for this paper. But  above 5 points should be addressed.\n\nAfter rebuttal,the authors pointed out all my concerns very well. Therefore I would recommend clear acceptance for this work.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Expressiveness and Approximation Properties of Graph Neural Networks', 'authorids': ['~Floris_Geerts1', '~Juan_L_Reutter1'], 'authors': ['Floris Geerts', 'Juan L Reutter'], 'keywords': ['Graph Neural Networks', 'Colour Refinement', 'Weisfeiler-Leman', 'Separation Power', 'Universality'], 'abstract': 'Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.', 'one-sentence_summary': 'A general methodology for assessing the expressive and approximation power of GNNs is presented.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'geerts|expressiveness_and_approximation_properties_of_graph_neural_networks', 'pdf': '/pdf/9d0fe7ff08261aae56611b7f670de9875c2a9cd9.pdf', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/expressiveness-and-approximation-properties/code)', '_bibtex': '@inproceedings{\ngeerts2022expressiveness,\ntitle={Expressiveness and Approximation Properties of Graph Neural Networks},\nauthor={Floris Geerts and Juan L Reutter},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wIzUeM3TAU}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Jake Topping', 'Francesco Di Giovanni', 'Benjamin Chamberlain', 'Xiaowen Dong', 'Michael Bronstein']",ICLR,Understanding over-squashing and bottlenecks on graphs via curvature,https://iclr.cc/virtual/2022/oral/6850,2022," Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a  curvature-based graph rewiring method to alleviate the over-squashing.",Oral 2: Structured learning,https://openreview.net/pdf?id=7UmjRGzp-A,https://openreview.net/forum?id=7UmjRGzp-A,7UmjRGzp-A,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it's impact.""}, {'title': 'Reply', 'comment': 'Thanks for the update! Performances of some datasets are not so significant. However, the overall paper makes a good contribution. I have increased my score.\n'}, {'title': 'Score update', 'comment': ""Thank you for the additional details and results.\n\nI have increased my rating to 10. \n\nJust note that Table 2 is now too wide and goes beyond the page's borders.""}, {'title': 'Reply #2 to Official Review of Paper1302 by Reviewer LDyA', 'comment': ""Thank you! We've also now expanded our experiments and analysis to better support the theory in our paper, the details of which are laid out in General Comment #2.""}, {'title': 'Reply #3 to Official Review of Paper1302 by Reviewer kUJT', 'comment': ""Our changes in this revision are described in General Comment #2, but as you said you were looking forward to them we'd like to highlight the additional experimental results. We feel that the added inclusion of Chameleon, Squirrel, Actor, Citeseer and Pubmed as datasets, +FA as a baseline method, and the new analysis on degree distributions make for a stronger paper. We again thank you for your feedback and for engaging in helpful discussion.""}, {'title': 'Reply #2 to Official Review of Paper1302 by Reviewer Za5a', 'comment': 'Thank you for your response. Based on your feedback we have substantially fleshed out the empirical side of our paper, namely:\n* Expanding the datasets included to include Chameleon, Squirrel, Actor, Citeseer and Pubmed;\n* Added the +FA method as an additional baseline;\n* Moved to using ""dense"" splits for the WebKB datasets as in [1,2] for consistency with existing literature, which has in turn raised accuracies for all experiments on these datasets;\n* And more fully described our experiment methodology, along with a measure of homophily from [2], in Section 5 and the new Appendix F.\n\nAgain we thank you for your feedback and we hope that the greater experimental support improves your confidence in our work.\n\n[1] Eli Chien and Jianhao Peng and Pan Li and Olgica Milenkovic, Adaptive Universal Generalized PageRank Graph Neural Network, International Conference on Learning Representations (2021)\n\n[2] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. International Conference on Learning Representations (2019)'}, {'title': 'General comment #2 (2021-11-22)', 'comment': ""We'd like to again thank the reviewers for engaging in helpful discussion. As discussed we have substantially expanded our experiment set, which we elaborate on below. We appreciate the feedback you've given on this and we agree that the fuller empirical support better supports the theory we propose, and so makes for a stronger paper. The changes in this revision can be found in Section 5 (page 8) and Appendix F (page 27).\n\nIn this revised version (2021-11-22) we have:\n* Expanded our experiment set to include Chameleon, Squirrel, Actor, Citeseer and Pubmed alongside Cornell, Texas and Wisconsin;\n* Added the +FA method from Alon et al 2020 as a benchmark;\n* Extended our analysis on the effect on graph topology from just number of edges added/removed to also include plots of the degree distributions, as well as the Wasserstein distance (the transportation-based metric mentioned in the previous General Comment) between the original and transformed graphs;\n* Included the measure of homophily from [1,2] in the results table, as well as restating the measure's definition in Appendix F;\n* Included all hyperparameters chosen for all experiments in Appendix F.3.\n\nThe revised results table is in Section 5, at the top of page 8. We have also taken this opportunity to refactor our code for these experiments, which we will publish when possible. We have made sure to make the code modular so that our experiments are reproducible, as well as extendable if any future works would like to build off of it with new models or methods. We feel that the changes described in General Comments #1 and #2 based off of your feedback have made for a much stronger paper, and if the reviewer agrees, we hope that they would consider raising their score. Again we thank you for your responses and we continue to look forward to the coming discussion.""}, {'title': 'Reply', 'comment': 'Thank you for the rebuttal.\n\nPlease include the clarifications on the experiments in the revised paper. Otherwise, it seems that the authors did not pay careful attention to experiments and show selected datasets that give good performances. \n\nI would also like to see updated results (at least for some concerns) with experiments. \n\n'}, {'title': 'Thank you for your reply', 'comment': 'Dear authors,\ngood work with this paper.\nBest;'}, {'title': 'Thanks', 'comment': '1. Thank you\n2. I agree that it might shift the focus of the paper. Thank you for the explanation.\n'}, {'title': 'Minor modifications added', 'comment': 'Thank you for your prompt reply.\n\n1. We have added a more explicit description of DIGL along the lines of our comment in the main text.\n\n2. Although this is an interesting point, we think it might shift the focus of the paper. For the sake of completeness, we limit ourselves to mentioning the following here. In a nutshell, one can find message passing functions that might not promote averaging/diffusion (constant signals in the limit) and hence in principle avoid over-smoothing. However, for such functions, over-squashing might still occur whenever topological bottlenecks and long-range dependencies exist.\n\n3. We thank the reviewer for pointing this out. We have updated the references accordingly.'}, {'title': 'Thank you for your response', 'comment': 'Thank you for your response.\n\nI think that the paper is accessible to a broader audience now!\n\n1. I suggest including the description of DIGL more explicitly in Section 5, as it is the main baseline.\n2. Thank you for the explanation about over-smoothing and over-squashing, I agree with the authors\' opinions. What kinds of GNNs do you consider as ""diffusion-based GNN architectures"", in contrast with ""any message-passing model""? If the authors find this relevant, consider including this discussion in the paper or the appendix as well.\n3. Minor comment: when looking up some of the referenced papers, I noticed that some bib entries refer to the arxiv version where a more updated version exists which includes more information like the discussion, reviews, supplementary material, etc. This is minor, but since the paper depends on previous literature so heavily, I suggest going through the bibliography and trying to replace arxiv entries, to make it easier for the readers to lookup papers. For example Kipf & Welling, Alon & Yahav, Liu et al., Klicpera et al., Defferrard et al.,  Velickoviˇc et al.,.\n\nI am looking forward to the additional experimental results.'}, {'title': 'Reply #1 to Official Review of Paper1302 by Reviewer LDyA', 'comment': 'We thank the reviewer for their analysis and for acknowledging both the relevance of the over-squashing issue in the GNN dynamics and the need for a formalization of such problem. We present below an answer to what the reviewer perceived as some level of arbitrariness in the notions we introduced and discussed.\n\nConcerning the somewhat arbitrary nature of definitions, we agree with the reviewer that when proposing new formalizations of empirical phenomena a level of arbitrariness is needed. Still, we wish to motivate a bit further our choices. Concerning the notion of over-squashing, rather than proposing a specific definition we suggested the Jacobian of node representations with respect to input features as a key object in assessing the dependence (or influence score as also noted in Xu et al., 2018). This is arguably the right mathematical quantity to monitor across an architecture to verify how information (messages) are propagating. The notion of curvature we introduced is again not the only working one. We proposed this definition because it both relates nicely to the well-known Ollivier curvature as derived in Theorem 2 and it allows us to control the neighbourhood of an edge precisely hence arriving to the main result in Theorem 4. Other curvature notions may be suitable to control the graph topology and investigate the existence of bottlenecks.\n\n> minor: Appendices F and G are announced at the beginnning of the Appendix section but I did not have them in my pdf document.\n\nThank you for pointing out the Appendix contents - the reference to Appendix F has been removed, and Appendix G (now Appendix F) describing the hardware specifications has been included in the PDF.'}, {'title': 'Reply #1 (Part 3 of 3) to Official Review of Paper1302 by Reviewer kUJT ', 'comment': ""### Evaluation\n\n> It would really improve the evaluation if the authors could experiment with more GNN types and more benchmarks. Specifically, if the authors could take the datasets and exact settings used in Alon & Yahav (2020) and show how the proposed SDRF method improves the results there.\n\nWe agree that strengthening the experiment section of this work would increase the support for the theory presented. To this end we are performing experiments on datasets from the wider standard suite such as Citeseer, Pubmed, Chameleon and Squirrel. We will also be including the +FA method proposed in Alon & Yahav (2020) as an additional benchmark. If time allows we will reproduce the experiments with GAT and GIN to increase variety in GNN type. The experiments in Alon & Yahav (2020) are something we would like to reproduce with SDRF in future, but due to the difference in codebases it is unlikely we will have time to do this before the end of the discussion period.\n    \n> It would be interesting to compare SDRF and the (computationally expensive) solution of Alon & Yahav in terms of the tradeoff between accuracy and computation cost (the overall number of added edges).\n    \nWe agree, and we will include the +FA benchmark in our comparison of the methods' topological effects on the graph (including proportion of added edges and the other graph metrics to be added).\n    \n> DIGL is compared to SDRF as the main baseline. However, I couldn't find what exactly does DIGL do. The paragraph about random-walk-based rewiring on page 7 does explain the general idea, but it is unclear whether this is explaining directly about DIGL, or in general about PageRank-style approaches?\n    \nDIGL is a pre-processing step proposed in Klicpera et al. (2019) in which the given adjacency $A$ is replaced with a new adjacency $A'$ obtained by some diffusion process on $A$. One of the main diffusion processes analysed in their paper is the PPR method, where $A'$ is obtained by sparsifying what we refer to as $R_{\\alpha}$ in our submission. In Theorem 6 we generally show that the PPR approach is generally not suited to address structural features of the graph like the bottleneck by showing that in general if we take $A' = R_{\\alpha}$, it is not possible to improve the Cheeger-constant of the new rewired graph arbitrarily well with respect to the Cheeger constant of the input graph (in contrast to a curvature-based approach). The result does not hold for the approach in DIGL only, but also applies to any method relying on the PPR matrix $R_{\\alpha}$ derived from a given adjacency $A$. We refer to Proposition 17 and Remark 18 in Appendix E for results that are more tailored to the actual strategy adopted in DIGL since there we also take into account the effect of the sparsification.\n\n### Additional questions to the authors\nIn our opinion over-smoothing and over-squashing are in principle different phenomena. The first occurs on diffusion-based GNN architectures when the depth becomes very large and is independent of the graph-topology and the learning task. Over-squashing instead occurs on any message-passing model depending on the graph-topology (i.e. do we have a bottleneck?) and the learning task (do we have long-range dependencies?) but is independent of the depth. Sometimes there might be a correlation among the two: for example, if we have long-range dependencies we might need many layers hence promoting over-smoothing. In general though, we believe these two problems should be dealt with as distinct issues.\n\n### Other minor comments\n* We have modified the paragraph about discrete curvature streamlining the discussion further. \n* Notations have been updated as suggested by the reviewer in the statement of Theorem 4.""}, {'title': 'Reply #1 (Part 2) to Official Review of Paper1302 by Reviewer kUJT ', 'comment': '### Writing\n* $(i,j)\\in E$ and $i\\sim j$ are equivalent notations to denote edges. The second one is just a shorthand and is preferred in equations.\n    \n* Yes $\\hat{a}$ in equation(1) should be $\\hat{A}$ and we have changed it.\n    \n* Yes $d_{G}$ is the shortest path. We have changed the name as suggested by the reviewer.\n    \n* ""the node features and representations are assumed to be scalar from now on"" - yes, this means that $p_{0} = p_{\\ell} = 1$.\n    \n* We agree with the reviewer that the role of self-loops as pointed out in Corollary 2 is not specifically relevant to the message of the paper and we have hence moved the Corollary to the Appendix. Concerning the differences between Corollary 2 and Theorem 1 in (Xu et al., 2018) we believe that the first result is not exactly contained in the second one. First, the role of self-loops seems to be only implicitly relied on in the _Jumping knowledge_ paper where the authors associate them to lazy random-walks. This has not been made explicit in the statement of Theorem 1 which also proposes an expectation type of result based on the ReLU activation function. On the other hand, Corollary 2 shows how for a graph neural network without self-loops - irrespective of the adopted non-linear activation and of the lazy random-walk interpretation - a node $i$ representation at layer $\\ell$ only depends on features reachable by walks of length _exactly_ $\\ell$. As far as we can see, this does not seem to be easily derivable from the analysis in (Xu et al., 2018). We have also added a Remark at pag. 14 in the Appendix pointing out to (Xu et al., 2018) about the connection between self-loops and lazy random walks.\n    \n* The precise definition of $\\lambda_{\\text{max}}$ can be found in Definition 4 in the Appendix, pag. 15. We now refer to it explicitly in the main text. Note that to avoid confusion with standard notations for eigenvalues we have renamed it to be $\\gamma_{\\text{max}}$.\n    \n* In the discussion surrounding Figure 3 we have used a shorthand $\\sharp_{\\square}^{0}$ for $\\sharp_{\\square}^{0}(0,1)$. We have changed the shorthand back to the usual notation to avoid confusion.\n    \n* The node $0$ in the computation of $\\sharp_{\\square}^{1}(0,1)$ is not included because as specified in the definition of $\\sharp_{\\square}^{i}(i,j)$ we never consider the node $j$ (this appears as the requirement $k \\neq j$ in the definition).\n    \n* In Corollary 4 the volume of geodesic balls refers to the quantity $\\lvert B_{r}(i)\\rvert$, for a given node $i$ and radius $r$, i.e. it is the number of nodes at distance smaller or equal than $r$ from node $i$. The volume of geodesic balls _grows polynomially_ if \n$  \\lvert B(i,r)\\rvert \\leq P(r), $\nfor any node $i\\in V$, with $P$ a polynomial in $r$. We have added this formula in the statement of Corollary 4 to make the result more accessible.\n    \n* We comment here about the details of Theorem 5 (now Theorem 4 in the revised submission). We have added further explanation in the paper as well. First about the assumptions: the requirement in (i) is just to control the message passing functions - in the linear case we would have the norm of the weight matrices. Assumption (ii) instead means that the curvature of the given edge is negative enough when compared to the degrees of the endpoint nodes (recall that by our definition $\\text{Ric}(i,j) > -2$). The further condition on $\\gamma_{\\text{max}}$ is meant to avoid pathological cases where we have a large number of degenerate 4-cycles passing through the same three nodes. If these conditions are satisfied, then we are able to prove that negatively curved edges are those creating bottleneck regarded as those graph edges where the propagation of information is negatively affected. More precisely, we show that there exist a large number of nodes $k$ such that GNNs - on average - struggle to propagate messages from $i$ to $k$ in two layers despite these nodes $k$ being at distance $2$ from $i$. This is measured explicitly by the Jacobian (or influence score).\n\n    \n* The notations in equation (5) are the standard ones in spectral graph theory. First, one defines a constant $h_{S}$ for each subset $S\\subset V$ to be the ratio between the number of edges from $S$ to its complement and the smallest volume among $S$ and its complement. The Cheeger constant is then defined as the smallest such $h_{S}$ over all possible subsets $S$. The significance of equation (6) - both for manifolds and graphs - mainly boils down to the possibility of relating spectral properties of an operator (the Laplacian) to topological features of the space (in this case the Cheeger constant). '}, {'title': 'Reply #1 (Part 1) to Official Review of Paper1302 by Reviewer kUJT', 'comment': 'We thank the reviewer for their thorough analysis and helpful and constructive feedback. We address below each point raised by the reviewer in order while also referring to the general comment above where we have listed modifications we have already made to the submission. As you have given us a detailed response we also want to respond thoroughly, and so our reply is broken into parts due to character limits.\n\n> Generality - If the analysis in the paper refers only to GCN and GNNs which perform averaging, please mention this explicitly.\n\nAlthough we have presented the sensitivity analysis for the case of averaging aggregations with normalized adjacency, one can also rephrase over-squashing results in terms of the Jacobian for GNNs where features are simply summed in the aggregation step (i.e. the adjacency is not normalized). Consistently with Lemma 1, we restrict to the setting where features and node representations at each layer are scalars to make the discussion simpler. In line with the _Jumping knowledge_ paper (Xu et al., 2018) we consider a GNN-model of the form\n\n$ h_{i}^{(\\ell + 1)} = \\text{ReLU}\\left(\\\\sum_{j\\in \\tilde{N_i}}h_{j}^{(\\ell)}w_{\\ell}\\right). $\n\nThe augmented neighbourhood $ \\tilde{N_i} $ is defined as $N_i\\cup \\{i\\}$. Differently from the setting of Theorem 1 in the _Jumping knowledge_ paper, the aggregation here is not an average but a simple sum. Let us now take nodes $i$ and $s$ such that $s\\in S_{r+1}(i)$ as in the statement of Lemma 1 in our submission. In this case, instead of simply considering the quantity $\\lvert \\partial h_{i}^{(r+1)}/\\partial x_{s}\\rvert$ we normalize the Jacobian entries, obtaining what is referred to as _influence score_ in (Xu et al., 2018):\n\n$ J_{r+1}(i,s) := \\frac{\\left\\vert \\frac{\\partial h_{i}^{(r+1)}}{\\partial x_{s}} \\right\\vert}{\\sum_{k}\\left\\vert \\frac{\\partial h_{i}^{(r+1)}}{\\partial x_{k}} \\right\\vert} $\n\nThis now represents a relative importance of feature $x_{s}$ on the representation of node $i$ at layer $r+1$. If - similarly to Theorem 1 in (Xu et al., 2018) - we assume that all paths in the computational graph of the model are activated with the same probability, then we obtain that on average\n\n$ J_{r+1}(i,s) = \\frac{ \\tilde{A}\\_{is}^{r+1} }{ \\sum\\_k\\tilde{A}^{r+1}_{ik} } \\leq \\frac{\\tilde{A}^{r+1}\\_{is}}{\\text{Vol}(B\\_{r+1}(i))} $\n\nwhere $\\tilde{A} = A + I$ and $\\text{vol}(S) = \\sum_{j\\in S}d_{j}$. In particular, we again find that if for example we have a tree structure, then the right hand side decays exponentially as $2^{-(r+1)}$.\n\nWe have added this comment in Appendix A.\n\n> Writing - as is, the paper can be perfectly understood only by audiences that are very familiar with all related work. (...) For example, the paper uses the names such as ""Ricci"", ""Poincare"", ""Ollivier"", and ""Forman"" frequently. (...) Specifically, calling the main method ""Balanced Forman"", gives the reader the feeling that they would not really understand the proposed method without understanding the original Forman first. (...) some parts of the paper can be moved to the appendix (...) For example (...) Corollary 2.\n\nWe provide here a general answer while referring below for more detailed comments. We have simplified the discussion in Section 3 about curvature in the smooth and discrete settings. Concerning the name of the curvature, we think that including Forman in the name is justified: this in line with other works where combinatorial notions of curvature on graphs usually contain the name Forman (such as Augmented Forman), which is known in the field (for example see _Comparative analysis of two discretizations of Ricci curvature for complex networks_, Samal et al. 2018). While space constraints prevent us from presenting the main curvature candidates on graphs in the main text which give some background to the name, we refer to Appendix C for a thorough review.\n'}, {'title': 'Reply #1 to Official Review of Paper1302 by Reviewer idLR', 'comment': 'We thank the reviewer for their comments, for their positive feedback on our geometric approach to the over-squashing issue and for their suggestions on how to improve the discussion about preservation of graph topological properties when comparing different rewiring methods. We have addressed below the comments in order.\n\n> A critical aspect of rewiring is its effect on the structure and topological properties of the underlying graph. This is not captured by just analyzing the number of added/ removed edges across the graph. I think that it would be more useful to analyze graph characteristics (such as the node degree distribution) or to measure the distance of the original and the rewired graph globally, e.g., with a transportation distance.\n\nSince any graph-rewiring method inevitably modifies the graph structure to reduce some negative effect or promote beneficial ones, the graph-edit distance, albeit elementary, offers a simple way of comparing the input graph with the rewired one. In this regard, any localized rewired method as the one we propose is bound to modify the graph topology only where needed hence allowing us to control the graph-edit distance more easily. However, we agree that more sophisticated methods of assessing the distance between the input graph and the rewired one should also be tested. We first note that as argued at page 7 of our submission, our curvature based method will generally preserve degree-distribution since edges with very large curvature are on average likely to have endpoints with large degree. Namely, we noted how $\\text{Ric}(i,j) < -2 + \\delta$, for some $\\delta \\in (0,2)$, implies that either $d_{i}$ or $d_{j}$ must be larger than $2/\\delta$. To help measure this we will also include the effect on some summary statistics for the degree distribution.\n\nInvestigating transportation distance also seems a good option. We offer an intuitive picture on why curvature-based methods should better preserve transportation distances among graphs. Since our SDRF algorithm works by adding edges among nodes that are at most at distance 2 on the given graph, the matrix of pairwise distances is only going to be modified by 1 for each pair of nodes whose minimal path passed through $i$ and $j$. In other words, since our rewiring steps are local and add edges among nearby nodes, the matrix of distances is affected only in a limited matter. Accordingly, we expect the Gromov-Wasserstein distance between the input graph and the rewired one to be generally smaller for our curvature-based method when compared to random-walk diffusion approaches. If time permits, we will also provide comparisons in our evaluation section for the Gromov-Wasserstein distance.\n\n> Can you comment on the cost of curvature-based rewiring vs. random walk-based rewiring? If curvature-based rewiring is less efficient than random walk-based rewiring, is this (in your experiments) mitigated by a reduced cost in the downstream task (due to the smaller number of edges).\n\nSDRF, our initial example of curvature-based rewiring, often does takes more time than DIGL due to its iterative structure. This being said, as you say we do see that the downstream GNN training is often faster on the datasets preprocessed with SDRF than with DIGL due to the computational cost from the volume of edges often added by DIGL. We will include some plots comparing the preprocessing and training times for the two methods for some of the experimental settings. \n\n> In section 4 you briefly remark that curvature-based rewiring may reduce over-smoothing. I think it could be interesting to expand on that.\n\nWe thank the reviewer for pointing this out. We have decided to remove this sentence since expanding on that could take a lot of space and compromise the flow of the paper. First, as we have also commented on in another review, over-smoothing and over-squashing are in principle different phenomena. The first occurs on diffusion-based GNN architectures when the depth becomes very large and is independent of the graph-topology and the learning task. Over-squashing instead occurs on any message-passing model depending on the graph topology (i.e. do we have a bottleneck and so have negatively curved edges?) and the learning task (do we have long-range dependencies?) but is independent of the depth. Sometimes there might be a correlation among the two, which is what we were hinting at in the paper: if we have long-range dependencies we might need many layers hence promoting over-smoothing. A rewiring addressing the bottleneck might in principle also decrease the average radius of dependencies meaning that in practice we might need less layers. In general though, we believe these two problems should be dealt with as distinct issues.'}, {'title': 'Reply #1 to Official Review of Paper1302 by Reviewer Za5a', 'comment': 'We thank the reviewer for their feedback, for the positive comment on our theoretical contribution and in particular for pointing us to ways to improve the evaluation section and the discussion about heterophily. We address below the comments in order:\n\n> Overall the paper is well written, however, the paper improve further to have better readability.\n\nWe have made changes to the text to reduce the amount of domain-specific terminology without affecting the flow of the paper, as described above in General Comment #1. We have also added further explanation around key results such as Theorem 4.\n\n> Throughout the paper, homophilic and hetherophilic nature of graphs are mentioned, however, the exact definitions of them are not clearly defined. Further, in experiments “low-homophily” is mentioned, but there is not measure for low or high homophily.\n    \nWe agree. In the revision, we will include the definition of the homophily index introduced in [2] and provide its value on the datasets used in the experimental section as in [1,2].\n    \n> ...is it fully-supervised node classification of semi-supervised node classification?\n    \nThere is some ambiguity here since we have node labels which indicates supervised learning, but as we also use the graph structure that can be viewed as semi-supervised. To keep with the terminology used in Klicpera et al 2019 we will use the term semi-supervised node classification and we have clarified this in the paper.\n\n> a) The selected baseline methods are limited, can the authors use further baselines methods? Can the proposed graph rewiring method be compared with the +FA method in [1]?}\n    \nWe agree. We will include the +FA method for comparison in the revision.\n    \n> b) To my knowledge Cora is not a homophilic dataset [1]\n    \nWe should perhaps phrase it more accurately. Using the definition of homophily in [1,2], Cora, Citeseer and Pubmed have a homophily index of 0.83, 0.71, and 0.79 respectively compared to 0.11, 0.06 and 0.16 of Cornell, Wisconsin, and Texas. Due to this large difference, we believe in this context it is fair to call Cora, Citeseer and Pubmed \'homophilic\' and Cornell, Wisconsin, and Texas \'heterophilic\'.\n    \n> It would be helpful if more datasets from hetherophilic graphs (Citeseer, Pubmed) are also considered in experiments, which would allow us to have a better understanding of the over-squashing phenomenon. ... c) Cornell, Texas and Wisconsin are small scale datasets. I suggest to experiments with Chameleon or/and Squirrel dataset, which have large number of nodes and dense graphs.\n    \nWe agree. We will include additional datasets in the revised experimental section. \n    \n> d) Why are accuracies of Cornell, Texas, and Wisconsin low? There are methods that have shown greater accuracy for these datasets [1]. I wonder whether the low accuracy is due to the use of GCN and if a different GNN method is used in with the proposed rewiring method the accuracy would increase. Do authors have any experience with other models?\n    \nThe accuracies shown are are lower compared to better SOTA methods because the experiments are performed with a simple GCN (we used the same PyTorch implementation as in Klicpera et al 2019 available from https://github.com/klicperajo/gdc/blob/master/models.py). We should stress that in our opinion this does not diminish the validity of our theoretical results, since our approach is preprocessing of the graph independent of the GNN architecture used downstream. Since the main contribution of our paper is theoretical, we believe experimenting with different architectures and curvature-based rewiring is important future experimental work. If time allows we will also be including experiments with the GAT and GIN architectures. With regards to [1] specifically, the difference in accuracies may also be influenced by train/val/test splits - [1] uses a 60/20/20 split referred to as ""dense"", whereas we used splits more like Klicpera et al 2019 referred to in [1] as ""sparse"". '}, {'title': 'General response #1 (2021-11-17)', 'comment': 'We would like to thank all the reviewers for taking the time to read our paper thoroughly and for providing insightful and productive comments, both on the theoretical and experimental contents of the paper. Below, we summarize the revisions we have made in response to the reviews. \n\nEdits made in current revision (2021-11-17): \n* Added a comment in Appendix A that extends Lemma 1 to include GNN types which do not perform averaging (see our response to Reviewier kUJT);\n* Simplified  the  discussion  in  Section  3  about  curvature in the continuous vs discrete settings to improve accessibility for a wider audience;\n* Moved Corollary 2 to the Appendix to improve the flow of the paper;\n* Corrected $\\hat{a}$ to $\\hat{A}$ in Equation 1;\n* Renamed $d_G$ as shortest-path rather than minimum-walk distance to better follow convention;\n* Added a remark at page 14 in the Appendix on the connection between self-loops and lazy random walks noted in (Xu et al., 2018);\n* Changed the variable $\\lambda_{\\text{max}}$ to $\\gamma_{\\text{max}}$ to avoid confusion with standard notation for eigenvalues, and added an explicit reference in the main text to Definition 4 in page 15 of the Appendix.\n* Written $\\sharp_{\\square}^{1}(0,1)$ instead of the shorthand $\\sharp_{\\square}^{1}$ in the discussion surrounding Figure 3 to improve readability;\n* Added further comments to motivate and explain the significance of our main result in Theorem 4.\n\nTo be added over the next few days:\n* Experimental results on other datasets including Citeseer, Pubmed, Chameleon, Squirrel, and Actor;\n* Addition of the +FA method from Alon et al 2020 (making the last GNN layer fully connected) as a benchmark;\n* Further measures of effect on graph structure beyond number of edges added/removed, including effect on degree distribution and a transportation-based metric (we will be looking at Gromov-Wasserstein distance);\n* Inclusion of the measure of homophily from [1,2] - the values will be included in the results table in Section 5, and the definition of the homophily index will likely be included in the Appendix due to space constraints and its introduction in other works;\n* Inclusion of all hyperparameters used for each preprocessing/model/dataset combination in the Appendix;\n* Inclusion of some plots comparing the preprocessing and training times for DIGL and SDRF for some of the experimental settings;\n* If time allows, we will reproduce the experiments with GAT and GIN to demonstrate the methods on other GNN types.\n\nAgain we thank you for your responses and feedback and we look forward to discussion over the next week.\n\n[1] Eli Chien and Jianhao Peng and Pan Li and Olgica Milenkovic, Adaptive Universal Generalized PageRank Graph Neural Network, International Conference on Learning Representations (2021)\n\n[2] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks. International Conference on Learning Representations (2019)'}, {'summary_of_the_paper': 'This paper analyzes oversquashing in GNN using geometric methods. The paper proposes a novel rewiring method based on negative curvature to construct graphs that are less susceptible to oversquashing. Though the theoretical contribution is strong the empirical evidence is not strong. Overall a good paper. ', 'main_review': 'Overall the paper is well written, however, the paper improve further to have better readability. It would be helpful to have more intuitive clarity on the use of hyperbolic spaces since many readers may not be familiar with the topic. Throughout the paper, homophilic and hetherophilic nature of graphs are mentioned, however, the exact definitions of them are not clearly defined. Further, in experiments “low-homophily” is mentioned, but there is not measure for low or high homophily. I suggest that the authors provide a proper definition for homophilic/heherophilic as [1,2] and provide numerical measures for datasets used in experiments as in [1]  \n\nThe main weak point of the paper is experiments. First of all, is it fully-supervised node classification of semi-supervised node classification? The experiments are not substantial in terms of dataset selections, comparisons with baseline methods and obtained accuracy. \n\na) The selected baseline methods are limited, can the authors use further baselines methods? Can the proposed graph rewiring method be compared with the +FA method in  [1]?\n\nb) To my knowledge Cora is not a homophilic dataset [1]. It would be helpful if more datasets from hetherophilic graphs (Citeseer, Pubmed) are also considered in experiments, which would allow us to  have a better understanding of the over-squashing phenomenon.\n\nc) Cornell, Texas and Wisconsin are small scale datasets. I suggest to experiments with Chameleon or/and Squirrel dataset, which have large number of nodes and dense graphs. \n\nd) Why are accuracies of Cornell, Texas, and Wisconsin low? There are methods that have shown greater accuracy for these datasets [1]. I wonder whether the low accuracy is due to the use of GCN and if a different GNN method is used in with the proposed rewiring method the accuracy would increase. Do authors have any experience with other models?  \n\n\n\nReferences\n[1] Eli Chien and Jianhao Peng and Pan Li and Olgica Milenkovic, Adaptive Universal Generalized PageRank Graph Neural Network,\nInternational Conference on Learning Representations (2021)\n\n[2] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph convolutional networks.  International Conference on Learning Representations (2019)', 'summary_of_the_review': 'A good paper with a novel ideas on oversquashing in GNN. The main limitation of the paper is the limited of empirical support.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature to mitigate over-squashing. This is motivated by a link between negatively curved edges and graph bottlenecks. The paper has a theoretical focus, but also provides a set of validation experiments to demonstrate the proposed approach.', 'main_review': 'Detailed comments:\n- A critical aspect of rewiring is its effect on the structure and topological properties of the underlying graph. This is not captured by just analyzing the number of added/ removed edges across the graph. I think that it would be more useful to analyze graph characteristics (such as the node degree distribution) or to measure the distance of the original and the rewired graph globally, e.g., with a transportation distance. \n- Can you comment on the cost of curvature-based rewiring vs. random walk-based rewiring? If curvature-based rewiring is less efficient than random walk-based rewiring, is this (in your experiments) mitigated by a reduced cost in the downstream task (due to the smaller number of edges).\n- In section 4 you briefly remark that curvature-based rewiring may reduce over-smoothing. I think it could be interesting to expand on that.\n', 'summary_of_the_review': 'I found the paper very interesting in that it analyzes the over-squashing problems through a new, geometric lens. The theoretical motivation for using curvature-based tools as opposed to random-walk-based rewiring is well-done. My main concern is that the authors’ notion of graph structure preservation is not convincing to me. The experiments could be more comprehensive. Please see detailed comments.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': ' The paper provides an analytical explanation for the over-squashing and GNN bottleneck phenomena. While Alon & Yahav (2020) demonstrated the over-squashing phenomenon empirically and provided mainly intuitions as to why it happens, this paper performs a deeper analysis and connects over-squashing to combinatorial curvatures. The paper shows that negatively curved edges are responsible for over-squashing. This is important because it allows to measure over-squashing, and pinpoint specific edges that are responsible for it in a given graph. \nFurther, the paper proposes a method (called SDRF) to re-wire the graph based on these insights and shows empirically how this method alleviates over-squashing.\n', 'main_review': '## Strengths\n* The paper provides good intuitions to over-squashing. This intuition is visualized in Figure 1 that gives a good intuition of over-squashing and of the proposed SDRF solution.\n* Further, the paper provides a good connection to graph curvatures that allows to measure and analyze over-squashing.\n* The paper provides a thorough analysis using the proposed notations and connects it to further geometric theoretical and classical ideas.\n* The paper provides a good conceptual comparison to random-walk-based rewiring and whether this can address the GNN bottleneck, and also an interesting discussion about the likelihood of GNN bottlenecks in homophilic and heterophilic datasets.\n* The evaluation shows that the proposed SDRF method outperforms the DIGL baseline without a significant increase in the amount of computation compared to the original graph structure, while the DIGL baseline adds an order of magnitude more edges, which increases the computation cost.\n\n## Weaknesses\n\n* Generality - I am not sure how do these results generalize beyond Graph Convolutional Networks (GCN). The use of GCN\'s augmented normalized adjacency matrix is assumed at the beginning of the paper, and this assumption is never questioned.\nAre Lemma 1 and Corollary 2 correct only for GCN, where it is common to use the augmented normalized adjacency matrix? Specifically, this matrix has $D^{-1}$ which decays messages over paths. What happens if nodes are summed (as in GIN)? How does the signal then decay with the number of layers? Intuitively, I can understand that summing an exponentially growing number of nodes in a single vector cannot compress all the information after some compression steps (that is, GNN layers). But how is this expressed in the curvature theory? If the analysis in the paper refers only to GCN and GNNs which perform averaging, please mention this explicitly.\n\n* Writing - as is, the paper can be perfectly understood only by audiences that are very familiar with all related work. The paper can be significantly improved by giving more background, explaining equations more intuitively, and avoiding unneeded citations which confuse the reader. For example, the paper uses the names such as ""Ricci"", ""Poincare"", ""Ollivier"", and ""Forman"" frequently. These references can confuse audiences who are not familiar with all terms and papers. Specifically, calling the main method ""Balanced Forman"", gives the reader the feeling that they would not really understand the proposed method without understanding the original Forman first. I believe that this is not the case, and that the paper *could* be standalone.\nWhile the authors show an impressive understanding and familiarity with the related and classical work, I advise the authors to avoid this ""name-dropping"", in favor of better readability and accessibility to a broader audience.\nI even think that some parts of the paper can be moved to the appendix, in favor of the readability of the remaining parts. For example, I am not sure that Corollary 2 is new (see below), and I\'m not sure whether this is directly related to the point the paper tries to make, or whether is it a side-result.\nSee more specific details below.\n\n* The Evaluation could also be strengthened by addressing more datasets and more GNN types.\nSee more details below.\n\n* Reproducibility - please provide more training details, hyperparameters tuned and ranges, and the final selected hyperparameter values for the experiments in Section 5. A table of dataset statistics (in the appendix) can also help, before and after applying every type of preprocessing method (similarly to Table 3, possibly with absolute numbers as well).\n\n\n## Writing\n\n* Section 2.1: what is ""$(i,j) \\in E$ if $i \\sim j$""? What does $i\\sim j$ mean? Does it simply mean that there is an edge between them? If it only means that there is an edge between them, why is this a different notation than $(i,j) \\in E$?\n\n* What is $\\hat{a}$ in Equation (1)? I cannot find its definition. Is it $\\hat{A}$? It is unclear from the text.\n\n* Section 2.1:\n>""$d_{G}$ is the standard minimum-walk (geodesic) distance on the graph""\n\nIs it simply the **shortest-path**? If so, I suggest using this term which is much more common to the broader audience. If not, the term ""minimum-walk (geodesic) distance"" should be better explained. \n\n* Section 2.1: \n> ""the node features and representations are assumed to be scalar from now on""\n\nthat is, $p_0 = 1$ and all $p_l$ equal to 1?\n\n* Corollary 2 - I think that the ""Jumping Knowledge"" paper (Xu et al., 2018) had already recognized this role of self-loops. What is the difference between this corollary and Theorem 1 in Xu et al. (2018)? Also, as the role of self-loops is not directly connected to the main contributions of this paper, I suggest considering moving it to the appendix, or removing it if it was already recognized by Xu et al, in favor of more space for explanations of other parts.\n\n* Section 3, definition of $\\lambda_{max}$ - what does ""traversing the same node"" mean? the same $k$ node from the previous definition of 4-cycles? A formal definition of $\\lambda_{max}$ will be helpful.\n\n* Section 3, the example describing Figure 3: I cannot understand the examples because in (ii), $\\sharp_{\\square}^{i}$ is defined for a specific edge $\\sharp_{\\square}^{i}(i,j)$, and here it appears as $\\sharp_{\\square}^{0}$ and $\\sharp_{\\square}^{1}$\n\nAlso, why is $\\sharp_{\\square}^{1}={5}$, and not also the node 0 is included, as it also creates a 4-cycle (1-0-3-5)?\n\n* Corollary 4: what is ""the volume of the geodesic balls"", and what does growing ""polynomially"" exactly mean? How is the volume measured, and what does it grow over? (what is the ""x axis""?)\n\n* Theorem 5 - it would help if the authors could clarify this theorem, explain its meaning in words and intuitively. I am not sure I understand why every detail there is necessary, and what are its implications. \n\n* Cheeger constant (Equations (5)+(6)) - I did not really understand the notations in Eq. (5), what is the meaning of Equation (6), what is its significance?\n\n##  Evaluation:\n1. It would really improve the evaluation if the authors could experiment with more GNN types and more benchmarks. Specifically, if the authors could take the datasets and exact settings used in Alon & Yahav (2020) and show how the proposed SDRF method improves the results there. The reason that this would be helpful is that Alon & Yahav showed that these datasets already suffer from over-squashing, by taking existing source code of other papers and improving their results. It would be interesting to compare SDRF and the (computationally expensive) solution of Alon & Yahav in terms of the tradeoff between accuracy and computation cost (the overall number of added edges).\n\n2. DIGL is compared to SDRF as the main baseline. However, I couldn\'t find what exactly does DIGL do. The paragraph about random-walk-based rewiring on page 7 does explain the general idea, but it is unclear whether this is explaining directly about DIGL, or in general about PageRank-style approaches?\n\n## Additional questions to authors\n* According to this analysis, in the authors\' opinion, are over-squashing and over-smoothing the same thing or not? What is the difference between them? (The answer to this question will **not** affect the rating negatively, as it is obviously out of the scope of this paper)\nIf the authors have an insightful explanation, I recommend including it in the body of the paper.\n\n\n\n### Other minor comments:\n* The paragraph about Discrete curvatures on graphs on page 4 is completely unclear for the audience who are unfamiliar with the described curvatures. I suggest delaying this to later or even moving to an appendix in favor of more text that will help clarify the rest of the paper.\n\n* Theorem 5(i) writes $\\ell \\in [0, L-1]$. For clarity, I suggest being consistent which previous notations, for example as Lemma 1 which denotes $0 \\leq \\ell \\leq ...$.\n', 'summary_of_the_review': 'Despite my many comments, I think that this is an important paper, which deepens our understanding of the over-squashing phenomenon from a geometric perspective.\nWhile many recent GNN papers introduce new application domains, new features, and new GNN architectures, this paper improves our understanding of the foundations of existing GNNs and their limitations, which is even more important.\n\nI will increase my rating if the authors would answer all questions in the ""Writing"" section above, in the ""Additional Questions"" section above, and provide more evaluation datasets (as detailed in the ""Evaluation"" section above).\n\n===== Update =====\n\nI have increased my rating to 10, good work.\n\nNote that Table 2 is now too wide and goes beyond the page\'s borders.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In this paper, the authors work on the over-squashing effect that has been recently observed in the GCN literature, namely the effect that, in a message-passing paradigm and for a learning problem that necessitates long-range interaction between distant nodes, the existence of bottlenecks in the graph (for instance an edge e with very large betweenness centrality) will intuitively distort the information that needs to travel from distant nodes and thus hinder the GCN\'s performance.\n\nOne natural direction of research is to add edges (and sometimes remove others in order to keep the complexity under control) to alleviate the bottlenecks. This rewiring process may be done in different ways and the authors suggest one particular algorithm that they compare with state-of-the-art.\n\nIn particular, the contributions are:\n- a precise (even if somewhat arbitrary) definition of an over-squashing measure between two nodes $i$ and $s$ that is simply the Jacobian of the node representation at $i$ with respect to the entry $x_s$. The smaller this measure, the less $i$ ""feels"" $s$, the larger the over-squashing effect. The goal of the paper is to find a way to understand how to alleviate the bottlenecks responsible for this effect\n\n- to this end, the authors suggest a new definition of Ricci-like curvature defined over the edges of the graphs. This new definition has one main property which is theorem 3: it lower bounds the Ollivier curvature. This then enables to obtain Cor. 4, and finally the main result Thm 5, that states, in a nutshell, that negatively curved edges are the ones causing bottlenecks.\n\n- a concrete rewiring algorithm is suggested, in which two steps are repeated until convergence or max iteration is reached: a first stochastic step is performed where an edge is added to alleviate the edge that has minimal curvature, before a deterministic step where the edge with maximal curvature (typically those in cliques) is removed if it is larger than a threshold\n\n- a tentative analysis, in the form of Thm 8, as well as experiments show the authors\' method performs as well or outperforms other rewiring methods (they mainly compare with DIGL) while keeping the number of total edges after rewiring under control', 'main_review': 'The paper is well-written and well-organized. Formalizing the concept of over-squashing is interesting and timely, and is of interest to the field. This paper, as is often the case when one delves into novel grounds, makes somewhat arbitrary choices in order to move forward: the definition of over-squashing is somewhat arbitrary, the choice of working with that particular definition of Ricci curvature could be more motivated. Efforts are however made in an interesting direction, and the several discussions around the Cheeger constant gives depth to the paper, and its main messages.\n\n- minor: Appendices F and G are announced at the beginnning of the Appendix section but I did not have them in my pdf document\n', 'summary_of_the_review': 'This is novel work on a seemingly important effect hindering GCN performance in general. Giving a formal definition of over-squashing and providing an analysis based on curvature is novel work, and potentially very useful to the field. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Understanding over-squashing and bottlenecks on graphs via curvature', 'authorids': ['~Jake_Topping1', '~Francesco_Di_Giovanni1', '~Benjamin_Paul_Chamberlain1', '~Xiaowen_Dong1', '~Michael_M._Bronstein1'], 'authors': ['Jake Topping', 'Francesco Di Giovanni', 'Benjamin Paul Chamberlain', 'Xiaowen Dong', 'Michael M. Bronstein'], 'keywords': ['Graph neural networks', 'Geometric deep learning', 'Differential geometry', 'Ricci curvature'], 'abstract': ""Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a  curvature-based graph rewiring method to alleviate the over-squashing."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'topping|understanding_oversquashing_and_bottlenecks_on_graphs_via_curvature', 'pdf': '/pdf/f6b974eac8792a0d8d59633044276dabbf9d01c9.pdf', '_bibtex': '@inproceedings{\ntopping2022understanding,\ntitle={Understanding over-squashing and bottlenecks on graphs via curvature},\nauthor={Jake Topping and Francesco Di Giovanni and Benjamin Paul Chamberlain and Xiaowen Dong and Michael M. Bronstein},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=7UmjRGzp-A}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Steeven Janny', 'Fabien Baradel', 'Natalia Neverova', 'Madiha Nadri', 'Greg Mori', 'Christian Wolf']",ICLR,Filtered-CoPhy_ Unsupervised Learning of Counterfactual Physics in Pixel Space,https://iclr.cc/virtual/2022/oral/6542,2022," Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.",Oral 2: AI applications,https://openreview.net/pdf?id=1L0C5ROtFp,https://openreview.net/forum?id=1L0C5ROtFp,1L0C5ROtFp,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""This paper introduces the Filtered-CoPhy method, an approach for learning counterfactual reasoning of physical processes in pixel space. The approach enables forecasting raw videos over long horizons, without requiring strong supervision, e.g. object positions or scene properties. \n\nThe paper initially received one strong accept, one weak accept, and one weak reject recommendations. The main reviewers' concerns relate to clarifications and consolidations in experiments, including stronger baselines, experiments on real data, or more diversity on the datasets. The rebuttal did a good job in answering reviewers' concerns, especially by providing new experimental results and analysis. Eventually, all reviewers recommended a clear acceptance after authors' feedback. \n\nThe AC's own readings confirmed the reviewers' recommendations. The proposed approach is a meaningful extension of CoPhy for the unsupervised prediction at the pixel level. The proposed approach is solid, clearly described, and overcomes important limitations of previous methods. The dataset is also an important outcome for the community. Causality and counterfactual reasoning are of primary importance for the design of effective and explainable AI prediction models: this paper brings therefore an important contribution to the ICLR community.""}, {'title': 'Increased my rating', 'comment': 'Sincerely thank you for the quantitative comparisons and detailed explanations, and the visual examples are quite impressive.\n\nI increased my rating by 3. '}, {'title': 'Thanks', 'comment': 'I appreciate your work looking into the significance of the results in E.2 and your willingness to incorporate my suggestion in the conclusions.'}, {'title': 'Discussions', 'comment': 'Thank you for your quick response, and for your acknowledgement of our work to provide a convincing revision to our paper. Please find our answers to your questions below:\n\n**=== REMARK 1**: *For section E.2, could you report the statistical tests used to conclude that ""there is no significant difference of quality between do-operation types""?*\n\nThank you for your careful reading of our revised appendix. We actually do think that there might be differences in prediction performance according to intervention types, at least for certain tasks. We have made this clear in our initial author response, in particular to our answer to reviewer v4ms where we wrote “*We observe a difference of 3dB in favor of the “Move” do-operation, which is unsurprising, as it is the least disturbing intervention*“. However, we acknowledge a wrong statement in the caption of Figure 16 of the appendix, where we falsely claim that there is no significant difference. \nWe also like to apologize for another small labeling error: the legends (labels) of the x-axis of the bar-graph of figure 16 should be inverted (“Remove” instead of “Rotate” and vice-versa). \nWe will fix the caption and the figure on the camera-ready version of the paper, after acceptance. \nWe also followed your request and performed a statistical test. To be precise, we computed the two-sided p-value for the null hypothesis that the PSNR distributions (averaged over time) of experiments done with (1) “Rotate/Remove” do-operations and, (2) “Move” do-operations are independant and have identical mean value but different standard deviation. The tests indicate differences for BlockTowerCF and BallsCF, but not for CollisionCF\n\n\n\n|         | BlocktowerCF |   BallsCF   | CollisionCF |\n|:-------:|:------------:|:-----------:|:-----------:|\n|         | Move/Remove  | Move/Remove | Rotate/Move |\n| p-value |   1.50e-94   |   2.12e-77  |     0.53    |\n\n\nProperly evaluating the impact of each do-operation on prediction quality is difficult, and quantitative measures are often misleading. We complemented them by qualitative measures: the visual demonstrations we put in the paper and on the project webpage allow the reader to assess the effect of each do-operation visually. We suggest that there are only small differences in quality between the different types of do-operations in, both, the predicted dynamics and the quality of the reconstruction of the objects themselves.\n\n**=== REMARK 2** : *Thanks for affirming my observations about the failures of the method. I do understand that it may be difficult to perform given the amount of additional material you added to the paper in the revision, but could you include perhaps one or two sentences about these failure modes in the conclusion? Something like ""Counterfactual prediction of video frames remains a challenging task, and Filtered CoPhy still exhibits failures in maintaining rigid structures of objects over long prediction time-scales. We hope that our benchmark will inspire further breakthroughs in this domain.""*\n\nWe will integrate this into the camera ready version of the paper, after acceptance. We can’t do it immediately, since the submitted pdfs can’t be updated anymore since November 22th. \n'}, {'title': 'Comparisons on the real-world dataset', 'comment': ""Thank you for your quick response, please find our answer to your question below :\n\n**=== REMARK**: *I sincerely appreciate the detailed explanations and the new results on the Block Tower dataset. The qualitative results look good. Then, could you please provide some comparisons with other existing work on this new dataset (at least the compared models used on FilteredCoPhy)?*\n\nThank you for acknowledging our work to follow the reviewer's advice and improve our submission. As we said in the revised appendix F, there is little work around the Block Tower dataset, and to the best of our knowledge, we are the first to target video forecasting on this task. There are actually two published papers that perform experiments on this dataset. Lerer & al (2017) introduced the dataset as a validation for their stability prediction model. Forward prediction is used as a proxy task to encourage physical reasoning. Yet, predictions are not evaluated in a quantitative manner and performed over segmentation images with simulation data. Wu et al. (2017) also performs stability prediction, but they use an explicit physics engine to predict stability from a single frame.\n\nHowever, yes, comparing with the baselines methods we proposed for the FilteredCoPhy benchmark is feasible. We train PhyDNet, UV-CDN (4 keypoints) and PredRNN on this task. We followed the same procedure as we did with our methods: we tried to finetune each model pre-trained in simulation by freezing different parts of each model. We report our results in the following table :\n\n|      |  Ours | UV-CDN | PhyDNet | PredRNN |\n|:----:|:-----:|:------:|:-------:|:-------:|\n| PSNR | 26.27 |  25.77 |  24.26  |  34.40  |\n\n\nWe optimized the methods and explored different settings and hyper-parameters. Best results were achieved by freezing the weights of the last layer of the PhyCell of PhyDNet, and the dynamics encoder of UV-CDN. For PredRNN, we freezed the last layer of the ST-LSTM. Our method significantly outperforms UV-CDN and PhyDNet, following the statement we made in simulation. We refer to the discussions in the paper on the simulation experiments, but in a nutshell, our method leverages several contributions, among which are the shape coefficients and the high-dimensional encoding of the dynamics encoder CoDy. PredRNN is again a very interesting baseline, since the PSNR appears to be very high. Nonetheless, qualitative assessment of the video confirms that this high score is mainly due to accurate background reconstruction. Cube trajectories are not accurately predicted, in contrast to our model. PredRNN consistently outputs a video of a non-moving stable tower for each 116 test examples. We provide visual examples in the github repository of our project : https://github.com/filteredcophy/FilteredCoPhy/tree/main/BlocktowerIRL\n""}, {'title': 'Additions to discussion?', 'comment': 'I am quite impressed at the number of experiments you were willing and able to do in response to reviewer questions.  I do have two additional minor questions.\n\n1. For section E.2, could you report the statistical tests used to conclude that ""there is no significant difference of quality between do-operation types""?\n\n2. Thanks for affirming my observations about the failures of the method.  I do understand that it may be difficult to perform given the amount of additional material you added to the paper in the revision, but could you include perhaps one or two sentences about these failure modes in the conclusion?  Something like ""Counterfactual prediction of video frames remains a challenging task, and Filtered CoPhy still exhibits failures in maintaining rigid structures of objects over long prediction time-scales.  We hope that our benchmark will inspire further breakthroughs in this domain.""'}, {'title': 'Waiting for the comparisons on the real-world dataset.', 'comment': 'I sincerely appreciate the detailed explanations and the new results on the Block Tower dataset. The qualitative results look good. Then, could you please provide some comparisons with other existing work on this new dataset (at least the compared models used on FilteredCoPhy)?\n\nWith the positive results, I would like to consider increasing my rating.'}, {'title': 'Answers to sqvy', 'comment': 'Thank you for your valuable comments and appreciations. Please find the answers of your questions below:\n\n**=== REMARK 1**: Have the author(s) tried/given a thought how other object-centric representations such as slot-attention would affect the counterfactual performance?\n\nYes, this is an interesting point, since keypoint detection is very close to slot-attention mechanisms. Using slot-attention is future work that we are interested in pursuing. We would like to point out that slot-attention has been proposed for static images, and we are not aware of published work which has extended it to RGB input video.\n\n**=== REMARK 2**: It would make the paper stronger if the CoPhy like baseline is reported i.e one using ground truth object positions. This would give at least an upper bound for the proposed benchmark.\n\nWe added a comparison with the CoPhy like baseline as an upper bound. However, while our keypoint detector indeed does discover points close to the center of mass of each object, as the method is not unsupervised, there is no explicit enforcement of this rule: the de-rendering module could perfectly learn a keypoint representation far from the natural choice. However, the CoPhyNet error published in the original paper is measured as Mean squared error between centers of mass, whereas CoDy error is measured as MSE between keypoints, so the results would not have been comparable.\nThus, as you suggested in another comment, MOT metrics should be particularly relevant for this task, as they evaluate how well an object is tracked, and this allows us to propose comparisons to CoPhyNet. Our results are provided in the revised appendix E.1, Table 11. We evaluate CoPhyNet, the model introduced in Baradel et al., on the FilteredCoPhy benchmark. We trained CoPhyNet by providing it with ground truth 3D positions of the objects in A, B and C, and minimizing the MSE between the output of the model and the positions in D. To evaluate CoPhyNet in the same domain as CoDy, we project the prediction on the camera plane (using the parameters of the camera), and then compute the MOT metrics between the projected predictions and the projected ground truth CoMs.\n\n**=== REMARK 3**: It would be beneficial to add a quantitative metric based on the predicted and ground truth location of the center of mass (for both Table 1 and Table 3).\n\nThis is indeed something that deserves more experiments. Figure 15 in the appendix suggests that discovered keypoints are close to the ground truth center of mass of the objects. Interestingly, and as said above, this constraint is (a) not enforced in any manner in our model and (b) absolutely not necessary for accurate image reconstruction. Following your advice, we completed Table 1 of the paper by measuring the error related to the CoM of the objects. You will find the results in the revised appendix E.1, where we compare MOTA and MOTP metrics between our method, UV-CDN and the soft upper bound CoPhyNet as mentioned above.  Using the same metrics for table 3 is not straightforward: since there are more keypoints than objects we would have to modify metrics definition to allow one-to-many association between keypoints and CoM. Thus, we decided to report measurements solely for configurations where the number of keypoints matches the number of objects.\n\n**=== REMARK 4**: Also, since the keypoint points can be tracked, it would add more credibility if tracking metrics such as MOT are added as well.\n\nWe used MOTP and MOTA metrics to answer your previous question, and reported the results in the revised appendix.\n\n**=== REMARK 5**: During the training stage, can the sequences AB be sent to the CoDy (i.e dynamics model) as well? Since uk should be the same for AB and CD, this acts as a consistency check.\n\nSending A to CoDy in order to predict B is absolutely feasible. It can be related to a data augmentation technique to improve the dynamical model in CoDy. Nonetheless, this method is not necessary for FilteredCoPhy, which contains enough experiments to represent a significant part of the data distribution. To validate this claim, we train CoDy using this trick on blocktowerCF. We did not observe a significant improvement concerning the MSE on keypoints (from 9.58e-3 without the trick of AB to 9.25e-3 with the trick). \n\n**=== REMARK 6**: I acknowledge the code released during the review period by the author(s). I have had a chance to briefly look over it and hope that the authors document the code (with a README and instructions on how to run the code) if the paper is accepted.\n\nWe are sorry for not having had the README ready before. We now updated the repository with explanations and instructions for dataset generation, training and evaluation of our methods. We also worked on the code to make it easier to use, add keypoints generation and evaluation scripts (including MOT metrics implementation) and pre-trained models.\n'}, {'title': 'Answers to tY4H', 'comment': 'Thank you for your valuable comments and appreciations. You clearly took the time to also read the quite long appendix, and we are delighted that you appreciate the extensive studies that we have conducted. Please find the answers of your questions below:\n\n**=== REMARK 1**: The paper would be even better if the diversity of the benchmark dataset could be improved.\n\nGenerally speaking, we agree that more diversity would have made our benchmark even better. That said, there are some constraints on diversity, which derive from the properties we optimized: we generated a large-scale dataset with challenging regularities and clear definitions, integrating the constraints identifiability and counterfactuality, which we described in section 3 of the paper. We believe that these constraints are quite important, but they are also difficult to enforce, and adding even more diversity to the benchmark would have complicated this task. As said in the paper, in particular collisions are difficult to learn because their actions are both intense, brief, and highly non-linear, depending on the geometry of the objects in 3D space. Adding the additional problem of counterfactuality, which requires the unsupervised estimation and exploitation of confounders, makes the task even harder.\n\nWe would also like to point out that, up to our knowledge, our dataset is the only counterfactual video dataset requiring predictions in pixel space. While CF reasoning is done in other work, it is classically restricted to classification problems. \n\n**=== REMARK 2**: Perhaps more could be said about the visible failures of the approach. The network does not seem to succeed at \nlearning the structure of rigid 3d bodies, as we see from the videos where the cubes visibly distort and lose their edges over successive frames.\n\nYou have correctly observed one of the failure cases of the method, in that objects have a slightly blurrier shape when they reach positions that are under-represented in the dataset, i.e. cubes that land very far from the original tower positions. In addition, we also observe some lack of rigidness when the prediction horizon is increased. This is particularly visible for the cylinder in the CollisionCF subset. Some animated examples are given on our project website: https://filteredcophy.github.io\n\nFinally, we also noted a bias toward lower frequencies in both the dynamical model and the derendering module. The predicted trajectories tend to be slightly smoother than the ground truth trajectories.\n'}, {'title': 'Answers to v4ms', 'comment': ""Thank you for your valuable comments and appreciations. Please find the answers of your questions below:\n\n**=== REMARK 1**: *The authors may give more empirical analyses about the impact of using different types of interventions.*\n\nWe provided additional empirical analyses in appendix E.2. We conducted two experiments: (1) we measured the overall PSNR depending on the range of the do-operation. Our model performs almost uniformly on the displacement distribution, which shows robustness to the do-operation. (2) we measure the PSNR for each do-operation type, ie. ‘Remove’ and ‘Move’. We observe a difference of 3dB in favor of the “Move” do-operation, which is unsurprising, as it is the less disturbing intervention. \n\n**=== REMARK 2**: *The proposed method is only compared with two existing approaches, including PhyDNet and a modified version of V-CDN. It would be better if the authors could include more existing approaches into the experimental comparison, for example, the keypoint-based model, the VAE-based stochastic model, and the ConvLSTM-based deterministic model.*\n\nWe added additional baselines, but we would like to first explain general troubles in identifying truly comparable baselines to assess the reliability of our method. This is due to the nature of the task: our benchmark requires to predict an entire video from a single image, and the knowledge of another video. To the best of our knowledge, VCDN is the most suited comparison baseline, but still needs access to ground truth data, (this was extensively studied and discussed in the appendix of the paper). Video generation methods appear at first glance a reasonable choice for comparison methods. Nonetheless, most of them leverage a known part of the video to forecast the rest of it. This is actually critical for our benchmark : the knowledge of a small (dynamical) part of CD may alleviate the need for (a) physical reasoning and (b) counterfactual reasoning. They can be avoided simply by pursuing the initial movement from the known part of the video, e.g. a stable block tower will be trivial to forecast knowing that the tower is stable during the first second. \n\nNonetheless, we agree that adding methods from the video generation literature will be a valuable addition to our paper. We trained the VAE-based stochastic model (SVG-LP) and the ConvLSTM-based model (PredRNN) on each task of our dataset, doing our best to adapt them to the counterfactual task at hand, for which they were not designed.\n\nDespite our effort, the stochastic baseline failed to solve the FilteredCoPhy task and provided unusable results. We explored different configurations, hyperparameters and seeds. We decided not to add SVG-LP to the baselines in the paper, as the results were too bad to be shown. This method does not seem to be suited for solving this kind of tasks. PredRNN indeed appears to be a challenging baseline. However, while demonstrating excellent capacity to reconstruct background, it appears that the method fails to accurately track the objects forward in time This is one of the strengths of our methods: the object centric representation is made explicit by the use of keypoints. We reported our experiments in table 1 (revised main paper) and provided visualizations in figures 6 (revised main paper) and 18, 19, 20 (revised appendix). \n\n**=== REMARK 3**:  *All experiments are conducted in a simulated environment based on CoPhy, which is somewhat insufficient compared with previous video prediction literature (though I understand the paper studies a new problem). Since the proposed method does not require the supervision of confounders, I wonder if it can be generalized to real-world datasets such as Human3.6M, KTH, or BAIR robot pushing.*\n\nThis is indeed an interesting study to conduct, although real datasets do not provide the leverage we have to target real counterfactual tasks with a control over identifiability, counterfactuality, etc. (see discussion in section 3). Given the short time available during the rebuttal period, we focused on one additional real-world dataset using real physical cubes (Lerer et al, ICML'16). More details are given in the revised appendix of the paper (appendix F).\n\nTo the best of our knowledge, we are the first to use this dataset for video prediction, as (Lerer et al.,2016) and (Wu et al. 2017) use it for stability prediction, while actual trajectory forecasting was not their objective.\n\nThe dataset consists of 516 videos of towers in stable or unstable configuration. The difficulty with this dataset lies in the low quantity of video available for training. Thus, we leverage the models trained on Blocktower from our own FilteredCoPhy benchmark and fine-tune the models on the videos from this dataset. We provide empirical results in Appendix F of the revised version of our paper. We observed satisfactory forecasting in pixel space, indicating that our method is transferable to other tasks in real life scenarios in a short time. \n""}, {'title': 'Common Answer', 'comment': 'We thank the reviewers for their efforts and their reviews of high quality. We are happy that they appreciated:\n- **Our valuable contribution**: “This paper studies an interesting problem of counterfactual video prediction”, “... a crucial one to get counterfactual predictions”, “Introduces a sophisticated approach that is well-described and justified”, “feel that the paper is a strong contribution towards visual counterfactual reasoning“.\n- **The attention given to the generation of the dataset**: “it improves the CoPhy benchmark”, “The new benchmark proposal considering Identifiability and Counterfactuality constraints is systematic and alleviates the issues in the CoPhy benchmark”, “Introduces and justifies an important new benchmark task”.\n- **The quality of experiments and ablations**:  “Strong empirical evaluation that includes ablation studies and exploration of the learned representations”, “The paper provides good empirical results and extensive ablation studies for the effectiveness of each model component.”, “ It uses sensible benchmarks including previous methods and common-sense baselines”\n\nWe are also happy that one of the reviewers appreciated the large quantity of supplementary information in the appendix and even found time to look at our code.\n\nWe answered each reviewer separately using the open-review discussion tool. In a nutshell, we principally addressed the concerns spotted around the comparison baselines and upper bounds. Counterfactual reasoning on video implies to rethink the way we design dataset and models, thus finding actually comparable methods is a challenging task. We also evaluated our model against real world datasets, but fair comparisons are difficult or impossible, since these tasks are not based on counterfactual physics or rigid multibody dynamics and therefore do not correspond to the objectives of our paper.\n\nWe also improved the definition of the localized L-PSNR measured introduced in section 5, making it more precise: instead of computing the PSNR on small areas near the objects, we compute a segmentation image that separates the foreground from the background. We then compute the PSNR on the pixels belonging to the foreground. This gives a better representation of the true quality of the compared methods.\n  \nHere is a summary of the changes in the paper:\nIn the main paper, we changed tables 1 and 3 and figure 6.\nWe added the following sections to the appendix:\n- Appendix E: additional evaluations and baselines on the proposed benchmark (Table 11 and Figure 16)\n- Appendix F: evaluations of the proposed method on a real world dataset (Figure 17)\n- Appendix G: We updated figures 18, 19 and 20 with visualization computed with PredRNN.\n'}, {'summary_of_the_paper': 'This paper studies an interesting problem of counterfactual video prediction, which aims to predict the future frame (D) based on the initial frame (C) and an observed video sequence (AB). AB can be seen as a demonstration that is driven by the same confounders, i.e., the physics parameters such as mass and initial velocities. \n\nThe paper follows the previous models from CoPhyNet for dynamics modeling and confounder estimation, and has two improvements over the work of CoPhy, in my point of view: \n- First, it improves the CoPhy benchmark.\n- Second, it presents a new model based on the representations of high-dimensional features, 2D keypoints, and corresponding coefficients. The new form of representation allows the model to be trained in an unsupervised manner only with the supervision of RGB images, as opposed to the training procedure with the supervision of object positions in CoPhyNet. \n\n', 'main_review': 'Strengths:\n- The paper is well-written.\n- The proposed encoding-decoding framework and the corresponding two-stage learning process are reasonable.\n- the newly designed representation (i.e, high-dimensional features + 2D keypoints + coefficients) is shown to facilitate unsupervised learning from pixel space.\n- The paper provides good empirical results and extensive ablation studies for the effectiveness of each model component.\n\nWeaknesses: \n- The authors may give more empirical analyses about the impact of using different types of interventions. \n- The proposed method is only compared with two existing approaches, including PhyDNet and a modified version of V-CDN. It would be better if the authors could include more existing approaches into the experimental comparison, for example, the keypoint-based model [Minderer et al., 2019], the VAE-based stochastic model [Ref1], and the ConvLSTM-based deterministic model [Ref2]. \n- All experiments are conducted in a simulated environment based on CoPhy, which is somewhat insufficient compared with previous video prediction literature (though I understand the paper studies a new problem). Since the proposed method does not require the supervision of confounders, I wonder if it can be generalized to real-world datasets such as Human3.6M, KTH, or BAIR robot pushing.\n\n[Ref1] Stochastic Video Generation with a Learned Prior. ICML 2018.\n\n[Ref2] PredRNN: Recurrent Neural Networks for Predictive Learning Using Spatiotemporal LSTMs. NIPS 2017.\n\n', 'summary_of_the_review': 'This paper explores an interesting problem of learning counterfactual physics from pixels. It presents a solution by extending a previous work, CoPhyNet, with new forms of unsupervised video representations. \n\nOne of my concerns is that it does not provide sufficient empirical comparisons with existing video prediction models other than PhyDNet and the modified V-CDN. Furthermore, I would increase my score if the effectiveness of the method can be validated in a real-world dataset.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': ""This work extends CoPhy, in which the author(s) address the problem of predicting counterfactual outcomes of physics-based tasks from pixel space (CoPhy used ground truth object positions). To do this, they learn a keypoint representation of the scene and use them to extract the confounders (such as velocities of objects, masses, etc). The author(s) also propose a new benchmark (built upon CoPhy's benchmark) for counterfactual prediction (after intervening the initial set of objects in the scene) which satisfies the Identifiability and the Counterfactuality constraints of causality."", 'main_review': 'Strengths:\n1) Filtered-CoPhy seems to be the next intuitive step from CoPhy from a modeling perspective. It is incremental work and a crucial one to get counterfactual predictions (for physics-based simulations) working in an unsupervised fashion.\n\n2) The new benchmark proposal considering Identifiability and Counterfactuality constraints is systematic and alleviates the issues in the CoPhy benchmark.\n\n3) I acknowledge the code released during the review period by the author(s). I have had a chance to briefly look over it and hope that the authors document the code (with a README and instructions on how to run the code) if the paper is accepted.\n\n\nWeakness:\n\n1) Keypoint based encoder/decoder seems to be a good choice. Have the author(s) tried/given a thought how other object-centric representations such as slot-attention would affect the counterfactual performance?\n\n2) It would make the paper stronger if the CoPhy like baseline is reported i.e one using ground truth object positions. This would give at least an upper bound for the proposed benchmark.\n\n3) It would be beneficial to add a quantitative metric based on the predicted and ground truth location of the center of mass (for both Table 1 and Table 3).\n\n4) Also, since the keypoint points can be tracked, it would add more credibility if tracking metrics such as MOT are added as well. Here the author(s) can assume N=number of objects in the scene (otherwise this metric cannot be computed).\n\n5) During the training stage, can the sequences AB be sent to the CoDy (i.e dynamics model) as well? Since $u_k$ should be the same for AB and CD, this acts as a consistency check.', 'summary_of_the_review': ""Overall I feel that the paper is a strong contribution towards visual counterfactual reasoning (making the framework unsupervised as opposed to CoPhy) and would strongly urge the author(s) to add the experiments mentioned in the Weaknesses section. Based on the novelty of the paper, I am voting for a weak acceptance of the paper.\n\n=======\nPost-rebuttal decision:\nAfter reading the comments of the authors as well as reviews of the other reviewers, I'm very much satisfied with the additional experiments added in the paper and I believe this paper would be a good contribution towards unsupervised visual counterfactual prediction. Hence, I've increased my score to 8 (Accept)."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""The paper introduces a testing approach, dataset, and method for counterfactual video predictions, using 3d physics simulation videos.  Importantly, the approach predicts directly from pixel space rather than requiring spoon-fed keypoints.  It employs a counterfactual approach to establish a network's capacity to learn causal relations.  Separating the problem into one of parsing the inputs into keypoints and additional coefficients, inferring object attributes, and sequential prediction from an input frame + object attributes, it proposes an architecture which is based on combining modules for each of these learning tasks (but where keypoints are learned in an unsupervised way).  Besides this key architectural innovation, it adds an inductive bias by applying directional Gaussian filters to the keypoint maps.  The paper checks that the network actually works as intended by empirically examining the effect of changing object coefficients.  Three other ablation analyses, one that combines two of the modules (rather than separating them via stop-grad) and one that removes coefficients, and one comparing the handcrafted filter bank with learnable ones, add confidence to the approach.  In the supplement, the paper also analyzes in detail how the method compares to a previous method (Transporter).  It uses sensible benchmarks including previous methods and common-sense baselines where either the original or counterfactual input is used as a prediction, and does relatively well on this challenging problem, although the video on the website shows that there is still plenty of room for improvement."", 'main_review': 'Strengths\n - Introduces and justifies an important new benchmark task for video predictions involving inanimate objects\n - Introduces a sophisticated approach that is well-described and justified\n - Strong empirical evaluation that includes ablation studies and exploration of the learned representations\n - Thorough comparison with a previous approach that sheds light on reasons for the improvement\n - The counterfactual style of evaluation may be used in other domains of AI\n\nWeaknesses\n - The paper would be even better if the diversity of the benchmark dataset could be improved.\n - Perhaps more could be said about the visible failures of the approach.  The network does not seem to succeed at learning the structure of rigid 3d bodies, as we see from the videos where the cubes visibly distort and lose their edges over successive frames.\n\nOther comments\n - Many incomplete sentences in the supplement.  I would be happy to list them if this would be beneficial to the authors; I assume this was due to time limitations.', 'summary_of_the_review': 'This paper introduces an ambitious new baseline for counterfactual 3d physics predictions in pixel space.  It introduces a sophisticated method based on separate modules for image parsing, parameter inference, and prediction, that outperforms two previous approaches V-CDN and PhyDNet.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space', 'authorids': ['~Steeven_JANNY2', '~Fabien_Baradel1', '~Natalia_Neverova1', 'madiha.nadri-wolf@univ-lyon1.fr', '~Greg_Mori2', '~Christian_Wolf1'], 'authors': ['Steeven JANNY', 'Fabien Baradel', 'Natalia Neverova', 'Madiha Nadri', 'Greg Mori', 'Christian Wolf'], 'keywords': [], 'abstract': 'Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'janny|filteredcophy_unsupervised_learning_of_counterfactual_physics_in_pixel_space', 'pdf': '/pdf/cbd75b662eaa377753b892113b221d062f26511e.pdf', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/filtered-cophy-unsupervised-learning-of/code)', '_bibtex': '@inproceedings{\njanny2022filteredcophy,\ntitle={Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space},\nauthor={Steeven JANNY and Fabien Baradel and Natalia Neverova and Madiha Nadri and Greg Mori and Christian Wolf},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=1L0C5ROtFp}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Nicolas Papernot', 'Thomas Steinke']",ICLR,Hyperparameter Tuning with Renyi Differential Privacy,https://iclr.cc/virtual/2022/oral/6747,2022," For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=-70L8lpp9DF,https://openreview.net/forum?id=-70L8lpp9DF,-70L8lpp9DF,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper tackles a problem at the intersection of AutoML and trustworthiness that has not been studied much before, and provides a first solution, leaving much space for a lot of interesting future research.\nAll reviewers agree that this is a strong paper and clearly recommend acceptance.\nI recommend acceptance as an oral since the paper opens the door for a lot of interesting follow-ups.'}, {'title': 'Response', 'comment': 'Thanks the authors for the rebuttal, which answers my questions.'}, {'title': 'Thanks for the reply', 'comment': ""Thanks for clarifying this and for adding comments about this to the paper. This modified example seems more convincing yet slightly artificial as these modified $Q$ and $Q'$ also depend on $k$. Perhaps this is a situation where one could possibly see the difference of having tight $(\\varepsilon,\\delta)$-bounds and RDP bounds. But as you write in Section 3.2: 'This negative result also extends to Rényi DP..', so you are only claiming you have this for RDP which looks correct. I think this is a really nice paper, I have raised my score by 1.""}, {'title': 'Adaptive Search Methods', 'comment': 'The reviewer asked about adaptive hyperparameter search methods. We recently became aware of an independent paper which studies this question entitled [The Role of Adaptive Optimizers for Honest Private Hyperparameter Selection](https://openreview.net/pdf/66b8c252a9cd76c64ac9e620dd9768a180ed47c9.pdf).\nThis new paper analyses adaptive search methods using composition results and compares them with the results of Liu & Talwar (2019), which are for non-adaptive search (a.k.a. ""grid search"") like our results. We have added a citation in our submission.\n\n'}, {'title': 'Response', 'comment': 'Thanks for the author response, my questions have been addressed.'}, {'title': 'Response to reviewer 4Ysq', 'comment': 'We thank the reviewer for their time and comments.\n\n>_** Could you comment on this tradeoff between lambda and eps in the bound of Proposition 17? Could you give more intuition on why the randomness in k would be crucial and/or some other example that illustrates this? **_\n\nThe reviewer has astutely observed that the negative result for our strawman approach (Proposition 17) is somewhat specific to pure $\\varepsilon$-DP or at least $(\\lambda,\\varepsilon(\\lambda))$-RDP with not-too-small values of $\\lambda$. This is because the ""bad"" event is relatively low-probability. Specifically, the high privacy loss event has probability $(1+e^{-\\varepsilon})^{-k}$. This is small, unless $\\varepsilon \\ge \\Omega( \\log k)$.\n\nWe can change the example to make the bad event happen with constant probability. However, the base algorithm will also not be pure $\\varepsilon$-DP any more. \nSpecifically, we can replace the two distributions in Proposition 17 with the following:\n$$Q=(1-\\exp(-1/k),\\exp(-1/k))$$\n$$Q’=(1-\\exp(-\\varepsilon_0-1/k),\\exp(-\\varepsilon_0-1/k))$$\nIf we repeat this base algorithm a fixed number of times $k$, then the corresponding pair of distributions is given by\n$$A=(1-\\exp(-1),\\exp(-1))$$\n$$A’=(1-\\exp(-k\\varepsilon_0-1),\\exp(-k\\varepsilon_0-1)$$\nNow we have $\\mathrm{D}_\\infty(A\\|A’)=k\\varepsilon_0$ and the bad event happens with probability $e^{-1} \\approx 0.36$.\nOn the other hand, $\\mathrm{D}_\\infty(Q\\|Q’)=\\varepsilon_0$ like before, but $\\mathrm{D}_\\infty(Q’\\|Q) = \\log(1-\\exp(-\\varepsilon_0-1/k)) - \\log(1-\\exp(-1/k)) \\approx \\log(k\\varepsilon_0+1)$.\nBut we still have a good guarantee in terms of Renyi divergences. In particular, $\\mathrm{D}_1(Q’\\|Q)\\le (\\varepsilon_0+1/k)\\log(k\\varepsilon_0+1)$, and we can set $\\varepsilon_0 \\le o(1/\\log k)$ to ensure that we get reasonable $(\\lambda,\\varepsilon(\\lambda))$-RDP guarantees for small $\\lambda$. \n\nAt a higher level, it should not be a surprise that this negative example is relatively brittle. Our positive results show that it only takes a very minor adjustment to the number of repetitions to obtain significantly tighter privacy guarantees for hyperparameter tuning than what one would obtain from naive composition. In particular, running a fixed number of times $k$ versus running $\\mathsf{Poisson}(k)$ times is not that different, but our positive results show that it already circumvents this problem in general.\n\nWe have added these comments to the appendix after Proposition 17.\n\n>_** There is something strange at the bottom of p. 22: equation going over the line. **_\n\nWe thank you for pointing this out. We fixed this typographical error in the revised manuscript.\n'}, {'title': 'Response', 'comment': ""I wouldn't say that an extra $-\\eta \\rho$ term is strictly an improvement, since $\\eta$ can be negative.\n\nI would like to thank the authors for responding; All of my concerns have been carefully addressed.""}, {'title': 'Response to reviewer SYbz (part 2 / 2)', 'comment': 'Specific Comments.\n\n* The $\\lambda=1$ case, which is the KL divergence, is defined in the appendix. Setting $\\hat\\lambda=1$ in our result is valid, but in this case the value of the divergence is multiplied by $0$, so it’s not actually necessary to quantify it. We have added a footnote.\n* We have added a proof of Corollary 4 in the appendix and double-checked this calculation. The reviewer is correct that the result can be improved with an extra $-\\eta\\rho$ term.\n* In Lemma 7 we have clarified that q and q’ are attained by applying the same arbitrary postprocessing to Q and Q’ respectively.\n* The “smoothness” of the derivative of the PGF does depend on the right-tail heaviness of the distribution, but there are other factors involved. E.g. the negative binomial distribution has a very heavy right tail, but its PGF behaves quite well in this regard. There is an interplay between the PGF and the privacy guarantee. We have slightly elaborated this intuition, but ultimately the connection to the PGF is not particularly intuitive and we must rely on the formal analysis.\n* When $p=r=\\infty$ then the application of Holder’s inequality in the proof of Proposition 15 is simple – i.e. $\\int_S Q(x)^\\lambda Q’(x)^{1-\\lambda} \\mathrm{d}x \\le (\\max_x Q(x)/Q’(x))^\\lambda \\cdot \\int_S Q’(x) \\mathrm{d}x$. The expression $r\\lambda-r/p = r(\\lambda-1/p)$ should be evaluated as $\\infty$ (the $r/p$ term is not significant in the limit). We have clarified this expression.\n* The total ordering in the example on page 23 does not necessarily correspond to the ordering of probabilities. We have added a short remark on this.\n* Page 25 / Appendix E: We apologize for this appendix not being complete. We have largely rewritten this with complete proofs and improved results.\n\nWe would like to thank the reviewer for carefully reviewing our manuscript. We have applied all the minor corrections requested in the rest of your review.\n'}, {'title': 'Response to reviewer SYbz (part 1 / 2)', 'comment': ""We thank the reviewer for their time and thorough comments. In the following, we respond inline to the reviewer’s comments. \n\n>_**Nonetheless, the authors might want to compare their method with the noise perturbation method proposed by Chaudhuri et al. (2013).**_\n\nThank you for pointing out that we neglected to mention the work of Chaudhuri et al. Although these results are not directly comparable to ours, they are related. Our results are algorithm-agnostic in the sense that we only assume that the base algorithm is differentially private. Chaudhuri et al. follow an approach that assumes a stability property of the algorithm, which allows the application of a variant of the exponential mechanism to hyperparameter selection. Whereas the approach by Chaudhuri requires that one analyze the learning algorithm itself, our algorithm-agnostic approach is more readily applicable to modern machine learning systems, as they generally do not satisfy the stability criterion required. We have added a citation and brief comparison in our revised manuscript.\n\n>_**The paper is mostly written with DP-SGD in mind, but it does not consider when the model's evaluation on a hold-out validation set is incorporated in the base algorithm**_\n\nThis is a good point, which we glossed over. The DP-SGD training procedure should only look at the training data and the evaluation should be conducted on held out data. From a privacy perspective, we can dedicate the same privacy budget ($\\varepsilon$) to the evaluation as for the training, since these are disjoint sets. That is, we would evaluate the accuracy on the held out data at the end of each training run corresponding to a hyperparameter candidate value by, for instance, adding Laplace/Gaussian noise to preserve privacy. In this case, the privacy loss from evaluation would not be added to the loss from training since these are two separate datasets. \n\nAlternatively, the base algorithm could simply evaluate on the training data (and appeal to the generalization properties of differential privacy) and the held out data could be preserved for evaluating the final output from the repeated algorithm. We would still need to add noise to the evaluation, but now we must add this privacy loss to that of the training as it is not fresh data. We have added a footnote regarding this choice in Section 3.1.\n\n>_**I see that distributions with finite support (e.g. the truncated binomial) are not considered in this study. But in practice, one might want to limit the number of hyperparameter searches (e.g. due to computational constraints)**_\n\nWe have added a result to Appendix E which covers truncating the distribution, although we did not consider this in our experiments. The message of this result is simply that the privacy guarantee would degrade gracefully if we did truncate.\nComputational efficiency is indeed an important consideration and this would speak to using the Poisson distribution, as it is highly concentrated, compared to the negative binomial distribution.\n\n>_**Could the authors comment on how to determine an appropriate size of the candidate set ($m$)? A simple heuristic is $m=\\mathbb{E}[K]$ but the authors might have something better in mind.**_\n\nThis is a good question. The choice should not be particularly brittle – i.e., choosing a slightly too large or too small search space should not dramatically alter the usefulness of the results. Our intuition is that the search space $m$ would be dictated by the application and then we would pick $K$ so that, say, $P[K \\ge m] \\ge 2/3$.""}, {'title': 'Response to reviewer mxg8', 'comment': 'We thank the reviewer for their time and feedback. In the following, we respond inline to the reviewer’s concerns and questions. If the reviewer has any further questions, we would be happy to elaborate in a further response.\n\n>_**First, for the current problem formulation, it is possible that the same parameter will be tried multiple times, which is definitely a waste of privacy. Not sure whether people will do it in practice.**_\n\nOur problem formulation follows that of Liu & Talwar. We follow this formulation both for simplicity and for consistency with the prior work. It is indeed potentially inefficient to repeat some runs. However, we remark that, since the algorithms are randomized, running them multiple times does not entail that we have the same output multiple times.\n\n>_**Second, the paper assumes the following scheme satisfies DP: randomly choose $j\\in [m]$, and run $M_j$. Note that this is not equivalent with assuming each $M_j$ is DP, where the latter is more realistic. For example, in the hyperparameter tuning of DP-SGD, it easily holds that each run (with different clipping norm) satisfies DP. However, it is not clear to me whether randomly selecting one clipping norm, and then running DP-SGD is differentially private. The authors need to justify the relationship between these two assumptions. Naively speaking, the second can not lead to the first assumption. Please correct me if I miss something.**_\n\nDifferential privacy is ""convex"" in the sense that if $M_1,M_2,\\cdots,M_m$ are each individually differentially private, then randomizing over these – i.e., running $M_J$ for a random $J \\in [m]$ is also differentially private with the same parameters. This convexity property is important for the problem setup and we added a note emphasizing this in Section 3.3.\n'}, {'title': 'Response to reviewer CBQT', 'comment': 'We would like to thank the reviewer for their time and feedback. In the following, we respond inline to the reviewer’s questions. \n\n>_**1. How would this method be extended to be used with adaptive search methods? Grid search can be too time consuming/inefficient, especially when used with DP-SGD (as DP-SGD needs per-example gradients that are expensive to obtain) and there are some reinforcement learning based methods that can yield optimal hyper parameters much faster than grid search. Can this method be modified, maybe with a higher privacy expenditure, to be used in those cases?**_\n\nThis is a good question. Our analysis does not support adaptively choosing the hyperparameter candidates based on the outcomes of prior trials. It seems likely that an arbitrary adaptive hyperparameter search would not admit any better analysis than composition. We could consider whether some particular RL-based approach could be analyzed and would appreciate it if the reviewer has any suggestions. That said, we do see certain refinements to naive hyperparameter tuning that are worth considering. One example is early stopping, which is commonly used in hyperparameter tuning to avoid wasting computational resources when the first training epochs of specific trials indicate they will not provide a competitive outcome. We thank the reviewer for highlighting this; we added a discussion of it to the conclusion of our revised manuscript.\n\n>_**2. What kind of real-life attack scenario would you envision that could actually use the best models obtained by hyper parameter tuning to extract information. Basically, what type/how much information about a model do the set of optimal hyper parameters leak?**_\n\nIntuitively, hyperparameter choices leak increasingly more information as the search space grows: in theory at least, an exponentially-large hypothesis space could leak arbitrary information about the training set. For a realistic search space, information about only a few unlucky individuals would be leaked, but this could happen in surprising ways which are hard to predict. Our intuition is that it is outliers who are most vulnerable, as they are most likely to singularly affect hyperparameter choices. However, in general differential privacy degrades gracefully, so the hyperparameter search does need to be abused for it to leak significant amounts of private information and our positive results show that, with a little bit of care, we can provably thwart privacy attacks even for large hyperparameter searches.  \n'}, {'summary_of_the_paper': 'This paper studies the problem of hyper parameter tuning in the setting of differentially private training of machine learning models. The paper first shows that the hyper parameters used to train a model and the corresponding utility can leak information about training (through experimenting with outliers). They then go on to introduce and analyze ways that would help release DP models trained with the ""best"" set of hyper parameters, with small privacy leakage. If for hyper parameter tuning, $m$ models are trained with DP, then the leakage for releasing  the best model would be $m\\epsilon$ with simple composition. The  method introduced by the paper, however, builds on prior work by Liu&Talwar and improves this to become $2\\epsilon$, through randomly choosing and running hyper parameter settings. ', 'main_review': 'The problem studied here is a really important one, since it is directly related to actual deployment of DP models in real life settings. The proposed method seems solid and the experimental results look promising. I do have a few clarification questions:\n\n1. How would this method be extended to be used with adaptive search methods? Grid search can be too time consuming/inefficient, especially when used with DP-SGD (as DP-SGD needs per-example gradients that are expensive to obtain) and there are some reinforcement learning based methods that can yield optimal hyper parameters much faster than grid search. Can this method be modified, maybe with a higher privacy expenditure, to be used in those cases?  \n \n2. What kind of real-life attack scenario would you envision that could actually use the best models obtained by hyper parameter tuning to extract information. Basically, what type/how much information about a model do the set of optimal hyper parameters leak?\n\n', 'summary_of_the_review': ""I don't see any major issues with the paper and I find the problem addressed very relevant. "", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper has made the following contributions. Firstly, this paper illustrates how simply tuning hyperparameters based on non-private training runs can leak private information. Second, this paper provides privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Their results improve and extend the work of Liu and Talwar (STOC 2019).', 'main_review': 'This paper considers an interesting and important problem: how hyperparameter tuning on private dataset can leak information, where it provides an intuitive SVM example. The paper then considers how to reduce the leakage. The problem is formalized in the following way: we pick a total number of runs $K$ from some distribution. Then, for each run  $k = 1, 2,\\ldots, K$ , we pick an index $j_k \\in [m]$ uniformly at random and run $M_{jk}$ . Then, at the end, we return the best of the $K$ outcomes. The paper proposes theoretical guarantees when $K$ comes from truncated negative binomial distribution, or Poission distribution, which strictly generalizes the previous results. Furthermore, it also proposes a new method of computing privacy leakage when $K$ comes from a general distribution, which should be of independent interest. Finally, the paper conducts empirical experiments to show the improvement if the new method.\n\nMy only concern for this paper is the problem formalization. First, for the current problem formulation, it is possible that the same parameter will be tried multiple times, which is definitely a waste of privacy. Not sure whether people will do it in practice. Second, the paper assumes the following scheme satisfies DP: randomly choose $j \\in [m]$, and run $M_j$. Note that this is not equivalent with assuming each $M_j$ is DP, where the latter is more realistic. For example, in the hyperparameter tuning of DP-SGD, it easily holds that each run (with different clipping norm) satisfies DP. However, it is not clear to me whether randomly selecting one clipping norm, and then running DP-SGD is differentially private. The authors need to justify the relationship between these two assumptions. Naively speaking, the second can not lead to the first assumption. Please correct me if I miss something.', 'summary_of_the_review': 'Generally speaking, I recommend acceptance of this paper.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper provides an considerable improvement to the DP analysis of hyperparameter tuning of DP algorithms (such as DP-SGD). The analysis is carried out using Rényi differential privacy (RDP), and the DP bounds are RDP bounds that contain the RDP parameters of the underlying mechanisms (that give the private candidates). Most importantly, the paper considerably improves the state-of-the-art of Liu and Talwar (2019). Also, a nice counterexample using SVMs is constructed, that shows the importance of this problem.\n', 'main_review': 'The paper is very well written and the analysis seems solid (though I did not check every line). The problem is important and the paper improves the state-of-the-art so its merits are clear.\n\nMy only critique:\n\nAs emhpasised in Sec. 3.2, the ‘strawman approach’ of fixing m does not work for pure eps-DP. However, I find this a bit misleading since you are not observing (eps,0)-DP of hyperparameter tuning but (eps,delta) (or RDP). So the delta might actually play a crucial role here. Allowing bit of delta in the DP bound, the randomness in choosing the number of repetition might perhaps be not that crucial. Or vice versa, in (eps,0), you would not get great gains from that randomness.\n\nI suspect that this fact that you have to have the randomness in the number of repetitions actually is a requirement of the RDP analysis that you carry out. As far as I see, in Proposition 17 you claim it is not, but I do not fully see why that would be the case. The result of Proposition 17, i.e., that the RDP of k repetitions is not RDP for less than eps’(lambda), does not really give a ‘counter-example’ in the same  way as that result for pure epsilon. For example, suppose the underlying mechanism is eps=0.5 - DP. Then, you choose lambda = 1.9483. Then, eps - (log(1+exp(-eps))/(lambda-1)) ~ 1e-4 in which case the bound does not really say anything even for quite large numbers of k. And you can of course make that bound arbitrarily small by choosing lambda appropriately. I.e., if you let delta > 0, I think there is room for tighter analysis also for fixed k.\n\nCould you comment on this tradeoff between lambda and eps in the bound of Proposition 17?\n\nCould you give more intuition on why the randomness in number of repetitions k would be crucial or some other example that illustrates this?\n\nOther:\n- There is something strange at the bottom of p. 22: equation going over the line.\n', 'summary_of_the_review': 'All in all I think this is a very nice analysis and improves the state-of-the-art. Also, this is a very important problem. With small modifications I think this ought to be accepted. My only critique: I am just not entirely convinced that randomness in the number of repetitions is crucial for having tight bounds for DP hyperparameter tuning, I hope the authors can clarify my concerns.\n\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': ""The authors make the following contributions regarding differentially private hyperparameter tuning :\n* As an example, the authors train an SVM with a weight penalty and show that, in presence of an outlier, a membership inference attack can be employed to infer from the hyperparameter whether or not the outlier was a part of the training set.\n*The authors provide an algorithm for private hyperparameter tuning, which consists of running a learning algorithm that satisfies Rényi differential privacy (RDP) for a *random* number of times ($K$), each with a hyperparameter drawn uniformly at random from a finite candidate set. The authors first prove RDP guarantees of the algorithm when $K$ is sampled from a truncated negative binomial distribution and Poisson distribution. Then they proceed to prove RDP guarantees for any distribution of $K$ supported on $\\mathbb{N}\\cup \\{0\\}$ ¹.\nThe authors propose a way to measure the utility of the algorithms by looking at the expected quantile of the output. The results of their utility analysis, coupled with an experiment on MNIST, show that the algorithm with Poisson distribution performs better than those proposed by Liu and Talwar (2019) in an intermediate range of privacy budget ($\\varepsilon$).\n\n¹ From what I understand, we can obtain a tighter generic bound by going through the proof of Lemma 7. But the authors opt to use (7) since the postprocessing often leads to a bound that is independent of $q$ and $q'$ when plugging in a specific distribution.\n"", 'main_review': ""DP-focused machine learning systems would benefit from this work, as most of them require hyperparameter tuning. The algorithms provided in this work help us avoid a large cost of composition and perform tuning at much smaller privacy loss. In addition, the generic bound allows us to be flexible on the distribution of the number of runs ($K$), though it requires some effort to translate the logarithmic term into something usable. I have checked all the proofs and there are no critical errors.\n\nThe authors have sufficiently compared their method with previous approaches. Specifically, using the main results, the authors show that their method extends and improves upon the work of Liu and Talwar (2019). The authors also compare their method with the exponential mechanism-based method (Gupta et al., 2010; Theorem 10.2) and show in Section D.4 that both methods have the same lower bounds of the utility guarantees up to constants.\n\nNonetheless, the authors might want to compare their method with the noise perturbation method proposed by Chaudhuri et al. (2013). This method requires the “stability” condition on the training procedure, which might not be tractable for neural networks. But for linear classifiers, the algorithm (Algorithm 1) only adds noises of size $O(1/n\\varepsilon)$ to the scores, which is quite attractive for training on large datasets.\n\nThe paper is mostly written with DP-SGD in mind, but it does not consider when the model's evaluation on a hold-out validation set is incorporated in the base algorithm $Q$, which is a common practice in training an ML model. From an application point of view, the authors might want to discuss a bit how the model’s evaluation (in classification or regression) can be made DP or RDP as a part of $Q$. \n\nI see that distributions with finite support (e.g. the truncated binomial) are not considered in this study. But in practice, one might want to limit the number of hyperparameter searches (e.g. due to computational constraints), so such distributions might come into play. Have the authors performed some experiments on these against the truncated negative binomial and Poisson distribution? I wonder if the privacy-utility tradeoff is better when restricting to finite support.\n\nCould the authors comment on how to determine an appropriate size of the candidate set ($m$)? A simple heuristic is $m=\\mathbb{E}[K]$ but the authors might have something better in mind.\n\n### **Specific comments**\n* Page 2: In Eq (2), the special case where $\\lambda=1$ should be mentioned here, as it is also in the range of $\\hat\\lambda$ in the main results.\n* Page 6: When I tried to derive (6) from (5), I got an extra $-\\rho\\eta$ term from rewriting\n$$\\epsilon+\\rho(\\lambda-1)+(1+\\eta)\\left(1-\\frac1{\\hat\\lambda}\\right)\\hat\\epsilon=\\rho\\lambda+\\rho(1+\\eta)(\\hat\\lambda-1)=\\rho(\\lambda-1)+\\rho\\hat\\lambda(1+\\eta)-\\rho\\eta,$$\ncombining this with the rest of the terms, and then choosing $\\lambda$ and $\\lambda'$ so that the equality holds in the following inequality:\n$$ \\left(\\rho(\\lambda-1) + \\frac{\\log\\mathbb{E}[K]}{\\lambda-1} \\right)+\\left(\\rho\\hat\\lambda(1+\\eta) + \\frac{(1+\\eta)\\log(1/\\gamma)}{\\hat\\lambda}\\right)- \\rho\\eta \\\\\n  \\geq  2\\sqrt{\\rho\\log\\mathbb{E}[K]} +2(1+\\eta)\\sqrt{\\rho\\log(1/\\gamma)}-\\rho\\eta. $$\nTo clarify this, I think the authors should provide the proof of Corollary 4 somewhere in the Appendix, as I find the bound to be non-trivial.\n* Page 7: In Lemma 7, “$q$ and $q'$ are arbitrary probabilities” is misleading. When I saw the statement for the first time, I read this as “$q$ and $q'$ can be anything in $[0,1]$”, but the proof indicates that they take specific forms in order for the Lemma to hold true. One way to resolve this issue is by directly stating the definition of $q$ and $q'$ after (7).\n* Line 1-2 in Page 8:\n> Vaguely, $f'$ being smooth corresponds to the distribution $K$ being spread out (i.e. far from being a point mass).    \n>\nThis is not true; if $X$ is a point mass at $1$ i.e. $\\Pr{[X=1]}=1$, then the PGF of $X$ is $f(x)=x$, and so $f'(x)=1$ which is smoother than any polynomial.  \nLooking at the definition of PGF, the smoothness of $f'$ should depend on the right-tail heaviness of the distribution of $K$. Specifically, a heavier right tail corresponds to a faster growth rate of $f'$, which in turn leads to a larger RHS of Eq. (7) when $q > q'$. This observation is in line with the privacy-utility tradeoff: more probabilities of sampling a large $K$ (i.e. more utility) lead to a larger privacy loss.\n* Page 18: when $p=r=\\infty$, then the value of $r/p$ is unclear. What is the convention for this case?\n* Page 23: It is mentioned below the definition of $Q,Q',A,A'$ that “the total ordering prefers the first option (corresponding to the first coordinate probability)” which should refer to the ones with the probabilities $1-b-c$ and $1-b'-c'$. In other words, we assume that $1-b-c>b$ and $1-b'-c'>b'$. However, the approximations below suggest that $b>c>1-b-c$ and $b'>c'>1-b'-c'$.\n* Page 25: The proof of Lemma 20 is missing. Does it appear in Bun & Steinke (2016)?\n\n### **Minor corrections**\n* Page 6: In Remark 5: “$(\\lambda_2,\\varepsilon)$-RDP implies $(\\lambda_2,\\varepsilon)$-RDP” $\\rightarrow$ “$(\\lambda_2,\\varepsilon)$-RDP implies $(\\lambda_1,\\varepsilon)$-RDP” and “ann” $\\rightarrow$ “any”.\n* Page 14: In the footnote, “experssion” $\\rightarrow$ “expression”\n* Page 16: In the definition of $g(y)$, $t$ should be $t^*$.\n* Page 21: In the third display equation, the equal sign should be replaced with $>$.\n* Page 23: First line in Section D.4: “hyperparamter” $\\rightarrow$ “hyperparameter”\n\n### **References**\nBun, M., & Steinke, T. (2016). Concentrated differential privacy: Simplifications, extensions, and lower bounds. TCC'16.  \nChaudhuri, K., & Vinterbo, S.A. (2013). A Stability-based Validation Procedure for Differentially Private Machine Learning. NIPS'13.  \nGupta, A., Ligett, K., McSherry, F., Roth, A., & Talwar, K. (2010). Differentially private combinatorial optimization. SODA '10.  \nLiu, J., & Talwar, K. (2019). Private selection from private candidates. STOC'19.\n"", 'summary_of_the_review': 'This work provides careful privacy and utility analysis of private algorithms for hyperparameter tuning. All of the analyses and the proofs are sound, and the experiments give good comparisons between various distributions.  To make the methods widely applicable, the authors might want to comment on how the model’s evaluation on a hold-out validation set can be integrated into the DP workflow. Overall, this is a strong paper and I recommend it for publication.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'title': 'Hyperparameter Tuning with Renyi Differential Privacy', 'authorids': ['~Nicolas_Papernot1', '~Thomas_Steinke2'], 'authors': ['Nicolas Papernot', 'Thomas Steinke'], 'keywords': ['differential privacy', 'hyperparameter tuning'], 'abstract': 'For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm’s hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private.', 'one-sentence_summary': 'We provide privacy guarantees for hyperparameter search procedures, showing that tuning hyperparameters leaks private information, but that, under certain assumptions, this leakage is modest.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'papernot|hyperparameter_tuning_with_renyi_differential_privacy', 'pdf': '/pdf/8832d0e112b9fd6c5c8f0be8a093625e4de6e337.pdf', '_bibtex': '@inproceedings{\npapernot2022hyperparameter,\ntitle={Hyperparameter Tuning with Renyi Differential Privacy},\nauthor={Nicolas Papernot and Thomas Steinke},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=-70L8lpp9DF}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Sebastian Flennerhag', 'Yannick Schroecker', 'Tom Zahavy', 'Hado van Hasselt', 'David Silver', 'Satinder Singh']",ICLR,Bootstrapped Meta-Learning,https://iclr.cc/virtual/2022/oral/6253,2022," Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=b-ny3x071E5,https://openreview.net/forum?id=b-ny3x071E5,b-ny3x071E5,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper addresses a meta-learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo-metric. The algorithm first bootstraps a target from the meta- learner, then optimizes the meta-learner by minimizing the distance to that target under a chosen pseudo-metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta-optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L=1$, the method is attractive for meta-learning community. All reviewers agree to champion this paper. Congratulations on a nice work.'}, {'title': 'Replies to follow-up questions', 'comment': ""Thank you for the raised score and the kind words, we are glad that we could inspire you! Please see below for replies to your follow-up questions:\n\n- *NFL:* recall that BMG is a strict generalization of MG (Eq. 3). Hence, the comparison is less between two distinct methods, but rather between a specific known instance (MG) and the more general class of methods it belongs to. Thus the question you are asking is whether there are problems where the MG instance is best of the class. In principle, there should be problems where this is the case, although we cannot say directly what such a problem might look like. For MG to be the best solution, it must be the case that L2 is the best matching function; a possible reason for this could be noisy or non-stationary data, so that metrics that involve data sampling (like the KL-divergence) become biased or suffer from high variance. Simultaneously, it must be the case that the best target bootstrap is one gradient step on the objective. It is not clear when this would be the case. For L=1 to be best, the problem should suffer from a highly non-linear loss landscape; however, if it does, a gradient descent step is unlikely to be the best update (any form of curvature correction should be an improvement; c.f. Cor. 1). \n\n\n- *Lookahead Optimizer*. Thank you for mentioning this paper. The basic notion of producing a forward-looking target (in this case, under a different update rule) is indeed the same and is in turn inspired by previous works from few-shot learning. The update to the slow weights, i.e. moving in the direction of the target in parameter space, is equivalent to using an L2 matching function. Whereas our work focuses on meta-learning an update rule, giving rise to meta-gradients, this work focuses on a setting where slow and fast weights are different parameters for the learner that are updated differently. To see how BMG relates to this apprach, consider how it can be reduced to the lookahead optimizer: first, suppose that the meta-parameters $w$ in BMG are used to 'initialize $x$' at each step. For the BMG update, set $K$ (the number of updates to backpropagate through) to $0$. Target matching under the squared Euclidean norm then gives $\\nabla_w \\mu(\\tilde{x}, x^{(K)}) = \\nabla_w \\mu(\\tilde{x}, x) = \\nabla_w \\mu(\\tilde{x}, w) = \\nabla_w  1/2|| \\tilde{x} - w ||_2^2 = -(\\tilde{x} - w)$, and so the BMG update (Eq. 2) becomes $\\tilde{w} = w + \\beta (\\tilde{x} - w)$, which is equivalent to the lookahead optimizer if $\\tilde{x}$ is $k$ steps under a fixed algorithm.""}, {'title': 'Yet another question.', 'comment': 'What is the relationship between BMG and the Lookahead optimizer [A]?\n\nReferences\n- [A] Zhang et al., Lookahead Optimizer: $k$ steps forward, 1 step back, NeurIPS 2019, https://arxiv.org/pdf/1907.08610.pdf.'}, {'title': 'Another question.', 'comment': 'I am convinced that BMG will be better than MG in most tasks we care about and am eager to try it in my own work. However, the no free lunch theorem tells us that there must be some task in which BMG will do worse than MG. What could such a task look like? Would all examples of such a task be pathological? I realize this is a difficult question.'}, {'title': 'Thank you.', 'comment': 'I will champion this paper. I wish I had written it myself.'}, {'title': 'No secret sauce', 'comment': ""To the best of our knowledge, there is no extra secret sauce that drive this performance gain. The two agents are implemented in the exact same code base - the difference between them is *only* a change of the meta-objective, as detailed in the paper. There are no tricks involved in the implementation of the BMG meta-objective, it is implemented *exactly* as described in appendix C. \n\nIt's a good review - we agree with all points that you brought up!""}, {'title': 'Thank you for your response.', 'comment': '- This is fine. Would you mind sparing a sentence or two in the main text to inform the reader that you do not experiment with such models, despite such models technically falling under your purview? I think this would strengthen the work.\n- Great!\n- Great!\n\nA further question that is not meant to be antagonistic, rather for my own understanding: is there anything, anything at all, that could explain the empirical difference between STACX* and BMG that is not captured within the text? Is there any ""secret sauce"" that has escaped mention?\n\nAlso, is there anything in my original review that is inaccurate, even slightly? You need not worry about offending me. This is a selfish request for some feedback on my reviewing.\n'}, {'title': 'Thank you for your review', 'comment': '\nDear reviewer, \n\nThank you for a thorough and in-depth review that will help us strengthen the manuscript. We will take your suggestions into account as we prepare the final version of the manuscript; please see below for answers to your questions.\n\n- *Main text vs appendix*. Thank you for bringing this to our attention; we will do a careful review of the paper as we prepare the final version and see if we can add more experimental details in the main paper, as well as adding algorithm boxes for each of the experimental setup.\n\n- *Few-shot experiment*. “Hot expert” vs “Cold expert”. The intuition follows from the distillation argument [1]: that is, by raising the temperature of the target distribution, the resulting “[...] soft targets have high entropy, they provide much more information per training case than hard targets [from a cold expert] and much less variance in the gradient between training cases.”\n\n- *Formal definition of training procedure*. Thank you for your suggestion, we will provide a more detailed description of the training procedure in the final version of the paper.\n\n- *Theoretical results*. Thank you for sharing your thoughts. We have strengthened Thm 1 to apply to a wider class of target bootstraps. To say something more precise than what we have in the paper, further assumptions on the objective, update, and matching function are needed; we hope that the analysis we presented can serve as inspiration for further theoretical research into more specific problem settings.\n\n [1] Hinton, Vinyals & Dean: Distilling the Knowledge in a Neural Network. 2015. '}, {'title': 'Thank you for your review', 'comment': 'Dear reviewer, \n\nThank you for an insightful review and feedback, we will take this with us as we revise the manuscript. Please see below for answers to your questions.\n\n- *Learned sequence model update rules*. We are sympathetic to the reviewer’s comment; yet there is only so much we can present in a given space constraint. For the simpler experiments, we opted for fixed update rules so that we can visualize their behavior; for larger experiments, SOTA baselines use fixed update rules. We do agree with the reviewer that this is a great avenue for further research.\n\n- *Theoretical results*. That is entirely fair. Even so, we do believe that it is useful to understand on a high level – even if in a simplified setting – what the method does. To make stronger theoretical claims, further structure is required.  We do hope our analysis can serve as inspiration for future theoretical research in more specific settings.\n\n- *Minor comments*. Thank you for spotting this - you are indeed correct on both accounts.'}, {'title': 'Thank you for your review', 'comment': 'Dear reviewer, \n\nThank you for a thorough review and questions that will help us improve the manuscript. Please see below for answers to your questions.\n\n- *Choosing target / distance*. Indeed, in this paper the practitioner must choose the bootstrap and metric. Our goal was to present a strategy that is simple, generally applicable, and that has few hyper-parameters to tune. Thus, the strategy we examine unrolls the meta-learned update rule for a further number of steps $(L-1)$, followed by a gradient step on the objective to ensure the target is grounded. The intuition for this bootstrap comes from acceleration - the target is a future point on the trajectory of the learned update rule. The final gradient step pushes the trajectory in a descent direction. For the matching function, we similarly opted for popular (pseudo-)metrics, L2 and KL, and found that they worked well. With that said, it should be possible to design better bootstraps / metrics in specific settings by exploiting some known structure (e.g. convexity). We believe that deriving principled target bootstraps for specific problem settings is an exciting area for future research.\n\n- *Extending analysis*. As mentioned above, this is an exciting avenue for future research. To provide a more insightful result, we have revamped Thm 1 to more clearly demonstrate the inherent trade-offs in choosing the target bootstrap. To say something sharper about the bootstrap, further structure must be imposed on the problem. Different structures will yield different solutions. For instance, if the objective is assumed to be convex, we can replace the Taylor Series approximations with global inequalities, which makes the analysis amenable to longer bootstraps. Alternatively, if some structure is imposed on the meta-learned update rule, it would be possible to analyze the L>1 setting even if the objective is non-convex.\n\n- *Other comparisons*. As the reviewer noted in their summary, beyond image classification, we report a new state-of-the-art result on a popular meta-learning benchmark, namely Atari (where we double performance relative to the STACX baseline). We have not studied other benchmarks than those reported in the paper. As the purpose of this paper is to provide an improved method for gradient-based meta-learning, we have focused on comparisons to gradient-based meta-learning methods to ensure that our proposed method does indeed offer an improvement among this class of algorithms.'}, {'title': 'Thank you for your review', 'comment': 'Dear reviewer, \n\nThank you for a thoughtful review and excellent questions. We will take your feedback onboard as we prepare a final version of the paper. Please see answers to your questions below.\n\n- *Specific targets and L=1*. We sympathize with the sentiment and have strengthened Thm 1 to be more general (it merges Thm 1. and Cor. 2). In particular, it shows the inherent trade-off between targets that are “safe” and targets that carry high learning signal. In general, without stronger assumptions on the objective, update, and matching function, it is difficult to say something stronger. With that said, we believe that Thm 1 can serve as a useful starting point for more specialized results that develop stronger theoretical insights for specific problem settings.\n\n- *Connection to TD-learning*. The inspiration from TD-learning in our method is the general idea of using future “predictions” as targets. The reviewer raises a good point: in TD-learning, the grounding happens at the beginning of the bootstrap, whereas we ground the target at the end of the bootstrap. The motivation for our approach is that it produces a target on the parameter trajectory of the update rule, thus embodying acceleration (reach future points faster). The final gradient step pushes the target off the trajectory in a descent direction, and as such is guaranteed to be a slightly better target. In contrast, taking a gradient step first and then applying $(L-1)$ bootstrap steps does not guarantee that the target is better than following the update rule for $L-1$ steps. We ran an ablation on Atari with the reviewers suggested TB, but found that it performs worse overall; with L=4, it achieves a median HNS of ~4.8, compared to  ~6.1 for our report result for L=4.\n\n- *Non-linearity and target robustness*. While we generally agree with this point, please note that a short bootstrap (L=1) does not suffer more than the standard meta-gradient (e.g. Eq. 3). Hence this boils down to how “far” it is safe to bootstrap. We observe an inverted U-shape relationship between performance and the length of the bootstrap, depicted in Fig. 2, where increasing the horizon is beneficial up to a point, after which bootstrapping further leads to performance degradation. '}, {'summary_of_the_paper': 'The paper proposes *Bootstrapped Meta-Learning,* a new meta-learning algorithm for hyperparameter optimization. Drawing inspiration from temporal difference learning techniques in reinforcement learning, the meta-learner is asked to predict the result of additional unrolled steps of the optimization process, by minimizing a carefully selected distance to a target generated during training. This allows for longer meta-learning optimization horizons, without the need for differentiation through longer optimization trajectories. The method is tested for hyperparameter optimization for reinforcement learning, including learning the exploration hyperparameter for a behaviour policy, and in multi-task meta-learning.', 'main_review': '**Originality:** The method proposed in the paper is, to the best of my knowledge, novel. The related work section adequately connects the algorithm with existing work in similar directions. \n\n**Significance:** The empirical results show considerable improvement w.r.t. well-performing baselines; moreover, the general idea behind the method could inspire future research.\n\n**Rigour:** Both the theoretical results and the experimental protocols seem sound and solid to me.\n\n**Strengths**\n\n- The method is based on a conceptually compelling and inspiring idea.\n- The empirical results are remarkable. The ablations and additional experiments (also from the Appendix) help in understanding what matters in the practical algorithm, as well as highlighting the important parts of the contribution.\n- In particular, being able to meta-learn hyperparameters for a behaviour policy can open up new avenues for exploration in reinforcement learning.\n- The theoretical results have a clear scope (although not so large) and provide some easy to understand local improvement guarantees.\n- The paper is well-written and generally easy to follow.\n\n**Minor Concerns / Questions**\n\n- I believe the name *matching function* makes the presentation of the method a little bit harder to digest. Since the function is a pseudometric (i.e., the larger it is, the larger the distance from the target), it should really be called with a name that reminds the reader of this nature (e.g., *mismatch function*).\n- I enjoyed the theoretical results, but it is a pity that they only deal with targets of specific forms and, especially, with $L=1$ only. Ideally, theoretical result with a dependency on $L$ would shed some light on the benefits and limitations of longer bootstrapping horizons.\n- Can the authors elaborate on the connection between the way the bootstrapping target is formed in their method and traditional temporal difference learning? In particular, the grounding role of that subtracted gradient ""nudging the trajectory in a descent direction"" is the same as the one of the reward in temporal difference learning; but, while the reward is at the beginning of the trajectory, the grounding is here at the end of the optimization subtrajectory. Is there any mathematical connection beyond the general shared motivation?\n- As briefly touched upon in some passages of the paper, when the underlying function is highly nonlinear, there is the risk that the bootstrapping mechanism can lead the optimization process in worse areas of the landscape. For instance, if the function in Figure 1 had a bump/plateaux where $\\tilde w$ is, the bootstrapping mechanism would cause more troubles than standard meta-gradients. Why is this not happening in practice?\n\n-------\n*After rebuttal*: I am happy with the answer the authors provided and the update to the paper, which will help the readers understand the relationship between the proposed method and TD-learning. Overall, the improvements make me believe that this paper should be highlighted at the conference, to give other researchers working in the field the possibility to get inspired by this new idea. I am thus raising my score to 10.', 'summary_of_the_review': 'The algorithmic and empirical contributions of the paper, as well as the theoretical grounding, largely justify in my opinion its acceptance at the conference.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""\u200c\u200cThe paper presents a new meta-learning algorithm to address two shortcomings of standard meta-optimization algorithms: curvature (the meta-learner's objective is typically constrained to the same type of geometry as the learner), and limited evaluation (the meta-objective is evaluated only with-in a K-step horizon, ignoring future learning dynamics). The proposed algorithm addresses these two issues by minimizing the distance to a bootstrapped target under a chosen metric. Empirically, the new algorithm  achieved a new state-of-the art for model-free agents on the Atari ALE benchmark and yielded gains in multi-task meta-learning. Theoretically, some guarantees on performance improvements are provided.\n"", 'main_review': '**\u200cOriginality**\n\nThe paper is original and novel, proposing a new algorithm to overcome two shortcomings of standard meta-optimization algorithms: curvature mismatch and limited evaluation.\n\n**Quality**\n\nThe paper is technically sound, and claims are backed by solid experiments in the reinforcement learning and multi-task meta-learning evaluation setting.\n\n**Clarity**\n\nThe paper is clear, however, the algorithm description in section 3 is very abstract. I think the paper would benefit from a running example and a dedicated section and pseudo-code describing the algorithm and how it can be instantiated in different experimental settings.\n\n**Significance**\n\nThe work is significant and will benefit the reinforcement and meta-learning community by addressing some of the limitation of the current meta-learning algorithms.\n\n**Limitations**\n\n- The theoretical analysis is limited to noiseless 1-step target updates.\n- The experimental evaluation in the multi-task meta-learning setting is limited to only compare with MAML on computer vision applications. \n\n**Questions to Authors**\n\n- Some engineering / handcrafting is still required by the machine learning practitioner to select what ""target"" the meta-learner is going to optimize, as well as the proper ""metric"" for the meta-learner to optimize for. Could the authors comment a bit about what heuristics they used when making these decisions? and whether the automation for this process is possible or not? \n- What would it take to extend the analysis beyond 1-step noiseless target updates?\n- How does the performance of BMG compare to alternative meta-learning algorithms like R2D2, Meta-OPT-net and prototypical networks? Have the authors experimented with other meta-learning benchmarks beyond image classification?\n\n**Minor Typos**\n\n- Abstract: ""show that metric"" -> ""show that a metric""\n', 'summary_of_the_review': '\u200c\u200c\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper broadly considers meta-learning, a.k.a. bilevel optimization, across single-task, multi-task, supervised learning, and reinforcement learning settings. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters (assuming a differentiable inner-loop): first, since the meta-learning objective is typically computed from learner parameters after applying up to $K$ inner-loop updates, the meta-optimization is myopic in that it does not optimize for further inner-loop improvement after $K$ steps; second, since the functional form of the learner\'s objective $f$ is used to drive the outer-loop updates, the meta-learning objective inherits the curvature of $f$. The main algorithmic contribution consists of a family of meta-learning objectives called bootstrapped meta-learning, in which meta-parameters are optimized to bring post-inner-loop learner parameters $x^{(K)}$ closer to a bootstrap target (which are also learner parameters) computed from $x^{(K)}$. The authors show that bootstrapped meta-learning generalizes the ""direct"" (my terminology) gradient-based meta-parameter optimization used in many previous works, recovering it when using specific choices for the bootstrap computation function and learner parameter matching function. With certain strong assumptions, the authors theoretically motivate the use of gradient-based bootstrap target functions for bootstrapped meta-learning in terms of optimization progress. The authors make several experimental contributions: they use bootstrapped meta-learning to achieve state-of-the-art model-free performance on Atari-57, demonstrate the viability of bootstrapped meta-learning in few-shot image classification on miniImageNet, and show that the more flexible, general form of bootstrapped meta-learning can enable meta-learning parameters that do not appear in the computation graph for the task objective, e.g. meta-learning the exploration rate of the behavior policy in $\\epsilon$-greedy $Q$-learning.', 'main_review': 'Strengths\n- The proposed idea is simple.\n- The writing is clear.\n- To my knowledge, the proposed idea is novel yet concretely linked to many past works by virtue of generalizing them. The authors give the precise form to recover MG from BMG.\n- The proposed idea results in strong improvement of the STACX agent in Atari-57, resulting in a state-of-the-art result (caveat: for model-free agents; the gap to model-based agents remains large).\n- The authors demonstrate that the proposed idea is suitable in few-shot image classification, a popular application of meta-learning for few-shot learning.\n- The authors run informative ablation studies that support the intuition behind the benefit of BMG: resolving curvature and mitigating myopia.\n\nWeaknesses\n- Given that you say BMG is compatible with any update function (so long as it is differentiable in the meta-parameters), it would be nice to have some experiments on learned sequence model update rules (e.g. RNN). All current experiments use update rules with a fixed functional form.\n- I am not putting much weight on section 4 (""Performance Guarantees"") given the gap between its assumptions and results vs. what is actually implemented, and the restriction to local optimization.\n\nMinor comments\n- p. 3, target bootstrap paragraph, 3rd line: are we missing a learning rate for the expression of the target?\n- p. 5, the actor-critic RL objective and Eq. 4: as-is, we are always minimizing policy entropy; is there a sign error?', 'summary_of_the_review': 'I believe that the meta-learning community will find this paper interesting. It provides new insights into formulating meta-learning models and gives examples of such insights being applied to obtain empirical gains. The paper is well-written and features exemplary empirical execution.\n\nPOST-REBUTTAL: I have updated my score to 10 and confidence to 5. I think this paper should get an oral, if not best paper. It is the best in my batch and is arguably the best I have read all year. It may be the best paper I have ever peer-reviewed.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The paper presents Bootstrapped meta-gradients (BMG), an extension of typical meta-gradients (MG) for the task tuning meta-parameters that control the learning process (i.e. update step) of a learner.  In general terms, MG applies a (meta-)parameterised update rule to a learner for K steps, and then backpropagates through these updates to update the meta-parameters in the direction that improves the performance of the adapted learner.  The authors identify two limitations to this approach, and propose BMG to address them. (i) MG is myopic, in the sense that it does not account for future learning dynamics beyond these K steps, therefore BMG proposes to bootstrap a target from the K-step parameters (in practice by continuing to optimise w.r.t. parameterised update rule for L-1 steps, and then taking a final step w.r.t. a fixed objective to ground the signal). (ii) MG updates are necessarily restricted to be within the geometry of the parameterised learning process.  In contrast, BMG introduces a matching function to measure the distance between the learners K-step parameters and bootstrapped target in an arbitrary (and hopefully more-suitable space).\n\nAfter framing the problem and BMG, the authors provide a discussion of the necessary conditions for BMG to guarantee performance improvements, though ultimately the presented algorithm is justified empirically using experiments on (i) a toy RL problem, (ii) the Atari RL test suite, (iii) multi-task few-shot adaptation on an image recognition task.  In all settings, BMG provides significant improvement over meta-learning baselines — most impressively achieving a new SOTA on Atari.  Moreover, key features of BMG are highlighted, including the ability to extend the meta-learning horizon without increasing the number of updates steps through which we must backpropagate, and that behavioural parameters outside of the update rule (specifically, epsilon in epsilon-greedy exploration) can also be meta-learned.', 'main_review': '**Strengths**\n\n-  The paper is generally clear and well written.  It is content dense in and certain areas would benefit from more detailed discussions.  However, the appendices are very detailed and provide answers to almost all of the questions that arise during first reading.  In my suggestions below I’ve highlighted a few areas I feel this content would be especially beneficial (and one pare where the main text could be stripped-back as there is always a trade-off with fixed page limits).\n\n- The key ideas of BMG, bootstrapping a target to combat myopic MG updates and using matching functions to improve the meta-learning dynamics, are novel and well motivated.  I strongly believe that these ideas will be of interest to the broader meta-learning community and will likely lead to multiple future research directions.\n\n- The empirical results are strong and thorough.  The toy-model grid world in Sec 5.1 effectively highlights key properties of the proposed algorithm — such as the ability of BMG to exploit longer meta-learning horizons — and extensive testing on Atari environments (Sec 5.2) shows a significant improvement of STACX and leaves little doubt of the efficacy of BMG for self-tuning algorithms.   Moreover, the demonstrated performance improvement of BMG over MAML (sec 6) suggests broad possible applications and further endorses the proposed algorithm.\n\n- BMG also has a couple of nice properties besides raw performance: (i) achieving less myopic updates without having to backpropagate through many update steps of the inner-loop parameters and (ii) allowing for the meta-learning of “behavioural parameters” that are not used in the learning rule (exploration epsilon is used as the example).  Whilst (i) is clearly desirable in the pursuit of computational efficiency, (ii) is very intriguing and opens the door to new applications of meta-learning (indeed, whilst it is mentioned in the abstract and summaries, I believe this point is slightly undersold in the main text and that this experiment could be presented in more detail than the single paragraph at the end of Sec 5.1 it is given — however, given the page limit I can understand the authors predicament).\n\n**Weakenesses**\n\n- My primary concern is not with the content of the paper, but that the it can be quite difficult to intuitively link the numerous experiments back to intuition or interpretation of the results.  I believe this was largely because the exact methodologies are difficult to follow and, indeed, I found it was essential to refer to the appendices repeatedly to fully unpack the experiments and appreciate the results.  Whilst I am sympathetic to space constraints, I believe the authors would be well served to provide more detailed descriptions in the main text or, ideally, an algorithm box.\n\n- I find the implementation and implications of the experiment on multi-task few-shot learning (Sec 6) unclear in the following regards:\n\t- I do not understand the intuition of why a “hot” expert transforms more information than a “cold” expert, and, moreover, why BMG is able to use this to improve performance.  Could the authors clarify these points.\n\t- I feel this section in particular suffers from a lack of formal introduction of the task and methodology.  For example, the adaptation seems to be defined as a single step, whereas the appendix (D.2) notes that 10 are used during meta-testing.  Concretely, I think a formal description of the training procedure for BMG would be appropriate in the main text.\n\n- I do not find the analysis presented in Sec 4 (“Performance Guarantees”) especially insightful.  My reading is that, whilst the MG update presented in Lemma 1 can guarantee local improvement, practical implementations of BMG do not.  Empirically, however, grounding the bootstrapped target with a single final step on the meta-objective is sufficient for good performance.  This conclusion is evident from the experiments themselves, and so I would have no issue if Sec 4 was in the appendix.\n\n**Errata**\n\n- Sec 3, paragraph starting “To tackle myopia…”: the inline equation for $\\tilde{x}$ is missing brackets around the step counters for $x^{(K+L-1)}$.\n- Sec 5.2, second last paragraph: typo - “K is more sensitive curvature and the quality of data”.\n- Sec 6, sub-section BMG: typo - “raising the temperature in the expect allows”.\n- Sec 6, sub-section Setup: typo - “For 50 meta-updates and beyond..”: this should be 50k meta-updates I believe.', 'summary_of_the_review': 'Overall I believe this paper proposed an insightful and novel approach to addressing the stated limitations of typical MG approaches.  The authors do a good job of motivating the key innovations proposed and, given the significant research interest in MG’s in the past few years, it is reasonable to assume that the methodologies presented will be of broad interest.  Moreover, the experimental results are impressive and clearly demonstrate the improved performance of BMG and analyse where this comes from.  I do believe that the experimental results would be better served with more detailed descriptions of the problem settings and methodologies, however given the overall level of detail presented in the appendices I do not doubt there validity or significance.  Even so, for the stated reasons of novelty, interest and potential impact, I believe the paper is suitable for acceptance in the current form.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Bootstrapped Meta-Learning', 'authorids': ['~Sebastian_Flennerhag1', '~Yannick_Schroecker1', '~Tom_Zahavy2', '~Hado_van_Hasselt1', '~David_Silver1', '~Satinder_Singh2'], 'authors': ['Sebastian Flennerhag', 'Yannick Schroecker', 'Tom Zahavy', 'Hado van Hasselt', 'David Silver', 'Satinder Singh'], 'keywords': ['meta-learning', 'meta-gradients', 'meta-reinforcement learning'], 'abstract': 'Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule.', 'one-sentence_summary': 'We propose an algorithm for meta-learning with gradients that bootstraps the meta-learner from itself or another update rule. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'flennerhag|bootstrapped_metalearning', 'pdf': '/pdf/0eccd48eddcbf9cfc77b50cb0e97fb58937aee70.pdf', 'data': '', '_bibtex': '@inproceedings{\nflennerhag2022bootstrapped,\ntitle={Bootstrapped Meta-Learning},\nauthor={Sebastian Flennerhag and Yannick Schroecker and Tom Zahavy and Hado van Hasselt and David Silver and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=b-ny3x071E5}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Ananya Kumar', 'Aditi Raghunathan', 'Robbie Jones', 'Tengyu Ma', 'Percy Liang']",ICLR,Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution,https://iclr.cc/virtual/2022/oral/5946,2022," When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the ""head""). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR $\to$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=UYneFzXSJWh,https://openreview.net/forum?id=UYneFzXSJWh,UYneFzXSJWh,"[{'title': 'x-axis is the span of the in-distribution (training data)', 'comment': 'Thanks for the question! For this simplified figure, we assume that the in-distribution data is entirely along the x-axis. For example, in-distribution examples may be (1, 0) or (2, 0) or (-1, 0). But they must have 0 in the y-component, so they cannot be like (1,1).\n\nFormally, at the top of page 4 we define the training data, which is a matrix $X \\in \\mathbb{R}^{n \\times d}$. In this example, $d = 2$ so the matrix has $n$ rows and $2$ columns. Each row of $X$ is a 2-dimensional data point, and we assume the second coordinate is 0. So the second column of $X$ is entirely 0. Does this help?'}, {'title': 'What is the x-axis of Figure 2?', 'comment': 'With all respect, what does the x-axis of Figure 2 resemble? An axis for data - how? Thank you.'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper provides a solid and thorough analysis to the two basic methods of fine-tuning, linear probing (LP) and fine-tuning (FT). The authors provide an important and highly interesting observation about the performance of both in and out of domain (OOD) setting. They validate the known phenomena that FT outperforms LP in the in-domain (ID) setting, but demonstrate that when tested on OOD data, LP is in fact more performant and back this observation with a theoretical and empirical analysis.\nThe remedy provided is also a known, yet slightly less popular technique of setting the final layer (LP) first, then finte-tuning (FT-LP). The authors provide thorough experiments showing that this technique enjoys the best of both worlds, meaning ID and OOD. I found it worth noting that during the rebuttal period the authors provided experiments on additional larger scale datasets and models and the results of the paper carried over to these new setting.\nThe reviews agree that the analysis provided is both interesting and novel. Even though the paper does not provide a new technique, there is a consensus that the understanding it gives on known techniques is a welcome addition to ICLR.'}, {'title': 'Larger scale datasets and models, LP-FT', 'comment': ""As mentioned in the comment to all reviewers, the LP-FT runs on ImageNet-Renditions, ImageNet-sketch, ImageNet-A, and ImageNet-V2, with a larger scale CLIP vision transformer model, have finished running. **LP-FT did the best on all four OOD datasets and tied best on ID**. Please let us know if there's anything else we can do to further improve our paper. Thank you for your useful suggestions, and for engaging with us to improve our work.""}, {'title': 'LP-FT Runs Finished on ImageNet datasets', 'comment': 'The LP-FT runs on ImageNet-Renditions, ImageNet-sketch, ImageNet-A, and ImageNet-V2 have finished running. **LP-FT did the best on all four OOD datasets and tied best with fine-tuning on ID**---as predicted by the theory it gets the strong OOD accuracy of linear probing and the strong ID accuracy of fine-tuning. We apologize for the delay because it took some time for these larger scale jobs to finish running given our compute resources.\n\nFor the OOD datasets (ImageNetV2 is a replication of ImageNet so a small shift, so the results are similar to CIFAR-10 -> CIFAR-10.1 in our original paper):\n\n| OOD            | ImageNetV2 (Small shift) | Renditions | Sketch | ImageNet-A |Average      |\n|----------------|--------------------------|------------|--------|------------|------|\n| Fine-tuning    |**71.5**                  | 52.4       | 40.5   | 27.8       |        48.1 |\n| Linear probing | 69.7                     | 70.6       | 46.4   | 45.7       |        58.1 |\n| LP-FT          |**71.6**                  |**72.9**    |**48.4**|**49.1**    |       *60.5*|\n\nFor the ImageNet-validation set (ID):\n\n| ID             | ImageNet |\n|----------------|----------|\n| Fine-tuning    |**81.7**  |\n| Linear probing |  79.7    |\n| LP-FT          |**81.7**  |\n\nWe are especially excited that our **theoretical insights carry over to larger scale datasets and models that we did not consider when writing the paper, and brings us to a total of 10 OOD datasets**. Thank you to all the reviewers for their time and thoughtful suggestions.\n\nDetails:\n- We used a CLIP-ViT-B/16, vision transformer, around 4 times larger than a ResNet-50. This is the largest publicly available CLIP model.\n- LP-FT, LP, and FT used exactly the same amount of compute. For fine-tuning we fine-tuned for 10 epochs, for LP-FT we did 5 epochs of LP and then 5 epochs of FT. For linear probing we ran 10 epochs of linear probing.\n- We swept over three learning rates for each method, and early stopped and picked the best model based on ID (ImageNet Val) accuracy. The trends are the same if one uses OOD val to early stop and model select.\n'}, {'title': 'Overall Response', 'comment': 'We thank the reviewers for their time and thoughtful feedback. The reviewers agree that our work is ""very interesting and will be an important contribution"" (RVgHa), would ""be of interest to many in the community"" and ""recommend its acceptance"" (R5UET), ""interesting and easy to read"" (RVT2b), and appreciate that our paper ""discovers an interesting behavior of model fine-tuning"" and has ""extensive and detailed toy and benchmark experiments"" (RGpdo).\n\n(i) Main contribution: the bulk of the research contribution of this work is to provide insight and theoretical analyses/tools to study fine-tuning. Based on our analysis, one can draw several interesting implications (one being LP-FT which we focus on experimentally).\n\n(ii) Novelty: While we do not propose any new heuristics, our experimental contributions are novel with respect to robustness. We find a surprising result that linear probing often does better than fine-tuning OOD, and LP-FT improves both in-distribution and OOD accuracy. These are existing heuristics, but whose robustness gains haven\'t been investigated. We also provide insights into why different heuristics behave differently and how to design better fine-tuning methods keeping in mind robustness.\n\n(iii) Connection between theory and practice: we acknowledge that the theoretical settings considered are more stylized than real world settings. However, to the best of our knowledge, there is no existing analysis on precise trajectories of fine-tuning. Transfer learning theory focuses on linear probing---analysis of fine-tuning is scarce and challenging, and we lay out precise technical challenges in page 2 of the Introduction. It is important to ensure that our insights from the theoretical setting aren\'t misleading and in order to do this, we tested out numerous implications of our theory on real datasets (Section 4.2 and 4.3).\n\nWe have incorporated reviewer feedback in our revision, and we believe this makes our paper much stronger. Thank you to the reviewers for these suggestions:\n\n- Added results for linear probing and fine-tuning on four larger scale OOD datasets (ImageNetV2, ImageNet-R, ImageNet-Sketch, ImageNet-A) suggested by R5UET in Appendix B.5. We think this is a strong result because it suggests that our theory and intuitions generalize to larger scale OOD datasets requested by the reviewer.\n\n- Added more architectures (e.g. vision transformers) in Appendix B.4 and B.5.\n\n- Added comparisons to more fine-tuning methods (regularizing towards pretrained initialization, higher learning rate for head layer, side-tuning) in Appendix B.4. LP-FT does better both ID and OOD.\n\n- Added plots of effective robustness in Appendix B.6 - we see that LP-FT has higher effective robustness than fine-tuning, which we think makes our results stronger.\n\n- Tested the theoretical implications of Section 4.3 on more datasets, showing results in Appendix B.3.\n'}, {'title': 'Any other questions or suggestions?', 'comment': 'Thank you again for the comprehensive and useful review, and sorry to bother you. We just wanted to check if you had any other questions or suggestions - we would love to get feedback to further improve our work.\n\nSo far the review ""recommend(ed) its acceptance."" and there were some questions about larger scale datasets, architectures, and methods. We hope our response has addressed these and clarified the focus and novelty of the paper?'}, {'title': 'Accept', 'comment': ""I've read through the responses to other reviewers and changed my rating to accept.""}, {'title': 'Thank you -- makes sense', 'comment': 'That makes sense, and sounds fair! Sorry again to have bothered you about this.\n\nWe agree with your latest clarification, and have edited the discussion again based on that - thank you.'}, {'title': 'Thanks -- will look at other discussions', 'comment': ""I haven't thoroughly read through the discussions with the other reviewers yet, at that point if no issues arise I will change to accept recommendation. Thanks for the paper and the thorough response.\n\nWith regards to the concurrent work I have the same understanding, what I meant is that for the fine-tuning part they do what you've suggested in your original response above:\n> so we could initialize with the zero-shot final layer and then fine-tune the network""}, {'title': 'Thanks for the quick response - anything else we can do to improve the work?', 'comment': 'Thank you for the quick update! Sorry to follow up, but we just wanted to check what improvements could turn this into an ""accept"" recommendation? We would love to get feedback to improve our work going forward!\n\nSo far it looked like the review found the paper ""very interesting"" and an ""important contribution"" but the ""main weakness"" was ""contextualizing LP-FT"" - can we do better on this front? The other remaining point seemed to be about ""empirical verification"" of the the theory. Again, really appreciate you taking the time to write a thoughtful review so we apologize for bothering you about this.\n\nTo show that our findings are more general we now compare linear probing and fine tuning on ImageNet (ID) and four additional OOD datasets suggested by another reviewer, in Appendix B.5. We believe this is a strong support for our claims since these were datasets selected by another reviewer, and a larger architecture (vision transformer) but the observation that linear probing does better than fine-tuning for large distribution shifts still holds.\n\n> To better contextualize things in your added discussion, I believe this is what is done in a paper you\'ve cited https://arxiv.org/abs/2109.01903.\n\nThank you for bringing this up, and we have edited the discussion. Minor note: our understanding is that this paper (concurrent work, also submitted to ICLR) takes a weighted sum of the fine-tuned and zero-shot model in weight space, as opposed to LP-FT, but we will be happy to edit our discussion if we misunderstood.'}, {'title': 'Thank you for your response & minor additional point', 'comment': 'Thank you for clarifying many of my concerns, in particular the euclidean distance tables in addition to clarifying the typos. I will increase my score accordingly. \n\nOne minor point with regard to your response to ""CLIP, for which a zero-shot final layer can be constructed… LP-FT is still required?""\n\n> We like this suggestion - CLIP zero-shot models are another example where we have tradeoffs (zero-shot does worse than fine-tuning ID, but often better OOD), so we could initialize with the zero-shot final layer and then fine-tune the network. We have added this to the discussion.\n\nTo better contextualize things in your added discussion, I believe this is what is done in a paper you\'ve cited https://arxiv.org/abs/2109.01903.'}, {'title': '(1/2) Main concerns: contextualizing LP-FT and more datasets for empirical verification of theory', 'comment': '> ""The main weakness of this paper was contextualizing LP-FT""\n\n> ""is LP-FT something that has been used in the past?""\n\nWe apologize for the poor and confusing contextualization of LP-FT.\n\nFine-tuning is widely studied so there are many heuristics but they are poorly understood. **Our contribution is to theoretically understand** these better. This is particularly important for robustness/OOD because we find that the de-facto standard method of full fine-tuning is sub-optimal, but widely used in the OOD/robustness literature.\n\nMore specifically for LP-FT: Some people have used LP-FT as a heuristic for regular in-distribution fine-tuning [15,16]. However there isn\'t an understanding of why or when it does better, and LP-FT has not been used for OOD / robustness [9,10,11,12,13,14] or in popular recent pretraining papers like CLIP [4], SimCLR [5,6], MoCo [7], BYOL [8].\n\nWe have edited this in the manuscript and apologize for miscommunications. We\'ve **clarified in the intro that the method has sometimes been used as a heuristic (although not for OOD), with some cites**. Please let us know what else we can do to contextualize better.\n\n> ""performance improvement ID lead to performance improvement OOD""\n\n> ""so this could be an alternative explanation for the OOD boost of LP-FT that is orthogonal to the authors theory""\n\n> ""do the solutions found by LP-FT exhibit more effective robustness than the FT solutions?""\n\nThis is a great question. The **solutions found by LP-FT have higher effective robustness than FT**, because when they have similar ID accuracy, LP-FT does much better OOD. We have added Appendix B.6 to discuss this. Details:\n- On CIFAR-10 $\\to$ STL, there is no statistically significant difference between FT and LP-FT on ID, but LP-FT gets 8% higher accuracy OOD (Table 1).\n- If we look at checkpoints earlier in training for CIFAR-10 $\\to$ STL we can exactly equalize ID accuracy and compare OOD accuracies. For ID: LP-FT and FT both get 97.2%. For OOD: LP-FT (90.2%) > FT (81.8%).\n- In Appendix B.6 we plot the OOD accuracy against the ID accuracy for fine-tuning and LP-FT on Living-17. We plot these for three different pretrained models. In these new experiments added for the rebuttal, we see that the ID-OOD line for LP-FT is above the line for FT indicating effective robustness (as in [2]).\n\nWe thank the reviewer for the suggestion - we believe the finding that **LP-FT has higher effective robustness than FT makes our paper stronger**. This is particularly interesting because [2] and [3] show that it is uncommon for methods to have higher effective robustness.\n\n> The reviewer says the ""empirical verification of the theory (Section 4.3) is very interesting"" and asked if there are ""associated error bars?"" and said the ""euclidean distance experiment is very interesting why is it only conducted for one distribution shift?""\n\nWe agree with this point and for the rebuttal we have **added error bars and computed the distance that ID and OOD features move across the five datasets for both FT and LP-FT**. We have added these to Table 5 and Table 6 in Appendix B.3. We use Euclidean distance because in our feature distortion theory the norm of the change, and not just the direction, matters.\n\nWe see that in 9/10 cases (all except FT for FMoW), the **OOD features change less than the ID features** as predicted by the theory, and this is statistically significant. For example, on CIFAR->STL, fine-tuning, the distance moved is 1.70 \\pm 0.04 for OOD features and 2.23 \\pm 0.03 for ID features.\n\nAdditionally, across all 10 cases the **features change orders of magnitude less for LP-FT** than fine-tuning, suggesting that LP-FT indeed works for the reason predicted by the theory.\n\nEuclidean distance for ID examples (multiplied by 100):\n\n|       | CIFAR-10    | Entity-30   | Living-17   | DomainNet     | FMoW        |\n|-------|-------------|-------------|-------------|---------------|-------------|\n| FT    | 2.23 (0.03) | 3.05 (0.02) | 1.88 (0.01) | 207.6 (12.31) | 4.87 (0.15) |\n| LP-FT | 0.07 (0.00) | 0.03 (0.01) | 0.11 (0.01) | 0.19 (0.03)   | 0.57 (0.19) |\n\nEuclidean distance for OOD examples (multiplied by 100):\n\n|       | STL         | Entity-30   | Living-17   | DomainNet      | FMoW        |\n|-------|-------------|-------------|-------------|----------------|-------------|\n| FT    | 1.70 (0.04) | 2.60 (0.02) | 1.67 (0.01) | 159.97 (16.23) | 5.62 (0.3)  |\n| LP-FT | 0.04 (0.00) | 0.02 (0.00) | 0.09 (0.01) | 0.18 (0.02)    | 0.54 (0.17) |\n\n(Minor note: Yes, the features appear to change much more for DomainNet, which is perhaps why LP and LP-FT are substantially better there)\n'}, {'title': '(2/2) Other clarifications', 'comment': '> The reviewer asked if we have ""additional support for this claim"" that the ""gap between FT and LP grows as the quality of pretrained features improve"" and asked ""if the features are already good shouldn\'t LP be sufficient?""\n\nWe meant to say that the **OOD gap (important clarification!) between FT and LP grows as the quality of the pretrained features improve**. This comes from **Theorem 3.2**: as the features improve the ratio of the OOD error of LP to FT goes to 0, so LP does much better OOD. **As you said, the ID gap between LP and FT will narrow when the features are good but the OOD gap will increase**, so LP is sufficient if we have good features. We have clarified this in the updated manuscript.\n\n> ""Results."" paragraph of 4.2, where is the 76.5 number from\n\nWe apologize for this important typo, we copied from the table to the body of the text incorrectly. The number should be 74.4% as in Table 2, bottom row, right-most column. We did another pass over Section 4 to check for typos---please let us know if you have other concerns.\n\n> CLIP, for which a zero-shot final layer can be constructed… LP-FT is still required?\n\nWe like this suggestion - CLIP zero-shot models are another example where we have tradeoffs (zero-shot does worse than fine-tuning ID, but often better OOD), so we could initialize with the zero-shot final layer and then fine-tune the network. We have added this to the discussion.\n\n\nReferences:\n\n[4] Learning Transferable Visual Models From Natural Language Supervision. A. Radford et al. ICML 2021.\n\n[5] Appendix B.5 of the SimCLR paper: A Simple Framework for Contrastive Learning of Visual Representations. T. Chen et al. ICML 2020.\n\n[6] Page 5, paragraph 1, of SimCLR-v2: Big Self-Supervised Models are Strong Semi-Supervised Learners. T. Chen et al. NeurIPS 2020.\n\n[7] Appendix A.6 and A.7 of MoCo: Momentum Contrast for Unsupervised Visual Representation Learning. K. He et al. CVPR 2020.\n\n[8] Appendix D.3 of Bootstrap your own latent: A new approach to self-supervised Learning. JB Grill et al. NeurIPS 2020.\n\n[9] In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. S. M. Xie et al. ICLR 2021.\n\n[10] Using Pre-Training Can Improve Model Robustness and Uncertainty. D. Hendrycks et al. ICML 2019.\n\n[11] Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. D. Hendrycks et al. NeurIPS 2019.\n\n[12] Wilds: A Benchmark of in-the-Wild Distribution Shifts. PW Koh et al. ICML 2021.\n\n[13] The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. A. Andreassen et al. Arxiv 2021.\n\n[14] Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. J. Miller et al. ICML 2021.\n\n[15] https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb (the tutorial first linear probes, and then fine-tune, although it does not say why they do this or compare with just fine-tuning which is why we said some people have used it as a heuristic).\n\n[16] Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning. F. Kanavati et al. Medical Imaging with Deep Learning 2021. (their method FC-then-full first linear probes, and then fine-tunes, although it does not say why they do this or compare with just fine-tuning which is why we said some people have used it as a heuristic, in addition they do not look at OOD).\n'}, {'title': 'Key technical novelty is theory of fine-tuning', 'comment': 'We thank the reviewer for the positive review, expressing that we discover an ""interesting behavior of model fine-tuning"", the ""reasoning and intuition… are well explained"", and for appreciating the ""extensive… toy and benchmark experiments.""\n\nRegarding technical novelty, we wanted to clarify that **a key contribution of our paper is the theoretical analysis**. Recent transfer learning theory works [1,2] say that theoretical analysis of fine-tuning is missing and challenging because we need to analyze how the representations change, so we believe our theory is an important step forward. These prior works study training a linear classifier on frozen features, which has a closed form solution.\n\nWe believe that **theoretically analyzing fine-tuning of overparameterized models, and characterizing feature distortion, is our key technical novelty and contribution**. \n\n[1] In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. S. M. Xie et al. ICLR 2021.\n\n[2] On the Theory of Transfer Learning: The Importance of Task Diversity. N. Tripuraneni et al. NeurIPS 2020.\n'}, {'title': 'Clarifying that our focus is theory; added more baselines, pretrained models, datasets', 'comment': 'We thank the reviewer for the positive review and useful suggestions, and for agreeing that ""the observation that fine-tuning performance worse on the OOD test is new and interesting"" and appreciating the ""theoretical analysis"".\n\nWe believe the review focused more on the experiments so would like to clarify the focus of our paper. Many fine-tuning heuristics are used in practice, but it is unclear when they work---our **goal is theory and understanding** of when to use what method.\n- Fine-tuning theory is novel and challenging because we need to analyze how the representations change. Prior transfer learning work [1,2] recognizes this challenge and instead studies the simpler case where the representations are fixed.\n- LP-FT is a direct implication of our theory (which says that fine-tuning underperforms OOD because the initial random head is far from the optimal head). The goal of our experiments is to test if the theory\'s predictions hold up on real datasets. Besides LP-FT, we test other predictions from our theory in Section 4.3.\n\nWe believe that theory is our strongest contribution and novelty so we hope the reviewer takes these responses into account.\n\n> ""some important baselines are missing"" and the reviewer would like to see these ""discussed and compared""\n\nWe thank the reviewer for the useful pointers. We do not think LP-FT is a SOTA method, but an example insight of the theory---as predicted by the theory it gets the best of both worlds: the strong ID accuracy of fine-tuning, and strong OOD accuracy of linear probing. Combining it with ideas like spot-tuning, side-tuning, and joint fine-tuning is a good idea for future work and would likely boost its performance. We have added cites to these works and a discussion to the related works.\n\nThe strength of our theory is it can give insights into fine-tuning methods. For example, side-tuning freezes the pretrained weights and tunes a smaller side network---in our theoretical setting this would get good OOD performance as well because it reduces feature distortion.\n\nThat said, we have **added more fine-tuning methods for the rebuttal**---we now compare with l2-sp [3] and using a higher learning rate for the final layer, on the Living-17 dataset. LP-FT did better than both methods, ID and OOD. L2-sp requires an additional hyperparameter---we tuned this, doing a grid search over 5 regularization weights * 6 learning rate = 30 hyperparameters, even more so than fine-tuning and LP-FT where we just swept over 6 learning rates. Each configuration is run 3 times with different seeds. More details are in Appendix B.4.\n\n|                  | ID         | OOD        |\n|------------------|------------|------------|\n| LP               | 96.5 (0.1) | 82.2 (0.2) |\n| FT               | 97.1 (0.1) | 77.7 (0.7) |\n| FT (10x Linear)  | 97.2 (0.2) | 80.4 (0.3) |\n| FT (regularized) | 97.1 (0.2) | 80.0 (0.4) |\n| LP-FT            | 97.8 (0.1) | 82.6 (0.3) |\nL2-sp corresponds to FT (regularized). We used Living-17 because that\'s what we used for all ablations in Section 4.3.\n\n> ""why different pre-trained models are utilized for different ID and OOD pairs""\n\n> ""In fact, the authors could use different models for each ID and OOD pair.""\n\nWe used different pretraining methods because we require reasonably good pretrained features for the downstream task. For example, for the satellite dataset FMoW we use a model pretrained on unlabeled satellite images---an ImageNet pretrained model would not be suitable and performs poorly on satellite images. Similarly, a satellite image pretrained model would not be appropriate or do well on CIFAR.\n\nThat said, we agree with your broader point and have **added more pretraining models for the rebuttal**---we ran the Living-17 experiments with a CLIP-ResNet-50 and a CLIP-ViT-B/16 (a vision transformer that is much larger than ResNet-50), and the same findings hold. We have added these to Appendix B.4.\n\nIn addition, we have **added results for larger datasets**: ImageNet $\\to$ ImageNet-v2, ImageNet-R, ImageNet-Sketch, ImageNet-A, in Appendix B.5. Fine-tuning still does better than linear probing ID, but worse OOD when the shift is large.\n\nWe believe that these additional experiments make our paper stronger, and show that our findings are more general, so we thank the reviewer for the suggestions.\n\n[1] In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. S. M. Xie et al. ICLR 2021.\n\n[2] On the Theory of Transfer Learning: The Importance of Task Diversity. N. Tripuraneni et al. NeurIPS 2020.\n\n[3] Explicit Inductive Bias for Transfer Learning with Convolutional Networks. X. Li et al. ICML 2018.\n'}, {'title': '(1/3) Core contribution is theory of fine-tuning, which is challenging', 'comment': 'We thank Reviewer 5UET for saying that the ""results, both theoretical and empirical, would be of interest to many"" and for ""recommend(ing) its acceptance"". We also thank the reviewer for the detailed and useful suggestions---**we have run suggested experiments (more datasets, more architectures, more baselines)**, which we believe makes our paper stronger.\n\nWe believe the review focused more on the experiments so would like to clarify the focus of our paper. Many fine-tuning heuristics are used in practice, but it is unclear when they work---our **goal is theory and understanding** of when to use what method. We also believe that **the finding that linear probing does better than fine-tuning for large distribution shifts is novel** and interesting.\n\n> The reviewer likes that the theoretical results ""offer intuitions that transferred well to practical results"" but mentions that it is ""disconnected from realistic settings... two-layer network... squared error loss""\n\nWe agree---we would love to study more realistic models but want to emphasize that studying fine-tuning for our setting is already challenging and a big step forward.\n- **Analyzing fine-tuning is challenging even for two-layer models** because we need to analyze how the representations change. Prior transfer learning work [5, 6] recognizes this challenge and instead studies the simpler case where the representations are fixed.\n- We think it is a strength that our theory leads to insights that hold up on many real datasets, for example that ID features change more than OOD features, features change much less for LP-FT, and OOD early stopping does not solve the problem with fine-tuning (Section 4.3).\n\nWe believe that theory is our strongest contribution and novelty so we hope the reviewer takes these responses into account.'}, {'title': '(2/3) Ran vision transformer, more datasets, more baselines', 'comment': '> ""unclear whether the results from this paper would hold at larger scales.""\n\n> ""distribution shifts like ImageNetV2, ImageNet-R, ImageNet-A, ObjectNet and ImageNet Sketch were not considered""\n\n> ""only a single architecture, ResNet-50, is used""\n\nIn our original paper we used popular distribution shift datasets like DomainNet, Breeds, CIFAR $\\to$ STL, and the remote sensing dataset ""Functional Map of the World"". We have now **added results on larger scale datasets**: we linear probe or fine-tune on ImageNet, and evaluate on **ImageNetV2, ImageNet-R, ImageNet-A, and ImageNet-Sketch**. This is for a **CLIP ViT-B/16 (vision transformer), the largest publicly available CLIP model** [7], which is much larger than a ResNet-50.\n\nRecall that our theory says that linear probing does better than fine-tuning when the shift is large. We expect linear probing to do better on ImageNet-R, ImageNet-A, ImageNet-Sketch. We expect fine-tuning to do better on ImageNetV2 because it is collected by ""repeating the dataset curation process"" of ImageNet (quote from the ImageNetV2 paper)---this is similar to CIFAR-10 $\\to$ CIFAR-10.1 in our original paper.\n\nWe find that fine-tuning gets 2% higher accuracy ID than linear probing, but averaged across the four OOD datasets fine-tuning gets 10% lower accuracy OOD. LP-FT is currently running and will be included, if accepted, in the camera ready.\n\nFor the ImageNet-validation set (ID):\n\n| ID             | ImageNet |\n|----------------|----------|\n| Fine-tuning    |  **81.7**|\n| Linear probing |     79.7 |\n\nFor the OOD datasets (ImageNetV2 is a replication of ImageNet so a small shift, so the results are similar to CIFAR-10 -> CIFAR-10.1 in our original paper):\n\n| OOD            |  ImageNetV2 (small shift) | Renditions | Sketch | ImageNet-A |*OOD Average*|\n|----------------|---------------------------|------------|--------|------------|-------------|\n| Fine-tuning    |                   **71.5**|       52.4 |   40.5 |       27.8 |       *48.1*|\n| Linear probing |                      69.7 |    **70.6**|**46.4**|    **45.7**|       *58.1*|\n\nWe think this is a strong result because it suggests that **our theory and intuitions generalize to larger scale OOD datasets requested by the reviewer**. We include more details in Appendix B.5.\n\nWe have also added **new results for a CLIP vision transformer** (ViT-B/16) on Living-17 to Appendix B.4. Indeed, we find that fine-tuning does better than linear probing ID, but underperforms OOD. LP-FT does better than both ID, and closes over 75% of the gap OOD. These results are from early stopping on ID validation data—-LP-FT actually achieves 87.9% OOD accuracy (better than LP) in the middle of training.\n\n|       | ID         | OOD        |\n|-------|------------|------------|\n| LP    | 97.5 (0.1) | 87.6 (0.5) |\n| FT    | 97.8 (0.0) | 81.5 (2.1) |\n| LP-FT | 98.0 (0.0) | 86.1 (0.1) |\n\n> ""overlooks simple baselines""\n\n> ""given the finding that end-to-end fine-tuning significantly changes the weights of the backbone (which arguably hurts OOD performance), it would be natural to consider""\n\n> (1) ""smaller learning rate for the backbone, and a higher one for the untrained final linear layer""\n\n> (2) ""regularizing the difference of the weights of the backbone to the original weights""\n\nAs the reviewer mentions, intuitions from the feature distortion theory also suggest that some other methods like (1) and (2) reduce feature distortion and can improve OOD accuracy to some extent. This is the right insight, and LP-FT is an extreme version where the learning rate for the features is 0 initially. **We view (1) and (2) as other methods that can work based on our theory, as opposed to baselines.**\n\nWe have **added results for methods (1) and (2) on Living-17** in Appendix B.4. For (1) we used a 10x larger learning rate for the linear classifier layer, repeating the grid search over 6 learning rates, and for (2) we grid searched over regularization weights in {1.0, 0.1, 0.01, 0.001, 0.0001}, on top of the grid search over 6 learning rates we did for fine-tuning and LP-FT (so 30 hyperparameter configurations * 3 replication runs = 90 runs). As expected, we found that these methods do better than fine-tuning OOD. LP-FT still does better ID and OOD. \n\n|                  | ID         | OOD        |\n|------------------|------------|------------|\n| LP               | 96.5 (0.1) | 82.2 (0.2) |\n| FT               | 97.1 (0.1) | 77.7 (0.7) |\n| FT (10x Linear)  | 97.2 (0.2) | 80.4 (0.3) |\n| FT (regularized) | 97.1 (0.2) | 80.0 (0.4) |\n| LP-FT            | 97.8 (0.1) | 82.6 (0.3) |\n\nWe used Living-17 because that\'s what we used for all ablations in Section 4.3.'}, {'title': '(3/3) Other responses on related work', 'comment': '> ""Apart from the theoretical results, there is not a lot of novelty introduced by this work.""\n\n> ""two-stage fine-tuning strategy where end-to-end fine-tuning follows linear probing… in the first lesson of https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb""\n\nIn addition to the theoretical contributions, we believe **the finding that linear probing does better than fine-tuning for large distribution shifts is novel.**\n\nWe thank the reviewer for this link, and have added a cite to the tutorial---in our original paper, we say ""LP-FT has sometimes been used as a fine-tuning heuristic, we show that it addresses the ID-OOD tradeoff theoretically and empirically"".\n\nThe reviewer asked for better contextualization of LP-FT, and what the novel aspects are:\n- LP-FT is a direct implication of our theory (which says that fine-tuning underperforms OOD because the initial random head is far from the optimal head). The goal of our experiments is to test if the theory\'s predictions hold up on real datasets.\n- Some people have used LP-FT as a heuristic. However there isn\'t an understanding of why or when it does better, and **LP-FT has not been used for OOD / robustness** [5,12,13,14,15,16] or popular recent pretraining papers like CLIP [4], SimCLR [5,6], MoCo [7], BYOL [8]---they use linear probing or vanilla fine-tuning.\n\nThat said, we appreciate your point so please let us know how we can contextualize things better.\n\n> Other related work\n\nIn the related works we now clarify that [1] is a promising approach, and have added [2]-[4]. We note that [1] appears to be concurrent work based on the ICLR guidelines.\n\n[1] mentions on page 8 that they got comparable performance by ensembling the two models in output space. **For the rebuttal, we ran an additional comparison on Living-17**, where we ensemble the outputs of the linear probed and fine-tuned models, choosing a weight $\\alpha$ to maximize ID validation accuracy. Averaged over three runs, ID accuracy of this ensemble was 97.1\\% and OOD accuracy was 80.8\\%, so better than fine-tuning but LP-FT is better both ID (97.8%) and OOD (82.6%). Finally, we note that we do not think LP-FT is the SOTA method, and we hope that combining our insights with other methods could lead to better performance.\n\n> ""authors do not mention the fact that models like CLIP allow the possibility of starting with a good set of initial weights for the final layer""\n\nThis is a great point, we have added this to the discussion.\n\nReferences:\n\n[5] In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness. S. M. Xie et al. ICLR 2021.\n\n[6] On the Theory of Transfer Learning: The Importance of Task Diversity. N. Tripuraneni et al. NeurIPS 2020.\n\n[7] Learning Transferable Visual Models From Natural Language Supervision. A. Radford et al. ICML 2021.\n\n[8] Appendix B.5 of the SimCLR paper: A Simple Framework for Contrastive Learning of Visual Representations. T. Chen et al. ICML 2020.\n\n[9] Page 5, paragraph 1, of SimCLR-v2: Big Self-Supervised Models are Strong Semi-Supervised Learners. T. Chen et al. NeurIPS 2020.\n\n[10] Appendix A.6 and A.7 of MoCo: Momentum Contrast for Unsupervised Visual Representation Learning. K. He et al. CVPR 2020.\n\n[11] Appendix D.3 of Bootstrap your own latent: A new approach to self-supervised Learning. JB Grill et al. NeurIPS 2020.\n\n[12] Using Pre-Training Can Improve Model Robustness and Uncertainty. D. Hendrycks et al. ICML 2019.\n\n[13] Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty. D. Hendrycks et al. NeurIPS 2019.\n\n[14] Wilds: A Benchmark of in-the-Wild Distribution Shifts. PW Koh et al. ICML 2021.\n\n[15] The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. A. Andreassen et al. Arxiv 2021.\n\n[16] Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. J. Miller et al. ICML 2021.\n'}, {'summary_of_the_paper': 'This paper discovers an interesting behavior of model fine-tuning: the performance is worse compared to linear probing on OOD data (i.e., data from other domains), especially when the distribution shift between inner distribution and out of distribution are big. The explanation provided in the paper is that fine-tuning distorts the feature representations, overfits on inner distributions, and thus has a higher error on OOD data. The authors also provide a simple solution to this issue by fine-tuning with a classification head initialized from linear probing and had better results in all the benchmarks they have in the paper. ', 'main_review': 'The strength of this paper is the extensive and detailed toy and benchmark experiments. The reasoning and intuition of why fine-tuning underperforms on OOD data are well explained and discussed throughout the paper. The suggested solution by combining linear probing and fine-tuning also has good performance. ', 'summary_of_the_review': 'Although the solution proposed in this paper is not fancy, the reasoning, intuition, and experiments are well written. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper studies the problem of how to fine-tune a pre-trained model and obtain better results for both ID and OOD. Two methods, fine-tuning and linear probing, are investigated and compared, then a new two-step variant called LP-FT is derived. Results further verify that LP-FT obtains the best performance for ID and OOD tests compared with FT and LP.', 'main_review': 'Pros:\n\n1. the observation that fine-tuning performance worse on the OOD test is new and interesting \n\n2. the proposed method LP-FT is simple yet effective\n\n3. theoretically analysis is further provided to show why LP-FT works\n\nCons:\n\n1. some important baselines are missing like [a-c], which should be discussed and compared\n\n[a]. Guo, Yunhui, et al. ""Spottune: transfer learning through adaptive fine-tuning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[b]. Zhang, Jeffrey O., et al. ""Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks."" Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part III 16. Springer International Publishing, 2020.\n\n[c]. Ge, Weifeng, and Yizhou Yu. ""Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n2. why different pre-trained models are utilized for different ID and OOD pairs? In fact, the authors could use different models for each ID and OOD pair.', 'summary_of_the_review': 'This paper discovers that vanilla fine-tuning performs worse than linear probing for the OOD tests and then develops a new method combining these two techniques sequentially. Results on several datasets verify its effectiveness. Even there exists some minor problems, this paper is interesting and easy to read. Thus, I tend to give the ""weak accept"" score.\n\n-----POST REBUTTAL-----\n\nThe authors have addressed my concerns. Thus, I increase my score from 6 to 8.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper explores how different strategies for fine-tuning affect in- and out-of-distribution performance. The authors contrast linear probing (updating only the parameters of the final linear layer), end-to-end fine-tuning (updating all parameters of the model) and a two-stage approach, where linear probing is followed by end-to-end fine-tuning. While end-to-end fine-tuning typically improves in-distribution performance, the authors show that it can also underperform linear probing out-of-distribution. The paper theoretically analyzes the tradeoffs in a simplified scenario with two-layer networks, finding that end-to-end fine-tuning can ""distort"" pre-trained features. Their experiments on a number of datasets including CIFAR, WILDS-FMoW and others, confirm the intuitions from their theory. The proposed mitigation strategy, a two-stage fine-tuning approach where end-to-end fine-tuning follows linear probing is found to be beneficial, especially out-of-distribution.\n\n\n**Update:** The authors addressed most of the concerns raised by this and other reviews, and I am raising my score accordingly.', 'main_review': '**Strengths:**\n\n1. This paper explores two increasingly impactful research directions, fine-tuning pre-trained models and generalization under distribution shifts. I believe their results, both theoretical and empirical, would be of interest to many in the community\n\n2. The theoretical results, despite studying a very simple setting that is unlikely to be used in any real experiments, offer intuitions that transferred well to practical results.\n\n3. The paper is clear and well written.\n\n**Weaknesses:**\n\n1. Experiments could be more comprehensive. For instance, while this paper analyses several distribution shifts (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR->STL, CIFAR 10.1 and FMoW), it was surprising that distribution shifts like ImageNetV2, ImageNet-R, ImageNet-A, ObjectNet and ImageNet Sketch were not considered by this work. Moreover, only a single architecture, ResNet-50, is used for the experiments. As they stand, it is unclear whether the results from this paper would hold at larger scales.\n\n2. This work overlooks simple baselines. For instance, given the finding that end-to-end fine-tuning significantly changes the weights of the backbone (which arguably hurts OOD performance), it would be natural to consider having a smaller learning rate for the backbone, and a higher one for the untrained final linear layer. It is not uncommon to do so in practice. Moreover, authors could have explored regularizing the difference of the weights of the backbone to the original weights, encouraging them to not be changed too much. Finally, the authors do not mention the fact that models like CLIP allow the possibility of starting with a good set of initial weights for the final layer, as they can be used in a zero-shot setting. If the intuitions presented in this paper hold, this prevent the ""distortion"" that happens when the last layer is far from the optimum.\n\n3. The theoretical results are disconnected from realistic settings. Some clear examples are the assumption of a two-layer network, a squared error loss (while the de facto standard for classification is cross-entropy), and considering the worst case loss over distributions of bounded norm.\n\n4. Apart from the theoretical results, there is not a lot of novelty introduced by this work. The two-stage fine-tuning strategy where end-to-end fine-tuning follows linear probing is commonplace and thought in introductory courses in deep learning, e.g. in the first lesson of https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb\n\n5. Comparison to previous work is lacking. In particular, explicit comparisons of LP-FT with other recent robustness-oriented fine-tuning methods would greatly strengthen this work [1-4, among others]. \n\n\n**References:**\n\n[1] Wortsman, Mitchell, et al. ""Robust fine-tuning of zero-shot models."" arXiv preprint arXiv:2109.01903 (2021).\n\n[2] Aghajanyan, Armen, et al. ""Better fine-tuning by reducing representational collapse."" arXiv preprint arXiv:2008.03156 (2020).\n\n[3] Jiang, Haoming, et al. ""Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization."" arXiv preprint arXiv:1911.03437 (2019).\n\n[4] Zhu, Chen, et al. ""Freelb: Enhanced adversarial training for natural language understanding."" arXiv preprint arXiv:1909.11764 (2019).', 'summary_of_the_review': 'Overall, while there are several points of concern that could strengthen the paper, I believe the results and theory presented by this work would still be of interest to many in the community, so I recommend its acceptance.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper contrasts fine-tuning (i.e., modifying all network weights) and linear probing based on their relative ID/OOD performance. It is known that fine-tuning (FT) outperforms linear probing (LP) ID. This paper presents that the reverse is true OOD (FT outperforms LP). This paper suggests that this occurs because fine-tuning distorts features in conjunction with the final linear layer. Instead, if a final linear layer is trained first, the features do not have to move that much during full fine-tuning. The authors refer to this method as LP-FT and show it often outperforms LP and FT both OOD and ID.', 'main_review': 'This paper has many strengths:\n- It is not well known that FT often outperforms LP OOD -- the empirical and theoretical study of this phenomenon may be very important to the community.\n- LP-FT outperforms LP and FT on many distribution shifts.\n\nIn addition, the paper has some weaknesses with associated actionable items:\n\n1) The main weakness of this paper was contextualizing LP-FT. In the abstract and introduction, it seems as though LP-FT is a method that is being introduced by this paper. For instance, the abstract states ""our analysis suggests the simple two step-strategy of linear probing then full fine-tuning"". It is not until the final page of the main paper that the related work section states ""LP-FT has sometimes been used as a fine-tuning heuristic"". Moreover, this is presented without any citations. I would very much appreciate clarity on this issue, is LP-FT something that has been used in the past? If so, by which papers and why?  Does this reference that in fine-tuning, sometimes the hyperparameters in the final layer are decoupled from those used for the encoder [1] and performance improves? As shown by [2, 3], performance improvement ID lead to performance improvement OOD, and so this could be an alternative explanation for  the OOD boost of LP-FT that is orthogonal to the authors theory. For instance, in the effective robustness framework of [2], do the solutions found by LP-FT exhibit more effective robustness than the FT solutions?\n\n2) Section 4 is hard to follow in its current form as it is not clear exactly from where numbers are derived. For instance in the first line of the  ""Results."" paragraph of 4.2, where is the 76.5 number from?\n\n3) One of the networks studied by this paper is CLIP, for which a zero-shot final layer can be constructed (e.g., the final linear layer does not need to be constructed from scratch before fine-tuning). One possible addition to the paper could be discussion of this setting, and in particular if LP-FT is still required.\n\n4) There are a few claims in the paper which could benefit from additional support. In particular, in the conclusion it is stated that the ""gap between FT and LP grows as the quality of pretrained features improve"". Are the authors referencing the single MOCO-v1 vs. MOCO-v2 experiment or is there additional support for this claim? Are the authors referring to absolute or relative difference between FT and LP? I am wondering as this claim seems a bit counterintuitive, if the features are already good shouldn\'t LP be sufficient?\n\n5) The empirical verification of the theory (Section 4.3) is very interesting and could be more thorough. In particular, are there associated error bars for the 0.019 and 0.017 numbers as this seems quite close to conclude that one is larger. This euclidean distance experiment is very interesting, why is it only conducted for one distribution shift? Moreover, what happens when analyzing cosine distance instead of cosine distance?\n\n[1] https://arxiv.org/pdf/2106.04560.pdf\n[2] https://arxiv.org/abs/2007.00644\n[3] https://arxiv.org/abs/2107.04649', 'summary_of_the_review': 'This paper is very interesting and will be an important contribution if concerns are properly addressed. An essential concern is the contextualization of the method LP-FT --- is this method (or a modification) something that people have explored previously and if so in what context. In the current framing of the abstract and introduction, LP-FT seems to be introduced by this paper. In addition, this papers empirical verification of the theory (Sec 4.3) appears very promising but could benefit from additional detail and experiments (e.g., more than one distribution shift in the euclidean distance experiments).\n\nEdit: authors have addressed many concerns and I have changed my score to 6.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution', 'authorids': ['~Ananya_Kumar1', '~Aditi_Raghunathan1', '~Robbie_Matthew_Jones1', '~Tengyu_Ma1', '~Percy_Liang1'], 'authors': ['Ananya Kumar', 'Aditi Raghunathan', 'Robbie Matthew Jones', 'Tengyu Ma', 'Percy Liang'], 'keywords': ['fine-tuning theory', 'transfer learning theory', 'fine-tuning', 'distribution shift', 'implicit regularization'], 'abstract': 'When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the ""head""). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).', 'one-sentence_summary': 'Fine-tuning does better than linear probing (training a linear classifier on pretrained features) in-distribution, but worse out-of-distribution (OOD)---we analyze why this happens and propose a way to get the benefits of both.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'kumar|finetuning_can_distort_pretrained_features_and_underperform_outofdistribution', 'pdf': '/pdf/5d8a4ae4492042b22b07eabc7a9abcfa517f419c.pdf', 'supplementary_material': '/attachment/34f81a5b333e131206e26a14e9de6125508dcbc8.zip', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/fine-tuning-can-distort-pretrained-features/code)', '_bibtex': '@inproceedings{\nkumar2022finetuning,\ntitle={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},\nauthor={Ananya Kumar and Aditi Raghunathan and Robbie Matthew Jones and Tengyu Ma and Percy Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UYneFzXSJWh}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Vadim Popov', 'Ivan Vovk', 'Vladimir Gogoryan', 'Tasnima Sadekova', 'Mikhail Kudinov', 'Jiansheng Wei']",ICLR,Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme,https://iclr.cc/virtual/2022/oral/6241,2022," Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=8c50f-DoWAu,https://openreview.net/forum?id=8c50f-DoWAu,8c50f-DoWAu,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper is exceptionally well summarized by Reviewer QC5G which is difficult to improve up on. I will save the readers the effort of reading more text (without adding more substance). The reviewers unanimously rated this paper highly. The discussion has been robust,  enlightening and also has improved the revised paper.'}, {'title': 'Responses and clarifications OK', 'comment': 'The responses from the authors provided detailed clarifications about the work.  I keep my recommendation to accept the paper.'}, {'title': 'Some clarification', 'comment': 'We would like to thank the reviewer for the comments and address the concerns in the following.\n\n1. One of the main concerns is unclear motivation behind the proposed Maximum Likelihood (ML) SDE solver and the reasons why it should be better than general-purpose Euler-Maruyama (EM) one. As established in the Theorem 1, the ML solver optimizes log-likelihood of sample paths $X_{kh}$ under generative scheme $\\hat{X}$ given by the formula (11). It means that the sequences $X_{kh}$ of real data points corrupted with Gaussian noise according to the forward SDE (6-F) are more likely to come as solutions of the reverse SDE (6-R) obtained with the ML SDE solver than with any other solver belonging to the class described in the Theorem 1 (we\'d like to note it is only this class that is parameterized with three parameters $\\kappa$, $\\omega$ and $\\sigma$ while the ML solver itself does not have hyperparameters other than step size).  \nThus, for any step size $h$:  \na) ""real"" sequences $X_{kh}$ are best described by the ML SDE solver $\\hat{X}$;   \nb) as a consequence, score-matching network $s_{\\theta^{*}}$ is more likely to point to the correct directions (of increasing of the logarithm of noisy data density) because its inputs $\\hat{X_{kh}}$ are ""closer\'"" to the ones seen at training $X_{kh}$.   \nCombined together, a) and b) suggest that the ML SDE solver relies on better approximations of likely data trajectories which can reduce the negative effect caused by SDE discretization and lead to samples of higher quality compared to other SDE solvers from the class.   \nAlthough in principle it holds for every step size $h$, for small step sizes the ML SDE solver does not differ much from standard EM SDE solver as derived in Appendix B and discussed after the Theorem 1, so the benefits of the proposed ML solver in terms of higher quality are more clear when the step size is large. We demonstrate this feature of our ML solver in Table 3: only the ML solver allows us to use as few as 6 reverse diffusion steps while others seem to suffer from large discretization errors resulting in significant MOS degradation. Also, the fact that we can use larger step sizes with almost no loss of quality is illustrated by speech samples on our web demo page.\n\n2. The middle part in eq.(40) implies that $\\hat{\\kappa_{t,h}}$ = $\\frac{(1 - \\gamma_{0,t}^{2}) \\nu_{t-h,t}}{\\gamma_{0,t}\\beta_{t}h} - 1$ which equals right-hand side of the definition of $\\kappa_{t,h}^{\\star}$ in eq.(10). Thus, the equality $\\hat{\\kappa_{t,h}} = \\kappa_{t,h}^{\\star}$ in the right-hand side of eq.(40) is correct.\n\n3. Our proposed ML solver is indeed a first-order scheme. Employing second-order sampling schemes could lead to acceleration in terms of the number of solver steps, but each step would be more computationally intensive, i.e. the number of function evaluations (the slowest part in a diffusion model) would be bigger. Moreover, the fact that we deal with SDEs rather than ODEs makes things even worse: estimating first-order differential of the drift term $s_{\\theta}(x, t)$ in the reverse SDE would require second order derivatives of $s_{\\theta}$ in $x$ as follows from Ito\'s formula (see second-order schemes derivation in [1]). Considering this, we decided to limit ourselves to solvers requiring only one function evaluation per timestep, so predictor-corrector and second-order schemes are out of the scope of our paper because of their potential computational inefficiency.\n\n4. In contrast with unconditional models, it is typically possible to tune diffusion models with strong conditioning to generate high-quality samples with a few reverse diffusion steps (see [2], [3]) despite using a first-order sampling scheme rather than more sophisticated techniques. We believe it is because strong conditioning (as in our case) makes score-matching task easier and training results in score-matching networks of better quality, which means fewer reverse diffusion steps for a good output quality. E.g. the limiting case (ii) from the Theorem 1 shows that for simple data distributions and ideally-trained score-matching networks our ML solver provides exact data reconstruction even when one reverse diffusion step is used.\n\n[1] Glasserman, P. (2004) Monte Carlo Methods in Financial Engineering, Chapter 6.  \n[2] Chen, N. et al. (2021) WaveGrad: Estimating Gradients for Waveform Generation  \n[3] Popov, V. et al. (2021)  Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech'}, {'title': 'Intuition behind proof', 'comment': '5. The proof of the Theorem 1 is rather technical but here is a short summary of the main steps. The key Lemma 1 helps us to express \n$s_{\\theta}(x,t)$ as a linear combination of $x$ and conditional data expectation $E[X_{0}|X_{t}=x]$. It allows us to write down an explicit \nexpressions for $Law(\\hat{X_{t-h}}|\\hat{X_{t}})$ in terms of $\\hat{X_{t}}$ and $E[X_{0}|\\hat{X_{t}}]$ (36) and the corresponding transition probability $\\hat{p_{t-h|t}}(x_{t-h}|x_{t})$ (39). From the general properties of Gaussian distributions we derive similar expressions for $Law(X_{t-h}|X_{t},X_{0})$ in terms of $X_{t}$ and $X_{0}$ (42) and the corresponding transition probability $p_{t-h|t}(x_{t-h}|x_{t})$ (45). We then minimize cross-entropy between distributions with transition densities $\\hat{p_{t-h|t}}(x_{t-h}|x_{t})$ and $p_{t-h|t}(x_{t-h}|x_{t})$ (46) subject to parameters $\\hat{\\mu}$, $\\hat{\\nu}$ and $\\hat{\\sigma}$ of (39). Finding optimal $\\hat{\\mu}$, $\\hat{\\nu}$ and $\\hat{\\sigma}$ lead us to optimal values in formula (10).'}, {'title': 'Thanks for the comments', 'comment': 'We would like to thank the reviewer for the comments and for high appreciation of our work!'}, {'title': 'Thanks for the comments', 'comment': 'We would like to thank the reviewer for the comments and for high appreciation of our work! We addressed small remarks in the revision.'}, {'summary_of_the_paper': ""This paper proposes a method to perform voice conversion through recent methods in Diffusion Based Probability Modeling (DPM) through Scochastic Differential Equations (SDE), building upon recent work Grad-TTS and Glow-TTS. The architecture is an encoder-decoder setup, trained separately, described in upcoming paragraphs. Viewed at a high level, the encoder maps input to 'noise', and the decoder inverts noise to output, in keeping with the SDE formalism. To transform average features to output voice, speaker info is conditioned on the average voice features before feeding to decoder. \n\nThe encoder learns to map input voice features to an average mel spectrogram. In order to do this, the Montreal Forced Aligner is used to align mel frames to phonemes. The average output voice (for encoder training) is obtained by aligning and averaging out corresponding phonemes in the dataset. \n\nThe main novelty in this paper is claimed to be in the decoder setup solving the reverse SDE. The authors derive a modified SDE that does MLE inference on the original reverse SDE. Through this approach, the claims are that the likelihoods are better, and its estimation is 'faster' (ostensibly, owing to getting around step size limitations). \n\nEvaluations are carried out for VCTK and LibriTTS, and quite well done, and the samples provided show that the approach works. Comparisons are made against several other approaches, include subjective evaluations and FID scores, and look for similarity and naturalness metrics. They also compare against different flavors of SDE setups (Variance Exploding, Preserving, etc.).\n\n\n"", 'main_review': ""Post rebuttal\n-----------------\nMy initial review was largely positive, but for a few issues with the motivation \n- what does the MLE solver give over the usual Euler-Mayurama integration setup?\n- some checks on the formulation and intuitions behind proof.\n\nThe authors have sufficiently answered my concerns:\n- The MLE solver gives an optimized trajectory for data, even though the overall scheme is still first order, as shown through a simple example. The authors argue that while second (or higher) order schemes are very much possible, they would need more function evaluations and therefore not necessarily faster (this point could use another demo). \n- The formulation as far as I can see is correct, as shown in the author rebuttal. Intuitions behind proof are also summarized.\n- I think a lot more work could be done in exploring diffusion probability models on the lines of this paper, in regards to numerical integrators and formulation setup.\n\nI am raising my score.\n\nInitial Review \n-----------------\nI reviewed this paper with great interest. It is rather remarkable how much progress has been made in the last few years in speech modeling. It is also interesting that the newer models (and this is one of them) compute the input-output alignment differently from the attention mechanism used in works such as Tacotron. Likewise, it is inspiring that generative modeling research (e.g. flow based, and DPMs) finds application in speech models quite rapidly, and with telling practical use (e.g. WaveGlow vocoder).\n\nStrengths \n-------------\n+ Builds upon current work in DPMs (Grad-TTS, Wave-Grad) to come up with a setup for voice conversion. \n+ Uses a sensible scheme involving computing an 'average voice' into which speaker information is added and output voice is decoded. \n+ Paper contains a good review of existing work, and it is possible to piece together developments in score based modeling and adaptations in speech. \n+ Maximum Likelihood solver formulation for reverse SDE is well motivated and gives good results. It produces a first order scheme - not very different from Euler-Mayurama - with three additional hyperparameters. Supposedly, this results in an improvement in sample quality through optimized likelihood.\n\nWeaknesses\n-----------------\n- I found the mathematical derivations not very clearly explained. The development is generally easy enough to follow if we work out the slightly tedious derivations. However, I think it would benefit from a few lines of 'intuition'. What is the intuition behind the derivations, why does it work and in what types of SDEs does such a procedure work? \n- There seems to be an inexactitude (or mistake) in the derivations (equation 40). I think we get $ k^*_{t,h} = 1+ \\hat{k}_{t,h} $ (i.e. the original derivation needs an extra 1 on the right hand side).\n\n- The motivations behind the MLE derivations aren't very clear (at least to me). Is it that since we maximize likelihood, the setup is more optimized and therefore produces better samples, or are we talking about being able to take 'larger' step sizes (smaller N)? \n\n- As such, it seems to me that the scheme is O(h), and should not work well when timesteps are large. It would be nice to see some explanation or analysis of why the setup converges in such a small number of iterations. I would expect a second order scheme to converge faster. Likewise, a predictor-corrector scheme (as in the stem SDE works by Yang Song) improves performance. \n\n\n"", 'summary_of_the_review': ""On the whole, the work looks plausible for voice conversion, even if it only used the base Euler-Mayurama scheme. The derived scheme produces better samples, or does so allowing use of much larger timesteps (~ N = 6). Nevertheless, I am not totally convinced because\n1) It is not explained why the optimization 'works' in an intuitive sense. I assume that it provides a path with better likelihood since it has been optimized\n2) The derivations ought to be gone over very carefully, and it is crucial that the details are correct. I would like the other reviewers to take a careful look at the derivations, especially so as I feel that there is a mistake in equation (40), which could carry over to the other steps as well. \n\nI rule this work as a marginal reject, but can change my decision after discussion and clarification. "", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes a diffusion probabilistic model-based voice conversion method for one-shot voice conversion scenario.  The proposed method can generate high-quality converted speech compared to state-of-the-art approaches.  Furthermore, to improve the real-time factor, a novel stochastic differential equations solver is proposed which makes the diffusion model faster.  The proposed solver is also suitable for other generative tasks.\n', 'main_review': 'Strengths:\n(1) A diffusion probabilistic model-based voice conversion method has been proposed for the one-shot voice conversion scenario.  \n(2) The proposed method can generate high-quality converted speech compared to state-of-the-art approaches. \n(3) To solve the disentanglement problem, a new approach in which the encoder predicts the ""average voice"" is proposed.  This idea is interesting and novel.\n(4) For the decoder part, the diffusion-based method is adopted which reconstructs the converted voice from the ""average voice"".\n(5) To solve the problem of slow inference of the diffusion probabilistic model, a novel inference scheme is developed which significantly reduces the number of iterations.\n(6) A novel stochastic differential equations solver, named maximum likelihood SDE solver, is proposed with theoretical analysis and empirical studies.\n(7) A lot of experiments have been conducted to validate the effectiveness of the proposed method.\n(8) Besides the experiments on VC, the maximum likelihood sampling scheme is also validated by the CIFAR-10 image generation task.\n', 'summary_of_the_review': 'Based on the above main review (especially the strengths of the paper), the proposed method is novel enough and the experiments can well support the effectiveness of the proposed method.  think the paper could be accepted for publication on ICLR.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper tackles the problem of one-shot many-to-many voice conversion (with both unseen source and target speakers) using a diffusion model on mel-spectrograms. The authors propose a dedicated architecture able to generalize to unseen speakers naturally (without relying on Phoetic Posteriorgrams (PPGs) as in previous works) by conditioning the diffusion model on ""average-phoneme"" spectrograms together with the target speech. They also address the important problem of the errors arising from using the Euler-Maruyama scheme with large discretization steps by proposing a novel SDE solver, allowing to perform voice conversion with as few as 6 diffusion steps. They demonstrate the applicability of this solver on other modalities (CIFAR-10) using on unconditional model. Extensive and rigorous experiments are conducted on the voice conversion task and an accompanying web page is provided with numerous and convincing voice conversions.', 'main_review': 'This paper is well-written and remarkable on many aspects:\n- A novel SDE solver than can be of interest for anyone interested in Diffusion models is proposed. This solver is provably better than Euler-Maruyama discretization in the setting of a small number of diffusion steps,\n- The idea to condition the diffusion model on the ""average voice"" is novel and fully exploited by the proposed architecture,\n- There are extensive experiments that demonstrate the relevance of this approach,\n- A website showcasing many voice conversions is available and the examples are convincing,\n- The relevant literature is properly addressed,\n- Code will be available.\n\nSmall remarks:\n- p. 2: Chen et al. 2021a also reported results (for vocoding) using only as few as 6 diffusion steps.\n- ""most of them lack theoretical grounds"" may be a bit harsh.\n\n\n\n', 'summary_of_the_review': 'A paper of high quality, featuring substantial contributions that can be valuable to anyone interested in diffusion models. Code and models are available and may have practical usages.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'Same potential harmful applications as any voice conversion method.', 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme', 'authorids': ['~Vadim_Popov1', '~Ivan_Vovk1', '~Vladimir_Gogoryan1', '~Tasnima_Sadekova1', '~Mikhail_Sergeevich_Kudinov1', '~Jiansheng_Wei1'], 'authors': ['Vadim Popov', 'Ivan Vovk', 'Vladimir Gogoryan', 'Tasnima Sadekova', 'Mikhail Sergeevich Kudinov', 'Jiansheng Wei'], 'keywords': ['speech', 'voice conversion', 'diffusion models', 'stochastic differential equations'], 'abstract': 'Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis.', 'pdf': '/pdf/468145b46e459c5ba69e7017b6ef4eaece277e94.pdf', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'popov|diffusionbased_voice_conversion_with_fast_maximum_likelihood_sampling_scheme', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/diffusion-based-voice-conversion-with-fast/code)', '_bibtex': '@inproceedings{\npopov2022diffusionbased,\ntitle={Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme},\nauthor={Vadim Popov and Ivan Vovk and Vladimir Gogoryan and Tasnima Sadekova and Mikhail Sergeevich Kudinov and Jiansheng Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=8c50f-DoWAu}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Yusong Wu', 'Ethan Manilow', 'Yi Deng', 'Rigel Swavely', 'Kyle Kastner', 'Timotheus Cooijmans', 'Aaron Courville', 'Anna Huang', 'Jesse Engel']",ICLR,MIDI-DDSP_ Detailed Control of Musical Performance via Hierarchical Modeling,https://iclr.cc/virtual/2022/oral/6967,2022," Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance,  and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience.",Oral 1: AI Applications,https://openreview.net/pdf?id=UseMOjWENv,https://openreview.net/forum?id=UseMOjWENv,UseMOjWENv,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposed MIDI-DDSP, a structured hierarchical generative model which offers both detailed expressive controls (as in traditional synthesizers) as well as the realistic audio quality (as in black-box neural audio synthesis). Overall the reviews are very positive. All the reviewers unanimously agree that the paper is very well-written and presented a very convincing model and a meaningful step-up from the earlier work of DDSP. The authors also presented a well-documented website for the project and promised to release the source code. The reviewers raised some clarifying questions and minor corrections which the authors addressed during the response. Therefore, I vote for accept.'}, {'title': 'Author Response to Reviewer FkaH', 'comment': 'We thank you for the review and the generous comments. We\'ve done our best to address your questions with paper revisions and the comments below. \n\n>"" In paragraph 3.3, the authors write that the note expression controls are pooled over the duration of the corresponding note. To my understanding, the controls would rather be unpooled, repeated or upsampled, as there are more frames than notes (very clear in B.4).""\n\nHuge thanks for pointing it out! This is indeed a typo, and we have changed the sentence to ""Note expression controls are unpooled (repeated) over the duration of the corresponding note to make a conditioning sequence…"" in the revised version of the manuscript.\n\n>"" The paragraph about the dataset doesn\'t mention that it also contains the MIDI ground truth. This could be made explicit.""\n\nThis is indeed an oversight. We have changed the text to now highlight the inclusion of MIDI ground truth to the introduction of the dataset. We changed the introduction to the dataset to ""The URMP dataset contains solo performance recordings and their ground truth note boundary of 13 string instruments and wind instruments, which allows us to test generalization to different instruments.""\n\n>"" The paragraph entitled ""Expression Generator"" in 4.2 features what looks like a residue of an unwanted sentence""\n\nThanks! We\'ve fixed them in the revised version of the manuscript.\n\n>"" In Appendix B.3, the definition for brightness seems to be lacking a sum over k, and I\'m having a hard time trying to understand the multiplication by i rather than k.""\n\nThanks a lot for pointing out the error in the definition! The definition of the brightness should be $\\frac{1}{T_n} \\sum_i^{T_n} \\sum_{k=1}^{60} k\\cdot h^k(i)$. We’ve fixed them in the revised version of the manuscript.\n\n>"" Some of these expression controls naturally lie in [0;1] (e.g. Volume Peak Position). However, it is not clear how others are normalized (e.g. Vibrato). If normalizing constants are precomputed using feature extraction on the whole dataset, it should be written.""\n\nWe\'ve added more details to clarify the text in this regard. All the expression controls are linearly scaled by adding an offset and divided by a coefficient. We determine the parameter for the scaling by computing the histogram of expression controls and adjusting the range to fit almost all points in [0,1]. We avoid simply normalizing the expression controls by min-max normalization to [0,1] because the long-tail effect and outlier values will narrow the effective control range, and the value adjustment is less distinguishable for users.\nThere is nothing special per-se about this approach, and other scaling methods such as normalizing by subtracting the mean and dividing by standard deviation should also work. We are planning to find a method for automatic note expression control scaling to allow the model to train on arbitrary datasets.\n'}, {'title': 'Author Response to Reviewer Z9iV', 'comment': 'We thank you for the review and the generous comments. We\'ve done our best to address your questions with paper revisions and the comments below. \n\n>"" While reading the paper, I was wondering the three sub-modules are trained simultaneously. If not, can other dataset be used for training each sub-module to enhance robustness or the performance?""\n\nThank you for the thoughtful suggestion! Yes, the three sub-modules in MIDI-DDSP are separately trained (further detailed in Figure 3). There\'s no reason why one could not use other datasets to train/pre-train each module separately to enhance the robustness or even learn a different style of playing. Hopefully, we can explore this idea in future work.\n\n>"" For synthesis generator module, I couldn\'t find some ablation test about the loss function.""\n\nThanks very much for considering this problem. We have additional qualitative results of this ablation of loss function available here: https://midi-ddsp.github.io/#loss-ablation.  We compare the original MIDI-DDSP trained on multi-scale spectral loss and GAN objective with two variants: ""MIDI-DDSP Params Loss"" (Loss on synthesis parameters instead of the multi-scale spectrogram, also with GAN loss) and ""MIDI-DDSP without GAN"" (multi-scale spectrogram loss, but no GAN). We found that ""MIDI-DDSP params loss"" has a similar sound quality to the original MIDI-DDSP, while MIDI-DDSP without GAN produced worse output quality (duller, less realistic sounds).\n\nFull audio samples can be found here: https://github.com/MIDI-DDSP/MIDI-DDSP.github.io/blob/master/eval_set_sample_for_loss_ablation.zip.\n\n>"" The authors argue that the method can be easily extended when the multi-instrument transcription model is ready. However, I see there exist many challenges (e.g. the model relies on CREPE in the DDSP synthesis part, also the type of an instrument set between transcription part and MIDI-DDSP should be matched.), the tone can be lowered.""\n\nMany thanks for pointing this out. We agree that the original language overstated the ease of adapting to polyphony. We have softened the language to ""It is important to note that the system relies on pitch detection and note detection, so is currently limited to training on recordings of single monophonic instruments. This approach has the potential to be extended to polyphonic recordings via multi-instrument transcription and multi-pitch tracking, which is an exciting avenue to explore for future work."".""\n\n>"" In page 3, ""Realistic Note Synthesis"", ""a"" is missing in a word \'concatenative\'.""\n\nThanks! We\'ve fixed them in the revised version of the manuscript.\n'}, {'title': 'Author Response to Reviewer 6Ata', 'comment': 'We thank you kindly for reviewing the manuscript and for your generous comments. We\'ve done our best to address your questions with paper revisions and the comments below.\n\n>”One question I have is about the choice of 3 stage modeling (notes -> expression -> synthesis -> audio) as opposed to the 2 stage process used by MIDI2Params (notes -> synthesis -> audio). I appreciate that injecting semantically meaningful performance features provides an additional level of control over the generation process. Does this additional stage also account for the performance improvement of MIDI-DDSP over MIDI2Params? Or is this improvement more attributable to better/bigger models?""\n\nThanks very much for considering this problem. To answer this question, we added an additional ablation experiment without using note expressions, where the synthesis generator is trained to directly use the note sequence instead of conditioning on a combination of note sequence and note expressions. This ablation is trained with the same loss and GAN objective and uses the same hyperparameters as MIDI-DDSP. Qualitatively, the ablation audio sounds similarly good as MIDI-DDSP. However, occasionally it will generate notes lacking almost any expression (constant f0 and loudness throughout the whole note) that sound less realistic. (please see https://midi-ddsp.github.io/#expression-ablation for further details). \n\nThus, we conclude that the expression controls slightly increase the quality of automated synthesis. More importantly, they also enable a user to make edits at the note expression level, and these new controls ultimately enable more natural-sounding synthesis; this type of editing is not an option in MIDI2Params or the ablation.\n\nYou can find all experiment result samples at https://github.com/MIDI-DDSP/MIDI-DDSP.github.io/blob/master/no_expression_experiment.zip. \n\n>"" Also, related to the previous point, I am a little confused by the MIDI2Params comparisons in Table 1. I am under the impression that the expression step is an innovation of this work (Section 3.2). How are you able to make these breakdown comparisons with MIDI2Params for these substages notes -> expression and expression -> synthesis. Have I misunderstood MIDI2Params?""\n\n\nGiven a set of notes, MIDI2Params will generate f0 and loudness contours. Then the DDSP decoder will predict synthesis parameters from those generated f0/loudness curves.\n\nFor notes -> expression, we evaluate teacher-forced prediction accuracy of the note expression module in MIDI-DDSP. For MIDI2Params, no such expression controls exist, so for evaluation, we use teacher-forced prediction accuracy in a manner that is as similar to our system as is possible. We feed in frame-wise notes and previous ground-truth frame-wise output (f0 and loudness) to auto-regressively generate the next frames for the next note. After the output of the next note is generated, we feed them into the DDSP decoder as usual to get the synthesis parameter. We then evaluate the note expression control accuracy by extracting them from synthesis parameters generated from MIDI2Params.\n\nFor expression -> synthesis, we are evaluating the reconstruction performance of the synthesis generator. MIDI-DDSP has additional conditioning (note expression + notes) compared to MIDI2Params (notes), which makes it not a completely apples-to-apples comparison, but it does fairly highlight the advantage of the three-stage generation over the two-stage model.\n'}, {'summary_of_the_paper': 'This paper extends the DDSP generative modeling approach (Engel et al., 2020) to higher levels of abstraction for high-level MIDI -> audio synthesis with intermediate levels of semantic control over the generation process.', 'main_review': 'This paper develops a parametric model of musical audio that leverages insight into the music domain to construct meaningful featurizations of music, together with models trained to invert these featurizations, to construct a hierarchical autoencoder with meaningful semantic features that can be used to control the music generation process.\n\nThe technical contributions of this paper consist of a semantic ""expression"" featurization of DDSP parameters (Section 3.2) and two inverse models: (1) a synthesis generator for converting the expression features back to DDSP synthesis parameters and (2) an expression generator for converting MIDI data back to expression features. The features and models are well-described at a high level in the main paper, and the appendix provides a thorough & precise description of the remaining details. The empirical study seems convincing, with comparisons to both academic prior work (MIDIParams) and commercial software (Ableton, FluidSynth). And the demos sound qualitatively good to my ear.\n\nThe paper thoroughly relates this work to related work in the field, making many thoughtful connections to both the symbolic & audio generative modeling literature. These connections both help situate this work in the broader literature, and also provide some welcome scaffolding for thinking more broadly about the field of generative music modeling. I also appreciate the connections to the signal processing literature.\n\n\nOne possible criticism of this paper is that, because its focus is on applying machine learning rather than in the development of machine learning techniques, perhaps it would be better suited for a music domain conference (e.g., ISMIR). I strongly support its publication at ICLR: the paper does a thorough job situating itself in the machine learning literature, making it accessible and appealing to the broader machine learning audience.\n\n\nOne question I have is about the choice of 3 stage modeling (notes -> expression -> synthesis -> audio) as opposed to the 2 stage process used by MIDI2Params (notes -> synthesis -> audio). I appreciate that injecting semantically meaningful performance features provides an additional level of control over the generation process. Does this additional stage also account for the performance improvement of MIDI-DDSP over MIDI2Params? Or is this improvement more attributable to better/bigger models? \n\nAlso, related to the previous point, I am a little confused by the MIDI2Params comparisons in Table 1. I am under the impression that the expression step is an innovation of this work (Section 3.2). How are you able to make these breakdown comparisons with MIDI2Params for these substages notes -> expression and expression -> synthesis. Have I misunderstood MIDI2Params?', 'summary_of_the_review': 'This is a well-written, creative paper. The models are interesting. The figures are excellent. The connections to related work are extensive and thoughtful.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': ""This paper proposes a music performance modeling network using three sub-modules which are expression generator, synthesis generator, and DDSP inference. The idea of using these three sub-modules to create three-level performance modeling (the three-level control values are notes, performance features, and synthesis parameters) is interesting and the result shows that the proposed model can give more ability of control while not harming the overall audio generation performance. It seems the proposed method that having three trainable networks with two hand-crafted methods (actually note detection is not the part of the contribution of this paper, so I didn't count it) gives full pipeline of hierarchical modeling and this is a good research contribution."", 'main_review': 'The paper is well-written, so I only have few minor questions.\n\nWhile reading the paper, I was wondering the three sub-modules are trained simultaneously. If not, can other dataset be used for training each sub-module to enhance robustness or the performance?\n\nFor synthesis generator module, I couldn\'t find some ablation test about the loss function. \n\nThe authors argue that the method can be easily extended when multi-instrument transcription model is ready. However, I see there exist many challenges (e.g. the model relies on CREPE in DDSP synthesis part, also the type of an instrument set between transcription part and MIDI-DDSP should be matched.), the tone can be lowered.\n\nIn page 3, ""Realistic Note Synthesis"", ""a"" is missing in a word \'concatenative\'.\n', 'summary_of_the_review': 'The paper opens a more user controllability in music performance modeling and this is a nice contribution. Also, the idea of having rule-based method with trainable networks in a series of a chain seems useful in many other applications.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper presents a controllable rendering engine for MIDI files, based on the DDSP framework. Given F0 and loudness contour, DDSP can estimate the parameters of a harmonic + noise synthesis model, to render a corresponding audio file. Similar to MIDI2Params, which predicts framewise FO and loudness contours from a MIDI file, MIDI DDSP introduces an intermediate hierarchical level, allowing to control some newly introduced ""expression controls"". A mapping from MIDI files to ""expression controls"" is learnt, so that MIDI files can be automatically rendered. However, because these note wise controls are explicit, they also allow human manipulation of the performance rendering.\nInfluence of the expression controls is assessed with a correlation study, showing that the human manipulation has the expected effect on the generated performance. This quantitative evaluation is further confirmed by convincing audio examples.', 'main_review': 'This paper introduces a layer of controllable expression parameters for the audio rendering of MIDI files. The contribution is threefold:\n1. The authors introduce heuristics to extract note-wise expression parameters from low-level synthesis parameters. The choice of the controls are relevant to the dataset the authors use (acoustic instruments).\n2. They introduce the synthesis generator, allowing to predict the frame-level synthesis parameters, hence bypassing the DDSP decoder that used to predict them from F0 and loudness contours. The introduction of this model allows a better reconstruction error than the original DDSP. Also, it transfers the manipulation of the sound from the mentioned contours (which are frame-level) to note-level expression controls, more expressive and controllable.\n3. The smart design of the expression controls allows them to be controlled by humans, hence allowing further manipulation of the synthesis. Each control is well defined, normalized between 0 and 1, and coarse enough (note-level vs. frame-level) so that a user can easily set them to their preferred value, enhancing the customization possibilities. Human manipulation of the controls seem to have the desired effect, as shown by the correlation study in Table 2 and further supported by the audio material.\nA few comments:\n- In paragraph 3.3, the authors write that the note expression controls are pooled over the duration of the corresponding note. To my understanding, the controls would rather be unpooled, repeated or upsampled, as there are more frames than notes (very clear in B.4).\n- The paragraph about the dataset doesn\'t mention that it also contains the MIDI ground truth. This could be made explicit.\n- The paragraph entitled ""Expression Generator"" in 4.2 features what looks like a residue of an unwanted sentence\n- In Appendix B.3, the definition for brightness seems to be lacking a sum over k, and I\'m having a hard time trying to understand the multiplication by i rather than k.\n- Some of these expression controls naturally lie in [0;1] (e.g. Volume Peak Position). However, it is not clear how others are normalized (e.g. Vibrato). If normalizing constants are precomputed using feature extraction on the whole dataset, it should be written.\n', 'summary_of_the_review': 'Overall, this works is very well written. Everything should appear pretty clear to anyone familiar with the DDSP framework. The principle is simple yet extremely effective, as it improves reconstruction quality and parameters estimation compared to the state of the art. This new control layer represents a lot of added value compared to MIDI2Params, which was already a great addition to the original DDSP framework. In addition to the paper, the website provides very convincing audio examples which further assess the quality of this work.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling', 'authorids': ['~Yusong_Wu1', '~Ethan_Manilow1', '~Yi_Deng4', 'rigeljs@google.com', '~Kyle_Kastner1', '~Tim_Cooijmans1', '~Aaron_Courville3', '~Cheng-Zhi_Anna_Huang1', '~Jesse_Engel1'], 'authors': ['Yusong Wu', 'Ethan Manilow', 'Yi Deng', 'Rigel Swavely', 'Kyle Kastner', 'Tim Cooijmans', 'Aaron Courville', 'Cheng-Zhi Anna Huang', 'Jesse Engel'], 'keywords': ['Audio Synthesis', 'Generative Model', 'Hierarchical', 'DDSP', 'Music', 'Audio', 'Structured Models'], 'abstract': 'Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance,  and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wu|mididdsp_detailed_control_of_musical_performance_via_hierarchical_modeling', 'pdf': '/pdf/e26b385d95d67af36d02a385047be6f7a0d6f47b.pdf', 'one-sentence_summary': 'Controlling musical performance and synthesis with a structured hierarchical generative model', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/midi-ddsp-detailed-control-of-musical/code)', '_bibtex': '@inproceedings{\nwu2022mididdsp,\ntitle={{MIDI}-{DDSP}: Detailed Control of Musical Performance via Hierarchical Modeling},\nauthor={Yusong Wu and Ethan Manilow and Yi Deng and Rigel Swavely and Kyle Kastner and Tim Cooijmans and Aaron Courville and Cheng-Zhi Anna Huang and Jesse Engel},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UseMOjWENv}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['X.Y. Han', 'Vardan Papyan', 'David Donoho']",ICLR,Neural Collapse Under MSE Loss_ Proximity to and Dynamics on the Central Path,https://iclr.cc/virtual/2022/oral/6353,2022," The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=w1UbdvWH_R3,https://openreview.net/forum?id=w1UbdvWH_R3,w1UbdvWH_R3,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""This paper extends the Neural Collapse (NC) phenomenon discovered by Papyan, Han and Donoho (2020) on deep learning image classifications with Cross Entropy (CE) loss, to the scenario with Mean Squared Error (MSE), that achieves similar performance to CE and favors deeper analysis. In particular, the paper shows that the least square loss can be decomposed orthogonally into a 'central' path as the optimal least square loss, and its perpendicular loss. Moreover, the paper shows by experiments that after the zero training error (Terminal Phase of Training, or TPT) the perpendicular loss is typically much smaller than the optimal least square loss, and the optimal least square loss is further decomposed into the NC1 loss which is the dominance and NC2/3 loss (even smaller than the perpendicular loss). Such a discovery with loss decomposition is very thought provoking to understand the training dynamics of deep neural networks.\n\nReviewers unanimously accept the paper, so is the final recommendation.""}, {'title': 'Raised score', 'comment': 'Hi,\n\nI decided to raise the score considering the effort put by the authors. Still, I think another iteration would benefit the message in the paper.'}, {'title': 'Comment', 'comment': 'I would like to thank the authors for their clarifications. I believe it is a good paper and I would recommend accepting it.'}, {'title': 'Re: Reply to response', 'comment': 'We are very grateful for the reviewer\'s decision to raise our score. Additionally, we thank the reviewer for these new, detailed comments on the updated paper. Based on the reviewer\'s new feedback, we have made additional revisions to the paper. The revision is uploaded to OpenReview.\n\nBelow are point-by-point replies to the reviewer\'s new comments and suggestions:\n\n1. *""The (updated) argument for NC1 makes use of the divergence of the singular vales of the SNR. Could your theory be adapted for settings, where this divergence cannot happen due to norm constraints or penalties on the features (for example induced by weight decay)?""*\n   #\n   We thank the reviewer for this thought-provoking question. As-is, there is no short, immediate adaptation of the SNR-divergence theory to the case where the norms of the features are constrained. With more derivations, we speculate that such an extension would involve (1) showing that the feature norm constraint effectively restricts the features to some mathematically-characterizable, bounded feasible region and (2) showing that the SNR would diverge until it reaches the boundary of such a region. We are definitely interested in proving such a result in future reports, but see no simple way of showing it within the scope of the current paper.\n   #\n\n2. *""Would you add some verbosity to the proof of corollary 2? \'Based on Equation 10, we deduce the within-class covariance, $\\Sigma_W$, becomes negligible compared to the between-class covariance. Therefore, the normalized matrix [...] undergoes activation collapse (NC1) \' Could you pair this with a calculation of the form $\\lim_{t\\to\\infty}\\Sigma_{B}^\\dagger(t)\\Sigma_{W}(t)=\\dots=0.$""*\n   #\n   Thank you for this feedback. Based on the reviewer’s recommendation, we now provide a detailed derivation of $\\lim_{t \\to \\infty}\\Sigma_{B,t}^\\dagger\\Sigma_{W,t} = 0$ in the proof of Corollary 2 using only results from Corollary 1.\n   #\n\n3. *""Furthermore, could you provide a short proof or reference to \'The only C-column matrices possessing C−1 equal, non-zero singular values are Simplex ETFs.\'""*\n   #\n   We thank the reviewer for this suggestion. Based on the reviewer’s recommendation, we now explicitly prove this fact in Lemma 7 of Appendix D.6---prior to the proof of Corollary 2. Additionally, we include a reference to this result in the intuitive discussion after Corollary 1 of the main text.\n   #\n\n4. *""Regarding 5. I was under the impression, that your theory still works for classification layers without bias. From your answer it seems, that this might not be the case. Could you clarify?""*\n   #\n   Thank you very much for this thought-provoking question. The reviewer indeed identified an interesting subtlety in the theory. A bias---which, as we show in (A2), performs a global mean subtraction on the features---is indeed needed to induce a Simplex ETF structure on the classifier and features. If there were no bias, the classifiers and features would not converge to a globally-mean-subtracted structure like the Simplex ETF. We speculate the limit would be instead an orthogonal matrix, although we have not formally gone-through such a derivation.\n\n   More formally, the Simplex ETF has a rank-1 nullspace due to the global-mean subtraction of the features caused by the bias. Removing the bias would prevent the global-mean subtraction and therefore eliminate the rank-1 nullspace. ""Filling-in"" that rank-1 nullspace of the Simplex ETF would result in an orthogonal matrix.\n   #\n\n5. *""I think the added footnote 11 is rather confusing and from my point of view provides no benefit … In fact, the only clarification needed here, is that $\\mathcal{X}$ is indeed a manifold (which boils down to a condition on the derivative of the covariance). Furthermore, one could be more specific, as the gradient is not projected onto $\\mathcal{X}$, but on its tangent space.""*\n   #\n   We thank the reviewer for this feedback. We have revised the text to say:\n   “...where the operator $\\Pi_{\\mathcal{T}_\\mathcal{X}}X$ projects the gradient onto the tangent space of the manifold...”\n\n   For the footnote, to address both this feedback and that of Reviewer Jnw3, we have now shortened Footnote 11 to only state that $\\mathcal{X}$ is the well-known Generalized Stiefel manifold with a relevant reference. All additional redundant text has been removed.\n   #\n\nWe again thank the reviewer for their meticulous effort in evaluating this paper as well as for their many suggestions that improved its quality. We would be happy to provide additional clarifications for any additional feedback the reviewer may have.'}, {'title': 'Reply to response', 'comment': 'Thank you for your detailed response. My concerns were adressed and I will raise my score.\n\nSome remarks regarding the updated manuscript / your comments:\n\n- The (updated) argument for NC1 makes use of the divergence of the singular values of the SNR. Could your theory be adapted for settings, where this divergence cannot happen due to norm constraints or penalties on the features (for example induced by weight decay)? \n\n- Would you add some verbosity to the proof of corollary 2? \n  *""Based on Equation 10, we deduce the within-class covariance, $\\Sigma_W( \\dots)$, becomes negligible compared to the between-class covariance. Therefore, the normalized matrix [...] undergoes activation collapse (NC1) ""*  \n  Could you pair this with a calculation of the form $\\lim_{t\\to \\infty} \\Sigma_B^\\dagger(t) \\Sigma_{W_{LS}}(t) = \\dots = 0$.  \n  Furthermore, could you provide a short proof or reference to *""The only C-column matrices possessing C−1 equal, non-zero singular values are Simplex ETFs""*.\n\n- Regarding 5. I was under the impression, that your theory still works for classification layers without bias. From your answer it seems, that this might not be the case. Could you clarify?\n\n- Regarding 8. Indeed, it\'s an interesting question. When reading the manuscript, it appeared to me, that this section was written already with the idea of a connection in mind. This is why I asked.\n\n- I think the added footnote 11 is rather confusing and from my point of view provides no benefit. Manifolds are ubiquitous in geometry and topology and therefore also appear in various contexts in machine learning.   \nWhy the clarfification? For other mathematical objects such as vectors, functions or sets one does usually not point out that there are other papers using these objects for formalizing there arguments.   \nIn fact, the only clarification needed here, is that $X$ is indeed a manifold (which boils down to a condition on the derivative of the covariance).\nFurthermore, one could be more specific, as the gradient is not projected onto $X$, but on its tangent space.  \nI am aware, that the footnote was added in response to a question by Reviewer Jnw3, but still, I suggest to remove it.\n'}, {'title': 'Re: Thanks', 'comment': ""Thank you for this response! We greatly appreciate the reviewer's kind thoughtfulness in taking time during the busy week to provide us this update.\n\nWe are happy to hear that the reviewer is satisfied with the technical details.\n\nWe look forward to hearing the reviewer's final recommendation. We are also happy to make additional revisions on the paper's presentation based on any additional feedback.""}, {'title': 'Thanks', 'comment': 'Hi,\n\nfirst of all, thank you for your positive attitude and your detailed respose. Also, sorry for my late reply, I had a few deadlines in the meantime as well.\n\nThe responses look alright. Since my critique about the work was mainly presentation, I will have to have a more detailed look about the updated manuscript. So, I cannot really tell my final recommendation, hopefully by the weekend. As far as the technical questions, however, I am covered.\n\nCheers,'}, {'title': 'Authors\' Response to Reviewer ""9pr9"" (Part 1/2)', 'comment': 'We thank the reviewer for their careful reading of this paper as well as their positive assessment of its relevance. This paper is the culmination of over a year’s worth of careful empirical and theoretical research, so we are very grateful for the reviewer’s validation of its quality.\n\n#\n\nBelow, we give point-by-point responses to the reviewer’s comments and describe the revisions we made to address them. The revised paper has been uploaded to OpenReview. We would be happy to add additional clarifications and revisions to the paper to address any additional recommendations from the reviewer. \n\n1. *""Overall, the paper is well-written, but there are some places that are not easy to follow, for example, Section 1.1. I would recommend the authors to state the problem and all the necessary notions first and only after this to introduce what NC is. The first sentence of the introduction also does not sound good.""*  \n    #\n   We thank the reviewer for this feedback. Following the reviewer’s feedback, we have rewritten Section 1.1 to introduce all necessary notions first before introducing NC. Additionally, we have rewritten the first sentence.  \n    # \n2. *""The authors investigate the NC phenomenon from the point of view of the last classifier layer only and do not consider how DNNs generate the features that are passed as input to this classifier.""*  \n    #\n   Yes, we agree with the reviewer that this work does not completely capture the multi-layer aspect of deep nets.  \n    #\n   On the other hand, the unconstrained features model *does* give insights into the ability of deep networks to perform collapse because it models the interaction between deep net feature engineering and classifiers during training. In particular, the unconstrained features model is a mathematical model of feature engineering mechanisms that have unlimited flexibility. This flexibility is exactly motivated by the overparameterization and strong-nonlinearity induced by the stacking of many layers in modern deep nets. At the same time, the central path characterizes the situation where network classifiers are optimal relative to a fixed set of engineered features—which is empirically observed in our supplementary experiments on canonical deep nets and architectures.  \n    #\n   This paper mathematically shows that the interaction of such a feature engineering mechanism with unlimited flexibility and an optimally-paired classifier leads to the Neural Collapse—with dynamics that can be expressed in closed form. Since realistic deep nets indeed possess both exceptional feature engineering flexibility and features-and-classifiers pairs that are effectively on the central path, the unconstrained features model on the central path is a well-motivated model for understanding the ability of deep nets to perform collapse.  \n    #\n3. *""It is still not clear how the NC phenomena are connected to generalization.""*  \n    #\n   In this paper, we show that the prevalent empirical practice of training past zero-error towards zero-loss—within the (experimentally-motivated) unconstrained features model on the central path—has various exact consequences including explicit closed-form expressions for the trajectory of the classifiers and training features. Only after developing these rigorous *descriptive* characterizations of Neural Collapse can researchers then deduce equally-rigorous *predictive* theorems regarding its connection to generalization.  \n    #\n   More specifically, armed with this paper’s precise characterization of the trained classifier, we are now using—in on-going projects—linear algebra, functional analysis, and multivariate statistics tools to investigate the performance of the trained classifier on fresh data—i.e. generalization. We plan to document this in future, follow-up reports.  \n    #'}, {'title': 'Authors\' Response to Reviewer ""9pr9"" (Part 2/2)', 'comment': '4. *""While MSE delivers similar performance to cross-entropy loss (CE), it is less used in classification settings than CE.""*  \n    #\n   We agree that deep nets are typically trained with Cross-Entropy loss rather than MSE loss. At the same time, recent papers by many researchers consider the MSE situation—such as [Demirkaya, Chen, and Oymak, 2020], [Hui and Belkin, 2020], [Poggio and Liao, 2020a,b], [Mixon, Parshall, and Pi, 2020], and [Ergen and Pilanci, 2020]. Therefore, we feel it is worth noting that scholars’ attitudes about MSE *are* changing: Partly because it has been shown—by works such as [Demirkaya, Chen, and Oymak, 2020] and [Hui and Belkin, 2020]—that training with MSE can produce well-performing networks for image-classification tasks (even the more challenging ones such as CIFAR and ImageNet).  \n    #\n   Moreover, there are insights one can glean from analyzing the MSE loss that currently go deeper than those possible with the more analytically-complex Cross-Entropy. These MSE-based findings possess scientific value since MSE-trained deep nets display many of the same empirical behaviors as Cross-Entropy-trained ones. In other words, the deeper results discovered from analyzing the MSE loss may constitute first-steps in eventually extending the derived theoretical insights to broader, more-complex settings.  \n    #\n5. *""In Section 1.3 I would recommend adding references to datasets and architectures.""*  \n    #\n   Thank you for this suggestion. Following the reviewer’s suggestion, we now add datasets and architecture references in Section 1.3.  \n    #\n6. *""I would recommend adding an explanation into Section 1.2 about how classification performed when the model is trained with MSE loss.""*  \n    #\n   Thank you for this suggestion. Our experiment in Table 1 (analogous to Table 1 of [Papyan, Han, Donoho 2020]) of the appendix shows that models trained with MSE loss perform comparably when compared to the models trained with CE loss. Following the reviewer’s suggestion, we now mentioned this fact explicitly in Section 1.2.  \n    #\n7. *""Could you please clarify for me whether NC4 is an independent condition or it follows from (NC1-NC3)?""*  \n    #\n   Thank you for this question. NC4 indeed follows from NC1-3. This is explicitly shown within the proof of Theorem 1 in [Papyan, Han, and Donoho 2020] (starting from their Equation 7b).  \n    #\n8. *""Could you please clarify how condition (A2) is related to empirical observations?""*  \n   #\n   In our discussion of (A2), we show that biases on the central path are effectively doing global mean subtraction. In our Figure 2 measurements, we showed that the deviation from the central path is negligible. Therefore, we see that an empirical network is effectively acting as a bias-less network where the features have been globally-mean-subtracted—leading to (A2).\n   #\n\nWe again express our deep gratitude to the reviewer for their careful reading of the paper and positive affirmation of the quality of our work.  Given the above description of the relevance of MSE-based theoretical research, the potential insights from the unconstrained features model, as well as the revisions we made to improve the paper, might the reviewer consider raising their score even more to help this work gain acceptance? '}, {'title': 'Authors\' Response to Reviewer ""pKnQ"" (Part 1/2)', 'comment': ' We thank the reviewer for their careful reading of this paper as well as the meticulous efforts dedicated to examining the proofs. Based on the reviewer’s questions, comments, and recommendations, we have made many revisions that significantly improved the quality of the paper. The revised paper has been uploaded with this submission. In particular, addressing the reviewer’s main concern, we show that our theoretical results are indeed correct by (1) presenting a more precise formalization of (NC1) than that used by [Papyan, Han, Donoho 2020] and (2) now proving explicitly in Appendix D.6 that the renormalized gradient flow does not change the left and right singular vectors of the renormalized SNR matrix. \n\n#\nBelow, we provide point-by-point responses to the reviewer’s feedback and comments as well as the revisions we made to address them. We would be happy to add additional clarifications and revisions to the paper to address any additional recommendations from the reviewer.\n \n#\n\n 1. *""The main result (Section 3.3) is formalized in terms of the singular values of the SNR matrix. However, the interpretation is not obvious. I would advice to expand on the footnote 9 and move it to the main text as a brief discussion of the findings.""*  \n    #\n    Thank you for this suggestion. Following the reviewer’s suggestion, we moved footnote 9 to the main-text and elaborated upon the interpretation.  \n    #\n\n2. *""There is an additional assumption which is required for Theorem 3, which is not mentioned in the statement and the subsequent discussion, but somewhere later: The within-class covariance $\\Sigma_W$ is full rank. While one expects the assumption to be fulfilled, this assumption should still be stated more prominently.""*  \n    #\n    Thank you for this feedback. We now explicitly state that $\\Sigma_W$ is full-rank immediately preceding the statement of Theorem 3.  \n    #\n\n3. *""It is claimed that Corollary 1 and Eq. 11 imply neural collapse. So in particular, they imply (NC1): $\\Sigma_W$→0. However, these corollaries are concluded from the dynamics specified in Equation 5, where the representations are always such that $\\Sigma_W=I$ is constant.""*  \n    #\n   Thank you for pointing this out. We now state the definition of (NC1) more precisely as the convergence of $\\Sigma_B^\\dagger \\Sigma_W$ to 0 with training. This is, in fact, the actual empirical quantity measured by [Papyan, Han, and Donoho, 2020] in their Figure 6 when demonstrating the occurrence of (NC1). It is also more intuitive as the classification performance is determined by the size of the ""noise"" (captured by $\\Sigma_W$) relative to the size of the class-means (captured by $\\Sigma_B$). We suspect that [Papyan, Han, and Donoho 2020] only used $\\Sigma_W \\to 0$ for rhetorical simplicity.  \n    #\n   This more precise formalization of NC1 resolves the apparent discrepancy noted by the author. Indeed, we prove the non-zero singular values of the SNR matrix $\\Sigma_W^{-0.5} M$ tend to infinity, which is equivalent to saying that $\\Sigma_B^\\dagger \\Sigma_W$ tends to zero (because $\\Sigma_B=MM^T/C$), which implies the revised definition of (NC1).   \n    #\n\n4. *""The first limit in Corollary 2, i.e. the statement that the singular vectors of SNR stay constant with respect to t, is reasoned for by the fact that $L(w(t))$ only depends on the singular values (and not the singular vectors) and so its gradient does not depend on the singular vectors. However, the dynamics in Eq 5 do not only depend on the gradient, but also on an projection operator, which might change the singular vectors.  \n    Intuitively, the dynamics in Eq 5 correspond to gradient updates followed by a renormalization step, i.e. matrix multiplication such that $X$ has identity covariance. This multiplication might possibly chance the singular vectors.""*  \n    #\n    We thank the reviewer for their feedback and the meticulous reading of our supplementary material and its proofs. We now prove a new Lemma 6 (in Section D.6, immediately before the proof of Corollary 2) explicitly showing that the renormalized gradient flow will not change the singular vectors of the SNR matrix. We revised the proof of Corollary 2 to be more precise by invoking Lemma 6.  '}, {'title': 'Authors\' Response to Reviewer ""pKnQ"" (Part 2/2)', 'comment': '5. *""It might be a good idea to reduce the notation in section 2, by considering only the case of zero bias and move the general case to the appendix. In Section 3, the setting is reduced to the case of no bias term anyway.""*  \n    #\n   We thank the reviewer for this suggestion. The bias-term in Section 2 and corresponding Figure 2 (measured on deep nets with bias) plays a key role in justifying our theoretical assumptions in Section 3. In particular, Figure 2 shows that real deep nets (with bias) converge to having weights and biases on the central path (defined with bias). In Section 3, to justify assumption (A2), we then explain that having biases on the central path is equivalent to having mean-subtracted features with no biases at all. Given the key role of the bias in justifying the relevance of our theory, we ask for the reviewer’s understanding for our choice to incorporate bias in Section 2.  \n    #\n\n6. *""Is symmetry really required for equation 8 to hold, or does invertibility suffice?""*  \n    #\n   Thank you for this question. The reviewer is correct: Invertibility will indeed suffice for Equation 8 (which is now Equation 7 in the revision). Within this paper, we choose to consider only symmetric $A$ as we will later be interested exclusively in the case where $A = \\Sigma_W$.  \n    #\n\n7. *""Could you explain the terminology central path (Equation 4)? I guess, central refers to the least-squares optimality, but why is it a path? It does not appear to be one dimensional.""*  \n    #\n   During optimization, the classifier-feature pair ""traverses"" a path where the classifier remains MSE-optimal relative to the features i.e. the classifier-feature tuple is ""following a central path"". Therefore, we call the entire set where this classifier-features optimality holds the ""central path"" so that any path landing within this set will then, rhetorically, be ""on the central path"".   \n    #\n\n8. *""In appendix D, there is a rather informal discussion on the feature space as a fiber bundle. Can the renormalized gradient flow be formalized as a connection on this bundle?""*  \n    #\n   We thank the reviewer for proposing this idea: We find it fascinating. In the process of writing the paper, we have primarily thought of modeling the training dynamics as a flow on the features. We adopt only the language of differential geometry to specify the tools we use to complete the analysis. We agree it is interesting to consider whether the renormalized gradient flow might be formalized into a connection on the fiber bundles. An answer to this question is not immediate to us, but we are very curious to hear any ideas the reviewer might have in mind.  \n    #\n\n9. *""There is a typo in Corollary 1. The second limit should depend on c3 instead of c4.""*  \n    #\n   Thank you very much. This is indeed a typo and we have fixed it.\n    #\n\nWe again express our gratitude to the reviewer for their dedicated time and effort in examining the content and proofs of this paper. Given our demonstration of the correctness of our proofs as well as the other clarifications above, might the reviewer consider raising their score for this paper and recommending it for acceptance? \n\nWe would also be happy to provide additional clarifications to any details of the proofs that the reviewer thinks should be further elaborated upon.'}, {'title': 'Authors\' Response to Reviewer ""pHXk""', 'comment': 'We thank the reviewer for their positive assessment of this paper. Based on the reviewer’s feedback, we have revised the paper to better clarify our contributions as well as the intended description of Figure 2. The revised paper has been uploaded to OpenReview. \n\n#\nBelow, we provide point-by-point responses to the reviewer’s feedback. We would be happy to add additional clarifications and revisions to the paper to address any additional recommendations from the reviewer. \n\n#\n\n  1. *""I think the paper contains too much information to be wised packed in a single 9-page conference paper. Specifically, for one of its main contribution, namely the empirical study of the NC with MSE loss, almost all the supportive experiments are deferred to the Appendix, while the main body of the paper only focuses on explaining the theoretical part. I have doubts on such practice (claim the contribution are two-fold while only mainly presenting one of them in the main paper). Note that the authors also admit in Section 1.3 that the empirical study is too long to be included in the paper.""*  \n\n     #\n     We agree with the reviewer on this aspect. We had previously tried to publish the MSE experiments within a stand-alone paper, but reviewers felt that the empirical experiments themselves lacked novelty. Therefore, they are now a supplementary part of this theoretical work. In response to the reviewer’s feedback, we revised our abstract to clarify that the empirical experiments are a preliminary demonstration rather than a main contribution. We also removed the experiments from the bullet list of main contributions in Section 1.3, and only discuss it afterwards while clearly explaining that it is an supplementary deliverable.  \n     # \n\n  2. *""Some statements need more clarification. In the legend of Figure 2, the term ""Lperp"" should be referred to as $L_{\\text{LS}}^\\perp$ in the caption.""*  \n\n     #\n     Thank you for pointing this out. We have fixed have fixed all typos in the legend.\n     #\n\n  3. *""Also, in the caption it says ""early in the training, $L_{\\text{LS}}^\\perp$ becomes negligible compared to the dominant term LNC1""; however, I don\'t see this from the figure: for many of them, the $L_{\\text{LS}}^\\perp$ curve is on the above of LNC1 during the early phase of training. How to explain this?""*  \n\n     #\n     Thank you for bringing this to our attention. What we intended to say is that the phenomenon where $L_{\\text{LS}}^\\perp$ is smaller than LNC1 begins to occur at an early epoch relative to the entire training process. We did not intend to say that this occurs during all early epochs of training. We have revised the wording to say ""starting from an early epoch in training"" rather than simply ""early in training"".  \n     #\n\nWe again thank the reviewer for their time and effort dedicated to reading and suggesting improvements to this paper. Given the clarifications and revisions described above, might the reviewer consider raising their score?'}, {'title': 'Authors\' Response to Reviewer ""Jnw3"" (Part 1/3) ', 'comment': 'We thank the reviewer for their careful and detailed reading of our paper. Based on the reviewer’s questions and recommendations, we have made many revisions that significantly improved the clarity of the paper. The new, revised paper has been uploaded to OpenReview. \n\n#\nBelow, we give point-by-point responses to the reviewer’s comments and describe the revisions we made to address them. We would be happy to add additional clarifications and revisions to the paper to address any additional recommendations from the reviewer.\n\n#\n\n 1. *""I would spend more time on the central path, which I find an interesting concept. What does it really mean? Are the features H supposed to be \'fixed\' or at least \'fixed within infinitesimal time steps\'?""*\n#\n    We are very happy that the reviewer shares our interest in the central path abstraction. The reviewer’s intuition is correct. For a classifier-features pair to lie on the central path, the classifier in the pair has to exactly equal the MSE-optimal classifier corresponding to the features i.e. the optimal classifier that would result from fixing $H$ (within that infinitesimal timestep) and computing an MSE-optimal classifier for that $H$. We now explicitly present this interpretation after the introduction of the Central Path in Equation 4. We thank the reviewer for this clarifying feedback.  \n#\n 2.   *""A major weakness of the work is writing and structure. While very interesting ideas are in it, and it is clear that there is a worthwhile message, it is very hard to understand in precise detail the claims, so that a \'third\' reader can derive their own insights. As one example, the intro is very technical, with lots of forward references, and extends till p4. It creates a feeling of repetition and unclarity at the same time, eg, proposition 1 is basically defined twice. Lots of different concepts, terminologies, and ideas are mixed and the text often jumps from one place to another, even referencing later parts of the text that have not been read, assuming someone does read a paper sequentially.""*  \n#\n    We thank the reviewer for this feedback. Following this suggestion, we have abbreviated the introduction (to only approximately two pages) to clarify and reduce the repetition in the paper. In particular, in Section 1.3, we removed all technical descriptions, forward references, and discussions that overlap with later claims (besides a bullet-list summary).\n#\n 3.   *""Another example is that not notation is not always clearly explained. For instance, what is the time t in equation 5? I suppose time steps during training.""*\n#\n    Thank you for this feedback. The variable $t$ represents a continuous moment in time during the training of the features and classifiers. Time is continuous since our theory examines gradient flow in continuous time rather than gradient descent in discrete time steps. In response to this feedback, we now explicitly define t immediately before its introduction in Equation (5).  \n#\n 4. *""For instance, equation 5 assumes that the features $H$ (and thus $X$) are converged, that is the manifold of the identity-covariance features is fixed? Or, is it assumed that in infinitesimal training steps the features $H$ (and the manifold $H$) remain roughly equal?""*\n#\n    The word ""manifold"" possesses many meanings in the literature that could lead to confusion. Some papers consider a ""manifold of features"" (consisting of transformations of all potential images after passing through multiple deep net layers) which is evolving as the network parameters are optimized. We are *not* using the word manifold in this sense and are *not* claiming that a ""manifold of features"" has converged to an identity-covariance feature manifold. Instead, the manifold in our theory is the set of all matrices having identity within-class covariance. In other words, our assumption is that the features are renormalized to have an identity within-class covariance throughout the optimization of the deep network—and therefore always stay within this set (sometimes called a Generalized Stiefel Manifold). We thank the reviewer for pointing out this ambiguity and we now explicitly clarify this distinction in Footnote 10 of the text.  '}, {'title': 'Authors\' Response to Reviewer ""Jnw3"" (Part 2/3)', 'comment': '   5. *""In practice, what does it mean to have the loss $L_{\\text{LS}}$ for given features $H$, since the features $H$ change during the training?""*\n#\n    Consider some fixed (last-layer) features $H$—for example at a fixed moment in time. Suppose, one were hypothetically allowed to optimize over the classifier $W$. Then, $L_{\\text{LS}}$ is the smallest possible value achievable on the loss, $L$, when only varying $W$. The $W$ minimizing the loss for the given $H$ is $W_{\\text{LS}}$, which depends on $H$. And the loss achieved for that given $H$ with $W_{\\text{LS}}$ is $L_{\\text{LS}}(H)$. Thank you for pointing out this potential confusion. In response to the reviewer’s feedback, we now explicitly clarify this intuition in the discussion of the Central Path after Equation (4).  \n#\n 6. *""How is this computed in the plots of figure 2?""*\n#\n    $W_{\\text{LS}}$ has a closed mathematical form, due to Webb and Lowe (1990), as a function of $H$ (Proposition 1). Therefore, in Figure 2, we can explicitly compute $W_{\\text{LS}}$—and hence $L_{\\text{LS}}$—using this closed form solution.  \n#\n 7. *""In the sentence \'As LNC1 decreases, individual activations would need to tend to their corresponding class-mean\', how is this conclusion derived? Do you assume $W_{\\text{LS}}$ to be fixed? Otherwise, $W_{\\text{LS}}$ can also reduce the loss, no (in fact, that is the point of learning)?""*\n#\n    We thank the reviewer for pointing out this subtlety. We have rewritten the relevant part of the text (the discussion below Theorem 2) to clarify this intuition. In particular, the intuition is as follows: First, observe that $L_{\\text{NC2/3}}$ is a function of the class-means and MSE-optimal classifiers. Minimizing $L_{\\text{NC2/3}}$ will push the (unextended) class-means and classifiers towards the same Simpex ETF matrix. Next, note that the within-class variation is independent of the means. Thus, despite the fact that classifiers are converging towards some (potentially large) ETF matrix, we can always reduce LNC1 by pushing $\\Sigma_W$ towards 0 i.e. (NC1). Thus, to simultaneously minimize both components, (NC1)-(NC3) must all occur.  \n#\n 8. *""Although that is a point for the original NC paper, I think NC4 is self-evident.""*\n#\n    Yes, we agree.  \n#\n 9. *""Focusing on the figure 2, it shows that NC2/3 is much much smaller. Does this mean that the model basically distributes features \'uniformly\' early on (thus NC2/3 goes to zero fast) and from thereon, it tries to group/cluster each class features as much as possible?""*\n#\n    Yes, this intuition is indeed correct. To make this insight explicit to readers, we have added it to the caption of Figure 2. We thank the reviewer for this feedback.  \n#\n 10. *""What about overfitting? How well does the models generalize if keeping the training till zero loss? Doesn\'t this contradict standard practices, like \'early stopping\'? To put otherwise, what would happen if having small training sets, say 50 or 100 examples per class.""*  \n     #\n     The original NC paper (Papyan, Han, Donoho, 2020) showed empirically that training past zero error towards zero loss improves generalization. In our supplementary Table 1 in Appendix A, we also show analogous empirical findings for networks trained with MSE loss. This aligns with other related research works by [1] and [2] who have made similar observations. We have not investigated the effect of training sample size on this phenomenon and we agree it would be a very interesting direction for future research. We suspect that early stopping may indeed be useful in cases where sample size is small or the data is noisy, but, since we have not personally investigated such cases, we can not make any conclusive claims. We thank the reviewer for this thought provoking question and have added a new Footnote 2 into the main text to discuss early stopping.  \n\n     #\n     [1] The Implicit Bias of Gradient Descent on Separable Data  \n     [2] Reconciling modern machine-learning practice and the classical bias–variance trade-off  \n     #\n  11. *""Isn\'t the zero-global mean after bias $b_{\\text{LS}}$ an obvious conclusion, in that after subtracting the bias, the average is zero-mean? That is the point of the bias, no?""*  \n      #\n      Yes, we completely agree with the reviewer. We only chose to give a detailed description of this fact in the text in order to clarify this to readers from a diverse variety of backgrounds. \n      #'}, {'title': 'Authors\' Response to Reviewer ""Jnw3"" (Part 3/3)', 'comment': '  12. *""How do you obtain the $A^{-1}$ in equation 8? Is this part of some definition, or Linear Algebra? More generally, is equation (8) suppose to derive a result or to state it? How do you go from $W(AH)$ to $W(H) A^{-1}$? I think you mean to say that the $W$ operator is linear (matrix multiplication), so the multiplier $A$ can go out, but how do you derive the $A^{-1}$?""*  \n      #\n      Thank you for this question. Indeed, Equation 8 (which is now Equation 7 in the revision) derives from linear algebra and the definition of $W_{\\text{LS}}$. It is intended to demonstrate the following result: the multiplication of the MSE-optimal classifier corresponding to the features $AH$, i.e. $W_{\\text{LS}}(AH)$, by $AH$ is the same as the multiplication of the MSE-optimal classifier for the features $H$, i.e. $W_{\\text{LS}}(H)$, by $H$. We have expanded the equation to clarify these facts.  \n      #\n\n  13.   *""What is the intution behind the signal-to-noise ratio matrix in the off-diagonal elements? Class confusion so to speak?""*  \n        #\n        Thank you for this question. The off-diagonal elements of $M^T \\Sigma_W^{-1} M$ i.e., the Gram of the SNR matrix, is the class-confusion matrix. The off-diagonals of the SNR matrix itself does not possess any intuitive interpretation that we are aware of.  \n        #\n\n  14.   *""Section 3.3 is very involved and I am not sure I understand how the NC1-4 are derived. It is stated what we are to conclude from it, but no guidance is provided on why this is the case. I am not asking for the proof, but for the interpretation of the results.""*  \n        #\n        Thank you for this feedback. An intuitive summary of the reasoning is as follows:\n        #\n        Fact #1 of Corollary 1 shows that the eigenvalue of the SNR matrix $\\Sigma_W^{-0.5} M$ tend to infinity. As such, the within-class covariance $\\Sigma_W$ becomes negligible compared to the class means $M$. This is exactly NC1.  \n        #\n        Fact #3 of Corollary 1 (which is now Fact #2 in the revision) shows that all the (non-zero) eigenvalues of the SNR matrix are becoming equal. The only matrix which has equal (non-zero) eigenvalues is a Simplex Equiangular Tight Frame (ETF). Thus, this implies NC2.  \n        #\n        Given NC1-2 on the central path, NC3-4 follow directly from Theorem 1 of [Papyan, Han, and Donoho 2020].  NC4 is also self-evident from NC1-NC3, as the reviewer already noted.\n        #\n        Based on the reviewer\'s feedback, we now explicitly describe this intuition in the revised Section D.6 after Corollary 1 and before Corollary 2. We thank the reviewer for this clarifying suggestion.\n\n 15. *""For instance, in equation (11) we have the $\\omega_{\\text{max}}$ in the denominator, while #1 in Corollary 1 tells that the eigevalues go to infinity. Does this imply that the SNR divided by inifinity goes to 0?""*  \n     #\n     Thank you for this feedback. The entries of the SNR matrix tend to infinity at the same rate as omega_max. Once the SNR matrix is rescaled by $1/\\omega_{\\text{max}}$, we obtain that the rescaled matrix tends to a finite limit given by the singular vectors of the SNR matrix at initialization. This is the claim of Equation (11).\n     #\n\nWe again reiterate our gratitude to the reviewer for the above detailed recommendations, we truly feel that they led to revisions that greatly improved the quality and clarity of the paper. Given these improvements, might the reviewer consider raising their score for this paper and recommending it for acceptance?'}, {'summary_of_the_paper': ""This paper extends the recent work on Neural Collapse, using Mean Squared Error (MSE) instead of CE, as MSE is easier for analysis. With this, the paper shows that the least square loss can be decomposed into one that corresponds to a so called 'central' path (namely a set of optimal tuples (W, b, H) given H for which there is an optimal loss), and a perpendicular loss. The paper shows that the perpendicular loss is much smaller than the optimal least square loss and thus Neural Collapse appears due to the fact that the optimizer focuses on the central path. Empirical studies confirm the findings."", 'main_review': ""The strengths of the paper:\n- The phenomenon of Neural Collapse is certainly an interesting and thought-provoking and analyzing it further to understand its premises is certainly worthwhile of publication. In that sense, the contributions of this work are very relevant and welcoming.\n- The experiments seem to corroborate that the intuitions/derivations of the paper are on the right track. I particularly like the idea of decomposing the loss in two terms, one that is the 'optimal loss' and a secondary loss that accumulates the remaining errors.\n- I find the connection with the dynamics of learning a fascinating direction, and I think this is where research should focus on. It would be very interesting if indeed the dynamics of the learning, perhaps by examining the eigenvalues of the signal-to-noise ratio matrix, lead to Neural Collapse. However, I am not sure about this, because in all honesty it was hard to parse the respective subsection.\n- I would spend more time on the central path, which I find an interesting concept. What does it really mean? Are the features H supposed to be 'fixed' or at least 'fixed within infinitesimal time steps'? I believe this is the largest and most interesting contribution, and a good proof why using the MSE is relevant in this context.\n\nThe weaknesses of the paper:\n- A major weakness of the work is writing and structure. While very interesting ideas are in it, and it is clear that there is a worthwhile message, it is very hard to understand in precise detail the claims, so that a 'third' reader can derive their own insights. As one example, the intro is very technical, with lots of forward references, and extends till p 4. It creates a feeling of repetition and unclarity at the same time, eg, proposition 1 is basically defined twice. Lots of different concepts, terminologies, and ideas are mixed and the text often jumps from one place to another, even referencing later parts of the text that have not been read, assuming someone does read a paper sequentially. Another example is that not notation is not always clearly explained. For instance, what is the time t in equation 5? I suppose time steps during training. For the lack of writing clarity alone, I am not sure if the paper should be accepted, it would be a pity for the work itself.\n- It is not clear (at all) what are the 'assumptions' that are made for the sake of the analysis.\n-- For instance, equation 5 assumes that the features H (and thus X) are converged, that is the manifold of the identity-covariance features is fixed? Or, is it assumed that in infinitesimal training steps the features H (and the manifold H) remain roughly equal? In practice, what does it mean to have the loss L_{LS} for given features H, since the features H change during the training? How is this computed in the plots of figure 2?\n-- In the sentence 'As LNC1 decreases, individual activations would need to tend to their corresponding class-mean', how is this conclusion derived? Do you assume W_{LS} to be fixed? Otherwise, W_{LS} can also reduce the loss, no (in fact, that is the point of learning)?\n-- Although that is a point for the original NC paper, I think NC4 is self-evident.\n- Focusing on the figure 2, it shows that NC2/3 is much much smaller. Does this mean that the model basically distributes features 'uniformly' early on (thus NC2/3 goes to zero fast) and from thereon, it tries to group/cluster each class features as much as possible? What about overfitting? How well does the models generalize if keeping the training till zero loss? Doesn't this contradict standard practices, like 'early stopping'? To put otherwise, what would happen if having small training sets, say 50 or 100 examples per class.\n- Isn't the zero-global mean after bias b_LS an obvious conclusion, in that after subtracting the bias, the average is zero-mean? That is the point of the bias, no?\n- How do you obtain the A^{-1} in equation 8? Is this part of some definition, or Linear Algebra? More generally, is equation (8) suppose to derive a result or to state it? How do you go from W(AH) to W(H) A^-1? I think you mean to say that the W operator is linear (matrix multiplication), so the multiplier A can go out, but how do you derive the A^-1?\n- What is the intution behind the signal-to-noise ratio matrix in the off-diagonal elements? Class confusion so to speak?\n- Section 3.3 is very involved and I am not sure I understand how the NC1-4 are derived. It is stated what we are to conclude from it, but no guidance is provided on why this is the case. I am not asking for the proof, but for the interpretation of the results. For instance, in equation (11) we have the \\omega_max in the denominator, while #1 in Corollary 1 tells that the eigevalues go to infinity. Does this imply that the SNR divided by inifinity goes to 0? Generally, I am not sure what am I to take from this section.\n\n"", 'summary_of_the_review': 'All in all, the paper had very nice ideas, but the writing and presentation is suboptimal. This means that it is not ready yet for publication.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper studies the phenomenon of Neural Collapse (NC) and empirically shows that it occurs during the training of deep networks with the MSE loss. Then it theoretically analyzes NC with MSE loss by decomposing it and introducing the notion of central path. It shows a closed-form dynamics predicts NC in this setting.', 'main_review': 'Strengths:\n- The paper empirically shows that NC also occurs when training deep networks for classification using the MSE loss on five canonical datasets and three backbone networks.\n- The theoretical part of the paper to justify the NC phenomenon for the unconstrained features model seems rigorous. The decomposition of the loss function and how it is helpful in understanding NC is interesting.\n\nWeaknesses:\n- I think the paper contains too much information to be wised packed in a single 9-page conference paper. Specifically, for one of its main contribution, namely the empirical study of the NC with MSE loss, almost all the supportive experiments are deferred to the Appendix, while the main body of the paper only focuses on explaining the theoretical part. I have doubts on such practice (claim the contribution are two-fold while only mainly presenting one of them in the main paper). Note that the authors also admit in Section 1.3 that the empirical study is too long to be included in the paper. \n\n- Some statements need more clarification. In the legend of Figure 2, the term ""Lperp"" should be referred to as ""L^\\perp"" in the caption. Also, in the caption it says ""early in the training, L^\\perp becomes negligible compared to the dominant term LNC1""; however, I don\'t see this from the figure: for many of them, the L^\\perp curve is on the above of LNC1 during the early phase of training. How to explain this?', 'summary_of_the_review': 'The strength and weakness of the paper are very clear, as described above. I would give an overall score of marginally above the acceptance threshold based on its theoretical nature and serious study of the phenomenon.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'Recently, Papyan, Han and Donoho (2020) have observed that training neural networks beyond zero training error leads to simplex arrangements of the features. The submission studies this phenomenon, called neural collapse, from a theoretical perspective, when training is done via minimizing the square loss.\n\nIn particular, it is shown that the square loss can be split, such that one summand corresponds to the quality of the features (quantified by the loss of the MSE classifier on them) and that the other summand corresponds to the quality of the classification layer (quantified by the its deviationfrom the least squares classifier). Furthermore, the first summand quantifies the closeness of the features to neural collapse.\nIn a second step, the authors investigate the gradient flow induced from the first summand and derive closed form solutions.', 'main_review': '**Pro**\n\n\\+ The submission is well motivated and coherently structured. (Empirical observation -> Splitting the loss function in summands -> Examining the gradient flow from the predominant summand)\n\n\\+ The theory is able to explain, some aspects, of the empirical observations by Papyan, Han and Donoho (2020) . It separates itself from the prior work, as it contains closed form solutions of the class means (of renormalized features).\n\n\\+ The submission is relevant, as understanding why and how optimization of current neural networks works is crucial for making informed improvements in future algorithms.\n\n**Contra**\n\n\\- The main result (Section 3.3) is formalized in terms of the singular values of the SNR matrix. However, the interpretation is not obvious. I would advice to expand on the footnote 9 and move it to the main text as a brief discussion of the findings.\n\n\\- There is an additional assumption which is required for Theorem 3, which is not mentioned in the statement and the subsequent discussion, but somewhere later: The within class covariance $\\Sigma_W$ is full rank. While one expects the assumption to be fulfilled, this assumption should still be stated more prominently.\n\n\\- There is a correctness issue, due to which I currently cannot give a higher score. The conclusions might not necessarily be wrong, but I am a bit wary. I hope, this can be resolved with the author response. \n\n1. It is claimed that Corollary 1 and Eq. 11 imply neural collapse. So in particular, they imply (NC1): $\\Sigma_W \\to 0$. However, these corollaries are concluded from the dynamics specified in Equation 5, where the representations are always such that $\\Sigma = I$ is constant.\n\n2. The first limit in Corollary 2, i.e. the statement that the singular vectors  of SNR stay constant with respect to $t$, is reasoned for by the fact that $L(w(t))$ only depends on the singular values (and not the singular vectors) and so its gradient does not depend on the singular vectors. However, the dynamics in Eq 5 do not only depend on the gradient, but also on an projection operator, which might change the singular vectors.  \nIntuitively, the dynamics in Eq 5 correspond to gradient updates followed by a renormalization step, i.e. matrix multiplication such that $X$ has identity covariance. This multiplication might possibly chance the singular vectors.\n\n\n*Questions and suggestions unrelated to score:*\n\nIt might be a good idea to reduce the notation in section 2, by considering only the case of zero bias and move the general case to the appendix. In Section 3, the setting is reduced to the case of no bias term anyway.\n\nIs symmetry really required for equation 8 to hold, or does invertibility suffice?\n\nCould you explain the terminology central path (Equation 4)? I guess, central refers to the least-squares optimality, but why is it a path? It does not appear to be one dimensional.\n\nIn appendix D, there is a rather informal discussion on the feature space as a fiber bundle. Can the renormalized gradient flow be formalized as a connection on this bundle?\n\nThere is a typo in Corollary 1. The second limit should depend on $c_3$ instead of $c_4$.', 'summary_of_the_review': 'I am quite positive on the submission, however, there are two issues related to the proofs of the theorems. These issues might only be due to a misunderstanding on my part, but I am not sure. I hope the authors can clarify on this. Until then, I rate the submission as ""5: marginally below the acceptance threshold"".\n\n**Post author response update**\n\nThe authors adressed the correctness issues.\n\nRegarding 1. The authors confirmed, that Corollary 1, does *not* imply that NC1 ($\\lim_{t\\to\\infty} \\Sigma_{W,t} = 0$) and presented the remedy of replacing NC1 by a scale sensitive version, corresponding to the ratio of inner class variations to the between-class variations.  While this is a weaker result, it is still a wortwhile contribution and strongly connected to the observations by Papyan, Han and Donoho (2020).  \nIn fact, the updated definition of NC1, is precisely the quantity measured in the experiments by Papyan, Han and Donoho.\n\nRegarding 2. The authors added the missing proof to the appendix, that the singular vectors of SNR indeed stay constant when subject to the dynamics from Eq. 5.\n\nWith these two major issues out of the way I will adapt my recommendation to **8, accept**.\n\nLast but not least, I want to thank the authors for their detailed and clear responses. I really appreciate, that you took the time and effort to already updating the manuscript with proofs for my initial and follow-up questions.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors of this paper proposed a theoretical explanation of Neural Collapse (NC) for the DNNs with MSE loss. In particular, the authors showed that MSE loss can be decomposed into a sum of terms that corresponds to NC conditions and proposed the theoretical model (central path) that explains why NC emerges in DNNs with MSE under some mild assumptions.', 'main_review': 'Pros:\n\n--The authors provided extensive experiments to show that NC occurs in DNNs with MSE loss.\n\n--The authors provided motivation and explained the required conditions of the theorems and the limitations of their results. Overall, the conditions of the theorems do not look too restrictive and unrealistic.\n\n--The appendix has an extensive overview of related work.\n\n--In the paper, a new theoretical construct, central path, is proposed and analysed.\n\nCons:\n\n--Overall, the paper is well-written, but there are some places that are not easy to follow, for example, Section 1.1. I would recommend the authors to state the problem and all the necessary notions first and only after this to introduce what NC is. The first sentence of the introduction also does not sound good.\n\n--The authors investigate the NC phenomenon from the point of view of the last classifier layer only and do not consider how DNNs generate the features that are passed as input to this classifier. It is still not clear how the NC phenomena are connected to generalization. While MSE delivers similar performance to cross-entropy loss (CE), it is less used in classification settings than CE.\n\n\nAdditional comments:\n\n--In Section 1.3 I would recommend adding references to datasets and architectures.\n\n--I would recommend adding an explanation into Section 1.2 about how classification performed when the model is trained with MSE loss.\n\n--Could you please clarify for me whether NC4 is an independent condition or it follows from (NC1-NC3)?\n\n--Could you please clarify how condition (A2) is related to empirical observations?', 'summary_of_the_review': 'I would recommend accepting the paper. The paper further extends our theoretical understanding of deep learning. The result is new and supported by experiments.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path', 'authorids': ['~X.Y._Han1', '~Vardan_Papyan1', '~David_L._Donoho1'], 'authors': ['X.Y. Han', 'Vardan Papyan', 'David L. Donoho'], 'keywords': ['neural collapse', 'deep learning theory', 'deep learning', 'inductive bias', 'equiangular tight frame', 'ETF', 'nearest class center', 'mean squared error loss', 'MSE loss', 'invariance', 'renormalization', 'gradient flow', 'dynamics', 'adversarial robustness'], 'abstract': ""The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC."", 'one-sentence_summary': 'Neural Collapse occurs empirically on deep nets trained with MSE loss and studying this setting leads to insightful closed-form dynamics.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'han|neural_collapse_under_mse_loss_proximity_to_and_dynamics_on_the_central_path', 'pdf': '/pdf/75799bbe466f7240935655cbfaa930c9628a915e.pdf', 'code': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/neural-collapse-under-mse-loss-proximity-to/code)', '_bibtex': '@inproceedings{\nhan2022neural,\ntitle={Neural Collapse Under {MSE} Loss: Proximity to and Dynamics on the Central Path},\nauthor={X.Y. Han and Vardan Papyan and David L. Donoho},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=w1UbdvWH_R3}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Fan Bao', 'Chongxuan Li', 'Jun Zhu', 'Bo Zhang']",ICLR,Analytic-DPM_ an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models,https://iclr.cc/virtual/2022/oral/7167,2022," Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\times$ to $80\times$ speed up.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=0xiJLKH-ufZ,https://openreview.net/forum?id=0xiJLKH-ufZ,0xiJLKH-ufZ,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper presents an analytic approach for estimating the optimal reverse variance schedule given a pre-trained score-based model. The experimental results demonstrated the efficacy of the proposed method on several datasets across different sampling budgets. Given the recent interest in score-based generative models, I believe that the paper will find applications in various domains. I am pleased to recommend it for acceptance.'}, {'title': 'Thanks for the update!', 'comment': 'Thank you very much for the valuable suggestions and the update on the score. We highly appreciate it.'}, {'title': 'Thanks for your reply', 'comment': 'Thanks for your replies to all reviews and I have read all of them. The Monte Carlo approximation in other reviews is very common in the Bayesian community when estimating a term that has no closed-form expression and I have no concern about it. The above reply is helpful and I realize that the specific study on the conditional Gaussian with a more complicated mean is non-trivial and can be very helpful for a series of models. As other reviewers point out, this paper is novel in the community of DPMs, so I upgrade my recommendation. Also, I would keep suggesting that it is helpful to discuss some Bayesian works which also rely on similar measures between p_target and p_opt and obtain analytic results. Please also take the related works about the Gaussian Markov process into account, because it is very related to DPMs. The difference or similarities will be helpful for future works along this line.'}, {'title': 'Thanks for the update', 'comment': 'Thanks for the update. We highly appreciate it.'}, {'title': 'Response to the authors', 'comment': 'I thank the authors again for clarifying: please see **post discussion period update** in my **Summary Of The Review**.'}, {'title': 'Thanks for the valuable question', 'comment': 'Thanks for the valuable question.\n\nIndeed, in order to get the optimal trajectory for DDPM, we need to calculate every term $L(t, s)$ (see Eq.(17) in [4]) appeared in the variational bound. $L(t, s)$ has an expectation term and this term is estimated by $M$ Monte Carlo samples. Empirically, the $M$ samples are drawn from the training dataset. Here we also use $M$ to denote the number of Monte Carlo samples as Analytic-DPM.\n\n\nFormally, $L(t, s)=\\mathbb{E}_q KL(q(x_s|x_t,x_0)||p(x_s|x_t))$ when $s>0$, which can be written as \n\n$$\nL(t, s)=\\frac{d}{2} [  C_{t,s} + A_{t,s} \\mathbb{E}_q || x_0 - \\frac{1}{\\sqrt{\\bar{\\alpha}}_t}(x_t + \\bar\\beta_t s_t (x_t) ) ||^2 / d ],\n$$\n\n\nwhere $C_{t,s},A_{t,s}$ are only related to the forward variance $\\beta_1,\\cdots,\\beta_N$ and  $\\mathbb{E}_q || x_0 - \\frac{1}{\\sqrt{\\bar\\alpha_t}}(x_t + \\bar\\beta_t s_t (x_t) ) ||^2 / d$ is the expectation term to estimate. Similarly, the expectation term also appears in $L(t, s)$ when $s=0$.\n\nThe expectation term is estimated using $M$ Monte Carlo samples:\n\n$$\n\\Phi_t = \\frac{1}{M} \\sum_{m=1}^M || x_{0,m} - \\frac{1}{\\sqrt{\\bar\\alpha_t}}(x_{t,m} + \\bar\\beta_t s_t (x_{t,m}) ) ||^2 / d, \\quad  x_{0,m}, x_{t,m} \\sim q(x_0,x_t).\n$$\n\nThereby, each $\\Phi_t$ requires $M$ function evaluations. To calculate $\\Phi_t$ for all $t=1,\\cdots,N$, we need a total of $MN$ function evaluations.\n\nAs a result, the post-processing of DDPM also requires $MN$ function evaluations.\n'}, {'title': 'Response to the authors', 'comment': 'I thank the authors for clarifying most of my questions as well as incorporating my suggestions in their paper. \n\nI only want to follow up on one point: the authors say that ""[...] the DDPM with OT [4] also requires a post-processing of $MN$ additional function evaluations (see Section 4.3 in [4]) before getting the optimal trajectory. Thereby, under the OT setting, DDPM and Analytic-DDPM have the same function evaluations, and the comparison is fair.""\n\nI don\'t understand why a plain DDPM would need $MN$ function evaluations. The $M$ refers to the number of Monte Carlo samples to compute $\\Gamma_n$ (which is specific to Analytic-DDPM) so it would not apply to plain DDPM? Wouldn\'t DDPM only need $N$ function evaluations? Could the authors clarify?\n\n[4] Watson et al. Learning to Efficiently Sample from Diffusion Probabilistic Models. arXiv:2106.03802.'}, {'title': 'Thanks', 'comment': ""Thanks for the updates. I read the author's response and will keep my recommendation.""}, {'title': 'Summary of the revision', 'comment': 'We sincerely thank all the reviewers for the valuable comments, which help to improve the quality of our work. We summarize the revision in the updated version as follows:\n* We revised Section 3 to emphasize our technical contributions\n* We moved Proposition 1 to Appendix A.5, and added more discussion of it in Section 5\n* We moved some experimental details to the main paper (see Section 6) and added missing ones (see Appendix F.3\\&F.5)\n* We added more experiments on the number of Monte Carlo samples (see Appendix G.2)\n* We added more experiments on the bounds of Theorem 2 (see Appendix G.3)\n* We showed values of clipping thresholds at $n=2$ (see Table 7)\n* We added a comparison to other classes of generative models (see Appendix G.8)\n* We added discussion on the extra cost of the Monte Carlo estimate (see Appendix H.1)\n* We added discussion on the stochasticity of the variational bound (see Appendix H.2)\n* We added a comparison to other Gaussian models (see Appendix H.3)\n* We added discussion on future works (see Appendix H.4)\n'}, {'title': 'Response to reviewer ejLx', 'comment': 'We thank reviewer ejLx for the acknowledgement of our contributions and the valuable comments.\n\n## Q1: Compare the clipping at $n=2$ to bounds in Theorem 2\n\nThanks for the suggestion. We compare them as suggested and the former is 1 to 3 orders of magnitude smaller than the latter, when $K$ is small (e.g., $K<100$). Please see Table 7 of Appendix G.4 in the updated revision for details. We have added more discussion to the main paper (see Section 6) in the updated version. \n\n\n\n\n## Q2: The post-processing method can be quite expensive and the fairness under the OT setting\nThanks for the valuable comment.\n\n* Suppose we perform inference on $M_1$ samples. The relative additional cost of Analytic-DPM is $\\frac{M N}{M_1 K}$ (please see details in our response to common concern 2). In our experiments, we found that a smaller $M$ (e.g., $M=10$) is sufficient for Analytic-DPM (please see our response to common concern 1 (1.2)), making the relative additional cost small if not negligible. For instance, on CIFAR10, let $M=10$, $N=1000$, $M_1=50000$ and $K\\ge 10$, we obtain $\\frac{M N}{M_1 K}\\le 0.02$ and Analytic-DPM still consistently improves the baselines as presented in Table 2*. \n\n* Further, the additional calculation of the Monte Carlo estimate $\\Gamma$ occurs only **once** given a pretrained model and training dataset, since we can save the results of $\\Gamma = (\\Gamma_1, \\cdots, \\Gamma_N)$ in Eq.(8) and reuse it among different inference settings (e.g., trajectories of various $K$). The reuse is valid, because the marginal distribution of a shorter forward process $q(x_0, x_{\\tau_1}, \\cdots, x_{\\tau_K})$ at timestep $\\tau_k$ is the same as that of the full-timesteps forward process $q(x_{0:N})$ at timestep $n=\\tau_k$. Indeed, in our experiments (e.g., Table 1,2), $\\Gamma$ is shared across different selections of $K$, trajectories and forward processes.\n\n* Finally, the DDPM with OT [4] also requires a post-processing of $M N$ additional function evaluations (see Section 4.3 in [4]) before getting the optimal trajectory. Thereby, under the OT setting, DDPM and Analytic-DDPM have the same function evaluations, and the comparison is fair.\n\nPlease see more details in the common concern 2.\n\n[4] Watson et al. Learning to Efficiently Sample from Diffusion Probabilistic Models. arXiv:2106.03802.\n\n## Q3: How often the estimate is actually clipped\n\nThanks for the valuable suggestion. We have plotted the ratio of estimates that were clipped over different $M$ and $n$ in Figure 9 (Appendix G.3 in the revised version). For all $M$, the curves of the ratio v.s. $n$ are similar, and the estimate is clipped more frequently when $n$ is large. This is as expected because when $n$ is large, the gap between the upper bound in Eq. (12) and the lower bound in Eq. (11) tends to zero. The results also agree with the plot of the bounds in Figure 5 (Appendix G.3 in the original paper).\n\n\n## Q4: The estimate is biased even when the correct score is known\n\nThanks for the suggestion. We have explicitly mentioned it in Section 4. \n\n\n## Q5: Lower $M$ and plotting variance\n\nThanks for the suggestion. We added the results of Analytic-DPM over a wide range of $\\{1,3,10,100,10000,50000\\}$.\nUnder both the NLL and FID metrics, $M=10$ achieves similar results to that of $M=50000$. The results are presented in Table 1*. Please see details in our response to common concern 1 (1.2).\n\nBesides, we plotted the standard deviations of the estimate when $M=1,10,100$ and the results agree with those in Figure 3 in the original paper. We have added the new results to Appendix G.2 in Figure 4\\&5 in the revised version. See details in our response to common concern 1 (1.3). \n\n## Q6: The number of Monte Carlo samples used for results in Tables 1,2,3\n\nThanks for the suggestion. We set $M=50000$ on CIFAR10, $M=10000$ on CelebA 64x64 and ImageNet 64x64 and $M=1000$ on LSUN Bedroom by default without a sweep. See details in our response to common concern 1 (1.1).\n\n## Q7: The optimal variance could also be used for training\n\nThanks for the insightful suggestion. It is a promising future work and we have added a discussion in Section H.4.\n'}, {'title': 'Response to reviewer a5eg', 'comment': 'We thank reviewer a5eg for the positive comments and valuable suggestions.\n\n## Q1: On the connection to other Gaussian models and their results\n\nWe appreciate the reviewer for the valuable comment. Below, we directly compare our theoretical results to existing work, especially expectation propagation (EP) with Gaussian process (GP) (e.g., [1*]).\n\nIt is true that both EP and Analytic-DPM use moment matching as a key step to find analytic solutions of $KL(p_{target}||p_{opt})$ terms, and we provide a full proof of moment matching for completeness. However, to our knowledge, the connection of moment matching and DPMs has not been revealed in prior literature. \nFurther, compared to EP, we emphasize that it is highly nontrivial to calculate the second moment of $p_{target}$ in DPMs because $p_{target}$ involves an unknown and potentially complicated data distribution.\n\n\n* In EP with GP (e.g., [1*]), $p_{target}$ is the product of a single likelihood factor and all other approximate factors for tractability. In fact, the form of the likelihood factor is chosen such that the first two moments of $p_{target}$ can be easily computed or approximated.\nFor instance, the original EP [2*] considers Gaussian mixture likelihood (or Bernoulli likelihood for classification) and the moments can be directed obtained by the properties of Gaussian (or integration by parts). Besides, at the cost of the tractability, there is no converge guarantee of EP in general. \n\n* In contrast, $p_{target}$ in our paper is the conditional distribution $q(x_{n-1}|x_n)$ of the corresponding joint distribution $q(x_{0:N})$ defined by the forward process.\nNote that the moments of $q(x_{n-1}|x_n)$ are nontrivial to calculate because it involves an unknown and potentially complicated data distribution.\nTechnically, in Lemma 13, we carefully use the law of total variance conditioned on $x_0$ and convert the second moment of $q(x_{n-1}|x_n)$ to that of $q(x_0|x_n)$,\nwhich can be expressed as the score function surprisingly as proven in Lemma 11. This is regarded as a novel and insightful contribution to the literature of DPMs by reviewers FHw9, Xtgn and Qdy8.\n\n\nWe have revised Section 3 to emphasize our technical contributions and added a comparison to EP with GP in Appendix H.3.\n\n\n[1*] Hyun-Chul Kim and Zoubin Ghahramani. Bayesian gaussian process classification with the em-epalgorithm.\n\n[2*] Thomas Peter Minka. A family of algorithms for approximate Bayesian inference.\n\n'}, {'title': 'Response to reviewer Qdy8', 'comment': 'We thank reviewer Qdy8 for the acknowledgement of our contributions and the valuable comments.\n\n## Q1: Practical implication of Proposition 1\n\nThanks for the question. Currently, Proposition 1 is purely theoretical and its practical implication is unclear. We have clarified this in Section 5 in the revised version.\n\n\n## Q2: Whether timestep is a good metric to measure efficiency in Table 3\n\nSince the score-based models are nearly the same for all methods compared in Table 3, it is natural to compare the number of timesteps, or equivalently the number of model function evaluations, for efficiency. In fact, we have validated that the averaged time of a single model function evaluation in compared methods are almost the same, as presented in Appendix F.5 (Table 4) in the revised version. Also see our response to common concern 2 for more discussion about the efficiency.\n\n## Q3: Should the comparison to other classes of generative models be conducted or not\n\nThanks for the suggestion. The primary focus of our work is on improving the performance and efficiency of DPMs. Thereby, DPMs and their variants serve as the most direct baselines to validate the effectiveness of the proposed method. Despite this, we have added a new table (Table 10) in Appendix G.8 of the updated version to compare with other classes of generative models including GAN, VAE, Flow and EBM. As shown in Table 10, Analytic-DPM achieves competitive sample quality results among various generative models, and meanwhile significantly reduces the efficiency gap between DPMs and other models.\n\n'}, {'title': 'Response to reviewer Xtgn', 'comment': ""We thank reviewer Xtgn for the acknowledgement of our contributions and the valuable comments.\n\n## Q1: Application of this method to other data modalities\n\n\nThanks for the suggestion. It would be interesting to apply Analytic-DPM to other data modalities, e.g. speech data [1*]. We leave it for future work and have added a discussion in Appendix H.4. \n\n## Q2: Performance of Analytic-DPMs on methods that learn forward process variance schedules\n\nThanks for the suggestion. As presented in Appendix E, our method can be applied to VDMs and we're trying to reproduce VDMs. We leave it for future work. \n\n[1*] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-grad: Estimating gradients for waveform generation.""}, {'title': 'Response to reviewer FHw9', 'comment': 'We thank reviewer FHw9 for the acknowledgement of our contributions and the insightful comments.\nWe have updated our paper by adding a discussion about the bias caused by the Monte Carlo method, and clarifying the issues about the tuning procedure of $M$, sensitivity of $M$ and the additional computation cost.\nBelow, we provided a point-to-point response to all comments.\n\n\n## Q1: $f(\\hat{\\sigma}_t^2)$ as a stochastic lower bound of $\\mathbb{E}[f(\\sigma_t^2)]$ and the potential bias\n\nThanks for the valuable comment. Generally, $f(\\hat{\\sigma}_t^2)$ is not a stochastic lower bound of $\\mathbb{E}[f(\\sigma_t^2)]$, since $f$ is not concave in $\\sigma_t^2$. Actually, the ELBO can be written as a function of $\\sigma_t^2$ in the form of $f(\\sigma_t^2) = B(\\frac{A}{\\sigma_t^2} + \\log \\sigma_t^2) + C$, where $A, B, C$ are constants unrelated to $\\sigma_t^2$ and $A, B > 0$. $f(\\sigma_t^2)$ is convex when $0 < \\sigma_t^2 < 2A$ and concave when $2A < \\sigma_t^2$. Please refer to Proposition 2 of Appendix H.2 in the revised version for a formal proof.\n\nHowever, in this paper, \n$f(\\hat{\\sigma}_t^2)$ is a stochastic lower bound of $\\mathbb{E}[f(\\sigma_t^{*2})]$ because $\\mathbb{E}[f(\\sigma_t^{*2})]$ is the optimal ELBO. \nPlugging in $\\hat{\\sigma}_t^2$, the bias of $\\mathbb{E}[f(\\hat{\\sigma}_t^2)]$ w.r.t. $\\mathbb{E}[f( \\sigma_t^{*2})]$ is due to the Monte Carlo method as well as the error of the score-based model.\nThe former can be reduced by increasing the number of Monte Carlo samples. The latter is irreducible if the pretrained model is fixed, which motivates us to clip the estimate, as discussed in Section 3.1. We have revised the main text in Section 3 and added a detailed discussion in Appendix H.2 of the updated version.\n\n\n## Q2: The tuning procedure of $M$\n\nThanks for the suggestion. We do not tune $M$ in the original paper. In fact, we use a maximal $M$ without introducing too much computation. \nSee more details in the common concern 1 (1.1).\n\n## Q3: How results depend on $M$\n\nThanks for the suggestion. Based on the original results of varying $M$ in Appendix G.2 and newly added experiments in Table 5 of Appendix G.2 (or see Table 1* in the common concern 1 (1.2)), we conclude that Analytic-DPM is not sensitive to $M$ and usually a small $M$ (e.g. $M=10$) is sufficient for good results.\n\n## Q4: Large $M$ incurs more computation cost during inference\n\nThanks for the valuable comment. As mentioned in the response to Q3, usually a small $M$ is sufficient for accurate inference. Suppose that we have to use a large $M$ in the setting mentioned in the comment (i.e., inference using CPUs). Given a pretrained model and training dataset, we can calculate $\\Gamma = (\\Gamma_1, \\cdots, \\Gamma_N)$\noffline (i.e., on GPUs) and deploy it together with the pretrained model. Consequently, the online inference cost of Analytic-DPM is exactly the same as DPM. In fact, in the paper, we calculate $\\Gamma$ first and reuse it throughout our experiments (e.g., over different selections of $K$, trajectories and forward processes). \nPlease see more details in our response to the common concern 2.\n'}, {'title': 'Common concern 2', 'comment': '\n## Common concern 2 (from reviewers FHw9, ejLx): The extra cost of the Monte Carlo estimate $\\Gamma_n$\n\n* The extra cost of the Monte Carlo estimate is small compared to the whole inference cost.\nIn fact, the Monte Carlo estimate requires $M N$ additional function evaluations. \nDuring inference, suppose we generate $M_1$ samples or calculate the log-likelihood of $M_1$ samples with $K$ timesteps. Both DPMs and Analytic-DPMs need $M_1 K$ function evaluations. Employing the same score-based models, the relative additional cost of Analytic-DPM is $\\frac{M N}{M_1 K}$. In our experiments, we found that a very small $M$ (e.g., $M=10$ or $100$) is sufficient for Analytic-DPM (see our response to common concern 1 (1.2 and 1.3)), making the relative additional cost small if not negligible. For instance, on CIFAR10, let $M=10$, $N=1000$, $M_1=50000$ and $K\\ge 10$, we obtain $\\frac{M N}{M_1 K}\\le 0.02$ and  Analytic-DPM still consistently improves the baselines as presented in Table 2* below. We have also added the results to Appendix G.2 in Table 6 in the revised version.\n\n\n* Further, the additional calculation of the Monte Carlo estimate occurs only **once** given a pretrained model and training dataset, since we can save the results of $\\Gamma = (\\Gamma_1, \\cdots, \\Gamma_N)$ in Eq.(8) and reuse it among different inference settings (e.g., trajectories of various $K$). The reuse is valid, because the marginal distribution of the shorter forward process $q(x_0, x_{\\tau_1}, \\cdots, x_{\\tau_K})$ at timestep $\\tau_k$ is the same as that of the full-timesteps forward process $q(x_{0:N})$ at timestep $n=\\tau_k$. Indeed, in our experiments (e.g., Table 1,2), $\\Gamma$ is shared across different selections of $K$, trajectories and forward processes. Moreover, in practice, $\\Gamma$ can be calculated offline and deployed together with the pretrained model and the online inference cost of Analytic-DPM is exactly the same as DPM.\n\nWe have updated Section 3 and added Appendix H.1 in the revised version to address this comment.\n\n\n| # timesteps $K$ | 10 |  25 | 50 | 100 |  200 | 400 |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| NLL $\\downarrow$ |\n| Analytic-DDPM ($M=10$) | 5.47 | 4.80 | 4.38 | 4.07 | 3.85 | 3.71 |\n| DDPM        | 6.99 | 6.11 | 5.44 | 4.86 | 4.39 | 4.07 |\n| FID $\\downarrow$ |\n| Analytic-DDPM ($M=10$) | 33.69 | 11.99 | 7.24 | 5.39 | 4.19 | 3.58 |\n| DDPM        | 44.45 | 21.83 | 15.21 | 10.94 | 8.23 | 4.86 |\n\nTable 2*: The NLL and FID comparison between Analytic-DDPM with $M=10$ Monte Carlo samples and DDPM on CIFAR10 (LS).\n'}, {'title': 'Common concern 1', 'comment': ""We thank all the reviewers for their appreciation of our novel contributions as well as the valuable comments, which help to further improve. Below, we first address some common concerns. Then, we address the individual comments to each reviewer.\n\n## Common concern 1 (from reviewers FHw9, ejLx): The tuning procedure of $M$ and more experiments on varying $M$\n\nWe clarify how $M$ is selected in the original paper and add more experiments on varying $M$.\n\n### 1.1 The tuning procedure of $M$ in the original paper\n\nWe did not tune $M$ in the original paper. In fact, we used a maximal $M$ without introducing too much computation. Specifically, we set $M=50,000$ on CIFAR10, $M=10,000$ on CelebA 64x64 and ImageNet 64x64 and $M=1,000$ on LSUN Bedroom by default without a sweep. \nAll of the samples are from the training dataset. We used the default settings of $M$ for all results in Tables 1, 2 and 3. We have added the experimental details in Appendix F.3 in the revised version.\n\n\n### 1.2. How results depend on $M$\n\nIn Appendix G.2 of the original submission, we evaluated Analytic-DPM with $M=100$ and found that it has a small variance and achieves  almost the same NLL results as those of $M=50,000$.\n\nFollowing the reviewers' suggestion, we added the results of Analytic-DPM over a wide range of $M \\in \\\\{1,3,10,100,10000,50000\\\\}$.\nUnder both the NLL and FID metrics, $M=10$ achieves similar results as those of $M=50,000$ on CIFAR10 (LS) and those of $M=10,000$ on ImageNet 64x64. The results are presented in Table 1* below. \nWe have also added the results to Appendix G.2 in Table 5 in the revised version.\n\n\n| CIFAR10 (LS) | NLL $\\downarrow$ | FID $\\downarrow$ |  ImageNet 64x64 | NLL $\\downarrow$ | FID $\\downarrow$ |  \n|  ----        | ----        | ----        |  ----          | ----       | ----        |\n| $M=1$        | 6.220±1.126 | 34.05±4.97 | $M=1$          | 4.943±0.162 | 31.59±5.11 |\n| $M=3$        | 5.689±0.424 | 34.29±2.88 | $M=3$          | 4.821±0.055 | 31.98±1.19 |\n| $M=10$       | 5.469±0.005 | 33.69±2.10 | $M=10$         | 4.791±0.017 | 31.93±1.02 |\n| $M=100$      | 5.468±0.004 | 34.63±0.68 | $M=100$        | 4.785±0.003 | 31.93±0.69 |\n| $M=50000$    | 5.471       | 34.26      | $M=10000$      | 4.783       | 32.56      |\n\nTable 1*: FID and NLL results of Analytic-DPM with different $M$. We use $K=10$ for CIFAR10 (LS) and $K=25$ for ImageNet 64x64.\n\n\n### 1.3. The variance of the estimate $\\Gamma_n$ over different $M$\n\nWe plotted the standard deviations of the estimate $\\Gamma_n$ at different timesteps $n$ when $M\\in \\\\{1,10,100\\\\}$. \nIn all cases, the variance decays fast as $n$ increases. Further, $M=10$ is sufficient to achieve a small standard deviation relative to the mean (e.g. less than 10\\% of the mean) for all $n$. \nSee Figures 4\\&5 in Appendix G.2 of the revised version for the results.\n\n\n""}, {'summary_of_the_paper': 'The paper studies diffusion probabilistic models, and derives the optimal mean and variance (as functions of the expected data score) for the reverse process. Authors then propose to plug in a Monte Carlo estimate of the the variance for the reverse process and experimentally show how this leads to improved results with trained and/or pretrained models in terms of FID and NLL. In addition, authors combine their approach with recent work that optimizes for ""knot"" locations given a fixed number of knots for faster sampling. ', 'main_review': ""**novelty, significance**\n\nThe result on optimal variance is new, to the best of my knowledge. The authors also do a reasonable job in convincing me that a Monte Carlo estimate of it is useful for inference. Given the recent progress and interest on DPMs, I think this work will also have reasonable significance and influence. \n\n**presentation**\n\nAuthors do a reasonable job on the writing and presenting experimental results. Past works are adequately and appropriately cited, to the best of my knowledge. \n\n**Technical quality and correctness**\n\nOne thing I'll add here is that once the Monte Carlo estimate of $\\sigma_t^2$ is plugged into the bound computation, it seems we end up with a stochastic lower bound of the ELBO (assuming the loss is concave in $\\sigma_t^2$). The important thing here to note perhaps is that bias is introduced. To put it more concretely, say the quantity being estimated is $\\mathbb{E}[ f(\\sigma_t^2) ]$, where I've written the bound as the expectation of the loss $f$ as a function of $\\sigma_t^2$. The estimator $f(\\hat{\\sigma}_t^2)$ is now a stochastic lower bound on the original quantity by Jensen's, since \n$$\\mathbb{E} [f(\\hat{\\sigma}_t^2)] \\le \\mathbb{E} [f( \\mathbb{E} [ \\hat{\\sigma}_t^2 ] ) ]  = \\mathbb{E} [f( \\sigma_t^2)].$$\n\nI think there should be some discussion about this. I'm assuming $f$ is concave in $\\sigma_t^2$, mostly reasoning from past bounds, but authors should perhaps make parts of the discussion more precise. \n\n**Experimental results**\n\nAuthors do a reasonable job in evaluating their method. One particular point I didn't get is how $M$ (number of samples for estimating the expected score) is chosen. The are a couple of potential issues here.\n- Selecting $M$ requires additional hyperparameter tuning, potentially; the tuning procedure should be reported. \n- How results depend on $M$ isn't entirely clear just from reading the main text (maybe there's some discussion in the appendix, but I didn't have time to read all content in the appendix). Ideally, some discussions should appear in the main text.\n- Large $M$ incurs more compute cost during inference -- while this seems less an issue when inference is run on GPUs (since most scenarios, I'd guess, there's enough cores to parallelize the Monte Carlo samples), this could be an issue for CPU inference. How does the run-time in practice compare in this case? Note while practical systems don't tend to run training on CPUs, inference on CPUs is still quite common. "", 'summary_of_the_review': 'Authors study choosing the optimal variance for the reverse process in DPMs and propose to Monte Carlo estimate it for improved inference.  Technical quality, writing, and experiments are mostly good with the two minor caveats I described above. \n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes a theoretically grounded method for estimating *optimal* reverse process variances for DDPMs and DDIMs. This method can be applied to a trained DDPM / DDIM after the fact and lead to improved likelihoods and faster sampling (when used together with optimal trajectory search). The proposed method and theoretical insights also perform strongly in practice across a range of models and datasets.', 'main_review': '**Strengths**\n* Strong theoretical motivation and strong empirical results to support it.\n* Nicely written, does a great job putting prior work in the context of the new insights on optimal reverse mean and variance.\n* Despite the abundance of theory / derivations, the paper still remains accessible.\n\n**Weaknesses**\n\nI am convinced by the paper in its current form. But if I had to list something:\n* It would be great to see applications of this method to other data modalities, such as for example speech / sound generation.\n* The performance of Analytical-DPMs on methods that learn forward process variance schedules (e.g. VDMs) would also be great to see.', 'summary_of_the_review': ""The paper provides valuable insights into optimal reverse process variance of DDPMs and DDIMs, and makes connections between the proposed optimal variance and previous handcrafted choices, etc. The improved understanding of these model classes could have been sufficient to recommend acceptance. However, the empirical results, especially those around faster sampling are also strong and convincing. DDPMs / DDIMs achieve high sample quality and it's primarily their sampling speed that prevents practical application off this model class in real-world systems. This works makes a significant step towards enabling faster sampling for this model class."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposed a modification of the diffusion probabilistic models (DPMs) called analytic-DPM that is based on an analytic estimate of the optimal reverse variance. Using this analytic estimate, the proposed method can achieve fast and performant inference through the Monte Carlo method and pre-trained score-based model. The derivation of the optimal reverse analytic mean and variance is proven to be associated with the score function. Upper bounds and lower bounds are provided for the optimal reverse variance, the relationship between the data covariance and the score function is shown. Experimental results are provided by comparing the proposed method with existing variants of DPMs, through both negative log-likelihood and FID as metrics. The experimental results suggest that the proposed method can potentially provide better performance more efficiently compared to alternatives.', 'main_review': 'Strength:\n1. the paper derives the optimal reverse variance for diffusion probabilistic models as well as its lower and upper bound. This leads to more efficient DPMs with better performance compared to existing DPM variants. It also leads to new insights into DPMs such as why the way the reversed variance is chosen by existing work is not ideal.\n\n2. The paper is clearly presented. The technical results reported in the paper are non-trivially obtained. The relationship between the paper and existing works is well discussed and well-motivated.\n\n3. Experimental results compared to other variants of DPM are well discussed. It also demonstrates the advantage of the proposed method compared to existing DPM variants.\n\nWeakness:\n1. Although proposition 1 establishes the relationship between data covariance and score function, it is unclear to me what is the practical implication of proposition 1.\n\n2. In the experiment, it is unclear to me whether timestep is a good metric to measure efficiency in Table 3. Does each method spend roughly the same time at each timestep?\n\n3. While the authors compare the proposed method with other existing variants of DPMs. Is there any reason why the comparison between the proposed method and other classes of generative models such as GAN should be conducted or not?\n\n', 'summary_of_the_review': 'Correctness: I think the paper is mostly correct. I am not sure if using timestep is a good metric to demonstrate the efficiency of the proposed method. Since efficiency is a major aspect of the proposed method, it would be desirable to make a clarification on this issue.\n\nNovelty and significance: I think the derivation of the optimal reverse variance is novel and insightful. The lower bound and upper bound of the optimal reverse variance is also useful in practice. While the proposed method is performant compared to existing DPM variants, these existing variants achieve better or comparable FID or negative log-likelihood with enough timesteps. These strong baselines suggest the (potentially limited) headroom left for improvement for the proposed method.\n\nOverall, I think the paper solves an interesting problem in DPM. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['Yes, Potentially harmful insights, methodologies and applications'], 'details_of_ethics_concerns': 'Performant generative models can be misused in situations such as deep fake. ', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper studies an estimate of the reverse process of a diffusion probabilistic model (DPM). The reverse process is usually estimated by minimizing the KL divergence between the forward and the reverse process. Authors present that the optimal mean and variance of the reverse process have analytic forms w.r.t. the score function. The form of the optimal mean coincides with the parameterization in the previous work and justifies a reweighted variant of the variational bound proposed in the recent work. Different from the handcrafted strategies employed in other work, the authors propose a novel estimation for the variance and analyze its bias. To reduce the bias, bounds of the optimal reverse variance are analyzed and the estimate is clipped based on the bound. Furthermore, the KL divergence between the forward and the optimal reserve process also has an analytical form and as a result, the authors propose the optimal trajectory which has minimal KL value. Finally, the authors present the relationship between the score function and the data covariance matrix and assess the proposed approach in the experiments. The experiment results show that the analytic results improve the efficiency, likelihood, and sample quality.', 'main_review': ""Strengths:\n\n1. The paper is clearly written and well organized. Contents are easy to follow. There are many technical details for readers to understand the results, such as the derivation of Theorem 1.\n\n2. The analytic results are interesting and novel. According to the introduction and the related work sections, the optimal forms of the reverse process of DPM didn't appear in the previous DPM work and I believe it will help people in the field of DPM better understand this type of models. The bias analysis and the bound of the variance are helpful to understand the estimate and improve the estimate performance. The authors also make detailed discussions on these results, which are very helpful.\n\n3. The experiment results are strong. The authors not only validate their analytic forms but also present the outperformance of their methods. The experiment results show significant improvements in their methods.\n\nWeaknesses:\n\n1. The DPM is a Gaussian-distribution-based simple model and similar analytic results can exist in similar models. The Gaussian model considered in this paper has a simple Markov property, which as a result has decomposable probability. The optimization is via forward KL divergence. Therefore, for example, the classical result on the expectation propagation algorithm with the Gaussian process can simplify the derivation: the decomposable form of the probability guarantees the forward KL decomposable, and minimizing the forward KL is equivalent to moment matching. So, I am concerned that the contribution is not as much as what was introduced in the paper. I also believe that such kind of connection will be helpful to figure out the possible further directions along this line and also save energy to get new results. I suggest authors add some discussions on the connection to other Gaussian models and their results."", 'summary_of_the_review': 'The paper contributes interesting analytic results to the DPM models and the methods perform well in practice. However, the DPM is a simple Gaussian model and similar results can appear in other similar models.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In Sec. 2, the authors review the general framework of Diffusion Probabilistic Models~(DPMs) with non-Markovian forward processes from [1] which has DDPMs [2] and DDIMs [1] as special cases. They emphasize that the variance of the reverse process, such as in DDPM and DDIM, are generally hand-crafted.\n\nIn Sec. 3, the authors present their main theorem: the optimal variance of the reverse process at step $n$ is a linear function of the expected squared norm of the score at step $n$. As a post-processing scheme of learned models, they then propose to replace the commonly used hand-crafted variances by an estimator of their optimal variance: in particular, the score is replaced by the learned score model and the expectation is replaced by a Monte Carlo estimate (with $M$ samples) thereof. In the remainder of this section, the authors derive bounds for the optimal variance and state that in practice their estimator is clipped according to these bounds.\n\nIn Sec. 4, the authors repeat the derivation of an optimal variance and bounds thereof when only a subset (of size $K$) of the $N$ timesteps are used for inference as is very common in the literature [1, 3, 4]. Furthermore, they show how their estimator in this setting can be used to still compute the optimal subset according to the commonly used DP algorithm introduced in [4].\n\nIn Sec. 6, they apply their post-processing scheme to models trained on CIFAR-10, CelebA 64 and ImageNet 64 and show that it generally leads to improved likelihood and FID scores. They show that their post-procecssing scheme consistently outperforms suboptimal hand-crafted choices (such as in DDPM and DDIM).\n\nReferences:\n\n[1] Song et al. Denoising Diffusion Implicit Models. ICLR 2021.\n\n[2] Ho et al. Denoising Diffusion Probabilistic Models. NeurIPS 2020.\n\n[3] Dhariwal & Nichol. Diffusion Models Beat GANs on Image Synthesis. arXiv:2105.05233.\n\n[4] Watson et al. Learning to Efficiently Sample from Diffusion Probabilistic Models. arXiv:2106.03802.', 'main_review': 'Strengths:\n- Paper has single well-executed idea (optimal variance can be computed as a function of score) \n- Improvements over generally fair base lines are achieved by only post-processing; no additional training needed\n- Paper is generally well-written and straightforward to read\n- I did not check all Lemmas (appendix) in detail, however, it seems that their proofs are generally very rigorous and detailed.\n- The bounds in Theorem 2 are quite nice. Given that DPMs are often used for images, the specific bound for the data distributions supported in the $d$-dimensional cube are quite applicable.\n\nWeaknesses:\n- For $K<100$, FID scores of the proposed method greatly rely on a trick of clipping the variance of the step $n=2$ (can be seen in appendix G.4). This seems to be a crucial element and should be discussed more in the main paper. In particular, I would like to see how this clipping compares to the bounds (and the clipping) of the optimal score wrt Theorem 2.\n- The post-processing method can be quite expensive. For example, on ImageNet the best values are achieved using $MK= 400000$ additional ($M=100, K=N=4000$) function evaluations. For even trajectories (ET), the obvious solution would be to simply use $K \\ll N$ (results for this are shown in paper), however, if I am not mistaken, for the optimal trajectory (OT), computed using the DP algorithm from [4], $MN$ functions have to be evaluated for any $K$ (see appendix B and eq (14)). Therefore, in my opinion, the comparison of DDPM and Analytic=DDPM in the OT setting of Table 1 is unfair.\n\nSuggestions:\n- It would be helpful to understand how often the estimator is actually clipped. I suggest to compute $R$ (maybe $R=100$) estimators for, say $M=[1,10, 50, 100, 500, 1000]$, and plot he number of the ratio of estimators that was clipped over $M$. This could be done for a few different instantiations of $n$ (I guess there will be more clipping for $n$ being small).\n- the estimator $J$ in (14) is biased even when the correct score is known ($J$ is the log of an unbiased (Monte Carlo) estimator); it might be nice to mention this fact.\n- It would be nice to see even lower $M$ in the ablations in G.2. Instead of only indicating variance by plotting the estimator in Figure 3, it would be nice to see the estimated variance of the sampler directly.\n- Please state the number of Monte Carlo samples used for results in Tables 1,2,3.\n- To me it seems that the optimal variance could also be used for training, using for example only $M=1$. I would be curious to see how this performs compared to using the optimal variance only as a guide for post-processing. I greatly encourage the authors to try this (in case there are no major problems I am missing)', 'summary_of_the_review': 'Overall, I vote for accepting. The paper provides an important insight in DPMs and shows improved results for pretrained models by a simple post-processing technique. My major concern about the paper is that OT [4] does not work well in combination with their method, which in my opinion makes the work slightly less significant. I hope the authors can address this concern in the rebuttal period.\n\n**Post discussion period update:**\nI strongly vote for accepting (and also changed the correctness score from 3 to 4). All of my concerns, questions, and suggestions have been addressed by the authors in the discussion period. I thank the authors for this productive reviewing process.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models', 'authorids': ['~Fan_Bao1', '~Chongxuan_Li1', '~Jun_Zhu2', '~Bo_Zhang2'], 'authors': ['Fan Bao', 'Chongxuan Li', 'Jun Zhu', 'Bo Zhang'], 'keywords': ['diffusion probabilistic models', 'generative models'], 'abstract': 'Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \\textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\\times$ to $80\\times$ speed up.', 'one-sentence_summary': 'We propose an analytic framework of estimating the optimal reverse variance in DPMs.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'bao|analyticdpm_an_analytic_estimate_of_the_optimal_reverse_variance_in_diffusion_probabilistic_models', 'pdf': '/pdf/541cdc9e000367bb0bd3fc42201573ed434094c8.pdf', 'supplementary_material': '/attachment/82cdf46f2e4104c5546cc812a2654ad6d79347f0.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/analytic-dpm-an-analytic-estimate-of-the/code)', '_bibtex': '@inproceedings{\nbao2022analyticdpm,\ntitle={Analytic-{DPM}: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},\nauthor={Fan Bao and Chongxuan Li and Jun Zhu and Bo Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0xiJLKH-ufZ}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Lixu Wang', 'Shichao Xu', 'Ruiqi Xu', 'Xiao Wang', 'Qi Zhu']",ICLR,Non-Transferable Learning_ A New Approach for Model Ownership Verification and Applicability Authorization,https://iclr.cc/virtual/2022/oral/6144,2022," As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets.",Oral 2: AI applications,https://openreview.net/pdf?id=tYRrOdSnVUy,https://openreview.net/forum?id=tYRrOdSnVUy,tYRrOdSnVUy,"[{'title': 'Verification of Eq.(2)', 'comment': 'Hi, thanks for your interesting work! According to your released code, I think Eq.(2) in current version should be **min** not **max**.  '}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper addresses two important aspects of deep learning: model transferability and authorization for use. It presents original solutions for both of these problems. All of the reviewers agree that the paper is a valuable contributions. Minor concerns and critical remarks have been addressed by the authors during the discussion.'}, {'title': 'Thanks for your careful response.', 'comment': ""The authors' response has addressed my concerns. I lean to accept this paper. ""}, {'title': 'Thank you for the clarification', 'comment': ""I have carefully read the responses and other reviewers' comments. My concerns have been properly addressed. I think this paper will contribute a lot to the machine learning field. Thus, I decide to raise my score to 8.""}, {'title': 'Response for Reviewer RSPY', 'comment': "">1. In Table 1, what is the number of training epochs when transferring MT to MM? Did you try to increase the epochs of fine-tuning? If you train for enough epochs, the model would eventually reach the original accuracy. The sensitivity analysis regarding the epochs of your fine-tuning is necessary when compared to training from scratch and transfer learning from the original model to the target task.\n\nThank you very much for the question and comment. In experiments for Table 1, we set the number of training epochs as 50 for both Supervised Learning (left) and NTL (right). We tried increasing the number of training epochs to 100 on a number of different datasets and the results were similar. The only fine-tuning-related experiments in our work are fine-tuning-based watermark removal attacks (FTAL, RTAL, EWC, and AU), which are shown in Table 2, with the number of fine-tuning epochs set to 200. Indeed we should have a sensitivity analysis for the number of these fine-tuning epochs. Thanks for pointing this out. We have conducted additional experiments with different numbers of fine-tuning epochs for Table 2: 400, 600, 800, 1000 (in addition to the 200 epochs in the original submission). Taking the AU removal attack as an example, the CIFAR10 data with and without the trigger patch have the performance of 13.1/87.3 with 400 epochs, 13.0/87.1 with 600 epochs, 13.1/87.3 with 800 epochs, and 13.3/87.2 with 1000 epochs. We can observe a similar level of effectiveness for NTL under these different numbers of fine-tuning epochs. The results on MNIST and part of VisDA show a similar trend, and more experiments are ongoing. We plan to add a section in the Appendix once we obtain all the experimental results. \n\n>2. The training complexity of using your NTL approach and the GAN training should be introduced in this paper? The computing time of the MMDs during each time step is at least twice your training time?\n\nThanks for the questions. NTL costs more time when compared with supervised learning (SL), and the additional overhead is mainly brought by MMD calculation and GAN training if it is the source-only case. In the experiments, we observe that the additional overhead is not very significant. For instance, in each training epoch of CIFAR10 on the VGG-13 backbone, NTL takes 57.2 seconds while SL takes 50.1 seconds. As for the training of our generative augmentation framework, the first stage (standard GAN training process) takes 40.8 seconds on average for each epoch. In the later augmentation stage, it takes 28.6 seconds on average for each epoch. \n\n>3. The proposed methodology is well presented. However, the differences between the proposed model and related SOTA works should be presented clearly.\n\nWe are not completely sure which aspects the reviewer meant by related SOTA works. Below we provide some explanations based on our understanding of the context of the reviewer's comment and have made changes in the paper accordingly. If our understanding does not match what the reviewer meant, we are happy to further explain and improve it.\n\nFor NTL methodology, to our best knowledge, we are the first to propose an optimization objective for reducing model generalization ability inspired by the Information Bottleneck Theory, and we are not aware of close SOTA works. At the beginning of the Methodology section, we have explained how NTL is inspired by the Information Bottleneck but addresses a different problem. As for the adversarial augmentation framework, although it is based on GAN, our aim is not to propose a new GAN but to design an effective augmentation method for NTL, and thus we did not discuss the SOTA works of GAN. We have added these explanations in Section 3 of the revision and made them bold, to clarify the difference between our work and the prior works, for both NTL and GAN. In addition, we have added more references in the Related Work section, on approaches for domain adaptation and works related to IP protection for DL models (in particular on Membership Inference Attack and Model Inversion Attack).""}, {'title': 'Response for Reviewer RSPY', 'comment': '>4. Comparing Table 2 and Table 3, it can be seen that sometimes the source-only method shows greater performance compared to the target-specified method. The reasons why would this happen are interesting since providing the target-domain target should be more accurate when removing some part in the generalization space. However, the experiments seem does not agree with it.\n\nThanks for the comment. In our experiments of Target-Specified NTL, both the training and testing sets are subsets of the target domain, and they are not overlapping with each other. In this case, if there is a significant difference between the target training and testing sets, although the NTL training can effectively reduce the performance on the training set, the performance on the testing set may not decrease to the same extent. As for Source-Only NTL, our augmentation framework generates data with different directions and distances. It is possible that the generated data completely covers the target testing set, in which case the performance of Source-Only NTL can be better than that of Target-Specified NTL. There could also be other factors, e.g., various types of randomness. This is indeed an interesting aspect that we plan to investigate more in future work.\n\n>5. A future research section should be added to the revision.\n\nThanks for the suggestion. We have included a brief discussion of future work in the revision (Section 5 is now Conclusion and Future Work), including suggested directions from the reviewers and some of our thoughts.'}, {'title': 'Response for Reviewer FcxC', 'comment': '>1. The presentation should be improved. The first paragraph in the intro is too long. It is better to divide it into several paragraphs to better demonstrate the key points of this paper.\n\nThank you very much for the suggestion. We have split the original first paragraph into two in the revision. The new first paragraph explains the importance of IP protection for deep learning models, and the second paragraph discusses existing methods for protecting the DL model IPs, explains why they are limited, and introduces our focus in this work. We also polished the language throughout these paragraphs and the rest of the section.\n\n>2. I am not sure if it is necessary to list the contributions in the introduction. Such contributions have been described clearly in intro and abs. It seems that you do not need to restate them.\n\nWe have reorganized the text related to contribution and removed repetitive discussion in our revision. This actually gives us more space to discuss related and future work in the following sections. Thanks!\n\n>3. Key related works are missing. For an AI company, they need to be aware of many adversarial attacks, such as reprogramming attacks, model-inversion attacks. These works are also related to IP protection of deep learning. It would be better to conclude these attacks as related works as well. Some discussions should be also added for general readers of ICLR.\n\nWe have included more discussions on relevant adversarial attacks in the related work section, including membership inference attack and model-inversion attack, and have made other minor changes throughout the section.\n\n>4. Some notations should be changed. For example, we will not use X or Y to present distributions, instead, we will use them to represent random variables. It is better to use \\sP_X to represent the distribution corresponding to a random variable X. It is unnecessary to use GMMD, you can use MMD(P, Q; k), where k is a Gaussian kernel (you can follow the notations from recent deep kernel MMD papers). \n\nThanks for pointing this out. We have modified the notations of distributions and MMD accordingly in the revision.\n\n>5. How many times do you repeat your experiments? I did not see the error bar/STD values of your methods. This should be provided to verify that the experimental results are stable.\n\nIn the original submission, as mentioned in the second paragraph of Section 4 (right before Section 4.1), our experiments were repeated three times with different seeds (2021, 2022, 2023) and the average performance was reported in the main paper. We also included the error range (error bar) of our experiments in Appendix C.4. From the experimental results, we can observe that the performance of our method has only little fluctuation under different seeds, which shows the stability of our method.\n\n>6. If we consider adding bandwidth to your kernel function, how does the kernel bandwidth affect your results?\n\nThanks for the question. In our implementation, we use a series of Gaussian kernels to compute the MMD approximator, which is implemented as the MK-MMD (specified in Appendix C.5). The bandwidth of the used kernels is determined at the high level by two parameters (mul and num). We select the source-only NTL cases of MNIST, CIFAR10, and VisDA-T as the representatives to conduct experiments with different kernel bandwidth (mul = 1.0, 2.0, 3.0 with num = 5, and num = 3, 5, 7 with mul = 2.0), and obtain the following results:\n\nMNIST (source / target): mul=1.0 (98.8 / 14.5); mul=2.0 (98.9/14.7); mul=3.0 (98.7 / 14.6)\n\t\t\t    num=3 (98.8 / 14.6); num=5 (98.9/14.7); num=7 (98.8 / 14.7)\n\nCIFAR10 (source / target): mul=1.0 (87.7 / 10.3); mul=2.0 (87.8 / 10.2); mul=3.0 (87.8 / 10.3)\n\t\t\t    num=3 (87.7 / 10.5); num=5 (87.8 / 10.2); num=7 (87.7 / 10.4)\n\nVisDA-T (source / target): mul=1.0 (95.5 / 14.7); mul=2.0 (95.7 / 14.7); mul=3.0 (95.6 / 14.8)\n\t\t\t    num=3 (95.7 / 14.5); num=5 (95.7 / 14.7); num=7 (95.7 / 14.4)\n\nFrom the above results, we can see that the kernel bandwidth does not seem to have a significant impact on NTL performance. We have added the above results in the revision as Appendix C.5.\n'}, {'title': 'Response for Reviewer 1wBe', 'comment': '\n>1. Can you imagine uses of this to other kinds of models, e.g., language models, or is this mainly meaningful for image data?\n\nThank you very much for the question. Currently, our NTL mainly focuses on image-related tasks. Applying NTL to other models is indeed interesting and we have included it in future work discussion (Section 5) in the revision. In particular, it could be interesting and challenging to apply our NTL in language applications. The current version of NTL is not directly applicable as the GAN-based data augmentation method cannot generate NLP task data. Moreover, to our knowledge, generating language data of different domains could be difficult, and thus developing an NTL with restricted domain generalization ability in NLP tasks deserves an in-depth study. In addition to the domain NTL, we are very interested in restricting the language model to certain tasks within a particular language.\n\nIt would also be interesting in future work to explore other image tasks such as semantic segmentation and object detection/tracking, where more complex datasets exist (in the current work for image classification, VisDA is the most complex dataset we can find in domain adaptation/generalization). Finally, another possible future direction is NTL for Multi-Task Learning, where we can restrict the language model to certain tasks. We have added a brief discussion of this in the revision as well.\n\n>2. It sounds like an NTL representation by nature is highly vulnerable to training data privacy attacks, like membership inference. Have you considered if one could use the NTL representation to particularly efficiently generate samples from (something close to) the training data distribution?\n\nThanks for the question. We feel that this question inspires a very interesting direction to explore for future work, i.e., does the model with intellectual property protection have the same robustness to adversarial attacks as the model without such protection? Below are some of our initial thoughts and investigations.\n\nFor membership inference attack (MI), to our knowledge, the attacking target is to determine whether a sample has been learned by the target machine learning model, where this sample is similar to the training data used in the learning process. We believe that what MI attack cares about are those samples that have correct predictions when they are forwarded to the target model. Taking a lung cancer diagnosis model as an example, the MI attack is interested in whether the data of a particular lung cancer patient has been used to train this diagnosis model, but not in whether a liver cancer patient has been learned by this model since the model intuitively outputs wrong predictions on liver cancer data. Therefore, considering our problem, we think that the MI attack could aim to determine whether a sample from the source domain has been learned by the target model. In our initial investigation, we forward the source domain data to both models trained with NTL and standard supervised learning and compare the cosine similarity between their predictions. We observe that the cosine similarity is very close to 1, which indicates that there is no significant prediction difference between models trained with NTL and standard supervised learning in terms of the source domain data. This could indicate that the model trained with our NTL is not more vulnerable to membership inference attacks (more quantitative investigations are needed in the future work).\n\nAs for sample generation from the NTL representation, we assume that you mean a type of model inversion attack that aims to reconstruct the input data from its corresponding model prediction (please correct us if it is not the case). As aforementioned, the prediction of models trained with NTL and standard supervised learning on the same source domain data sample is very close, and therefore we think that the vulnerability of the model trained with NTL is similar to that of the model trained with standard supervised learning in terms of the source domain data. As for the data from other similar domains, intuitively, the model trained with NTL should not learn the semantic information within such data, and thus we hypothesize that there should not be significant information embedding in the model trained with NTL in terms of those similar domains. That is to say, NTL can likely protect the model from model inversion attacks to some extent, but more quantitative investigations are needed in the future work.\n'}, {'title': 'Thanks for All Reviewers', 'comment': 'We would like to thank all the reviewers for their insightful comments and constructive suggestions. Below we provide our detailed response to each reviewer. We have also uploaded a revised version of our submission, with major changes highlighted in blue (there are also other minor changes on wording and typos). We look forward to further feedback and discussion.'}, {'summary_of_the_paper': 'In the era of deep learning, pre-trained models have been regarded as intellectual properties of AI companies. Thus, protecting these models has been more and more important. To achieve this aim, this paper proposes a non-transferable learning (NTL) method to capture the exclusive data representation in the learned model and restrict the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: \n1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, the NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets.\n2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. The NTL-based authorization approach instead provides a data-centric protection, which is called applicability authorization, by significantly degrading the performance of the model on unauthorized data.\nIn general, this paper contributes a novel method to the field, and experiments verified the success of the proposed method. ', 'main_review': 'Pros:\n+ The research direction is promising and important in the real world. Nowadays, AI companies will train their own deep models with abundant labelled data that costs a lot of resources. Thus, it is a good timing to research how to protect these models, which have become very important and practical. \n+ This paper proposed a method that can be effective solutions to both model verification and authorization, which is general and is promising to be applied in other applications.\n+ This paper is easy to follow. Experiments are enough to support the claims made in this paper. A plus should be that experiments are conducted with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets.\n\nCons:\n- The presentation should be improved. The first paragraph in intro is too long. It is better to divided it into several paragraphs to better demonstrate the key points of this paper.\n- I am not sure if it is necessary to list the contributions in the introduction. Such contributions have been described clearly in intro and abs. It seems that you do not need to restate them.\n- Key related works are missing. For an AI company, they need to be aware of many adversarial attacks, such as reprogramming attacks, model-inversion attacks. These works are also related to IP protection of deep learning. It would be better to conclude these attacks as related works as well. Some discussions should be also added for general readers of ICLR.\n- Some notations should be changed. For example, we will not use X or Y to present distributions, instead, we will use them to represent random variables. It is better to use \\sP_X to represent the distribution corresponding to a random variable X. It is unnecessary to use GMMD, you can use MMD(P,Q; k), where k is a Gaussian kernel (you can follow the notations from recent deep kernel MMD papers).  \n- How many times do you repeat your experiments? I did not see error bar/STD values of your methods. This should be provided to verify that the experimental results are stable.\n- If we consider to add bandwidth to your kernel function, how does the kernel bandwidth affect your results?', 'summary_of_the_review': 'In general, considering the significance of the researched problem, this paper can be accepted by the ICLR2022. However, some points should be clarified and strengthened in the revision. I would like to strongly support this paper if my concerns can be fully addressed.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper introduces the idea of ""non-transferable learning"", which is roughly what the name indicates. The authors explain the value of this as a security/IP protection tool to protect the model from being used on unauthorized data. In addition, this presents a kind of attack against domain adaption works that try to improve generalization bounds without access to source data.', 'main_review': ""Basically, the authors design a clever technique for learning nuisance-dependent representations. Such a representation can be made to perform accurately for a particular source domain, but poorly for another target domain. Furthermore, the authors design a GAN type technique for generating samples outside the source domain to serve as a kind of generic target domain. This is obviously important, as one cannot know to which target domain the model would be later adapted to.\n\nThis is a very interesting paper, although I have to say I'm not an expert in this topic at all. Most of the paper is really nicely written and is pretty easy to follow. The experimental verification is clear and detailed, but mostly limited to small images, so it's hard to say how it actually performs in some real-life scenarios.\n\nCouple questions come to mind:\n- Can you imagine uses of this to other kinds of models, e.g., language models, or is this mainly meaningful for image data?\n- It sounds like an NTL representation by nature is highly vulnerable to training data privacy attacks, like membership inference. Have you considered if one could use the NTL representation to particularly efficiently generate samples from (something close to) the training data distribution?"", 'summary_of_the_review': 'Non-transferable learning is an interesting idea to explore, and this is the first step in that direction. I can imagine that there will be a lot of follow-up ideas both for attacking this, as well as improving upon it. I would definitely recommend accepting this for ICLR.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'No particular concerns. The authors already addressed some in their submission.', 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'Protecting the intellectual property of the trained models has received appealing attentions. Existing researches to protect  intellectual property fall into two major categories: ownership verification and usage authorization. To this end, the authors propose to utilize non-transferable learning to achieve both the goal of ownership verification and usage authorization. Extensive experiments on several representative datasets validate the effectiveness of the proposed method in terms of ownership verification. \n\nGenerally, this paper proposes a novel idea to address a practical problem in real-world applications, which could inspire many readers to follow it and have an important influence on the community of computer vision. I support the acceptance of this paper for a better ICLR conference. ', 'main_review': 'This paper could be significantly improved via addressing the following issues:\n1. In Table 1, what is the number of  training epoches when transfering MT to MM? Did you try to increase the epochs of fine-tuning? If you train for enough epochs, the model would eventually reach the original accuracy. The sensitivity analysis regarding the epoches of your fine-tuning is necessary when compared to training from scratch and the transfer learning from the original model to the target task.\n2. The training complexity of using your NTL approach and the GAN training should be introduced in this paper? The computing time of the MMDs during each time step is at least twice your training time?\n3. The propsoed methodology is well presented. However, the differences between the proposed model and realted SOTA works should be presented clearly.\n4. Comparing Table 2 and Table 3, it can be seen that sometimes the source-only method shows greater performance compared to the target-specific method. The reasons why would this happen are interesting since providing the target-domain target should be more accurate when removing some part in the generalization space. However, the experiments seem does not agree with it.\n5. A future research section should be added in the revision.', 'summary_of_the_review': 'This paper proposes an interesting question and gives the corresponding solution. I recommend the acceptance of this paper. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'title': 'Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization', 'authorids': ['~Lixu_Wang1', '~Shichao_Xu1', '~Ruiqi_Xu1', '~Xiao_Wang11', '~Qi_Zhu2'], 'authors': ['Lixu Wang', 'Shichao Xu', 'Ruiqi Xu', 'Xiao Wang', 'Qi Zhu'], 'keywords': ['Domain Adaptation', 'Transfer Learning', 'Societal Considerations of Representation Learning', 'Model Watermark'], 'abstract': 'As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wang|nontransferable_learning_a_new_approach_for_model_ownership_verification_and_applicability_authorization', 'pdf': '/pdf/cc0b829e495ebd36c4e0dcce6f5d044ad4dce58d.pdf', 'one-sentence_summary': 'We propose a novel Non-Transferable Learning (NTL) method to restrict the model generalization ability to certain domains for model ownership verification and applicability authorization.', 'supplementary_material': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/non-transferable-learning-a-new-approach-for/code)', '_bibtex': '@inproceedings{\nwang2022nontransferable,\ntitle={Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization},\nauthor={Lixu Wang and Shichao Xu and Ruiqi Xu and Xiao Wang and Qi Zhu},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=tYRrOdSnVUy}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Albert Gu', 'Karan Goel', 'Christopher Re']",ICLR,Efficiently Modeling Long Sequences with Structured State Spaces,https://iclr.cc/virtual/2022/oral/6960,2022," A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps.  A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \), and showed that for appropriate choices of the state matrix \( A \), this system could handle long-range dependencies mathematically and empirically.  However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution.  We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.  Our technique involves conditioning \( A \) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.  S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",Oral 2: Structured learning,https://openreview.net/pdf?id=uYLFoz1vlAC,https://openreview.net/forum?id=uYLFoz1vlAC,uYLFoz1vlAC,"[{'title': 'Please use GitHub for code questions', 'comment': 'The functionality is available in version v1 of the repository: https://github.com/HazyResearch/state-spaces\nThe outputs of the module will be correct, but it may be more difficult to train a model using TBPTT.\n\nPlease direct all further questions about code to the GitHub issues in the repository.'}, {'title': 'Follow up question', 'comment': 'Thanks for the reply. \n\nI am another researcher interested in applying your model (which is ingenious!) to our data.\n\nSimilar to Jason who asked the original question, we are interested in using the CNN version of S4 but due to memory constraints we cannot use the full sequence at inference time, but instead have to divide the sequence into small chunks. For RNN we just keep the final hidden state of each chunk and use that as the starting hidden state of the next chunk and iterate, but we are not sure how to do this for S4.\n\nSince you mentioned that your code already has this functionality, could you point out **where it is located and how to use it**? Is it the `step` method for S4 and S4D modules? Also, since you mention ""numerical instability"", will one obtain the same result at inference time if one uses the chunking method? '}, {'title': 'Updated revision completed', 'comment': 'To all readers: We have uploaded an updated version of the S4 paper, including the Camera Ready Revision for ICLR 2022. We would also like to note that a version of the paper is on arXiv, which contains additional content that did not fit in the page limits of the ICLR version: https://arxiv.org/abs/2111.00396\n\nThis content addresses several recommendations from the reviewers, such as how to interpret the multidimensional and linear aspects of S4. Most importantly, the arXiv version has an additional section that ablates the state matrix A to show the importance of HiPPO and trainability, which several reviewers were curious about. We thank all reviewers and chairs again for their valuable feedback.'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""All reviewers agreed this was a very strong submission: it was clearly written, was theoretically and experimentally interesting, and had excellent motivation. A clear accept. Authors: you've already indicated that you've updated the submission to respond to reviewer changes, if you could double check their comments for any recommendation you may have missed on accident that would be great! The paper will make a great contribution to the conference!!""}, {'title': 'Thank you for response', 'comment': 'Thank you for your answers to the questions raised.  The authors have addressed the main questions/concerns I raised.  Remaining questions are sufficient to be addressed in future work which this paper enables.  The addition of limitations and next steps is also appreciated.  '}, {'title': 'Thanks for your detailed answers', 'comment': 'Thank you very much to the authors for their detailed answers to my questions. The provided discussion is indeed very interesting and clarify the answer to most of my questions. Although there exists several follow up questions that I would have, this is not a drawback, but a side effect of an intriguing paper. To reiterate, both the theoretical and experimental content are substantial, which clearly warrants publication at the conference. I look forward to seeing further works regarding theoretical analyses and applications of S4.'}, {'title': 'Thanks for the comment', 'comment': 'Thanks for pointing out the typo! It has been fixed in the updated draft.\n\nIf we are interpreting your question correctly, you are asking: how can we compute an SSM on a finite ""chunk"" of the data, using an arbitrary starting state $x_0$, while also computing the final state $x_N$? This calculation is naturally done using the recurrent view, but that loses the parallelization; can the calculation be done using the convolutional view?\n\nThe answer to this question is yes, and in fact our released code has this functionality. However, it currently is slower than the vanilla SSM convolution calculation for technical reasons, and additionally may be more prone to numerical instability. We are very interested in the application of this functionality for future work.'}, {'title': 'Response to Reviewer 7JCQ', 'comment': 'We sincerely appreciate the reviewer’s thorough and thoughtful review. Overall, the reviewer found our paper well-written and the model’s performance compelling. The reviewer’s comments suggested ways to improve the presentation, which we respond to in order:\n\n1.  Our updated manuscript includes a discussion of limitations and next steps. This includes a remark on where S4 has had the most difficulties so far (text data), as well as some ideas for extensions on both theory and applications.\n\n2. Some of our tasks had relatively low data, such as the sequential image tasks (50k train examples), some LRA tasks, and the time-series forecasting tasks (where each epoch took only a few seconds) where S4 consistently outperformed the Informer baseline. We hypothesize that S4 performs well in low-data regimes on time-series data because SSMs are a continuous-time model and have strong inductive bias for such time-series data.\n\n3. We thank the reviewer for numerous presentation suggestions in the experiments. We have improved many of the references between tables and text and the ordering of tables, although some elements of the previous draft were preserved for formatting and space considerations.\n\n4. Indeed, the trends in the context size in lower versus higher layers are not hard and fast rules. One explanation is that unlike CNNs which use local filters and pooling -- and are very explicitly designed to be hierarchical -- our S4 model is more generic, and each layer is allowed to learn dependencies of any length. Any trends that appear are emergent properties of the model, and investigating the learned representations in more detail is an interesting question for future study.'}, {'title': 'Response to Reviewer Bxrv', 'comment': 'We are encouraged that the reviewer appreciates our theoretical contributions and strong empirical results. We respond to the reviewer’s questions in order:\n\n1. Most of our results use numbers reported directly from prior work. We included the LSSL where applicable (sequential image classification and speech classification tasks), but the LSSL did not have results on many of our tasks. As the reviewer notes, this is due to the scalability problems of the LSSL; it would not have been able to solve the LRA Path-X task due to memory and computation constraints.\n\n2. The representations used by the LSSL and S4 are similar, and we do not currently have guarantees on when S4 would perform better. The main advantage of the S4 parameterization is computational.\n\n3. Our theoretical results show that the previously defined HiPPO matrices are all NPLR. The reviewer asks the natural question about whether the converse holds: are all NPLR matrices also HiPPO operators? We do not currently know the answer to this question, which is a very interesting direction of future study. Empirically, training these parameters does provide a consistent performance increase. (See also response to Reviewer ncZn)\n\n4. As the reviewer notes, the error accumulation problem is tied to autoregression, and not the sequence model itself. S4 does not address the error accumulation problem per se, but we note that its superior modeling of long-range dependencies may yield more consistent predictions over long horizons. This is an interesting future direction.'}, {'title': 'Response to Reviewer ncZn', 'comment': 'We are glad the reviewer found the ideas in the paper compelling and clear. The reviewer asked several insightful questions about our proposed model, which we respond to in order:\n\n1. One answer is that the *discretized* matrix $\\overline{A}$ is the one that needs to be close to unitary, and in fact it is by the formula (3). At a higher level, unitary can be considered a side effect and not the main condition needed to handle long dependencies; the HiPPO theory guarantees handling long context by design.\n\n2. Empirically, we have found that matrices in a neighborhood of HiPPO matrices perform better (see below ablation). This leads to the following interesting question: our theoretical results show that the previously defined HiPPO matrices are all NPLR, but the converse question is an extremely interesting direction for future study.\n(a) The S4 parameterization (Algorithm 1) actually makes it difficult to disentangle learning A from B and C. We note that prior work on the LSSL performed a similar ablation: they found that learning the A matrix provides a consistent boost in performance (e.g., up to 5% on the Speech Commands dataset)\n(b) We performed this ablation using a smaller model on the sequential CIFAR dataset and got 88.38% for HiPPO matrix A, versus 81.36% for random NPLR matrix A. This is still substantially better than non-SSM models, but the HiPPO initialization has a clear advantage.\n\n3. Multidimensional inputs are handled by the $H$ broadcasting described in Section 3.4 (as well as mentioned in question 4 here). While state spaces technically can handle multidimensional signals (by changing the shape of the $B$ and $C$ matrices), this is actually subsumed by the $H$ broadcasting followed by a linear map to mix the features. Earlier versions of this work did consider higher-dimensional signals, but found no computational or empirical benefit.\n\n4. The proposed feature sharing was actually considered in earlier versions of this work, but discarded for simplicity. The main reason is that although sharing the A and B parameters does save parameters, it does not save computation. Essentially, the Cauchy kernel needs to be computed for each of the H copies of (A, B, C); even if A and B are constant between each copy, the fact that C is different means each of the H copies still needs to be re-computed separately. Hence there is no computational benefit to sharing some of the parameters.\nAs mentioned, this sharing was previously considered; our released code does in fact contain options to share the A, B, or C parameters.\n\n5. S4 by itself is indeed linear, and nonlinearities are indeed introduced in the depth direction between the S4 and linear layers (Section 3.4). We note that prior work (the LSSL) showed theoretical results explaining why linear RNNs mixed with non-linearities in the depth direction can recover non-linear RNNs.\n\n6. We agree with the reviewer’s definition of irregularly-sampled data and have changed the title of this subsection in the revised submission.\nWe also agree with the reviewer’s point about the computation issue with truely irregularly-sampled data. Indeed, such a setting can still be handled by SSMs, but they will then lose the convolutional representation. One perspective is that in control theory, only linear *time-invariant* (LTI) systems are related to convolutions, while irregularly sampled data would be *time-dependent* as the reviewer points out. Such considerations are interesting directions for future work.'}, {'title': 'Response to All Reviewers', 'comment': 'We thank all reviewers for their time and thoughtful feedback. Overall, all reviewers understood the core technical results of our paper and provided perceptive comments and questions about our proposed model. All reviewers appreciated our model’s theoretical and empirical results on long time series across many tasks and data modalities, and provided helpful suggestions for discussion and clarification. Their feedback has helped improve our updated submission, which includes:\n\n- A paragraph on limitations and future directions in the Discussion section\n- Improving presentation of the experimental section (e.g. order of tables)\n- (Minor) We have additionally renamed our model from S3 to S4\n\nWe have responded individually to each reviewer in the direct responses.'}, {'title': 'Can one combine the convolutional and recurrent methods?', 'comment': 'As the reviewers have indicated with their scores, this seems to be a great paper which hopefully lives up to its potential as a fast long range sequence model.\n\nI believe there is a small typo in equation (4).  The last term should have a $\\overline{\\mathbf{C}}$ in it.\n\nAlso, I want to ask if one can combine both the fast recurrent method and the fast convolutional method.  One could imagine for example a document summarization task where one wants to use the convolutional method to process a long document and then the recurrent method to auto-regressively generate a summary of that document.  However, it is not clear if the convolutional calculation gives access to final hidden state $x_N$ which would be needed for this.  Similarly, one might want to perform the convolutional method but with a seed \nstate $x_0$ which is not $0$.  For example, imagine processing an extremely long sequence in batches with the convolutional method.  One needs to be able to start the next batch where the previous batch left off. Is this possible?'}, {'summary_of_the_paper': 'The paper extends previous work on linear state space models (SSM), where the state transition matrix is fixed to be a highly structured _HiPPO_ matrix, which has provably beneficial properties for memorizing long-term information from continuous-time signals. The main contribution of the paper is regarding the computational aspect of the model. Namely, a novel approach is proposed for computing the convolutional kernel associated to the discretized SSM unrolled over time. This is done by first showing that the state transition matrix can be decomposed as the sum of a normal (i.e. diagonalizable with orthonormal eigenvectors) and a low-rank (rank 1 or 2) matrix, and this representation is then combined with techniques from numerical linear algebra to reduce the problem to computing the Cauchy kernel (i.e. a well-studied problem). The resulting structured state space model (S3) is then placed into a deep neural network setting, and extensive experiments are carried out on various tasks, such as: 1) Long Range Arena, a benchmark collection for scalable transformers, 2) raw speech classification (length-16k audio signals), 3) generative modelling on CIFAR-10 and WikiText-103, 4) sequential image classification on sMNIST, pMNIST and sCIFAR. Overall, the model seems to perform very well on each task either performing close to SoTA or setting a new high score.', 'main_review': 'The paper seems well written both regards to clarity and citations. Contentwise, the theoretical and experimental parts are interesting and relevant. The novel contribution, which is efficient computation of the discretized convolution kernel, is highly technical with details given in the appendix, but the authors did a good job at summarizing the key ideas. On the LRA benchmark, which was originally introduced to benchmark scalable transformer variants, the model sets a new high score on all problems, outperforming transformers on their home turf. The model also outperforms competitors on a raw audio dataset, including a SoTA CNN variant specialized to audio. Overall, the performance is compelling on all considered tasks, in respect of both accuracy and efficiency.\n\nI have just a few questions/remarks:\n\n1) The authors mention in Section 2.2 that linear SSMs can perform poorly due to vanishing/exploding gradients. The given HiPPO matrix is given by the sum of a normal and a low-rank (i.e. NPLR) matrix, and in particular it is not unitary. I wonder then how it gets around the aforementioned problem? I expect the answer can be found by looking up the cited papers, but if there is a short answer to this, it might be good to include it in the discussion.\n2) Related to the previous question, it turns out the authors only use the HiPPO matrix as an initialization (although as a very sensible one), but then the algorithm is free to learn any NPLR matrix. Is it true in general for NPLR matrices that they result in well-behaved SSMs in the previous sense, or does that only hold for a neighborhood around the HiPPO matrix? Additionally, I would be interested in what happens (e.g. how performance changes) when a) the HiPPO matrix  $\\mathbf{A}$ is fixed throughout the training, and only the parameters $\\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^N$ are trainable, and b) if the model is initialized from a random NPLR matrix, but not the HiPPO?\n3) Is the model able to handle multidimensional input signals $\\mathbf{u}(t) \\in \\mathbb{R}^d$?\n4) In Section 3.4, it is mentioned that a multidimensional feature map of size H is created by defining H independent copies of S3. I am wondering if it would be more efficient if these copies shared the input-to-state ($\\mathbf{B}$) and state-to-state ($\\mathbf{A}$) mappings, and only differed in the state-to-feature mapping ($\\mathbf{C}$)? Do the authors expect this would negatively affect the results? Somehow it seems wasteful to me for each feature-coordinate $y^i(t)$ to have its own separate $N$-dimensional state representation, instead of sharing a common one (perhaps with a larger state size $N^\\prime >> N$).\n5) The S3 itself seems to be a linear model, which made me wonder where the nonlinearities are introduced into the deep model? Perhaps, is there an activation placed on the feature map after linearly mixing H independent copies of S3?\n6) In Section 4.3 paragraph _Irregularly sampled data_, it seems what the authors really mean is a resolution change. As far as I know, irregular sampling means the data is sampled on a highly non-uniform time grid, which means that in eq.(3) the step size $\\Delta_t$ would become time dependent. It looks like this might make the computation of the convolution kernel in eq.(5) a bit problematic (the discretized $\\bar{\\mathbf{A}}_{\\Delta_t}$ matrices might not commute with eachother).', 'summary_of_the_review': 'This is a solid paper with a novel technical contribution that utilizes nontrivial insights for efficient matrix computations, and with a strong experiments section. The experiments demonstrate not only that the model can be an efficient alternative to transformers on tasks requiring long-range reasoning, but that it also shows promise as a generic sequence model that can be applied across a broad range of tasks. Overall, there are not many drawbacks of the paper for me, other than some unanswered questions in my mind.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper presents a novel parameterization for the established state-space models (SSM), which tackles the scalability problem in linear state-space layers (LSSL) in modeling long-range dependencies for very long sequences. The proposed technique reparameterizes the structured state matrix in LSSL that allows the state to memorize the past, a key component following the continuous-time memorization theory. A complete theoretical description of this reparameterization and empirical evaluations on a diverse set of benchmarks are presented. The proposed model, S3, achieves astonishing results both in terms of performance and computational efficiency. ', 'main_review': 'The paper presents a strong and clear theory for the proposed reparameterization. Its advantages over the prior work linear state-space layers (LSSL) are explained very well theoretically and presented empirically. \nThe convolutional kernel defined by LSSL is a convolutional interpretation to unrolling SSMs over time, granting parallelization to SSMs during training. The proposed model S3 resolves the bottleneck in this formulation by reducing it to a Cauchy kernel, resulting in a significant improvement in the space and computation complexity. \nThe model is also compared against the state-of-the-art models in a diverse set of benchmarks. Especially the performance in the Long Range Arena (LRA) benchmark is superior. \n\nI have the following questions for the authors.\n- The base LSSL model is not included in most of the benchmarks, particularly in the LRA. Is it due to the scalability problem of LSSL? \n- Is it expected to get improved results over the LSSL (see Tab. 4 and 5)? Does the proposed reparameterization guarantee a “more” optimal state matrix A? \n- The diagonal matrix $\\Lambda$ and the vectors $p$ and $q$ are trainable parameters that construct the state matrix A which is initialized to be a HiPPO matrix (Section 3.4). Do these trainable parameters preserve the structure of the matrix A to be HiPPO during and after training? Is it necessary? \n- In autoregressive generation tasks (i.e., the model is fed with its own prediction), the sequence models (RNNs, TCNs, etc.) are likely to suffer from error accumulation problem as the prediction horizon increases. Could the authors comment on S3’s performance in a similar task? Can S3 alleviate this problem? \n\n\n-- Post-rebuttal edit --\n\nI thank the authors for their rebuttal. I read other reviews and the author responses. It is clear that this is both empirically and theoretically a strong paper. The improvements over the baselines are substantial. Looking forward to seeing the follow-up work.', 'summary_of_the_review': 'This is theoretically and empirically a solid paper. The evaluations show that its superior performance is not limited to modeling long-term dependencies only, but it can be a strong alternative to the established sequence models. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The authors propose a sequence modeling approach called the structured state space model (S3) which parameterized the SSM in a more computationally efficient manner.  This is done through decomposing the structured state matrices A into a low-rank and skew-symmetric term and expanding the SSM in frequency space and using a multipole-like evaluation.  This approach maintains much of the gains and efficiencies of past SSM approaches while being more computationally stable and efficient as demonstrated on several tasks across broad domains (speech, images, text).  The paper first steps through the theoretical motivations and justifications of this approach and then performs a number of experiments to demonstrate the competitive or superior performance on a wide range of LRD tasks.  ', 'main_review': 'Strengths:\n* The motivation for the approach is clear both from a high-level as well as mathematical viewpoint.  The authors do a good job walking through the theoretical justification and solution of the approach and proposed algorithm.\n\n* The discussion and analysis is performed on a broad domain of tasks.  As the authors discuss, many approaches today are narrowly focused around a single task/domain which limits our broader understanding of LRD modeling.  By using such a varied set of experiments, the usefulness of this approach is well highlighted.\n\n* The introduction is very well written.  The general reader may be less familiar with some of these past works and the authors do an excellent job highlighting the motivation and current status of efforts in this line of study.  Similarly the background is clear and detailed without being unnecessarily complicated or verbose.  \n\n* The performance is compelling from both an efficiency and accuracy standpoint across a number of tasks.\n\n------------------------\n\nWeaknesses:\n* Limitations and next steps aren’t explicitly discussed.  It would be helpful for the authors to include even a few sentences on this.\n\n* A known issue with approaches like transformers is the need for huge datasets (even by deep learning standards). How does the performance of this approach vary as a function of dataset size?  Does this method work in a low-data regime?\n\n* The results section is more difficult to follow, especially compared to the rest of the text.  Many of the results are included in the appendix (which is fine especially given their extensiveness).  However, some of the table references are strange.  Table 4 is given, but not directly referenced in the main text (should go with Raw Speech Classification on p.8).  Table 5 is not referenced until after Table 6.  The section ""Irregularly Sampled Data"" doesn\'t explicitly reference where those results can be found (i.e. Table 4).  This makes the results section a little bit unclear as the reader has to the text-to-table mapping.   Table 4 caption is a bit unclear (i.e. requires going to the text to interpret what things mean).\n\n* Figure 2 (feature visualization): the authors state that the visualization of the low-level features shows that context over a small area is being learned and the rest of the image is ignored.  However, two of the activations seem to span the entire kernel.  Similarly, one of the higher-level filters is over only a small area.  This seems to be more of a general trend (low learns mostly, but not completely localized, high learns mostly, but not completely global), but not a hard and fast rule.  Is there a hypothesis as to why only some, but not all filters in these layers follow these trends?  With the current visualization, a number of the filters appear identical (just a few solid rows at the top)- are these degenerate or is this a limitation of the visualization?  There is less “hierarchical” structural differences in the convolutional filters (Figure 5 appendix).  Could the authors explain this?\n\n* Minor: Figure 1, I missed the green K several times (spent a while looking for the green in the figure).  Perhaps altering the perceptual qualities of the font could help draw attention to these variables more.   \n', 'summary_of_the_review': 'My recommendation is to accept this work.  This paper proposes a novel parameterization for solving SSM which provides computational efficiency and accuracy gains.  The approach is explained and justified theoretically and the performance is evaluated on a number of tasks across different domains.  The text is clear and well written.  The authors explicitly state the goal of finding general methods that work on a broad range of tasks and this approach offers a good step toward solving that problem under realistic computational constraints.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Efficiently Modeling Long Sequences with Structured State Spaces', 'authorids': ['~Albert_Gu1', '~Karan_Goel1', '~Christopher_Re1'], 'authors': ['Albert Gu', 'Karan Goel', 'Christopher Re'], 'keywords': ['sequence models', 'state space', 'RNN', 'CNN', 'Long Range Arena'], 'abstract': ""A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies.  Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps.  A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically.  However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution.  We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.  Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel.  S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'gu|efficiently_modeling_long_sequences_with_structured_state_spaces', 'pdf': '/pdf/a8eedf494f6698cb467c310c59d3ea6488546805.pdf', 'one-sentence_summary': 'We introduce the S3 model based on new algorithms for state spaces that is particularly effective on long-range dependencies.', 'supplementary_material': '/attachment/0eca97b47ae6f910c4be46fcb179d7790227b78a.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/efficiently-modeling-long-sequences-with/code)', '_bibtex': '@inproceedings{\ngu2022efficiently,\ntitle={Efficiently Modeling Long Sequences with Structured State Spaces},\nauthor={Albert Gu and Karan Goel and Christopher Re},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uYLFoz1vlAC}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Haobo Wang', 'Ruixuan Xiao', 'Yixuan Li', 'Lei Feng', 'Gang Niu', 'Gang Chen', 'Junbo Zhao']",ICLR,PiCO_ Contrastive Label Disambiguation for Partial Label Learning,https://iclr.cc/virtual/2022/oral/6039,2022," Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=EhYjZy6e1gJ,https://openreview.net/forum?id=EhYjZy6e1gJ,EhYjZy6e1gJ,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper presents PiCO, a novel approach for partial label learning, which achieves very strong performance close to that of fully supervised learning and outperforms PPL baselines. The experiments are extensive with very impressive results and the analysis are thorough.'}, {'title': 'thanks for the response', 'comment': 'I would like to thank the authors for the informative response. Although I disagree with the setup using clean dev sets, I understand that this is common in several research areas (and sometimes also in unsupervised learning e.g. unsupervised parsing in NLP). \n\nSatisfied with the response, and the latest draft, I would like to keep my original score. '}, {'title': 'Thanks for discussion!', 'comment': 'We thank the reviewer for the extra time and effort spent. We definitely believe that the discussion in the last few days can make our paper more solid. We will incorporate the new results and the fruitful points in our future revision.'}, {'title': 'final comments', 'comment': 'Thank you for the detailed responses. I feel more confident in this work, especially after you provided your explanation and new results. I will keep my original score of 8.'}, {'title': 'Added supervised results on CUB-200', 'comment': 'Dear reviewer, as you may have requested, we just uploaded another revision adding the supervised results on CUB-200 dataset. In Table 6 of Appendix B.3, we can see that PiCO achieves competitive results compared to the supervised setup ($-1.88$% Accuracy) when $q=0.01$. We can also see that when $q=0.1$,   while still with the best results among the competitors, the performance discrepancy between PiCO and the supervised results is enlarged to $13.00$% inaccuracy. These empirical findings are still consistent with our former responses. Hope this is helpful!'}, {'title': 'Response to Reviewer 7qAy for supervised comparison', 'comment': 'Thanks for keeping up the discussion.\n\nFirst, we are absolutely sure about our code and method implementation. We fully guarantee the correctness and reproducibility of the code. That said, with numerous rounds of code reviewing, we do assert to the reviewer that the correctness of our code and the cases the review had pointed out is not presented in PiCO. Also, we uploaded the code at the time of submission, please feel free to check/run it, by running ```bash run.sh```.\n\nSecond, we think you are absolutely right about the natural noise appearing in the dataset. We do *not* claim PiCO, nor any other approaches proposed in the regime of PLL, can get close to the supervised performance in any case. As we simply adopt the exact identical experimental protocol and the dataset, the outcome of this paper is that PiCO is able to approach the supervised performance under certain scenarios and on certain datasets. We do extrapolate that when the natural noise is more widely possessed, the partial label setting might be more challenging especially compared to a full-supervised setup.\n\nThird, we respectfully disagree with the reviewer that our baseline is not strong enough. For one thing, we did carefully tune our ResNet-18-based baseline model. The others results can be better like the reviewer rightfully listed out; nonetheless, with a stronger backbone model such as WideResNet and PreActResNet, the resulting enhancement is not surprising. We chose the original ResNet18 architecture as it is one of the most commonly used in the AI community. Further, this paper is very much not about architectural choices but is more emphatical about the label disambiguation algorithm and contrastive learning. For another, we may reference some high-star open-source GitHub repositories, such as https://github.com/kuangliu/pytorch-cifar, where our ResNet-18-backbone baseline yields a lower **93.02%** accuracy on CIFAR-10.\n\nLast but not least, we kindly ask the reviewer to consider PiCO from a label disambiguation and partial label learning standpoint. As we mentioned, the PLL setup is conceptually reduced to a full-supervised setup and the core challenge is just how to perform the label disambiguation. Put it another way, the excellent performance from PiCO gives us very strong confidence in the capacity of this framework on label disambiguation. In the regime of partial label learning, this is, in our honest opinions, a very promising result/framework that is worthy of spreading.\n\nIn what follows, we also provide detailed clarification for addressing the concerns of the Reviewer.\n\n**Q1. somehow the model is cheating during training --- one way this could happen is if the pseudo targets are randomly chosen every epoch since the supervision signal from the gold label would be stronger as it would always be included.**\n\n**A:** Sorry for confusing the reviewer. We suppose here ""pseudo targets"" refers to the ""candidate label set"", which defines the range that the pseudo targets being updated. It can be checked in our codes that the candidate label sets are generated at the beginning and we **never change** the value of the candidate label sets anymore. When training, the ```DataLoader (train_loader)``` samples data examples with the originally generated candidate label sets. **Nothing will be randomly reset during training.** We refer the reviewer for the attribute ```given_label_matrix``` (i.e. the candidate set) of class ```CIFAR10_Augmentention``` in ```utils/cifar10.py``` and ```utils/cifar100.py``` to see whether it is modified or randomly reset during training. \n\n**Q2: The supervised learning baseline is a weak baseline.**\n\n**A:** We strictly implemented the supervised learning model by using the same experimental setup as PiCO. We used ResNet18 and contrastive learning, which definitely leads to different results. But, we believe we have reported **fair** results for supervised learning models. We also note that we have provided demo codes as well as a shell file ```run.sh``` for running the code, the reviewer can set the parameter ```--partial_rate 0.0``` to train a supervised model and check whether our results are real or not.\n\n**Q3: Is there a reason why you did not provided supervised results for CUB-200, and do you have ideas why the performance gap is bigger for CUB-200 than CIFAR-10 or CIFAR-100?**\n\n**A:** That\'s definitely our bad. The answer is the same as Q2 that we used different experimental setups but the results are fair for all the comparing methods. We will report the supervised results of CUB-200 soon. \n'}, {'title': 're: supervised comparison', 'comment': '> When the label disambiguation is perfectly accomplished, i.e. the label space uncertainty is reduced to zero, the PLL setup is equivalent to being fully supervised.\n\nI see. This is roughly backed up by the strong training accuracy performance in Table 5.\n\nIn general, I am not sure that optimizing your loss function guarantees for the ""gold label"" to be predicted, since there is often natural noise in the data due to natural ambiguity and/or human error in annotation, but I am not sure to what extent this is for the relevant datasets.\n\nAgain, sorry if I was not being clear and let me be more direct. Although I gave the paper a high score, I still have some skepticism of the results because the performance for PLL is so close to supervised learning. I have some small concerns that either a) somehow the model is cheating during training --- one way this could happen is if the pseudo targets are randomly chosen every epoch, since the supervision signal from the gold label would be stronger as it would always be included, or b) the supervised learning baseline is a weak baseline.\n\n> supervised models\n\nAfter further inspection, it appears supervised models typically perform better than the numbers included in this paper. For example, Wide ResNet (2016) scores about 96% accuracy on CIFAR-10 and 81% accuracy on CIFAR-100.\n\nFor CUB-200 I assume you are using the configuration where bounding box is not provided at train or test time. I think that supervised Bilinear-CNN (ICCV 2015) achieves higher than 80% accuracy. Is there a reason why you did not provided supervised results for CUB-200, and do you have ideas why the performance gap is bigger for CUB-200 than CIFAR-10 or CIFAR-100?\n\nPerhaps I have misunderstood whether these supervised baselines are directly comparable with your results, so feel free to clear things up. :)'}, {'title': 'Response to Reviewer 7qAy for quick points', 'comment': 'We refresh the answer down below. Hope this time the explanation could further help! Let us know if you have any further questions.\n\n**Q1. I was hoping you would explain more why your performance in the paper is surprising or not.\xa0Some scientific hypothesis or additional analysis would be helpful.**\n\n**A:** Got you! Basically, this performance closeness can be intuitively explained as follows. When the label disambiguation is perfectly accomplished, i.e. the label space uncertainty is reduced to zero, the PLL setup is equivalent to being fully supervised. Notice that this is also due to the fundamental assumption of the PLL --- the gold label appears in the label candidate set. From this standpoint, as we are discussing PiCO\'s excellent performances, it implies that PiCO achieves supreme label disambiguation effect thanks to the contrastive learned embeddings and the prototypes.\n\nIndeed, to comprehensively answer this question is very much multidisciplinary. It depends on the development of contrastive learning, PLL methods and etc. Through PiCO, we pioneer to validate the strong functionality of contrastively trained embeddings for PLL setup both empirically and theoretically. Though a bit unrelated, there has also been a few papers centered on the theory of pure contrastive learning (not for PLL) as in [1,2,3] that could be helpful to check. To sum up, we look forward to keeping the efforts pushing this direction of contrastive learning for PLL in the future.\n\n[1] Saunshi N, Plevrakis O, Arora S, et al. A theoretical analysis of contrastive unsupervised representation learning[C]//International Conference on Machine Learning. PMLR, 2019: 5628-5637.\n\n[2] HaoChen J Z, Wei C, Gaidon A, et al. Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss[J]. arXiv preprint arXiv:2106.04156, 2021.\n\n[3] Zhang Y, Hooi B, Hu D, et al. Unleashing the Power of Contrastive Self-Supervised Visual Models via Contrast-Regularized Fine-Tuning[J]. arXiv preprint arXiv:2102.06605, 2021.\n\n**Q2. For the sake of discussion, I also think that unsupervised clustering with image features may get close to supervised performance in some cases after mapping clusters with to labels, although I haven\'t recently looked at results for the relevant datasets.**\n\n**A:** The answer to this question very much overlaps with Q1 :). The unsupervised clustering perhaps also has room to improve, indicated by the performance of PiCO on PLL benchmarks. However, there is one phenomenon difference between unsupervised setting and the PLL is with the acquirement of the gold label in the candidate set serving as a weak supervision signal. Further, whether the unsupervised clustering could get close or not to the supervised performance also depends on other factors, such as the complexity of the data distribution and etc. While think this is definitely an interesting direction, we do posit this goes way beyond the scope of this paper --- with PiCO, we are dedicated to the PLL setting and standardized benchmarks.\n\n**Q3. Also, I would be curious to know if you think the ""closeness"" is because supervised models still needs to be improved.**\n\n**A:** As well, the answer to this question can also be related to Q1. We cannot be sure if the supervised models still need to be improved, but as we mentioned above, in the optimal condition where the label disambiguation is perfectly done, the PLL is equivalent to a supervised setup.\n\n**Q4. Are random labels chosen at beginning and kept throughout? It is not 100% clear to me that random ""pseudo labels"" are chosen at beginning and kept until the end of training.**\n\n**A:** If we get the reviewer right, the \'\'pseudo labels\'\' here may refer to the \'\'pseudo targets\'\'. And the answer to this question is that the pseudo targets are set as uniform vectors $s_j=\\frac{1}{|Y|}\\mathbb{I}(j\\in Y)$ (not random) at the beginning and updated by using the candidate label sets as well as the prototypes. In addition, the true gold labels were never revealed to the model, nor any part involved in the training.\n\nThe attribute ```confidence``` in Python class ```Partial_loss``` stands for the pseudo targets. We refer the reviewer to ```utils/utils_loss.py, line 5-29``` in our codes. \n\n**Q5. Can you verify that all nodes are using the same set of randomly selected ""pseudo labels""?**\n\n**A:** This is certainly our bad in terms of writing. Technically, we implemented PiCO using a distributed setup because we thought parallelization would be needed. However, while we train PiCO we had never actually enabled the distributed setup but only resorted to one single GPU training. We will definitely make this clearer in the code that we release upon publication. Hope this helps!\n\n**Q6. typo: Loss wight**\n\n**A:** Thank you!'}, {'title': 'quick points', 'comment': 'Thanks for the nice work and detailed response. I wanted to quickly follow up on a couple points.\n\n1. More should be said about the closeness of PLL performance and supervised learning performance.\n\nSorry if I was not clear. I was hoping you would explain more why your performance in the paper is surprising or not. Certainly it is surprising to me that performance in PLL can be so close to fully supervised setting. This explanation does not necessarily need new empirical results, but some scientific hypothesis or additional analysis would be helpful. Perhaps I am missing something obvious and you can explain to me in the comments why the performance is surprising or not.\n\nI appreciate the connection to EM and feel it is related, as well are the results on fine-grained classification (table 3). For the sake of discussion, I also think that unsupervised clustering with image features may get close to supervised performance in some cases after mapping clusters with to labels, although I haven\'t recently looked at results for the relevant datasets.\n\nAlso, I would be curious to know if you think the ""closeness"" is because supervised models still needs to be improved.\n\n2. (new point) Are random labels chosen at beginning and kept throughout?\n\nThis is perhaps a silly question, but I would like to double check anyway. I was looking at code provided in the supplementary material. It is not 100% clear to me that random ""pseudo labels"" are chosen at beginning and kept until the end of training. Can you please verify if this is the case? Similarly, I noticed that there is support for distributed training --- can you verify that all nodes are using the same set of randomly selected ""pseudo labels""?\n\n---\n\ntypo: Loss wight'}, {'title': 'Response to Reviewer fAwc', 'comment': 'Thank you! As a briefing, we added the following section in our revision to respond to the reviews: (I) we added experiments in the label-dependent setup in Appendix B.5; (ii) we added derivation of the second equality in Eq. (9) in Appendix A. We provide a more detailed explanation and responses as the following.\n\n**Q1: Experiments are mainly conducted on datasets with uniformly generated candidates, it is not clear how PiCO performs on non-uniform datasets, such as in the label-dependent setting where the probability of label flipping depends on its ground-truth label.**\n\n**A:** This is indeed a good point, thank you! In our original version, we provide an experimental validation on the non-uniform case for CIFAR-100 with hierarchical classes, where false positive labels are generated within the same superclass.\n\nAdditionally, we add a new set of experiments for the label-dependent setting shown in Appendix B.5. With slightly more details, we closely follow [1] and generate false candidates with varying probabilities w.r.t. the ground-truth. The results demonstrate that PiCO achieves significantly better performance than the baselines under this setup.\n\n[1] Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11091–11100. PMLR, 2021. \n\n**Q2: Why does the second equality in Eq. (9) hold? It would be appreciated if a clear explanation is provided.**\n\n**A:** We are sorry for the lack of clearness. We have added more clarification and derivation in the revision. We may refer the reviewer to Appendix A in the revised manuscript. Thank you for pointing this out. \n\n**Q3: There is a redundant space notation in Table 1 (the method CC with q=0.5 on CIFAR10).**\n\n**A:** Our bad! Please check the revision.'}, {'title': 'Response to Reviewer k7zr', 'comment': 'Thank you very much for your comments and suggestions! We are happy you enjoyed the paper. We have updated our paper based on the suggestions and comments of the reviewers. Summary of revision: (i) we added the procedure of hyperparameter selection in Appendix B.1; (ii) we added related works about prototype learning literature in Appendix D. \n\n**Q1: As the approach is EM-like, it also inherits some cons from EM. This is however not presented in the paper. For instance, in which case the learning will get stuck in a bad optimum? What is the consequence? Is there any way to avoid that?**\n\n**A:** We acknowledge the bad optima problem with the conventional EM family algorithms. However, the PLL problem setup naturally differs because, as we describe in Section 2, it restricts the gold label to be included in the candidate set. In that regard, we believe that this piece of information could largely help avoid the bad optimum problem that occurs in a pure unsupervised setup --- for instance, running EM for unsupervised k-means clustering. \n\nIn our extensive experiments on all well-known benchmarks in this field, we did not find this problem of falling into bad optima for PiCO. However, we do acknowledge that PiCO, or any other PLL methods, may have this problem when the supervision signal is too scarce. Technically, this may be alleviated by using standard techniques in the literature of the EM algorithm like [1]. Yet this may have gone beyond the scope of this paper where our focus is on a commonly adopted PLL setup.\n\n[1] Shaban A, Farajtabar M, Xie B, et al. Learning Latent Variable Models by Improving Spectral Solutions with Exterior Point Method[C]//UAI. 2015: 792-801.\n\n**Q2: It is unclear how the models were fine-tuned. Clean dev sets can provide lots of information for removing labeling ambiguity.**\n\n**A:** Generally, we strictly follow the standard procedure in the recent literature of PLL [2,3]. Namely, to conduct the hyperparameter selection, we rely on a clean dev set that is drawn from the training set at a ratio of 10%. After this step, we transform the clean dev set back to its original PLL form and incorporate it into the training set to accomplish the final model training.\nAlongside making the comparison with the prior works fair, we additionally want to refer the reviewer to Appendix B.2 for ablation results. PiCO does not seem to be sensitive to the hyperparameter choices. We hope this may mitigate the concern raised by the reviewer on the functionality of the clean dev set.\n\n[2] Lei Feng, Jiaqi Lv, Bo Han, Miao Xu, Gang Niu, Xin Geng, Bo An, and Masashi Sugiyama. Provably consistent partial-label learning. In NeurIPS, 2020.\n\n[3] Hongwei Wen, Jingyi Cui, Hanyuan Hang, Jiabin Liu, Yisen Wang, and Zhouchen Lin. Leveraged weighted loss for partial label learning. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 11091–11100. PMLR, 2021. \n\n**Q3: It seems that the paper mentions too little about prototype learning literature.**\n\n**A:** We have added more discussions about prototype learning in Appendix D.'}, {'title': 'Response to Reviewer 7qAy', 'comment': ""Thanks very much for your insightful comments and suggestions! We have updated our paper based on the comments of the reviewers. Summary of revision: (i) we added discussion about the closeness of PLL performance and supervised learning performance in Appendix B.2; (ii) we tested the training time of different prototype calculation methods in Appendix B.6.\n\n**Q1：More should be said about the closeness of PLL performance and supervised learning performance. This comment may seem counterintuitive, but I do not think this closeness can be so easily glossed over, especially when it is one of the main results.**\n\n**A:** This is a very good point, thank you! Indeed, while we show that PiCO displays on-par results with a fully-supervised setup in Table 1, we want to identify that this closeness does depend on the dataset setup, in particular on the average number of false candidates $q$ in section 4.1. We show the most commonly used setup for $q$ in order to compare with prior works (Table 1). However, if we increase $q$, as the label space uncertainty gets aggravated, PiCO together with all other PLL methods would anticipate a performance drop.\n\nTo further analyze this point, we added another set of empirical evaluations in our newest revision. We refer the reviewer to Table 5 in Appendix B.2, where we evaluate the training accuracy of the generated pseudo targets. As an example, we found that under the setting of $q=0.05$, the pseudo targets achieve a 96.27% training accuracy on CIFAR-100. In addition, in Figure 7, we show that the mean max confidence score of the pseudo target is close to 1 over the training dataset. This is somewhat a promising result to us because it demonstrates PiCO has managed to disambiguate a majority of the uncertain partial label sets. Not only does it interpret the performance closeness of PiCO against the fully-supervised setup, but also highlights the effectiveness of our contrastively trained representations in enabling strong label disambiguation. \n\n**Q2: Treatment of clustering is simplistic. K-means is a very specific type of clustering algorithm, and only one of multiple classic clustering techniques.**\n\n**A:** We apologize for this confusion! We do not include a K-means process in our PiCO method, as shown in the pseudo-code provided in Appendix C. PiCO is related to center-based clustering (e.g. K-means) in our theoretical analysis. We draw a connection between our contrastively learned representation in PiCO with the center-based clustering algorithms in Section 5. We will make this clearer in our revision.\n\n**Q3: How is the 'novel' prototype-based label disambiguation so different from using a softmax layer? A softmax layer will also have a prototype vector for each class.**\n\n**A:** Notably, the prototype vector calculated in PiCO is based on the contrastive learned embeddings. We may refer the reviewer to Eq. (8). The major difference between our prototypes and the softmax layer is attributed to the uniformity term (b) in Eq. (8). We argue that the minimization of such a term encourages information preservation in the contrastive embedding space, as proved in [1], which also makes the contrastive prototypes more representative.\n\n[1] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In ICML, volume 119 of Proceedings of Machine Learning Research, pp. 9929–9939. PMLR, 2020.\n\n**Q4: I can see the convenience of momentum style updating of prototype vectors, but re-computing them every iteration (or every N iterations) is not so expensive and at most increases training by 2x.**\n\n**A:** The reviewer rightfully pointed out a possible variant for the calculation of the prototypes. While re-computing the prototype over the entire training set per-iteration can be overly expensive, we add a new experiment where the prototypes are re-computed by averaging embeddings of all training examples at the end of each epoch. As we can see from Table 8, this variant does achieve the on-par result with PiCO but ends up with a much slower training speed.\n""}, {'summary_of_the_paper': 'The authors present a new technique for partial label learning (PLL). PLL is the task where the labels for each instance include both the ground truth label and a randomly sampled set of distractor labels, and during training the model learns a latent decision for which among this set is the ground truth. The technique presented by the authors uses a combination of momentum (in the representation) and contrastive learning (to augment the label set) that leads to improved PLL results, reaching nearly fully supervised performance.', 'main_review': 'Strengths\n- Incredibly strong PLL performance close to supervised learning and substantially stronger than baselines.\n- Helpful ablations and results on various datasets.\n- Provides theoretical connection of the proposed method to EM.\n\nWeaknesses\n- More should be said about the closeness of PLL performance and supervised learning performance. This comment may seem counterintuitive, but I do not think this closeness can be so easily glossed over, especially when it is one of the main results.\n- (minor) Treatment of clustering is simplistic. K-means is a very specific type of clustering algorithm, and only one of multiple classic clustering techniques.\n\nQuestions and comments:\n\n- How is the “novel” prototype-based label disambiguation so different from using a softmax layer? A softmax layer will also have a prototype vector for each class.\n- I can see the convenience of momentum style updating of prototype vectors, but re-computing them every iteration (or every N iterations) is not so expensive and at most increases training by 2x.\n', 'summary_of_the_review': 'This seems like a particularly strong result. More could be said about the implications of their results, but otherwise seems like a good paper.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposes an innovative approach to partial label learning, where an instance is assigned with some false positive labels besides its true label (due to difficulty in labelling). The approach blends contrastive learning with prototype learning: (i) the former helps to form good clusters for the latter to learn prototype representations (ii) in return the latter helps to select positive samples for the former. Theoretically, the authors prove that the two of them work collaboratively in the EM fashion. \n\nThe paper demonstrates the effectiveness of the proposed approach using a typical setting for the task. Specifically, CIFAR 10 and CIFAR 100 were used and each instance was randomly assigned false labels with some probabilities. The proposed approach achieved impressive results, substantially outperforming five recent models in the literature. Moreover, its performance strongly approaches the one of supervised learning. \n\nThe paper also presents a much stricter setting when false positive labels are semantically correlated with true labels. The proposed approach also gained impressive results on CUB-200 and CIFAR-100-H datasets. Moreover, the paper shows several in-depth analyses.\n\n', 'main_review': ""## Strengths \n\nThe problem that the paper tackles, namely partial label learning, is important in the real life where labelling is difficult due to semantic ambiguity (e.g. husky vs malamute). The problem has several connections especially with weakly supervised learning. \n\nThe approach proposed in the paper is very well motivated. The idea of making contrastive learning and prototype learning is sound as they can work collaboratively in the EM fashion (proved in the paper). \n\nThe approach is backed by very impressive empirical results. The presented analyses support the motivation of the approach, e.g. cluster visualisation shows that the clusters are well formed with few classification errors. \n\nThe paper is well written with a clear structure. Reproduction doesn't seem difficult. \n\n\n## Weaknesses \n\nAs the approach is EM-like, it also inherits some cons from EM. This is however not presented in the paper. For instance, in which case the learning will get stuck in a bad optimum? What is the consequence? Is there any way to avoid that?\n\nIt is unclear how the models were fine-tuned. Did the authors use clean dev sets which don't have any false positive labels? If this is the case, would it be more realistic to fine-tune the models on noisy dev sets instead? One concern is that clean dev sets can provide lots of information for removing labelling ambiguity. For instance, one can achieve reasonably good prototype representations just from a few clean labelled instances. \n\nIt seems that the paper mentions too little about prototype learning literature. \n\n"", 'summary_of_the_review': 'This is a strong paper tackling an important problem. The approach is interesting and the technique is sound. The claims are well backed by impressive empirical result and theory. Therefore, overall, the contributions of the paper will have a certain impact to the community.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This work approaches the partial label learning problem where each training example is annotated with multiple candidate labels, in contrast to the conventional supervised learning setup that the ground-truth label is provided. The proposed framework comprises two key components: (1) a contrastive learning module that uses the classifier output to select positive pairs and (2) a label disambiguation method that uses the contrastive prototypes to update the pseudo targets in a moving-average style. The experimental results look quite strong, where the performance of PiCO nearly approaches the fully-supervised results.', 'main_review': 'I have the following comments.\nPros:\n1.\tThe structure of this paper is well-written and easy to follow. The motivation is clear, and the solution is simple but effective.\n2.\tI think the theoretical justification of the proposed method from the expectation-maximization perspective is very interesting. It is a generic result and potentially help the community understand the property of contrastive learning.\n3.\tThe experimental results are quite strong and the ablation study also looks good. I especially appreciate the results on new fine-grained datasets to test the performance of PLL methods.\nCons:\n1.\tExperiments are mainly conducted on datasets with uniformly generated candidates, it is not clear how PiCO performs on non-uniform datasets, such as in the label-dependent setting where the probability of label flipping depends on its ground-truth label.\n2.\tWhy does the second equality in Eq. (9) hold? It would be appreciated if a clear explanation is provided.\n3.\tThere is a redundant space notation in Table 1 (the method CC with q=0.5 on CIFAR10).\n', 'summary_of_the_review': 'This paper is clear and technically solid. The theoretical justification is interesting.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'PiCO: Contrastive Label Disambiguation for Partial Label Learning', 'authorids': ['~Haobo_Wang1', '~Ruixuan_Xiao1', '~Yixuan_Li1', '~Lei_Feng1', '~Gang_Niu1', '~Gang_Chen6', '~Junbo_Zhao1'], 'authors': ['Haobo Wang', 'Ruixuan Xiao', 'Yixuan Li', 'Lei Feng', 'Gang Niu', 'Gang Chen', 'Junbo Zhao'], 'keywords': ['Partial Label Learning', 'Contrastive Learning', 'Prototype-based Disambiguation'], 'abstract': 'Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity.  Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO.', 'pdf': '/pdf/f9275b96d741f229db4e61a15ce5f2a499c9ee67.pdf', 'one-sentence_summary': 'A synergistic PLL framework that leverages contrastive learning for enhanced representation and improved label disambiguation.', 'supplementary_material': '/attachment/23c0ecc1a1d5842f6f810a3a17d15f9016553246.zip', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wang|pico_contrastive_label_disambiguation_for_partial_label_learning', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/pico-contrastive-label-disambiguation-for/code)', '_bibtex': '@inproceedings{\nwang2022pico,\ntitle={Pi{CO}: Contrastive Label Disambiguation for Partial Label Learning},\nauthor={Haobo Wang and Ruixuan Xiao and Yixuan Li and Lei Feng and Gang Niu and Gang Chen and Junbo Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EhYjZy6e1gJ}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Anirudh Goyal', 'Aniket Didolkar', 'Alex Lamb', 'Kartikeya Badola', 'Nan Rosemary Ke', 'Nasim Rahaman', 'Jonathan Binas', 'Charles Blundell', 'Michael Mozer', 'Yoshua Bengio']",ICLR,Coordination Among Neural Modules Through a Shared Global Workspace,https://iclr.cc/virtual/2022/oral/6383,2022," Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=XzTtHjgPDsT,https://openreview.net/forum?id=XzTtHjgPDsT,XzTtHjgPDsT,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper takes inspiration from Global Workspace Theory to propose a modification for attention-based network architectures. This is exemplified both in transformer models and in recurrent models (RIMs). The key idea is to replace the quadratic, pairwise communication between ""specialist"" units (which in transformers corresponding to the positions) by a higher-order communication model which consists in a competitive, sparse writing step into a shared workspace, followed by a reading step where information is broadcasted from the global workspace to all specialists. The competitive writing step establishes a limited bandwidth channel for this communication which encourages specialization.\n\nThe reviewers agree that this is an interesting and very well-written paper which unifies several existing ideas. The main contribution of this paper is in establishing a connection to GWT which may inspire future research to keep developing these ideas. The experiments on relatively small tasks (but challenging ones) provide a good proof of concept. Some concerns pointed out by some of the reviewers include a certain overstatement of the capabilities of the proposed model, as well as lack of experiments that scale up the model to larger and unstructured datasets. The authors replied with additional experiments included in the appendix, which in my opinion address these concerns convincingly. \n\nOverall, this is a strong paper and I recommend acceptance. I encourage the authors to take into account the reviewer\'s suggestions in the final version. I also think that the connection to related work could be improved, as there is several related works [1, 2, 3] which asks/investigates similar questions to this paper and should probably be acknowledged:\n- The ""shared global workspace"" of this paper (Transformer + SW) is reminiscent of the Star-Transformer [1], as well as other more recent works which use special units (e.g. CLS tokens) to encode ""global"" representations. While that work does not include the competitive component (the ""bottleneck""), I think it should be acknowledged.\n- Variants of transformers with competition among specialists via sparsity have also been proposed, e.g. adaptively sparse transformers [2]. That framework is an alternative to top-k softmax used in this paper.\n- Empirical studies which analyze the redundancy among specialists (in this case attention heads) and propose strategies to prune them have also been made by [3]. \n\n[1] https://arxiv.org/abs/1902.09113\n[2] https://arxiv.org/abs/1909.00015\n[3] https://arxiv.org/abs/1905.09418 \n\nMinor point: ""Hence unlike pairwise interaction, messages passed among neural modules in the shared workspace setting also include HO interaction terms"" -- I believe higher-order interaction happens too every two layers with pairwise interaction. Perhaps this should be clarified.'}, {'title': 'Thanks :) ', 'comment': 'We thank the reviewer for the review. We are enthused that the reviewer found the experimental results very promising, and is enthusiastic about the paper. We will clarify the technical details in the next version of the paper.\n'}, {'title': 'Multi-agent + Hyperparameters + Experiments on BabyAI', 'comment': 'We thank the reviewer for their time in reviewing the paper and providing constructive feedback. \n\n**Multi-agent tasks are more challenging as each agent would get a different input but also predict a different action.**\n\nEvaluating the proposed architecture on mutl-agent RL where different agents can follow different objectives would be an interesting area for future work.  For this work, we have shown the utility of the idea by using it across different architectures (Transformers and slot based models), and different tasks. \n\n**Regarding Hyperparameters**\n\nFor selecting the hyper-parameters, we tried various different values for each hyper-parameter for the baselines as well as the proposed method. For example, for the patch size we tried 4 x 4, 6 x 6, 8 x 8, 16 x 16 and found that 4 x 4 worked best for detecting equilateral triangles. For object tracking, 6 x 6 is the inherent grid size in the dataset which we do not modify while the image size is 224 x 224. We process each image in cater object tracking using a convolutional network and do not split it into patches before processing.  For the memory slots, we experimented with various values and have reported the best for each experiment. We have also presented an ablation over the number of memory slots for the bouncing balls experiments where we show that 5 slots work best. \n\n**more experiments on BabyAI**\n\nTo further show that the proposed model is useful in more general cases we conduct experiments on the babyai framework[1]. BabyAI is a benchmark for studying the sample efficiency for RL algorithms for grounded language learning. Here, an agent is navigating the environment with the aim to complete its goal described by natural language. The tasks in this benchmark are procedurally generated and hence zero-shot generalization becomes a challenge for RL algorithms.\n\nIn Babyai, we evaluate the model on different environments than those we used for training. We train on - GoToObj, GoToRedBall, GoToRedBallGrey, GoToLocal, PutNextLocal, PickUpLoc, GoToObjMaze, GoTo, Pickup, UnblockPickup, Open, Unlock, PutNext, Synth, SynthLoc, GoToSeq, SynthSeq, GoToImpUnlock. We evaluate on - BossLevel. We also evaluate a harder version on BossLevel (which we call BossLevelGen) where we have more rooms and larger room sizes. Thus this is a zero-shot generalization setting. The goal behind testing on multi-task setup is to see if the division of labour among different modules can help the policy to generalize in a zero-shot manner to more complex tasks (BossLevel).\n\n| Model | Num Modules | BossLevel | BossLevelGen | \n| -------- | ------------------- | -------------  | ------------------- |\n| RIM          |    10           |      0.28     |   0.09               |\n| RIM + SW |  10            |      0.35     |    0.21              |         \n\nWe can see that RIM with shared workspace convincingly outperforms RIMs with pairwise communication. Also, babyai is a general navigation environment that is not sparse thus confirming that shared workspace does show promising improvements in more general settings and offers superior generalization performance. \n\nWe would like to point out that we do provide results on language modeling in appendix section G.1.2 where we show that replacing the pairwise communication in TIMs with shared workspace style attention results in a lower perplexity. We would also like to point out that both atari and bouncing balls are settings where the performance does not depend on small portions of the input. For bouncing balls one needs the ability to model higher-order interactions (since more than 2 balls may collide at the same time) which is not present in pairwise attention but is possible using a shared workspace.\n\n[1] https://arxiv.org/abs/1810.08272\n\n\n\n\n'}, {'title': 'More experiments on BabyAI', 'comment': 'We thank the reviewer for their time in reviewing the paper and providing feedback. We are enthused that the reviewer find the proposed method  appealing. We ran more experiments to address the concerns of the reviewer. \n\n**approach will scale to larger, more unstructured datasets**\n\nTo further show that the proposed model is useful in more general cases we conduct experiments on the babyai framework [1]. BabyAI is a benchmark for studying the sample efficiency for RL algorithms for grounded language learning. Here, an agent is navigating the environment with the aim to complete its goal described by natural language. The tasks in this benchmark are procedurally generated and hence zero-shot generalization becomes a challenge for RL algorithms.\n\nIn Babyai, we evaluate the model on different environments than those we used for training. We train on - GoToObj, GoToRedBall, GoToRedBallGrey, GoToLocal, PutNextLocal, PickUpLoc, GoToObjMaze, GoTo, Pickup, UnblockPickup, Open, Unlock, PutNext, Synth, SynthLoc, GoToSeq, SynthSeq, GoToImpUnlock. We evaluate on - BossLevel. We also evaluate a harder version on BossLevel (which we call BossLevelGen) where we have more rooms and larger room sizes. Thus this is a zero-shot generalization setting. The goal behind testing on multi-task setup is to see if the division of labour among different modules can help the policy to generalize in a zero-shot manner to more complex tasks (BossLevel).\n\n| Model | Num Modules | BossLevel | BossLevelGen | \n| -------- | ------------------- | -------------  | ------------------- |\n| RIM          |    10           |      0.28     |   0.09               |\n| RIM + SW |  10            |      0.35     |    0.21              |         \n\nWe can see that RIM with shared workspace convincingly outperforms RIMs with pairwise communication. Also, babyai is a general navigation environment that is not sparse thus confirming that shared workspace does show promising improvements in more general settings and offers superior generalization performance. \n\nWe would like to point out that we do provide results on language modeling in appendix section G.1.2 where we show that replacing the pairwise communication in TIMs with shared workspace style attention results in a lower perplexity. We would also like to point out that both atari and bouncing balls are settings where the performance does not depend on small portions of the input. For bouncing balls one needs the ability to model higher-order interactions (since more than 2 balls may collide at the same time) which is not present in pairwise attention but is possible using a shared workspace.\n\n[1] https://arxiv.org/abs/1810.08272\n'}, {'title': 'More experiments on BabyAI + Presentation + Neural Modules/Specialists ', 'comment': 'We appreciate that the reviewer enjoyed reading the paper. \n\nThe reviewer raises the point that the claims in the paper are not well supported in the paper. Specifically that the proposed model is suited for inputs where there is inherent sparsity and not in more general cases as we claim. We thank the reviewer for allowing us to clarify this. We would first like to explain our thinking behind using the current experiments presented in the paper.  We also ran more experiments on the grid world navigation task, to show that the proposed method achieves better results as compared to using pair-wise interactions.\n\nWe would like to point out that shared workspace offers 3 main properties:\n\n- Introduces a communication bottleneck that forces the model to focus on only the most relevant information. \n- Incentivizes division of labor among specialists (modularization) \n- Captures higher-order interactions.\n\nTo show the effectiveness of a communication bottleneck, we specifically tested the proposed model on sparse inputs where attending to only relevant information would be necessary. This is the reasoning behind the vision tasks such as cater, sort-of-clevr, etc. To show that the proposed model incentivizes modularization we use the multi-mnist generation task and the bouncing balls task. The ARI measure used in the bouncing balls task measures how well the slots in the base model (in this case the rim modules) are clustered in terms of the objects that they capture. Therefore a higher ARI indicates better clustering, hence better modularization. We can see that using the proposed shared workspace does achieve the best ARI. Capturing higher-order interactions would be particularly useful in environments like atari and bouncing balls since they generally contain interactions of more than 2 objects at a time. We can see that shared workspace communication does have a better performance in both bouncing balls (better ARI [figure 9] and better mse [table 2]) and atari while pairwise communicative attention has a poorer performance which could partly be attributed to its failure to capture higher-order interactions. We would also like to point that bouncing balls and atari are more general frameworks where the information is not sparse. We have also tested the proposed model on language modeling, which is also a more general framework. The results for language modeling are presented in appendix section G.1.2. We can see that using shared workspace in TIMs results in a lower perplexity as compared to pairwise attention.   \n\n**Zero shot generalization in BabyAI**\n\nTo further show that the proposed model is applicable in more general settings, We use the babyai framework [1]. Here, an agent is navigating the environment with the aim to complete its goal described by natural language. The tasks in this benchmark are procedurally generated and hence zero-shot generalization becomes a challenge for RL algorithms. In babyai, we evaluate the model on different environments than those we used for training. We train on - GoToObj, GoToRedBall, GoToRedBallGrey, GoToLocal, PutNextLocal, PickUpLoc, GoToObjMaze, GoTo, Pickup, UnblockPickup, Open, Unlock, PutNext, Synth, SynthLoc, GoToSeq, SynthSeq, GoToImpUnlock. We evaluate on - BossLevel. We also evaluate a harder version on BossLevel (which we call BossLevelGen) where we have more rooms and larger room sizes. Thus this is a zero-shot generalization setting.The goal behind testing on multi-task setup is to see if the division of labour among different modules can help the policy to generalize in a zero-shot manner to more complex tasks (BossLevel).\n\n| Model | Num Modules | BossLevel | BossLevelGen | \n| -------- | ------------------- | -------------  | ------------------- |\n| RIM          |    10           |      0.28     |   0.09               |\n| RIM + SW |  10            |      0.35     |    0.21              |         \n\nWe can see that RIMs with shared workspace convincingly outperforms RIMs with pairwise communication (higher is better). Also, babyai is a general navigation environment that is not sparse thus confirming that shared workspace does show promising improvements in more general settings and offers superior generalization performance. \n\n[1] https://arxiv.org/abs/1810.08272\n\n**Regarding Presentation**\n\nWe thank the reviewer for giving us thorough feedback regarding the presentation of the paper, we will incorporate the feedback in the next revision of the paper. \n\nRegarding “neural modules” and “specialists” - The reviewer is correct that “neural modules” and “specialists” refer to the same thing. For example, in the bouncing balls experiment each slot that represents a single object is the “neural module” or “specialist”. '}, {'summary_of_the_paper': ""This paper proposes a communication framework to have multiple modules communicate and switch precedence efficiently, taking inspiration from Global Workspace Theory in cognitive science. The primary contribution is a scheme to replace complete pairwise communications in modularized architectures with a single, limited-capacity workspace that persists and changes over stages of computation. This workspace is implemented with a read/write scheme that iterates through stages of computation (layers in a Transformer or steps in a recurrent architecture): in the write phase, the shared memory is updated according to the current states of the modules that are most informative to the shared memory's current state as determined by a key-value attention scheme, with the modules competing via softmax. In the read phase, the internal states of all modules are updated via another key-value attention scheme. The advantages are claimed to be 1) higher order interaction among modules because every module learns from every other one (at least, more than pairwise), dynamic filtering because the memory persists and updates stage to stage; and linear computational complexity because the number of memory slots doesn't change much (and it's typically small, 1-10).  \n\nThe related work motivates this paper by the classical AI principle that intelligent systems should have multiple specialized modules rather than one general entity. It distinguishes from prior slot-based memory work in that memory writes here are sparse and competitive, and prior work on reducing computational complexity of Transformer dot product attention through its persistent memory. It aptly assesses itself as a unification of existing ideas. \n\nThe experimentation section tests different parts of the proposed scheme. The triangles experiment tests the comparative speed to convergence (and accuracy) of the HO communications here compared to pairwise in baseline Transformer. The MNIST generation experiment with TIMs shows that the shared workspace gives an advantage on domains where input dimensions are mostly independent. The CATER experiment shows a similar result to the triangles experiment (quickly picking out only relevant information) but in a time series, and the Sort-of-CLEVR experiment again reinforces the power of the shared workspace on sparse tasks. The physical reasoning shows general improvement, and the Atari performance shows considerable improvement due to modularization. \n"", 'main_review': 'I enjoyed reading this paper! I think the strengths fall into three main categories: good problem setup that took the reader from concept to formulation well, an interesting idea with good scoping, and an impressive experimentation suite. More details:\n\n**Good problem setup:** I found the discussion of advantages of a shared workspace compelling. The proposed structure is intuitive and simple, but nonetheless convincing about its claimed advantages of higher-order communications, dynamic filtering, and complexity advantages. The qualitative parts of the writing are strong, effectively building a case for reading this paper. \n\n**Interesting idea with good scoping:** I appreciate that this is one architectural change that considers all inputs and outputs, and shows its advantages over the right baselines. This work avoids the trap of making minor changes and then being unable to really ablate them. The paper does seem to be a unification of existing ideas, but I distinguish that from (and prefer it over) merely concatenating existing ideas. \n\n**Impressive experimentation suite:** There are of course many experiments with thematic consistency, which is great - always better than papers with two testbeds and virtually the same experiment over and over. The first four testbeds make a nuanced point about sparse inputs, and the results on Atari are particularly exciting because they make a general claim and show significant improvement. \n\nMy critiques are as follows, in order of significance: \n\n**Content**\n1. Main critique: the goals of this paper seem to be to show that accuracy and performance improve **overall**, with a mechanism of higher order communications, dynamic filtering, and linear complexity that we will be convinced are attributes of the SW. This paper doesn\'t quite get there, though it gets part of the way there. \n\nFrom my understanding, Triangles, CATER, and Sort-of-CLEVR all tell us that the higher-order communication and single channel will help identify relevant information earlier in the pairwise communication, leading to faster convergence - *if* information is sparse in the input. The MNIST generation experiment tells us something similar about independent regions in inputs. Figs 4 and 5 are convincing to this end. However, the main claim - higher-order communication and a sparser connection of graphs - help for inputs with sparse information in various locations - is less surprising than a claim of general improvement. Even if that\'s still interesting (it is), the intro and conclusion seem to make a more general claim. Physical reasoning and Atari show more general improvement (and the results on Atari are impressive), but they\'re also the weakest, most qualitative parts of Sec 4. The ideal solution would be more robust testing of general improvement in these two testbeds, but if not then I would at least appreciate a claim that acknowledges the common and specific nature of the first four testbeds. \n\nThis is the crux of the motivation behind my score: I worry that this paper overstates its claim and the real claim is elegant but not that surprising. It\'s made even murkier by a couple promising results that receive little attention. \n\n2. Relatedly, the Atari section makes a claim about considerable improvement due specifically to more appropriate modularization. That would be cool and a real testament to this approach, but I would need more specific experimentation to demonstrate that the effect really was more appropriate modularization. \n\n**Presentation**\n1. The mathematical formulation is quite clear, but it\'s not presented clearly. I had to puzzle over the paragraph for a while, not because it was overly complicated but because it took a lot of treasure hunting to find all the parts and put them together. The issue is that the *Notation* and *Step 1/2/3* sections write the math into large blocks of text without pause. It would be more helpful to present some end-to-end equations, then break those down in text. It can be unclear without doing it myself whether something is being element-wise updated, transformed, dotted, etc. \n\n2. I really appreciate the slew of testbeds. However, that\'s how they come across: a laundry list of experiments, one after another, even though they support similar claims. That robustness can be shown by having a claims-driven structure for the results section rather than a testbed-driven structure, which to the reader is arbitrary and doesn\'t deliver the important information as well as it could. E.g. section titles like ""Performance advantage on sparse data"", then talk about the different results that show it. If one experiment shows multiple advantages, multiple sections can still point to the same figures. \n\n3. The figures are confusing - they are cramped, small, and have lots of acronyms I have to hunt down in various parts of the paper. They\'re all helpful content-wise, but this could be shown better with something as simple as better aesthetics and tagging.\n\n4. Nit: the text seems to switch between ""specialist"" and ""neural module"". Are these the same thing? Is ""specialist"" an abstraction/metaphor for ""neural module""? Is ""neural module"" an example of ""specialist""? All of those are great, just signpost and explain the switch. \n\n \n', 'summary_of_the_review': '**Strengths:**\n1. Good problem setup\n2. Interesting idea with good scoping \n3. Impressive experimentation suite \n\n**Critiques:**\n*Content:*\n1. The experimentation section mainly convinces me that this architecture is good for accuracy and convergence speed on sparse inputs, not that it\'s generally advantageous - but the latter seems to be the claim of the paper, and the former is less surprising given the nature of the architecture (though still interesting - it\'s compelling to see that the architecture works). \n2. The experimentation on general improvement is a bit weak. \n\n*Presentation*\n1. Formulation is clear, but it\'s presented piecemeal in long paragraphs that make it hard to parse and put together. Since the formulation is simple (a good, elegant thing), it\'s frustrating to spend a lot of time hunting down all the pieces.\n2. Excellent thoroughness in testbeds, but the results don\'t have to be presented in such a process-driven way. They can and should be claims/driven. \n3. The figures need to be more readable \n4. ""Specialist""/""neural module"" distinction is unclear (see main review)', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposes a modification for attention-based network architectures drawing inspiration from the global workspace theory in cognitive science. Essentially connections are made sparser, with different ‘specialist’ units communicating with each other through a limited bandwidth channel.\n', 'main_review': 'Strengths\n\n1. Interestingness of the approach\n\nThe proposed method is appealing since it applies a theory from cognitive science (the global workspace theory) to information processing in networks. The implementation of the idea is quite simple - individual ‘specialist’ units interact with a shared memory layer instead of with each other directly, and the shared memory then broadcasts information back to each of the specialists. The exact means by which this interaction is carried out is by using key-value attention. This approach when applied to transformers leads to much higher computational efficiency (linear instead of quadratic in the sequence length).\nFurther research in probing similar ideas for much more efficient information processing, backed by ideas from cognitive science, would be very beneficial for the field in my view and this paper would help the community in that regard.  \n\n2. Experimental Evaluation \n\nThe paper includes exhaustive experimental evaluation over 5-6 environments (including object tracking, and relational reasoning) where the solution requires considering a small portion of the input data, the authors demonstrate that adding the shared workspace model to attention based architectures like Transformers and RIMs leads to superior asymptotic performance, and faster learning. These environments include object tracking (CATER), relational reasoning (sort of CLEVR) and Atari games. The authors also include details about the experiments and algorithm implementation in the appendix, which will aid in reproducibility. \n\nWeaknesses \n\n1. Unclear if the approach will scale to larger, more unstructured datasets (where current attention based architectures have already been shown to thrive)\n\nMost of the experiments considered involve problems that only require a small portion of the input to be solved (eg: just the patch with the points for the equilateral triangles, or just the target object for Cater object tracking). I am concerned whether the framework proposed here will also be effective in settings where this is not necessarily the case, such as general modeling of language and images. For example, adding shared workspace to transformers imposes a communication bottleneck between representations at different positions of the sequence. It is possible that for problems where the solution does not depend on only a small portion of the input, considering the pairwise relationships of representations at every point in the sequence is critical for good performance. \nTo study this, the shared workspace model would have to be evaluated on larger, more unstructured datasets, where transformer based architectures have already been demonstrated to do well (for example the data on which GPT-2 was trained). Adoption of this approach would be much more widespread if the authors can demonstrate that on these larger datasets, training transformers with the shared workspace doesn’t lead to worse performance than training the regular transformer based models that are currently used. ', 'summary_of_the_review': 'The paper proposes an implementation of an interesting theory from cognitive science for more efficient information processing in networks, by making connections between entities sparser. While there is extensive evaluation on environments, most of these involve processing a small portion of the input. The paper will be made a lot stronger if the approach can be shown to scale to larger more unstructured datasets where transformers are known to work well. \n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents a method of using external memory called ""shared workspace"" for communication among different neural modules or ""specialists"". The key idea is that there are limits on the communication bandwidth and the specialist modules must compete for access. The communication limit encourages specialization and compositionally and facilitate better synchronization. Experiments over a variety of tasks indicate the proposed Shared Workspace model improves performance over baselines.', 'main_review': 'Strengths:\n\n- The paper is written very well. The introduction provides a good motivation and challenges and illustrates the use cases clearly using examples. The related work compares the proposed approach with other memory-based neural models. The description of methods is concise, but sufficiently detailed.\n\n- The proposed approach is original to the best of my knowledge. The key idea is very intuitive and motivated by insights from cognitive science literature.\n\n- The authors perform experiments on a wide variety of tasks including a toy task for detecting equilateral triangles, multi MNIST Generation, object tracking, relational reasoning, physical reasoning and Atari video games. I also like that the authors use different types of backbones (Transformers and RIMs) in different experiments which indicates the proposed Shared Workspace method is not specific to certain kind of backbones.\n\nWeaknesses:\n\n- Although the authors perform experiments with wide variety of tasks, multi-agent tasks are missing where I believe coordination is more important. In all the tasks used in the paper, different neural modules process different parts of the input to make a common prediction. Multi-agent tasks are more challenging as each agent would get a different input but also predict a different action.\n\n- It is unclear how certain hyper parameters are chosen. For example why is the patch size 4x4 in equilateral triangles, 6x6 in object tracking? The number of memory slots, size of memory slots, etc. different across different experiments. How are these chosen?\n\n- The authors propose two versions of Shared Workspaces, soft and hard (SSW and HSW). Some experiments one contain a single version. Why are both SSW and HSW not evaluated in all the experiments?', 'summary_of_the_review': 'The paper proposes a novel method for coordination between neural modules which is well motivated. The experiments are comprehensive although some details are missing. Addition of multi-agent tasks would make the paper stronger.\n\nEdit after author response:\nI read the author response and I would like to maintain my positive rating. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'Paper proposes a novel mechanism for information exchange between different neural subnets. It replaces the pairwise interactions with a share memory space. The memory is updated by a competitive scheme which the top-k updates are selected via the key-query-value\nattention mechanism. The shared memory/workspace then broadcast the updated state to all other upstream specialist subnets. Each specialist subnet then update its representation/weights accordingly.\n\nPaper claims 3 advantages of this approach: \n1. Higher-order (HO) interaction among neural modules\n2. Dynamic filtering due to persistence of memory. \n3. Lower computational complexity of using shared workspace for synchronizing different specialists\n\nExperimental results are very promising across a wide variety of tasks, namely DETECTING EQUILATERAL TRIANGLES; SORT-OF-CLEVR; CATER: OBJECT TRACKING;  for transformer architectures. Also, experiments on RIM for the BOUNCING BALL task and TIMS for the MULTIMNIST GENERATION and Language Modeling Task were also done. Finally, the approach was also experimented on model  free RL on ATARI game task.', 'main_review': ""Strengths\n1. Novel and significant contribution by proposing a new mechanism for inter-subnets information exchange/learning.\n2. Of high relevance to the ICLR community. The generic approach cuts across different AI discipline as the experiments are performed on vision, language and reasoning tasks.\n3. Experiments are comprehensive and well-explained.\n\nWeaknesses\nMinor: Some technical details are not elaborated for readers who are less familiar with the respective topics. For example, the section on the key-query-value attention mechanism is quite brief. It's the key contribution on the success of the proposed approach."", 'summary_of_the_review': 'Paper proposes a novel approach which produces commendable experimental results across a wide range of tasks in vision, language and reasoning. The impact of this work is very significant and is of high relevance to the community. While I am not entirely familiar with the cited prior work on key-query-value attention mechanism which forms the backbone of the proposed approach, the entire paper is very well-written and comprehensible. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'No concerns.', 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Coordination Among Neural Modules Through a Shared Global Workspace', 'authorids': ['~Anirudh_Goyal1', '~Aniket_Rajiv_Didolkar1', '~Alex_Lamb1', '~Kartikeya_Badola1', '~Nan_Rosemary_Ke1', '~Nasim_Rahaman1', '~Jonathan_Binas1', '~Charles_Blundell1', '~Michael_Curtis_Mozer1', '~Yoshua_Bengio1'], 'authors': ['Anirudh Goyal', 'Aniket Rajiv Didolkar', 'Alex Lamb', 'Kartikeya Badola', 'Nan Rosemary Ke', 'Nasim Rahaman', 'Jonathan Binas', 'Charles Blundell', 'Michael Curtis Mozer', 'Yoshua Bengio'], 'keywords': ['slot based recurrent architectures', 'attention', 'transformers', 'latent bottleneck.'], 'abstract': ' Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities.  We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally  specialized  components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have  a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise  independent specialists.\n', 'one-sentence_summary': 'communication among different specialist using a shared workspace allowing higher order interactions ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'goyal|coordination_among_neural_modules_through_a_shared_global_workspace', 'pdf': '/pdf/19aac83e8824498df7b9d1e6952523f7c068218b.pdf', 'supplementary_material': '/attachment/05665d6b26ef6ebbf7a57aa77ddae775cc08aafa.zip', 'data': '', 'code': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/coordination-among-neural-modules-through-a/code)', '_bibtex': '@inproceedings{\ngoyal2022coordination,\ntitle={Coordination Among Neural Modules Through a Shared Global Workspace},\nauthor={Anirudh Goyal and Aniket Rajiv Didolkar and Alex Lamb and Kartikeya Badola and Nan Rosemary Ke and Nasim Rahaman and Jonathan Binas and Charles Blundell and Michael Curtis Mozer and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=XzTtHjgPDsT}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['S Chandra Mouli', 'Bruno Ribeiro']",ICLR,Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks,https://iclr.cc/virtual/2022/oral/6947,2022," Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\text{te} \circ X'$ not observed in training, as in training we observe $T^\text{tr} \circ X'$, where $X'$ is a hidden variable. This work argues that when the transformations in train $T^\text{tr}$ and test $T^\text{te}$ are (arbitrary) symmetry transformations induced by a collection of known $m$ equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict $Y$ in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks.",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=avgclFZ221l,https://openreview.net/forum?id=avgclFZ221l,avgclFZ221l,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes asymmetry learning for learning counterfactual classifiers, i.e. classifiers which are invariant to certain symmetry transformations w.r.t. hidden variables that differ between the training and test sets.\n\nThe reviewers universally agreed that the proposed setting, and theoretical contribution, were interesting and novel. They also praised the writing quality, but had some quibbles about the quality of the experiments, and discussion of prior work. Neither of these concerns were considered significant enough to be a barrier to acceptance, but the authors should try to improve them, if possible.'}, {'title': 'Acknowledgment', 'comment': 'Thank you very much for addressing the few concerns I had.'}, {'title': 'Other questions/comments? ', 'comment': 'We believe we have addressed all your concerns. Please let us know if you have other questions or comments.'}, {'title': 'Other questions/comments?', 'comment': 'We believe we have addressed all your concerns. Please let us know if you have other questions or comments.'}, {'title': 'Other questions/comments?', 'comment': 'We believe we have addressed all your concerns. Please let us know if you have other questions or comments.'}, {'title': 'Official Response to Reviewer XTXH (Part 1/2)', 'comment': 'We thank the reviewer for the positive comments and valuable feedback. \n\n**Q1:** The method appears to identify which of some set of transformations one should be invariant to (and learn correspondingly invariant representations). \n\n**A1:** A minor clarification: Given $m$ sets of symmetry transformations, the method finds which of these sets of transformations it should be invariant to. The method does not find which of the individual transformations within a particular set one needs to be invariant to. \n\n**Q2:** But we must first have prepared a collection of all possible symmetry transformations, which seems like a limitation of the method. \n\n**A2:** Out-of-distribution prediction without test distribution examples always requires additional assumptions or domain information: It is, unfortunately, a requirement of all causal queries. Without assumptions, the labels can arbitrarily change for out-of-distribution inputs. For instance, domain adaptation methods assume the availability of test distribution of the covariates $X$ and that $P(Y|X)$ remains the same, Invariant risk minimization (IRM) [1] and related methods assume the availability of data collected under multiple distinct environments observed in test. In this work, we use possible input symmetries as the domain knowledge. While the practitioner has to specify a collection of possible symmetries, we allow the flexibility of them being incorrect for the task at hand, since our method will choose the symmetries that are independent of the target $Y$ in training. \n\n**Q3:** Additionally, is there a trade-off where considering more symmetry transformations makes the method slower?\n\n**A3:** If we are given $m$ possible sets of symmetry transformations, the computational cost is O(m). In our causal search of symmetries, we use Greedy Equivalence Search (GES) described at the end of Section 4.2 that tests O(m) invariant representations instead of the prohibitive O(2^m) possible invariant representations. In all our experiments, this is enough to find the right invariances that achieve good OOD accuracy. \n\n\n\n'}, {'title': 'Official Response to Reviewer XTXH (Part 2/2)', 'comment': '**Q4:** The experiments and comparisons are relatively minimal and are only in a toy environment. From the description of the method I assume there are computational difficulties in practically implementing this method for more ""real-world"" problems. The papers would be strengthened by either having larger more realistic experimental comparisons, or barring that an explanation of the practical difficulties of applying the method to larger problems and future steps that could resolve them.\n\n**A4:** We have not experienced computational difficulties in implementing the method. The computation of COMP in Equation (7) can be approximated by sampling random labels and fitting the model to these random labels (amounts to training the model on new datasets). In our experiments, we estimate the complexity by Monte Carlo sampling 5 different random labelings. Additionally, as discussed above, Greedy Equivalence Search does not need to evaluate all the 2^m possible invariant representations (the greedy complexity is O(m)). \n\nWe have updated our results section with additional experiments on images. Appendix A.4 details the experiments on MNIST-{3,4} images where we are given finite transformation groups. Appendix A.5 details the experiments on CIFAR10 images where we are given infinite transformation sets that may not form a group. Please see the details below. \n\n\n**MNIST-{3,4} with finite transformation groups.** We follow the out-of-distribution experiments of [2] where the equivalence relations are provided as transformation groups (e.g., $90^\\circ$ rotations) over images. We use the Colored-MNIST-{3,4} dataset [2] that only contains digits 3 and 4. This was done only to avoid any confounding factors while testing if the proposed method can learn the correct invariances, not for any practical considerations (e.g., rotated 6 is a 9 and would interfere with some experiments, etc.). \n\nWe consider equivalence relations obtained from 3 different transformation groups: rotations by $90^\\circ$, vertically flipping the image, and permuting the RGB color channels of the image. \nWe test our method on 4 classification tasks where each task represents the case where the target $Y$ has different invariances, i.e., invariant to all three groups, to two, to one, invariant to none (and sensitive to the remaining groups).\n\nWe show in Tables 2&3 in the paper that our method is able to find the correct invariances and the best OOD accuracy. In Tables 4&5, the method is excessively invariant (to vertical flip) but still achieves within 1% of the best OOD accuracy. The OOD test accuracy of standard CNN with no invariance ($\\mathcal{F}_\\emptyset$) is typically very low except in Table 5 where sensitivity to all groups is required.\n\n**CIFAR10 experiments with infinite/nongroup transformation sets.** We also tested our method when a given set of transformations over the images does not form a group. We used (a) arbitrary rotation transformations over an image and (b) shifting the hue of an image. Note that for a bounded image, arbitrary rotation is not a group due to image cropping. \nWe tested our method on 2 classification tasks: (i) invariant to both sets of transformations, and (ii) invariant to arbitrary rotations, but sensitive to hue shifts.\n\nWe show in Tables 6&7 in the paper that our method is able to find the correct invariance and achieves the best OOD accuracy whereas the standard CNN with no invariance has poor OOD performance. \n\n**References:**\n\n[1] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.arXiv preprint arXiv:1907.02893, 2019.\n\n[2]  S Chandra Mouli and Bruno Ribeiro. Neural network extrapolations with G-invariances from a single environment. In International Conference on Learning Representations, 2021.\n\n\n'}, {'title': 'Official Response to Reviewer RsMV', 'comment': 'We thank the reviewer for the positive comments and valuable feedback. \n\n**Q1:** The experimental section consists of only two simulated toy examples. While these showcase how the method is supposed to work, it remains unclear how well the method would work in a real-world, uncontrolled environment. \n\n**A1:** Thank you for the feedback. We have updated our results section with additional experiments on images. Appendix A.4 details the experiments on MNIST-{3,4} images where we are given finite transformation groups. Appendix A.5 details the experiments on CIFAR10 images where we are given infinite transformation sets that may not form a group. Please see the details below. \n\n\n**MNIST-{3,4} with finite transformation groups.** We follow the out-of-distribution experiments of [1] where the equivalence relations are provided as transformation groups (e.g., $90^\\circ$ rotations) over images. We use the Colored-MNIST-{3,4} dataset [1] that only contains digits 3 and 4. This was done only to avoid any confounding factors while testing if the proposed method can learn the correct invariances, not for any practical considerations (e.g., rotated 6 is a 9 and would interfere with some experiments, etc.). \n\nWe consider equivalence relations obtained from 3 different transformation groups: rotations by $90^\\circ$, vertically flipping the image, and permuting the RGB color channels of the image. \nWe test our method on 4 classification tasks where each task represents the case where the target $Y$ has different invariances, i.e., invariant to all three groups, to two, to one, invariant to none (and sensitive to the remaining groups).\n\nWe show in Tables 2&3 in the paper that our method is able to find the correct invariances and the best OOD accuracy. In Tables 4&5, the method is excessively invariant (to vertical flip) but still achieves within 1% of the best OOD accuracy. The OOD test accuracy of standard CNN with no invariance ($\\mathcal{F}_\\emptyset$) is typically very low except in Table 5 where sensitivity to all groups is required.\n\n**CIFAR10 experiments with infinite/nongroup transformation sets.** We also tested our method when a given set of transformations over the images does not form a group. We used (a) arbitrary rotation transformations over an image and (b) shifting the hue of an image. Note that for a bounded image, arbitrary rotation is not a group due to image cropping. \nWe tested our method on 2 classification tasks: (i) invariant to both sets of transformations, and (ii) invariant to arbitrary rotations, but sensitive to hue shifts.\n\nWe show in Tables 6&7 in the paper that our method is able to find the correct invariance and achieves the best OOD accuracy whereas the standard CNN with no invariance has poor OOD performance. \n\n\n**Q2:** How often are the assumptions made about the data generating process expected to hold \'in the wild\'?\n\n**A2:** Thanks for the question. It really depends on the type of task. We expect physics tasks to have a number of symmetries (e.g., conservation of energy, conservation of momentum, conservation of angular momentum, etc.). Image tasks tend to be symmetric to some types of color changes (brightness, hue, small color balance differences), image scaling symmetries, limited angle rotation symmetries, and symmetries to small occlusions (modern self-supervised learning methods seem to rely on these small occlusions). Time series may be time stationary (which is a time shift symmetry) and may be ergodic (which is a time average symmetry). \n\n**Other comments:**\n\n**Q3:** The figures and table appear out of place. Table 1 is referenced in the Results section, but is shown two sections before without context. Figure 1 appears a bit too early in the paper (first mentioned in ""Illustrative SCM example""), while Figure 2 appears much later (first mentioned before Figure 1, after that mentioned before Theorem 1). \n\n**A3:** We have moved Table 1 and Figure 2 and updated their references.\n\n**Q4:** The caption text for Figure 2a(iii) is clipped out. The subfigure is also way too small to be readable without zooming in. \n\n**A4:** We have fixed the caption and increased the font sizes of all the figures. \n\n**Typos:** We have fixed these typos (and other small typos) in the paper, thanks. \n\n**References:**\n\n[1] S Chandra Mouli and Bruno Ribeiro. Neural network extrapolations with G-invariances from a single environment. In International Conference on Learning Representations, 2021.\n\n'}, {'title': 'Official Response to Reviewer GzS3 (Part 1/2)', 'comment': ""We thank the reviewer for the positive comments and valuable feedback. \n\n**Q1:** I wish the paper offered a greater discussion of prior work on counterfactual invariance as it relates to out-of-domain generalization. I hope the authors can address that in a future version. Additionally, while the authors have offered some discussion of [2] and [3], per my reading, [1] seems to offer a contradictory view (from this paper) of whether learning an invariant OOD classifier is solvable via interventional data augmentation or not. It would be great if the authors could share how one relates to the other. I'm not sure I found a convincing argument laid in this paper other than the example in Figure 1(c).\n\n**A1:** We have added a more in-depth comparison of our work with the existing counterfactual methods in Appendix A.3. \n[1, 4] propose counterfactual data augmentation for text datasets where human annotators are asked to make minimal modifications to the input document so as to change its label (for example, by changing a few positive words to negative words) while keeping style, etc. fixed. This type of augmentation essentially asks the labelers to identify all the causal features in the document and make modifications to those features alone. This can be seen as obtaining new counterfactual examples by simulating the causal model and requires knowing the true function that describes how the features affect the labels. We consider the more realistic setting where we do not have access to such a collection of counterfactual examples. \n\nWe will clarify why the results of [1] are not contradictory to our paper. In our paper, data augmentation refers to the traditional automated interventional data augmentation [6] under a mostly unknown data generation process, as opposed to the counterfactual data augmentation in [1] that either considers a fully-specified toy SCM or relies on humans-in-the-loop to generate counterfactual data. In Figure 1(c) we show that interventional data augmentation is not sufficient for the OOD task. However, if one had access to the fully-specified causal model, one could generate the counterfactual data shown in Figure 1(d) and learn an OOD classifier with the counterfactually augmented data (as done in [1]). But our work does not assume access to these counterfactual examples as [1] did. We will make this distinction clear in the paper. \nAdditionally, we prove that a counterfactual invariant classifier can be constructed from interventional data augmentation alone if the lumpability condition (Definition 2) is satisfied. This is not the case in Figure 1(d). In the trivial case, if one considers a single set of symmetry transformations, lumpability holds and interventional data augmentation is always enough.\n\nWang & Jordan [3] use counterfactual language to formally define and learn non-spurious, disentangled representations from a single environment. Our work is different in the following ways. In the structural causal model (SCM) of [3], the authors assume that there are no confounders between the observed $X$ and the label $Y$. However, in our SCM, we allow unobserved confounders $X^\\dagger$ and $U_i, i\\in \\mathbb{D}$. The hidden transformation variables $U_i, i\\in \\mathbb{D}$ are confounders because they affect both the observed input $X$ and the labels $Y$. We leverage the fact that confounders are related to symmetries (and do not affect X arbitrarily) to resolve the issue of unobserved confounding. Wang & Jordan also require pinpointability of the cause of the observed $X$. In our setting, this is typically not possible since there are multiple paths of transformations from $X^\\dagger$ to the same observed $X$. Thus, all the parents of $X$ may not be pinpointable, specifically the transformation variables $U_1, … U_m$. \n\n Veitch et al. (2021) [2] define counterfactual invariant predictors $f(X)$ when $X$ has a single parent $Z$ and provide conditions such predictors must satisfy over the observed distribution. Note that Veitch et al. assume that part of the input $X$ ($X^\\perp_Z$) is not causally influenced by the confounder Z. In our scenarios this is not generally true. For instance, under a color change, the entire image $X$ changes. Still, we show that the notion of a counterfactual invariant predictor exists. Hence, the definition of Veitch et al. in Lemma 3.1 of a counterfactually invariant predictor that requires segment of X to not causally depend on Z, a fundamental result of their work, unfortunately **does not apply** to our setting (since X may have no such segment). ""}, {'title': 'Official Response to Reviewer GzS3 (Part 2/2)', 'comment': '**Q2:** The experiments in the paper are on simulated tasks with limited number of variables. It is unclear how this method might work in high dimensional spaces, such as text. Though it\'s not necessarily a concern, I think it would be useful if the authors could comment on the same.\n\n**A2:** Thank you for the feedback. We have updated our results section with additional experiments on images. Appendix A.4 details the experiments on MNIST-{3,4} images where we are given finite transformation groups. Appendix A.5 details the experiments on CIFAR10 images where we are given infinite transformation sets that may not form a group. Please see the details below. \n\n**MNIST-{3,4} with finite transformation groups.** We follow the out-of-distribution experiments of [5] where the equivalence relations are provided as transformation groups (e.g., $90^\\circ$ rotations) over images. We use the Colored-MNIST-{3,4} dataset [5] that only contains digits 3 and 4. This was done only to avoid any confounding factors while testing if the proposed method can learn the correct invariances, not for any practical considerations (e.g., rotated 6 is a 9 and would interfere with some experiments, etc.). \n\nWe consider equivalence relations obtained from 3 different transformation groups: rotations by $90^\\circ$, vertically flipping the image, and permuting the RGB color channels of the image. \nWe test our method on 4 classification tasks where each task represents the case where the target $Y$ has different invariances, i.e., invariant to all three groups, to two, to one, invariant to none (and sensitive to the remaining groups).\n\nWe show in Tables 2&3 in the paper that our method is able to find the correct invariances and the best OOD accuracy. In Tables 4&5, the method is excessively invariant (to vertical flip) but still achieves within 1% of the best OOD accuracy. The OOD test accuracy of standard CNN with no invariance ($\\mathcal{F}_\\emptyset$) is typically very low except in Table 5 where sensitivity to all groups is required.\n\n**CIFAR10 experiments with infinite/nongroup transformation sets.** We also tested our method when a given set of transformations over the images does not form a group. We used (a) arbitrary rotation transformations over an image and (b) shifting the hue of an image. Note that for a bounded image, arbitrary rotation is not a group due to image cropping. \nWe tested our method on 2 classification tasks: (i) invariant to both sets of transformations, and (ii) invariant to arbitrary rotations, but sensitive to hue shifts.\n\nWe show in Tables 6&7 in the paper that our method is able to find the correct invariance and achieves the best OOD accuracy whereas the standard CNN with no invariance has poor OOD performance. \n\n\n**Typos**: We have fixed these typos (and other small typos) in the paper, thanks. \n\n**Presentation:** We have fixed the sentence in the introduction, increased the fonts in all figures, and moved related work to Section 6. \n\n**Q3:** In justifying Assumption 1, can you provide a citation that is more recent considering the science in this area has evolved considerably since 1982? \n\n**A3:** We provide a more recent reference on human visual perception and symmetries (Westphal-Fitch et al. (2012)) to justify Assumption 1. \n\n**References:**\n\n[1] Kaushik, D., Setlur, A., Hovy, E. H., & Lipton, Z. C. Explaining the Efficacy of Counterfactually Augmented Data. ICLR 2021. \n\n[2] Victor Veitch, Alexander D’Amour, Steve Yadlowsky, and Jacob Eisenstein. Counterfactual invariance to spurious correlations: Why and how to pass stress tests. NeurIPS 2021\n\n[3] Yixin Wang and Michael I. Jordan. Desiderata for Representation Learning: A Causal Perspective. arXiv:2109.03795 [cs, stat], September 2021.\n\n[4] Kaushik, Divyansh, Eduard Hovy, and Zachary C. Lipton. ""Learning the difference that makes a difference with counterfactually-augmented data."" arXiv preprint arXiv:1909.12434 (2019).\n\n[5] S Chandra Mouli and Bruno Ribeiro. Neural network extrapolations with G-invariances from a single environment. In International Conference on Learning Representations, 2021.\n\n[6] Shorten, Connor, and Taghi M. Khoshgoftaar. ""A survey on image data augmentation for deep learning."" Journal of Big Data 6.1 (2019): 1-48.\n\n[7] Gesche Westphal-Fitch, Ludwig Huber, Juan Carlos Gomez, and W Tecumseh Fitch. Production and perception rules underlying visual patterns: effects of symmetry and hierarchy. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1598):2007–2022, 2012.'}, {'summary_of_the_paper': 'Update\n\nI have read the author response and the updated version of this paper. I am delighted to see that the authors have incorporated my feedback and I believe this makes the paper stronger than its previous version. I have updated my scores and recommend that the paper be accepted.\n\n------------------------------------------------------------------\n\nThis paper proposes Asymmetry Learning, a new learning paradigm to obtain counterfactually invariant classifiers. If the observed covariates are a result of some transformations applied to a hidden variable, and these transformations differ between training and test datasets, a classifier may not generalize to the test set. The authors argue that when these transformations are a result of a collection of equivalence relations, finding OOD-invariant classifier boils down to finding the simplest causal model that defines the causal relationships between the labels and these symmetry transformations. To this end, the authors propose a scoring criterion to identify the simplest DAG in a DAG search space that ensures that the label is invariant of all transformations while maximizing the likelihood of the observed training set. Using their proposed scoring function, the authors employ Greedy Equivalence Search to identify the DAG with the highest score. Experiments on simulated physics tasks suggest that the method works as intended.', 'main_review': '- The paper is well motivated, and examines an important problem of classifiers failing out-of-domain.\n- The paper\'s contributions could be significant to what is an emerging area of research. Other than [1, 2] and [3], I have not seen any other paper that offers a theoretical framing for the relationship between counterfactual invariance and OOD generalization and provides a way to achieve the same. \n- The paper flows smoothly and is easy to understand.\n- I wish the paper offered a greater discussion of prior work on counterfactual invariance as it relates to out-of-domain generalization. I hope the authors can address that in a future version. Additionally, while the authors have offered some discussion of [2] and [3], per my reading, [1] seems to offer a contradictory view (from this paper) of whether learning an invariant OOD classifier is solvable via interventional data augmentation or not. It would be great if the authors could share how one relates to the other. I\'m not sure I found a convincing argument laid in this paper other than the example in Figure 1(c).\n- The experiments in the paper are on simulated tasks with limited number of variables. It is unclear how this method might work in high dimensional spaces, such as text. Though it\'s not necessarily a concern, I think it would be useful if the authors could comment on the same.\n\n[1] Kaushik, D., Setlur, A., Hovy, E. H., & Lipton, Z. C. Explaining the Efficacy of Counterfactually Augmented Data. ICLR 2021.\n[2] Veitch et al. (2021)\n[3] Wang and Jordan (2021)\n\nTypos and presentation: \n- Introduction Line 2: ""the task is requires"" -> ""the task requires""\n- Break the second sentence of the first paragraph of introduction.\n- Layer 3 description of Pearl\'s causal hierarchy: ""X\\dagger describe an hidden variable"" -> ""X\\dagger describe a hidden variable""\n- Your figures are blurry upon printing the paper. Please increase the font of the text in the figures.\n- Section 4.1: ""Our next results requires imposing"" -> ""Our next results require imposing""\n- Section 4.1 under Definition 2: ""Definition 2 holds for a equivalence relation"" -> ""Definition 2 holds for an equivalence relation""\n- Related Work should either be Section 6 or be Section 2 (right after introduction) and not be in between the theory and empirical results.\n- In justifying Assumption 1, can you provide a citation that is more recent considering the science in this area has evolved considerably since 1982?', 'summary_of_the_review': 'Overall, I think the paper presents a good contribution but I do have some minor concerns that I would like the authors to address as I have mentioned above.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors propose an approach for constructing classifiers that achieve out-of-distribution (OOD) generalization using a new learning paradigm they call _asymmetry learning_. They consider OOD tasks where the test input is obtained from the training input by applying a sequence of (random) input transformations. To obtain an invariant OOD classifier that generalizes well to both in-distribution and out-of-distribution samples, the authors introduce the concept of _counterfactual invariant representations over symmetric transformations_. They show how learning the invariant representations can be cast as a causal structure discovery task and propose a score-based (GES-based) algorithm for finding the causal directed acyclic graph (DAG) that best describes the invariances.', 'main_review': '**Update**: After reading the rebuttal and the other reviews, I have decided to bump up my original score to an 8. Thank you to the authors for addressing our concerns in great detail.\n\n**Strengths**:\n- The methodology has a strong theoretical basis, and is a result of combining insight from different fields. The idea behind this approach, is in my opinion, quite elegant.\n- The paper is well-written and the work is well-placed in the literature, the illustrations are well-thought-out and facilitate understanding the method.\n\n**Weaknesses**:\n- The experimental section consists of only two simulated toy examples. While these showcase how the method is supposed to work, it remains unclear how well the method would work in a real-world, uncontrolled environment. How often are the assumptions made about the data generating process expected to hold \'in the wild\'?\n\nOther comments:\n- The figures and table appear out of place. Table 1 is referenced in the Results section, but is shown two sections before without context. Figure 1 appears a bit too early in the paper (first mentioned in ""Illustrative SCM example""), while Figure 2 appears much later (first mentioned before Figure 1, after that mentioned before Theorem 1).\n- The caption text for Figure 2a(iii) is clipped out. The subfigure is also way too small to be readable without zooming in.\n- Reference to ""Accounting for unobserved confounding in domain generalization"" appears to be duplicated.\n- page 2, Layer 1 - ""causal"" misspelled as ""casual""\n- page 3, Symmetry transformations - Sentence starting with ""Although"" appears incomplete. I would suggest conjoining this sentence and the one before: ""... equivalence classes, although ..."".\n- page 8, Causal structure discovery, third row from the bottom: ""between distribution"" -> ""between **a** distribution""\n- page 9, second row: add comma before ""under the assumption""', 'summary_of_the_review': 'The authors propose an interesting idea for constructing OOD classifiers starting from counterfactual invariance for symmetric transformation, an idea that is well-formulated in this paper. Despite the lack of real-world validation, I think the paper has a strong theoretical basis and is a decent contribution to the literature.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': ""This paper considers a class of out of distribution (OOD) problems where at test time there may be new symmetry transformations of the input X (i.e., they don't change the label Y). The authors explain why standard invariances learned by data augmentation may not be OOD invariant. Next, the paper presents a method for learning OOD-invariant representations through causal structure discovery. This hinges on the concept of being counterfactually invariant to the symmetry transformations that could appear at test time but don't affect Y. Next is an algorithm for discovering the structure of a causal DAG which largely revolves around deciding whether or not there is an edge U_i -> Y; this existence question corresponds to whether or not Y is invariant to the transformation U_i (IIUC).\n\nThe paper test this approach on tasks in a simple simulated physics environment."", 'main_review': 'Strengths:\nThis paper does a good job of describing an interesting and important problem, invariance to OOD symmetry transformations, and motivates why existing approaches may not work well. The approach novel as far as I\'m aware, and clearly describes relevant concepts such as defining what we would want our OOD-transformation invariant representations to satisfy and how learning them is linked to causal structure discovery.\n\nWeaknesses:\n- The method appears to identify which of some set of transformations one should be invariant to (and learn correspondingly invariant representations). But we must first have prepared a collection of all possible symmetry transformations, which seems like a limitation of the method. Additionally, is there a trade-off where considering more symmetry transformations makes the method slower?\n-  The experiments and comparisons are relatively minimal and are only in a toy environment. From the description of the method I assume there are computational difficulties in practically implementing this method for more ""real-world"" problems. The papers would be strengthened by either having larger more realistic experimental comparisons, or barring that an explanation of the practical difficulties of applying the method to larger problems and future steps that could resolve them.', 'summary_of_the_review': 'Interesting ideas with relatively toy empirical results.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks', 'authorids': ['~S_Chandra_Mouli1', '~Bruno_Ribeiro1'], 'authors': ['S Chandra Mouli', 'Bruno Ribeiro'], 'keywords': ['out-of-distribution classification', 'symmetries', 'counterfactual invariances', 'geometric deep learning'], 'abstract': ""Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\\text{te} \\circ X'$ not observed in training, as in training we observe $T^\\text{tr} \\circ X'$, where $X'$ is a hidden variable. This work argues that when the transformations in train $T^\\text{tr}$ and test $T^\\text{te}$ are (arbitrary) symmetry transformations induced by a collection of known $m$ equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict $Y$ in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'mouli|asymmetry_learning_for_counterfactuallyinvariant_classification_in_ood_tasks', 'pdf': '/pdf/f15da1dc02ded9aba4a26e8ade750b28429da30f.pdf', 'one-sentence_summary': 'Counterfactual-invariant representations for symmetry transformations', '_bibtex': '@inproceedings{\nmouli2022asymmetry,\ntitle={Asymmetry Learning for Counterfactually-invariant Classification in {OOD} Tasks},\nauthor={S Chandra Mouli and Bruno Ribeiro},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=avgclFZ221l}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Evan Hernandez', 'Sarah Schwettmann', 'David Bau', 'Teona Bagashvili', 'Antonio Torralba', 'Jacob Andreas']",ICLR,Natural Language Descriptions of Deep Features,https://iclr.cc/virtual/2022/oral/5989,2022," Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=NudBMY-tzDr,https://openreview.net/forum?id=NudBMY-tzDr,NudBMY-tzDr,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper presents a method to interpret neurons in the vision neural models by generating natural language description that specifies the activation selectivity of a given neuron. The proposed method first identifies an exemplar set of input image regions that corresponds to a neuron, then searches a natural language description by optimizing the point-wise mutual information between descriptions and the exemplar set.\n\nStrength:\n- Reasonable method design and clear writing\n- Important problem and broad applications\n- Extensive experiments for evaluation of the proposed method\n\nWeakness:\n- Need more discussion on the limitations of the proposed method\n- Elaboration on the human inter-annotation agreement\n- Analysis on method transferability across tasks.'}, {'title': 'checked the added examples in Figure 12', 'comment': 'thank you. Nothing further from me.'}, {'title': 'Reviewer comments incorporated in newest revision', 'comment': 'Thanks again to all the reviewers for the helpful suggestions! We’ve uploaded a new version of the paper that incorporates some of the reviewer comments. These include:\n- **[R3]** changing the title to “Natural Language Descriptions of Deep **Visual** Features” to better reflect the scope of the paper\n- **[R1, R2]** mentioning Figure 3 in the main text and elaborating on MILAN’s failure modes\n- **[R1, R2, R3]** adding a large set of randomly selected MILAN descriptions to the appendix, which highlights some additional success and failure cases of MILAN; see Figure 12\n- **[R3]** adding dataset-wide corpus statistics for MILANNOTATIONS as an additional row in Table 5\n- **[R3]** adding an additional table to the appendix with corpus statistics for MILAN-generated descriptions, and some discussions about how these compare to the MILANNOTATIONS corpus statistics\n- **[R1]** clarifying how MILAN descriptions are sampled and reranked in Section 3.4; we did this in the text instead of adding a numbered algorithm\n- **[R1]** a discussion in the appendix (Section D, Figure 14) about how the max-word-diff neurons found in Section 5 correspond to non-robust behavior in the model. Using one such neuron (which was automatically detected by MILAN), we constructed several copy-paste adversarial attacks for ResNet18-ImageNet that are similar to the attacks found in Compositional Explanations [Mu and Andreas, 2020].\n- **[Iro Laina]** a discussion of “Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning” in Related Work.\n\nFinally, following **R3**’s suggestion, we experimented with using an off-the-shelf image captioning model to zero-shot describe a neuron’s top-activating image regions. We used a SAT model trained on the COCO captioning dataset. Because this captioning model is designed to take a single image as input, we feed it only the single most-activating image for each neuron, with the activation mask applied to the image at low opacity. This results, qualitatively, in vague captions that mention a single high-level object in the image and are frequently suffixed with the phrase “in the dark”--e.g., “a dog sits in the dark”--which both miss the interesting low-level visual properties of the image region and the visual properties it shares with the other top-activating image regions that were not input to the model. Quantitatively, this baseline achieves a BERTScore (f) of .15 relative to the human annotations in all of MILANNOTATIONS, which is substantially lower than both MILAN and NetDissect.\n\nWe are exploring whether this baseline can be improved by using the approach of Laina et al. (2020) to select a “most representative” description for the exemplar sets from among the SAT-generated captions. We will include any findings in the final version of the paper. However, both our initial results and Laina et al. suggest that this will not be sufficient to capture the full range of neuron behavior that we want to describe.\n'}, {'title': 'Re: a note on related work', 'comment': ""Thank you for bringing this to our attention! We agree that it's extremely relevant (and a cool paper!), and we'll discuss it in detail in the next version of our paper. We quite like the idea of captioning a group of images by selecting the centroid of their individual captions, and hope to incorporate it into some version of the baseline experiment suggested by reviewer YviU.""}, {'title': 'A note on related work', 'comment': 'Dear authors,\n\nThank you for sharing your paper!  It was very interesting to read, and I am looking forward to the dataset.\n\nAs a related piece of work, I would like to  kindly note our own paper _“Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning”_ published at NeurIPS 2020.  \n  \nIn the paper, we consider the problem of automatically generating natural language descriptions to characterize clusters of images forming _in the representation space_ of self-supervised algorithms, instead of individual neurons and their activating image patches. We also find that, often, image clusters correspond to compositions of concepts (e.g. dogs on grass) and that models trained on datasets such as Conceptual Captions are insufficient to capture sets of images that have lower-level details in common. Since in our work we did not have access to a specific dataset describing groups of images, I believe that MILANNOTATIONS can also find use in better analyzing, understanding, and evaluating concepts emerging in self-supervised representations, and I am thus looking forward to the data release. \n\nAs our paper tackles a related task, I would be grateful if you could discuss it in your updated paper. \n\nKind regards,\nIro Laina'}, {'title': 'Response to R3 (2/2)', 'comment': '**Why not use MILAN to fix more impactful spurious correlations, e.g. gender or racial bias?** We agree this would be interesting to explore! Our fixing experiments were intended as a proof of concept that these kinds of interventions can have measurable and controllable effects on the underlying model. We do believe MILAN has the potential to reduce model sensitivity to spurious features in real-world scenarios. However, past work [e.g. 1, 2] has shown that proper handling of gender/racial bias requires substantial care, so we think these experiments belong in a separate paper. We hope to do this in future work.\n\n*References*\n\n[1] Hendricks et al. Women Also Snowboard: Overcoming Bias in Captioning Models. 2018\n[2] Wang et al. Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations. 2019.\n[3] Welleck et al. Neural Text Generation with Unlikelihood Training. 2019.\n[4] Welleck et al. Consistency of a Recurrent Language Model with Respect to Incomplete Decoding. 2020.\n[5] Zhang et al. Mitigating Unwanted Biases with Adversarial Learning\n'}, {'title': 'Response to R3 (1/2)', 'comment': 'Thank you for the comments and suggestions!\n\n**The scope of the work should be clarified.** We agree, and we’ll change the paper title to “Natural Language Descriptions of Deep Visual Features” to reflect the finer scope. We designed MILAN to be general enough that it could be applied to other classes of models--e.g. NLP models--but since this paper focuses on vision models, we will make that explicit in the title.\n\n**Do these results transfer to (non-vision) tasks?** Our experiments focus on vision models, so we do not want to make over-broad conclusions about how MILAN will generalize to non-vision tasks. That said, we think this is a promising area for future research! Questions to answer (for applying MILAN to e.g. NLP models) include: (1) how can we obtain good exemplar sets for these neurons, and (2) what kinds of concepts would we expect to see in these models? (e.g., what is the analogue of a “perceptual feature” in language?)\n\n**What about transferring to more diverse vision datasets?** We’ve shown some evidence  that MILAN can generalize to neurons trained on new datasets (Section 4), but there are certainly limitations. We wouldn’t expect MILAN trained on ImageNet/Places365 data to generalize to, e.g., models trained on medical images. In such a setting, MILAN would likely still be able to describe low-level perceptual features (edges, spatial relations, etc.), but not ones detecting high-level medical concepts. We think that domain-specific gaps like this one could be closed by collecting additional data for domain-specific concepts like those in medical image analysis.\n\nWe chose ImageNet/Places365 because these datasets contain a diverse array of concepts that are relevant to many vision tasks (object detection, semantic segmentation, scene classification, etc.). The datasets are similar to each other insofar as they both contain objects and people in scenes; however, the object- and scene-level content differs widely (e.g., ImageNet contains 125 distinct kinds of dogs, while Places365 contains no dog-specific classes but 3 separate categories of parking lots). The fact that annotation models trained on the Places365-specific subset of MILANNOTATIONS generalize to ImageNet indicates that some degree of transfer is already possible.\n\n**How do off-the-shelf image captioning models work on this task?** We agree this would be an interesting baseline. We’re working on applying a SAT model trained on COCO; we will add it to the appendix in the next version we upload. \n\n**What are some stats on generated descriptions?** Good question. We are working on adding a table to the appendix that is similar to Table 5, but for descriptions generated on a held-out model.\n\n**What does the released dataset look like?** Table 5 in the appendix provides a high-level overview of collected data for different models. We realize this is missing dataset-wide statistics; we’ll update it to include that. We’ll also add a large set of random examples from the dataset to the appendix to give a sense for what people say.\n\n**Did you control for bias in the dataset?** We did not implement any special controls for bias. To the extent that the exemplar image regions were taken from the data that models were trained on, MILANNOTATIONS inherits whatever biases are implicitly present in the underlying datasets (ImageNet/Places). We anticipate that MILANNOTATIONS contains some biases in how annotators choose to describe people--for example, annotators sometimes seem to refer to large groups of light-skinned people as “people,” while specifically using specific racial terms for other groups. Other biases (that are different in their nature) include a tendency for annotators to focus on entities in the highlighted regions of the images, instead of the visual commonalities between the regions. Some work attempts to mitigate the former kinds of biases by introducing adversarial losses to models trained on the data or by augmenting the underlying dataset [e.g., 1, 2, 5]; we think this line of work is complementary to ours, and could be used to reduce bias in future iterations of MILAN/MILANNOTATIONS.\n\n**How do you search over the space of neuron descriptions?** We realize this is a little unclear in the text, and we’re working on adding some additional context. To answer the question: we use beam search to obtain a set of high-probability candidate descriptions by taking the whole beam after the final step of the search, and then we rerank the descriptions in that set according to their PMI with the exemplars. We chose beam search because it is a relatively standard search procedure in image captioning, VQA, and other sequence generation tasks. Alternative search procedures include top-k or top-p sampling, but these have been shown to favor more disfluent descriptions [3, 4]. We will add an explicit numbered algorithm describing the procedure to the final version of the paper.\n'}, {'title': 'Response to R2', 'comment': 'Thank you for the comments.\n\n**The limits of the approach should be discussed further.** We agree--we tried to capture some of the limitations with Figure 3, at the end of Section 4, and in the Ethics Statement. We will expand on these in the final version and will include some discussion of Figure 3 in the main text.'}, {'title': 'Response to R1', 'comment': 'Thank you for your comments! Answers to some of your questions follow.\n\n**Why is inter-annotator BERTScore lower than the model BERTScore?** The inter-annotator scores measure something fundamentally different than, e.g., the scores MILAN achieves on held out data. The lower inter-annotator scores reflect that different annotators often describe different aspects of the masked image regions; the higher held-out scores for MILAN reflect that MILAN is good at matching *at least one* of the human descriptions (since BERTScore is a multi-reference metric).\n\n**Can MILAN be scaled up by incorporating datasets like GQA/Visual Genome or large pretrained models like CLIP?** Absolutely! However, doing so will require solving several technical challenges, which we think is an excellent topic for future work.\n\nDatasets like GQA/Visual Genome, on their own, lack descriptions of low-level perceptual features like edges and colors. We collected MILANNOTATIONS to close that gap, but these other datasets could be used *in addition* to MILANNOTATIONS. For example, the bounding boxes in Visual Genome could be turned into masked image regions like the kind that MILAN is trained on, and the bounding box labels could be turned into free-form textual descriptions. \n\nRegarding large pretrained models like CLIP: these models could potentially provide better representations for the exemplar images that are input to MILAN. Since these models were trained on a wide range of images featuring many (object- and scene-level) visual concepts, leveraging their representations in the MILAN models might fill in gaps missing from MILANNOTATIONS. The challenge is that it is not immediately clear how to apply these models to *image regions* instead of entire images. \n\n**Figure 3 is not mentioned in the text; any further observations about the failure modes?** We’ll update the draft to include some discussion of Figure 3. We’ll also add more examples of model outputs. \n\nFigure 3 shows examples of MILAN’s broad failure modes: incorrect generalizations, vague descriptions, and specific semantic errors, including contextual mistakes. In some cases, MILAN generates correct captions that lack sufficient granularity: for instance, the sea life and sand in Figure 3 indeed share similar color patterns, but a more specific caption would mention sea cucumbers (unseen in the training examples). In other cases, MILAN makes semantic errors ranging from interpretable mistakes to, sometimes, vague or disfluent descriptions. Figure 3 shows two example semantic errors that we believe result from the difficulty of the captioning task, which requires using context to describe only a highlighted image region, and further requires generalizing across very different examples.\n\n**What do non-robust units look like? Can MILAN detect them automatically?** It is possible that the max-word-diff neurons (e.g. shown in Figure 12) reliably correspond to non-robust neurons of the kind identified by Mu & Andreas (2020). We are looking into whether this is the case, and will update the paper with our findings.'}, {'title': 'General Response', 'comment': 'We would like to thank all the reviewers for their positive feedback! We are excited that they found our proposed method to be “concise, straightforward, and well-motivated” [R1] and the paper to have “clear hypotheses” [R3] with thorough testing [R2]. \n\nWe also appreciate the many recommended improvements. **We are working on incorporating reviewer comments into an updated version of the draft, and we hope to have the updated version posted in the next day or two.** In the meantime, we wanted to respond to some of the comments.'}, {'summary_of_the_paper': 'This paper describes a novel procedure (MILAN) to interpret deep learning models for computer vision by generating natural language description that specifies the activation selectivity of a given neuron in the model. For this aim, they first define an exemplar set of input image regions for each neuron by thresholding its activation value. Then they search a natural language description by optimizing the point-wise mutual information between descriptions and the exemplar set. The probability distributions for calculating the mutual information are approximated by training the SAT model and a two-layer LSTM language model on a newly collected dataset (MILANNOTATIONS), which includes annotations of 20k units labeled by human participants. The authors first test the generalizability of MILAN descriptions across different model architectures, datasets, and tasks, showing its privilege of generating higher agreement with human annotations compared to baseline methods. They then demonstrate three interesting applications of MILAN procedure and show how these natural language descriptions help us to understand and control the learned models.', 'main_review': '- Strength\n  - This paper is well-written and easy to follow. The authors provide sufficient technical details for readers to understand and reproduce their work. Data, code, and the trained model will be open source.\n  - This paper picks up an intriguing topic that aims to interpret deep learning models by investigating hidden units and summarizing their exemplar activation by natural language descriptions. The proposed method is concise, straightforward, and well-motivated.\n  - The natural language descriptions can capture categorical, relational, and logical structure across different levels in the learned features. It’s nice to see low-level features like edges (“the top boundaries of horizontal objects”), middle-level features like shapes (“Poles and legs”), and relatively high-level features like objects (“dog faces”) could all be generated quite well by this same model.\n  - The results suggest generalizability across different model architectures, datasets, and tasks. This makes MILAN readily useful for many other potential applications, including the three interesting experiments shown in section 5-7.\n- Comments\n  - The model is trained on a newly collected dataset MILANNOTATIONS. Each unit was annotated by three human participants. But the inter-annotator agreement among human annotations seems not quite high (Table 4, Figure 10), compared to the BERTScore between model-generated descriptions and human annotations (Table 2, Table 3). How did the authors handle this inter-annotator inconsistency during their model training? Is there any additional quality control/validation performed for this MILANNOTATIONS dataset?\n  - Following the first point, I wonder if the authors would consider scaling up their methods by leveraging the existing large, multimodal datasets like GQA/Visual Genome or visual-language model trained on large paired image-text datasets like CLIP/ALIGN?\n  - Fig. 3 is not referenced in the main text. And I think these failure modes are interesting, and taking a closer look at them might be inspiring for improving this model in future studies. Do authors have further comments or thoughts about this result?\n  - The results in Fig. 4 are quite interesting. The bar chart suggests low-level visual features are more described by adjectives, middle-level units need more prepositions and verbs to describe relational features, and high-level units need more complex composition of words thus resulting in longer length and deeper parser trees. This probably aligns well with our intuition and expectation. For units that may contribute to those ""non-robust"" model behavior, are they described by more nouns with higher max word diff? Will the proposed MILAN model be able to detect those ""non-robust"" units and edit the network to improve its performance?\n- Minor:\n  - What do different dots refer to in Fig. 5?\n', 'summary_of_the_review': 'Overall I think this work is well-motivated, technically sound, and showing promising results that support potential applications for interpreting and improving deep learning models for computer vision. Some minor changes could be made to improve the clarity. More details about how authors control / validate the quality of the MILANNOTATIONS dataset could be included.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The authors introduced MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. \nThis is done by searching for descriptions that maximize point-wise mutual information with the image regions in which the neurons are active.\n\nIt uses (Bau, et. al. 2017) model for the selection of 15 images with regions, and Xu et. al. 2015 (Show-Attend-Tell) with a modification in pmi (probability of mutual information) for describing these regions. The Show-Attend-Tell model is trained on MILANANOTATIONS dataset, a large contribution of the technique. It is a dataset of images and fine-grained region descriptions. The dataset is comprised of 20K neurons (sets of regions) with descriptions.\n\nTESTING:\nThe testing section is a large part of the contribution as well.\n* Section 4: MILAN obtained higher agreement with human annotations on held-out networks than baseline. It also shows that the model works across architecture, dataset, and task\n* Section 5: neurons captioned with many adjectives or prepositions are relatively important to model behavior\n* Section 6: Models trained on blurred faces acquire neurons selective for blurred faces\n* Section 7: Networks devotes substantial capacity to identifying text labels in images.\n', 'main_review': 'The paper is about visualization and explainability. It is good read and inspirational to me since I have been an advocate of the sub-field. To be able go deeper on making use of the intermediate stages of a network, for visibility as well as new AI product features. The summary above shows the short of the discoveries. I have to experiment with the technique myself to go further. However, it looks promising.\n\nI also appreciate the amount of effort put on testing the system, on many architectures, datasets, and tasks. If there is more, I would like to have better characterization of the limits of the approach. \n\nGrammars, typos, etc.:\npg.1, par.1: convlution —> convolution\n', 'summary_of_the_review': '* A higher level of explainability of the regions responsible for network final result\n* A thorough analysis of the actionable insights that can be used for the model\n* Limits of the approach needs to be discussed further\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This work proposes a framework for interpreting model behaviour by generating language descriptions of neurons in the model. The method is trained to maximize mutual information between the description and input examples that activate the neuron. The experiments show results for image classification, generation, and unsupervised representation learning. In particular, this method yields model description that achieve higher BERTScore than max-likelihood training when transferring to neurons in new architectures. Moreover the authors show how this technique can be used to compute correlations between descriptions and neuron importance (e.g. wrt classification accuracy), interpreting model behaviour (e.g. anonymized models still select for unblurred faces), and that one can remove spurious feature corrleations by removing models with specific descriptions.', 'main_review': ""I want to preface my review by stating that I am a domain expert in NLP and not CV. While I attempted a preliminary search for related work, I am not sure if there is prior work in specifically generating language descriptions of neurons (I understand that there is prior work in explaining neuron behaviour by examining inputs that activate it). Hence I may significantly modify my review based on the response of my peer reviewers.\n\nThis is a strong piece of work that clearly states its hypotheses and carefully designs experiments to test said hypotheses. The technical novel of the particular method is low, however I do not think that is the point of this work. What this paper does show is a novel way to interpret model behaviour, and allows for very useful downstream applications (e.g. fast filtering of neurons activated by a particular feature through text selection). I advocate for acceptance of this work, and propose some potential improvement. I do have some concerns about the scoping of this work. The title and description suggests a more general method, however experiments are purely based on images. I think the paper title should be more finely scoped (NL descriptions of Deep Image Features).\n\nMajor:\n- Do these transfer results generalize to other tasks? or are they specific to image models? Or are they specific to the fact that all of these image models are trained/evaluated on (two) datasets that are similar to each other in terms of image distribution? I think perhaps the paper makes this technique seem more portable than it actually is. For example I would have to collect data for my task distribution in order for this technique to work. If you can show this working with pretrained zero-shot image captioning models than this result would be a lot more convincing.\n- How do image captioning models do on the task of generating descriptions conditioned on inputs? The implicit hypothesis here is that they do not work well because they operate on higher levels of abstraction, but I would like to see empirical results of this by using caption models as baselines (e.g. zero-shot, fine-tuned).\n- What do generated descriptions look like? How long are the descriptions? What is the vocabulary size? How diverse are the descriptions?\n- What does the released dataset look like? Do you control for bias in the description? Diversity? Coverage?\n- This objective not only needs to maximize over all input space but also all description space. The former is talked about but the latter is skimmed over (by stating that beam search is performed). It's unclear whether beam search would yield useful description if the description is more complex (e.g. in a language task like question-answering). \n- It would be nice to show spurious feature filtering results on something that is not a contrived task (e.g. classification with spurious text labels on top left corner of image) and instead on something impactful (e.g. removing racial/gender bias by filtering out neurons with corresponding descriptions).\n\n\nMinor:\n- typo; convlutional network\n\nQuestions:\n- Why is there so much variance across architecture pairs? For example large gains on AlexNet -> Places and ResNet -> ImageNet but small gains on ResNet -> Places and AlexNet -> ImageNet."", 'summary_of_the_review': 'Strong paper demonstrating how to generation descriptions of image features and how to use method to analyze model behaviour and filter out neurons by description.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Natural Language Descriptions of Deep Visual Features', 'authorids': ['~Evan_Hernandez1', '~Sarah_Schwettmann2', '~David_Bau1', '~Teona_Bagashvili1', '~Antonio_Torralba1', '~Jacob_Andreas1'], 'authors': ['Evan Hernandez', 'Sarah Schwettmann', 'David Bau', 'Teona Bagashvili', 'Antonio Torralba', 'Jacob Andreas'], 'keywords': [], 'abstract': 'Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.', 'pdf': '/pdf/842234024e58a8d5073a88b3c04282011b8e20a7.pdf', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'hernandez|natural_language_descriptions_of_deep_visual_features', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/natural-language-descriptions-of-deep-visual/code)', '_bibtex': '@inproceedings{\nhernandez2022natural,\ntitle={Natural Language Descriptions of Deep Features},\nauthor={Evan Hernandez and Sarah Schwettmann and David Bau and Teona Bagashvili and Antonio Torralba and Jacob Andreas},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NudBMY-tzDr}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Mia Chiquier', 'Chengzhi Mao', 'Carl Vondrick']",ICLR,Real-Time Neural Voice Camouflage,https://iclr.cc/virtual/2022/oral/6987,2022," Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries.",Oral 1: AI Applications,https://openreview.net/pdf?id=qj1IZ-6TInc,https://openreview.net/forum?id=qj1IZ-6TInc,qj1IZ-6TInc,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes a novel neural voice camouflage method that learns predictive attacks without any constraints about input and output. It is general, robust, and real-time that could be used in a real-world scenario. The experiments are solid, the in-depth analyses are convincing.'}, {'title': 'Response to the authors', 'comment': ""Thank you for your considerate responses to my review and for conducting extensive additional experiments in a short period of time. They have resolved most of the concerns.\n\nIn addition, I also propose several minor comments that might be added to future revisions.\n* I recommend removing the human's head in Figure 2. It confused me to think that the signal farther to the head is pronounced earlier.\n* The explanation and the added glossary helped me understand this paper more comfortably. However, I still think it might be easier if the indexing becomes more precise. (e.g. express the generated noise in Figure 2 as $\\alpha_{t-r:t}$  instead of $\\alpha_{t}$)\n\nTo sum up, I think this paper is much more improved thanks to the authors and other reviewers, so I will raise my score from 5 to 8.""}, {'title': 'Thank you for your response', 'comment': ""I am raising my score to 8 since my concerns are resolved and the paper has enhanced further, thanks to the other reviewer's suggestions and the author's delicate revision.\n\nI'd love to see the paper presented at ICLR.""}, {'title': 'Responses to Reviewer z8ss', 'comment': ""Thank you very much for your comments. We hope we clarified your confusions below, and please let us know if you need any additional information.\n\n**Difficult to understand especially due to the way of indexing. For example, αt+δ+r means a noise to be added to the speech up until t+δ+r?**\n\nThis definition is provided in the line after Equation 1 and in Figure 2. In addition, to make it more clear, we have added a section summarizing the notation in Appendix A1.\n\n**I think the δ is rather a room for computation time than an exact computation time. Is it right?** \n\nYes, that’s correct. We updated our definition of delta to be an upper bound in the description of Figure 2. \n\n**Comparison to other NVC methods**\n\nThe only online methods are the microphone jamming approach, which requires specialized hardware, or the reinforcement learning approach, which isn’t applicable since they only attack a dataset of 10 words. To understand how the reinforcement learning approach would work assuming it worked perfectly, we ran an oracle experiment. In the dataset, we replaced all the 10 words that their approach was able to target with random letters, ensuring that the length of the word did not change. We then computed the WER of this modified dataset with the unmodified dataset, which gives us the upper bound performance of the reinforcement learning approach. Our results were that the WER went from 11.3% on the unmodified dataset to 14.7% in this modified dataset, still significantly underperforming our approach. This shows that even if this baseline worked perfectly, our approach would still be preferable.\n\n**Ablation study about varying δ**\n\nPlease let us know if we misunderstood, but we believe you were asking about whether we retrain a new model per delta in Figure 6 (which is now Figure 5). This plot is showing the result of shifting delta on a fixed model. This is important because the attack may not play at the right time due to physical variabilities in software & hardware. The result shows that our system is robust and therefore practical in real-life scenarios. In addition, we found your idea of retraining a model per delta interesting, and provided an additional plot to reflect this in Appendix A3. This plot shows that as we train a model with a larger delta, the WER drops, as expected. \n\n**Black Box** \n\nThank you for your suggestion. We passed the attacked inputs through Wav2Vec2, where the attacks were generated by the forecasting model that was trained with DeepSpeech. Please see the paper for results in Table 1 and analysis in Section 4.2, as well as the response to reviewer j1oS. In summary, our result is still effective in black box settings.\n\n**Related Work**\n\nThanks for the suggestion, we clarified that real-time machine learning includes other methods of making it real-time besides improving inference speed.\n\n**High sampling rate/Instantaneous computation**\n\nThis is just a simple calculation that if the sampling rate is 16KHz, then the time it takes for a single sample to be recorded is the reciprocal of that, which is 0.0000625 seconds, and this is within milliseconds. \n\n**There should be a reference about DeepSpeech, and in Section 3.4 (or in the appendix section)**\n\nWe apologize for omitting these details. We added a new subsection, 4.1, which describes the DeepSpeech model, the implementation we built off of, and the dataset. We also describe the Wav2Vec2 model and its implementation. \n\n**When I read this paper, I really wondered about the audio samples (generated perturbation only / speech + perturbation). However, I cannot see it and I cannot open the video in the supplementary material**\n\nWe have converted our demo to mp4, and reuploaded it. Please let us know if you are still unable to access it. \n\n**How the WER can be larger than 100%? (off-line PGD)**\n\nThis can happen if the predicted sentence contains more incorrect words than the ground truth sentence. \n\n**I'm a little confused about using 0.5s delay, considering the summation of computation and playback time? (0.014s + 0.5s)** \n\n.014 has to be less than 0.5s. We chose a larger delta 0.5s in order to give enough room for variance, even though our forward pass is only 0.014s.\n""}, {'title': 'Responses to Reviewer j1oS', 'comment': 'Thanks for your helpful experiments suggestion ideas! We ran them and report the results below. Please let us know if you need any additional information.\n\n**Language Model**\n\nThank you for your suggestion. We ran the experiment you suggested for our approach as well as all baselines. We used the same Language Model that DeepSpeech uses out of the box. We added these results in a new column in Table 1, and we also included a paragraph describing our results in Section 4.2. In summary, these new results show that the attack still works and outperforms baselines.\n \n**Black Box**\n\nThank you for your suggestion. We also ran the experiment you suggested and tested our approach in a black box setting by sending the attacked inputs through Wav2Vec2. We found it still outperforms other black box baselines. Online PGD does outperform our method in this case, however that is a white box attack. \n\n**Ethics Statement**\n\nWe also added a new paragraph to the ethics statement. Please let us know if there’s something missing. \n\n**Model & Dataset Details**\n\nWe apologize for omitting these details. We added a new subsection, 4.1, which describes the DeepSpeech model, the implementation we built off of, and the dataset. We also describe the Wav2Vec2 model and its implementation. \n\n**Figure 6 & WER**\n\nYes, good catch. We have updated the paper with this suggestion.  Please note, Figure 6 became Figure 5. \n'}, {'title': 'Responses to Reviewer 9gbp', 'comment': ""We appreciate your thoughtful review and we hope we addressed your concerns. Please let us know if you'd like any further information.\n\n**Vocal timbres & formants**\n\nThank you for pointing these points out. These are good points and you are right. We have modified the paper to reflect this. Specifically, we revised the claim to now say that our approach attacks instances rather than vocal timbres. Additionally, we removed our statement that the attacks resemble formants. \n\t\n**User study**\n\nThank you for your suggestion. We conducted a user study to investigate this. We found that our attack does not significantly impact the clarity of the speech for humans to understand it. We asked human subjects to manually transcribe both attacked inputs and non-attacked inputs, and we found people only made 4% more errors when our attack is present. We have included this analysis in section 4.4 of the paper. \n\n**Is multiplier m in Figure 6 the same as Power of Noise in Table 1? And what is the unit of WER in Figure 6? Why such a big discrepancy in WER?**\n\nSorry for this confusion. We have updated Table 1 to make it clear that the values were multiplicants, not power. We updated Figure 6 (which is now Figure 5) to clarify that the units for WER are percentages. The big discrepancy is because as you make the attack louder, the signal-to-noise ratio gets worse, breaking the ASR model even more. \n""}, {'summary_of_the_paper': 'This paper proposes a novel attack approach with a purpose of disrupting the automatic speech recognition system. The proposed method, called Neural Voice Camouflage, works in real time by forecasting attacks ahead of time when they are added to speech streams. The authors conducted experiments with the LibriSpeech dataset, and showed that the proposed model outperforms the conventional methods with or without defense mechanisms on the task of speech recognition (performance measured by WER/CER).', 'main_review': 'Strengths:\n- the paper is well structured and easy to follow\n- clearly explains how the proposed method works\n- evaluation framework is well designed and straight forward\n- experiments are solid and the results support the proposed method is working\n- in-depth analyses on the results: \nI really enjoyed the analysis shown in Figure 7\n\nWeaknesses:\n- some arguments are not validated:\nIn Section 4.4, the authors provide in-depth analyses and discussions on the attack characteristics. The first was whether the proposed model attacks vocal timbres, and concludes it does and attacks are speaker-dependent by stating that the attack performance drops - i.e., both WER and CER drop - when swapping attacks for speakers. However, it may not be true unless the experiments were carefully conducted with speech samples where different speakers say the same content. It would be interesting to see the results of voice-converted samples.\n\n- some observations are not scientifically grounded:\nIn Figure 3 and 4, the authors repeatedly state that the attacks resemble speech ""formants"" but I don\'t see any formant-like structures in the spectrograms. If the authors are referring to the wave-like frequency components, I\'m quite confident they are not formants. If time in seconds is denoted in the x-axis and the corresponding text is aligned and overlaid, it will be easier to determine.\n\nSome minor comments:\n- supplementary video was helpful to experience how the attack sounds like, but it was pretty disturbing. And the white noise was inaudible so I couldn\'t make comparison. If such attacks are practically to be used, it would be good to perform a user study for perceptual evaluation of different attack methods.\n- is multiplier m in Figure 6 the same as Power of Noise in Table 1? And what is the unit of WER in Figure 6? Why such big discrepancy in WER?', 'summary_of_the_review': 'This paper presents Neural Voice Camouflage, a real-time attack method that disrupts in streaming ASR systems. The methodology is clearly explained and the main contributions are well supported by a solid evaluation framework and carefully designed experiments. Thorough analyses on the results provide useful insights for researchers working in the same domain.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'A novel technique that prevents the ASR (DeepSpeech) from correctly recognizing the speech is presented. The proposed method works in real-time and is robust against some defenses.', 'main_review': '### Strengths\n\n- The motivation for the problem, the formalization, and the experiments are clearly written and well-organized.\n- The proposed method is tested in various situations that reflect real-world scenarios, and successfully deployed the method in a real room environment.\n- The authors also show that the attack is specific to the speaker; which partially implies that the NN-based approach is crucial for a given problem.\n\n### Limitations\n\n- Though practical ASR applications use an additional LM (language model) to correct the output, such cases are not examined. By looking at some examples of attacked transcription and the ground truth labels, I\'m pretty sure that the accuracy of the model will increase if aided with LM. This is the main reason I\'m giving a score of 5, and I\'m willing to adjust my judgment if authors succeed in examining and discussing the effect of LM.\n- One of the limitations of this work is that it was only tested with a single specific ASR model; I think this should be also mentioned in the ""Ethical considerations"" section.\n\n### Questions\n\n- What DeepSpeech model are the authors referring to? Please be specific about the model information and cite the literature if necessary; the authors might want to add a section in the appendix for this.\n- In Figure 6, I wonder if the authors forgot to multiply 100 on WERs. It\'ll be clearer if ""(%)"" is added to every WER/CER in plots.', 'summary_of_the_review': ""Though this work has established the important problem and nicely tested the proposed method, it has failed to address an important component of ASR, LM. Thus I'm initially giving a score of 5."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper proposes a Neural Voice Camouflage (NVC) method that has three important characteristics, which are essential for an NVC method to be used in practical scenarios: general, real-time, and robust. Since the proposed method trains a model to learn predictive attacks without any constraints about input and output, it can be applied to any vocabulary in a real-time scenario, and it is also difficult to defend the attack. On the contrary, the previous gradient-based adversarial attacks take a lot of time to compute the attack, so it is difficult to be used in a real-time scenario. Other than that, other previous methods are trained to attack only a few target words or utilize a pre-defined frequency region that can be easily filtered out.\n\nIn experiments, this paper shows that the proposed method is really effective by showing that the WER&CER of an ASR model significantly increases with the method compared to other NVC methods. Furthermore, this paper conducts various analyses on the behavioral characteristics of the method that can give many insights for future work. Moreover, various experiments, which are conducted with considerations about the situation where the method is used in the real world, are also shown in this paper.', 'main_review': 'I think that the idea of training a model to learn predictive attacks is a contributive and effective way for an NVC model to be used in a real-world scenario. Also, the various experiments in this paper seem that the authors have considered a lot about the real-world scenario and can give many insights to the future works.\n\nHowever, there are several concerns about this paper.\n1. Personally, this paper was difficult to understand especially due to the way of indexing. For example, $\\alpha_{t+\\delta+r}$ means a noise to be added to the speech up until $t+\\delta+r$?\n2. I think the $\\delta$ is rather a room for computation time than an exact computation time. Is it right?\n3. I think it would be better if there is a comparison with a previously proposed real-time NVC method even if it works only in a certain frequency region. This is because it seems more plausible online NVC model compared to the online PGD.\n4. The paragraph, ""How robust is the attack to temporal shifts?"" is a little difficult to understand. When I read it, I think it is not an ablation study about varying $\\delta$, but I think the paragraph is saying about it (e.g. the sentence, the larger the delay $\\delta$, the further into the future our model needs to predict.""). Plus, I think there should be an ablation study about varying $\\delta$ for training the NVC model.\n5. When it comes to the real-world scenario, I think it is also a very important condition where we do not know about the ASR model. Therefore, I think it would be better to conduct experiments showing the performance drop when using an ASR model which is different from the ASR model used in training.\n\n**Personal Opinion**\n* When I read a paragraph ""Real-time Machine Learning"" in Section 2 at first, I felt it could not have been written because this paper is not about speeding up the inference speed. Therefore, I think it would be better if it explains how the delay enables this method to be operated in real-time.\n* I think there should be a reference about the numbers appearing when it explains the relationship between the high sampling rate and instantaneous computation.\n* There should be a reference about DeepSpeech, and in Section 3.4 (or in the appendix section), I think the architecture about DeepSpeech and the Language Mdeol should be written.\n* When I read this paper, I really wondered about the audio samples (generated perturbation only / speech + perturbation). However, I cannot see it and I cannot open the video in the supplementary material.\n* How the WER can be larger than 100%? (off-line PGD)\n* I\'m a little confused about using 0.5s delay, considering the summation of computation and playback time? (0.014s + 0.5s)\n', 'summary_of_the_review': 'I think this paper proposes a contributive NVC model and can give many insights. However, I personally think that this paper is a little difficult to read and the experiments are a little weak to support its contribution. Therefore, I give a score of 5 for this paper.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Real-Time Neural Voice Camouflage', 'authorids': ['~Mia_Chiquier1', '~Chengzhi_Mao2', '~Carl_Vondrick2'], 'authors': ['Mia Chiquier', 'Chengzhi Mao', 'Carl Vondrick'], 'keywords': ['automatic speech recognition', 'predictive models', 'privacy'], 'abstract': ""Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries. "", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'chiquier|realtime_neural_voice_camouflage', 'pdf': '/pdf/e2b96a38db73636bfa51d5ee4097373ddda15329.pdf', 'one-sentence_summary': 'We introduce predictive attacks, which achieve real-time performance in breaking automatic speech recognition models by forecasting the attack vector that will be the most effective in the future. ', 'supplementary_material': '/attachment/15a7fedce4527cfd5286ff5ab74da7a6be0319c1.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/real-time-neural-voice-camouflage/code)', '_bibtex': '@inproceedings{\nchiquier2022realtime,\ntitle={Real-Time Neural Voice Camouflage},\nauthor={Mia Chiquier and Chengzhi Mao and Carl Vondrick},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qj1IZ-6TInc}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon']",ICLR,Comparing Distributions by Measuring Differences that Affect Decision Making,https://iclr.cc/virtual/2022/oral/6696,2022," Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=KB5onONJIAU,https://openreview.net/forum?id=KB5onONJIAU,KB5onONJIAU,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Although by now there are several approaches for comparing probability distribution, the paper innovates by making their measure take into account the decision space and loss functions directly. The paper also frames its contribution within the literature at large. Reviewers were unanimous that the result is of major interest to the ICLR audience.'}, {'title': 'Re: Thank you for your review.', 'comment': 'I would like to thank the authors for their careful responses to my comments.\n\nI have read the responses and confirmed the changes which had been made to the paper. I am satisfied with the new additions to the paper based on my comments (f) and (g). I will keep my good score.'}, {'title': 'Thank you for your review. ', 'comment': 'Thank you for your detailed review and suggestions!\n\n**Q: The convergence properties of the empirical estimator of H-divergence deserve more discussion in the main paper. In particular, the role of Rademacher complexity in the bounds in Theorem 2 and Corollary 1 is not clearly explained.**\n\nThank you for this suggestion. We have added additional discussions in the updated paper. For both cases in Corollary 1, the Rademacher complexity vanishes at a rate of $O(1/\\sqrt{m})$ when the sample size $m \\to \\infty$. Combined with Theorem 2, we conclude that the estimation error for H-divergence vanishes at a rate of $O(1/\\sqrt{m})$. \n\n**Q: Need to discuss computational aspects. This clearly depends on the structure of the action set and loss function. The paper would benefit from the characterization of a set of (general) sufficient conditions under which computation of H-divergence is feasible.** \n\nWe have added a discussion section in the revised draft (currently in the appendix due to length limitations, but will be moved to main paper whenever length permits):\n\nThere are several situations where estimating the H-divergence is computationally feasible with guarantees:\n\n1. When $\\mathcal{A}$ is a small finite set, in which case we can enumerate all possible values of $a \\in \\mathcal{A}$. \n2. When $\\mathcal{A}$ is a $d$-dimensional vector space, and the loss function $\\ell(y, a)$ is convex in $a$, in which case we can accurately estimate the H-divergence in polynomial time by solving the optimization problem $\\inf_a  \\mathbb{E}[\\ell(Y, a)]$ with gradient descent. \n\nWith more complex loss functions (such as when the action is a deep generative model), it can be difficult to guarantee exact H-divergence estimation under bounded computation. Nevertheless a practical technique works well in our experiments: we use the same number of gradient descent steps for evaluating $H_\\ell\\left(\\frac{p+q}{2}\\right)$, $H_\\ell(p)$, and $H_\\ell(q)$. Intuitively, any ""optimization sub-optimality"" when estimating the three terms is roughly the same and approximately cancels out. '}, {'title': 'Thank you for your review. ', 'comment': 'Thank you for your detailed review and suggestions! \n\n**Q: Theorem 2 assumes that the same number of data points are sampled from p and q, respectively. I wonder if the result can be generalized to the case when the number of samples is different for p and q.**\n\nIt is straightforward to generalize Theorem 2.2 when the number of samples from p and q are different. This is because in the proof we separately bound the estimation error on the three terms $H_\\ell(p)$, $H_\\ell(q)$ and $H_\\ell((p+q)/2)$, and each of the three bounds hold for any number of samples.  \n\nNevertheless, unless there is a big difference in the number of available samples, we recommend using the same number of samples for p, q. This is due to our insight for efficient estimation: if we use n samples to estimate each of the three terms: $H_\\ell((p+q)/2)$ and $H_\\ell(p)$, $H_\\ell(q)$, they will approximately have the same amount of “overfitting” which cancel out. For example, when $p=q$ we prove a very low estimation error (Theorem 2.1) that is independent of the complexity of the loss function $\\ell$. \n\n**Q: Can H-divergence still be useful when the decision task is initially unknown (as in reward-free reinforcement learning) or uncertain (as in multi-task learning)?** \n\nThis is a great question, and in fact, we are currently working on extending H-divergence to multiple loss functions, where we define a new divergence that is the supremum of H-divergences for a family of loss functions. Intuitively, two distributions have positive divergence if there is at least one loss function with different optimal action. We find this construction useful in Bayesian optimization, robust optimization, and two-sample testing when the practitioner only knows the family of loss functions rather than an exact loss function. \n\n**Q: In Section 4.2, only the results for alpha = 0.05 are given. I wonder if similar patterns hold for other values of alpha.**\n\nWe have experimented additionally on alpha=0.01 and updated the paper with these results in Figure 4 in the Appendix. The results are qualitatively similar: by reducing significance level, all methods have less test power, but H-divergence still achieves the best test power across the board. \n'}, {'title': 'Thank you for your review. ', 'comment': 'Thank you for the thoughtful reviews and suggestions! \n\n**Q: Applications of divergences include not only two-sample tests but also other methods such as robust estimation and independence tests. A brief discussion about other applications would be helpful for readers.**\n\nThank you for this suggestion. We have added a discussion section (currently in the Appendix for space constraints but will be moved to the main paper whenever length permits). \n\nSpecifically, independence tests are two sample tests between p(x, y) and p(x)p(y), hence it is natural to use our method for independence tests. We can also design new robust estimation or optimization procedures by measuring perturbation with H-divergence. We have suggested additional applications in the discussion section such as training generative models. \n\n**Q: The variance of the proposed estimator is not discussed in the paper. Is it possible to discuss any result about the variance?** \n\nTheorem 2 shows that the probability of large deviation is exponentially small. Therefore, the variance is bounded (in fact, all moments are bounded). In the revised paper, we added Corollary 1 to show that the standard deviation of the estimator is at most $4 \\max(\\mathcal{R}^p_m(\\ell), \\mathcal{R}^q_m(\\ell)) + \\sqrt{2C^2/m}$ where $m$ is the number of samples, and $\\mathcal{R}_m$ is the Radamacher complexity as defined in Section 3.5 which usually goes to $0$ at a rate of $O(1/\\sqrt{m})$. \n'}, {'summary_of_the_paper': 'This paper presents a new class of discrepancies between two continuous probability distributions. The proposed class, which is called the H-divergence, contains an extended class of Jensen Shannon divergences called H-Jensen Shannon divergences and another class (2) called the H-Min divergences as special cases. The conditions that the two probability distributions have non-negative H-divergence are given. It is seen that the set of H-Jensen Shannon divergences includes the set of squared Maximum Mean Discrepancies as a subset. Estimation and convergence of the H-divergence are discussed. The H-divergence is applied to propose two-sample tests and experiments suggest that the proposed tests outperform some existing tests in terms of power. The proposed methods are applied to climate data for decision making in agriculture and energy production. ', 'main_review': ""**Strengths:**\n\n(a) The proposed class of divergences, H-divergences, includes well-known divergences such as Jensen Shannon divergence and squared Maximum Mean Discrepancy as special cases. The H-divergence also contains new classes of divergences, including the ones defined in Equations (1) and (2), which appear to be potentially useful in practice.\n\n(b) Some results are given to prove the convergence of the empirical H-divergence to its theoretical one; see Theorem 2 and Corollary 1. \n\n(c) The conditions on the non-negativity of the H-divergence are obtained; see Section 3.3. These theoretical results partly justify the proposed divergence as a reasonable discrepancy between two probability distributions.\n\n(d) A sufficient number of experiments are given to demonstrate the usefulness of the presented methods; see Sections 4 and 5. \n\n(e) The paper is clearly written. Appendix provides helpful information including detailed proofs of the theoretical results of the main article.\n\n&nbsp;\n\n**Weaknesses:**\n\n(f) Applications of divergences include not only two sample tests but also other methods such as robust estimation and independence tests. However the paper does not discuss possible applications other than two sample tests and plots in Figure 3. A brief discussion about other applications would be helpful for readers.\n\n(g) The variance of the proposed estimator $\\hat{D}^{\\phi}_{\\ell}  (\\hat{p}_m || \\hat{q}_m) $ is not discussed in the paper. Is it possible to discuss any result about the variance?\n\n&nbsp;\n\n**Minor Comments:**\n\n(h) p.1, Section 1, 1st paragraph, l.13: Add a full stop after 'point'.\n\n(i) p.1, Section 1, 2nd paragraph, l.1, etc.: There are at least three different expressions, namely, *H*-divergence, H-divergence and H-Divergence, in the paper to denote the same divergence. The expression should be standardized throughout the paper.\n\n(j) p.4, Section 3.3, l.6 up: This property allow us to  ===>  This property allows us to "", 'summary_of_the_review': 'The paper is generally well-written. Properties of the proposed class of divergences are well-investigated. A sufficient number of experiments are provided to demonstrate the usefulness of the proposed method.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'I do not find any ethical issues with this paper.', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a new category of divergences, called H-divergence, for measuring the discrepancy  between two probability distributions. H-divergence is based on the optimal loss with respect to a chosen decision task and is therefore decision dependent. Further, it generalizes a few well known divergences such as the Jensen-Shannon divergence and the maximum mean discrepancy family. The paper proves a few properties of H-divergence including a convergence result when H-divergence is estimated using a finite set of samples. Further, three examples are used to illustrate the applications of H-divergence  including two sample tests, assessing climate change, and feature selection, which demonstrate the advantage of H-divergence compared with other commonly used discrepancies.', 'main_review': 'Strengths\n\n1. Measuring the difference between two probability distributions is a fundamental problem in machine learning. Although many different types of divergencies have been proposed in the literature, they are typically decision independent. H-divergence seems a simple yet effective way of incorporating decision related domain knowledge into the discrepancy measure. \n2. The theoretical results are non-trivial are provide insightful connections between H-divergence and other commonly used discrepancies. \n3. The paper is well written. The theoretical results are solid and clearly explained. The three examples  clearly illustrate the broad applicability of H-divergence. \n\nQuestions:\n\n1. Theorem 2 assumes that the same number of data points are sampled from p and q, respectively. I wonder if the result can be generalized to the case when the number of samples is different for p and q. \n2. Can H-divergence still be useful when the decision task is initially unknown (as in reward-free reinforcement learning) or uncertain (as in multi-task learning)? \n3. In Section 4.2, only the results for alpha = 0.05 are given. I wonder if similar patterns hold for other values of alpha. \n', 'summary_of_the_review': 'Overall I think this is a very interesting paper. The new family of discrepancies is likely to find broad applications in machine learning and data science and can potentially inspire the development of other decision dependent discrepancy measures. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes H-divergence, a new type of divergence based on H-entropy, to compare two probability distributions. This new divergence includes some of the commonly used integral probability metrics and f-divergences, such as Jensen Shannon divergence and MMD as special cases. A crucial property of H-divergence is that it takes into account the decision loss; namely, it compares two distributions in a way that distinguishes them based on the optimal decision loss, i.e., ""two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution."" It also provides an empirical estimator for H-divergence and studies its convergence properties. The paper studies several use cases of H-divergence, including two-sample tests. Experiments demonstrate that H-divergence achieves higher test power than tests based on MMD under the same type I error rate. Another important use case of H-divergence is understanding whether differences between distributions are significant enough to affect decision making (from the viewpoint of minimizing loss) in different experiments, including how climate change affects various economic activities.\n', 'main_review': 'Strengths: \n\nProposes a new type of divergence that compares two probability distributions from the lens of optimal decision-making. By setting an action set and loss function, one can identify ""how far two distributions are"" in terms of leading to the same optimal action. Moreover, the paper clearly explains the connection of H-divergence with f-divergence and integral probability metrics. Overall, H-divergence can be seen as a valuable contribution to the family of divergences used in machine learning tasks. \n\nExperiments are satisfactory. Especially, the flexibility achieved by using H-divergence by choosing application tailored actions and losses highlight the merit of H-divergence in performing two samples test. \n\nWeaknesses:\n\nThe convergence properties of the empirical estimator of H-divergence deserve more discussion in the main paper. In particular, the role of Rademacher complexity in the bounds in Theorem 2 and Corollary 1 is not clearly explained. It will be good to add a discussion in line with proof of Corollary 1 (on how fast Rademacher complexity vanishes) into the main paper. \n\nThe paper does not give enough detail about computational aspects. This clearly depends on the structure of the action set and loss function. The paper would benefit from the characterization of a set of (general) sufficient conditions under which computation of H-divergence is feasible. \n', 'summary_of_the_review': 'This paper makes a fundamental contribution to the family of divergences used for machine learning tasks and provides convincing evidence on the usefulness of H-divergence. However, there is still room for improvement related to the computation of H-divergence. Please describe the feasibility of computation of H-divergence for the experimental examples studied in the paper in your response.\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Comparing Distributions by Measuring Differences that Affect Decision Making', 'authorids': ['~Shengjia_Zhao1', '~Abhishek_Sinha1', '~Yutong_He1', '~Aidan_Perreault1', '~Jiaming_Song1', '~Stefano_Ermon1'], 'authors': ['Shengjia Zhao', 'Abhishek Sinha', 'Yutong He', 'Aidan Perreault', 'Jiaming Song', 'Stefano Ermon'], 'keywords': ['probability divergence', 'two sample test', 'generative model'], 'abstract': 'Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'zhao|comparing_distributions_by_measuring_differences_that_affect_decision_making', 'pdf': '/pdf/e99719a7a6796b569cc6afdf6f42024d0df2fbea.pdf', '_bibtex': '@inproceedings{\nzhao2022comparing,\ntitle={Comparing Distributions by Measuring Differences that Affect Decision Making},\nauthor={Shengjia Zhao and Abhishek Sinha and Yutong He and Aidan Perreault and Jiaming Song and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=KB5onONJIAU}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Rachid Riad', 'Olivier Teboul', 'David Grangier', 'Neil Zeghidour']",ICLR,Learning Strides in Convolutional Neural Networks,https://iclr.cc/virtual/2022/oral/7069,2022," Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=M752z9FKJP,https://openreview.net/forum?id=M752z9FKJP,M752z9FKJP,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""The paper proposes a method to learn the stride of downsampling in deep networks using a  gradient-based learning approach. The main idea is to work in the frequency domain and to learn the cropping mask in that domain. The authors also introduce a regularization for applications seeking computationally and memory efficiency. The authors investigate the interest of the approach on a number of datasets with audio and image data. \n\nThe reviewers praised the paper, appreciating the elegance of the approach and the effort made to thoroughly evaluate it. The reviewers also appreciated the clarity of the exposition and the care in the reporting of the results. The reviewers also expressed some concerns about several choices of the design (stride sharing) and the lack of detail in some experiments (computational /memory efficiency). Finally, the reviewers wished the paper had more theoretical grounding.\n\nThe authors submitted detailed responses to the reviewers' comments. After reading the responses, updating the reviews, and discussion, the reviewers found that the responses were ‘reinforcing [their] initial assessments' and their several concerns were satisfactorily addressed. Moreover several of the reviewer’s suggestions clearly already led to an improved manuscript with very thorough experimental evaluation and simpler approaches for stride sharing. \n\nThe paper proposed an elegant, learning-based, approach to one of the most important design choices in deep network architecture design: the strides in the convolutions. The authors provided a careful and thorough experimental evaluation, and moreover improved it during the review process following the reviewer’s feedback. \n\nAccept, definitely. ""}, {'title': 'Reviewer response', 'comment': 'I thank the authors for providing their feedback, addressing my concerns. I still hold the opinion that this paper should be accepted.'}, {'title': 'Post rebuttal comment', 'comment': 'I was already convinced of the quality of the paper.\nReading other reviews and comments by the authors did nothing other than reinforcing my initial assessment. The authors\n\n- ablated the importance of disentangling strides among the two axis, with positive results;\n- added an analysis of computational and memory cost of their model, and mention it within a ""limitations"" section;\n- clarified my doubts about why they limit (in resnets) the employment of DiffStride to several layers only;\n- provided interesting insights about the possibiliy to condition the strides on the input itself;\n- explain why spectral pool outperforms aliased strided convs given equality in stride.\n\nIn summary, I strongly advocate for accepting this paper.'}, {'title': 'Response to Reviewer ubnz', 'comment': 'We thank the reviewer for their feedback.\n\n>“Authors demonstrate that the proposed method can recover from different initial strides and learn the optimal one. However, my most important concern is the lack of appropriate comparisons with neural architecture search approaches, since - in my view - these are the most direct competitors of the proposed method. Therefore, I would expect experiments and appropriate discussions between the cost and benefits between the proposed method and neural architecture search approaches.”\n\nMentioning neural architecture search is indeed relevant. NAS and DiffStride are however complementary: NAS does not alleviate the combinatorial explosion of hyperparameters but is efficient at prioritizing exploration and finding dependencies between parameters. NAS would benefit from differentiable hyperparameters (it is stated as an explicit goal in [5]). Traditionally, NAS has not focused on extensive stride parameter search: [1] mentions the explosion of stride parameters with depth and only evaluates 3 possible strides, 1-2-3, that deteriorates the overall performance. Other neural architecture search approaches do not even cross-validate for strides due to the exponential search space of strides [2,3,4]. The goal of DiffStride is to remove this issue altogether by learning strides by backpropagation directly. The comparison of NAS without DiffStrides, NAS with DiffStrides, DiffStrides without NAS is experimentally challenging and beyond the scope of this initial paper.\n\n>“The proposed method seems to target only residual architectures. Discussion on how the proposed method could be used with other architectures would be beneficial. Also, providing experiments with different architectures (apart from resnet) on the same dataset would also further improve the confidence on the obtained results.”\n\nWe focused our image experiments on ResNets as 1) it is the most widely used convolutional architecture, 2) learning strides with residual connections is not as trivial as for purely feed-forward architectures. However, we kindly bring to the reviewer’s attention that our audio classification architecture does not involve any residual connection and alternates convolutions along the time and frequency axes. Still, to address the reviewer’s concern, we added new experiments with EfficientNet-B0 in Appendix, which has seven layers of strided convolutions, and where DiffStride improves over spectral pooling and strided convolutions, consistently with our ResNet-18 results.\n\n> “What is the actual overhead of the using the proposed method in real applications? How much does the training time and memory usage increases?”\n\nWe thank the reviewer for this suggestion and agree that our discussion of limitations could be improved. To this end, we added in the appendix the peak memory usage and per-step time (forward+backward) on a V100 GPU (See Table A.2 in the Appendix). This shows that while DiffStride is more costly in training due to the necessity of computing and storing gradients with respect to strides, at inference the cost is identical to that of spectral pooling.  We also extended our “Limitations\'\' paragraph to propose other avenues for reducing the computational load of DiffStride. We also concede that efficiency was not our primary objective in this work. Efficiency gains could be obtained by exploring compiler options and implementation choices (esp. rematerialization options for backward efficiency and separation of the real/imaginary part after FFT to benefit from optimized loss precision primitives in further processing).\n\n[1] Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. 2017.\n\n[2] Efficient Neural Architecture Search via Parameters Sharing. Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, Jeff Dean Proceedings of the 35th International Conference on Machine Learning,\n\n[3] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. ""DARTS: Differentiable Architecture Search."" International Conference on Learning Representations. 2018.\n\n[4] Wu, Bichen, et al. ""Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\n[5] Neural Architecture Optimization, Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, Tie-Yan Liu (NeurIPS 2018)'}, {'title': 'Response to Reviewer 34rN', 'comment': ""We thank the reviewer for their feedback.\n\n>“In particular, I'd at least like some discussion (or thoughts) on why it doesn't improve densenet performance, “\n\nWe do not have a definitive answer for the lack of improvement for spectral DenseNet. Clearly, the limited number of downsampling layers (only 2) naturally reduces the space of stride configurations to a few, equivalent ones when sampling strides in [1; 3]. We added a mention of this in the “Limitations” paragraph. In future work, we aim to explore whether DenseNet/Resnet hybrid networks like DSNet [1] could explain whether the location of the striding layers or the dense shortcut modules explain the lack of improvement from spectral pooling. We added new experiments with EfficientNet-B0 in Appendix, which has seven layers of strided convolutions, and where DiffStride improves over spectral pooling and strided convolutions, consistently with our ResNet-18 results.\n\n> “and I'd like to see experimental results presented on the impact of the use of DiffStride during training and inference, both in terms of measurable memory usage and impact on training & inference wall time. Obviously such results can be caveated with a statement that the implementation could be improved as per the existing discussion in the limitations section. “\n\nWe agree that our discussion of limitations could be improved. To this end, we added in the appendix the peak memory usage and per-step time (forward+backward) on a V100 GPU (See Table A.2 in the Appendix). This shows that while DiffStride is more costly in training due to the necessity of computing and storing gradients with respect to strides, at inference the cost is identical to that of spectral pooling.  We also extended our “Limitations” paragraph to propose other avenues for reducing the computational load of DiffStride. In particular, a main computational bottleneck is the need to operate in the Fourier domain. We mention a potential alternative, which has already been employed for non-learnable Spectral Pooling, namely the use of the Hartley transform, a real-valued counterpart to the Fourier transform. We also concede that efficiency was not our primary objective in this work. Efficiency gains could be obtained by exploring compiler options and implementation choices (esp. rematerialization options for backward efficiency and separation of the real/imaginary part after FFT to benefit from optimized loss precision primitives in further processing).\n\n> “Please check and fix algorithm 1 - I believe line 4 should be using the output from the filtering with the mask, rather than the raw FFT result. It would also be better not to reuse the  symbol on lines 4 and 5, and it would also be helpful if the symbols (the variants of the intermediate computations , }) were clearly labelled on Fig 1.”\n\nWe thank the reviewer for pointing out this typo. We fixed Algo 1 and clarified the notations. We also modified Figure 1 accordingly. \n\n[1] Zhang, C., Benz, P., Argaw, D.M., Lee, S., Kim, J., Rameau, F., Bazin, J.C. and Kweon, I.S., 2021. Resnet or densenet? introducing dense shortcuts to resnet. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 3550-3559).\n""}, {'title': 'Response to Reviewer e1mG (cont.)', 'comment': "">“Another point that may be worth discussing as an extension/future work. May the DiffStride technique be employed within a conditional computation framework, by predicting the striding parameters conditioned on the current example? I think that this strategy might succeed especially when aiming at optimizing the computational cost of a model. I do not expect such an extention to be implemented and tested within the rebuttal period. I would just like the authors' opinion about that (potential, concerns etc).”\n\n\nThis is indeed a promising direction that is unlocked by casting strides as learnable parameters of the model. We have considered this during the late stages of development of DiffStride, however this comes with several challenges:\n\n**Benefits**: For instance this is an important argument to have adaptive striding for the speech inputs. There is speed variability among speakers and psychoacoustical studies that demonstrate fast online adaptation of humans [1]. Image classification would also benefit from being able to adaptively select the downsampling factors, to provide e.g. invariance to scale or to reduce the computational cost for tasks where low frequencies (e.g. textures) are enough to discriminate between images which allows for aggressive downsampling. \n\n**Challenges**: The first challenge is computational. Even though DiffStride changes the shapes of intermediate shapes between each batch, they are constant between backward passes which allows for batching. On the other hand, predicting strides on a per-example basis would make batching difficult, and would require padding all intermediate representations to the largest dimension in the batch. The second challenge is in terms of modelling: what kind of architecture would be appropriate to predict such strides? It would ideally be a lightweight model that can aggregate statistics over the whole input, such as a small CNN. Finally, in the main model, it is unclear whether the same convolutional kernels could be simultaneously applied to examples with different striding, i.e. inputs with different truncated frequencies. \nWe thank the reviewer for mentioning this possibility and added a mention in our conclusion. \n\n\n> “In Tab. 1, the Speaker Id dataset provides no information whatsoever, as all methods score a perfect accuracy of 100.0±0.0  I suggest the authors to remove this dataset, as it is apparently beyond trivial, and including it downgrades the quality of the experiment rather than increasing it.” \n\nWe agree that these results do not bring anything to the paper. We kept it at first for completeness, but removed the mention of this dataset in our revised manuscript. \n\n> “On CIFAR-100, the authors show that SpectralPool is significantly better than strided convs even with the same downsampling hyperparameters. Can the authors provide an intuition for this behavior?”\n\nSpectral pooling and DiffStride act as lowpass filters before downsampling and remove the high frequency components of inputs and preserve the low frequencies. This is unlike strided convolutions that introduce aliasing during the downsampling step as was demonstrated in [2,4]. In [2], Zhang they also shows that performing lowpass filtering before downsampling improves the robustness of classical, spatial-pooled convolutional neural networks. Besides, signals in several modalities tend to be biased towards low frequencies as we mentioned in Section 2.3. This is supported by image classification experiments in [3] where spectral pooling outperforms alternatives, which the authors attribute to preserving low-frequency information. Our results with spectral pooling confirm these results on all tasks and architectures that we consider [3]. Moreover, the observation made in Paragraph “Regularizing the complexity” that rounding fractional strides learned by DiffStride and using them to train a model with strided convolutions gives very poor performance furthermore corroborates the higher robustness to strided configurations of pooling the frequency domain.  We now mention aliasing in introduction and Section 2.3.\n\n[1] On-line plasticity in spoken sentence comprehension: Adapting to time-compressed speech. Patti Adank and Joseph T.Devlin. NeuroImage 2010.\n\n[2] Making convolutional networks shift-invariant again. In ICML, 2019 Richard Zhang\n\n[3] Spectral representations for convolutional neural networks. In Neurips, 2015 O Rippel, J Snoek, RP Adams\n\n[4] How Convolutional Neural Networks Deal with Aliasing, Ribeiro and Schoen, ICASSP'21.""}, {'title': 'Response to Reviewer e1mG', 'comment': 'We thank the reviewer for their feedback.\n\n\n> “In Sec 3.1, the authors claim that as ""DiffStride learns different strides for the time and frequency axes"", and as such ""this justifies using a different parameter for each dimension rather then sharing strides"". However, this validation is flawed. Just because the model takes advantage of this flexibility, it doesn\'t necessarily mean that it is beneficial in the end. This might be the case if we completely trusted gradient descent to reach global optimums of the cost function, which is not the case. Indeed, the final stride configuration might depend much on the initialization, as also suggested by the experiment in Fig. 3. Therefore, to validate such a claim the authors should simply include a baseline model where DiffStride is applied with shared striding parameters for the time and frequency axes.”\n\nWe thank the reviewer for pointing out this claim, and we agree that the existence of equivalence classes as shown in Fig. 3 invalidates it. To address this concern, we also trained the multi-task audio classification model with a shared stride for both axes and report the results in a new Table A.7. This shows that sharing the strides reduces the average performance on this task, and now refer to this result in the text to support our choice of not sharing the strides.\n\n>“”Fig. 4 would be much clearer if on the x axis MACs or FLOPs were represented, instead of the regularization term. This would help quantify how much computation DiffStride can save without losing significant performances. As it is, the value of the regularization term dos not tell much about actual computational cost.“”\n\n\nWe agree that MACs or FLOPs would be more interpretable. In the limited time of the rebuttal period, this appeared challenging as Tensorflow profiling tools do not track complex-valued operations (and thus do not count DFT and IDFT). Moreover, we wanted to disentangle the “theoretical” computational cost that depends only on strides (as illustrated by the regularization term) from implementation choices. However, we agree that Fig.4 does not provide enough hints on the real, practical performance of DiffStride in terms of wall time and memory usage. To address this issue, we added in the appendix the peak memory usage and per-step time (forward+backward) on a V100 GPU (see new Table A.2 in the Appendix). This shows that while DiffStride is more costly in training due to the necessity of computing and storing gradients with respect to strides, at inference the cost is identical to that of spectral pooling. We also extended our “Limitations” paragraph to propose other avenues for reducing the computational load of DiffStride. We also concede that efficiency was not our primary objective in this work. Efficiency gains could be obtained by exploring compiler options and implementation choices (esp. rematerialization options for backward efficiency and separation of the real/imaginary part after FFT to benefit from optimized loss precision primitives in further processing).\n\n>“To my understanding, within experiments the authors substitute a few downsampling layers (not even all of them) in a network with DiffStride. As a reader, I would have expected that every convolutional layer would be equipped with its own learnable striding. I grasp that this may not be practical but I don\'t have a clear view of the reason. The authors should consider adding some motivation behind this choice.”\n\nOur audio classification architecture performs downsampling at every convolution layer, so we do the same with DiffStride. ResNets first downsample with a max-pooling operator with stride 2 and then only have a few strided convolutions (namely, 4), regardless of their depth (i.e. switching from a ResNet18 to a ResNet50 only adds non-strided convolutions between downsampling layers), and we replace all of them on ImageNet, while only replacing 3 out of 4 on CIFAR. This is necessary to process small inputs, such as 32x32 CIFAR images, for which only a few downsampling layers are enough to reach a spatial dimension of 1x1. We do not replace the first max-pooling operator to keep our comparison consistent (we are only replacing strided convolutions in this work).\nCurrently, we only replace strided convolutions with DiffStride modules. In future work, we would like to replace non-strided convolutions DiffStride layers initialized with a stride of 1. We however will be in position to perform these changes only after reducing the cost of DiffStride (see new appendix A.2) by reducing the compute overhead due to non optimized complex operations (e.g. with Hartley transform, a real-valued counterpart to the Fourier transform) and the memory overhead due to dual spatial/spectral representation (e.g. with spectral non-linearities, exploration of rematerialization options).\n\n(Cont. in next comment)'}, {'title': 'Response to Reviewer BTMH', 'comment': 'We thank the reviewer for their feedback.\n\n>“However, from the experiment results (especially table3 and 4), it seems that the proposed method has marginal improvement compared to regular strided conv and spectral pooling baselines. It seems that the default setting can already achieve very good results on ImageNet and how it will affect the model performance when the model is large enough remains unclear. The benefit of learnable strides is not fully demonstrated. ”\n\nWe acknowledge that for image classification, DiffStride does not improve significantly over the best stride configurations found in the literature by cross-validation. However, this is not the main goal of our contribution. Our contribution is the first downsampling layer with a learnable stride, which replaces multiple training runs for cross validation with a single training run with stride optimization. This is a clear benefit we stress in the introduction "" even when initializing strides randomly, our model converges to the best performance obtained with the properly cross-validated strides of He et al"".\n\nThe hypothesis that we test in our image classification experiments is the following: “can learning strides by backpropagation remove the need for cross-validation?”. The fact that our ResNet-18 architecture becomes insensitive to the choice of strides when using DiffStride, while it is significantly more affected by poor choices of strides for baselines, supports this claim, with a variance over initializations an order of magnitude below that of strided convolutions. We added **new experiments with EfficientNet-B0 in Appendix**, that confirm our findings.\n\nThe fact that DiffStride does not discover a significantly better stride configuration for ResNet18 than the configuration of He et al. could be underwhelming or, more likely, an indication of the extensive exploration of hyperparameters by the ResNet authors. This is unlike the less explored classifier we use for single and multi-task audio classification, where DiffStride outperforms strided convolutions on four tasks out of five. \n\nWhat we hope will be the impact of our contribution on the community is that designing new convolutional architectures (or other architectures that involve downsampling), where finding the best strides configuration would require tedious cross-validation, will be easier with DiffStride which can retrieve the optimal performance in a single training without cross-validation. \n\n>“It would be great if the authors could implement one or more future works in Sec. 4 to showcase its capability further and show the proposed module\'s overheadein detail to let the readers better understand its limitation”\n\nExtending DiffStride to 1D or 3D requires extending our approach to new modalities, baselines and datasets, which we leave for future work.\n\nHowever, we agree that our discussion of limitations could be improved. To this end, we added in the appendix the peak memory usage and per-step time (forward+backward) on a V100 GPU (See Table A.2 in the Appendix). This shows that while DiffStride is more costly in training due to the necessity of computing and storing gradients with respect to strides, at inference the memory cost and run time are identical to those of spectral pooling.  We also extended our “Limitations” paragraph to propose other avenues for reducing the computational load of DiffStride. We also concede that efficiency was not our primary objective in this work. Efficiency gains could be obtained by exploring compiler options and implementation choices (esp. rematerialization options for backward efficiency and separation of the real/imaginary part after FFT to benefit from optimized loss precision primitives in further processing).\n\n>“Overall I think this paper presents a novel idea to learn stride in the downsampling layer. However, the current results are not good enough to showcase its effectiveness by making the stride learnable.”\n\nDiffStride is a first attempt at learning strides by backpropagation, which had not been shown to be doable in the past. We experiment on 8 classification tasks both on audio and images, with different architectures, and show the following: \n* DiffStride removes the need for cross-validating strides, and allows recovering the performance of the best configuration found by cross-validation in previous work on ResNets (likely the most widely studied convolutional architecture), in a single training phase, regardless of the initialization.\n* For less explored architectures (see our audio classification experiments), DiffStride finds new stride configurations that outperform strided convolutions significantly. \n* DiffStride allows to  regularize computation and memory cost of the models directly by backpropagation. \nWe additionally showcase the interpretability capabilities in Table 2, where the strides can provide insights about frequency biases in datasets and tasks and singling out the relevant scales used by a model.'}, {'title': 'Response to reviewers', 'comment': 'We thank the reviewers for their feedback and suggestions. This helped improve our submission and better support our claims. We summarize here the main changes brought to the manuscript:\n\n* Addressing a concern of all reviewers, we added a comparison between DiffStride and baselines in terms of wall time and memory usage, and extended the description of limitations.\n* Following the suggestions of Reviewer e1mG, we removed the LibriSpeech dataset from audio classification experiments.\n* To better support learning independent stride values for each dimension, we added an ablation study that compares sharing strides vs learning independent ones on audio classification.\n* We added results with EfficientNet-B0 in Appendix to address a concern of Reviewer ubnz.\n\nWe did our best to address the main concerns raised by reviewers and we hope that these improvements will be taken into consideration. To better highlight the changes brought to our manuscript, they appear in *brown* in the revised manuscript. Under acceptance, we will revert this change in color. We also answer each reviewer separately below.'}, {'summary_of_the_paper': 'This paper proposes DiffStride as a drop-in replacement to standard downsampling layers. It extends previous work Spectral Poolnig and learns the size of the cropping box in the frequency domain by backpropagation. Experiments are conducted on audio and image classification, and the results show that the model can learn non-integer stride and adapt different initial stride well.', 'main_review': ""The idea is interesting and makes sense to me. The writing is clear and easy to understand.  However, from the experiment results (especially table3 and 4), it seems that the proposed method has marginal improvement compared to regular strided conv and spectral pooling baselines. It seems that the default setting can already achieve very good results on ImageNet and how it will affect the model performance when the model is large enough remains unclear. The benefit of learnable strides is not fully demonstrated. It would be great if the authors could implement one or more future works in Sec. 4 to showcase its capability further and show the proposed module's overheadein detail to let the readers better understand its limitation.\n\n"", 'summary_of_the_review': ""Overall I think this paper presents a novel idea to learn stride in the downsampling layer. However, the current results are not good enough to showcase its effectiveness by making the stride learnable. The authors may want to find more ways, as discussed in Sec.4, to show its value.\n\n-- Post rebuttal\nAfter reading the authors' response, I raise my rating to 8."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In this paper, the authors propose DiffStride, a technique for learning the stride of downsampling operations in neural networks by gradient descent. Specifically, similarily to SpectralPool, the feature map is transformed to the frequency domain by a Discrete Fourier Transform, where it is then cropped according to learnable parameters. The authors envision a simple soft-relaxation of the cropping in order to allow the gradient to flow towards cropping parameters. Performances against both standard and random stride policies are reported on a number of datasets, both for audio and image recognition. Moreover, the authors introduce a regularization objective that penalizes the use of small strides, in the interest of encouraging downsampling for improving computational and memory cost.', 'main_review': 'PROs\n- The paper is very well written and pretty much clear at a first read. All the introduced components are properly motivated intuitively, and figures and algorithms really help the reader in understanding.\n- The authors excel in framing their work within related art. Specifically, they discuss both standard alternatives such as pooling, as well as alternatives based on fractional stride. For all of them, issues are properly pointed out such that the motivation behind this paper is very clear.\n- The paper is, to the best of my effort, technically sound. In terms of novelty, one might argue it being a bit incremental over SpectralPool, as it substitutes the cropping hyperparameters with learnable values. However, i) this step is not trivial, as the cropping operation is non-differentiable, and the authors introduce a solution to that and ii) this modification enables a significant improvement in performances, registered across a number of datasets.\n- The authors support their proposal by carrying out experiments on multiple datasets. Specifically, 6 datasets are employed for audio recognition and 3 (CIFAR-10, CIFAR-100 and Imagenet) are used for image recognition. In my opinion, for a paper of this type, attacking a very fundamental and obiquitous operator in modern architectures, being able to showcase improvements on non-toy datasets such as Imagenet is remarkable and noteworthy.\n- For every experiment, the authors report the mean and standard deviation over multiple trials, strenghtening the reliability of the conclusions.\n- The authors properly discuss limitations of their work, by highlighting the computational cost, some failure cases on DenseNets and implementation challenges on specific types of hardware.\n\n---\nCONs\n\nI think this paper is very solid, and I consider the following points as minor concerns.\n- In Sec 3.1, the authors claim that as ""DiffStride learns different strides for the time and frequency axes"", and as such ""this justifies using a different parameter for each dimension rather then sharing strides"". However, this validation is flawed. Just because the model takes advantage of this flexibility, it doesn\'t necessarily mean that it is beneficial in the end. This might be the case if we completely trusted gradient descent to reach global optimums of the cost function, which is not the case. Indeed, the final stride configuration might depend much on the initialization, as also suggested by the experiment in Fig. 3. Therefore, to validate such a claim the authors should simply include a baseline model where DiffStride is applied with shared striding parameters for the time and frequency axes.\n- Fig. 4 would be much clearer if on the x axis MACs or FLOPs were represented, instead of the regularization term. This would help quantify how much computation DiffStride can save without losing significant performances. As it is, the value of the regularization term dos not tell much about actual computational cost.\n- To my understanding, within experiments the authors substitute a few downsampling layers (not even all of them) in a network with DiffStride. As a reader, I would have expected that every convolutional layer would be equipped with its own learnable striding. I grasp that this may not be practical but I don\'t have a clear view of the reason. The authors should consider adding some motivation behind this choice.\n- Another point that may be worth discussing as an extension/future work. May the DiffStride technique be employed within a conditional computation framework, by predicting the striding parameters conditioned on the current example? I think that this strategy might succeed especially when aiming at optimizing the computational cost of a model. I do not expect such an extention to be implemented and tested within the rebuttal period. I would just like the authors\' opinion about that (potential, concerns etc).\n- In Tab. 1, the Speaker Id dataset provides no information whatsoever, as all methods score a perfect accuracy of $100.0 \\pm 0.0$. I suggest the authors to remove this dataset, as it is apparently beyond trivial, and including it downgrades the quality of the experiment rather than increasing it.\n- On CIFAR-100, the authors show that SpectralPool is significantly better than strided convs even with the same downsampling hyperparameters. Can the authors provide an intuition for this behavior?  ', 'summary_of_the_review': 'Overall, I recommend the paper for acceptance. The contribution is novel and clearly motivated, and experimental results are encouraging across different datasets. I think this paper can be of interest for many within the ICLR community. There are some improvement points as discussed above, but in my opinion the strengths clearly outnumber the weaknesses.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes an approach to learning optimal striding parameters in convolutional networks. The proposed approach, DiffStride, is a downsampling layer that builds on spectral pooling to allow for integer output dimensions, but arbitrary strides by cropping in the Fourier domain. Unlike spectral pooling, DiffStride relaxes the parameters of the cropping mask to be differentiable, and using the stop-gradient operator in the cropping. Results show that the proposed approach can work well as a drop-in replacement for normally fixed pooling layers. It is also demonstrated that from different random initialisations a variety of different pooling approaches can be learned that acheive similar performance. Use of a regularisation term the attempts to encourage time and space efficiency reduces this variability and allows accuracy to be traded off.', 'main_review': ""Positives\n---------\n\nOverall I feel that the paper presents a really neat idea well. Besides a few minor issues (see below) the paper was enjoyable to read and describes the ideas well. There is thorough experimentation on a range of tasks and model architectures that demonstrate the power of the proposed approach. \n\n\nConcerns\n--------\n\nReally, the only major concern I have is around the rather limited discussion of the limitations. In particular, I'd at least like some discussion (or thoughts) on why it doesn't improve densenet performance, and I'd like to see experimental results presented on the impact of the use of DiffStride during training and inference, both in terms of measurable memory usage and impact on training & inference wall time. Obviously such results can be caveated with a statement that the implementation could be improved as per the existing discussion in the limitations section.\n\n \nMinor points\n------------\n\nPlease check and fix algorithm 1 - I believe line 4 should be using the output from the filtering with the mask, rather than the raw FFT result. It would also be better not to reuse the $\\tilde{y}$ symbol on lines 4 and 5, and it would also be helpful if the symbols (the variants of the intermediate computations $y$, $\\tilde{y}$}) were clearly labelled on Fig 1.\n"", 'summary_of_the_review': ""Overall, as can be seen from my review I'm satisfied that this paper makes a good contribution."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In this work the authors introduce a differentiable stride formulation, which allows for learning the stride value. To this end, they propose learning the size of a cropping mask in the Fourier domain, which allows for learning how to perform resize in a differentiable way. Authors present experiments on several datasets on different domains (image, audio), including large scale datasets.', 'main_review': '+ The paper is well-written and the proposed formulation is sound. \n\n+ Also extensive experiments are provided on several datasets to demonstrate the benefits of the proposed method.\n\n\n- Authors demonstrate that the proposed method can recover from different initial strides and learn the optimal one. However, my most important concern is the lack of appropriate comparisons with neural architecture search approaches, since - in my view - these are the most direct competitors of the proposed method. Therefore, I would expect experiments and appropriate discussions between the cost and benefits between the proposed method and neural architecture search approaches.\n\n- The proposed method seems to target only residual architectures. Discussion on how the proposed method could be used with other architectures would be beneficial\n\n- Also, providing experiments with different architectures (apart from resnet) on the same dataset would also further improve the confidence on the obtained results.\n\n- What is the actual overhead of the using the proposed method in real applications? How much does the training time and memory usage increases?', 'summary_of_the_review': 'Overall, I think this is a good paper with an interesting approach on differential stride learning. There is no theoretical discussion and I would like to see some additional experiments, however I feel that this work is slightly above acceptance threshold, mainly given the novelty of the application.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Learning Strides in Convolutional Neural Networks', 'authorids': ['~Rachid_Riad1', '~Olivier_Teboul2', '~David_Grangier1', '~Neil_Zeghidour1'], 'authors': ['Rachid Riad', 'Olivier Teboul', 'David Grangier', 'Neil Zeghidour'], 'keywords': ['Strides', 'Convolutional neural networks', 'Downsampling', 'Spectral representations', 'Fourier'], 'abstract': 'Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.', 'pdf': '/pdf/1bc01ea49b5a288387ac5de300847b1d6690d940.pdf', 'one-sentence_summary': 'We introduce DiffStride, the first downsampling layer with learnable strides for convolutional neural networks.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'riad|learning_strides_in_convolutional_neural_networks', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/learning-strides-in-convolutional-neural/code)', '_bibtex': '@inproceedings{\nriad2022learning,\ntitle={Learning Strides in Convolutional Neural Networks},\nauthor={Rachid Riad and Olivier Teboul and David Grangier and Neil Zeghidour},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=M752z9FKJP}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Minghao Guo', 'Veronika Thost', 'Beichen Li', 'Payel Das', 'Jie Chen', 'Wojciech Matusik']",ICLR,Data-Efficient Graph Grammar Learning for Molecular Generation,https://iclr.cc/virtual/2022/oral/7012,2022," The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points.",Oral 2: AI applications,https://openreview.net/pdf?id=l4IHywGq6a,https://openreview.net/forum?id=l4IHywGq6a,l4IHywGq6a,"[{'title': 'Thank you for your interest in our work!', 'comment': 'Sorry for the late reply, we totally missed this!\n\nThe evaluation is done on a set of well-established metrics, typical for evaluating the quality of machine-synthesized molecules. Wet lab evaluation will be attempted in future. Our learned graph grammar provides a means to generate more data for evaluation (as you suggest), and optimization based on the latter.'}, {'title': 'A question related to the design', 'comment': 'Hello authors,\n\nThanks for presenting this interesting work! The graph grammar learning idea is truly fascinating!\n\nI have a naive question. How do you conduct the evaluation?\n\nIt seems you can evaluate all metrics easily. If these metrics are very easy to evaluate, why can not you just evaluate more data to augment the limited training dataset?  In most cases, I suppose some properties of proteins can not be evaluated without wet experiments.\n\nI am not familiar with the problem background. Looking forward to your reply!\n\nBest,\nCan'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper presents an approach to learn graph grammars for molecule generation in a very data-efficient way.  The approach combines bottom-up grammar construction by contracting graph substructures and evaluation-driven learning of the parameters in grammar construction in order to optimize metrics of interest.  This paper is well written and the graph grammar learning approach is novel and can potentially have impact beyond just generating molecules.  All reviewers unanimously recommended acceptance of this paper.\n\nA few things emerged in the reviews and discussions with authors, regarding in particular the computational cost and scalability of the approach and the actual molecule sampling process after learning.  I hope the authors can clarify these in the paper, and as the authors said in the discussion that exploring a more expressive model that uses learned probabilities on the production rules can make the model more powerful, which is a promising direction for the future.'}, {'title': 'Thanks for the response', 'comment': 'Thanks for the detailed response that resolves my concerns! I would maintain the current score and recommend acceptance of the paper.'}, {'title': 'Thanks for the response', 'comment': ""Thanks for your effort. You clarified all my doubts, I am now convinced this paper deserves to be accepted and I'm raising my score to 8. Good job!""}, {'title': 'General Response', 'comment': 'We thank all reviewers for the detailed and encouraging reviews! We are grateful that the reviewers recognized the novelty of the topic, the clarity of writing, the fact that our approach addresses serious limitations of existing works, the impressive performance on small datasets, and its potential impact.\n\\\n\\\nWe have addressed specific comments and suggestions in the individual responses and also updated the paper accordingly. In summary, we provided additional experimental results justifying our algorithmic components to reviewer 4wRv, and gave more details and backgrounds about the algorithm to reviewers oHTg and Qrrj. In the reply to reviewer LtXp, we elaborate on the scalability aspect, which was mentioned in several reviews.\n\\\n\\\nWe hope that we have cleared all your concerns and we will be happy to provide further information if needed.'}, {'title': 'We thank the reviewer for their interesting comments! Please see our rebuttal below. ', 'comment': '\n- **Model Works ""Only"" on Small Datasets** \\\nOur goal was to design a molecule generation model for the ""few data"" scenario, for which there are no solutions yet. It is indeed true that our approach solves this challenge with added complexity - amongst others, by applying symbolic knowledge representation which, besides having several benefits, is known to be more involved. For practical scenarios where the data is too large for our model (~1k samples), we recommend to use other deep learning-based methods. \n\\\n\\\nNevertheless, there are several important points on optimization to be considered in practical applications. First, the convergence of learning can be accelerated by switching the naive MC sampling to a more advanced method (e.g., importance sampling [3]). Further, the implementation can be improved. System-level code optimization has been out of our focus so far but we expect considerable acceleration can be achieved by adding parallelization.\n\\\n\\\nLastly, we want to stress that, unlike what the wealth of DL-based models suggests, the small data scenario is much more common in practice [4]: \n> even though such big datasets (and access to them) are becoming common place, they do not represent the datasets most materials researchers work with on a day-to-day basis. Within general experimental material research projects, researchers generally produce no more than a hand full of data points (c.q. samples) when optimizing a production method, synthesizing a new material or tuning an existing one for a specific application.\n\n\n- **Related Work on Task-based Learning** \\\nThank you for pointing out this connection. We added some such works in the updated paper.\n\n\n[3] Feller, William. An introduction to probability theory and its applications, vol 2. John Wiley & Sons, 2008. \\\n[4] Vanpoucke et al. Small data materials design with machine learning: When the average model knows best  featured, Journal of Applied Physics 128, 054901 (2020).'}, {'title': 'We thank the reviewer for their interesting comments! Please see our rebuttal below. ', 'comment': '\n- **(Q1) Computational Cost**\n\\\nAlthough we did not optimize our implementation, which could be substantially accelerated, we note that for the small data scenario, the current runtimes are quite manageable: for the 117-sample training set, our model takes 1 hour per training epoch and we train it for 20 epochs (the 200 in the appendix was an unfortunate typo).\n\\\n\\\nSince there are no existing solutions for the scenario (small training data) we address, we can only compare to the models addressing the large data scenario, which obviously requires considerable training efforts. Amongst those approaches, our approach is closest to the vocabulary (Jin et al., 2018 & 2020) and grammar-based models (Dai et al., 2018; Kajino, 2019; Nigam et al., 2021). The vocabulary-based models extract the substructure vocabulary in a preprocessing step. At runtime, Jin et al. (2018) apply enumeration to assemble substructures, whose time complexity is exponential to the substructure size and heavily relies on assumptions of the nature of the substructures. HierVAE (Jin et al., 2020) also took several days to (pre)train on the large dataset. Dai et al. (2018) and Nigam et al. (2021) apply a manually constructed grammar, which is not at all comparable to our grammar that captures the specifics of the dataset at hand. Theoretically, the grammar construction of MHG (Kajino, 2019) is not polynomial anymore since it also applies a graph isomorphism test. Yet, in practice it runs rather fast since the system simply decomposes everything to the finest level; as a result, the grammar does however not capture the critical substructures. Overall, we note that this comparison is lacking since the existing works are of very different nature (see also the reply to reviewer LtXp).\n\n\n- **(Q4) Clarification about Algorithm**\n\\\nPlease see ""Details about Molecule Generation using the Learned Grammar"" and ""Pruning"" in the reply to reviewer oHTg, where we detail that the constructed grammar is *completely* determined by the optimization process (described in Sec. 4.2). We only remove duplicate rules. Since every rule captures a specific structural aspect of the training data, we expect any automated post-processing beyond the optimization process to be potentially dangerous. In practice, one could resort to domain experts (e.g., chemists), who are at a better position to judge whether rules are redundant.\n\n\n- **(Q5) Number of Rules Produced for Evaluation Data**\n\\\nFor Isocyanates, the final, optimized grammar has 34 production rules. There are 78 rules for Acylates and 32 rules for Chain Extenders. For the large polymer dataset, there are in total 341 production rules. The number of production rules is positively correlated with the size of the training set. Despite the fact that a small number of production rules can generate a large number of samples, the constructed grammar becomes more complicated to cover the underlying diversity of the training samples when the training set is larger, resulting in more production rules.\n\n\n- **Minor Issues / Clarifications** \n  - **(Q2) Statement about Performance on More Training Data** \\\nOriginally, we had additional results about how the model performed with increasing amounts of data, but we removed them because they are not too different from the 0.15% dataset we reported in the paper. However, based on the discussion with reviewer 4wRv, we have added them back to the paper.\n  - **(Q3) Why are some results in the tables underlined?** \\\nThe underlines highlight second-best results. They are now applied consistently throughout the paper and explained in the captions.'}, {'title': 'We thank the reviewer for their helpful comments! Please see our rebuttal below.', 'comment': '\n- **Formatting**\n\\\nWe fixed the notation, but after some thoughts, we did not move Appendix A and Table 3 into the main paper. For one reason, this requires substantial space that goes over page limit. More importantly, our approach targets scenarios with small datasets (see also the item ""Model Works Only on Small Datasets"" in the reply to reviewer LtXp); thus, we only conducted these experiments for comparison. We could shorten the related work but would need to cut additional parts. We wanted to provide this intuition first, however, we are open to suggestions how to fit the contents to page limit.\n\n\n- **(Q1) Hyperparameters**\n\\\nOur algorithm actually has no such hyperparameters as you mention (the learned grammar is ""... *completely* determined by the sampled order of hyperedges""). We have clarified this in the updated paper. In fact, the algorithm chooses the set of production rules that performs best, without any constraint on the number or the length of the rules.\n\n\n- **(Q1) Pruning**\n\\\nWe do not do any pruning but keep all (unique) production rules that are learned. In this way, the constructed grammar contains all structural information of the input samples.\n\n\n- **(Q2) Details about Molecule Generation using the Learned Grammar**\n\\\nInitially, we tried uniform random sampling of the rules. However, the possibility of generating arbitrarily large molecules (i.e., by choosing production rules with a non-terminal symbol on the right-hand side more likely) sometimes resulted in a never-ending generation process, a problem in practice.\n\\\n\\\nFor that reason, we condition the generation strategy based on (non)terminal symbols in the rules: during generation, we exponentially increase the probability of the production rules without non-terminals symbol on the right-hand side, based on the iteration number. Formally, the probability to select a certain production rule $r$ at iteration $t$ is $p(r) = Z^{-1}exp(\\alpha t x_r)$, where $x_r$ is a binary value indicating whether rule $r$ contains only terminal symbols on the right-hand side, and $Z$ is a normalization factor. We used $\\alpha=0.5$ in our experiments, since it turned out to reduce the generation time sufficiently while maintaining satisfactory diversity. This part was missing in the submission and we have added it.\n\\\n\\\nSince we focus on designing a model that can comprehensively represent the molecular design space in this paper, we use this simple generation procedure. It could be replaced by a more advanced one using (reinforcement) learning to select molecules from the design space (e.g., molecules with specific properties). Ideally, as the reviewer suggests, these two components can be jointly learned. This is our immediate future work.\n\n\n- **(Q3) Clarification about Algorithm**\n\\\nIt is true that finding an optimal grammar amounts to a discrete optimization problem, where the space of grammar rules is combinatorial. MC sampling coupled with REINFORCE algorithm falls in the category of stochastic methods to solve discrete optimization, and it is known to find a global optimum efficiently [2]. In our algorithm, REINFORCE is only used for gradient computation, since the grammar construction process is non-differentiable. In other words, we do not formulate the construction as a Markov decision process and do not use RL. \n\\\n[2] Yan, Di, and H. Mukai. ""Stochastic discrete optimization."" SIAM Journal on control and optimization 30.3 (1992): 594-612.\n\n\n- **(Q4) Choice of Optimization Objectives**\n\\\nGenerally, our method can support any combination and any number of optimization objectives. In our experiments, we chose diversity and RS for two reasons.\n(I) On the one hand, we consider them to be two of the most important metrics in practice: diversity reflects the comprehensiveness of the generative model, while RS represents the quality of generated molecules. Observe that they nicely complement each other (as can be seen in Fig. 4) and, with both being the objectives, the grammar learning automatically takes this trade-off into account. Lastly, the experimental results illustrate that optimizing only for diversity and RS yields reasonable performance on other metrics as well.\n(II) On the other hand, we want to emphasize the fact that our approach is very different from existing works, which perform distribution fitting based on distribution statistics, such as logP and SA. We are able to consider qualitative metrics which are critical for practical chemical engineering (in our case, judging whether a generative model can indeed discover molecular structures that are both novel and synthesizable). '}, {'title': 'We thank the reviewer for their detailed comments! Please see our rebuttal below.', 'comment': '- **About Model Components**\n  - **(Q1) Feature Extractor** \n\\\nTo demonstrate the plug-and-play capability of our system, we ran additional experiments with the most simple embeddings from deepchem [1] (through concatenating atom and bond features). We obtain comparable but slightly worse results (see Appendix D, Figure 6).\n\\\n[1] https://deepchem.readthedocs.io/en/latest/api_reference/featurizers.html \n\n  - **(Q2) Stability of REINFORCE** \\\nInitial experimental results indicated that the algorithm converged to a similar value across different random seeds, although the trajectories varied to some degree. We have added figures to demonstrate these results in Appendix E.\n\n\n- **(Q3) Scalability** \\\nFor general considerations regarding the data size, we stress that, unlike what the wealth of DL-based models suggest, the small data scenario is much more common in practice. See the in-depth discussion under ""Model Works Only on Small Datasets"" in the reply to reviewer LtXp. Although we did not optimize our implementation, which could be substantially accelerated, we note that for the small data scenario, the current runtimes are quite manageable: for the 117-sample training set, our model takes 1 hour per training epoch and we train it for 20 epochs (the 200 in the appendix was an unfortunate typo).\n\n\n- **(Q3) Benefits of More Data** \n\\\nWe conducted such experiments on 0.3% of the large dataset and obtained only slight performance gains (see the updated Table 3). This can be explained by the nature of our approach. Unlike regular deep learning, every grammar (i.e., even the ones constructed based on very small datasets) fully captures the training data. As a consequence, larger datasets and the resulting grammars are expected to improve only distribution statistics and diversity but not the generally achieved quality. \n \\\n \\\nWe can indeed derive an estimate about the obtained gain when training on the entire large polymer dataset, based on the considerations in Jin et al. (2020). The latter work mined a rather comprehensive vocabulary of 436 substructures, which is covered by only (randomly selected) 436 molecules. The 0.15% dataset we consider contains 117 and the above-mentioned 0.30% dataset contains 239 such molecules. Consequently, we expect the distribution statistics to increase slightly when considering all 436 molecules that cover these motifs, but we do not expect major gains if going beyond (that is, including the remaining ~80k training samples for training). '}, {'summary_of_the_paper': 'This paper introduces a new grammar-based generative model for the molecule generation task. The graph grammar is defined as a set of production rules that operate on module graphs (i.e. the molecule graphs are not linearized as done in some previous work). The graph grammar is learned by iteratively contracting hyperedges (i.e.  edges that can connect multiple nodes, defined by simple chemistry-inspired rules) into non-terminal nodes and the submission proposes to learn how to sample the hyperedges by a REINFORCE algorithm that optimizes for several molecule generation metrics. \n\nThe evaluation is done both on small and large molecule generation datasets. Notably, the proposed method achieves strong performance while being very data-efficient.', 'main_review': 'The proposed method is novel and opens potential for new line of research in graph grammar. I personally find the submission to be clear and well-written. Overall, I am positive and would like to give a ""accept"". \n\nHowever, I have few more comments that I hope to help improve the paper. Several of them are around the modeling choices and some of them are to clarify my understanding of the practical usefulness of the proposed method.\n\n1. The proposed method uses pretrained graph neural network to generate the sampling weights and claim that this can be replaced by any plug-and-play feature extractor. Can the authors provide data to support this claim? If this is actually true, I would suggest to use the simplest feature extractor to keep the proposed method clean.\n2. Is the REINFORCE algorithm stable across random seeds? My hunch is that they are not. Can the authors provide error bars in for their data? Can the authors provide convergence curves to demonstrate the learning? \n3. While the proposed method is efficient, I have the impression that it does not scale up well to more data (the authors have to subsample the training set for the experiments in Table 3). My first request is for the authors to report the actual computation time for the proposed method on the experiments they have done. Second, aside from the computational challenge, can the proposed algorithm benefit from more data? If so, can we estimate how much we can gain by including the whole training set of the large polymer dataset?', 'summary_of_the_review': 'I find the proposed method to be novel and effective. the paper is clear and well-written. I have a few comments around modeling designs but I think the comments are addressable. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'None', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a sample efficient hypergraph grammar learning algorithm for generating molecules. The bottom-up search algorithm conducts iterative contraction according to a learned policy, where in each iteration, several hyperedges are sampled, contracted into a single node, and written into a production rule as part of the final grammar. The search policy is being trained using RL where the rewards are given by evaluating a set of molecules generated by the policy. The proposed method is evaluated on a small and a large monomers dataset and outperforms existing molecule generation algorithms in terms of the synthesizability and the membership rate.', 'main_review': 'This paper proposes a generic solution for learning to generate graphs in an extreme few-shot manner where only dozens of examples are provided, which is pretty common in the polymer/monomer domain. The method is general and can have a large impact on the graph learning community. Although technically the proposed method is similar to some previous work like MHG[1], it cleverly preserves the class-specific properties by operating on subgraphs instead of individual atoms. Also, the idea of learning a grammar search algorithm using RL instead of directly learning the grammar is novel and interesting in the domain. The proposed method is well-supported by experiments on datasets of different scales.\n\nThe paper is well-structured overall but there is still room for improvement. The notation in section 4 is a bit unclear. In section 4.1 the subscripts are usually used for denoting the number of iteration, and the superscripts are used for different connected components. While in section 4.2 right before Equation (3) we see there is $e_t^{(j)}$, where the meaning of superscript $(j)$ is unclear. And $t$ here is used both for denoting the current iteration and the total number of iterations. Also if I understand correctly, $X$ is a binary matrix, and instead of $p(X)=\\prod_t\\prod_j \\phi(e_t^j;\\theta)$, it should be $p(X)=\\prod_t\\prod_j \\phi(e_t^j;\\theta)^{X_{tj}} (1-\\phi(e_t^j;\\theta))^{1-X_{tj}}$?\n\nFor the experiment section, both Appendix A and Table (3) contain critical information for understanding the experiment setting and results, and so they should be put earlier in the main text. \n\nQuestion:\n\n1. The paper proposes a clean algorithm for learning grammar. But in practice, there must be a lot of hyperparameters to tune, such as the number of production rules, the maximum/minimum length of a single production rule, etc.? How do you decide these parameters? Do you do any pruning for the production rules?\n\n2. The main text does not mention how to generate molecules using the learned grammar rules. Do you sample the production rules uniformly randomly during testing? If yes, do you plan to learn a policy for generating molecules? How to jointly learn the generation policy together with the rule search policy?\n\n3. The space of grammar rules is combinatorial as mentioned in section 4.2. How does RL circumvent that and learn a reasonable policy?\n\n4. Why do you only use diversity and RS as the optimization objectives, as mentioned in Appendix A?\n\n[1] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimization. In International Conference on Machine Learning, pp. 3183–3191. PMLR, 2019.', 'summary_of_the_review': 'Overall, this paper makes decent contributions in both technical and empirical aspects, and therefore I recommend accepting the paper provided that the authors correct the writing issues.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""The paper presents a data-efficient graph grammar-based approach to molecular generation (with a focus on polymer generation). The model requires that the molecule be represented as a hyper-graph. The productions of the grammar are basically rules for contracting connected nodes into a single node (a non-terminal symbol), which are later expanded during generation. Each rule is essentially derived from a random sample of the hyper-graph edges: the grammar is produced by iteratively contracting the nodes incident to the edges into non-terminal nodes until the hyper-graph consists of a single non-terminal node. This procedure is applied simultaneously to all the graphs of the training set. Once a grammar is obtained, it is optimized by maximizing some desired metrics (such as diversity). The optimization is non-differentiable since it requires generated graphs to evaluate the objective function, and thus the authors resort to approximate the gradient's expectation using MC sampling and the REINFORCE score function estimator.\nThe experiments aim at showing that the proposed model is data-efficient (achieving strong performances using a small fraction of the molecules used by competitors), capable of extrapolation (since it can produce molecules outside of the training set), and explainable (since it is shown to identify functional groups which characterize the family of generated molecules).\n\n"", 'main_review': 'This is a pretty solid contribution. Although the paper is written clearly, I admit I had trouble trying to understand the method initially, but once I got the big picture, everything lined up nicely. The methodology in itself (generative learning through graph grammars) is not new, and the proposed model presents some similarities to the HMG model of Kajino. However, it draws from previous methods in a clever way and addresses most of their limitations (for example, the production rules involve substructures instead of single atoms). This, in my opinion, is a plus. The main advantage of this model is its data efficiency, which the experiments corroborate nicely. Nonetheless, I have a few points I would like the authors to address before I give complete acceptance. \n\n1) It is unclear to me what is the computational cost of constructing the grammar and training the model since generating the production rules involves graph matching (and although you affirm this isn\'t an issue). How does your training/grammar inference cost compares to other approaches? \n\n2) ""we also show that with more training data (0.3% of the whole dataset), our method can achieve better performance"". I might have missed it, but where is this shown?\n\n3) Why are some results in the tables underlined? You should explain it in the captions.\n\n4) This is more of a request for clarification. How is the final set of grammar rules chosen? If I understood correctly, the bottom-up construction process generates a lot of near-duplicates, as well as potentially useless production rules, especially during the initial iterations. Is there any pruning process after the optimization has been performed, or just everything is kept?\n\n5) How many rules does the grammar construction process produce in the datasets you evaluated?', 'summary_of_the_review': 'I am decidedly leaning towards acceptance. In my opinion, this is a strong contribution that deserves to appear at ICLR. I will most likely raise my score to an 8 after the authors will answer my questions and modify the text accordingly where needed.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'None', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposed an interesting grammar learning-based data-efficient molecule generation model. \nThe key idea of this method is to learn a set of production rules via a hyper-edge selection process that optimizes a set of evaluation metrics. \nThis method achieves impressive performance on extremely small datasets and achieves competitive performance compared with end-to-end models trained on a large dataset. \n', 'main_review': 'Strength:\nThis method is very data efficient given grammar-based learning. \nThe hypergraph contraction-based production rule learning process is very novel (at least from my perspective).\n\nWeakness:\nIt seems like the model can only work on a small set of molecules to generate the rules due to the computational complexity.\nThe way this model learns the production rule is actually very related to a domain called task-based learning, where people tend to optimize their model towards some non-differentiable evaluation metrics. \nThe authors should reference some of those works, e.g. \n""Task-Based Learning via Task-Oriented Prediction Network with Applications in Finance"", \n""Task-based End-to-end Model Learning in Stochastic Optimization"".', 'summary_of_the_review': 'Due to the novelty of this data-efficient grammar-based molecule generation method and its impressive performance, I recommend to accept this paper.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'N/A', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Data-Efficient Graph Grammar Learning for Molecular Generation', 'authorids': ['~Minghao_Guo1', '~Veronika_Thost1', '~Beichen_Li1', '~Payel_Das1', '~Jie_Chen1', '~Wojciech_Matusik2'], 'authors': ['Minghao Guo', 'Veronika Thost', 'Beichen Li', 'Payel Das', 'Jie Chen', 'Wojciech Matusik'], 'keywords': ['molecular generation', 'graph grammar', 'data efficient generative model'], 'abstract': 'The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points.\n', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'guo|dataefficient_graph_grammar_learning_for_molecular_generation', 'pdf': '/pdf/c17b0db09f98b3279ad677650f18acbf907883ce.pdf', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/data-efficient-graph-grammar-learning-for/code)', '_bibtex': '@inproceedings{\nguo2022dataefficient,\ntitle={Data-Efficient Graph Grammar Learning for Molecular Generation},\nauthor={Minghao Guo and Veronika Thost and Beichen Li and Payel Das and Jie Chen and Wojciech Matusik},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=l4IHywGq6a}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Meng Qu', 'Huiyu Cai', 'Jian Tang']",ICLR,Neural Structured Prediction for Inductive Node Classification,https://iclr.cc/virtual/2022/oral/5948,2022," This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.",Oral 2: Structured learning,https://openreview.net/pdf?id=YWNAX0caEjI,https://openreview.net/forum?id=YWNAX0caEjI,YWNAX0caEjI,"[{'title': 'Response', 'comment': 'Thanks for following up! \n\nIf I understood correctly, you meant we can direct treat the $\\tau$ function of each node as logits for label prediction? If this is the case, then it is exactly the GNN baseline in our experiment. You might check Table 1 and Table 2 for the detailed comparison.\n\nBy the way, the code of SPN is now available at https://github.com/DeepGraphLearning/SPN.'}, {'title': 'Response ', 'comment': 'Hi, thanks for your super quick reply!\n\nIt is true that for sum-product loopy BP, the $\\tau$ functions no longer satisfy the conditions in proposition 1, and the marginal inferred by loopy BP would be somewhat different from the pseudomarginal.\nBut I am curious about the practice perspective. I mean during training, the cross-entropy loss is directly applied on top of $\\tau$ functions. To some extent, $\\tau$ looks like the predictive logits of the target category distribution (not exactly true, but practically we could view it from this perspective). Therein, I suspect the experimental results would be good if directly using $\\tau$ to predict the label for each node (maybe you should scale $\\tau$ to a valid density during testing). Did you conduct relevant experiments to verify this hypothesis?'}, {'title': 'Response', 'comment': ""Hi Zijing. Thanks for your question!\n\nIndeed, we could directly infer the node labels based on the output of the node proxy network. But by doing that we would have thrown away the CRF, as the node proxy network $p_\\theta(y_s)$ is trained by standard cross entropy loss.\n\nThe power of CRF lies in the joint label distribution $p_\\theta(\\mathbf{y}_V)$. The goal of max-product loopy BP is to infer the best node labels from this *joint* distribution. This set of best labels may differ from what you'd get by taking the best label from each node *marginal* distribution, which can not model the label dependencies between nodes.\n\nFor sum-product loopy BP, the goal is to infer the marginal label distribution on each node. During training, we implicitly force $\\sum_{y_s} \\tau_{st}(y_s, y_t) = \\tau_t(y_t)$ (through the proxy objectives), and thus the marginal distribution inferred by sum-product loopy BP would be very similar to the node-wise pseudomarginal on training graphs. But during inference, the constraint may not hold on test graphs, which means that those $\\tau$ functions no longer satisfy the conditions in proposition 1, and thus the marginal distribution inferred by sum-product loopy BP can also be different from the pseudomarginal on each node.""}, {'title': 'Question on node-level inference', 'comment': 'Thanks for your reply! It has addressed my questions above. But here I have another follow-up question.\n\nIn the inference phase, say the node-level one specifically here, you propose to run loopy BP for the estimate of the marginal $p_\\theta (y_s )$. Why not directly apply the output of proxy networks? I mean according to Proposition 1, the pseudomarginal is approximately equal to the marginal of loopy BP. In practice, it seems that using the pseudomarginals in inference phase is more attractive than running loopy BP since the latter is time-consuming.'}, {'title': 'Response', 'comment': 'Thanks for your interest in our paper!\n\nCRF and SPN w/o proxy differ in how they parameterize the theta-functions. For CRF, we directly parameterize the theta-functions as the logits computed by GNNs. For SPN w/o proxy, we parameterize the theta-functions according to Eq. (5) of our paper, where GNNs are used to compute the tau-functions (pseudomarginals), which are further combined into the theta-functions.\n\nWe are now finalizing the code. Once done, we will upload the code to https://github.com/DeepGraphLearning/SPN. Stay tuned!'}, {'title': 'difference between CRF and SPN w/o proxy', 'comment': 'Dear authors,\n\nThanks for your excellent work. It is exactly interesting to combine the idea of CRF and GNN. \n\nI have some questions about figure 2. What is the difference between CRF and SPN w/o proxy? It seems that both two methods train the model via the maximin game in Eq.3 with loopy belief propagation.\n\nBesides, would you plan to release the code?'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Most of the existing GNN based methods model the node labels independently and ignore the joint dependency of node labels. The CRF-based methods work in this setting, but they are hard to learn. Hence, this paper proposes to ease the learning difficulty by solving the proxy problem and simplifying the max-min problem.\n\nThe SMN model proposed in this work is much cheaper than the CRF method. For parameters, since the node GNN and edge GNN share parameters in layers, only a few amounts extra parameters are introduced. As for the training time, it just doubles the general GNNs. Compared with CRF methods, the cost saved by SMN is significant.\n\nEmpirically, SMN works well in most settings, in terms of both node-level accuracy and graph-level accuracy, the different backbones, and different datasets. Meanwhile, the authors provide results to show the effect of refinement, the shared GNNs, the different learning methods, convergence, and a tiny case study. The experimental results are significant and well organized.\n\nAfter the rebuttal and discussion, all reviewers are in a favor of accepting this submission.'}, {'title': 'Response', 'comment': 'Thank you for the helpful suggestions on the caption of fig.3 and the name of the model!\n\nWe agree that the caption and the model name should be further revised. For now, we are not able to update the draft as the function is currently closed, but we will keep editing the paper to further improve its quality.'}, {'title': 'Reply', 'comment': 'Thank you. \nRegarding fig.3, the caption is still unclear, it is ungrammatical, and should mention that the colour coding represents different possible labels. \n\nRegarding model naming: I hadn\'t paid much attention to the name of the model on my first read, but reviewer LsFg is right: it\'s a terrible name. Sticking to it, I think the paper is sure to confuse readers, reduce its reach, and in the years to come, each mention might need to be introduced by ""the name given to the model is another misnomer, it should really be called (...)"". '}, {'title': 'Response', 'comment': 'Thank you for the follow-up questions and suggestions!\n\nUsing MCMC (e.g., Gibbs sampling) to replace loopy BP for optimization is an interesting suggestion. Nevertheless, MCMC requires a burn-in period, which can be expensive. To solve the problem, one idea is to leverage k-step contrastive divergence, which is an MCMC method used for optimizing restricted Boltzmann machines. Basically, the method runs Gibbs sampling for only a few steps to speed up optimization. We did additional comparison against this method on Cora*, Citeseer*, and Pubmed*, where GAT was used as the backbone model and k was set to 10. The results are presented as follows:\n\n|            | Cora* | Citeseer* | Pubmed* |\n|:----------:|:-----:|:---------:|:-------:|\n| 10-step CD |  53.08 |    44.18   |   49.04  |\n|     SMN    |  58.78 |    49.02   |   52.91  |\n\nWe can see that MCMC still gets worse results. The reason might be that we only ran MCMC for 10 steps for efficiency purposes, and thus the samples were not good enough, yielding inferior results. Despite that, we do agree that combining with MCMC would be a promising direction to further improve optimizing CRFs, and we will further explore how to address the efficiency issue of MCMC in our context.'}, {'title': 'reply', 'comment': 'Interesting, thanks!'}, {'title': 'response', 'comment': 'Thanks for addressing these and clarifying!\n\nI did still have a question though. You mention: ""This training instability is not caused by loopy BP, but by the nature of the maximin game.""\xa0To make this claim, I think you would need to use a theoretical argument or run experiments showing that the instability remains no matter what inference algorithm is used. The draft notes that MCMC can be used (Sec. 3.3 states: ""This can be done by MCMC, but the time cost is high, so approximate inference is often used, such as loopy belief propagation (Murphy et al., 1999)."") but only loopy BP is used in the experiments. How then can one be sure that the training instability is not caused by loopy BP if you only tried to solve the maximin game when using loopy BP?\xa0Might it be more stable with MCMC? '}, {'title': 'Response to Reviewer LsFg (2)', 'comment': '----------\n\n### Q2: Comparison with CRF.\n\n**Response:** Further comparing our method with CRF is another insightful suggestion! \n\n**(1) Why would SMN outperform CRF? Why is the proxy problem superior?**\n\nBoth classical CRF and our method try to maximize data likelihood for model training, and the difference lies in how we achieve that.\n\n- CRF: The typical way for likelihood maximization is to formalize the problem as a maximin game or a saddlepoint problem, as elaborated in Eq. (3) of Sec. 3.3. Then the problem can be optimized by alternating between loopy BP and gradient descent. However, such a maximin game is often hard to optimize by nature due to training instability, which is also observed in training generative adversarial networks (GANs). This is illustrated in Fig. 2 of our draft, where we can see that the training curve of CRF is very unstable. This training instability is not caused by loopy BP, but by the nature of the maximin game.\n\n- Our Method: Our method performs likelihood maximization in a different way. We investigate the optimality condition that a model with maximum likelihood should satisfy. Then we design a proxy problem whose solution roughly satisfies the optimality condition, and thus this proxy problem can be viewed as a surrogate for likelihood maximization. The major attractive property is that optimizing this proxy problem only requires ""local supervision"" on nodes and edges, and thus our method is much more stable in terms of training, achieving much better results.\n\nTherefore, although CRF and our method both try to maximize data likelihood, our method has better training stability, yielding better results.\n\nBesides, another difference is that we parameterize the joint distribution in a special way, i.e., Eq. (5) of Sec. 4, which allows our approach to have better theoretical properties as mentioned in Prop. 1.\n\n**(2) Why is refinement not helpful?**\n\nThe refinement process further optimizes model parameters using the maximin game. Nevertheless, as aforementioned, solving the maximin game is unstable. Besides, the joint distribution derived by solving the proxy problem is already very close to the optimal solution. Because of these two reasons, refinement might not be very helpful in some cases. In our experiments, only on datasets with a large number of training data (e.g., PPI-10, PPI) does refinement further improve the final results (see Tab. 4).\n\n----------\n\n### Q3: There are some writing issues, i.e., SMN, definition of GNNs, and CRFs\n**Response:** Thank you for the helpful suggestions! We have fixed the writing issues in the revised draft, including spelling out SMN the first time it appears, revising the definitions of GNNs and CRFs in Sec. 3.2 and Sec. 3.3, and fixing the typos (see the light blue text).\n\n----------\n\n### Q4: The generalization abilities of GNNs\n**Response:** We are sorry about the confusion. GNNs only try to estimate the marginal distributions on nodes. In contrast, modeling the joint distribution requires two models to estimate the marginal distributions on nodes and edges respectively. Intuitively, these two models for node marginals and edge marginals can mutually correct each other, resulting in better generalizability. Nevertheless, we are unaware of any theoretical proof of that.\n\n----------\n\n### Q5: About the DBLP Dataset\n**Response:** We are sorry that some details of the DBLP dataset were missing due to space limits. Below is the detailed description of the dataset.\n\nWe build the DBLP dataset from the DBLP citation network. Scientific papers from eight conferences are treated as nodes, and we further split them into three categories for classification according to conference domains. For each paper, we compute the mean GloVe embedding of words in the title and abstract as node features.\n\nWe split the dataset into three disjoint graphs for training/validation/test.The training graph contains papers published before 1999 (with 1999 included). The validation graph contains papers published between 2000 and 2009 (with 2000 and 2009 included). The test graph contains papers published after 2010 (with 2010 included). There exists an undirected edge between two papers if one paper cites the other one. Cross-graph edges (e.g., an edge between a paper in the training set and a paper in the validation set) are removed.\n\nWe have updated the draft accordingly (see the light blue text in the Sec. F.2).\n\n----------\n'}, {'title': 'Response to Reviewer LsFg (1)', 'comment': 'Thanks for the insightful comments!\n\n----------\n\n### Q1: The connection between our method and piecewise training.\n\n**Response:** It is a great point to draw connections to piecewise training, as piecewise training is a useful tool for training CRFs and is well-recognized in the CV domain.\n\n**(1) Major difference between our method and piecewise training.**\n\nBoth our method and piecewise training can be understood as trying to train a local model on each node $s$ and edge $(s,t)$ to predict their labels, i.e., $\\tau_s(y_s)$ and $\\tau_{st}(y_s, y_t)$. In this sense, both methods share similar ideas.\n\nThe major difference lies in how we combine those local pieces on nodes and edges into a joint distribution. Formally, as in Eq. (2) of our draft, the joint distribution of pair-wise CRFs can be formulated as $p(\\mathbf{y}_V | \\mathbf{x}_V) \\propto \\exp (\\sum \\theta(y_s) + \\sum \n \\theta(y_s, y_t)) $, where we need to specify the functions $\\theta(y_s) $ and $\\theta(y_s, y_t) $.\n\n- Piecewise Training: In piecewise training, the functions are essentially specified as $\\theta_s(y_s) = \\log \\tau_s(y_s)$ and $\\theta_{st}(y_s, y_t) = \\log \\tau_{st}(y_s, y_t)$.\n\n- Our method: As in Eq. (5), our method sets the parameters as $\\theta_s(y_s) = \\log \\tau_s(y_s)$ and $\\theta_{st}(y_s, y_t) = \\log \\tau_{st}(y_s, y_t) - \\log \\tau_s(y_s) - \\log \\tau_t(y_t)$.\n\n**(2) Theoretical advantage of our method over piecewise training.**\n\nWe first discuss the theoretical advantage of our method. As explained in Sutton et al. (2009), piecewise training essentially maximizes a lower bound of the likelihood function on training data, but ""If the graph is connected, however, then the bound is not tight, and in practice it is extremely loose."" This implies that the joint distribution learned by piecewise training often cannot well fit training data.\n\nIn contrast, by solving the proxy problem as in our method, the moment-matching conditions are roughly satisfied, meaning that the joint distribution learned by our method can better fit training data.\n\nFor example, on tree-structured graphs, if $\\tau_s(y_s)$ and $\\tau_{st}(y_s, y_t)$ can perfectly fit the local node/edge marginals on training data, then the joint distribution learned by our models can perfectly fit the training data, whereas the joint distribution formed by piecewise training cannot. To better illustrate that, let\'s consider a toy case where a graph only has two linked nodes, and each node has two possible labels 0 and 1. Suppose the ground-truth joint label distribution $p(y_s, y_t)$ is as follows:\n\n|             |  y_t = 0 |  y_t = 1 |\n|:-----------:|:--------:|:--------:|\n| **y_s = 0** |   0.34   |   0.20   |\n| **y_s = 1** |   0.36   |   0.10   |\n\nSuppose also that $\\tau_s(y_s)$, $\\tau_s(y_s)$, and $\\tau_{st}(y_s, y_t)$ can perfectly fit the marginal distributions on nodes and edges, i.e., $\\tau_s(y_s) = p(y_s)$, $\\tau_t(y_t) = p(y_t)$, and $\\tau_{st}(y_s, y_t) = p(y_s, y_t)$.\n\nThen the joint distribution learned by our method with these local components is:\n\n|             |  y_t = 0 |  y_t = 1 |\n|:-----------:|:--------:|:--------:|\n| **y_s = 0** |   0.34   |   0.20   |\n| **y_s = 1** |   0.36   |   0.10   |\n\nThis is the same as the ground-truth joint label distribution, and the label configuration $(y_s = 1, y_t = 0)$ gets the highest probability, which agrees with the ground-truth data distribution.\n\nFor piecewise training, the joint distribution is computed as:\n\n|             |  y_t = 0 |  y_t = 1 |\n|:-----------:|:--------:|:--------:|\n| **y_s = 0** |   0.44   |   0.11   |\n| **y_s = 1** |   0.40   |   0.05   |\n\nWe can see that this distribution is quite different from the ground-truth joint distribution, and the most likely label configuration is $(y_s = 0, y_t = 0)$, which is inconsistent with the ground-truth data. Therefore, we see that piecewise training does not fit training data well, which is undesirable.\n\n**(3) Empirical comparison of our method and piecewise training.**\n\nWe also compare against piecewise training empirically on four datasets with the same GNN backbone (GAT). The results over 10 different runs are presented as follows:\n\n|                    |     Cora*    |   Citeseer*  |    Pubmed*   |     DBLP     |\n|:------------------:|:------------:|:------------:|:------------:|:------------:|\n| Piecewise Training | 57.08 | 48.04 | 51.95 | 82.93 |\n|    SMN   | 58.78 | 49.02 | 52.91 | 84.84 |\n\nWe can see that our method achieves consistently better results.\n'}, {'title': 'Response to Reviewer jqoo', 'comment': 'Thank you for the insightful suggestions! Also, thanks for pointing out all these related papers on modeling joint label dependency. We have discussed these papers in the revised draft (see the blue text in related work).\n\n----------\n**Q1: It is unclear how to extend the model to fit continuous node labels.**\n\n**Response:** It is a very good point to extend the model to fit continuous node labels. As our model is designed for discrete random variables, one straightforward solution is to discretize the continuous labels into discrete values. In the future, we will explore using more advanced methods to deal with continuous node labels.\n\n----------\n**Q2: Insufficient baselines for comparison.**\n\n**Response:** Comparing against more methods for modeling label dependency is another good point. Among the suggested papers, CopulaGNN (Ma et al., 2021) is mainly designed for continuous and count regression tasks, and it is hard to apply the model to our setting. For LCM (Wang et al., 2021), the codes are not openly available. Therefore, we only ran experiments to compare against G3NN. More specifically, we use the same GAT as backbone for both G3NN and SMN, and we run both methods with 5 seeds. The results are presented as follows:\n\n|          |     Cora*    |   Citeseer*  |    Pubmed*   |     DBLP     |\n|:--------:|:------------:|:------------:|:------------:|:------------:|\n| G3NN-LSM | 58.60 | 51.12 | 52.44 | 79.64 |\n| G3NN-SBM | 58.28 | 50.96 | 52.20 | 78.06 |\n|    SMN   | 59.66 | 50.84 | 53.78 | 84.67 |\n\nWe can see that SMN outperforms G3NN in all cases except Citeseer, which has the fewest number of average nodes (4.0) and edges (4.3) per graph (see Table 8 in appendix), making it easier to be modeled by the graph generative model of G3NN.\n\n----------'}, {'title': 'Response to Reviewer RSMV', 'comment': 'Thanks for your helpful comments!\n\n**Q: Further explain Fig3.**\n\n**Response:** In Fig. 3, we show three cases to illustrate that SMN can predict node labels more precisely than GNN. Specifically, each row corresponds to a case, where different colors represent different node labels. For each case, we show the prediction of GAT, edge GNN, and SMN-GAT. In all three cases, SMN-GAT correctly predicts all the node labels, so the ground-truth node labels can also be found in the last column.\n\nIf we check the prediction of GAT (i.e., first column), we see that GAT makes inconsistent predictions in that some linked nodes are predicted to have different labels. For the edge GNN, it also makes incorrect predictions in the last case. By combining GAT and edge GNN together, SMN-GAT makes correct predictions in all the three cases.\n'}, {'title': 'Response to Reviewer DQ8r', 'comment': '**Q: Additional comparison to other methods, e.g., SSVMs.**\n\n**Response:** Thanks for the helpful suggestions! As you pointed out, we only compared against GNN and CRF. Following your suggestions, we added additional comparison against SSVM, and the results are presented as follows: \n\n|      | Cora* | Citeseer* | Pubmed* | DBLP |\n|:----:|:-----:|:---------:|:-------:|:----:|\n| SSVM |  42.7 |    33.2   |   43.2  | 51.5 |\n|  SMN |  58.8 |    49.0   |   52.9  | 84.8 |\n\nWe can see that our approach SMN achieves better results than SSVM, as SMN has higher model capacities.\n'}, {'summary_of_the_paper': 'Authors study the problem of node labeling in the inductive case, i.e., at test time the goal is to label all the nodes of a given graph.\nFor that problem, several variants of GNNs and CRFs have been proposed in the past, and the authors propose a Structural Markov Network that uses GNNs to model the potential functions of a CRF with the subtle difference that a proxy optimization problem is solved to make learning more efficient. Experiments are provided to demonstrate the applicability of their method.\n', 'main_review': 'Strengths\n* Paper is well written\n* The proposed method is a novel contribution that combines ideas from CRFs and GNNs.\n\nWeaknesses\n* I consider the experiment section to be a minor weakness. I understand that the method sits in between CRFs and GNNs, but it would be great to compare against methods beyond those from CRFs and GNNs, e.g., SSVMs and others.\n* Another minor weakness. The technical contribution is limited as it builds on old ideas from graphical models. My understanding is that the key to efficiency relies on solving the proxy problem, which, as the authors stated, is an idea that dates back to at least the early 2000s in the context of graphical models.\n ', 'summary_of_the_review': 'In my opinion, the paper contains good contributions that are worth publishing at ICLR. While I think the experiments section could benefit from additional comparisons, the current state of the empirical evaluation is reasonable. A score of 7 would reflect better my evaluation of this work as I find the technical contributions to be okay but somewhat limited.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a CRF for classifying nodes in graphs where the CRF has a potential for each node and edge in the graph. GNNs are used for computing the potentials, one GNN for node potentials and one GNN for edge potentials. For general graphs, computing the partition function is intractable, so approximations are used during both learning and inference.  Learning draws from prior work and combines learning pseudomarginals of nodes and edges with GNNs and optionally some steps of ""refinement"" by optimizing a maximin game equivalent to likelihood maximization. Inference uses sum-product or max-product loopy BP (they perform similarly, though max-product is slightly better). The procedure is called a ""Structured Markov Network"" (SMN). Experimental results on several graph node classification tasks show that SMNs outperform several baselines, including both CRFs with standard training (using approximate inference during training) and CRFs trained with pseudolikelihood. \n', 'main_review': 'Update following revision / author comments:\n\nThanks for clarifying and providing details on how the method compares to piecewise training. Having all of those details in the final paper would make it much stronger, and so I have raised my score. \n\nFor what it\'s worth, I\'d suggest trying to describe the method in as general terms as possible so that people working on other structured prediction problems are more likely to learn about it and use it. The method is general and appears very promising, but currently the paper is very much written for graph problems, so I fear that researchers outside of the graph community may overlook it. Choosing a more specific name for the method could also help. The name ""Structured Markov Network"" is a bit too broad, as it just sounds like a Markov Network, which is a synonym for MRF. Maybe working ""proxy"" into the name you choose would be helpful.\n\nOriginal review follows:\n\nStrengths:\n\nThe methods are well-chosen for the task. The method is simple enough that other researchers may use it.\n\nThe empirical results are solid and interesting. \n\nThe paper is well-written. \n\nWeaknesses:\n\nThe original piecewise training paper (Sutton & McCallum, 2009) is cited, but is not discussed in any detail. No connection is made between the proxy problem and piecewise training. I think a connection should be made and discussed, especially because the experiments do not involve actually solving the proxy problem but rather omit the marginal consistency constraint (""The last consistency constraint...can be ignored during optimization""... ""We also tried some constrained\noptimization methods to handle the consistency constraint, but they yield no improvement""). When refinement is not used (it is omitted in the main experiments since it doesn\'t consistently help) and when KL divergence is used for the divergence measure (as it is in the experiments), the actual training objective becomes even more similar to piecewise training. Piecewise training is fairly well-known in other communities like the vision community, e.g., Lin et al. (2016), so I think it\'s really important for this paper to draw a connection to piecewise training. I would suggest including piecewise training as a baseline to compare to, but I think that the SMN (without the marginal consistency constraint, without refinement, and when using KL) actually corresponds to a natural way to apply piecewise training to this problem. \n\nAfter reading the paper, I was confused by one part of the results: Why would SMN outperform CRF? That may suggest that the approximations made during learning are beneficial, which deserves follow-up investigation. A related question is: why is refinement not helpful? The CRF-G* settings correspond to using the same models as the corresponding SMN-G* settings, but the former seek to directly solve the maximin game using loopy BP for inference during learning (I believe), rather than use the proxy problem. The SMN results are consistently better than the corresponding CRF ones. Why is the proxy problem superior? Perhaps the CRF objective is not as good for learning as the proxy problem? The proxy problem can be viewed as using ""local supervision"" on specific nodes and edges, which may be more learnable from supervised datasets than the traditional CRF objective which is log loss on labelings of entire graphs. Or maybe it\'s due to training stability: An SMN that solely solves the maximin game (is this the same as the CRF-G* models?) is described as being unstable in Sec 5.5, #5. Can you provide more details about that? Is the instability due to the use of loopy BP as the inference algorithm? How about if you pretrain the GNNs for the potentials with maximum likelihood?\n\n\n\nSome more specific suggestions are below:\n\nSpell out SMN (""Structured Markov Network"") the first time it appears in Sec 1. I would actually also suggest changing the terminology to something more specific. ""Structured Markov Network"" is a pretty generic term that would evoke several kinds of existing graphical models in people\'s minds. E.g., some people use the term ""Markov Network"" to refer to undirected graphical models in general, and so the addition of the term ""structured"" does not really add anything since graphical models are already ""structured"". \n\nThe definition of GNNs in 3.2 seems unnecessarily limited. A graph neural network does not have to produce distributions over anything -- it could simply represent a graph via autoencoder training with an L2 loss, for example. Please see surveys on GNNs, such as Zhou et al. (2021), which provide a richer characterization of GNNs. If you wish to define GNN in a more constrained way for purposes of this paper, then please add ""In the context of this paper, we define a GNN to be"".\n\nSec. 3.2: ""However, GNNs approximate only the marginal label distributions of nodes on training graphs, which may generalize badly and result in poor approximation of node marginal label distributions on test graphs."" -- Why might they generalize badly? Would an estimate of the full label distribution be expected to generalize better? Is the paper implying here that noisy estimates of the marginals would generalize better than a noisy estimate of the joint?\n\nSec. 3.3: ""Conditional random fields (CRFs) build graphical models for node classification."" CRFs are much broader than node classification. See my comment about GNNs above. \n\nSec. 3.3: ""intractable partition function"" -- The intractability depends on the graph, right? E.g., consider chains, trees, acyclic graphs..\n\nSec. 4.1: ""compute the a representation""\n\nMore details are needed about the DBLP dataset. What exactly is the structure of the ""citation graphs"" mentioned? Also, it appears that there is only a single graph in train, validation, and test -- is that correct? The test set contains papers from ""after 2010"" -- does that include 2010 or is it only from 2011 onward? Are the train/val/test graphs disjoint? The papers in the test set will mostly be citing papers from the training and validation sets, or are the latter removed from the test graph to avoid overlapping nodes among splits?\n\nReferences:\n\nGuosheng Lin, Chunhua Shen, Anton van dan Hengel, Ian Reid. Efficient piecewise training of deep structured models for semantic segmentation. CVPR 2016.\n\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun. Graph Neural Networks: A Review of Methods and Applications. 2021.\n\n', 'summary_of_the_review': 'While the paper is well-written and the results show consistent improvements, the paper is lacking in terms of any connection made to piecewise training (which the best version of the SMN essentially boils down to, as far as I can tell) and an analysis of why optimizing the proxy problem is superior to the original learning problem. These are potentially fixable issues. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""The paper targets the task of graph node classification in the inductive setting, taking an input node and edge feature representations, and inferring a categorical label for each node. It focuses on the case where it is not sufficient to classify a node independently of its neighbours, but where information stemming from predictions for neighbouring nodes needs to be taken into account. \nIt improves over the current approaches by offering a performant and efficient method to combine advantages of CRFs (joint inference) with the representational power afforded by GNNs, while overcoming the computational challenges. In particular, this paper improves on the closest attempts at combining CRF and GNN, Ma+ 2018 and Qu+ 2019, which use pseudolikelihood; Qu+ 2019 is used as baseline in experiments (“GMNN”), while pseudoloikelihood as training objective is investigated in sec5.5.5.\n\n--- update after rebuttals\nI have read other reviews and the authors' replies, as well as extra experiments. I am maintaining my score."", 'main_review': 'The technical approach exploits, in an innovative way, an insight from graphical models on the relation between pseudomarginals and the joint distribution stated in the main reference (Wainwright and Jordan 2009), to devise a formulation of a a surrogate training objective (the proxy problem) which turns out to be relatively easy to solve, leading to high predictive performance. The problem under consideration is important, and current solutions have the limitations pointed out in the paper (limited expressive capacity of CRF vs only marginal node classification for GNN models).\n\nThe paper reads very well, and hardly has any errors or inconsistencies. Information is provided at the right point, is complete and accurate. The split between main paper and supplementary material is good. The narrative and exposition flow well. \n\nThe model and algorithm descriptions are excellent. \n\nBaselines are strong, relevant, discussed well and evaluated fairly. The experimental methodology is appropriate, well implemented, described well. A large number of analytic experiments complete the main findings. The two evaluation metrics (node and graph-level accuracy) are adequate and it is an advantage that the proposed method can optimize for either at inference time. Experimental reporting is very good, with error bars. Experimental setups are presented accurately and consistently, at an adequate level of detail. The datasets are standard and easy to access, which contributes to reproducibility.\n(NB fig3 is missing a complete caption; colour coding is not explained; as a result sec5.5.7 doesn’t prove its point)', 'summary_of_the_review': 'If the scoring existed, I would give this paper a 9/10. Having to choose between 8 and 10, I have decided to go for 10, as I consider the paper to be much better than just ""good"".\nThe paper is very good under all aspects. Its contribution is clear, and the problem under consideration is real. One might argue that the innovation is slim because it consists of a single technical contribution; for this reason this might not be a game-changing paper, but it certainly improves on the state of the art by solving a tricky problem. The paper also has the merit of establishing a bridge between neural and graphical model techniques, thus encouraging further work which I expect to be fruitful.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper aims to break the label independence assumption in existing GNNs. To this end, a combination of GNNs and CRF is proposed, named SMN. SMN improves GNNs by modeling the joint distribution among discrete node labels; SMN is a promising alternative to CRF as it provides efficient learning and inference procedures. Empirically, SMN achieves better node-level and graph-level prediction accuracies than several existing models, with minor additional runtime overhead.', 'main_review': 'Strengths\n- The motivation of model design is clear and reasonable. Modeling label dependencies is a natural way to improve GNNs, which is also supported by experiments.\n- The model inherits advantages of CRF, such as describing the dependency of node labels and providing probabilistic interpretation. In the meantime, the model supports efficient learning and inference.\n- The model is technically sound.\n- Experiments cover several benchmarks. The model is tested on multiple datasets and shows very promising results.\n- The paper is well-written and easy to follow.\n\nWeaknesses\n- The model is proposed for modeling discrete node labels on a graph. It is unclear how to extend the model to fit continuous node labels.\n- Insufficient baselines for comparison. For neural models that can describe node label dependencies, the paper compares with GMNN only. However, there are various recent works that are not compared, e.g., $G^3NN$ (Ma et al., 2019), CopulaGNN (Ma et al., 2021), LCM (Wang et al., 2021) to cite a few.', 'summary_of_the_review': 'I think the model is reasonable both theoretically and empirically (though some recent baselines are not included for comparison). Overall, the merits outweigh the flaws, and I believe the paper is a good addition to the existing literature.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Neural Structured Prediction for Inductive Node Classification', 'authorids': ['~Meng_Qu2', '~Huiyu_Cai1', '~Jian_Tang1'], 'authors': ['Meng Qu', 'Huiyu Cai', 'Jian Tang'], 'keywords': [], 'abstract': 'This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'qu|neural_structured_prediction_for_inductive_node_classification', 'pdf': '/pdf/df1b628202430dff01a7eeed5b5e5a2e703d1bad.pdf', 'supplementary_material': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/neural-structured-prediction-for-inductive/code)', '_bibtex': '@inproceedings{\nqu2022neural,\ntitle={Neural Structured Prediction for Inductive Node Classification},\nauthor={Meng Qu and Huiyu Cai and Jian Tang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=YWNAX0caEjI}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Nicholas Carlini', 'Andreas Terzis']",ICLR,Poisoning and Backdooring Contrastive Learning,https://iclr.cc/virtual/2022/oral/6317,2022," Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=iC4UHbQ01Mp,https://openreview.net/forum?id=iC4UHbQ01Mp,iC4UHbQ01Mp,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper studies attacks on the self-supervised training pipeline of multi-modal models, e.g., CLIP and related models.  The reviewers agree that the poisoning results are impressive in that they achieve good poisoning success with a fairly small number of samples.  The threat model is fairly specific to one (high profile) type of self-supervised training, but the concepts presented are likely portable to the study of other related training pipelines.'}, {'title': 'Thank you for the feedback', 'comment': 'The authors have done a nice job to address my concerns. I am raising my score to an 8.'}, {'title': 'Thanks for the feedback', 'comment': 'I agree with the authors about the contributions (in Novelty). After reconsidering, I raise the rating from 5 to 8.'}, {'title': 'Author Response', 'comment': 'We thank the reviewer for their positive feedback. If there are any future questions we’d be happy to answer them.'}, {'title': 'Author Response', 'comment': 'We thank the reviewer for the helpful suggestions:\n\n> Conceptual captions generalization\n\nAt the reviewer’s suggestion, we have confirmed that our attack succeeds on the YFCC dataset (15 million images). Specifically, when poisoning between 2 and 512 images for this one model trained, the attack succeeds at [8,16,64,128,512] poisons and fails to poison for this run at [2, 4, 32, 256] poisons. It is difficult to meaningfully extrapolate success rates here, but the fact that the attack succeeded at all for 8/15,000,000 = 0.00005% of the dataset is surprising.\n\nThe YFCC dataset is significantly larger and so takes comparatively longer to train CLIP---600 GPU hours per training run. Therefore at this time we have exactly n=1 model trained to provide evidence the attack works in this setting, and will try to run a few more models for the final paper.\n\n> Defending against our attack\n\nWe agree; defenses that would prevent our attack would be an important contribution. Here, though, we argue that this is a problem for future work. The purpose of our paper is to show that it is important to consider the security of a model’s training dataset—especially when that dataset is scraped from the internet. Developing techniques to actually prevent this is a new and separate question.\n'}, {'title': 'Author Response', 'comment': 'We thank the reviewer for their questions:\n\n> Novelty\n\nWe agree that the algorithm we use to poison machine learnings models is not sophisticated. However we emphasize the main contribution of our paper is not the technique method used to poison the models, but the experimental evaluation which shows how effective these attacks are on the new domain of multimodal contrastive classifiers.\n\n> Dips in Figure 2\n\nThe dips in Figure 2 are not statistically significant. The margins of error on this plot are sufficiently large that we can not say with confidence that the accuracy drops. In all cases the shaded region corresponds to one standard deviation (68% confidence). We will try to increase the number of training runs to reduce the variance here to make the figure more clear for future readers.\n\n> Location of trigger\n\nWe do have some intuition here. The purpose of a backdoor attack is to construct a patch that generalizes to new target images. By placing the patch randomly throughout the image, we introduce a form of “data augmentation” that makes it more likely for the attack to generalize. However we haven’t actually been able to experimentally confirm this.\n\n> Why was zero-shot performance not impacted?\n\nConsistent with prior work we have found that backdoor attacks do not reduce the model’s accuracy. The reason given by prior work (and that we agree with) is as the reviewer suggests: the model has sufficient capacity to fit the entire training dataset and also memorize a few outliers that we poison.\n'}, {'title': 'Author Response', 'comment': 'We thank the reviewer for raising these points.\n\n> *Multimodal* contrastive learning\n\nWe agree with the reviewer here; we tried to clarify throughout the abstract (“*Multimodal* contrastive learning methods like CLIP train on noisy and uncurated training datasets.”) introduction that “We show that this adversary can mount powerful targeted poisoning (Biggio et al., 2012) and backdoor attacks (Gu et al., 2017; Chen et al., 2017) against *multimodal* contrastive models.” We’ll fix the remaining locations that we missed in the paper.\n\n> Novelty\n\nOur primary objective in this paper is to measure the effectiveness of poisoning attacks on multimodal contrastively trained models. We agree that our attacks make use of existing techniques, however we believe that this is a strength to our results: it shows the simplicity of these attacks is enough to be potent in practice. The novelty in our paper is therefore not in the method itself, but in the evaluation where we find multimodal contrastive classifiers are exceptionally vulnerable to attack.\n\n> Figure 5 (left)\n\nWhen generating this figure, we did experiment with training for more/fewer steps, however on the whole we always observed a similar effect where the backdoor success rate remains fairly consistent throughout the dataset sizes.\n\n> Writing comments\n\nThanks for noticing these -- we have updated the paper to make the writing more clear here.'}, {'summary_of_the_paper': 'This paper explores data security in multimodal contrastive learning. In particular, it designs an image-text pair generation to poison the dataset, driving the model to misclassify a particular test input or a group of images with a small patch. While the generation method is quite simple, the attack success rate is impressive (only 3 out of 3 million images to conduct target poisoning attacks). This paper reminds us that it is potentially dangerous to train models on noisy and uncurated Internet scrapes, which is applied in some SOTA algorithms.', 'main_review': '**Strength**\n\n(1) Even though some progress is made in dataset security research, we still need to demonstrate the urgency and importance of addressing such data security issues. This paper did an excellent job, especially achieving a high attack success rate in a SOTA algorithm which requires lots of computational resources.\n\n(2) The proposed method is simple but effective, and the experimental results are convincing.\n\n**Weakness**\n\n**Major**\n\n(1) The title ""Poisoning and Backdooring Contrastive Learning"" might overclaim the contributions of this paper. Multimodal contrastive learning trains the vision model from natural language supervision as indicated in (Radford et al., 2021), which is similar to weakly supervised learning (Keeping in mind that we only poison the vision model). Meanwhile, the difficulties of the poisoning attack heavily rely on the type of supervision provided (the authors also discuss part of this in Section 3.2). Contrastive learning can be applied to supervised (Chechik et al., 2010), weakly supervised, and unsupervised data (Oord et al., 2018). As a result, the title fails to exactly convey the difficulties of the poisoning and the contributions of this study. Actually, I was astonished by the title since I even thought the authors have successfully poisoned (totally) unsupervised learning after reading the title.\n\nIn conclusion, I would encourage to use ""multimodal contrastive learning"" instead of ""contrastive learning"" in the title. It would also be better to modify some descriptions in the paper, such as\n\n""AS we are the first to study poisoning and backdoor attacks on **multimodal** contrastive learning methods"" in Section 2.3.\n\n(2) I\'m concerned about this paper\'s lack of novelty. This study only proposes a simple poisoning and backdooring approach which is not technically novel, failing to match the ICLR\'s novelty standards.\n\n(3) I am curious about the results in Figure 5 (left). Since we always use 30 epochs and a batch size 1024, it would take fewer iterations to train the model on a smaller dataset. I wonder whether the ASR plateau is due to insufficient convergence. If we have more iterations, will the model converge better and learn better representation, letting the backdoor Z-score increase as well?\n\n**Minor**\n\n(4) The writting should be polished further. Some discussions in experimental results are verbose from Section 5.1.1 to Section 5.1.3.\n\n(5) typo:\n\n(5-a) ""**minimizes** an inner product between the embeddings while **maximizing** ..."" in Section 2.2 -> ""**maximize** an inner product between the embeddings while **minimizing** ..."".  This is because the larger the inner product becomes, the more similar the image embedding and text embedding are.\n\n(5-b) ""however because the majority of labels are correct"" in Section 3.1. Is there any part missing? \n\n(5-c) ""... one of the three cases above ..."" in Adversary Objective of Section 2.3: three cases -> two cases (feature extractor, zero-shot classifier) or four cases (feature extractor, zero-shot classifier) x (poisoning attack, backdoor attack)?', 'summary_of_the_review': ""This paper successfully attacks SOTA multimodal contrastive learning, which reveals the security threat from unfiltered data. Personally, I appreciate the authors' effort to demonstrate the urgency and importance of addressing data security issues.  However, I am afraid that this paper does not meet the ICLR's novelty requirements. \n\nIf the authors could convince me of the issue of novelty, I would reconsider the rating."", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper describes poisoning and backdooring attacks on CLIP, a recent method to (pre)-train multimodal networks with a contrastive objective. The setting here is different from a classical supervised BadNet-like setup in that learned embeddings are not necessarily going to be directly mapped to a class of choice. Authors evaluate both zero-shop learning setup and the linear probes to find that both of them can be successfully backdoored and poisoned with much fewer requirements. Authors extensively evaluate their attack and pinpoint what hyperparameters make it more effective. ', 'main_review': 'Strengths:\n+ Poisoning of an interesting setting\n+ Much lower poisoning ratio requirement\n+ Evaluation over an extremely demanding task\n\nWeakness:\n+ Minor methodological contribution over current literature\n\nThe paper is very well written and presents a convincing argument that poisoning and backdooring attacks are very effective against CLIP. Evaluation of how size and placement of the trigger changes performance of the attack is particularly interesting, especially the dip observed for larger triggers. Finally, attack performance as a function of model parameters in Figure 5 demonstrates that increase in number of parameters can help learn more generalisable features (point at 5), but from that point onwards, it only leads to increased vulnerability. \n\nI only have a small number of clarification questions: \n\n* Why do you think there are dips in performance in Figure 2 with increasing number of samples?\n* A finding that location of the trigger dictates its performance depending on a number of samples is interesting, do you have any intuition as to why this is happening? Do you believe it’s a function of the trigger used?\n* Do you think zero-shot ImageNet performance was not affected because the base model had a large number of parameters? \n\nTypos:\n+ “However because the majority of labels are correct.” unfinished sentence In constructing the caption set.\n\n', 'summary_of_the_review': 'Paper shows an extensive evaluation of both poisoning and backdoor attacks on CLIP with a very demanding task. Authors demonstrate vulnerability with just a few samples. I suggest an accept. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper illustrates how easily contrastive learning, particularly on multi-modal data, can be mislead by a small amount of poisoned or backdoor data instances. ', 'main_review': 'Contrastive learning is a widely used technique for self-supervised learning and it is common to apply it to data that is scraped from the internet without careful review by a human.  The authors show that this use of ""uncurated"" data makes contrastive learning vulnerable to poisoning and backdoor attacks and they show that poisoning even a small number of instances can be very effective.\n\nTo my knowledge, this is the first paper that investigates the issue of a poisoning / backdoor attack for contrastive learning. I was surprised at how little data needs to be poisoned for this attack. I agree with the authors that their findings are especially alarming given the fact that contrastive learning is often used on uncurated data. The authors do a thorough job of including experiments that show how varying different aspects of the data or attack influence the success rate. I found the patch size experiments to be an interesting finding. Overall, the paper is well written and the experimental process is clearly described.\n\nThe main weakness of this paper is how well the findings generalize beyond the Conceptual Captions dataset as the authors only present results on a single (but very large) dataset. Are the threats to contrastive learning simply artifacts of that dataset or are they also present in other multi-modal datasets used for contrastive learning? Do these vulnerabilities also generalize to other types of datasets (besides multi-modal ones) used for contrastive learning?\n\nIn addition, while it is a contribution to identify these vulnerabilities, the paper would be much stronger if the authors could also present a solution that addresses these vulnerabilities. \n\nMinor issues:\n- Section 3.1: the last sentence in the second to last pargraph has an incomplete sentence.\n- Caption on Figure 4: ""orage"" should be ""orange""\n', 'summary_of_the_review': 'The paper is one of the first to illustrate the vulnerability of contrastive learning to poisoning / backdoor attacks. However, the experiments only involve a single dataset and it is unclear how well the findings can generalize to other datasets.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['Yes, Potentially harmful insights, methodologies and applications'], 'details_of_ethics_concerns': 'The authors show how to exploit contrastive learning via poisoning and backdoor attacks. It is useful to have this vulnerability explained and the authors point out in their Ethics Section that their goal is to highlight this weakness and spur the research community on to find solutions.', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper studies the performance of contrastive learning under both poisoning and backdooring attacks. Since contrastive learning enables using cheap unlabelled data to obtain an embedding function, the detailed experiments conducted in this paper call a question that whether training on those cheap data scraped from the internet is desirable.', 'main_review': 'This paper focuses on the multi-modal classifications, especially on the recent multi-modal classifier CLIP. CLIP is trained on the extreme amount of internet data with text captions and has shown great generalization to other datasets. The proposed poisoning and backdooring attacks are able to inject a small amount of perturbed noise data into the training dataset and cause desired adversarial behavior. The fact that only injecting a few noise data can misbehave the underlying embedding function is interesting.\n\nThe paper is easy to follow and provides a very detailed study of this behavior. I have no further questions since the detailed experiments well support the effectiveness of both proposed attacks. This work can lead to new defense algorithms that focus on detecting those poisoned/backdoored data scraped online.', 'summary_of_the_review': 'The paper is well-written and conducts very detailed experiments, I recommend acceptance.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Poisoning and Backdooring Contrastive Learning', 'authorids': ['~Nicholas_Carlini1', '~Andreas_Terzis1'], 'authors': ['Nicholas Carlini', 'Andreas Terzis'], 'keywords': ['Contrastive Learning', 'Poisoning attack', 'Backdoor attack', 'CLIP'], 'abstract': 'Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input  with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'carlini|poisoning_and_backdooring_contrastive_learning', 'pdf': '/pdf/abd77f0543a72cd26da355efc5680de233f120af.pdf', 'one-sentence_summary': 'We argue poisoning and backdooring attacks are a serious threat to multimodal contrastive classifiers, because they are explicitly designed to be trained on uncurated datasets from the Internet.', 'data': '', '_bibtex': '@inproceedings{\ncarlini2022poisoning,\ntitle={Poisoning and Backdooring Contrastive Learning},\nauthor={Nicholas Carlini and Andreas Terzis},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iC4UHbQ01Mp}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Huaxiu Yao', 'Linjun Zhang', 'Chelsea Finn']",ICLR,Meta-Learning with Fewer Tasks through Task Interpolation,https://iclr.cc/virtual/2022/oral/7141,2022," Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=ajXWF7bVR8d,https://openreview.net/forum?id=ajXWF7bVR8d,ajXWF7bVR8d,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Current meta-learning algorithms suffer from the requirement of a large number of tasks in the meta-training phase, which may not be accessible in real-world environment. This paper addresses this bottleneck, introducing a cross-task interpolation in addition to the existing intra-task interpolation. The main idea is very simple, which can be viewed as an incremental adding-up to existing augmentation methods. However, the method is well supported by nice theoretical results which highlight the relation between task interpolation and the Rademacher complexity. In fact, this is not a trivial extension of existing work.  Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. All reviewers agree to champion this paper. Congratulations on a nice work.'}, {'title': 'Summary of Paper Revision', 'comment': 'We appreciate all the reviewers for their valuable and constructive comments. According to these comments, we have improved the paper and summarized the main changes as follows:\n\n1. Included the comparison between MLTI and other strategies on full-size miniImagenet and DermNet (Appendix E.2)\n2. Added the results of Huffpost -- a few-shot text classification dataset, and tieredImagenet (Appendix E.8)\n3. Provided base model analysis and interpolation layer analysis (Appendix E.5)\n4. Revised the dataset descriptions and hyperparameter settings to include more details (Appendix D.1 and E.1)\n5. Added more discussion on previous works in Appendix C, and more explanation of the theoretical analysis in Section 4.\n6. Since more results and discussions are included in the Appendix, some Appendix indexes have been changed accordingly.\n'}, {'title': 'Response to Reviewer id95', 'comment': 'We really appreciate your constructive suggestions to improve the paper. Below, we address your concerns point by point. Please kindly let us know if your concerns are addressed and whether you have any further concerns. \n\n\n**Q1**: Theoretical Analysis\n\n**A1**: Thanks for pointing this out. We have revised the theoretical part by adding more explanations. Specifically, we’ve now stated more explicitly that the loss function induced by MLTI is approximately the standard loss function plus a positive term. We explained that such a positive term is a quadratic function on the parameter phi_i’s, and therefore can be interpreted as a regularization term. Furthermore, we added more explanations around Theorem 1, and indicated that the purpose of Theorem is to show how this regularization term improves the generalization error bound. Please kindly let us know if you have any additional questions.\n\n\n---\n\n**Q2**: Sample selection in NLS setting\n\n**A2**: Thank you for your comment. In our paper, for each interpolated task, only k pairings are used as the new support set, and thus it is still k-shot learning. To be more specific, assume we have two sets (S1 and S2) to be interpolated. We first shuffle samples in S2 and then interpolate the shuffled S2 with S1.\n\nWe conduct the experiments by allowing all-pairwise combinations and report the results of miniImagenet-S and Dermnet-S 1-shot setting in Table R-8.\n\n**Table R-8**: Comparison between MLTI and MLTI with all-pairwise combinations\n\n|                |    miniImagenet-S   |      Dermnet-S      |\n|----------------|:-------------------:|:-------------------:|\n| MLTI           | 41.58 $\\pm$ 0.72% | 48.03 $\\pm$ 0.79% |\n| MLTI-all pairs | 41.72 $\\pm$ 0.75% | 48.07 $\\pm$ 0.82% |\n\n\nFrom the results, we find that using more pairs does not provide much benefit to the predictive accuracy. One possible reason for this is that the current combinations have already made use of most of the essential information in the training data for this method. A similar phenomenon has also been observed in other data augmentation techniques, e.g., using bootstrap can hypothetically generate infinitely many new samples. But in practice, people find that generating the same size as the original training data is sufficient. Similarly, the vanilla mixup, although one can again generate infinitely many new samples, people commonly only generate the same amount of new samples as the training data and find it performs well in practice. \n\n---\n\n**Q3**: subscript ""cr"" in eq5\n\n**A3**: cr represents “cross”. We have revised the paper to clarify this.\n\n---\n\n**Q4**: Point out the difference between MetaMix in eq5\n\n**A4**: Thanks for the suggestion, we have revised our descriptions around eq5 and explicitly point out the differences.\n'}, {'title': 'Response to Reviewer maUk (2/2) ', 'comment': '**Q3**: Results on full-size image data\n\n**A3**: The original submission already shows the performance of MLTI under the full-size miniImagenet and DermNet in Figure 2, where miniImagenet with 64 training classes and DermNet with 150 training classes are full-size datasets. All results are reported with the same meta-testing tasks. We further provide the comparison between MLTI and other baseline strategies in the following Table R-3 and Appendix E.2 (Table 8), where MLTI also shows its promise even under the full-size setting.\n\n**Table R-3**: Results on full-size miniImagenet and DermNet\n\n | Backbone | Strategies   | miniImagenet-full |                   | DermNet-full      |                   |\n|----------|--------------|:-----------------:|:-----------------:|:-----------------:|:-----------------:|\n|          |              |       1-shot      |       5-shot      | 1-shot            | 5-shot            |\n| MAML     | Vanilla      | 46.90 $\\pm$ 0.79% | 63.02 $\\pm$ 0.68% | 49.58 $\\pm$ 0.83% | 69.15 $\\pm$ 0.69% |\n|          | Meta-Reg     | 47.02 $\\pm$ 0.77% | 63.19 $\\pm$ 0.69% | 50.10 $\\pm$ 0.86% | 69.73 $\\pm$ 0.70% |\n|          | TAML         | 46.40 $\\pm$ 0.82% | 63.26 $\\pm$ 0.68% | 50.26 $\\pm$ 0.85% | 69.40 $\\pm$ 0.75% |\n|          | Meta-Dropout | 47.47 $\\pm$ 0.81% | 64.11 $\\pm$ 0.71% | 51.10 $\\pm$ 0.84% | 69.08 $\\pm$ 0.69% |\n|          | MetaMix      | 47.81 $\\pm$ 0.78% | 64.22 $\\pm$ 0.68% | 51.83 $\\pm$ 0.83% | 71.57 $\\pm$ 0.67% |\n|          | Meta-Maxup   | 47.68 $\\pm$ 0.79% | 63.51 $\\pm$ 0.75% | 51.95 $\\pm$ 0.88% | 70.84 $\\pm$ 0.68% |\n|          | **MLTI (ours)**         | **48.62 $\\pm$ 0.76%** | **64.65 $\\pm$ 0.70%** | **52.32 $\\pm$ 0.88%** | **71.77 $\\pm$ 0.67%** |\n| ProtoNet | Vanilla      | 47.05 $\\pm$ 0.79% | 64.03 $\\pm$ 0.68% | 49.91 $\\pm$ 0.79% | 67.45 $\\pm$ 0.70% |\n|          | MetaMix      | 47.21 $\\pm$ 0.76% | 64.38 $\\pm$ 0.67% | 51.50 $\\pm$ 0.76% | 69.55 $\\pm$ 0.68% |\n|          | Meta-Maxup   | 47.33 $\\pm$ 0.79% | 64.43 $\\pm$ 0.69% | 51.18 $\\pm$ 0.83% | 69.07 $\\pm$ 0.72% |\n|          | **MLTI (ours)**         | **48.11 $\\pm$ 0.81%** | **65.22 $\\pm$ 0.70%** | **52.91 $\\pm$ 0.81%** | **71.30 $\\pm$ 0.69%** |\n\n---\n\n**Reference**\n\n[Ni et al. 2021] Ni, Renkun, Micah Goldblum, Amr Sharaf, Kezhi Kong, and Tom Goldstein. ""Data augmentation for meta-learning."" ICML 2021.\n'}, {'title': 'Response to Reviewer maUk (1/2)', 'comment': 'Thank you for your review and for acknowledging our empirical studies. We respond to your concerns below. We would really appreciate it if you could let us know whether your concerns are addressed by the response.\n\n**Q1**: Difference between MLTI and Meta-Maxup [Ni et al. 2021]\n\n**A1**: Below, we elaborate on the differences between individual task interpolation like Meta-Maxup [Ni et al. 2021] and MLTI. We have revised the paper to include these clarifications in Appendix C. The reasons why MLTI leads to more dense task distribution could be summarized under both label-sharing and non-label-sharing settings:\n\n- Under the label-sharing setting, MLTI densifies the task distribution by enabling cross-task interpolation. For example, in Pose prediction, we not only interpolate samples within each object, but cross-task interpolation significantly increases the number of tasks. Assume we have two objects (O1 and O2), individual task interpolation approaches (e.g., Meta-Maxup) only generate more samples in O1 or O2, where only one object information is covered. However, MLTI further allows generating tasks with both O1 and O2 information by interpolating data samples from O1 and O2.\n\n- Under the non-label-sharing setting, MLTI also leads to more dense task distribution. For example, in 2-way classification with 3 training classes (C0, C1, C2), there are three original tasks, i.e., three classification pairs (C0, C1), (C0, C2), (C1, C2). Individual task interpolation increases the number of samples for each classification pair by enabling data from mix(C0, C1), mix(C0, C2), mix(C1, C2). However, it does not distinguish pairs like (mix(C0, C1), mix(C0, C2)), whereas MLTI does by allowing cross-tasks interpolation.\n\nFinally, it is also worthwhile to mention that MLTI empirically outperforms individual task interpolation methods -- MetaMix and Meta-Maxup. \n\n---\n\n**Q2**: Randomly sampled location\n\n**A2**: As mentioned under equation (5), we use Manifold Mixup for task interpolation, where the interpolation layer (location) is randomly selected as suggested in [Verma et al. 2019]. We also empirically find that the randomly selected interpolation layer achieves the best performance when Manifold Mixup is used for task interpolation. The paper also mentions that Manifold Mixup can be replaced with other interpolation strategies (e.g., CutMix, mixup) under equation (5) and the selection of interpolation strategy has been discussed in Appendix D.1 and E.1. \n\nMost importantly, all interpolation-based models -- MetaMix, Meta-Maxup, MLTI use the same interpolation strategies for all experiments, i.e., randomly sampling location is used for all interpolation-based strategies when Manifold Mixup is adopted in the experiments. We have revised the paper to include these details in Appendix D.1 and E.1. Thus, the performance gains of MLTI over other strategies are not caused by the random sample strategy.'}, {'title': 'Response to Reviewer a9eU', 'comment': 'Thank you for reviewing the paper and your valuable comments and suggestions to improve the paper. We detail our response below and have revised the paper accordingly. Please kindly let us know whether our response addresses your concerns.\n\n**Q1**: Results on traditional FSL datasets\n\n**A1**: We have conducted experiments on tieredImageNet, and list the results in Table R-1 and in Appendix E.8 (Table 14) of the revised paper. Similar to miniImagenet and DermNet, we apply MLTI on tieredImageNet with fewer classes (tieredImagenet-S), where 10% of original meta-training classes, i.e, 35 classes, are used. The results show that MLTI outperforms other methods, further verifying its effectiveness.\n\n**Table R-1**: Results (Accuracies with 95% confidence) on tieredImageNet.\n\n|          | Strategies   |   tieredImageNet-S  |                   |\n|----------|--------------|:-----------------:|:-----------------:|\n|          |              |       1-shot      |       5-shot      |\n| MAML     | Vanilla      | 42.20 $\\pm$ 0.84% | 58.23 $\\pm$ 0.77% |\n|          | Meta-Reg     | 42.87 $\\pm$ 0.86% | 59.16 $\\pm$ 0.79% |\n|          | TAML         | 42.86 $\\pm$ 0.84% | 59.33 $\\pm$ 0.76% |\n|          | Meta-Dropout | 41.94 $\\pm$ 0.82% | 58.37 $\\pm$ 0.77% |\n|          | MetaMix      | 43.40 $\\pm$ 0.85% | 61.92 $\\pm$ 0.80% |\n|          | Meta-Maxup   | 43.69 $\\pm$ 0.88% | 60.00 $\\pm$ 0.82% |\n|          | **MLTI (ours)**  | **44.32 $\\pm$ 0.82%** | **62.22 $\\pm$ 0.79%** |\n| ProtoNet | Vanilla      | 43.35 $\\pm$ 0.82% | 59.98 $\\pm$ 0.77% |\n|          | MetaMix      | 44.14 $\\pm$ 0.83% | 60.97 $\\pm$ 0.81% |\n|          | Meta-Maxup   | 44.40 $\\pm$ 0.83% | 61.79 $\\pm$ 0.78% |\n|          | **MLTI (ours)**  | **45.47 $\\pm$ 0.86%** | **62.35 $\\pm$ 0.80%** |\n\n---------\n\n**Q2**: Experiments on other domains\n\n**A2**: The experiments in the original submission already span multiple domains including computer vision, healthcare, and biology. Here, NCI and Metabolism use 1,024-dimensional Morgan fingerprint features as input and aim to predict the property of molecules. Tabular Murris uses gene expressions as input for each cell and aims to classify the types of cells. Detailed descriptions of these datasets are discussed in Appendix D.1 and E.1 in the original submission.\n\nWe have now conducted additional experiments on few-shot text classification tasks (non-label-sharing setting). Following [Bao et al. 2020], we use Huffpost dataset to evaluate the performance and the results are listed in Table R-2 and Appendix E.8 (Table 14). We adopt ALBERT [Lan et al. 2019] as the pre-trained encoder and a classifier with two fully connected layers as the base model. Detailed dataset descriptions and hyperparameter settings are discussed in Appendix E.8.\n\n**Table R-2**: Results (Accuracies with 95% confidence intervals) on Huffpost.\n\n|          |              |      Huffpost     |                   |\n|----------|--------------|:-----------------:|:-----------------:|\n|          |              |       1-shot      |       5-shot      |\n| MAML     | Vanilla      | 39.51 $\\pm$ 1.07% | 50.68 $\\pm$ 0.90% |\n|          | Meta-Reg     | 40.32 $\\pm$ 1.05% | 50.96 $\\pm$ 0.98% |\n|          | TAML         | 40.03 $\\pm$ 1.00% | 50.89 $\\pm$ 0.88% |\n|          | Meta-Dropout | 39.89 $\\pm$ 0.98% | 51.03 $\\pm$ 0.91% |\n|          | MetaMix      | 40.64 $\\pm$ 1.02% | 51.65 $\\pm$ 0.92% |\n|          | Meta-Maxup   | 40.39 $\\pm$ 1.01% | 51.80 $\\pm$ 0.91% |\n|          | **MLTI (ours)**  | **41.06 $\\pm$ 1.04%** | **52.53 $\\pm$ 0.90%** |\n| ProtoNet | Vanilla      | 41.85 $\\pm$ 1.01% | 58.98 $\\pm$ 0.92% |\n|          | MetaMix      | 42.27 $\\pm$ 0.98% | 60.43 $\\pm$ 0.90% |\n|          | Meta-Maxup   | 42.39 $\\pm$ 1.01% | 60.27 $\\pm$ 0.88% |\n|          | **MLTI (ours)**  | **42.74 $\\pm$ 0.96%** | **61.09 $\\pm$ 0.91%** |\n\nAccording to the results, we find that our model also outperforms other strategies in Huffpost, demonstrating its effectiveness. \n\n---------\n\n**Q3**: Theoretical Analysis \n\n**A3**: We have revised our paper in light of your comments by adding more explanations in the theoretical analysis section. Specifically, we’ve now stated more explicitly that the loss function induced by MLTI is approximately the standard loss function plus a positive term. We explained that such a positive term is a quadratic function on the parameter phi_i’s, and therefore can be interpreted as a regularization term. We further added more explanations around Theorem 1, and pointed out that the purpose of Theorem is to show how this regularization term improves the generalization error bound. Please kindly let us know if you have any further questions or concerns.\n\n---------\n\n**References**\n\n[Bao et al. 2020] Bao, Yujia, et al. ""Few-shot text classification with distributional signatures."" ICLR 2020.\n\n[Lan et al. 2019] Lan, Zhenzhong, et al. ""Albert: A lite bert for self-supervised learning of language representations."" arXiv preprint arXiv:1909.11942 (2019). \n'}, {'title': 'Response to Reviewer A6CS', 'comment': 'Thank you for reviewing our paper and your constructive comments. We have fixed the typos in the paper, and we provide detailed responses below. Would you mind letting us know if our responses address your concerns?\n\n**Q1**: Full-size miniImagenet and DermNet. \n\n**A1**: Thanks for the great suggestion; we have added the results of all baselines on full-size miniImagenet and DermNet, which show the effectiveness of MLTI. Please kindly refer to Table R-5 and Appendix E.2 in the revised paper. The results further demonstrate the effectiveness of MLTI. All results are calculated on the same meta-testing tasks.\n\n**Table R-5**: Results on full-size miniImagenet and DermNet\n\n | Backbone | Strategies   | miniImagenet-full |                   | DermNet-full      |                   |\n|----------|--------------|:-----------------:|:-----------------:|:-----------------:|:-----------------:|\n|          |              |       1-shot      |       5-shot      | 1-shot            | 5-shot            |\n| MAML     | Vanilla      | 46.90 $\\pm$ 0.79% | 63.02 $\\pm$ 0.68% | 49.58 $\\pm$ 0.83% | 69.15 $\\pm$ 0.69% |\n|          | Meta-Reg     | 47.02 $\\pm$ 0.77% | 63.19 $\\pm$ 0.69% | 50.10 $\\pm$ 0.86% | 69.73 $\\pm$ 0.70% |\n|          | TAML         | 46.40 $\\pm$ 0.82% | 63.26 $\\pm$ 0.68% | 50.26 $\\pm$ 0.85% | 69.40 $\\pm$ 0.75% |\n|          | Meta-Dropout | 47.47 $\\pm$ 0.81% | 64.11 $\\pm$ 0.71% | 51.10 $\\pm$ 0.84% | 69.08 $\\pm$ 0.69% |\n|          | MetaMix      | 47.81 $\\pm$ 0.78% | 64.22 $\\pm$ 0.68% | 51.83 $\\pm$ 0.83% | 71.57 $\\pm$ 0.67% |\n|          | Meta-Maxup   | 47.68 $\\pm$ 0.79% | 63.51 $\\pm$ 0.75% | 51.95 $\\pm$ 0.88% | 70.84 $\\pm$ 0.68% |\n|          | **MLTI (ours)**         | **48.62 $\\pm$ 0.76%** | **64.65 $\\pm$ 0.70%** | **52.32 $\\pm$ 0.88%** | **71.77 $\\pm$ 0.67%** |\n| ProtoNet | Vanilla      | 47.05 $\\pm$ 0.79% | 64.03 $\\pm$ 0.68% | 49.91 $\\pm$ 0.79% | 67.45 $\\pm$ 0.70% |\n|          | MetaMix      | 47.21 $\\pm$ 0.76% | 64.38 $\\pm$ 0.67% | 51.50 $\\pm$ 0.76% | 69.55 $\\pm$ 0.68% |\n|          | Meta-Maxup   | 47.33 $\\pm$ 0.79% | 64.43 $\\pm$ 0.69% | 51.18 $\\pm$ 0.83% | 69.07 $\\pm$ 0.72% |\n|          | **MLTI (ours)**         | **48.11 $\\pm$ 0.81%** | **65.22 $\\pm$ 0.70%** | **52.91 $\\pm$ 0.81%** | **71.30 $\\pm$ 0.69%** |\n\n---\n\n**Q2**: soft-label to calculate the prototype.\n\n**A2**: Thank you for pointing out this idea to us. We tried the idea to compute prototypes using soft-labels on RainbowMNIST. Specifically, we use the interpolation ratio $\\lambda$ as soft labels to calculate the prototypes. The results are reported in Table R-6:\n\n**Table R-6**: Comparison of MLTI w/ or w/o using soft labels to construct prototypes\n\n| Strategies      |    RainbowMNIST   |\n|-----------------|:-----------------:|\n| Vanilla         | 65.41 $\\pm$ 1.10% |\n| MLTI+Soft label | 59.34 $\\pm$ 0.99% |\n| **MLTI (ours)**           | **70.14 $\\pm$ 0.92%** |\n\nFrom these results, we found that using soft labels to construct prototypes performs significantly worse than MLTI and Vanilla ProtoNet. We speculate this result is due to the prototype computed using soft-labels, combined with the mixing idea, which cannot distinguish between the different classes well enough. In the vanilla setting without mixing, each prototype has the interpretation to represent its own class. Now, with mixing (for simplicity, let’s consider a fixed non-zero mixing proportion lambda), each prototype contains a non-trivial proportion of information from more than one class, making it harder to distinguish between the different classes.\n\n---\n\n**Q3**: Interpolation layer analysis in Manifold Mixup\n\n**A3**: We conduct new experiments on Metabolism and Tabular Murris, where Manifold Mixup (i.e., interpolating features) is used for sample interpolation, where ProtoNet is used as backbone algorithm. We report the results in Table R-7 and Appendix E.5.2. The results indicate (1) fixing the interpolation layer can also boost the performance; (2) randomly selecting the interpolation layer achieves the best performance; (3) interpolating at the lower layer performs similarly as interpolating at the higher layer, indicating the robustness of MLTI with different selected layers. \n\n**Table R-7**: Analysis of interpolation layers. Layer 0, 1 represents randomly select layer 0 or layer 1 for interpolation. None means vanilla ProtoNet.\n\n|      Interpolation Layer      | Metabolism: 5-shot | Tabular Murris: 1-shot |\n|------------|:------------------:|:----------------------:|\n| None | 61.06 $\\pm$ 0.94% | 80.03 $\\pm$ 0.90% |\n| Layer 0    |  62.53 $\\pm$ 0.98% |    81.18 $\\pm$ 0.93%   |\n| Layer 1    |  62.38 $\\pm$ 0.94% |    81.25 $\\pm$ 0.90%   |\n| **Layer 0, 1** |  **63.47 $\\pm$ 0.96%** |    **81.89 $\\pm$ 0.88%**   |\n'}, {'title': 'Response to Reviewer hkv8', 'comment': 'Thanks a lot for reviewing our paper and acknowledging our contributions. We have conducted additional experiments as suggested and please kindly let us know if your concerns are addressed.\n\n**Q1**: Heavier feature extraction backbone. \n\n**A1**: We ran a new experiment with ResNet-12 as the backbone and evaluated the performance of 1-shot miniImagenet-S and DermNet-S with this heavier backbone. The results are reported in the following Table R-4 and in Appendix E.5.1. The results demonstrate the consistent effectiveness of MLTI even with deeper backbone models.\n\n**Table R-4**: Results of MLTI and other strategies with the heavier base model (ResNet-12)\n\n| Backbone | Strategies  |    miniImagenet-S  |      DermNet-S      |\n|----------|-------------|:-----------------:|:-----------------:|\n| MAML     | Vanilla     | 40.02 $\\pm$ 0.78% | 47.58 $\\pm$ 0.93% |\n|          | MetaMix     | 42.26 $\\pm$ 0.75% | 51.40 $\\pm$ 0.89% |\n|          | Meta-Maxup  | 41.97 $\\pm$ 0.78% | 50.82 $\\pm$ 0.85% |\n|          | **MLTI (ours)** | **43.35 $\\pm$ 0.80%** | **52.03 $\\pm$ 0.90%** |\n| ProtoNet | Vanilla     | 40.96 $\\pm$ 0.75% | 48.65 $\\pm$ 0.85% |\n|          | MetaMix     | 42.95 $\\pm$ 0.87% | 51.18 $\\pm$ 0.90% |\n|          | Meta-Maxup  | 42.68 $\\pm$ 0.78% | 50.96 $\\pm$ 0.88% |\n|          | **MLTI (ours)** | **44.08 $\\pm$ 0.83%** | **52.01 $\\pm$ 0.93%** |\n'}, {'title': 'Response to Questions about Reproducibility ', 'comment': 'Thanks for your interest and comments. We here reply to your questions one-by-one, and we also revised our paper to include more implementation details according to your comments.\n\n- Hyperparameters: Yes, we apply cross-validation to select hyperparameters. Following the traditional process of cross-validation, we randomly select some classes/samples for validation.\n- Metrics: Following the traditional setting in MAML [Finn et al. 2017], the intervals are calculated across all tasks. The maximum iterations and the query set size during evaluation have been reported in Table 5 and 7 in the Appendix of the original submission. The query set size in testing is the same as training. The optimal iteration to calculate the reported metrics is also determined by cross-validation.\n- NCI: The balanced dataset is generated from “NCI balanced”.\n- TDC: As discussed in Appendix E.1, we balance the data by randomly selecting at most 500 positive and 500 negative samples. If one subdataset has less than 500 positive/negative samples, we will use all positive/negative samples.\n- DermNet:\n    - We do not apply hash algorithms to remove images. Instead, we remove duplicate classes. \n    - The base model used in DermNet is the same as miniImagenet-S and ISIC, where a convolutional neural network with four convolutional blocks is used. We have put these details in Appendix E.1.\n- RainbowMNIST: we combine the training and test set of original MNIST data and randomly select 5,600 samples for each class. Each task has 1,000 samples in our experiments.\n\n**Reference**\n\n[Finn et al. 2017] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" In International Conference on Machine Learning, pp. 1126-1135. PMLR, 2017.\n'}, {'title': 'Questions to reproduce results', 'comment': 'Dear authors,\nWe have been trying to reproduce the experiments in the paper. In the process, we have come across the following questions:\n- How were the hyperparameters selected for datasets where no (meta-)validation set is given? No validation data were specified for Pose, ISIC, DermNet-S, NCI, and TDC. For miniImagenet-S, it is unclear whether the miniImangenet validation set is used. There is a mention of cross-validation in Appendix C.1, but it is unclear how and on what data this was carried out.\n- Metrics: How many episodes were used to calculate the reported metrics and confidence intervals, and were the intervals calculated across repeated experiments or across tasks? How large were the query sets during evaluation?\n- NCI: There exist two versions of the dataset: A ""balanced"" and a ""full"" one. From the reported accuracies, it appears that a balanced dataset was used. Is this the balancing from the ""NCI balanced"" dataset or was the full dataset balanced manually?\n- TDC: Appendix C.1 says ""We balance each subdataset by randomly selecting at most 1000 data samples"" -- in this context, we are unsure about the specific meaning of ""balance"": equal number of True vs. False labels, or equal number of samples across tasks?\n- DermNet:\n  - Prabhu et al. (2018) note that they performed perceptual image hashing to remove duplicate images. Was this also done here, and if so, with which hashing algorithm?\n  - Which model architecture was used for this dataset? The Resnet50v2 as used by Prabhu et al. (2018), or a different one?\n- RainbowMNIST: According to Finn et al. (2019), each task contains 900 samples. Were these samples drawn only from the 60K MNIST train images or also from the 10K test images?\n\nWe would be grateful for your clarifications regarding these questions. Many thanks in advance.'}, {'summary_of_the_paper': 'In this paper the authors propose a meta-learning method for few-shot learning. The propose approach, MLTI, creates new (artificial) tasks by interpolating two (existing) tasks form the training set during (meta-)training. The new tasks are generated by interpolating features/labels of two sampled tasks from the training set. The authors show that the proposed approach achieve good results in multiple datasets (both in regression and classification), multiple settings (”label sharing” and “non-label sharing”) and different algorithms (MAML and ProtoNets).\n', 'main_review': '### Pros\n+ The paper is well written and easy to follow.\n+ The idea of interpolating tasks in a meta-learning setting is novel, intuitive and simple.  Although previous work exists that augment the number of tasks, this is the first approach that augments across tasks (rather than within task).\n+ The authors shows good result on different datasets, settings and backbones.\n\n### Cons\n- I feel like results in more “traditional” (and larger) FSL datasets are missing. For example, it would be nice to see results in tieredImagerNet or metaDataset.\n- I also feel that the authors introduce the method as being a general meta-learning approach, but only show results on image. classification/regression. It would be nice to see results in other domains such as RL/NLP/etc tasks.\n- I find the theoretical analysis difficult to follow and potentially not very informative to the rest of the paper (that been said, I am not an expert on generalization theory/Rademacher complexity and cannot properly validate it).', 'summary_of_the_review': 'I recommend this paper for acceptance. The proposed idea is simple and novel, the paper is well written and the empirical evaluation is well executed. \n\n---\n**Post-rebuttal update**\n\nI thank the reviewers for the rebuttal and I keep my rating of 8. \nCongratulations for the nice work!', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': '[Summary]\nThis work tackles a scenario where there may not be a large number of training tasks available, which increases the susceptibility of meta-learning algorithms to meta-level overfitting/memorization problem. In particular, to cope with the scarcity of tasks, the paper proposes to augment the given task set through interpolation of tasks. The paper reports better performance than other methods on benchmarks that have fewer training tasks. \n', 'main_review': '[Strengths]\nThe work provides extensive theoretical analysis to provide theoretical guarantees as to how the proposed MLTI task interpolation method achieves better generalization. In contrast, previous methods that have employed common augmentation methods (e.g., label noise, CutMix, MixUp) without theoretical guarantees. \nThe work introduces scenarios that are more challenging than standard benchmarks by limiting the number of meta-training tasks.\nThe work provides extensive experiments across various datasets under such challenging scenarios and demonstrates better performance than previous methods, providing empirical support for the effectiveness of the proposed task interpolation method.\n\n[Weaknesses]\nI believe the work has minor technical novelty compared to the related work by Ni et al [1]. In particular, Ni et al. [1] performs several augmentations for meta-learning, one of which is MixUp for tasks. Using MixUp between any given pair of classes, Ni et al. [1] also creates new tasks.  \n\n[Comments]\nIn Related Work section, the work states that compared to work by Ni et al. [1], the proposed method directly desnifies the task distribution. But, doesn’t [1] effectively densify the task distribution, where a new task can consist of new classes that are constructed by using MixUp on pairs of classes? As such, I believe more discussions on this issue should help better differentiate the work from the related work.\nWhy does the proposed method randomly sample a location where features are to be interpolated? Is there an ablation study on the sampled location? I wonder if this technique is what makes the proposed method perform better than other works. I’m curious as to whether the proposed method, without this technique, still performs better than other works. The ablation study on this would be helpful for better understanding of differences from other works.\nAlso, how does it compare with related works on standard benchmarks, such as miniImageNet. I think that the proposed method should still work with a larger number of tasks and believe that these experimental comparisons can strengthen the contributions of the proposed method.\n\n[1] Ni et al. Data Augmentation for Meta-Learning.', 'summary_of_the_review': ""[Recommendation]\nDespite strong experimental results and analysis, at this point, I believe the technical novelties are not significantly different from the work by Ni et al. Thus, I believe the work is marginally below the acceptance threshold. If the above comments are addressed, I’m willing to increase the score.\n\n-----------------------------------------\n[Post-rebuttal]\nI thank the authors for the response, along with clarifications and updates in the manuscript. \nAs the authors have addressed most of my concerns, I'm happy to increase the score accordingly. "", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes an interpolation strategy for meta-learning to improve the learned model’s generalizability. The interpolation strategy is quite simple - interpolate between a pair of tasks, in contrast to existing methods such as adding label noise or data augmentation on each task individually. The authors show that the resulting gradient- or metric-based meta-learning framework (MLTI) induces a data-dependent regularizer that controls the Rademacher complexity leading to better generalization. MLTI is tested on specially curated datasets derived from standard benchmarking datasets. Furthermore, MLTI is also compared against existing data augmentation and interpolation strategies for meta-learning to illustrate its effectiveness.\n', 'main_review': 'strengths\n\nAlthough nifty, the idea of pair-wise task interpolation is an incremental change over the existing data augmentation approaches. The theoretical results, highlighting the relationship between task interpolation and the Rademacher complexity, are non-trivial extensions of the Zhang et al. ICLR 2021 and Yao et al. ICML2021 to account for pair-wise task interpolation. I view this as the primary contribution of the paper.\n\nThe comparison against existing data-augmentation baselines for both metric- and gradient-based meta-learning approaches is quite exhaustive. Furthermore, MLTI is tested on a wide variety of datasets. While the improvement on each dataset is only marginal, the consistent improvement in all datasets and across all approaches strengthens the paper’s contribution.\n\nThe paper is well-written and easy to follow.\n\nConcerns\n\nCurrent approaches in meta-learning rely on heavier backbones such as ResNet. As the goal of all the meta-learning methods is to improve the model’s generalizability, I think it is fair to evaluate the effectiveness of MLTI with heavier feature extraction backbones. Such a comparison is relevant as the proposed task interpolation is conducted on the features extracted from some intermediate layer of the network.\n', 'summary_of_the_review': 'Overall, the paper proposes a simple extension to standard data/task - augmentation methods for meta-learning but justifies it with rigorous theory. The theoretical results are non-trivial extensions/combinations of existing work. The effectiveness of the approach is evident from the extensive empirical evaluation. The contributions are strong, albeit limited to the meta-learning research community.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a task augmentation method via task-interpolation for data efficient meta-learning. While the traditional meta-learning methods highly rely on a large amount of data to retain diverse training tasks, the proposed method, MLTI generates tasks by interpolating the tasks which are obtained from training data. The experimental results on variety of few-shot learning dataset show that MLTI is effective when the meta-training data for constructing training tasks is not enough, for both gradient-based and metric-based few-shot learner. ', 'main_review': 'Strength\n1.\tThis paper proposes a novel task-augmentation method, which is affected by Manifold Mixup, which can be applied to many existing few-shot learning tasks.\n2.\tThe theoretical analysis shows that the proposed MLTI augmentation has a regularization effect and leads the meta-learner to have a better generalization capability.\n3.\tExtensive simulation results on variety of few-shot learning datasets and two representative few-shot learning methods show that the proposed MLTI is highly effective for meta-learning with fewer data. \n\nWeakness\n1.\tComparison with the prior methods in large dataset is missing. For example, in Table 3, the comparison results are provided only for small datasets or reduced version of large datasets. However, the proposed method is not restricted to small dataset. The ablation experimental result in Figure 2 shows that proposed MLTI is still effective when the full miniImageNet/DermNet dataset is used, although the performance gain becomes small when the full dataset is used. I suggest the authors to include the comparison of MLTI and prior methods with full size of miniImageNet and DermNet.\n\nQuestion\n1.\tIn Section 3, the authors mentioned that it is intractable to calculate prototypes with mixed labels. However, in prior work on semi-supervised few-shot learning [1], the prototypes are computed using soft-labels. What happens if we compute prototypes using soft-labels as done in [1]? \n2.\tSome additional studies on the interpolation layer would be helpful for understanding the proposed method. In Algorithm 1 and 3, the interpolation layer $l$ is randomly chosen in step 7. What happens if we fix $l$ instead of randomly sampling $l$ for every iteration? In that case, how are interpolating at lower layer and interpolating higher layer different? \n\nTypo: In last line of page 4, there is a typo (regularizaiton -> regularization)\n\n[1] Ren, Mengye, et al. ""Meta-Learning for Semi-Supervised Few-Shot Classification."" International Conference on Learning Representations. 2018.\n', 'summary_of_the_review': 'This paper proposes a novel task-augmentation method of MLTI and shows the effectiveness of proposed MLTI through the extensive simulation results and theoretical analysis. The proposed MLTI can be applied to both optimization-based and metric-based few-shot learning methods. Adding some experimental results would make the readers to better understand the proposed method.\nHowever, I believe the idea of this paper is valuable for few-shot learning field, and I recommend to accept this paper.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper describes a method for augmenting task selection in meta-learning, by interpolating support and query sets between two random tasks from the base dataset.  This is examined in two scenarios, label-shared LS and non-label-shared NLS, differing in whether the label space is the same between tasks (e.g. pose estimation) or different (classification to different discrete class sets).  In the former, label targets are interpolated as well as support set inputs, while in the latter, new classes are constructed by random cross-task pairings.  Comparisons are made to other interpolation augmentation approaches, including MetaMix, which interpolates in query set but not the support set.  The approach results in significant performance gains on multiple benchmarks in both settings.\n', 'main_review': 'I found the approach to be simple and relatively well explained, including ablations studies on large-point questions I had while reading, including its behavior and effectiveness for different sizes and number of classes in the original base dataset, as well as effects of inter- and intra- task interpolations.\n\nThe key difference between this work and MetaMix (Yao et al 2020) is incremental but important:  MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well.  This difference enables between-task interpolation which adds additional augmentation particularly in settings where few tasks can be drawn from the base data.\n\nI didn\'t follow much of the theoretical sections in detail, and had to look at the appendix proofs to even understand some of the notation in the main text.  In my somewhat limited understanding they seem reasonable.  These claim to show a theoretical generalization improvement in simplified settings (binary classification of single layer model, linear protonet feautres).\n\n\nAdditional questions:\n\nNLS:  In addition to a single set of correspondence pairs, the input examples for each class can be mixed with all-pairwise-combinations.  How many combinations are used?  That is, for two sets of k examples {xs_i} and {xq_j} (i,j in 1..k), one can form k^2 interpolated examples {a xs_i + (1-a) xq_j} using each i,j combination.  Are all of these combinations formed or just a single set of k pairings?  If using more than k pairings, this would change the task from k-shot to k^2-shot; but the l-layer features for each of the k^2 combinations could be computed, and then up to (k^2 choose k) tasks could be selected from these and used in the upper layer loss comparisons.  For k=5 that would increase interpolated pairs from 5 to 25, but potentially get up to 53130 upper layer loss comparisons from each task pair sample -- would this get even more benefit from this task augmentation technique?\n\neq 5:  what does the name of the subscript ""cr"" mean (does it stand for something)?\n\nIt could be useful to have a more explicit explanation of differences with MetaMix.  MetaMix will run the inner loop on the unmodified support set only, and use a mix of support+query in outer loop comparison optimization, whereas this work interpolates support set in inner loop as well.  This is already mentioned at a high level (fig 1 caption and sec 5 last paragraph), but I think it could be even clearer by pointing out the difference in the discussion around eq 5, that support set H^s,Y^s in the inner loop is mixed between tasks, whereas in MetaMix, only the H^q,Y^q are replaced by mixing.\n\n', 'summary_of_the_review': 'Overall, the approach is described well enough to understand the approach, and is emperically shown to result in decent performance gains in the low task data settings for which it is intended.  The theoretical sections corroborate this, but I found them hard to follow.\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Meta-Learning with Fewer Tasks through Task Interpolation', 'authorids': ['~Huaxiu_Yao1', '~Linjun_Zhang1', '~Chelsea_Finn1'], 'authors': ['Huaxiu Yao', 'Linjun Zhang', 'Chelsea Finn'], 'keywords': ['meta-learning', 'task interpolation', 'meta-regularization'], 'abstract': 'Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies.', 'one-sentence_summary': 'A new framework to densify the task distribution via task interpolation', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'yao|metalearning_with_fewer_tasks_through_task_interpolation', 'pdf': '/pdf/ebbfc5841da414394c96beeba92500546061461a.pdf', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/meta-learning-with-fewer-tasks-through-task/code)', '_bibtex': '@inproceedings{\nyao2022metalearning,\ntitle={Meta-Learning with Fewer Tasks through Task Interpolation},\nauthor={Huaxiu Yao and Linjun Zhang and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ajXWF7bVR8d}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Jason Wei', 'Maarten Bosma', 'Vincent Zhao', 'Kelvin Guu', 'Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew Dai', 'Quoc V Le']",ICLR,Finetuned Language Models are Zero-Shot Learners,https://iclr.cc/virtual/2022/oral/6255,2022," This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=gEZrGCozdqR,https://openreview.net/forum?id=gEZrGCozdqR,gEZrGCozdqR,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper examines the extent to which a large language model (LM) can generalize to unseen tasks via ""instruction tuning"", a process that fine-tunes the LM on a large number of tasks with natural language instructions.  At test time, the model is evaluated zero-shot on held out tasks.  The empirical results are good, and the 137B FLAN model generally out performs the 175B untuned GPT-3 model.\n\nAll reviewers voted to accept with uniformly high scores, despite two commenting on the relative lack of novelty.  The discussion period focused on questions raised by two reviewers regarding the usefulness of fine-tuning with instructions vs. multi-task fine-tuning without instructions.  The authors responded with an ablation study demonstrating that providing instructions at during tuning led to large gains.\n\nOverall the paper\'s approach and detailed experiments will be useful for other researchers working in this fast moving area in NLP.'}, {'title': 'Thanks for the reply!', 'comment': ""Thanks for the reply! I have read the reply and other reviewers' comment and my decision is unchanged.""}, {'title': 'Thanks for the reply', 'comment': 'The new appendix B2 results are great and address my primary concern.  One final curiosity - did the fine-tuning variants do just as well on held-in datasets?\n\nOverall I still mostly stand by my original assessment of the paper.  I think the paper is still a good paper, and probably would be in the top 10-20% of accepted papers according to my tastes.\n\n(Edit: oops, replied to the wrong place...)'}, {'title': 'Response to reviews (TLDR)', 'comment': 'We thank all four reviewers for the comprehensive feedback. As a summary, our work is about the striking ability for finetuned language models at scale to follow instructions. We present empirical results across 10 NLP tasks, with ablation studies that tease apart the key components of instruction tuning. Positive cross-task generalization results imply that instructional finetuning at scale can play a key role in developing generalist models.\n\nAll four reviews were positive. Two reviewers, however, brought up the insightful observation about whether instructions are really needed: could the model achieve the same zero-shot performance via multitask finetuning without instructions? To answer this question, we performed an ablation study by finetuning two such models that did not have instructions. One model was finetuned without any templates indicating the task, and another model was finetuned where each example had the task/dataset name prepended to the input. We then evaluated these models using the same task-split setup as the paper, and found that they performed well below the original model on average (~8 points lower on average across 19 datasets), demonstrating that instructions do indeed matter during finetuning. We added this ablation study into Appendix B.2 and also explain it in the individual responses to reviewers 1 and 3. For the additional points brought up by reviewers, we replied in the individual responses to reviewers and also revised the paper accordingly.\n\n(We meant to post this tldr during the response period. Sorry for the additional ping!)\n'}, {'title': 'Reply to Reviewer BGTp', 'comment': 'Thank you for the detailed review and the positive feedback! We aimed to propose a simple and intuitive method for improving the zero-shot performance of language models, and we are glad you found the empirical results thorough and impressive. We revised the paper below based on your comments—please let us know if you have any further clarifications or suggestions to improve the paper!\n\n> In Appendix A.1, Base LM vs FLAN might be a better comparison.\n\nIndeed, Base LM vs FLAN is the most relevant comparison for instruction tuning. We originally included the comparison to GPT-3 because readers were more likely to be familiar with the performance of GPT-3, but in the revision we added more text to highlight the Base LM vs FLAN comparison. Thank you for suggesting this and pointing out that the conclusions do not change much.\n\n> Which prompts do the Base LM results use? \n\nBase LM is similar to GPT-3 in that it is a pretrained language model without any instruction tuning. Hence, we use the same prompts as GPT-3 to evaluate Base LM. The GPT-3 paper only provided one prompt per dataset. We added this explanation to the first question/answer of the new FAQ section in the Appendix. \n\n> For Figure 5 can you add the untuned model to the curve with the x-axis=0 (0-task cluster)?\n\nThe reason we did not plot the untuned model (Base LM) on the curve at x=0 is that the Base LM model uses different prompts than instruction-tuned models (Base LM uses GPT-3 style language modeling prompts, whereas FLAN responds to instructions) and thus it would not be an apples-to-apples comparison. However, we added them in as horizontal lines labeled with “Base LM”.\n\n> Discussion in light of Sahn et al.\n\nThank you for pointing out this recent related paper (and for appropriately noting that it came out two weeks ago). We added this to our extended related work section (E.6) (as well as another recent related work, “MetaICL: Learning to Learn In Context”, which was posted on arxiv less than a month ago). Indeed, our model capacity explanation is just a conjecture, and it is challenging to compare the results across these papers because of differences such as model sizes, model types (decoder-only vs encoder-decoder), pretraining data, task mixtures, and type of instructions.\n\n> Results of Appendix C are interesting… this might imply that instruction-tuned models will become the new “base” model… is it possible to briefly mention it in the main paper?\n\nThank you for this helpful suggestion! We added a pointer to it in the first paragraph of the Discussion section.\n\n> Can the authors compare “fine-tuning/prefix-tuning an instruction tuned model with 16 examples” (appendix C but with only 16 examples)? \n\nAs the focus of our paper is zero-shot learning, we prioritized our computational resources during the response period for the ablation study requested by Reviewers 4nJJ and K8bM. However, we appreciate the suggestion and you appropriately pointing out that Chen et al. came out after the ICLR submission deadline—we added this to the paper as further work.\n'}, {'title': 'Clarifications', 'comment': '> Why not have templates per task cluster instead of per dataset?\n\nWe created unique templates per dataset because datasets in the same task cluster often differed slightly (e.g., “is this movie review positive” vs “is this yelp review positive”). The complete list of templates per dataset is given in the supplementary material. We added this clarification to the FAQ section in the appendix of the paper for additional visibility. \n\n> In Figure 6A, why was performance not increasing for untuned models w.r.t model size?\n\n*(This response is identical to the response given to Reviewer 4nJJ.)*\n\nFor context, Figure 6A is a check of correctness for Figure 6B. Figure 6A confirms that scale improves performance for tasks that were seen during instruction tuning, as expected. The untuned Base LM model performance in 6A is shown just for completeness.\n\nNonetheless, the fact that scale does not always improve the zero-shot performance of untuned Base LM is an interesting artifact. Initially, we were surprised, because Brown et al., 2020 shows that scale improves performance across a large number of tasks in aggregate.\n\nIt turns out that scale does not improve performance for certain tasks. This is especially true for zero-shot learning, and we think that this happens to be the case for the reading comprehension and sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot performance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does not show results on sentiment analysis, but Holtzman et al., 2021 find that zero-shot performance on SST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both GPT-3 and the models we use.\n\nThis artifact is certainly worth further study, but is outside the scope of instruction tuning. Ideally, we would have performed the Figure 6 ablation with cross-validation instead of a single split, which likely would have smoothed out that artifact. We added this explanation to the new FAQ section in the paper.\n\n\n> More templates didn’t help is particularly interesting… what about the opposite explanation that large models easily memorize a small number of templates?\n\nThis hypothesis was our original motivation for writing ten templates per dataset, and we were surprised to see in the Appendix B ablations that this did not make a substantial difference in performance when there was a large number of tasks. We think this result certainly warrants further investigation, but our intitution is that larger pretrained models, which are more data efficient, are better able to generalize.\n\n> UnifiedQA seems potentially worth citing.\n\nThanks for pointing us to this! We added it to the related work section.\n'}, {'title': 'Reply to Reviewer K8bM', 'comment': 'Thank you for the insightful review. Our paper proposed an approach that substantially improves zero-shot performance of language models, and we are glad that you liked the ablations and the prompt tuning experiments!\n\n> My biggest complaint is that it\'s not completely clear the instructions themselves are important at all…have templates that leave out ""instructions""\n\n*(This response is identical to the response given to Reviewer 4nJJ.)*\n\nThank you for this thoughtful comment! We performed an additional experiment for this that shows that including instructions during finetuning does indeed allow the model to better perform zero-shot tasks.\nWe added it to Appendix B2 in the revised paper and also summarize it below for convenience.\n\nTo tease apart the role of instructions, we considered several setups where the instructional prompts were removed during training. \n\n1. **No template**: Only the inputs and outputs were given to the model, as would be the case for a task-specific model. For example, for translation to French, the input would be *“the dog runs”* and the output would be *“le chien court”*. This does not distinguish among tasks during multi-task training.\n2. **Task/dataset name**: In this setup, each input is prepended with the name of the task and the name of the dataset. For example, for translation to French, the input would be *“[Translation: WMT’14 to French] The dog runs”*.\n\nWe compare these two setups to FLAN’s finetuning procedure, which used natural instructions (e.g., *“‘The dog runs.’ Translate this sentence to French.”*). \n\nFor no template finetuning, during zero-shot inference we use the natural instructions from FLAN (otherwise, the model would not know what task to perform). For task/dataset name finetuning, during zero-shot inference we used both the natural instructions from FLAN as well as task/dataset name. The results are shown in the table below:\n\n| Finetuning prompt | Inference prompt | Read. Comp. | Closed-B QA | NLI | Translation | 4-task Avg.\n| ----------- | ----------- | ----------- |  ----------- |  ----------- |  ----------- |  ----------- | \n| Natural instructions (=FLAN) | Natural instructions | 77.4 | 56.6 | 56.2 | 30.7 | **55.2** |\n| No template | Natural instructions | 58.2 | 25.5 | 50.2 | 15.0 | **37.3** |\n| Task/dataset name | Task/dataset name | 64.9 | 40.8 | 60.2 | 21.9 | **47.0** |\n| Task/dataset name | Natural instructions | 63.0 | 44.8 | 52.8  | 25.9 | **46.6** |\n\n(Training with task/dataset name achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for which the validation set only has 56 examples. FLAN also gets 83.9 with the best dev template, but the average template was only 64.1.)\n\nFinetuning with no templates has very poor performance, which is expected since the model is not able to distinguish between tasks during finetuning. Finetuning with the task/dataset name improves performance noticeably over no template (using task/dataset name for inference versus natural instructions for inference performed about the same in aggregate), but is still almost eight points behind FLAN on average across the four task clusters. This result indicates that finetuning with instructions is crucial for zero-shot performance on unseen tasks.\n'}, {'title': 'Reply to Reviewer DjKD', 'comment': ""Thank you for the detailed review and positive feedback! We proposed a method to improve the zero-shot performance of language models, and we are glad you found instruction tuning to be simple, effective, and of high practical value. We discuss your comments below. Please let us know if you have any additional suggestions to improve the paper!\n\n> Do FLAN prompts work for GPT3 or Base LM?\n\nA small but noteworthy difference between FLAN prompts and GPT-style prompts is that FLAN prompts are formulated as responding to an instruction, whereas GPT-style prompts are formatted as continuations of sentences. For instance, a FLAN prompt might be *“The dog runs. Translate this sentence to French.”,* whereas a GPT-style prompt might be *“‘The dog runs’ translated to French is:”.* \nFor this reason, FLAN prompts do not work well for pretrained language models without finetuning, so we did not report these results in the paper. Performance was near zero for most generation tasks. For instance, given the FLAN prompt *“The dog runs. Translate this sentence to French.”,* Base LM continues with *“The dog runs after the cat”* instead of actually translating the sentence. Hence, we used the established GPT-3 prompts for our Base LM baselines. For additional visibility, we added this to the new FAQ section in the Appendix.\n\n> Since instruction tuning will adjust all the parameters in the original pre-trained language model, there is a question what about what is the potential impact of this tuning process? \n\nThe goal of instruction tuning is to allow models to follow instructions for a range of tasks, including both seen and unseen tasks, and the way we do this modifies all parameters in the language model. Intuitively, we might expect instruction tuning to lead to worse perplexity on pure language modeling, as presumably the model is optimized for responding to NLP tasks instead of the original pretraining objective. We did not measure FLAN’s perplexity on the original pretraining data compared with Base LM, but for language modeling tasks shown in Appendix A.1, FLAN performs about the same or slightly worse.\n\n> Will [FLAN] drop any knowledge of any tasks, which will be a disadvantage when the task's labeled data is available?\n\nInterpreting this question as whether a massively multi-task model performs as well as a model finetuned on only a single task, we think this is still an open question. This likely depends on the specific mixture of tasks used in instruction tuning, since many similar tasks will likely lead to positive task transfer. An experiment could compare our FLAN model with a single-task model on a seen task, but we will leave this for future work since our paper focuses on zero-shot learning.\n\n> In the Analysis C in the appendix, it will be good to have results for tasks other than classification such as summarization or question answering, and also to have a baseline where the Base LM model is fine-tuned directly with the task labeled data (without prompt/soft-prompt).\n\nWe decided to prioritize our compute during the response period for the ablation study requested by two other reviewers (as prompt tuning / supervised learning is not the focus of this paper). However, thank you for the suggestion—we added this suggestion into the the Analysis C in the appendix as future work!\n""}, {'title': 'Clarifications', 'comment': '> Missing details about hardware usage and training time.\n\nInstruction-tuning our 137B parameter model for 30k steps took around 60 hours on a TPUv3 with 128 cores. Thank you for pointing this out—we added this detail into the manuscript.\n\n> Do you have unique prompts for each dataset or only for each [task] cluster?\n\nThe ten unique templates are for each dataset and not for a task cluster. This is because datasets in the same task cluster often differed slightly (e.g., *”is this movie review positive”* vs *“is this yelp review positive”*). The complete list of templates per cluster is given in the supplementary material. We added this clarification to the FAQ section in the appendix of the paper for additional visibility. \n\n> Further details on use of OPTIONS?\n\nThe motivation for the OPTIONS token is to allow the language model to learn to distribute probability mass over a known set of outputs. Borrowing the example from Holtzman et al., 2021, in vanilla rank classification, given an input *“People left the party because…”*, the probability mass on the *“it was 2am”* continuation was diluted by similar continuations such as *“it was 1am”*, *“it was 3am”*, etc. Hence, telling the model beforehand about the classification options tells it which continuations to give probability mass to. Base LM is not evaluated using OPTIONS, just rank classification. We did not perform a quantitative evaluation of adding up the probabilities assigned to all options, but for most tasks, the top output via greedy sampling when using OPTIONS was one of the provided options. \n\n> Figure 6A: why does the untuned model see worse performance with more parameters?\n\n(This response is identical to the response given to Reviewer K8bM.)\n\nFor context, Figure 6A is a check of correctness for Figure 6B. Figure 6A confirms that scale improves performance for tasks that were seen during instruction tuning, as expected. The untuned Base LM model performance in 6A is shown just for completeness.\n\nNonetheless, the fact that scale does not always improve the zero-shot performance of untuned Base LM is an interesting artifact. Initially, we were surprised, because Brown et al., 2020 shows that scale improves performance across a large number of tasks in aggregate.\n\nIt turns out that scale does not improve performance for certain tasks. This is especially true for zero-shot learning, and we think that this happens to be the case for the reading comprehension and sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot performance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does not show results on sentiment analysis, but Holtzman et al., 2021 find that zero-shot performance on SST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both GPT-3 and the models we use.\n\nThis artifact is certainly worth further study, but is outside the scope of instruction tuning. Ideally, we would have performed the Figure 6 ablation with cross-validation instead of a single split, which likely would have smoothed out that artifact. We added this explanation to the new FAQ section in the paper.\n\n> Figure 1 (Bottom) is possibly misleading, since AFAICT zero-shot FLAN underperforms few-shot GPT-3 on the majority of tasks.\n\nThank you for pointing this out. Although Figure 1 bottom is technically correct for the three task clusters shown, zero-shot FLAN does not outperform few-shot GPT-3 on task clusters that are not shown in the figure (e.g., translation, commonsense reasoning). Hence, to reduce the risk of misleading the reader that this result extrapolates to tasks not shown, we changed the caption to “Performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning improved performance substantially out of ten we evaluate.” \n\n> Not clear what ""turning the task around"" means for some tasks, or why this is a useful type of prompt diversity.\n\nRegarding templates that “turn the task around”, the motivation was that it would increase the number of tasks that the model could perform. We did not do this for all tasks (all templates are shown in the Supplementary Material). While outside of the scope of our evaluation, these templates could potentially be useful for open-ended generation tasks. We also added some examples of FLAN’s responses to open-ended generation tasks in Appendix G.'}, {'title': 'Reply to Reviewer 4nJJ', 'comment': 'Thank you for the insightful review! Our paper explores an instruction-tuned model that outperforms BaseLM and GPT-3, and we are glad you found our approach intuitive and conceptually compelling. Below is discussion on the points you brought up. Please let us know if you have any further suggestions on how to improve the paper!\n\n>  It is unclear whether models are actually ""learning to follow instructions"" or just learning a very large space of tasks from the fine-tuning procedure…models could potentially perform just as well with nonsense or missing prompts during fine-tuning.\n\n*(This response is identical to the response given to Reviewer K8bM.)*\n\nThank you for this insightful comment! We performed an additional experiment for this that shows that including instructions during finetuning does indeed allow the model to better perform zero-shot tasks.\nWe added it to Appendix B2 in the revised paper and also summarize it below for convenience.\n\nTo tease apart the role of instructions, we considered several setups where the instructional prompts were removed during training. \n\n1. **No template**: Only the inputs and outputs were given to the model, as would be the case for a task-specific model. For example, for translation to French, the input would be *“the dog runs”* and the output would be *“le chien court”*. This does not distinguish among tasks during multi-task training.\n2. **Task/dataset name**: In this setup, each input is prepended with the name of the task and the name of the dataset. For example, for translation to French, the input would be *“[Translation: WMT’14 to French] The dog runs”*.\n\nWe compare these two setups to FLAN’s finetuning procedure, which used natural instructions (e.g., *“‘The dog runs.’ Translate this sentence to French.”*). \n\nFor no template finetuning, during zero-shot inference we use the natural instructions from FLAN (otherwise, the model would not know what task to perform). For task/dataset name finetuning, during zero-shot inference we used both the natural instructions from FLAN as well as task/dataset name. The results are shown in the table below:\n\n| Finetuning prompt | Inference prompt | Read. Comp. | Closed-B QA | NLI | Translation | 4-task Avg.\n| ----------- | ----------- | ----------- |  ----------- |  ----------- |  ----------- |  ----------- | \n| Natural instructions (=FLAN) | Natural instructions | 77.4 | 56.6 | 56.2 | 30.7 | **55.2** |\n| No template | Natural instructions | 58.2 | 25.5 | 50.2 | 15.0 | **37.3** |\n| Task/dataset name | Task/dataset name | 64.9 | 40.8 | 60.2 | 21.9 | **47.0** |\n| Task/dataset name | Natural instructions | 63.0 | 44.8 | 52.8  | 25.9 | **46.6** |\n\n(Training with task/dataset name achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for which the validation set only has 56 examples. FLAN also gets 83.9 with the best dev template, but the average template was only 64.1.)\n\nFinetuning with no templates has very poor performance, which is expected since the model is not able to distinguish between tasks during finetuning. Finetuning with the task/dataset name improves performance noticeably over no template (using task/dataset name for inference versus natural instructions for inference performed about the same in aggregate), but is still almost eight points behind FLAN on average across the four task clusters. This result indicates that finetuning with instructions is crucial for zero-shot performance on unseen tasks.\n'}, {'summary_of_the_paper': 'This paper describes an approach to fine-tuning large language models which can improve zero-shot accuracy on unseen tasks. ', 'main_review': 'Overall well-written with compelling results, this paper describes a new language model (FLAN) and shows how it improves upon the zero-shot task performance of previous language models such as GPT-3. While the paper is lacking some additional analysis, I am hesitant to recommend extremely compute-intensive ablations due the large size of the model (137B parameters).\n\nStrengths:\n - Considers a reasonably wide set of 62 datasets; although the inherent arbitrariness in dataset clustering was listed as a limitation, the clusters look quite reasonable to me, and the removal of overlapping datasets (e.g., ""Reading Comprehension w/ Commonsense"") seems appropriate.\n - Results are better than a strong Base LM baseline, as well as existing state-of-the-art models (GPT-3)\n - Overall the approach is intuitive and conceptually compelling\n - Highly relevant to ongoing work on language modeling, prompt tuning, and zero-shot learning\n\nWeaknesses:\n - From these experiments, it is unclear whether models are actually ""learning to follow instructions"" or just learning a very large space of tasks from the fine-tuning procedure. In other words, even though prompt variance is reported at inference time, the models could potentially perform just as well with nonsense or missing prompts during fine-tuning. As far as I can tell, no experiments that rule out this possibility exist.\n - Although qualitatively useful, the analysis in 4.1 does not conclusively show that the number of instruction tuning clusters aids performance, or that this trend is likely to continue with more clusters. Most of the gain could be acquired by tasks which are most difficult, or most similar to the heldout task, and this analysis cannot disprove such an interpretation. A proper analysis would consider more heldout tasks and permutations of training data, but presumably this is prohibitively expensive.\n - The paper is missing important details about hardware usage and training time\n - Some possible issues which might be resolved by the additional questions below\n\nAdditional Questions:\n - ""For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset."" Do you have unique prompts for each dataset or only for each dataset cluster? Based on a cursory look at the supplementary material, I would assume the latter.\n - I didn\'t fully understand the justification for the OPTIONS token. Are the fine-tuned models successfully putting (almost) all of their probability mass on the corresponding options? How is the Base LM evaluated (if it\'s not fine-tuned, presumably it doesn\'t learn how to handle these options)? \n - Figure 6A: why does the untuned model see worse performance with more parameters?\n\nNits:\n - Figure 1 (Bottom) is possibly misleading, since AFAICT zero-shot FLAN underperforms few-shot GPT-3 on the majority of tasks\n - Not clear what ""turning the task around"" means for some tasks, or why this is a useful type of prompt diversity', 'summary_of_the_review': 'I give this paper a strong recommendation, in spite of some missing ablations. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper explores a simple and effective method to improve zero-shot performance of pretrained language models. Authors take a 137B parameter pretrained model and finetune it on multiple tasks verbalized via natural language instruction templates. As the result, the instruction-tuned model performs well on un-seen tasks with the zero-shot setting.', 'main_review': 'Pros:\n1. The problem addressed has high practical value: it tries to make large pre-trained language model more accessible to a range of NLP tasks. The ""instruction tuning"" idea will significantly reduce the cost for task-specific fine tuning, labeled data and prompt engineering compared to other approaches. \n2. The method is simple and easy to implement. Authors carefully design the experiment to minimize the leakage between the fine-tuning and inference data. Given that, it still shows superior performance on different types of NLP tasks. The result on specific task can be further improved when adapting with ""prompt tuning"" on labeled data, which shows that the instruction-tuning process does not drop much task-specific knowledge from the original pretrained model.\n3. The analysis presented in the main paper and the appendix is thorough enough. Authors also discussed about the limitation of model when downstream tasks are more similar to language modeling tasks.\n\nCons:\nThere are still a few questions that can be addressed to make the analysis comprehensive.\n1. Have authors try to use the FLAN prompts on GPT3 or BaseLM and how does the performance look like?\n2. Since instruction tuning will adjust all the parameters in the original pre-trained language model, there is a question what about what is the potential impact of this tuning process? Will it drops any knowledge of any tasks, which will be a disadvantage when the task\'s labeled data is available? In the Analysis C in the appendix, it will be good to have results for tasks other than classification such as summarization or question answering; and also to have a baseline where the BaseLM model is fine-tuned directly with the task labeled data (without prompt/soft-prompt).\n', 'summary_of_the_review': 'Overall, the paper proposed an interesting idea and showed strong empirical results, hence I vote for accepting.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper creates a dataset of over 60 NLP tasks described via instructions (using templates for each task) and finds this boosts zero-shot performance on unseen tasks.', 'main_review': '----\nDetailed comments:\n----\n\n- ""For each dataset, we manually compose ten unique templates"":  Why not have templates per task cluster instead of per dataset?  it is likely a relatively minor effect given the results from Appendix B but it seems like it could slightly prevent overfitting\n\n- The ablation in 4.1 was great (number of clusters).  Nit: I would have tried to move the (datasets per cluster/templates per dataset) ablation to the main body as well and shortened Section 3\n\n- The 4.2 (scaling laws) ablation is perhaps the most interesting of all.\n\n- In figure 6A, why was performance not increasing for untuned models w.r.t model size?  This seems to contradict findings from Brown et al where larger models did better on essentially all tasks.  Were there perhaps some poor datasets that happened to be in the held-in split (since the held-out tasks don\'t seem to have the same trend)?\n\n----\nAppendix:\n----\n\nI liked the section B ablations (as implied above).\n\nThat more templates per dataset didn\'t help is particularly interesting and suggests some questions. You hypothesize that more templates doesn\'t help because ""models at such scale do not easily overfit to a finetuning single task"" - but my intuition is for an opposite explanation -- that the models at such scale easily memorize a small number of templates!  One may even wonder if the instruction nature of the templates is helping at all.  \n\nFrom what I can tell, Appendix C on prompt tuning (which is very interesting) is maybe the primary evidence the instructions are important.  I think more could be done here, some ideas, probably there are better ways to test:\n- Have templates that leave out ""instructions"":  I would guess it wouldn\'t affect held-in task performance much, but would affect held-out tasks.  \n- Consider HellaSwag/PiQA/etc, where FLAN underperformed few-shot and even zero-shot.  One might hypothesize that if using a (subotimal) template that is less natural for language modeling, that zero-shot performance would suffer, but that FLAN performance wouldn\'t\n- One might hypothesize that the ""turn the task around"" templates help more than the other more straightforward templates that don\'t swap information between the prompt and response.\n- Easy but probably not great thing to try:  held-out tasks with wrong/useless templates\n\nA final thought:  It\'s not obvious that using as many training examples per dataset as possible is optimal, given that the model could overfit to dataset-specific spurious correlations.  This could be another area to investigate\n\n----\nMisc: \n----\n\n- UnifiedQA seems potentially worth citing as prior work', 'summary_of_the_review': ""Overall, the paper's idea is powerful (but of somewhat limited novelty) and the results are good (but not great).  Its greatest strength IMO was the ablations.  My biggest complaint is that it's not completely clear the instructions themselves are important at all - I suggest a few more experiments, though they don't seem crucial."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposes a simple method, ""instruction-tuning"", to improve the zero-shot learning capability of large language model, which 1) annotates prompts for a wide range of tasks and then 2) fine-tunes the model to ""answer/respond to"" those prompt. The empirical results are impressive: after instruction-tuning, the 0-shot performance is better than GPT-3 (0-shot, sometimes few-shot) on a wide range of datasets; nevertheless, on datasets with formats already similar to language modeling, the performance gain is negligible or even negative.\n\nThe paper also made a few other observations 1) performance benefits from the number of task clusters 2) instruction-tuning is only beneficial when the model size is larger enough, and 3) few-shot learning still helps. ', 'main_review': 'While the method is a simple and straightforward scaling up of concepts and ideas from prior works (e.g. Zhong et al, Adapting ...; Mishra et al, cross-task generalization ...), the empirical results are thorough and impressive (outperforming GPT-3 with a slightly smaller model). The analyses also helps us understand when this method would work and inform us about future research directions.\n\nBelow are my concrete questions and comments: \n\n**Additional Tasks Results (3.4)**\n\nIn Appendix A.1, the paper mainly draws conclusions based on comparisons between GPT-3 and FLAN, which I do not think are fair: GPT-3 and FLAN differ in model size and pre-training data distribution. Instead, I think Base LM vs. FLAN might be a better comparison between “Off the shelf LM” and “instruction-tuned” model (though it won’t change the conclusion).\n\nIt is also worth pointing out in the main paper that for most of the additional tasks, even though it does not lead to higher accuracy, the performance of FLAN is still at least comparable (e.g. <1% worse and difference generally negligible) to Base-LM 0-shot. The only outlier seems to be ReCorD where the performance drops significantly after instruction-tuning, and this probably deserves some discussion. \n\nAlso I might have missed it - for the Base LM 137B zero-shot result, is it on the average template, or the best template? \n\n**Number of Task Clusters (section 4.1)**\n\nFor Figure 5 can you add the untuned model to the curve with the x-axis=0 (0-task cluster)? This can help us understand how much even 1 cluster (e.g. summarization) may help. \n\n**Explanation for scaling (section 4.2)** \n\nIt is an insightful empirical result that instruction tuning only works when model size reaches 68B. However, I am not entirely sure about the potential explanation of “model capacity”. There might be two potential explanations to this phenomena: 1) “model capacity”: as the paper has mentioned, smaller pre-trained models do not have enough model capacity and underfit the instruction tuning data, and 2) “better OOD generalization”: better quality pre-trained models have higher OOD generalization ability (and OOD accuracy) and they are less likely to “overfit” to in-distribution data. \n\nI personally find the second explanation more convincing. For example, Sahn et al. (https://arxiv.org/abs/2110.08207) finds that even models with only 11B parameters can generalize to unseen tasks, using T5 (MLM) and a larger set of prompts. The use of MLM objectives (might) improve the pre-training quality, while more prompts reduce the “overfitting to in-domain data” issue. \n\nI appreciate the fact that the author explicitly states the model capacity hypothesis more as a conjecture rather than a solid explanation. It’d be great if the authors can support the explanation further with more empirical evidence. On the other hand, however, since the results from Sanh et al came out only 2 weeks ago,  I would not change the score based on the response to this question. \n\n**In-context Few-shot vs. Fine-tuned Few-shot (Section 4.3)**\n\nCan the authors compare “fine-tuning/prefix-tuning an instruction tuned model with 16 examples” (appendix C but with only 16 examples) with “in-context prompting” (in 4.3 of the main paper), similar to Chen et al. (https://arxiv.org/abs/2110.07814 )? This would further inform us how we should use the few-shot learning examples for larger language models: put it in-context, or fine-tune? Again, since the comparison of Chen et al. came out only 2 weeks ago and the paper limit is 9 pages, I would not change the score based on the response to this question. \n\n**Others**\n\nResults of Appendix C are interesting and potentially impactful - this might imply that instruction-tuned models will become the new “base” model for the pretraining-finetuning paradigm. Is it possible to briefly mention it in the main paper as well (and redirect the readers to the appendix to see the full results)?\n\nIt might be too late to change the name, but “Finetuned LAnguage Net” (FLAN) is uninformative, since it does not capture any unique aspect of this method. What does “LAnguage” mean here, ""natural language instruction"" or ""language model""? If it is the former, then directly including the word “instruction” might be better; and hopefully it’s not the latter, since even fine-tuned BERT on SST-2 counts as a fine-tuned language model ...\n\n**Typo**\nIntro: \nInstruction tuning is “a” simple method that, as ….\n\nConclusion:\nMoreover, our work supercedes recent work such “as” \n', 'summary_of_the_review': 'While the method is not new, the empirical results are strong and comprehensive. Though I disagree on the interpretation of some empirical results, overall the additional analyses bring us further insights on what method works for very large language models (i.e. > 100B dense model). I highly recommend the paper to be accepted to ICLR 2022. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '1: The contributions are neither significant nor novel.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'title': 'Finetuned Language Models are Zero-Shot Learners', 'authorids': ['~Jason_Wei1', 'bosma@google.com', 'vzhao@google.com', '~Kelvin_Guu1', '~Adams_Wei_Yu1', '~Brian_Lester1', '~Nan_Du1', '~Andrew_M._Dai1', '~Quoc_V_Le1'], 'authors': ['Jason Wei', 'Maarten Bosma', 'Vincent Zhao', 'Kelvin Guu', 'Adams Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew M. Dai', 'Quoc V Le'], 'keywords': ['natural language processing', 'zero-shot learning', 'language models'], 'abstract': 'This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.', 'one-sentence_summary': '""Instruction tuning"", which finetunes language models on a collection of tasks described via instructions, substantially boosts zero-shot performance on unseen tasks.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wei|finetuned_language_models_are_zeroshot_learners', 'pdf': '/pdf/16b50405ab1e3ac1e2f76190ee62a48c496c568d.pdf', 'supplementary_material': '/attachment/8e92660ea3d04e3380af73ef67555377395930c1.zip', 'code': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/finetuned-language-models-are-zero-shot/code)', '_bibtex': '@inproceedings{\nwei2022finetuned,\ntitle={Finetuned Language Models are Zero-Shot Learners},\nauthor={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=gEZrGCozdqR}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Olivia Wiles', 'Sven Gowal', 'Florian Stimberg', 'Sylvestre-Alvise Rebuffi', 'Ira Ktena', 'Krishnamurthy Dvijotham', 'Ali Taylan Cemgil']",ICLR,A Fine-Grained Analysis on Distribution Shift,https://iclr.cc/virtual/2022/oral/7003,2022," Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets.  Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. Code is available at github.com/deepmind/distribution shift framework.",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=Dl4LetuLdyK,https://openreview.net/forum?id=Dl4LetuLdyK,Dl4LetuLdyK,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper proposes a general framework to reason about fine-grained distribution shifts, evaluating a large set of different approaches in a variety of settings. All reviewers recommend acceptance. While concerns were raised, including questions about the generality of the framework, unsurprising “tips”, and unclear take-home messages, all reviewers find the work strong, with an elegant formulation, and useful insights. The AC agrees with the reviewers that this work addresses a very important problem, proposes an interesting unified framework and benchmark for domain shift analysis, and should be a valuable tool for the community to pursue further research in this area.'}, {'title': ""Response to author's response to my response on the authors response"", 'comment': 'Cool! I think all my questions were answered. Although I think using unsupervised domain adaptation algorithms for domain generalization is odd, when dedicated algorithms do exist towards domain generalization [1,2,3]. And keeping in mind that this is purely a empirical paper with reasonable technical novelty and the fact that the eventual observations and inferences are partly expected and partly surprising (from sec 4.1 and 4.2 ), I settle with a score of 8. \n\n1. Dubey, Abhimanyu, et al. ""Adaptive Methods for Real-World Domain Generalization."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n 2. Li, Ya, et al. ""Deep domain generalization via conditional invariant adversarial networks."" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n 3. Li, Da, et al. ""Deeper, broader and artier domain generalization."" Proceedings of the IEEE international conference on computer vision. 2017.'}, {'title': 'Response to question on CSV file', 'comment': 'Yes it is the accuracy in D-31. Yes we will add a description to the CSV, explaining the organization of results.'}, {'title': 'Response to the csv file', 'comment': 'Thanks, awesome work! I have raised my score to 10. For MPI3D, is is the accuracy in D-31? Could you please add a short description of the csv later?'}, {'title': ""Response to Reviewer U5qL's 2nd Response"", 'comment': '*Could the authors maybe publish a csv file with the full results of all models, such that interested researchers can quickly check the numbers?*\n\nWe thank the reviewer for appreciating our work and will release a CSV with the full set of results with the code release.\n\nIn the meantime, we have added to the supplementary an xlsx spreadsheet containing results for the three distribution shifts (**unseen data shift**, **low data drift** and **spurious correlation**). We include the best result for each seed for each model (over the hyperparameter sweep for that seed and model).\nWe do this for each value of $N$ for the **low data drift** and **spurious correlation** settings. We also include the results for Figure 9 and the results using the in distribution or out-of-distribution validation set (Figure 10). '}, {'title': ""Response to the authors' comments"", 'comment': 'I thank the authors for addressing my comments. I read through their responses and the changes in the revised paper version. I will respond to the ones I flagged as the most important ones in my initial review, please consider the ones I do not respond to as addressed adequately. Could the authors maybe publish a csv file with the full results of all models, such that interested researchers can quickly check the numbers? I think that the main benefit of the paper is the standardized benchmark. If the authors can release all numbers, it would make it easy for other researchers to compare. My score has been 8, but I would raise to a 10 if the authors released this csv file, because releasing the accuracy numbers would strengthen the benchmark and make it more useable for other researchers. For example, the timm repository which contains most relevant pytorch models (with pretrained checkpoints) has a csv reporting the accuracy numbers on ImageNet, including relevant preprocessing hyperparameters: https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv. Maybe the authors could post something similar, alongside with relevant hyperparameter choices.\n\n### Question 1 and 2: More samples from the true distribution seem to hurt performance.\nYes, thank you for the clarification, it makes sense now. \n\n### Missing bars, Figs. 10-12.\nConsidering the missing bars, I think my confusion stemmed from the change in ordering of the bars. But the bars did not disappear but merely changed their position. I still find this ordering makes it hard to read the bar plots in Figs. 10-12, but I understand that it is a trade-off of what one wants to show. I understand that seeing small differences fast would be harder with a heat map. I still think that heat maps would be better, but as the other reviewers do not seem to share my concerns, I am fine with this display. \n\n### Question 3: Takeaway 3.\nI appreciate the authors commenting on this question. And I agree with their response that an analysis similar to [2] would not work here. In contrast to ImageNet-C for example, we not have the mapping x_target = x_source+corruption and therefore, calculating the perceptual distance between x_corrupted and x_augmented is impossible.'}, {'title': ""Response to reviewer HwsU's 2nd response"", 'comment': '# Answers to questions\n\n*Question 1: I see. Thanks for clarifying that the assumption that only two factors of variation are present is not as restrictive as I assumed. Perhaps you can add little detail on this aspect into the main paper.*\n\nWe have added a reference at the start of Section 2.2 to Appendix E with the sentence “Our experimental framework can be used to compare more complex shifts, discussed in Appendix E.” \n\nWe have also added a couple of additional sentences in Appendix E discussing this: “Our framework for creating shifts allows for creating complex distribution shifts. We can define distribution shifts using a comparison operator on one or more labels to create a distribution shift. Shifts can then be composed together to create more complex shifts between train and test. This allows the exploration and evaluation of multiple entangled factors of variation. Finally, there is no necessity for methods to make use of the additional attribute information beyond the label, which allows the evaluation of unseen factors of variation (by not allowing methods to use knowledge of a given factor).”\n\n*Question 4: The experiment setting is still not clear to me. DA methods require, by definition, a source domain with labels and target domain with unlabeled data. So can you explain this statement ""We could operate precisely as those methods were originally described and throw away labels but, in our case, this would unfairly penalise DANN and CORAL. (We do not compare methods in the precise setting where we have data from two domains at train time: one labelled and one unlabelled.)""*\n\nWe follow [1] who apply these methods to a domain generalization setup. Methods such as DANN typically have two losses: (1) one is on the labelled domain to minimize prediction error and (2) the second is to bring the two domains closer together by enforcing some form of invariance in feature space (for example DANN uses an adversary to learn a representation such that the adversary cannot distinguish between the domains). [1] apply this in the domain generalization case (where they have N domains and labels for all domains but want to generalise, *at test time*, to an unseen domain) by applying loss (1) to all domains and loss (2) across domains to promote invariance of the features to the different domains. This should, hopefully, improve generalization to new domains. \n\nIn our case, we treat the different values of the nuisance attributes as different domains and proceed as follows. For all values of an attribute, we apply loss (1). We apply loss (2) across the different values of the attribute to enforce that the learned representation $z$ is invariant to the value of the nuisance attribute. If $z$ were invariant, the learned representation should be able to generalise in all shifts considered. We note that with the code, this implementation of the methods will be obvious. Please let us know if this is still unclear.\n\n[1] \u200b\u200bGulrajani et al. “In search of lost domain generalization.”\n\n\n# Additional Results\n\nWe have run MMD and GroupDRO on dSprites, MPI3D, and Camelyon on the unseen data shift and low data shift settings. Neither method performs significantly better on these datasets than the methods explored in the paper. The only exception is MMD on Camelyon, which achieves 79% (+/-2) top 1 accuracy on Camelyon (+8% better than the next best method: 71% (+/- 1)). This result is obtained using the in distribution validation set. However, if we look at performance on the out of distribution validation set (which was never seen during training), results change. The top 1 accuracy for MMD is 72% (+/-3) versus DANN which obtains 74% (+/- 1).\n\n\nThese results corroborate our conclusions that: (1) we can do better than the ERM method (**takeaway 1**); (2) results vary due to the domain (**takeaway 6 and 7**); and (3) that while domain generalization algorithms can improve performance, it is not clear a-priori which ones will, on which datasets (**discussion point 1**) and how to make these algorithms useful generically (**tip 4**) without trying all options.'}, {'title': 'Re: Response to reviewer zAfw ', 'comment': ""Thank you for your replies. I've raised my final score to 8 based on your response. Although the early version of the paper missed some important discussions as I mentioned, I admire the efforts and the elegant formulation to unify those distribution shift tasks. Hope that the final version of the paper could bring more insights to the audience in the corresponding fields.""}, {'title': 'Response to author response 1', 'comment': 'Thanks for your replies.\n\nQuestion 1: I see. Thanks for clarifying that the assumption that only two factors of variation are present is not as restrictive as I assumed. Perhaps you can add little detail on this aspect into the main paper. \n\nI now also understand the reasoning behind the latent factorization modeling. Strictly speaking it can be skipped, but if it is helping the modeling aspect, then it makes sense to explain it as such.\n\nQuestion 4: The experiment setting is still not clear to me. DA methods require, by definition, a source domain with labels and target domain with unlabeled data. So can you explain this statement ""We could operate precisely as those methods were originally described and throw away labels but, in our case, this would unfairly penalise DANN and CORAL. (We do not compare methods in the precise setting where we have data from two domains at train time: one labelled and one unlabelled.)""\n\n\n\n'}, {'title': 'Rebuttal of Additional Comments raised by Reviewer U5ql', 'comment': ""Here we address additional comments raised by the reviewer, not covered in the main questions.\n\n- *Distribution (or covariate) shift is usually defined using the definition in [1], section 2.1.1. Can you please comment on whether there is difference to your definition and what it means for the approaches one would use to tackle it?*\n\nThe definition of covariate shift as pointed out by the reviewer is a generic one where directly the distribution of observables $p(X)$ is changing, while the conditional distribution of labels, given inputs $p(Y|X)$ is invariant. While this is a mathematically consistent and well defined model, this approach just says that there is a (potentially arbitrary) shift present in $p(X)$ but it does not model how this shift is happening. In contrast, in our approach we assume that actually the underlying generative process $p(X|Z)$, as well as the labelling process $p(Y|Z)$ are invariant; but the distribution shift is happening due to a shift in the marginal distribution chosen for the attributes $p_\\textrm{new}(Y)$. In our opinion, this is the more realistic scenario -- for example, when collecting a dataset of medical images, the actual physical process of imaging $p(X|Z)$ would be invariant (as dictated by the laws of physics) while we may have control over the attributes such as age, gender, ethnicity of the patients, or the brand of the imaging equipment, particular hospital where the images are taken etc. The particular choice of the frequency of attributes induces a shift in the prior distribution of the latents from $p(Z) = \\sum_Y p(Z| Y) p_\\textrm{true}(Y)$ to $p_\\textrm{new}(Z) = \\sum_Y p(Z| Y) p_\\textrm{new}(Y)$. If we integrate over $Z$ and sum over $Y$, we get  $p_\\textrm{new}(X)$ that is in general different from $p(X)$ so we have a distribution shift. \n\nWe have added the following in Section 2.1 to clarify this: “This is a special case ... may vary.”.\n\n- *Data Augmentation. I don’t quite agree with the definition here.*\n\nWe appreciate the point that learning the generative model is not the practitioner's aim when utilising heuristic augmentations. Our aim here is to give a different interpretation of why these augmentations can be useful. We can view heuristic augmentation as attempting to create samples according to some known set of invariances within the domain (e.g. the model should be invariant to color changes). We discuss this further in our answer to question 4 in the main questions. If we could construct augmentations to describe all invariances, we could cover the true generative model. We have clarified this in the paper, by adding an additional paragraph **heuristic data augmentation** in Section 2.1.\n\n- *The tips in 4.2. are not really surprising.*\n\nWhile the tips are not necessarily surprising, it is good to validate that they are nevertheless true on these distribution shifts and when evaluated comprehensively. We have improved the grounding and wording as follows. For tip 1, we have improved the takeaway as stated in Answer 14 and have used this to ground our tip. For tip 2, we have added the qualitative sentence that this depends on the complexity of the data and quality of the generative model learned. For tip 3, we have included multiple references to previous work. Finally, for tip 4 we have removed the comment about DANN.\n\n- *I do not understand the right-most inequality in Eq.2*\n\nIt means the number of values that we see for the nuisance attribute is $> 1$ (e.g. we see at least two different colours when generalising to a third). We have updated the sentence describing how this shift manifests on dSprites to make this explicit: “In the dSprites example, ...”.\n\n- *What about heatmaps in Figures 10-13?*\n\nWe did think about this. However, we found that it made distinguishing small differences in model performance hard. Instead, we color code the bars and sort them. The color of the bar denotes the method, colored by type so a reader can extract which method does best as well as which group of methods.\n\n- *I did not understand whether model selection is part of the framework code?*\n\nThe framework allows for the metric and validation set used for model selection to be specified. We use this in Figure 10 in order to compare using an OOD versus ID validation set.\n\n- *please define $A_i$*\n\nSee section 2.1 in the first sentence: “We assume a joint distribution…”.\n\n- *Should it be $p_\\textrm{test}$ on the right hand side of the definition?*\n\nNo. All we’re saying is that the probability of a given attribute at train is correlated with another (as opposed to in $p_\\textrm{test}$ where the attributes are independent).\n\n- *“Shift 2: Low-data drift” -> Maybe mentioning that this issue is being studied by the fairness community would be good.*\n\nWe discuss work on fairness and bias in section 6 (paragraph benchmarking spurious correlation and low data drift). If there are other specific references the reviewer thinks are missing, we are happy to update the paper.""}, {'title': 'Rebuttal of Main Questions raised by Reviewer U5ql', 'comment': 'We thank the reviewer for their thorough and insightful review. We have made the suggested stylistic changes and appendix changes and have answered questions below. In this comment we focus on the main points raised by the reviewer. We discuss additional points in the second comment.\n\n- *Question 1 and 2: More samples from the true distribution seem to hurt performance.*\n\nWe think there was some confusion here. Higher N **does** lead to higher accuracy (as shown in Figure 11, Figure 12). In Figure 3, 4 (as stated in the captions) we are showing the average percent improvement **over** the ResNet baseline on the IID test set (so the non-biased dataset). As a result, what figures 3 and 4 show is that as N gets higher, there is less improvement over the ResNet baseline. We did it this way in order to average over the different datasets. \n\nTo clarify that the results are aggregate and on the test set, we have updated text in the first paragraph in section 4 to include: “We plot aggregate results in figures 3-7 by averaging results over the different datasets and complete results in the appendix in  figures 10-12, in which the results are broken down by dataset. All results reported are on the test set.”\n\nWe are not sure which bars the reviewer is referring to when they say there are bars missing in Figure 12. If the reviewer could point out the missing method, we will update the plot.\n\n- *Question 3: Takeaway 3.*\n\nThis was concluded based on the augmentations that performed best for the different datasets. For example, on MPI3D, in general color augmentations (color, contrast, brightness, etc) improve performance the most whereas spatial transformations (shear, translate, and rotate) hurt performance. On MPI3D, objects are always placed in the same position in the image and there is no spatial transformation, whereas there are variations in color. Therefore, augmentations that promote invariance to color transformations improve performance. Based on a similar analysis we concluded the statement. In takeaway 3, we have included a reference to [2], some more sentences discussing which augmentations help and don’t, and have reworded the takeaway to: “ Heuristic augmentation does not always improve generalization.”. \n\nIn comparison to [2], we note that setting up such an analysis is not so simple. We cannot simply compare the two distributions (as in [2]). Consider the simple example of color changes. An augmentation that inverts may be preferable to one that increases sharpness, as inversion decorrelates the precise color from the prediction. However, in perceptual or L2 space, the sharper image will be more similar to the original. Therefore, augmentations that promote invariance to properties of the true underlying generative model are preferable. The most straightforward way to evaluate this is to train a classifier on these augmentations and evaluate whether this improves downstream performance, as we did.\n\n- *Question 4: Question about contribution 1.*\n\nWith the ‘when’ and ‘why’ statements we were referring to section 2 which defines how shifts occur and how different general approaches (e.g. **weighted resampling**, **heuristic data augmentation**, **learned data augmentation**, and **representation learning**) can be used to improve robustness to these methods. However, as this was unclear, we have modified this sentence to say: “We propose a framework to unify different distribution shifts, defining how they arise and how different common approaches promote robustness to these shifts.”'}, {'title': 'Rebuttal for Questions 1-2 for Reviewer HwsU', 'comment': 'We thank the reviewer for their thorough and insightful review. We are glad they appreciate our work as a useful contribution to the community. We have made the changes suggested and have answered questions below. We have split the answers into multiple comments.\n\n- *Question 1: My major question to the authors of the paper is how general the proposed framework?*\n\nOur framework was evaluated on large in-the-wild datasets, allows for the consideration of multiple attributes including unobserved, entangled ones, and it can be used to explore more complex distribution shifts than those based on two observable attributes. We discuss this further below.\n\nCamelyon and iWildCam **are** large in-the-wild datasets (with hundreds of thousands of images, so of a similar scale to ImageNet) and explore real-world problems (e.g. tumour detection across different hospitals and detecting animals in camera trap imagery across different countries). So, conclusions could most likely be extrapolated to similar problems or setups (e.g. other medical imaging tasks for Camelyon). Overall, we appreciate the hesitancy in extrapolating to markedly different settings (e.g. ImageNet) where the challenges and properties may be different but we believe that similar problems and setups to those investigated would exhibit similar results (e.g. similar medical imaging tasks for Camelyon).\n\nAlso, our framework is more general than just considering two attributes. All the datasets were annotated with additional labels and attributes but there is **no** assumption that all factors of variation are known. Indeed in Camelyon and iWildCam this is the case and there is no assumption that the attributes labelled are not entangled with other unobservable factors of variation (as indeed may be the case in Camelyon and iWildCam).\n\nFinally, our framework for creating shifts allows for more complex distribution shifts than those used in the paper. We can define distribution shifts using a comparison operator on one or more labels to create a distribution shift. Shifts can then be composed together to create more complex shifts between train and test (see Appendix E.1-E.2). This allows the exploration and evaluation of multiple entangled factors of variation.  Finally, there is no necessity for methods to make use of the additional attribute information beyond the label, which allows the evaluation of unseen factors of variation (by not allowing methods to use knowledge of a given factor).\n\n- *Question 2: I could not pinpoint a single take-home message from the paper.*\n\nWe thank the reviewer for their comments and agree that it would be great to know, for a given dataset, what precise method to choose a-priori. However, we find that the conclusions can vary based on the particular attribute being considered (Appendix B.3) and dataset (Figures 10-12). Moreover, the choices may depend on the precise dataset properties (for example, are transformations arising from image-to-image transformations versus other types of change). Models may make implicit assumptions that are subtle to disentangle. Therefore, we hesitate to give one single meta-recommendation that may not generalise to an unseen dataset or attribute. However, our framework can be used to run similar experiments or to infer how models will perform on similar datasets and distribution shifts. We do raise this question in section 5 in order to encourage other researchers to investigate these problems and define concrete actions.'}, {'title': 'Rebuttal for Questions 3-6 for Reviewer HwsU', 'comment': '- *Question 3: I fail to see that significance of the latent factorization model proposed in sec 2.1.*\n\nWe agree with the reviewer that in principle we can integrate over the latents and describe the marginal model $p(X, Y)$ without a reference to a latent $z$. However, from a modelling point of view, we identify several benefits for including $z$.\n\nFirst, we use this formulation to characterize different approaches and why they should encourage generalization. There are two classes of models that rely on this latent factorization in addition to the B-VAEs. This is useful to understand why learned data augmentation can be helpful (e.g. in CAMEL [1], section 3, paragraph **learned data augmentation**) and how access to the true generative model would allow us to solve these distribution shift problems. Additionally, we can use it to characterise domain generalization methods and how they are trained for invariance to the irrelevant attribute (again in section 3, paragraph **domain generalization**).\n\nAnother benefit of introducing a latent representation is that , we can use it to model the case that $y$ is potentially a non-exhaustive list of the factors of variation. As a result, x is not deterministic given the set of attributes. We use a hidden representation $z$, conditioned on which, the generating process of images $p(x|z)$ is invariant across different domains. This additionally allows us to describe the true underlying generative process and differentiate it from the one conditioned on the given distribution of attributes: $p(x|y^{1…k})$. While $p(x|y^{1…k}), p_\\textrm{train}(x|y^{1…k}),  p_\\textrm{test}(x|y^{1…k})$ may vary, but $p(x|z), p_\\textrm{train}(x|z)$ and $p_\\textrm{test}(x|z)$ do not. \n\n- *Question 4: Also, DA works like DANN and CORAL generally make use of additional unlabeled data.*\n\nWe apply domain adaptation approaches by treating different attribute values of the nuisance variable as different domains and then enforcing that the features learned between these domains are invariant, similar to [2]. We could operate precisely as those methods were originally described and throw away labels but, in our case, this would unfairly penalise DANN and CORAL. (We do not compare methods in the precise setting where we have data from two domains at train time: one labelled and one unlabelled. We also do not evaluate test time adaptation which is an interesting area, but we left beyond the scope of the current paper.) We clarify this in section 3 (paragraph **domain generalization**). \n\nOur results seem to corroborate findings elsewhere (e.g. [2]) that find that most domain generalization methods do not help over ERM; however, we explore this on multiple distribution shifts and amounts of shift. We are happy to include more methods in our framework. We have added GroupDRO [3] and the MMD loss [4] into our framework and are currently running these methods on a subset of our datasets and shifts (unseen data shift on dSprites and Camelyon). We will update with results as they come in.\n\n- *Question 5: The domain adaptation works generally work with a covariate shift assumption.*\n\nThe covariate shift assumption is valid as the distribution over labels given a sample within any attribute value should be the same.\n\n- *Question 6: The authors could also contrast the in-distribution vs. out-of-distribution performance of the models.*\n\nThis is an interesting question. We find that there can be large generalization gaps for ViT and MLPs but this is not consistent across the two real-world datasets:\n- For Camelyon, we actually do find that ViT and MLP both have higher in-distribution validation scores than the ResNet models (>90% for MLP/VIT vs ~65% for the ResNets). However, when we look at the out-of-distribution test set, their performance is much worse (57% for MLP/ViT vs 64% for the ResNets). This problem is not solved by model selection (see Figure 12, which compares using the in-distribution and out-of-distribution validation set for model selection on iWildCam and Camelyon). \n- However, on iWildCam, the story changes. Here the in-distribution validation scores for ViT are around 66% whereas for the ResNet they’re around 63%. This translates to the out-of-distribution test accuracy, where the ViT model performs best. \n\nIt’s not clear why precisely ViT works better on iWildCam and doesn’t overfit. We hypothesise that the reason ViT may do better is due to its specific properties. The backgrounds are static across different locations, so both models must learn to ignore these values in order to estimate anything of use. For Camelyon, the ViT model may learn features that are too specific, whereas the CNN uses inductive biases on spatial invariance to generalise better. What precisely causes these models to generalise differently and how they operate in different settings is an interesting question that our framework can be used to address in the future.'}, {'title': 'Rebuttal for Question 7 for Reviewer HwsU', 'comment': '\n- *Question 7: Is Imagenet pretraining performing better only because it is trained on larger scale data compared to other methods?*\n\nIt is presumably because ImageNet is trained with more data. However, we note that this does not always improve performance much (as in Figure 10 for iWildCam and Camelyon) and for small values of N in Figure 11 for iWildCam and Camelyon). Therefore it seems to depend on how much the features learned on the pretrained data are helpful for the downstream task. In Fig7, all but pretraining on ImageNet are trained with the same amount of data. We clarify this in section 4, paragraph additional data.\n\n[1] Goel et al. “Model Patching: Closing the Subgroup Performance Gap with Data Augmentation”\n\n[2] Gulrajani et al. “In search of lost domain generalization.”\n\n[3] Sagawa et al. “Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.” \n\n[4] Li et al. “Domain generalization with adversarial feature learning”'}, {'title': 'Response to reviewer zAfw', 'comment': ""We thank the reviewer for appreciating our efforts and for their thoughtful comments.\n\n*Question 1: However, some conclusions in this paper violate my own observations. For example, the heuristic data augmentation method, Rand Augmentation, is found to be very helpful in long-tailed classification (a specific type of low-data drift) and some label noise tasks.*\n\nWe thank the reviewer for pointing this out. We noticed when running the RandAugment experiments that some augmentations were more helpful than others. As a result, we explored the impact of each augmentation for each dataset in the appendix in figure 8. We found that the utility of each augmentation depended on whether that augmentation promoted invariance to some nuisance attribute in the underlying generative model.  For example, on MPI3D, color augmentations (color, contrast, brightness, etc) improve performance the most whereas spatial transformations (shear, translate, and rotate) hurt performance. On MPI3D, objects are always placed in the same position in the image and there is no spatial transformation, whereas there are variations in color. We highlighted and discussed this point in one of the takeaway messages: **takeaway 3: Heuristic augmentation does not always improve generalization**.  We’ve also included a couple of sentences in takeaway 3 discussing this point, and have referenced a relevant recent work that explores this in depth in the context of common corruptions [1].\n\nTo clarify further that performance varies over datasets, as suggested, we have added a couple of lines in the dataset description (section 4, paragraph **Datasets**): “These datasets differ in their properties: dSprites, MPI3D, Shapes3D are all synthetic; SmallNorb consists of black and white images of toys; Camelyon consists of medical imagery and iWildCam of camera trap imagery. While we aggregate results in the main text, results differ across datasets so please refer to figures 10-12 for performance on each dataset.”\n\n*Question 2: Another concern is that should the ImageNet pretraining be used as a valid method for the study of robustness to distribution shifts? It may violate the settings of SC, LDD, and UDS by introducing samples from unknown distributions. For example, unseen data distribution in the given task and dataset may be compensated by the ImageNet dataset. That's why pretraining is usually forbidden in the tasks like OOD image classification or long-tailed classification. As a result, the superior performance of pertaining is not just frustrating but also (could be) unfair.*\n\nWe agree with the reviewer’s point that pretraining gives, effectively, the model more data to train with, which can be unfair with respect to other methods. We include this approach, as it is an extremely common method to improve model performance in practice, and so we believe it is valuable to understand how it performs in comparison to other methods. We note that while it is unfair (in that ImageNet pretraining effectively uses more data), ImageNet pretraining actually does not always perform particularly better than the baseline and is sometimes outperformed by other methods (e.g. on iWildCam and Camelyon in figures 10-11). \nWe leave it to people using our framework to decide on the most suitable set of approaches when performing their own studies (for example all methods can be run with pretraining initially or it can be held out). In the text, we have clarified that pretraining means that the model has been trained with additional data (see section 4, paragraph **additional data**).\n\n[1] Eric Mintun et al. “On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness”\n""}, {'summary_of_the_paper': '## Very useful study on calibrating OOD models and benchmarks\n\nThis paper provides a fine-grained analysis on nature of distribution shifts in image datasets through extensive experiments on various benchmark methods for robustness. They work with an assumption that the underlying factors of variation, including the label, are encoded through discrete attributes. The authors provide a concrete framework to put forth the latent factorization model, including appropriate grouping for various types of benchmark methods, distribution shifts as well as training conditions. Several experimental demonstrations are provided to compare and contrast robustness of existing methods leading to numerous useful observations.', 'main_review': '### Strengths\n-----------------------------\n\n- Firstly, this paper is a job well done! It was written very clearly and most parts were very easy to follow. The motivations of the paper are very clear. The major strength of the paper lies in the extensive experimental evaluation provided across the main paper and the supplementary, which may be used as a standardized reference for many future works. \n\n- The efforts made towards stringing various works aimed at generalization, as well as various natures of distribution shifts, on a common thread is a commendable effort. \n\n- The figures and the graphical illustrations are very well made, and effectively distill the major inferences of the experiments.\n\n### Required Clarifications\n-----------------------------\n\n- My major question to the authors of the paper is how general the proposed framework? You seem to select different datasets, but take only two attributes on each datasets which raises questions if the findings of the study hold with larger scale real world datasets with multiple factors of variation, often heavily entangled. I understand that these settings are chosen to keep the experiments tractable, but it would be helpful to provide insight into whether the framework holds for Imagenet scale datasets with unknown (and possibly unobservable) factors of variation [1].\nFor example, the inferences and observations in sec 4.1 and sec 5 are conditioned on the assumption that the factors of variation are known, which gives us a cue to which model might work (CycleGAN for low data-drift, pretraining for unseen data shift etc). But can this knowledge be extrapolated to judge *in-the-wild* datasets? This brings me to the next point.\n\n- I could not pinpoint a single take-home message from the paper. The empirical study results have high variance, and it can only be inferred that no single method works for all the cases. But can the authors, equipped with the knowledge of these experiments, draw any *meta recommendation* of mapping between (factors of variation, distribution shift, modeling) choices? Of course, I fully agree that the current findings are still incredibly useful, but I was expecting to see a more optimistic take home message for the readers : ) .\n- I fail to see that significance of the latent factorization model proposed in sec 2.1. While the formulation of the data, discrete attributes and the labels seem clear, I do not see the significance of introducing the latent factor *z*. The explanation seems equally effective by using only notation of (x,y). Also looking at the models used to achieve robustness, none of weighted resampling or data augmentation require any latent factorization. Similarly, Imagenet pretraining also does not necessarily relate to latent factors in the attributes. Beta-VAE however relates to latent factors but that\'s about it. Perhaps the whole section 2.1 can be better motivated in the context of the paper.\n- The domain adaptation works generally work with a *covariate shift* assumption [2]. How does that fit into the proposed framework? Also, DA works like DANN and CORAL generally make use of additional unlabeled data. How are the unlabeled data chosen for the experiments? While it is inferred (sec 4.2) that domain adaptation methods lead to limited improvements, the DA methods considered are quite old and primitive. Perhaps more recent DA methods might help?\n- The authors could also contrast the in-distribution vs. out-of-distribution performance of the models. It seems that arch. like ViT might do very well on training domain but lack generalization on OOD test data. Why is this so? Does this have to do with the strong prior that CNNs induce that help generalization?\n- Is Imagenet pretraining performing better only because it is trained on larger scale data compared to other methods? In Fig 7, are all models trained/pretrained using same amount of data?\n\n\n1. Hendrycks, Dan, and Thomas Dietterich. ""Benchmarking neural network robustness to common corruptions and perturbations."" _(2019).\n2. Ben-David, Shai, et al. ""A theory of learning from different domains."" _Machine learning_ 79.1 (2010): 151-175.', 'summary_of_the_review': ""Overall, I believe that the experiments conducted as part of this paper are very well structured and provide multiple useful insights to the community. However, the analysis still falls short in several places (see above). The latent factorization models needs to be better motivated, and the generality of the framework needs to be clarified. Still, this is a very useful work to the community, and if the authors could clarify the questions raised, I would be happy to update the score. I haven't gone through the supplementary material in detail. If any of the questions above have a direct answer in the suppl. material, the authors can directly point to that and I would be happy to update my comments. "", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors perform an extensive study of different types of distribution shifts to judge which methods perform best on which dataset and/or method. For this, they carefully define which distribution shifts they are interested in and perform a large scale study for each of the shifts. They propose a standardized framework (which can easily be extended to new methods) to train and evaluate new models and methods on the distribution shifts. ', 'main_review': '“find that a model trained on one set of hospitals may not generalise to the imaging conditions of another.” -> Training on “one set of hospitals” is a weird formulation and it’s not clear what is meant; set of MRIs from multiple hospitals or similar? Please rewrite this sentence.\n\n\n“These methods span the following 5 common approaches: architecture choice, data augmentation, domain generalization, adaptive algorithms, and representation learning.” The collection of approaches written in this way sounds a bit strange to me because the choices are neither orthogonal nor of the same kind. Architecture choice is a property of the model being trained (irrespective of the task), data augmentation heavily depends on the task, domain generalization IS a task, and representation learning is a whole area of machine learning. At this stage of the paper, I find this list very confusing and would suggest rewriting or clarifying this sentence.\n\n\n“We assume that distribution shifts arise…” Distribution (or covariate) shift is usually defined using the definition in [1], section 2.1.1. Can you please comment on whether there is difference to your definition and what it means for the approaches one would use to tackle it? From my understanding, it is actually the same because in [1], covariate shift is defined as (a) P(X) != P’(X) and (b) P (Y |X) = P’(Y |X). Here, the authors also have the condition (b) and write that P(y_1:k) != p_train(y_1:k) != p_test(y_1:k) which will result in the condition (a) P(x) != P’(X). In any case, distribution shift is a well-defined phenomenon; thus, it would be good if the authors could cite some standard definition (e.g., [1]) and comment why their definition is different (if it is different).\n\n\nPage 3: Data Augmentation. The authors write “alternatively” and I believe they refer to the previously proposed reweighting method. Writing “alternatively” means to me that they describe the methods separately first. Therefore, I am confused by the definition of p_aug since it still contains p_reweight. The authors should either (1) remove p_reweight from p_aug or (2) write “additionally” instead of “alternatively”. I would prefer (1).\n\n\nPage 3: Data Augmentation. I don’t quite agree with how data augmentation is defined here. The authors write that one can synthesize artificial data with a generative model that aims to approximate the true generative model and use this data as data augmentation. I don’t think this understanding of using data augmentation is how researchers generally think about data augmentation. I think the main driving factor for using data augmentation is to artificially increase the dataset size and to make the classifier more smooth around the data points, i.e. its decision should not change if the data distribution changes slightly. For this, researchers generally do not try to learn the true generative model, but rather use simple augmentations such as e.g., Gaussian noise, crops and horizontal flips. Later, the authors write that they use AugMix, RandAugment and AutoAugment as augmentations. All these augmentations are simple parametric functions that neither depend on the latent z nor on x since they can be used irrespective of the data and/or task. Learning the generative model is more of a GAN-like approach, so I would maybe split the paragraph into something like (1) Standard/Simple/Parametric/Heuristic data augmentation and (2) Learned data augmentation. The authors actually split the two notions of data augmentation in section 3. I would suggest just using the same split here. The authors actually claim in Takeaway 3 that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” Nevertheless, I think defining data augmentation in this way here is a bit preemptive.\n\nPage 7: “We report the mean and standard deviation over the five seeds.” I absolutely appreciate the effort done here for reporting the error over 5 runs as this is unfortunately not standard over even common in most papers.\n\n\nPage 4: “Test distribution…” please define A_i\n\n\nPage 4: “Shift 1:” Should it be P_test on the right hand side of the definition?\n\n\nPage 4: “Shift 2: Low-data drift” -> Maybe mentioning that this issue is being studied by the fairness community would be good.\n\n\nPage 4: “Shift 3: Unseen data shift – Some attribute values are unseen under ptrain but are under ptest.” Suggest to add “present” as in “but are present under p_test”.\n\n\nShift 3: “which we make explicit due to its important real world applications” -> importance\n\n\nShift 3: I do not understand the right-most inequality in Eq.2. Please explain.\n\n\nI really like the structure and content of section 2.2. The authors put a lot of effort into properly and carefully defining and disentangling different distribution shifts. I also appreciate the provided examples from DSprites to illustrate all the different shifts.\n\n\nFigure 3: I find it weird that using more samples from the true distribution seems to hurt performance in many cases. Additionally, checking Figure 11 in the Appendix, it looks like higher N is correlated with higher accuracy though it is hard to judge only looking at the image. The authors should comment on this.\n\n\nFigure 4: Does this Figure show the accuracy on the biased or non-biased datasets? Similar to Figure 11, in Figure 12, it again looks like a higher N results in higher accuracy. In Figure 12, some bars are missing.\n\n\nFigures 10, 11 and 12 are generally hard to read because there are so many bars and the only information one can extract is that there is a monotonic increase over the different models which is pretty useless since the ordering has likely been chosen such that there is a monotonic increase. Judging how N changes the bar heights is not really possible. I would suggest to maybe plot these Figures as heat plots similar to Figures 3 and 4. With heat plots, the authors could also write the resulting accuracy numbers into the heat map squares. This will allow researchers to cite their numbers in the future and also make reproducibility easier since these numbers can be easily compared against.\n\n\nIt is annoying that the Figures 10-12 in the Appendix are not plotted together and also not close to the corresponding text. I would highly suggest restructuring the Appendix to make parsing it easier, especially, since it is very long. For example, the code for the framework can be put at the very end such that it does not break the flow when looking at the additional results. \n\n\nTakeaway 3. I do not see how the presented evidence leads to the drawn conclusion that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” In B.2, the authors merely show that “No augmentation always leads to a strong boost in performance.” But it is not discussed in what way the successful augmentations approximate the true generative model. This type of analysis has done on ImageNet-C and CIFAR10-C in ref. [2] which the authors should cite here. In ref. [2], the authors define a minimal sample distance between the expected distribution shift and the added data augmentation and find strong correlation between the two. Here, a similar analysis would need to be performed in order to be able to do such a claim. In this light, tip 1 is also not grounded on evidence (although it is pretty obvious).\n\n\nI did not understand whether model selection is part of the framework code?\n\n\nThe tips in 4.2. are not really surprising (or novel). In my opinion, tip 1 is not grounded on the presented evidence since the authors did not show results that would support their takeaway 3. It is an obvious tip though and intuitively, it makes absolute sense, but the authors did not show results to support it. Considering tip 2, I think learning the perfect generative model is a hard task and I am not sure how feasible this task is for ImageNet scale datasets. Given how difficult and unstable GAN training can be, I am not sure how practical this tip is. The authors also did not show results on ImageNet, so it is hard to judge whether this tip would scale to ImageNet. Considering tip3, it is well known that pretraining on ImageNet is powerful as is has been shown in numerous previous works. As for tip4, I am not aware that DANN has been scaled to ImageNet. Additionally, DANN training (being a minimax optimization problem) can be unstable and depend on hyperparameters. Thus, I question the practicality of this tip.\n\uf0e8\tSome of these tips are not novel and the authors (merely) provide additional evidence for them. It would be nice if the authors could cite some papers where these findings have also been reported.\n\n\nReferences: \n[1] Bernhard Schölkopf et al. “On causal and anticausal learning”.\n[2] Eric Mintun et al. “On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness”\n\n\n', 'summary_of_the_review': 'I find the paper well written and easy to follow. The studied distribution shifts are thoughtfully and carefully defined. The authors perform a ton of experiments for the distribution shifts they want to study. I think the proposed benchmark / framework is a great addition to the research community as it will standardize model training and evaluation.\n\n\nPoints I would especially like to see addressed during the rebuttal (partially copied the most important points to me from the main review):\n\n1.\tFigure 3: I find it weird that using more samples from the true distribution seems to hurt performance in many cases. Additionally, checking Figure 11 in the Appendix, it looks like higher N is correlated with higher accuracy though it is hard to judge only looking at the image. The authors should comment on this.\n\n2.\tFigure 4: Does this Figure show the accuracy on the biased or non-biased datasets? Similar to Figure 11, in Figure 12, it again looks like a higher N results in higher accuracy. In Figure 12, some bars are missing.\n\n3.\tTakeaway 3. I do not see how the presented evidence leads to the drawn conclusion that “Heuristic augmentation improves generalization if the augmentation describes an attribute.” In B.2, the authors merely show that “No augmentation always leads to a strong boost in performance.” But it is not discussed in what way the successful augmentations approximate the true generative model. This type of analysis has been done on ImageNet-C and CIFAR10-C in ref. [2] which the authors should cite here. In ref. [2], the authors define a minimal sample distance between the expected distribution shift and the used data augmentation and find strong correlation between the two. Here, a similar analysis would need to be performed in order to be able to do such a claim. In this light, tip 1 is also not grounded on evidence (although it is pretty obvious).\n\n4. Contribution 1: ""We propose a framework to define when and why we expect methods to generalise."" I think the authors addressed the ""when"" question with their benchmark, but not really the ""why"" question. Can the authors comment on which results lead them to conclude why a certain method should generalize?\n\n\n-> I am also happy to discuss any other points I mentioned but these would be the most important ones to me.\n\n\n\n\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': '===================\nSummary: \nThe robustness to distribution shifts is one of the biggest concerns in deploying machine learning systems. This paper summarizes all distribution shifts into three prototypes: spurious correlation, low-data drift, and unseen data shift, together with two additional conditions: label noise and dataset size. Other more complex distribution shifts can be regarded as compositions of these components. After that, they evaluated19 methods, spanning 5 categories: architecture choice, data augmentation, domain generalization, adaptive algorithms, and representation learning, in 6 datasets. In the end, they provided a number of practical tips, useful conclusions, and promising directions for future researchers in this field.', 'main_review': ""===================\nStrengths:\nIn my opinion, the efforts to unify all the studies of distribution shifts into one general framework is the greatest contribution made by this paper. Recently, plenty of distribution shift settings become popular in various machine learning fields, such as long-tailed classification/detection, domain adaptation, out-of-distribution generalization, etc. However, they are all studied independently, despite the fact that a similar essence is shared by them under the surface. This framework puts them into the same testbed, which allows us to evaluate the scope and limitation of different methods in all kinds of settings and data types. Besides, the authors also provided a number of practical tips and useful conclusions based on massive experiments under diverse settings and datasets.\n\n===================\nWeaknesses:\n- However, some conclusions in this paper violate my own observations. For example, the heuristic data augmentation method, Rand Augmentation, is found to be very helpful in long-tailed classification (a specific type of low-data drift) and some label noise tasks. It can consistently improve 2-5 points of accuracy across different datasets and settings. Yet, Rand Augmentation looks to be the worst method in Figures 3 and 4. One possible explanation is that datasets I was using are all from real-world images, so the Rand Aug can reveal the underlying generative model p(x|y1:K), while the datasets used in this paper are mostly synthetic images or medical images, where Rand Aug didn't approximate the true underlying generative model p(x|y1:K). In order to prevent the conclusions in this paper from misleading future researchers, I suggest the authors summarize datasets into different types (like synthetic dataset, medical dataset, and normal dataset) and investigate them separately.\n- Another concern is that should the ImageNet pretraining be used as a valid method for the study of robustness to distribution shifts? It may violate the settings of SC, LDD, and UDS by introducing samples from unknown distributions. For example, unseen data distribution in the given task and dataset may be compensated by the ImageNet dataset. That's why pretraining is usually forbidden in the tasks like OOD image classification or long-tailed classification. As a result, the superior performance of pertaining is not just frustrating but also (could be) unfair."", 'summary_of_the_review': '===================\nJustification:\nI recognize the value of the proposed comprehensive framework and all systematic studies made by this paper. I would be more than willing to accept the paper once my concerns are properly addressed.\n', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'A Fine-Grained Analysis on Distribution Shift', 'authorids': ['~Olivia_Wiles1', '~Sven_Gowal2', '~Florian_Stimberg1', '~Sylvestre-Alvise_Rebuffi1', 'iraktena@google.com', '~Krishnamurthy_Dj_Dvijotham1', '~Ali_Taylan_Cemgil2'], 'authors': ['Olivia Wiles', 'Sven Gowal', 'Florian Stimberg', 'Sylvestre-Alvise Rebuffi', 'Ira Ktena', 'Krishnamurthy Dj Dvijotham', 'Ali Taylan Cemgil'], 'keywords': ['robustness', 'distribution shifts'], 'abstract': ""Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets.  Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. \nCode is available at github.com/deepmind/distribution_shift_framework.\n"", 'one-sentence_summary': 'We investigate and analyse the robustness of a variety of methods under distribution shifts using our flexible experimental framework.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wiles|a_finegrained_analysis_on_distribution_shift', 'pdf': '/pdf/6be366539738706234ad0b104ed82361a3c5f6e7.pdf', 'supplementary_material': '/attachment/98bd936d076633fae496d57988805e933efaca79.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/a-fine-grained-analysis-on-distribution-shift/code)', '_bibtex': '@inproceedings{\nwiles2022a,\ntitle={A Fine-Grained Analysis on Distribution Shift},\nauthor={Olivia Wiles and Sven Gowal and Florian Stimberg and Sylvestre-Alvise Rebuffi and Ira Ktena and Krishnamurthy Dj Dvijotham and Ali Taylan Cemgil},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Dl4LetuLdyK}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Boris Oreshkin', 'Florent Bocquelet', 'Felix G. Harvey', 'Bay Raitt', 'Dominic Laflamme']",ICLR,ProtoRes_ Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics,https://iclr.cc/virtual/2022/oral/6540,2022," Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code.",Oral 1: AI Applications,https://openreview.net/pdf?id=s03AQxehtd_,https://openreview.net/forum?id=s03AQxehtd_,s03AQxehtd_,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes a novel representation for pose authoring, and was uniformly lauded by all reviewers.  The AC concurs this paper is far above the threshold for acceptance at ICLR.'}, {'title': 'Re: Feedback to the full constraints issue', 'comment': 'We would like to thank Reviewer J8Sg for the additional feedback! To address the point raised by the reviewer, we will additionally report the quantitative result for ProtoRes in the full constraints scenario (the positions and rotations of all joints are given) in the supplementary of the revised manuscript.'}, {'title': 'Re:Re: Clarification about comment 9 ', 'comment': 'Thanks for the feedback. It is clear to me now. '}, {'title': 'Feedback to the full constraints issue', 'comment': '1. Full constraints.\nYes, it is the scenario where the positions and rotations of all joints are given.\n\n2. The results for full constraints. \nI agree that the full constraints case is not a typical case for IK, and in that case, the final IK may generate a very accurate result. But I still think the quantitative result of the proposed protoRes method with full constraints is meaningful. The result may help the readers to learn more about the limitations of the proposed method.'}, {'title': 'Thank you!', 'comment': 'We would like to sincerely thank Reviewer cvbx for carefully reading our manuscript and response, providing insightful comments and additional clarifications as well as for updating their score.'}, {'title': 'Re: Clarification about comment 9', 'comment': 'We thank Reviewer J8Sg for additional question. The dataset captures two people, one male and one female, both are professional performers, capable of delivering high dynamic range movements in a variety of scenarios, whose precise specifications were provided in the data collection protocol (see Table 6 in Appendix D). The MOCAP data from both subjects were retargeted to a standardized skeleton, which is a standard practice for MOCAP data collection. Our dataset achieves the following objectives that we set forth for our data collection exercise:\n- Collect data that have balanced coverage of male and female poses\n- Cover actions in a wide dynamic range of energy (low, neutral, high, injured)\n- Cover diverse variety of activities\n\nWe will clarify it in the revised version of the manuscript accordingly.'}, {'title': 'FinalIK does not support full constraints', 'comment': 'We thank Reviewer J8Sg for additional clarifications. We would like to ask the reviewer to provide extra feedback based on the additional pieces of information we include below.\n\n- First, we are not completely clear about the exact definition of the “full constraints” scenario. If this is the scenario in which the model is given positions and rotations of all joints, a non-learnable inverse kinematics solution capable of doing IK on all joints will produce the output with zero error, **provided that the pose is feasible** (i.e. bone lengths are respected). Essentially, the IK system in this case would deliver an identity transform, subject to arithmetic round-off errors. This implies that if such a system could be available, measuring its performance would not even be necessary. It would only be of interest to measure the accuracy of ProtoRes operating on a fully specified pose input. \n- Second, the “full constraints” experiment comparing FinalIK against ProtoRes is not feasible and we cannot possibly implement it, because by design FinalIK does not support the mode of operation with full constraints. This is a limitation of the system outlined in its documentation: http://www.root-motion.com/finalikdox/html/page8.html. We present a few excerpts from the documentation supporting the statement for the convenience of the reader. “FullBodyBipedIK does not have effectors for the fingers and toes… Using 10 4-segment constrained CCD or FABRIK chains to position the fingers however is probably something you don\'t want to waste your precious milliseconds on.”; “FullBodyBipedIK does not have elbow/knee effectors.”; “FullBodyBipedIK does not rotate the shoulder bone when the character is pulled by the hand.” Additionally, the actual set of potential effectors provided by the package in the Unity environment is even more limited. Realistically, the configuration available with FinalIK package provides us with at most 13 effectors total, including positional: body trunk (chest point), 2 hands, 2 feet, 2 shoulders, 2 thighs and rotational: 2 hands, 2 feet.\n- Third, we would like to point out that the aforementioned limitations of FinalIK stem from the typical use cases (i.e. resolution of sparse inputs) in which IK systems tend to be used. The underlying intent behind IK systems is to reduce the number of user inputs required to produce a believable pose. Therefore, the operation with ""full constraints"" seems to represent a use-case neither for FinalIK nor for ProtoRes.'}, {'title': 'Clarification about comment 9', 'comment': ""Thanks for the detailed feedback. The miniAnonymous dataset is captured by a MOCAP studio, so I'd like to ask how many persons are captured for the dataset?""}, {'title': 'About the comparison of the proposed method and FinalIK', 'comment': ""Thanks for the author's feedback. It is better to show the quantitative results of both methods with full constraints for clarification.""}, {'title': 'Clarification about VPoser', 'comment': 'I thank the authors for the detailed response. Some clarification about VPoser: I was suggesting using it as a way to interpolate between close samples based on nearest neighbors, not as the tool for finding samples on its own. I think the point about the variable number of inputs and reference frame difficulty is valid. '}, {'title': 'Author response to Reviewer J8Sg 1/4 ', 'comment': 'We would like to sincerely thank **Reviewer J8Sg** for careful reading of the manuscript and a positive and encouraging review of our work. We address the points raised by the Reviewer in detail below. Please do not hesitate to post additional comments and questions, we will be more than happy to address them.\n\n**Comment 1**\n(1) The proposed method is a new way for human animation, so the comparison with FinalIK should be more detailed. The speed of both methods should be clearly shown in Table 1.\n\n**Response 1**\nTo address the comment, we have added the quantitative inference speed comparison for the two methods to Table 1, as suggested by the reviewer. Our setup for the pose authoring inference scenario is NVIDIA Geforce RTX 2080 Super, Intel Core i7 2.30 Ghz. ProtRes inference time per pose solve, 5 positional effectors + 1 head lookat scenario, CPU (proprietary CPU based accelerator engine, multi-threaded with Burst compilation): 13.5 ms; GPU (OnnxRuntime): 5.5 ms. FinalIK inference time per pose solve, 5 positional effectors for full-body constraint + 1 head for look-at constraint scenario, CPU (single-threaded): 0.3 ms. Additionally, based on our practical use of the integrated system, we observe that ProtoRes runs at least at 100 fps speed, making its use in the context of the pose editing tool smooth and seamless: the user of the system does not experience any delays that could be caused by neural network computations. This is also obvious from the demo video 4_Final_IK_comparison.mp4, which was shot in real time, and compares ProtoRes and FinalIK side-by-side. In this video, we do not observe any delays on the ProtoRes side when running both methods at the same time in the same setup in real time. Our conclusion is that for the pose authoring applications ProtoRes offers a computationally viable solution that can be run seamlessly on both CPU and GPU.'}, {'title': 'Author response to Reviewer J8Sg 2/4', 'comment': '**Comment 2**\nAs mentioned in the paper, the proposed method satisfies the constraints approximately (P.9 The limitation of the current work). The construction results for both methods with full constraints may help to clarify the limitation.\n\n**Response 2**\nWe believe that our demo video 4_Final_IK_comparison.mp4 already reveals the limitation qualitatively. To better focus the reader’s attention on the qualitative results in this demo, while discussing the limitation in detail, we have updated the text discussing the limitation in Appendix L, Limitations as follows. \n\n“Constraints are not satisfied exactly, as opposed to the conventional systems. The limitation can be observed in the demo video 4_Final_IK_comparison.mp4, which compares ProtoRes and FinalIK side-by-side. For example, at seconds 18-21 we can see the chest position is not satisfied exactly by ProtoRes and the generated left foot position traverses the vicinity of the constraint as the user changes the position of the left foot. Another example like this can be seen at seconds 35-41, in which the left foot effector is not satisfied exactly by ProtoRes. It is interesting that for the rather peculiar configuration of effectors, FinalIK produces outputs that tend to severely twist the joints, perhaps beyond the capabilities of an average human. At the same time, ProtoRes tends to trade the precision of following the individual effectors in isolation with the plausibility of the overall pose. This is the price to pay for the ability of the model to inject the data-driven inductive bias that can be used to reconstruct pose from very sparse inputs. This could be mitigated using a conventional solver on top of the trained model. In this case, the model would produce a globally plausible pose, whereas the solver could only do the final pass to strictly satisfy certain constraints. Also, to provide additional flexibility in solving some of the constraints more strictly than the others, our model provides an effector tolerance mechanism that can help the user trade off the strictness of satisfying certain effectors vs. some others.”\n\n**Comment 3**\nThe generalization ability of both methods should also be investigated. How about performing both methods on non-human skeletons with similar structures.\n\n**Response 3**\nUnfortunately, the detailed investigation of the generalization ability of FinalIK is outside of the scope of our current paper as it is not our contribution. Generally speaking, FinalIK allows manually defining custom effectors and custom IK chains to accommodate for new skeletons via a tedious hand crafted process. On the other hand, the generalization ability of our core contribution ProtoRes has been investigated in the current work. We provide the demo video 7_Quadruped.mp4 and associated Appendix K.7 describing it, in which we demonstrate the results of learning a neural representation of the dog skeleton using ProtoRes. Note that we did not tune any hyperparameters or change our model training code beyond the dataset loader part to accomodate for the new data source. The demo shows successful learning results with the quadruped dataset supporting the same flexibility/realism and showcasing similar features as our human model. In our view, this provides evidence to support the promising generalization potential of our model beyond human skeletons. '}, {'title': 'Author response to Reviewer J8Sg 3/4', 'comment': ""**Comment 4**\n(2) Although this paper has provided very comprehensive results, some comparisons and analyses should be further shown in the experiment part.\n\n**Response 4**\nWe thank Reviewer J8Sg for their positive assessment of our work and provide detailed responses below.\n\n**Comment 5**\nIn Table 1, the inference speed & parameter numbers should be provided for machine learning-based methods.\n\n**Response 5**\nIn the revised manuscript we have provided the inference speed and and parameter counts in Table 1, as reviewer suggested. In particular, the inference speed of FinalIK on CPU is 0.3ms and the inference speed of ProtoRes on GPU is around 5.5 ms and on CPU 13.5 ms. Our setup for the pose authoring inference scenario is NVIDIA Geforce RTX 2080 Super, Intel Core i7 2.30 Ghz.\n\n**Comment 6**\nQuantitative evaluation for the look-at-loss should be presented to prove the effectiveness.\n\n**Response 6**\nIn our training and evaluation pipeline, we currently log only the losses currently presented in the manuscript. Therefore, collecting the metrics requested by Reviewer J8Sg will require additional code modifications and rerunning the training sessions. We will include the look-at-loss results in the final version of the revised manuscript. In the meanwhile, our qualitative results provide evidence to show the effectiveness of the proposed look-at loss.\n\n**Comment 7**\nTable 2 shows the ablation study for GPD, but how about only using GPD (without IKD) for the final result.\n\n**Response 7**\nUsing GPD without IKD is not feasible, because GPD predicts joint positions without respecting skeleton constraints, such as bone lengths, therefore its output cannot be used for pose design directly. Additionally, positions alone are not sufficient to model bone twisting around its axis, which is desirable in the upstream applications. Finally, the task of predicting only joint positions is significantly easier than the task of predicting full pose satisfying skeleton constraints, so results would not even be directly comparable and rotation output would be missing for rotation metrics computation.\n\n**Comment 8**\n(3) The dataset is one of the most important contributions in this paper. It is better to show more details about the dataset, especially the 'miniAnonymous' dataset.\n\n**Response 8**\nWe thank the reviewer for pointing this out, we have addressed the concern by extending Appendix D with additional information about datasets, including some descriptive statistics and more detailed description of the contents. Detailed responses to related questions follow.\n\n**Comment 9**\nHow many instances are included in the dataset?\n\n**Response 9**\nWe are uncertain about what is meant by ‘instances’. We list here some potentially relevant information, but we are happy to expand on other aspects given clarifications on the term ‘instance’.\n\nThe miniMixamo dataset contains 33 676 poses, and the miniAnonymous dataset contains 96 666 poses. The miniMixamo dataset contains poses from 1598 clips, while the miniAnonymous dataset contains poses from 1776 separate clips. Note that these details are available in Section 3.1 of the manuscript.\n\n**Comment 10**\nWhat kinds of actions for each instance are captured?\n\n**Response 10**\nWe have added the hierarchy of action styles and categories captured in the miniAnonymous and miniMIxamo datasets in Appendix D, especially Tables 6, 7. The following text has been added to address the concern: “The following action scenarios were used to collect MOCAP sequences in miniAnonymous. First, the following locomotion types: compass crouch, compass jogs, compass runs, compass walks were collected for female and male subjects under high energy, low energy and injured scenarios. The same locomotion types were collected under neutral energy feminine and neutral energy macho scenarios. Furthermore, under neutral energy generic scenario, for both female and male subjects, we collected following action, object and environment interaction types: archery, bokken fighting, calisthetics, door interactions, fist fighting, food, handgun, hands, knife fighting, locomotion, longsword fighting, phone, place, railing interactions, rifle, seated interactions, shotgun, standings, sword, wall. Among the latter categories, locomotion and handgun had following more detailed subdivisions. Locomotion: compass crawls, compass crouch, compass jogs, compass rifle aim walks, compass rifle crawl, compass rifle crouch, compass rifle jogs, compass rifle runs, compass rifle walks, compass runs, compass walks, rifle idles, walk carry heavy backpack, walk carry heavy sack, walk carry ladder, walk dragging heavy object. Handgun: downwards, level, upwards, verticals.”\n\n""}, {'title': 'Author response to Reviewer J8Sg 4/4', 'comment': ""**Comment 11**\nDoes the motion style have a big difference with the 'miniMixamo' dataset?\n\n**Response 11**\nIn Appendix D, we have included the histograms summarizing top 30 tags appearing in the descriptions of the individual frames that comprise both datasets. The analysis of the histograms shows that there is significant difference in the styles of motion captured in the datasets. The following text has been added to Appendix D. “The distributions of 30 most popular tags across the frames of the original dataset used to build miniMixamo and miniAnonymous are shown in Figure 5. It is clear that the two datasets cover some common activities such as walking and idling, for example. Additionally, there are numerous categories the two datasets emphasize separately. For example, miniMixamo focuses a lot on fighting and handling guns, whereas miniAnonymous has more neutral energy activities, provides extensive labeling of male/female poses as well as covers scenarios of handling various objects and food.” Tables 6, 7 provide additional information about motion types in both datasets. Particularly, Table 6 describing our dataset provides very fine grained specification of motion types based on our data collection protocol design.\n\n**Comment 12**\n(4) Some descriptions of the proposed unity tool should be presented. Does it just show the results of the neural network? Or some animation functions in the unity is also used as post-processings?\n\n**Response 12**\nWe thank the reviewer for the insightful comment. The Unity tool that is used to produce demo videos and qualitative pictures show the **raw neural network outputs without applying any post-processing**. This is done to ensure fair comparison of different methods and avoid any misleading results due to post-processing. We have clarified this in the manuscript when discussing our demos.\n\n**Comment 13**\n(1) What is the minimal number of the input constraints?\n\n**Response 13**\nThe minimal number of constraints is one.\n\n**Comment 14**\n(2) What kind of results will be output if a sequence of the ambiguous input constraints is given?\n\n**Response 14**\nIn a sense, all our results are based on ambiguous constraints, because we test the system based on sparse effector inputs, which provide very little information about the final pose. Our results indicate that the learned IK model is able to successfully reconstruct plausible poses using minimal inputs. For example, Figure 3 demonstrates how pose can be designed adding progressively more effector inputs. The pose guided with a small number of effectors (e.g. two effectors in the case of Figure 3, left) tends to be very generic, picking up a prominent pattern from the underlying dataset. Adding effectors (moving left to right in Figure 3) helps make the pose more and more specific. In general, our demos show that the learned pose manifold is reasonably smooth and stable: small perturbations in effectors result in small perturbations of the resulting pose.\n\n**Comment 15**\n(3) There are many interesting comparisons in the demo videos, so it is better to show some of them in the main paper.\n\n**Response 15**\nWe would like to thank the Reviewer for their positive assessment of our demo videos. Unfortunately, this year there is no extra page provided for addressing the comments. The current manuscript has already undergone a few compacting iterations in the attempt to strike the balance between being well-motivated, self-contained from the technical perspective as well as providing some intuitive visuals and supporting claims with strong quantitative evidence. Unfortunately, we have not been able to identify the pieces in the current manuscript that could be removed to provide space for additional qualitative results. We will be more than happy to look at this issue again if Reviewer J8Sg provides additional opinion on which parts of the main body could be removed to accommodate the new material, without compromising the clarity of the manuscript. \n\n**Comment 16**\n(4) In Table 5 and Table 6, there are some distinct results, i.e. 1.27 e-6. Are those typoes?\n\n**Response 16**\nWe sincerely thank the Reviewer for pointing out the typos. We have reviewed the tables and fixed the typos in the current revision of the paper.""}, {'title': 'Author response to Reviewer 5Kos 1/3 ', 'comment': 'We would like to sincerely thank **Reviewer 5Kos** for careful reading of the manuscript and a positive and encouraging review of our work. We address the points raised by the Reviewer in detail below. Please do not hesitate to post additional comments and questions, we will be more than happy to address them.\n\n**Comment 1**\nThe organization of the paper could be further improved. For example, the definition of 6D rotations is only explained in Sec 1.2. It would be better to mention it the first time this term appeared, otherwise, it is slightly confusing. Besides, it is suggested to merge the five contributions into three or four. e.g. merge the fourth and fifth contributions.\n\n**Response 1**\nAs suggested by the reviewer, we now define a 6D rotation representation that we employ the first time it is mentioned and we merge the fourth and fifth contributions. \n\n**Comment 2**\nSome important details are missing. a). How are the metrics L^{det}{gpd}, L^{det}{ikd}, L^{det}_{loc} defined and what are the units of them? b). How are the two datasets split into train and evaluation?\n\n**Response 2**\n\na) Indeed, we have moved the definitions of L^{det}{gpd-L2}, L^{det}{ikd-L2} and L^{det}{loc-geo} to appendices, together with Algorithm 1, without noticing that this caused some loss of clarity in the main manuscript. We thank the Reviewer for pointing this out. In the revised manuscript, we have added explicit definitions of the metrics to the Appendix E and referenced it in Section 3.2 while defining the training and evaluation setup. L^{det}{gpd-L2}, L^{det}{ikd-L2} are mean squared errors, therefore their units are in meters squared and L^{det}_{loc-geo} is the geodesic error, measured in radians.\n\nb) The dataset splitting procedure is described in Section 3.1: Datasets. In particular, for miniMixamo it is defined as “The resulting dataset is partitioned at the clip level into train/validation/test splits (with proportion 0.8/0.1/0.1,respectively) by sampling clip IDs uniformly at random. Splitting by clip makes the evaluation framework more realistic and less prone to overfitting: frames belonging to the same clip are often similar. At last, the final splits retain only 10% of randomly sampled frames (miniMixamo has 33,676 frames total after subsampling) and all the clip identification information (clip ID, meta-data/description, character information, etc.) is discarded.” For miniAnonymous the splitting procedure is the same as per “Then we create a dataset of a total of 96,666 subsampled frames following exactly the same methodology that was employed for miniMixamo.”'}, {'title': 'Author response to Reviewer 5Kos 2/3', 'comment': '**Comment 3**\nIt would be great if the paper could show more qualitative results. For example, the qualitative comparison between the proposed method and the previous methods.\n\n**Response 3**\nWe thank the reviewer for their interest in our qualitative results. To address the comment, in the revised manuscript we included an additional Appendix J presenting a few examples comparing FinalIK and Transformer against ProtoRes and discussing in detail the nuances of posing results of the three approaches. These complement our supplementary videos that contain much richer qualitative comparisons. Unfortunately, this year there is no extra page provided for addressing the comments. The current manuscript has already undergone a few compacting iterations in the attempt to strike the balance between being well-motivated, self-contained from the technical perspective as well as providing some intuitive visuals and supporting claims with strong quantitative evidence. Unfortunately, we have not been able to identify the pieces in the current manuscript that could be removed to provide space for the contents of the new Appendix J. We will be more than happy to look at this issue again if Reviewer 5Kos provides additional opinion on which parts of the main body could be removed to accommodate the material in Appendix J, without compromising the clarity of the manuscript.\n\n**Comment 4**\nIt would be great if the paper could show more analysis on the relation between inputs and model performances. For example, how is the performance influenced by the number of input joints and input types (joint positions or joint rotations which are better)?\n\n**Response 4**\nWe thank the reviewer for the insightful comment. To address the comment we will include an additional Appendix showing the effect of the number and type of effectors on the loss terms. From our previous preliminary experiments we know for a fact that increasing the number of effectors leads to improved accuracy, which is an intuitively appealing result. In our current implementation we are only logging the aggregate results, so collecting the associated statistics will require additional code modifications and rerunning the training session. So we need some extra time to produce the results. For the effect of the type of effector, we have not conducted experiments to measure the effects. Our randomized evaluation framework uses randomly sampled effector configurations, therefore disentangling the effector effects requires additional regression analysis. Again, this requires extra coding and time to compute the results and we need some extra time to provide them. We will include these results in the final version of the revised manuscript. \n\nMeanwhile, some observations from our practical experience with these models have emerged:\n\n\n- The flexible number of inputs to the model allows for a user to easily adapt the inputs to reach the desired level of precision. In Figure 3 of the manuscript, the four leftmost images show this process, where new effectors are iteratively added to match a desired pose evermore precisely. \n\n- Although we have no precise results yet on the final metrics with respect to input types (e.g. positions vs rotations), a universal conclusion that stems from our own use of the model and from skilled animators trying the tool is that positional effectors are more intuitive to use and accelerate initial pose authoring compared to rotations. Rotations and look-at targets tend to be used to fine-tune a pose authored first from positions. In other words, all types of effectors are useful and act synergistically: positions are often preferred to quickly author a base pose, rotations/lookat are often used in the finetuning phase.'}, {'title': 'Author response to Reviewer 5Kos 3/3', 'comment': ""**Comment 5**\nI have a concern about the overall framework. The paper resolves an ill-posed problem, i.e. there could be multiple feasible outputs given the one input. However, the paper uses the encoder-decoder framework which is only able to generate one output given the input. Therefore, I am wondering would it be better to change the current AE architecture to VAE so that multiple outputs could be produced? I would like to hear about the authors' options.\n\n**Response 5**\nWe thank the reviewer for pointing this out. Indeed, generative/distributional modeling sounds like a very interesting venue for future research. A few detailed points follow.\n\n- We would like to point out that architecturally, our approach can be modified straightforwardly into a generative model by predicting $\\mu$ and $\\sigma$ from the pose embedding, incorporating the KL divergence term and decoding pose from the associated latent variable, for example. However, there are a couple of problems that need a significant amount of work to make the generative approach viable and useful in our application scenario. \n- First and foremost, we need to develop effective user interface tools which would allow the user to explore the random outputs that the generative model might be able to produce without overwhelming the user with noise. We need to understand how to best present the noisy results to the user (density maps, a few potential poses, one pose per click etc.) and how to enable the user to navigate/select generated pose samples. This is an exciting and non-trivial problem in itself. Unfortunately, until it is solved, the practical use of generative models for our specific problem is very limited. The focus of our current work is to show that the problem of producing realistic human poses using a small set of heterogeneous variable user inputs has a computationally efficient, elegant and practical solution via learned inverse kinematics and neural modeling approaches. We believe that our results validate this and open up opportunities for future research, including generative modeling of partially defined human poses.\n- Second, the performance of generative models would need to be benchmarked to assess the quality of distributional modeling/predictions and provide an objective benchmark to support future model development in this area. Many existing works focus on demonstrating the prediction accuracy of generative models or evaluating the likelihood values of their predictions. We believe that more work needs to be done in this direction to make the assessment of generative models more thorough and objective. For example, high accuracy can be achieved by approaches that do not rely on generative modeling. Likelihood value assessment is not ideal either: the model that only generates samples with high likelihood (and scores well) basically is bound to produce only the most common values, which may not be attractive from the user perspective. The model that produces too many samples in the tails of distribution might not be good either, because it will be hard to steer by the user and will produce too many samples that do not fit the user's guidance signals. More research needs to be done in this direction to identify suitable ways of quantifying the diversity of samples, the adequacy of the overall distribution fit as well as the usefulness of the randomly generated samples for the user of the system. We believe this is a very exciting area for future research.""}, {'title': 'Author response to Reviewer cvbx 1/5', 'comment': 'We would like to sincerely thank **Reviewer cvbx** for careful reading of the manuscript and a constructive and encouraging review of our work. We address the points raised by the Reviewer in detail below. Please do not hesitate to post additional comments and questions, we will be more than happy to address them.\n\n**Comment 1**\nVariable input length: while variable input length is mentioned as a feature of the proposed system, it is not demonstrated via demo or experiments. Since there is only a fixed number of joints, I presume the system can use a fixed number of inputs (all the joints) and use a mask to indicate whether there is an input for that specific joint?\n\n**Response 1**\nWe would like to point out that the variable input length functionality is demonstrated via both demos and experiments. \n\n- Our architecture natively supports variable inputs of various types\n- Figure 3 provides qualitative demonstration of the poses that can be achieved using 2,4,5,7,4,7 effectors (left to right) of different types (position, look-at, rotation)\n- Demo video 1_ProtoRes_Demo.mp4 demonstrates that the effectors can be added and removed freely by the user, showing posing results with variable number and types of input effectors. To clarify, our ProtoRes model does not rely on masking of inactive effectors to process inputs, but performs parallel processing of active effectors, and combines them into a fixed-size pose embedding through the Prototype-Subtract-Accumulate process.\n- Our empirical results are based on the random evaluation framework, testing variable number of input effectors and variable effector types (see Section 3.2 and Appendix E). “The evaluation framework tests model performance on a pre-generated set of seven files containing 6,7...12 effectors respectively. Metrics are averaged over all files, assessing the overall quality of pose reconstruction in a scenario with sparse variable inputs.”\n\nAdditionally, we have used the approach in which the missing outputs are masked with learnable placeholders as a baseline. This is exactly what the Maksed-FCR baseline does (see Section 3.3, first paragraph) “The first baseline, Masked-FCR, is a brute-force unstructured baseline that uses a very wide J · 3 · 7 input layer (J joints, 3 effector types, 6D effector data and 1D tolerance) handling all effector permutations. Missing effectors are masked with 3 · J learnable 7D placeholders. MaskedFCR has 3 encoder and 6 decoder blocks to match ProtoRes.” Our results in Table 1 clearly indicate that this approach is inferior to ProtoRes, especially on the random benchmark, which tests the algorithms using a variable number of effectors mixing different effector types.\n'}, {'title': 'Author response to Reviewer cvbx 2/5', 'comment': ""**Comment 2**\nMissing simple baselines: based on the problem setup, one simple baseline is to use nearest neighbors to retrieve the closest pose from the dataset that satisfies the given input. Since MoCap data is continuous, it is highly possible that some realistic pose already exists in a large enough human motion database (such as AMASS [3], around 3 million frames). If no exact match exists, another naive approach could be finding close samples and interpolating between them either through simple linear interpolation or learned embeddings such as VPoser [4].\n\n**Response 2**\nWe thank the reviewer for the insightful comment and provide a detailed response below.\n\n- We would like to note that in our work we are looking for strong generalization, therefore the dataset is split into train/validation/test by sampling sequence ids first. Thus frames from the same sequence can only be part of one of the splits. Therefore, the generalization ability of the nearest neighbor approach will not be as high as one might expect if we could rely on the continuous nature of MoCap data and adjacent frames occuring in train and test splits. \n- The nearest neighbor approach is popular in motion completion and interpolation. In this case the notion of “exact match” is straightforward because (1) inputs are always the same, and (2) the frame of reference is always defined in the same way (e.g. root transform of the current frame). In our case, we solve a more challenging problem, in which any arbitrary combination of joints can be used as inputs. This makes the frame of reference ambiguous (e.g. if we don't use the root effector's full transform) and changing for every effector combination, making the “exact match” very hard to define for all possible effector combinations. \n- Moreover, the efficient nearest neighbor inference algorithm based on kd-tree would require a construction of the tree for each viable combination of input joints, implying combinatorial complexity. Similarly, VPoser [4] only models a fixed size input using fully connected MLP (please refer to Sections 3.1, 3.3 and Supplementary Section 8 in [4]). As such, a latent space learned by VPoser is not applicable for solving our problem. If we chose to use it naively and learn a separate model for every possible combination of inputs, we will face the same problem as the nearest neighbor, i.e. we will have to fit a combinatorial number of VPoser models for all possible input joint combinations. \n- We additionally handle the complexity of solving the problem with heterogeneous inputs: position, rotation, look-at, in arbitrary combinations and numbers. If we try to implement this using the nearest neighbor approach, we will have to face a few more problems. First, we will have to assign weights to position, rotation and look-at distance metrics and run grid search to figure out the best configuration of weights for each distance term. To achieve this, we will have to run a computationally involved grid search procedure to identify the optimal weight for each distance term. Second, since the distance term for this problem will be a custom composite function, we will have to work on a custom implementation of the kd-tree (see e.g. https://stackoverflow.com/questions/48042408/kd-tree-with-custom-distance-metric). Third, even if we code it successfully and resolve the inference speed problems, we will still have to face the combinatorial expansion of model space, which will be even bigger when we expand the space of possibilities with effector type in addition to the arbitrary joints.\n\nTo sum up, we argue that the baselines proposed by Reviewer cvbx are neither simple nor feasible in order to solve the full extent of our problem. VPoser [4] is not applicable to solve our problem, because it can only work with the fixed inputs covering the entire set of joints of the body and it can only handle positional inputs. We believe that the baselines we present in the paper correspond well to the complexity of the task we are solving and adequately represent the existing technology that can be straightforwardly applied to solve it.""}, {'title': 'Author response to Reviewer cvbx 3/5', 'comment': '**Comment 3**\nMissing motivation and analysis for the proposed ""ProtoRes"" architecture: as mentioned in the strength section, it is interesting the prototypical networks are used for this task. However, it is not well-motivated and not fully studied why the proposed network architecture is chosen for this task. Given the recent progress in Transformers, it is possible that the Transformer requires more data (which a dataset like AMASS can provide) but can learn a better embedding space for the pose synthesis/infilling task.\n\n**Response 3**\nThe initial motivation for using the prototypical network for this task was based on the fact the prototypical networks had been shown to be effective at encoding sparse semantic information. Since in our task we have only sparse pieces of information, it felt logical to pursue this powerful and simple approach. We have outlined this in the introduction. Furthermore, Section 2 further details the motivation in the discussion following equations (2)-(5). First, the prototypical approach is used to represent the pose defined by sparse effectors, which is a use case resonating well with a few-shot learning scenario and implementing the inductive bias that when a pose is specified by a few effectors, the mean of their representations is the most concise representation of the pose. We also show that extending such an approach using residual stacking methodology results in significant accuracy improvements in our problem. Second, in each block the representation of the entire pose is subtracted from the representations of individual effectors, implementing the inductive bias that the information in individual effectors is only valuable when it is different from what is already stored in the embedding of the entire pose. Finally, pose representation is accumulated across residual blocks (prototype across block representations) effectively implementing skip connections from very early layers. This is a computationally effective way of implementing distant skip connections. Empirically, we show that our approach is superior with respect to Transformer, both computationally and in terms of accuracy on two different datasets. In our view this is a significant contribution given our goal of democratizing AI assisted artistry and making associated tools accessible to everyone, whereas the Transformer based approach might be computationally prohibitive (10 times training cost compared to ProtoRes in our setup), especially with larger datasets. Moreover, we ran extensive ablation experiments studying different aspects of the proposed architecture, including, among other things, the experiments showing the advantages of the proposed  Prototype-Subtract-Accumulate residual architecture as well as the computationally efficient encoder/decoder design. \n'}, {'title': 'Author response to Reviewer cvbx 4/5', 'comment': ""**Comment 4**\nMissing generative aspect: for such an under-constrained problem, it is natural to assume that generative models such as normalizing flow [5] or VAE [6] are a more suitable model for generating diverse pose that conforms to the same user input.\n\n**Response 4**\nWe thank the reviewer for pointing this out. Indeed, generative/distributional modeling sounds like a very interesting venue for future research. A few detailed points follow.\n\n- We would like to point out that architecturally, our approach can be modified straightforwardly into a generative model by predicting $\\mu$ and $\\sigma$ from the pose embedding, incorporating the KL divergence term and decoding pose from the associated latent variable, for example. However, there are a couple of problems that need a significant amount of work to make the generative approach viable and useful in our application scenario. \n- First and foremost, we need to develop effective user interface tools which would allow the user to explore the random outputs that the generative model might be able to produce without overwhelming the user with noise. We need to understand how to best present the noisy results to the user (density maps, a few potential poses, one pose per click etc.) and how to enable the user to navigate/select generated pose samples. This is an exciting and non-trivial problem in itself. Unfortunately, until it is solved, the practical use of generative models for our specific problem is very limited. The focus of our current work is to show that the problem of producing realistic human poses using a small set of heterogeneous variable user inputs has a computationally efficient, elegant and practical solution via learned inverse kinematics and neural modeling approaches. We believe that our results validate this and open up opportunities for future research, including generative modeling of partially defined human poses.\n- Second, the performance of generative models would need to be benchmarked to assess the quality of distributional modeling/predictions and provide an objective benchmark to support future model development in this area. Many existing works focus on demonstrating the prediction accuracy of generative models or evaluating the likelihood values of their predictions. We believe that more work needs to be done in this direction to make the assessment of generative models more thorough and objective. For example, high accuracy can be achieved by approaches that do not rely on generative modeling. Likelihood value assessment is not ideal either: the model that only generates samples with high likelihood (and scores well) basically is bound to produce only the most common values, which may not be attractive from the user perspective. The model that produces too many samples in the tails of distribution might not be good either, because it will be hard to steer by the user and will produce too many samples that do not fit the user's guidance signals. More research needs to be done in this direction to identify suitable ways of quantifying the diversity of samples, the adequacy of the overall distribution fit as well as the usefulness of the randomly generated samples for the user of the system. We believe this is a very exciting area for future research.""}, {'title': 'Author response to Reviewer cvbx 5/5', 'comment': ""**Comment 5**\nMulti-task learning: I am not sure if multi-task learning is the right term for describing the loss terms: the look-at, rotation, position loss for joints are largely the same objective expressed in different modalities. Since all loss is calculated in a root-relative coordinate system, the rotation term should contain all information available to reconstruct 3D positions through forwarding kinematics. Using both rotation and positions more likely served as a more explicit representation of the objective. This objective has also been utilized in some recent works in pose estimation [1][2].\n\n**Response 5**\nWe would like to thank the reviewer for the insightful comment. The definition of multi-task learning is generally pretty broad - this is a multi-faceted domain. Based on Caruana 1997, the common features reminiscent of multi-task learning are as follows: shared input, shared representation, concurrent learning on multiple tasks, each task corresponds to a different neural network output and a different objective, concurrent learning on multiple tasks improves generalization by providing a stronger inductive bias. “Inductive bias is anything that causes an inductive learner to prefer some hypotheses over other hypotheses.” (Caruana 1997). We believe all these features are present in our setup. Our network has the same input, it has a shared representation, it has several outputs: local rotations and positions after an FK pass are good examples, the network is trained concurrently on several objectives, including local rotation loss and the global position loss, among others; we show that training on the global position and local rotation losses concurrently, improves generalization on both of them. The latter fact, in our view, confirms that the position- and rotation-related internal network information flows are genuinely different. It appears that solving positions and rotations requires the network to exercise slightly different inductive biases that, when combined together, produce a better overall result.  We feel that the Reviewer’s argument “Using both rotation and positions more likely served as a more explicit representation of the objective.'' does not actually contradict the multi-task scenario definition from Caruana 1997. We feel this is very much aligned with the notion of inductive bias, i.e. the ability of the learner to prefer some hypotheses over others, in turn making the overall objective easier to reach, or in other words making it more explicit. Indeed, we show both quantitatively and qualitatively that concurrent training on rotation and position losses improves generalization on both of them (cf lines 1, 6, 7 in Table 2). Note that since we are talking about generalization performance on the test set, it is clear that the network accumulates generalizable insights (inductive biases) helping it better solve unseen sparsely defined poses. Our qualitative example video 3_Loss_Ablation.mp4 demonstrates that this is not a coincidence. When rotation loss is removed, the network is able to solve the joint positions well, however rotations are totally off, making the overall pose implausible. With the full loss term including rotations, the network learns the inductive bias of choosing poses that respect both positions and rotations, making the overall pose qualitatively more appealing, which also shows quantitatively in improved position and rotation metric scores (cf lines 1 and 6 in Table 2). Finally, although [1,2] use a variety of loss terms, including a combination of position and rotation losses, they do not show that using a combination of rotation and position terms is critical for recovering a plausible full pose from a sparsely defined one. Our contribution in this domain is therefore two-fold: (i) we present qualitative and quantitative evidence implying that using both rotation and position is beneficial for recovering plausible poses from sparse variable user inputs and (ii) we introduce the randomized loss weighting scheme, which appears to be necessary to achieve synergistic effects while concurrently training on position and rotation losses, as follows from our Table 2.\n\n(Caruana 1997) Caruana, R. Multitask Learning. Machine Learning 28, 41–75 (1997)""}, {'summary_of_the_paper': 'This paper develops a system to generate a complete and realistic human pose based on a sparse set of inputs such as look-at direction and end-effector positions. It proposes a prototypical residual network dubbed "" ProtoRes"" to handle variable input length based on the user-specified parameters. A look-at-loss, randomized weighting scheme, and two datasets based on MoCap are proposed to learn the designed system, and a usable system for authoring realistic human poses is built. Experiments show that the network design and loss outperform baselines such as Transformer and existing Inverse Kinematics methods. The graphical demo built inside Unity also showcases the utility of the learned pose networks. ', 'main_review': '**Strength**\n\n1. **Impressive demo and usable system:** this work focuses on a practical and important problem: inverse kinematics based on a sparse set of user-specified constraints, and builds a usable system in Unity that shows impressive results in generating realistic and natural-looking human poses based on fast-changing inputs.  The proposed system looks complete and deployable for graphics and animation use-cases. \n2. **Proposed network:** the use of prototypical networks in the task of human pose generation or pose infilling, to the best of my knowledge, is novel, though it would be beneficial if the authors can provide more in-depth motivation and in-depth analysis of why such network design is more suitable in the proposed task. What inductive bias is prototypical networks leverage in this specific task? \n3. **Comprehensive ablation:** the ablation studies provided in the paper and supplement substantiate the benefits of the proposed learning procedure and loss weighting schemes. \n\n**Weakness**\n\n1. **Missing simple baselines:** based on the problem setup, one simple baseline is to use nearest neighbors to retrieve the closest pose from the dataset that satisfies the given input. Since MoCap data is continuous, it is highly possible that some realistic pose already exists in a large enough human motion database (such as AMASS [3], around 3 million frames). If no exact match exists, another naive approach could be finding close samples and interpolating between them either through simple linear interpolation or learned embeddings such as VPoser [4]. \n2. **Missing motivation and analysis for the proposed ""ProtoRes"" architecture**: as mentioned in the strength section, it is interesting the prototypical networks are used for this task. However, it is not well-motivated and not fully studied why the proposed network architecture is chosen for this task. Given the recent progress in Transformers, it is possible that the Transformer requires more data (which a dataset like AMASS can provide) but can learn a better embedding space for the pose synthesis/infilling task. \n3. **Missing generative aspect:** for such an under-constrained problem, it is natural to assume that generative models such as normalizing flow [5] or VAE [6] are a more suitable model for generating diverse pose that conforms to the same user input. \n\n**Minor Issues**\n\n1. **Multi-task learning:** I am not sure if multi-task learning is the right term for describing the loss terms: the look-at, rotation, position loss for joints are largely the same objective expressed in different modalities. Since all loss is calculated in a root-relative coordinate system, the rotation term should contain all information available to reconstruct 3D positions through forwarding kinematics. Using both rotation and positions more likely served as a more explicit representation of the objective. This objective has also been utilized in some recent works in pose estimation [1][2]. \n2. **Variable input length:** while variable input length is mentioned as a feature of the proposed system, it is not demonstrated via demo or experiments. Since there is only a fixed number of joints, I presume the system can use a fixed number of inputs (all the joints) and use a mask to indicate whether there is an input for that specific joint? \n\n\n==================================================\n\n\n**After author response**\n\nAfter reading the author\'s response and other reviews, most of my concerns are resolved. Thus, I would like to raise my score to 8 for this work for its novel use of prototypical networks in human pose modeling and the practical use case of the results. \n\n\n[1] Li, Jiefeng, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang and Cewu Lu. “HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation.” CVPR 2021\n\n[2] Rempe, Davis, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar and Leonidas J. Guibas. “HuMoR: 3D Human Motion Model for Robust Pose Estimation.”\xa0ICCV 2021\n\n[3] Mahmood, Naureen, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll and Michael J. Black. “AMASS: Archive of Motion Capture As Surface Shapes.”\xa0*CVPR 2019*\n\n[4] Pavlakos, Georgios, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, \n\nDimitrios Tzionas and Michael J. Black. “Expressive Body Capture: 3D Hands, Face, and Body From a Single Image.”\xa0CVPR 2019\n\n[5] Henter, Gustav Eje, Simon Alexanderson and Jonas Beskow. “MoGlow: Probabilistic and controllable motion synthesis using normalising flows.”\xa0*ACM TOG, 2020*\n\n[6] Yuan, Ye and Kris Kitani. “Diverse Trajectory Forecasting with Determinantal Point Processes.”\xa0ICLR 2019', 'summary_of_the_review': ""Overall, the proposed system is a practical and usable system for pose authoring based on sparse input. The demo is impressive and seems a great addition to inverse kinematic systems. On the other hand, the technical novelty seems lacking and some important baselines are missing. In all, I think this work is borderline and would like to hear the author's response before making the final decision. "", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper focuses on the learnable neural representation of the 3D human pose. A proto-residual network is proposed to construct a full human pose from sparse body part constraints. Several modifications are performed to improve the accuracy of the proposed model, such as look-at loss, two-stage decoder, residual scheme, etc. In the experiment, two new datasets for the static human pose modeling task are conducted and will be released publicly. The experimental results prove the effectiveness of the proposed methods.', 'main_review': ""Pros:\n\n(1) This paper focuses on an interesting and important task in human animation. The proposed method and animation tool may alleviate the workload of animation by the artists. \n\n(2) Several modifications about the model architectures are clearly described, and most of them make sense to me. \n\n(3) This paper provides comprehensive materials, including the experiments, appendix, and demo videos. The effectiveness of most of the proposed modules is proved by the experiments. \n\nCons:\n\n(1) The proposed method is a new way for human animation, so the comparison with FinalIK should be more detailed. \nThe speed of both methods should be clearly shown in Table 1. \n\nAs mentioned in the paper,  the proposed method satisfies the constraints approximately (P.9 The limitation of the current work). The construction results for both methods with full constraints may help to clarify the limitation. \n\nThe generalization ability of both methods should also be investigated. How about performing both methods on non-human skeletons with similar structures.\n\n(2) Although this paper has provided very comprehensive results, some comparisons and analyses should be further shown in the experiment part.\n\nIn Table 1, the inference speed & parameter numbers should be provided for machine learning-based methods. \n\nQuantitative evaluation for the look-at-loss should be presented to prove the effectiveness. \n\nTable 2 shows the ablation study for GPD, but how about only using GPD (without IKD) for the final result.\n\n(3) The dataset is one of the most important contributions in this paper. It is better to show more details about the dataset, especially the 'miniAnonymous' dataset.  \n\nHow many instances are included in the dataset? \n\nWhat kinds of actions for each instance are captured?\n\nDoes the motion style have a big difference with the 'miniMixamo' dataset?\n\n(4) Some descriptions of the proposed unity tool should be presented. Does it just show the results of the neural network? Or some animation functions in the unity is also used as post-processings?\n\nDetailed questions:\n\n(1) What is the minimal number of the input constraints?\n\n(2) What kind of results will be output if a sequence of the ambiguous input constraints is given?\n\n(3) There are many interesting comparisons in the demo videos, so it is better to show some of them in the main paper.\n\n(4) In Table 5 and Table 6, there are some distinct results, i.e. 1.27 e-6. Are those typoes?"", 'summary_of_the_review': 'This paper focuses on a very interesting topic and proposes two datasets that may facilitate the research area. The proposed methods are reasonable, and most of them are well evaluated.  Although some experiments should be added and some details are missing now,  I think it is still a good submission and ok to be accepted. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'I have reviewed this paper and I think there are no ethical concerns.', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a DNN-based framework to handle the challenging task of recovering 3D human poses (joint positions and local rotations) from sparse and variable user inputs (joint positions, orientations of a subset of whole body joints). The paper proposes a neural-network architecture with an encoder to extract pose encodings from partial user inputs and a decoder to generate complete poses from the encoded features. The paper further proposes two new datasets to train and evaluate the proposed models. Experimental results demonstrate the superiority of the proposed framework over previous methods. ', 'main_review': ""Pros:\n1. The paper solves an interesting and long-standing inverse-kinematics problem. To solve the problem, the paper novelly proposes a neural-network-based method to automatically handle the problem.\n2. In general, the paper is technically sound with experiments to support its argument. Additionally, the paper proposes two datasets to train and validate the proposed methods.\n2. In general, the paper is easy to follow.\n\nCons:\n1. The organization of the paper could be further improved. For example, the definition of 6D rotations is only explained in Sec 1.2. It would be better to mention it the first time this term appeared, otherwise, it is slightly confusing. Besides, it is suggested to merge the five contributions into three or four. e.g. merge the fourth and fifth contributions.\n2. Some important details are missing. a). How are the metrics L^{det}_{gpd}, L^{det}_{ikd}, L^{det}_{loc} defined and what are the units of them? b). How are the two datasets split into train and evaluation?\n3. It would be great if the paper could show more qualitative results. For example, the qualitative comparison between the proposed method and the previous methods.\n4. It would be great if the paper could show more analysis on the relation between inputs and model performances. For example, how is the performance influenced by the number of input joints and input types (joint positions or joint rotations which are better)?\n5. I have a concern about the overall framework. The paper resolves an ill-posed problem, i.e. there could be multiple feasible outputs given the one input. However, the paper uses the encoder-decoder framework which is only able to generate one output given the input. Therefore, I am wondering would it be better to change the current AE architecture to VAE so that multiple outputs could be produced? I would like to hear about the authors' options."", 'summary_of_the_review': 'This paper is interesting and technically sound in general. Therefore, my current rating is boardline accept.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics', 'authorids': ['~Boris_N._Oreshkin1', '~Florent_Bocquelet1', '~Felix_G._Harvey1', '~Bay_Raitt1', '~Dominic_Laflamme1'], 'authors': ['Boris N. Oreshkin', 'Florent Bocquelet', 'Felix G. Harvey', 'Bay Raitt', 'Dominic Laflamme'], 'keywords': ['inverse kinematics', 'deep learning', 'pose modeling'], 'abstract': 'Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code.', 'pdf': '/pdf/72eadcfe21558f0be18ff071adc50adc3ae85e5e.pdf', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'oreshkin|protores_protoresidual_network_for_pose_authoring_via_learned_inverse_kinematics', 'supplementary_material': '/attachment/ff67a06a7330e8faefa4288da4c1ce1b2e39fbb7.zip', '_bibtex': '@inproceedings{\noreshkin2022protores,\ntitle={ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics},\nauthor={Boris N. Oreshkin and Florent Bocquelet and Felix G. Harvey and Bay Raitt and Dominic Laflamme},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=s03AQxehtd_}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Bo Wan', 'Wenjuan Han', 'Zilong Zheng', 'Tinne Tuytelaars']",ICLR,Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling,https://iclr.cc/virtual/2022/oral/6111,2022," We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=N0n_QyQ5lBF,https://openreview.net/forum?id=N0n_QyQ5lBF,N0n_QyQ5lBF,"[{'title': 'Codes and data release.', 'comment': 'Codes and data are available at https://github.com/bobwan1995/cliora.'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes to perform unsupervised grammar induction over image-text pairs and used shared structure between the modalities to improve grammar induction on both sides. Authors find the paper clear, creative and interesting and recommend acceptance without hesitation.'}, {'title': 'Thanks', 'comment': 'Thank you for your clarifications. My score remains as it was (positive). I look forward to future work on the hierarchical visual structure.'}, {'title': 'Re: Thank you for your response. ', 'comment': 'Thank you so much for raising the score and providing detailed suggestions for the revision. We believe your recommendation would also benefit the development of the multimodal community. '}, {'title': 'Thank you for your response.', 'comment': ""Thanks for the clarification on your point about the unified structure, and for getting the results on unsupervised hyperparameter tuning. \nI now confirm that the paper is valuable to be presented at ICLR -- I'll raise my rating to show clear recommendation towards acceptance. \n""}, {'title': 'Re: Happy to see that most of my comments are resolved, and further discussion', 'comment': 'We appreciate your careful reading of our response and considering raising your rating. Based on your comments, we would like to further address your concerns as follows:\n\n**Unified Grammar:** “if it is an advantage (or disadvantage) to have a fully synchronous vision-language grammar.”\n\n**A:** To clarify, in this work, the “unification” is referred to as constructing a common VL representation for vision and language simultaneously. We focus on the “shared world” for VL representations. The motivation behind our interest is in the “shared world” (i.e., the synchronous vision-language structure to reveal the cross-modality semantic consistency) to enhance multi-modality understanding. Empirically, we have demonstrated that such a structured learning paradigm not only benefits the language grammar induction but also weakly-supervised visual grounding. \n\nWe agree that “the image would have different parsings associated with different captions”. As we discussed in Section 5, the work, though demonstrates the concept of shared structure representation, still has the limitations of failing to leverage the visual structure information. By proposing this new structure representation, we hope to provide another perspective of multimodal structure modeling (compared with visually-aided ones) and call for future research to explore its potential (i.e. advantages / disadvantages).\n\nAn interesting and potential point may be considering not only the “shared world” but also the “individual world”. Different modality has its own characteristics. Some previous works (Krishna et al., 2017) have claimed this opinion and build hierarchical structures for visual scenes (e.g., scene graphs) and natural languages (e.g., dependency/constituent trees), individually. Respecting the semantic consistency of the image-sentence pair as well as independent characteristics, a joint vision-language structure considering what is the shared structure (e.g., the shared VL structure we proposed in this work) and what is the individual structure for each modality is definitely a further work that is worth further studying. \n\n\n**Re: R #hzHn:** “...DIORA uses the PTB dev set for choosing the best parameters, while the authors choose their tuned hyperparameters, and only do unsupervised model selection... ”\n\n**A:** We have performed a couple of hyperparameter tuning experiments to justify our selection for the model’s initialization. To assess DIORA’s hyperparameter selection in an unsupervised way, we use the same criterion as our model selection approach, and tune DIORA’s hyperparameters based on the loss on the development dataset (without using the unannotated data). More concretely, we compute the average loss on the Flickr30k Entities development set for each hyperparameter value and select the hyperparameter with the smallest loss. Take the essential hyperparameter -- the learning rate as an example, we choose the learning rate from values of [1e-6, 1e-5, 1e-4]. When using a large learning rate (e.g., 1e-4), the average loss of selected modes is 17.8 and the C-F1 is 34.8±3.8. With learning rates 1e-5 and 1e-6, the average loss decreases to 4.7 and the C-F1 is stable at a value as reported. Therefore, for our model, we can adopt an unsupervised approach for both the hyperparameter tuning and the model selection.\n\n\n**Words that refer to part-of-speech tags as nonterminals**\n\n**A:** Thank you for pointing this out! We have had them fixed (including Section 3.2, Appendix E).\n'}, {'title': 'Happy to see that most of my comments are resolved, and further discussion ', 'comment': 'Thanks authors for your response! I appreciate that most of my comments have been addressed. I\'m now inclined to raise my rating -- will update it after the discussion period concludes. Below are the points on which I am still not convinced:\n\n--- \n**Unified Grammar** \n\nWhile I agree with the authors that this work is the first one proposing a ""unified"" vision-language grammar, if ""unified"" means that an image and parallel text have exactly the same parsing structure, I am not sure if it is an advantage (or disadvantage) to have a fully synchronous vision-language grammar. \n\nConsider an image that has several semantically equivalent captions with different syntactic structures, the image would have different parsings associated with different captions. This is very unnatural to me on the vision side, especially when linking to how humans acquire language: we arguably don\'t parse the visual scene in exactly the same way as how we parse text. The visual parses in this work are therefore less meaningful than text parses. \n\nTo clarify, I do not mean that modeling the visual structures in the current way is bad: it provides ways to better understand which visual part each text span corresponds to, and, from the engineering perspective, it enables easier fusion with text features. However, the current paper and ablation study did not convince me that such the ""unified"" VL setting should be claimed as an advantage compared to existing work, instead of just one choice among many possibilities. \n\n--- \n**Author Response**: ""Recent SOTA approaches, e.g. VC-PCFG and DIORA, have not mentioned if they select their checkpoint according to the annotations from the dev data."" \n\n**R #hzHn**: While I\'m happy to see the authors have done experiments with (fully unsupervised) loss based model selection and have clarified in Appendix F, I would like to emphasize that existing work in a problematic setting, especially when it has been concerned for years, should never be an excuse for conducting additional work in the setting. \n\nDIORA does select the best model based on the annotated dev data (https://aclanthology.org/N19-1116.pdf; Appendix A.1 Early Stopping). At the meantime, I am slightly worried about the hyperparameter settings (Appendix F): DIORA uses the PTB dev set for choosing the best parameters, while the authors choose their tuned hyperparameters, and only do unsupervised model selection. \nIdeally, the hyperparameters should also be tuned based on the current development loss. \n\n--- \n\nThere are still some words that refer to part-of-speech tags as nonterminals (e.g., Section 3.2, Appendix E). \nI\'m looking forward to seeing these fixed.\n'}, {'title': 'Response to All Reviewers', 'comment': 'We thank all reviewers for their time and valuable comments. In this work, we propose a new task VL grammar induction that aims to build a shared VL structure.  We appreciate reviewers acknowledge that:\n- This work introduced “the first visual-linguistic Grammar Induction in a real-world setting” (Reviewer hzHn), which “is intuitively correct” (Reviewer cvGw), and “might inspire future research on explainable multimodal processing” (Reviewer 3QG1).\n- Our proposed method CLIORA is “built based on an existing model” (Reviewer 3QG1) and is “interesting and inspiring” (Reviewer cvGw). The method and evaluation “are solid and make a lot of sense.” (Reviewer hzHn)\n- The parse results are “impressive” (Reviewer hzHn) and “convincing” (Reviewer 3QG1).\n\nWe sincerely hope that this work would benefit the explainable multimodal community. We have made some changes to address the comments from the reviewers. We would really appreciate it if the reviewers could read our Rebuttal Revision.'}, {'title': 'Response to Reviewer cvGw', 'comment': 'Thank you for acknowledging the novelty of our proposed method. All your concerns are addressed as follows:\n\n**Q: Image features**\n\n**A:** In the outside pass we aggregate the inside representation and outside representation with the outside algorithm (see Appendix D for the detailed description). As the inside representation contains visual cues, thus each outside representation is also aware of the visual information. Actually we tried to use the aggregated visual feature from the inside representation to reconstruct the original word, i.e., replace $P(x_i|h_{i,i}^{out})$ in Eq. 9 with $P(x_i|h_{i,i}^{out}, v_{i,i})$. However, we didn’t observe a performance gain with such modification. We suppose this might be because the outside representations already contain the visual information. \n\n--------------\n\n**Q: Max operator**\n\n**A:** We follow MAF (Wang et al., 2020b) and use the max operation for contrastive learning. The motivation of taking max operation is that visual grounding is remarkably unambiguous in the sense that only a tiny portion of image areas are assigned to a language phrase. Indeed the max operation will lead to a biased gradient. The empirical results show that the max operation works well in our method.\n\n\n--------------\n\n**Q: Hierarchical structure of the image**\n\n**A:** As we discussed in Section 5, the hierarchical visual structure is critical for joint VL grammar induction and we plan to explore it in future work.\n'}, {'title': 'Response to Reviewer 3QG1', 'comment': 'Thank you for acknowledging the impact of our work.\n\n**Q: what precise manner the image is actively contributing to the induction process.**\n\n**A:** The main purpose of this work is to induce a shared VL grammar. In this work, we not only use the visual information to regularize the language grammar induction, but also transfer the language structure to the visual space to construct a shared VL structure by using the context-aware VL representations. To select the meaningful spans, we expect the fine-grained image information (i.e. the visual objects) to provide the visual-grounded information (Shi et al.  (2019); Zhao & Titov (2020b)). Besides, such visual objects also provide global context information to enhance the feature representation. The explanation behind such enhancement is out of scope for this work and we leave it to future research on explainable modeling.\n'}, {'title': 'Response to Reviewer hzHn', 'comment': 'Thank you for the valuable suggestions and references. We hereby carefully address your concerns as follows:\n\n**Q: Model selection**\n\n**A:** Thanks for pointing this out. Model selection is a classic problem in an unsupervised setting due to the indirect evaluation w.r.t. the learning objective (see comments of Kyunghyun Cho in PRPN https://openreview.net/forum?id=rkgOLb-0W). Recent SOTA approaches, e.g. VC-PCFG and DIORA, have not mentioned if they select their checkpoint according to the annotations from the dev data.\n\nTo evaluate our model’s robustness to different model selection approaches, we follow a similar strategy as in PRPN and use the loss on the dev set (without using the annotations in the dev set) for model selection. More concretely, we compute the average loss on the dev set after each training epoch and select the model before the loss starts to increase. With such a strategy, we observe minor performance drop in both the baseline (0.4%) and our method (0.6%) compared with using the annotations on the dev set to select the final model (The results are conducted on the Flickr30k Entities dataset, and the trends are similar on MSCOCO). However, the performance gain (2.3%) of our method is still obvious. We update all the experiment results with such a strategy in the revised version.\n\n----------------------------------------\n\n**Q: Unfair comparison**\n\n**A:** Indeed, our model CLIORA is built upon DIORA which uses ELMo/Glove to initialize word embeddings and outperforms all the other works. To further address your concern, we remove the initialization of word embeddings in DIORA and build CLIORA upon it. Besides, we take the new strategy for model selection as mentioned above. The new experimental results are listed below and supplemented in the revision (Tab. 1). Impressively, when we start from such a weak baseline, our CLIORA obtains a large performance gain of 6.7% compared with DIORA and still outperforms VC-PCFG with 26.8% on the Flickr30k dataset. Such observation further validates our motivation that CLIORA can jointly learn vision and language structures.\n\n\n|        | **MSCOCO**  | **Flickr30k**\n|------ | ------|----------\n|DIORA† |  53.4±0.6 | 46.4±0.9\n|DIORA |   58.0±0.7 |  54.3±2.0\n|CLIORA†   | 56.2±0.7     |  53.1±1.8\n|CLIORA | 60.8±0.8   |   56.6±1.7\n\n(† indicates we use randomly initialized word embedding.)\n\n\n----------------------------------------\n\n**Q: Introduction (Discussion of “These works, however, fail to consider a unified VL structure”)**\n\n**A:** We respectfully disagree. We argue that the structure in Hong et al. (2021) is not “unified” in terms of the representation perspective. In their setting, they build a language and a vision constituency tree separately and match the nodes in the two trees. However, the structures of the two trees are not the same, as the number of terminals of the sentences (i.e. language words) and visual objects (i.e., object parts) is not the same. In our paper, we build a shared structure for vision and language. \n\n----------------------------------------\n\n**Q: Introduction (Nonterminals Clarification)**\n\n**A:** Here, we mean the nonterminals. It’s the same for the task definition section on page 3. We have clarified this in the revision.\n\n----------------------------------------\n\n**Q: Task definition & evaluation metrics**\n\n**A:** We define a critical concept as a meaningful phrase in the ground-truth constituency tree. In other words, “meaningful” phrases are phrases that can be grounded in the image. As there does not exist a dataset that fully labels all the critical concepts, we select a certain type of critical concept, the noun phrases, from the visual grounding annotations (i.e. Flickr30k Entities annotations). We think a critical concept is recalled when it is retrieved in the parsed constituency tree structure and simultaneously correctly grounded in the image. All the noun phrases are critical in the GT annotations. \n\n**CCRR:** Thanks for the suggestion. We have made changes accordingly.\n\n----------------------------------------\n\n**Q: Enabling a trained CLIORA model to parse pure text**\n\n**A:** Our CLIORA adopts two fusion strategies to leverage the information from vision and language: feature-level and score-level fusion. Actually, if we only take score-level fusion, the visual cues only help the training of CLIORA and are not used in the inference stage, which means we take only pure text as input. As shown in the third row of Tab. 4, with only score-level fusion, we can still obtain 1.6% performance gain compared with the baseline.\n\n----------------------------------------\n\n**Minor Comments**\n\n**A:** Thanks so much for the advice on the abstract and for pointing out the typos and the missing references.  All have been fixed, including:\n- CCRA $\\rightarrow$ CCRR\n- Yoon Kim et al. (2019b) $\\rightarrow$ Shi et al., (2019)\n- Section 4.3: arguments $\\rightarrow$ augments\n- Remove the dagger after VG-NSL+HI\n'}, {'summary_of_the_paper': 'This paper introduces a task of joint visual-linguistic grammar induction from parallel image-text data, presents models and metrics for the task, and shows strong empirical results.\n', 'main_review': '\n### Strengths \n\n- As far as I know, this is the first paper that proposes joint visual-linguistic grammar induction in a real-world setting (in contrast to synthetic settings; Hong et al., 2021).\n\n- The approach and the evaluation process are solid and make a lot of sense to me.  \n\n- The visually grounded parsing results are quite impressive. \n\n### Weakness\n\n- My major concern is about the model selection process and the potential unfair comparisons to existing work. \n    - Model selection: If I understood correctly, for text parsing, the best models are selected w.r.t. to the parsing performance on a 1000-example dev set (Appendix F). \\\nThis is an unrealistic setting (see https://aclanthology.org/2020.emnlp-main.614.pdf for discussions; in short, for any fancy unsupervised parsing model that uses a labeled development set, a supervised model trained on these development examples should be considered as a strong baseline) -- introducing unsupervised criteria for model selection is more important than our initial impression.\n\n    - Unfair comparison: CLIORA, the model proposed in this paper, uses DIORA as initialization, which uses ELMo to initialize word embeddings and the PTB labeled development set for model selection. This means that CLIORA has seen far more text than other baselines (VG-NSL, C-PCFG, VC-PCFG, and so on) and human language learners. \\\nThis issue also undermines the authors\' arguments about potential links to how humans learn language. I expect either a CLIORA trained from scratch (without DIORA initialization) or weakened arguments about the relationship between the current CLIORA and human language learning.\n\n- There seem to be some confusion on basic linguistic concepts, e.g., nonterminal vs. terminal symbols, and a few typos that affects smooth understanding (please see also detailed comments below). \n\n### Other comments and questions\n\n- Introduction: ""These works, however, fail to consider a unified VL structure, nor have they demonstrated impact on visual understanding.""  \\\nI don\'t think I necessarily agree with this statement, especially regarding Hong et al. (2021). Despite that there is a clear gap between their dataset and the real world settings, they are aligning the ""visual grammars"" to language grammars, yielding an arguably unified VL structure. \n\n- Introduction: ""The non-terminal symbol of a conventional constituency structure is a category label from a limited set (e.g., the set of part-of-speech (POS) tags) (Hopcroft et al., 2001). "" \\\nDo you mean *terminal* symbols here? We usually refer to POS tags (to clarify, phrase tags are not POS tags) by preterminal or terminal (depending on whether the phrase-structure grammar is lexicalized, i.e., whether it\'s considering real words or just POS tags), and refer to the phrase nodes by nonterminal nodes/symbols (e.g., NP, PP). It seems that this is not a typo -- I have the same questions for the following task definition section on page 3.\n\n- Task definition, evaluation metrics: if I understood correctly, CCRA requires some extra annotation of critical concepts -- how did you collect such annotations to determine which NPs are critical? \\\n(Very minor) based on the full name, CCRA should really be CCRR -- what does A stand for here? \n\n- Section 3.2, feature extraction: the Yoon Kim et al. (2019b) paper is not relevant to image features at all -- did you mean Shi et al., (2019)? \n\n- Table 1: what is the dagger after VGNSL-HI? \n\n- Section 4.3: did you mean ""augments"" by ""arguments""? \n\n- Some more thoughts regarding motivation limitations: humans arguably learns how to parse concrete sentences first, and can then generalize to abstract domains that are not visually groundable. In this work, it seems that the model only works when both the text and image are available, as there is a need to infuse visual features into text spans. Do you have any thoughts on enabling a trained CLIORA model to parse pure text without grounding signals?\n\n### Missing Reference\n\nKojima et al. [1] has strengthened the VG-NSL model by simplifying the architecture, and argued that such visually grounded models are potentially biased towards concrete noun phrases. However, the paper neither cited it nor discussed the relevant issues. \n\n[1] https://aclanthology.org/2020.acl-main.234.pdf \n\nThere have been a lot of relevant work earlier than 2019 on visual-semantic embeddings or structured visual understanding with text. To name a few, \n\n**Older work on structured image-text representations**\n\n[2] https://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf\n\n[3] https://openaccess.thecvf.com/content_cvpr_2018/papers/You_End-to-End_Convolutional_Semantic_CVPR_2018_paper.pdf\n\n**Contrastive loss for visual-semantic embeddings**\n\n[4] https://arxiv.org/pdf/1411.2539.pdf\n\n\n### Minor Editing Comments\n- I was confused about what CCRA is when reading the abstract -- would be good to include the full name and give an intuitive description of the metric. \n\n- Yoon et al. $\\rightarrow$ Kim et al. \n\n- Shi et al. (2019) proposes $\\rightarrow$ Shi et al. (2019) propose\n\n- In my opinion, putting Section 3.4 before 3.3 would better streamline the paper. \n', 'summary_of_the_review': 'This paper introduces the task of joint visual-linguistic grammar induction, and presents models, metrics and empirical results on it. While I appreciate the impressive results, I am concerned about the unrealistic model selection process (comparing model outputs to a large set of ground-truth parse trees) and the unfair comparison (the proposed model has access to much more unlabeled text data than baselines).', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'This paper presents a new model for grammar induction for text, with help from the coupled images. The model was built on top of an existing unsupervised grammar induction model used for text without image information. The experimental results show the approach was effective. The work essentially demonstrates some effective ways of leveraging the additional image information for improving the grammar induction task. The paper also discussed some weaknesses of the approach and future work.', 'main_review': 'The topic of grammar induction has been there for a very long time in NLP and is a very fundamental topic.  The model was largely built based on an existing model for purely text-based grammar induction. The model essentially makes use of neural networks to learn good latent representations (using a reconstruction loss) where the latent representation is defined with neural networks which yield scores for constituents and vector representations of them. The approach adopts the classical inside-outside process for the computing of the scores.\n\nThe paper essentially investigates what might be the effective methods for integrating image information into text for improved grammar induction. The execution of the paper was quite good, and the results are convincing. However, I feel the overall model is essentially a way to use image information to regularize the grammar induction process. Little can be said about in what precise manner the image is actively contributing to the induction process. Indeed, the authors also acknowledged something along with what I thought in the final section. Nevertheless, I think it is an interesting piece that might inspire future research on multimodal processing (for image + language).', 'summary_of_the_review': 'I think this is a reasonable piece, with good writing and a nice set of experiments. It would be helpful for future research in this domain.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper proposed a new method CLIORA to do unsupervised parsing and vision-language grounding. \nCLIORA is based on DIORA model. \nBut different from previous unsupervised parsing methods, CLIORA also induces alignment between constituents and image regions. \nIn order to train the model, the author introduces a contrastive loss.\nExperiment results show that the proposed method outperforms baseline unsupervised parsing methods and it also induces meaningful alignment between image regions and constituents.', 'main_review': ""Strengths:\nThe idea of jointly inducing structure in natural language and grounding the constituents with real-world images is intuitively correct.\nThe ablation study also shows that both feature-level fusion and score-level fusion (including the contrastive loss, if I understand correctly) helps in improving the parsing results.\n\nWeakness:\n1) The image features are only used for computing the inside pass. The image feature should contain information that can help predict the missing word, such that it could be used in the outside pass too. Selecting the best image region for predicting the missing word is also an intuitively correct way to build the vision-language alignment.\n2) The compute of sim(I, c_ij) includes a max operator, this could lead to a biased gradient.\n3) As the author mentioned in the discussion section, the model doesn't consider the latent hierarchical structure of the image. For example, the sentence describes the entire image, while each phrase describes part of the image."", 'summary_of_the_review': 'Overall, the proposed method is interesting and inspiring. \nThe idea should be interesting to both unsupervised parsing and multimodel communities.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling', 'authorids': ['~Bo_Wan1', '~Wenjuan_Han1', '~Zilong_Zheng1', '~Tinne_Tuytelaars1'], 'authors': ['Bo Wan', 'Wenjuan Han', 'Zilong Zheng', 'Tinne Tuytelaars'], 'keywords': ['Grammar Induction', 'Vision-Language Matching', 'Unsupervised Learning'], 'abstract': 'We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously.  We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning.  It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both.', 'one-sentence_summary': 'We introduce a new unsupervised vision-language grammar induction task to explore the multimodal information and induce a shared hierarchical structure for both image and language simultaneously.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wan|unsupervised_visionlanguage_grammar_induction_with_shared_structure_modeling', 'pdf': '/pdf/5c104842d13e8d6efd55b6d7c04f4373a39eae18.pdf', 'supplementary_material': '/attachment/d6b7ddd3bf6e5c1f4edc51b23b8cbe88b0db163d.zip', 'data': '', '_bibtex': '@inproceedings{\nwan2022unsupervised,\ntitle={Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling},\nauthor={Bo Wan and Wenjuan Han and Zilong Zheng and Tinne Tuytelaars},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=N0n_QyQ5lBF}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Yifei Wang', 'Jonathan Lacotte', 'Mert Pilanci']",ICLR,The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks_ an Exact Characterization of Optimal Solutions,https://iclr.cc/virtual/2022/oral/7125,2022," We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys.Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=Z7Lk2cQEG8a,https://openreview.net/forum?id=Z7Lk2cQEG8a,Z7Lk2cQEG8a,"[{'title': 'Response', 'comment': 'The international transmission of fiscal policy between trade partners with varied production and indebtedness characteristics https://slopegame.net may be studied in detail using the three-country approach.'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'A conceptually and technically highly innovative paper which reinforces an existing powerful connection between the critical set of two-layer ReLU networks and suitable convex programs with cone constraints. The reviewers are in strong consensus that the paper is sound and has merits for publication.'}, {'title': 'Response to Reviewer DhNM', 'comment': 'For the extension of our results to deep neural networks, please see our separate comment above. \n\nAbout the question: We believe the subsets of nearly minimal neural network and minimal neural network to be useful in our theorem statements: they provide a simple structure of the set of 2-layer ReLU neural networks and they are directly related to the solutions to the convex program.\n\nThanks for pointing it out. We add this comment to incorporate the bias term into the model.'}, {'title': 'Response to Reviewer P5NY', 'comment': 'Thanks for your effort in reviewing our paper and providing many helpful suggestions. We will include the suggestions in the final version, as well as the pointed references. In the remainder, we want to address the main points raised in the reviews.\n\nCompared to [1] and other results related to the convex formulation of regularized training problem, we focus on characterizing the relation between optimal solution to the regularized training problem and the optimal solution to the corresponding convex program. Comparatively, our analysis do not rely on the dual problem, which is the strategy used in [1] and several other works on the convex formulation. \n\nWe included the references pointed out about modifying the landscape by introducing the regularization term. \n\nWe discussed the extension to deep neural network in another comment above.\n\nFor the typos:\n\n1. Yes, it should be $m$ instead of $d$.\n\n2. For positive $\\alpha$, we have $\\sigma(Xu_i\\alpha_i)=\\sigma(Xw_i)$ and for negative $\\alpha$, we have $\\sigma(Xu_i\\alpha_i)=-\\sigma(Xw_i)$. This is the reason why we define the cone $C_{i+p}=C_i$ for $i\\in[p]$ and let $D_{i+p}=-D_i$ for $i\\in[p]$.\n\n3. We add more motivations to introduce the partitions of $D_i$ in the revision. \n\n[1] Tolga Ergen, Mert Pilanci, Convex geometry of two-layer ReLU networks.'}, {'title': 'Response to Reviewer FbD2', 'comment': 'Thanks for your effort in reviewing our paper and providing many helpful suggestions. We will include the suggestions in the final version, as well as the pointed references. In the remainder, we want to address the main points raised in the reviews.\n\n1. Our results heavily depend on the convex optimization formulation of the regularized training problem of two-layer ReLU networks. Going beyond the ReLU activation, there also exist convex optimization formulations for degree two polynomial activations [1,2]. We expect to extend our method to these activation functions in future work. Besides, it is possible to develop convex optimization formulations for the $\\mathrm{ReLU}^2$ activation. \n\n2. Regarding the extension to deep ReLU neural networks, we summarized our response in another comment above.\n\n[1] Burak Bartan and Mert Pilanci, Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization of Polynomial Activation Neural Networks in Fully Polynomial-Time.\n\n[2] Burak Bartan and Mert Pilanci, Training Quantized Neural Networks to Global Optimality via Semidefinite Programming.'}, {'title': 'Response to Reviewer VYzk', 'comment': 'Thanks for your effort in reviewing our paper and providing many helpful suggestions. We will include the suggestions in the final version. \n\nFor the extension to deep neural networks, see our official comment.\n\nThanks for pointing out these typos. We correct them in the revision. \n\n1. It should be $m$.\n\n2. The definition shall be $C_{i+p}=C_i$ for $i\\in[p]$.\n\n3. We change our definition of $C_i$ to be the closure of the convex cone of neurons $u \\in \\mathbb{R}^d$ such that $\\mathbf{1}(Xu \\geq 0) = s$. This makes the entry of $Xu$ corresponding to $0$s in $s$ to be non-positive.\n\n4. The cone shall be $B_i$ insead of $C_i$. '}, {'summary_of_the_paper': 'The paper explores the landscape of the objective function in training a single-hidden layer neural network with ReLU activation and L2 regularization. Impressively, the paper has the following contributions:\n\n1. It advances the results of Pilanci and Ergen from 2020 by showing that all *global* optimum points can be found via the convex program introduced by Pilanci and Ergen. \n2. It shows that for a large enough width of the hidden layer (at most 2*(n+1), where n is the number of training examples) there are no spurious valleys (i.e . all local minima are global), and GD won\'t get ""stuck"".\n3. It defines a subclass of single-hidden layer neural networks which it terms ""nearly minimal"". It characterizes  stationary points of the optimization problem by showing that every such point must be a nearly minimal neural network.\n4. It gives a polynomial time algorithm for checking whether a stationary point is a global optimum.\n', 'main_review': ""Strengths.\nI find the results to be novel extensions of of Pilanci and Ergen from 2020 and important theoretically: characterization of the global and local optimum points of the objective function.\n\nWeaknesses.\nThe paper only considers a single-hidden later. Can this analysis method be extended to multiple layers? \n\n\nSeveral typos and corrections I found:\n- The upper bound in the rightmost summation in equation (1) should be 'm' not 'd'.\n- Page 1, penultimate line,  no definition for $C_i$ for $i\\in[p+1, 2p]$ is given.\n- Page 3, Section 1.3, first paragraph, third line:\n  - Fix the mismatching brace. \n  - I believe the $\\sigma$ should be dropped.\n  - The $1(Xu\\geq0)=s$ seems to be a slightly different definition of the cone corresponding to $s$ than the original definition in page 1. Namely, the entries of $Xu$ corresponding to $0$'s in $s$ are required to be negative in this definition and nonpositive in the definition in page 1.\n- In equation (5), B(u_j, \\alpha_j) has vectors with $d+1$ entries (the last entry comes from $R_{>0}$ or $R_{<0}$ from the definition of  $B_i$) and $C_i$ has vectors with $d$ entries, so the containment under the summation doesn't make much sense here.\n\n\n\n"", 'summary_of_the_review': 'See above.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper establishes some very interesting connections between the global optimizer of the two-layer ReLU neural networks with the solution of a convex optimization program with cone constraints. More concretely, the authors construct explicit mappings between a smaller class of neural networks called minimal neural networks (that contains all the global optima) and the optimal solutions of convex programming problem. They also show that first-order methods such as SGD can find\nnetworks that can be merged to a minimal representation. Lastly, they provide an explicit path of non-increasing\nloss between any point on the loss landscape to the global minimizer. With this, they prove that has no spurious local minima, provided that the number of neurons is sufficiently large.', 'main_review': 'First of all I have to say this is a very interesting theoretical paper and is the best  among all submissions I have reviewed in ICLR 2022. This paper reveals an interesting hidden convexity structure in  two-layer ReLU neural networks by mapping minimal neural networks to a convex optimization problem and vice versa. The construction of the mapping is clean and elegant. The idea of connecting non-convex loss landscape with convex optimization can potentially be further explored to explain the success of deep learning.  This paper is strong enough and I do not see much weaknesses. But I left to the authors few minor comments which  can be seen as future considerations.\n\n- The first comment that comes to my mind is how the results obtained in the paper can be generalized to neural networks with other activation functions. It seems that the positive homogeneity of ReLU is the key to derive the convex programming problem. Can the authors comment on whether (and how) it is possible to establish similar results for smoother activation functios, e.g. $\\text{ReLU}^{k} $ or Sigmoid?\n\n- Can the authors comment on whether and how the results can be extended to deep ReLU neural networks? \n\nSome relevant references are missing.\n\n- Huiyuan Wang, Wei Lin, Harmless Overparametrization in Two-layer Neural Networks, arXiv:2106.04795.\n\n- Quynh Nguyen, On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths, arXiv:2101.09612. \n\n- Quynh Nguyen, Pierre Brechet, Marco Mondelli, When Are Solutions Connected in Deep Networks?, arXiv:2102.09671.\n \n', 'summary_of_the_review': 'This paper provides novel tools and insights in revealing the convexity structure of non-convex loss landscape of two-layer neural networks. The ideas developed here can be used to explain the global convergence of gradient descent algorithm in training neural networks. I recommend accepting the paper. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'none', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'Along the line of Pilanci & Ergen (2020), this submission deals learning two-layer ReLU neural networks through convex optimization. It introduces a number of new notions such as (nearly) minimal neural networks, developing a set of interesting tools, and draws connections between the minimal neural networks and the convex optimization landscape. This paper provides a rich framework along with new analyses and solutions for learning two-layer ReLU networks through convex cone optimization. ', 'main_review': 'Strengths:\n\nInteresting observations about and practical ways for constructing the optimal neural network set from the solution of the convex cone program.\n\nThe paper is mostly well-written and organized, and technically precise. \n\nThe analysis is mostly clean and easy to follow, which may be of interest for addressing related sets of nonconvex problems. \n\nWeaknesses:\n\nThe (or similar) regularized convex formulation has been studied in previous works, see e.g., Pilanci & Ergen (2020), Ergen & Pilanci (2021) among several recent others by Pilanci et al. Though a more-in-dept analysis as well as theoretical observations are provided, how technically novel and practical useful of these results shall be further compared and elaborated. \n\nErgen, Tolga, and Pilanci, Mert. ""Convex geometry of two-layer ReLU networks"", 2021.\n\nA number of related efforts dealing with learning two-layer ReLU networks by analyzing or modifying the landscape through introducing regularization terms are missing in the discussion; see e.g., Wang et al, 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. \nMoreocver, the contributions can be considerably enhanced by providing experimental validations across different tasks such as regression, or classification tasks. \n\nOne of the key difficulties in deep learning theory is that learning of deep neural networks is nonconvex due to their compositional structure. How would the results extend to deep neural networks?\n\n\n', 'summary_of_the_review': ""The paper contains some typos and grammar errors. \n\nin (1), the second summand should be m terms instead of d terms?\n\nI do not understand why the positive homogeneity of the ReLU activation function directly leads to σ(Xui)αi = σ(Xwi), as \\alpha_i's is not necessarily positive in general?\n\nThe paragraphs regarding the partions of D_i's are not easy to understand, which shall be improved. \n\n\n----------------------------------------------------\nAfter rebuttal: I have read the other reviews and authors' replies. My minor issues were addressed. I will keep my score."", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'NA', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In this paper, the authors study the training of two-layer ReLU networks with weight decay.  A previous paper (Pilanci and Ergen 2020) introduced a convex optimization problem that corresponds to this non-convex case.  In the present paper, the authors prove that all optimal solutions of the nonconvex formulation can be found via optimal solutions of this convex formulation.  (Whereas the Pilanci and Ergen paper only constructed a single solution).  The authors additionally show that a Clarke stationary point of the nonconvex objective corresponds to a global minimum of a subsampled version of the convex problem, and they provide a polynomial time algorithm to test if a neural network is globally optimal.  Finally, the authors prove that the nonconvex loss landscape has no spurious local minima provided the number of neurons is large enough.\n', 'main_review': 'Strengths of the paper:\n- The authors provide a complete characterization of the global minima to the nonconvex 2-layer ReLU network training problem, in terms of the solutions of a convex program\n- The approach does not rely on duality and/or lifting perspectives of previous convex perspectives on neural network training\n- The approach provides an algorithm for testing optimality of a neural network in the studied context\n- The work provides significant extensions of the work upon which it is based\n\nWeak points of the paper:\n- The method only applies to 2-layer neural networks with ReLU activation and weigh decay.    It is a great contribution that may inspire further developments to weaken these assumptions.\n\nQuestions for authors:\n\nOne of the technical themes of the paper is the idea of minimal / nearly minimal networks.  Several of the theorems are stated explicitly in terms of nearly minimal networks.  Is it necessary to have minimality in the theorem statements?  That is, could the primary claims be stated without reference to minimality (and where minimality would be a technical detail in the proofs)?\n\n\nAdditional feedback with the aim to improve the paper:\n\nIn the model there are no bias terms.  I presume this is because they can be tucked into X with a row of 1\'s.  Perhaps this is worth a comment in the paper.\n\nTypo: Sec 1.3 ""neruons""\n', 'summary_of_the_review': 'Clear accept.  It significantly clarifies the relationship of the solutions of the nonconvex ReLU neural network training problem with a corresponding convex program.  That relationship allows them characterize all global minima and to test whether particular networks are indeed global minima.  While the work only applies in the 2-layer case, it is nonetheless a significant theoretical extension that can inspire extensions to multi-layer cases.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions', 'authorids': ['~Yifei_Wang2', '~Jonathan_Lacotte1', '~Mert_Pilanci3'], 'authors': ['Yifei Wang', 'Jonathan Lacotte', 'Mert Pilanci'], 'keywords': ['Neural networks', 'global optimization', 'convex optimization', 'convex analysis'], 'abstract': 'We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys.\nOverall, we provide a rich framework for studying the landscape of neural network training loss through convexity.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wang|the_hidden_convex_optimization_landscape_of_regularized_twolayer_relu_networks_an_exact_characterization_of_optimal_solutions', 'pdf': '/pdf/9733b1623c23b45535cc2c126e6fb496e55e8049.pdf', 'supplementary_material': '/attachment/f17e392702c46d321772e0ae0d51d86683a951a1.zip', '_bibtex': '@inproceedings{\nwang2022the,\ntitle={The Hidden Convex Optimization Landscape of Regularized Two-Layer Re{LU} Networks: an Exact Characterization of Optimal Solutions},\nauthor={Yifei Wang and Jonathan Lacotte and Mert Pilanci},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Z7Lk2cQEG8a}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Marine Schimel', 'Ta-Chu Kao', 'Kristopher Jensen', 'Guillaume Hennequin']",ICLR,iLQR-VAE _ control-based learning of input-driven dynamics with applications to neural data,https://iclr.cc/virtual/2022/oral/6080,2022," Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data.",Oral 2: AI applications,https://openreview.net/pdf?id=wRODLDHaAiW,https://openreview.net/forum?id=wRODLDHaAiW,wRODLDHaAiW,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper introduces a novel control-based variational inference approach that learns latent dynamics in an *input-driven* state-space model. An optimal control solution (iLQR) is implicitly used as the recognition model which is fast and compact. Reviewers unanimously agree on the high quality writing and high significance of the work. This paper advances the horizon of nonlinear dynamical system models with unobserved input, an impactful contribution to the neuroscience and time series communities.'}, {'summary_of_the_paper': 'This paper proposes ILQR-VAE, a novel method that allows to simultaneously learn latent dynamics and infer unobserved control inputs. The method relies on IQLR solver and recent advances allowing for implicit differentiation to maximize an Evidence Lower Bound on log-likelihood of observation to infer a conditional distribution over inputs as well as latent states. \nAuthors show comparisons to other models (the closest one being LFADS) on toy datasets and benchmark datasets for neural data analysis methods. \niLQR-VAE is on par with state-of-the-art methods on many datasets, does not require any extensive hyperparameter optimization, and allows for fast inference when dimension of the latent processes is not too high. ', 'main_review': ""Authors present a novel method for simultaneously learning latent dynamics and inferring unobserved control inputs. The method performs very well on several toy datasets (autonomous and input-driven linear dynamical system + Lorenz attractor), is on par with state of the art methods on benchmark datasets for neural data analysis methods, and allows for better reconstruction of hand kinematics for primate recordings in a continuous reaching task. The main novelty of the method lies in the utilization of iLQR with implicit differentiation. The paper also extensively discusses LFADS and provides very interesting insights on LFADS.\n\nAn important component  of the method is the prior on input distribution. Authors show how to use a Gaussian prior or a multivariate Student prior (it allows for strong inputs when needed, represents the fact that inputs come as shared events and are spatially and temporally independent, and authors mention it might be one advantage over LFADS's autoregressive prior). This is a key idea of the paper and I would like to see more experiments highlighting the importance of the prior. I would recommend that the authors show (possibly in the appendix), the differences between the inferred inputs and performances when using different priors. \n\nThe derivation of the ELBO does not seem straightforward to me. To go from equation 7 to 8, don't you need to condition p(o|u) on z_0 as well? Maybe I am not correct, but I would appreciate if authors clarify this point (and correct the manuscript if needed)  \n\nAbout the computational complexity. The model is linear in T and cubic in n, which seems fast for low-dimensional latents only. Authors mention that iQLR-VAE enables fast learning of dynamics, but they also mention it as a limitation in the discussion. Could authors clarify this? Moreover, I believe the computational complexity comparison with LFADS in Figure 1 bottom left is not fair. Authors compare the number of training iteration without comparing the complexity of each iteration. I would like to see comparison for other datasets as well. \n\nThere are many performance metrics used in this paper to show that iQLR-VAE performs as well as other state of the art methods. However, there are no comparisons of the processes learnt by the different methods. Showing superimposed processes inferred by different methods  (not only LFADS) for several experiments would allow to gain insight into the methods and how they compare (in term of uncertainty, smoothness, etc...). Similarly, for Figure 4, it would be interesting to see a low dimensional representation of neural activity (and comparisons of different methods). For this experiment, is it necessary to use a latent processes of dimension 50? Wouldn't it be sufficient and more efficient to use lower dimensional latent for this task? \n\nMinor point : \n - In figure 4 caption, there is a typo and the last part should be panel D not C.\n"", 'summary_of_the_review': 'I recommend acceptance of the paper. This is a very interesting and novel method, that is shown to perform very well on several different datasets. \nThere are many quantitative experiments but showing qualitative comparisons would be very beneficial for gaining insights into how different methods compare. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes a control-based variational inference approach that learns latent neural dynamics in input-driven SSM. It utilizes iLQR in the recognition model that transforms it into an optimal-control problem. The recognition model in the proposed method is implicitly implied by the generative model and thus reduces the number of free parameters comparing to existing methods. The proposed methods are evaluated on sythetic chaotic attractor and real-world neural recordings.\n', 'main_review': 'Post-rebuttal: Thanks for addressing my concerns. The authors have updated the manuscript and added experiments. I will not change my recommendation.\n\n##########\n\nPros: \n\n+ Implicit recognition model implied by the generative model. It incorporates the inferred forward dynamics.\n\n+ Competitive performance to STOA method\n \n+ Small parameter space for easy hyperparameter tuning\n\nConcerns: \n\n- The method is extended and compared to LFADS. How about with other methods?\n\n- Unlike the bidirectional RNN in LFADS, the implicit recognition model should incorporate the dynamics from the trained generative model. This bias could be good or bad for the training. Can the authors address more?\n\n- There are other methods like VIND consider to incorporate the dynamics from the trained generative model. This better be compared or discussed.\n\n- Is this method fast or slow comparing to LFADS or other methods?\n\n- How well can the proposed method learn other typical types of dynamical systems such as fixed attractors, continuous attractors and etc.\n\nMinor:\n\n* Typo: ""LFDADS"" in paragrah 2, page 2', 'summary_of_the_review': 'Overall, I vote for accepting. The paper is well written. I like the idea of transforming to control problem. The implicit recognition model also interesting. This would be useful to learn neural dynamics from large population recordings. Hopefully the authors can address my concern in the rebuttal period. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents a new approach for inference in a model that simultaneously provides latent dynamics, initial conditions, and - importantly - external inputs. This approach is enabled by using the outcome of an optimization algorithm (iLQR) in the recognition model, recently enabled by other work in the field. The paper demonstrates the use of this approach on simulated data and real neural data, and compares to other classic and contemporary models of nonlinear dynamical systems.', 'main_review': 'strengths\n---------\nThe problem of simultaneously learning dynamics and ongoing external inputs is a crucial one, especially in fields such as neuroscience where complex dynamical systems are often analyzed through incomplete measurements of the system (i.e. not every neuron is monitored). This paper presents a sound approach to tackling this problem, though I admit my understanding if the control field is limited.\n\nThe paper is clearly written and easy to follow.\n\nI appreciate building up the complexity of the toy examples.\n\nComaprison to state-of-the-art methods is nice and clear.\n\n\nweaknesses\n----------\nFig. 1: ""Indeed, iLQR is unable to find initial conditions that would explain the data well, resulting in a much higher initial loss."" While the paper implicitly points to this outcome as an advantage of the more flexible prior, it is concerning that iLQR cannot find the initial condition of this very simple example. Is this an issue with the iLQR algorithm itself? The chosen prior? It would be very useful to understand this behavior in the simple example before moving on to more complex examples.\n\nThe leap from example 1 (nonlinear autonomous dynamics) and example 2 (linear dynamics + sparse inputs) to the real data (presumably nonlinear dynamics + ongoing inputs) begs the question: how well does the model perform with (nonlinear dynamics + sparse inputs)? Showing good performance in this regime would increase my confidence in the model\'s ability to handle more complex, real-world data.\n\nThere is very little reference to related work, especially with regards to inferring control inputs in dynamical systems (linear or nonlinear). The paper would feel much more complete with this information (which could be provided as an additional appendix if space is short).\n\nThe general problem of decoupling ongoing dynamics from external inputs is underdetermined, and results rely heavily on the structure imposed on these aspects of the model. In this paper the prior on the inputs is a sparse one, but what happens if there is mismatch between this prior and the real data? How will that affect the results (and their interpretation)? The paper comments on this several times (and shows how an AR prior in LFADS performs when inputs are sparse, but not the reverse), but I\'d appreciate a more thorough discussion of this issue in the Discussion.\n\nLearning in the proposed model seems to have some strong parallels with the EM algorithm, except that maxima from iLQR replace expectations. Is this true? Could be interesting to briefly draw this connection when describing the inference strategy.\n\n\nminor\n-----\nFig. 1 caption: first sentence, add ""system"" to end?\n\nparagraph that starts ""At the beginning..."", second sentence, typo: ""learning consists in _making_""\n\nsame paragraph: ""We note that this regime is facilitated here by our choice of generator dynamics, which we initialised to be very weak initially and therefore easily controllable."" What does it mean for the generator dynamics to be ""initialised to be very weak""? This is potentially an interesting point that is unclear to me as written. What does ""weak initialisation"" mean in this context?', 'summary_of_the_review': ""I'd lean towards accepting this paper, due to its novelty and thoroughness. I'd be willing to argue even more strongly for it if the authors could address some of the points above (especially the more complex toy example)."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data', 'authorids': ['~Marine_Schimel1', '~Ta-Chu_Kao1', '~Kristopher_T_Jensen1', '~Guillaume_Hennequin1'], 'authors': ['Marine Schimel', 'Ta-Chu Kao', 'Kristopher T Jensen', 'Guillaume Hennequin'], 'keywords': ['neuroscience', 'latent variable models', 'RNN', 'VAE', 'motor control', 'control theory', 'dynamical systems'], 'abstract': 'Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'schimel|ilqrvae_controlbased_learning_of_inputdriven_dynamics_with_applications_to_neural_data', 'pdf': '/pdf/c4b2a10a835b79e5cbaff71f6577c29236e964b5.pdf', 'one-sentence_summary': 'We develop a novel autoencoder that uses iLQR as an inference model and apply it to synthetic data as well as neural recordings from primate motor cortex.', 'supplementary_material': '/attachment/d56eef08c847ab2b7a4c4bdea44fe6464442601c.zip', 'data': '', '_bibtex': '@inproceedings{\nschimel2022ilqrvae,\ntitle={i{LQR}-{VAE} : control-based learning of input-driven dynamics with applications to neural data},\nauthor={Marine Schimel and Ta-Chu Kao and Kristopher T Jensen and Guillaume Hennequin},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=wRODLDHaAiW}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Asiri Wijesinghe', 'Qing Wang']",ICLR,A New Perspective on _How Graph Neural Networks Go Beyond Weisfeiler-Lehman__,https://iclr.cc/virtual/2022/oral/6437,2022," We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency.",Oral 2: Structured learning,https://openreview.net/pdf?id=uxgg9o7bI_3,https://openreview.net/forum?id=uxgg9o7bI_3,uxgg9o7bI_3,"[{'title': 'Related Work', 'comment': 'Thanks for the great work and congratulations!\n\nWhile reading the paper, it came to me that a previous paper of ours might be closely related in terms of the attempt to build a hierarchy (with local structures) to construct GNNs with different levels of discriminative powers beyond 1-WL, with provable justifications: [A Hierarchy of Graph Neural Networks Based on Learnable Local Features](https://arxiv.org/abs/1911.05256)\n\nI wonder if there might be some relation in the attempt in utilizing proper neighborhood subgraphs including the edges, but it would be very interesting to have a comparison or discussion.\n\n\n\n'}, {'title': 'Justification of adding 1', 'comment': ""The justification of adding 1 is with Lemmas 1 and 2 in the appendix. Without adding 1, these lemmas would fail. For example, two distinct pairs like (H_1, W_1) and (H_2, W_2) where W_1=W_2 and H_1 $\\neq$ H_2 cannot be distinguished. \n\nThe function $g$ has already been defined on Page 6 in the main content of the paper. Therefore, we don't define it again in the supplementary material.""}, {'title': 'add 1 design is not discussed.', 'comment': 'In Section 4,\n""Note that, to ensure the injectivity in the feature aggregation in the presence of structural coefficients, we add 1 into the first and second terms in Eq. 5. This design is critical for guaranteeing the expressiveness of GraphSNN beyond 1-WL, as will be discussed in the proofs of the lemmas and Theorem 4 later.""\nHowever, this design is not discussed in appendix C.\n\nBTW, in the proof of Theorem 4, ""For the second condition, by Lemmas 1 and 2 as well as the fact that MLP as a universal\napproximator (Xu et al., 2019) can be used to model and learn the functions f and g, we know that\nGraphSNN also satisfies this condition.""\nHowever, Lemmas 1 and 2 only use f, not g.\n'}, {'title': 'Author Response', 'comment': 'Hi Lingxiao,\n\nThank you very much for pointing this out to us!\n\nWe will check the reporting settings of all the baselines and revise the results to ensure a fair and consistent comparison in our final paper.'}, {'title': 'Problems about the results on TUDatasets ', 'comment': 'Congratulation to the great work! \n\nI would like to point an issue of the results on TUDatasets.\nI have checked the code that the reported accuracy is based on the average of 10 maximum validation accuracy on each fold (this value is higher than it should be). \nHowever all baselines are calculating the validation accuracy by  1) first averaging 10 validation accuracy curves, 2) picking up the epoch based on the best accuracy on the **averaged validation curve**, 3) report the mean accuracy and std based on the selected epoch. I noticed this issue because I also made the mistake before in our submission but corrected it later. \n\nAlthough we all know that there are lots of problems of these small TUDatasets and the evaluation procedure we followed is also not the ""best"", I still want to mention this issue so that the author can revise the result to keep a fair comparison. This is also important for future researchers to cite the correct result of the paper. Nevertheless this is still a great work! '}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes an efficient method for message passing that can incorporate structural information that is provably stronger than 1-WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the over smoothing problem. Overall speaking, all the reviewers like this paper quite a lot, although the also raised some minor concerns. The paper also attracted some unofficial reviewers who provided quite a few related works. The authors did a good job in interacting with the reviewers and addressing their minor concerns. So, we believe the paper is worth accepting, and could be a significant work in the field of graph neural networks.'}, {'title': 'After rebuttal ', 'comment': ""I thank the authors for addressing my comments. I appreciate also their response to the other reviewers' comments. I still believe it is a good paper, and I keep my score. ""}, {'title': 'Reviewer response', 'comment': 'I would like to keep my score.'}, {'title': 'Satisfied with the answers', 'comment': 'I thank the authors for their response. \nI am satisfied with the answers and think the scores the paper received are well deserved.\nI keep my original score. '}, {'title': 'Author Response', 'comment': 'Thanks for pointing out this paper. We will read it and have a check.'}, {'title': 'Author Response', 'comment': 'Thanks for pointing out this paper. We will read it and have a check.'}, {'title': 'Author Response', 'comment': 'Thanks for pointing out this paper. We will read it and have a check.'}, {'title': ' Related Work', 'comment': 'Thanks for the interesting work! Another related work is ""Nested Graph Neural Networks"" from NeurIPS 2021 (https://arxiv.org/pdf/2110.13197.pdf). It also uses subgraphs beyond subtrees to increase the expressive power of GNNs, and provably discriminates almost all regular graphs that 1-WL always fails. '}, {'title': 'Related Work', 'comment': 'Thanks for this interesting new perspective! \n\nI would like to point out our related work in NeurIPS 2021: ""Decoupling the depth and scope of Graph Neural Networks"" (https://papers.nips.cc/paper/2021/file/a378383b89e6719e15cd1aa45478627c-Paper.pdf). Our shaDow-GNN model also provably exceeds 1-WL by utilizing local subgraph structure, while being highly efficient and scalable. '}, {'title': 'Author Responses to Reviewer Z8WJ', 'comment': 'Q2. Over-smoothing - the empirical results are not explained. Do the authors have a conjecture as to why their proposed method circumvents over smoothing?\n\nThe reason why our method GraphSNN can alleviate over-smoothing is because structural coefficients capture structural connectivity between a target vertex and its neighbors. Thus, a neighbor whose structural connectivity is weak would pass little messages to the target vertex, whereas a neighbor whose structural connectivity is strong would pass a strong message to the target vertex.\n\nThanks for the suggestions on generalization to unseen graphs. This is an interesting research direction. We will explore it in our future work.'}, {'title': 'Author Responses to Reviewer Z8WJ ', 'comment': 'Thanks for the comments.\n\nQ1. Expressive power experiments - also an experimental evaluation showing what types of structures the proposed architecture can distinguish would be interesting to see (e.g., cycles, d-regular graphs).\n\nWe have added the experimental results and discussion into Appendix B in the revised version.\n\nWe consider an experimental evaluation setup called BL, which serves as the baseline for all experiments in this ablation study. In the setting of BL, the AGGREGATE$^I$ in GraphSNN is set to 1. Then, different variants of BL consider different local substructure counts as additional node features. This allows us to analyse what types of local substructures our proposed architecture can distinguish. \n\nThere are five variants of BL being considered in the ablation study:\n\n> BL$_{SC}$: Setting AGGREGATION$^I$ of GraphSNN to 1 and keeping structural coefficients for neighbors.  \n\n> BL$^{clique}_{NF}$: Setting AGGREGATION$^I$ of GraphSNN to 1, removing structural coefficients for neighbors, and adding additional node features (triangle and 4-clique counts) into the original feature vectors.\n\n> BL$^{clique}_{SC+NF}$: Setting AGGREGATION$^I$ of GraphSNN to 1, keeping structural coefficients for neighbors, and adding additional node features (triangle and 4-clique counts) into the original feature vectors.\n\n> BL$^{cycle}_{NF}$: Setting AGGREGATION$^I$ of GraphSNN to 1, removing structural coefficients for neighbors, and adding additional node features (cycle counts) into the original feature vectors.\n\n> BL$^{cycle}_{SC+NF}$: Setting AGGREGATION$^I$ of GraphSNN to 1, keeping structural coefficients for neighbors, and adding additional node features (cycle counts) into the original feature vectors.\n\nWe compare GraphSNN with GSN-v [1], BL$^{clique}_{NF}$, BL$\\_{SC}$ , and BL$^{clique}\\_{SC+NF}$ to analyze how our proposed architecture relates to the models with triangle and 4 clique counts as additional node features.\n\nSimilarly, we compare GraphSNN with ID-GNNs [2], BL$^{cycle}\\_{NF}$, BL$\\_{SC}$, and BL$^{cycle}\\_{SC+NF}$ to analyze how our proposed architecture relates to the models with cycle counts as additional node features. We concatenate the counts of cycles with length 1 to 4 starting and ending at the given source node with its original feature vector as in [2].\n\nTable 1 and Table 2 show the experimental results. As AGGREGATE$^I$ is set to 1 in the setting of BL, the performance gap between BL$\\_{NF}$ and BL$\\_{SC+NF}$ reflects the effectiveness of structural coefficients on enhancing relational inference between a target vertex and its neighbors. The performance gap between BL$\\_{SC}$ and GraphSNN below shows the effectiveness of AGGREGATE$^I$ in our proposed model GraphSNN. Furthermore, BL$\\_{SC+NF}$ consistently performs best since we incorporate both extra node features and structural coefficients into the feature aggregation. There is a small performance gap between BL$\\_{SC+NF}$ and GraphSNN due to augmented node features that can capture additional structural information that cannot be captured using structural coefficients.\n\n|Dataset    |GSN-v [1]       | BL$^{clique}_{NF}$          | BL$_{SC}$            | BL$^{clique}_{SC+NF}$        |GraphSNN        |\n|:---       |:----:         |:----:         |:----:           |:----:           |:----:         |\n|MUTAG      |92.20$\\pm$7.5   |90.21$\\pm$2.3  |94.06$\\pm$2.4    |95.16$\\pm$2.5    |94.70$\\pm$1.9|\n|PTC-MR     |67.40$\\pm$5.7   |67.13$\\pm$2.9 |70.18$\\pm$3.1    |71.04$\\pm$3.1    |70.58$\\pm$3.1| \n|PROTEINS   |74.59$\\pm$5.0   |76.42$\\pm$2.6 |78.05$\\pm$2.3    |78.66$\\pm$2.1   |78.42$\\pm$2.7|\n|BZR        |    -          |86.82$\\pm$3.1  |90.67$\\pm$3.1    |91.98$\\pm$3.2    |91.12$\\pm$3.0|\n|IMDB-B     |76.80$\\pm$2.0   |77.00$\\pm$3.1  |77.23$\\pm$2.8    |78.53$\\pm$2.9    |78.01$\\pm$2.8|\n\n\n|Dataset    |ID-GNN [2]       | BL$^{cycle}_{NF}$          | BL$_{SC}$            | BL$^{cycle}_{SC+NF}$         |GraphSNN        |\n|:---       |:----:         |:----:         |:----:           |:----:           |:----:         |\n|MUTAG      | 96.50$\\pm$3.2 |91.36$\\pm$2.1  |94.06$\\pm$2.4    |96.61$\\pm$2.3    |94.70$\\pm$1.9|\n|PTC-MR     | 61.90$\\pm$5.4   |67.57$\\pm$3.3 |70.18$\\pm$3.1    |71.76$\\pm$3.2    |70.58$\\pm$3.1| \n|PROTEINS   | 78.00$\\pm$3.5   |77.26$\\pm$2.5 |78.05$\\pm$2.3    |78.95$\\pm$2.5   |78.42$\\pm$2.7|\n|BZR        | 86.40$\\pm$3.0       |86.83$\\pm$3.3  |90.67$\\pm$3.1    |91.75$\\pm$3.4    |91.12$\\pm$3.0|\n|IMDB-B     |       -       |76.36$\\pm$2.6  |77.23$\\pm$2.8    |78.58$\\pm$2.4    |78.01$\\pm$2.8|\n\n[1] Improving graph neural network expressivity via subgraph isomorphism counting. Giorgos et al., arxiv.\n\n[2] Identity-Aware Graph Neural Networks. Jiaxuan You et al., AAAI 2021.\n\n'}, {'title': 'Author Responses to Reviewer HLe3', 'comment': ""Thanks for the suggestions. They're very constructive.\n\nQ1. In the beginning of page 2, the sentence ``compared with the methods of augmenting node identifiers ...'' needs more explanation. If I understand correctly, the latter claim is from the ablation study of  in Section 5.3. But I do not see its relationship with node id/random feature models.\n\nID-GNN [1] counts the number of cycles at each level of the computation graph, augments it with the root node's feature vector, and then simply applies the homogeneous message passing scheme. In the work of Sato et al. [2], i.e., the earliest work that has considered the effects of random features on the expressive power of GNNs, it has shown that the expressive power of GNNs can be improved by adding random features to each node. For instance, if each node is assigned a random feature, GNNs can determine the existence of a cycle of length m by checking whether there exists the same value as the root node at depth m. Thus, random features are considered as being able to identify nodes and accordingly identify cycles. \n\nIn our work, we don't explicitly augment these kinds of features. Furthermore, structural coefficients in our work can flexibly quantify various local structures in an overlap graph, and can also capture structural properties for different graph learning tasks by adjusting the value of $\\lambda$. We have added an explanation in the revised version. \n\n[1] Identity-Aware Graph Neural Networks, Jiaxuan You et al., AAAI 2021.\n\n[2] Random Features Strengthen Graph Neural Networks, Sato et al.\n\nQ2. According to the definition of $\\tilde{A}_{vu}$ with normalization above (5), can the first summation over $u \\in \\mathcal{N}(v)$ be simplified as 1?\n\nThe first summation of Eq. 5 can be simplified as $\\Big( \\sum_{u \\in \\mathcal{N}(v)}  \\tilde{A}\\_{vu} + 1\\Big)=1+|N_v|$, where $|N_v|$ represents the number of neighbors of vertex $v$. The reason why we add 1 for each neighborhood structural coefficient $\\tilde{A}_{vu}$ in this term is to ensure the injectivity of the feature aggregation in the presence of structural coefficients.\n\nQ3. Since structural coefficients emphasize strongly connected neighborhood, I am wondering whether it would hurt the performance when there is a task requiring long-range information [1] and the path is, to some extent, adversarially going through a path with weaker connectivity.\n\nWe need multiple GNN layers for long-range tasks. A node's receptive field grows exponentially with the number of layers in some of the GNN models such as GCN and GIN. Therefore, GCN and GIN suffers from the over-squashing problem since every node of them performs the direct summation of its neighbors. However, our method GraphSNN can alleviate this issue, because structural coefficients capture structural connectivity between a target vertex and its neighbors. Thus, a neighbor whose structural connectivity is weak would pass little messages to the target vertex, whereas a neighbor whose structural connectivity is strong would pass a strong message to the target vertex. Generally, studying tasks that require long-range information which is adversarially going through a path with weaker connectivity is an interesting research direction. We will explore it further.\n\nQ4. With respect to oversmoothing on node classification, I am wondering whether the graph operator (can be derived from (5)) with different $\\lambda$ and $\\gamma$ has dominating subspace [3] that aligns well with the labels. A possible experiment is to send features through leading eigenvectors and then do pure MLP, following [2].\n\nOur model does not behave like a spectral GNN, and has a different way to leverage structural information. Concretely, spectral GNNs leverage the spectrum of Laplacian through low-pass/high pass filters which cannot distinguish how a target vertex is structurally connected to each of its neighbors. In contrast, our model can capture such structural connectivity information using structural coefficients defined on overlap subgraphs. We observe that the structural connectivity information is important for alleviating over-smoothing. We have conducted an experiment to compare the overmoothing of spectral GNNs by DFNets with our proposed method GraphSNN (please refer over-smoothing analysis section in Appendix B for the experimental results of DFNet and GraphSNN). The results show that DFNet suffers more with over-smoothing when increasing the layers.\n\nQ5. Expressive GNNs typically show better performance on graph regression tasks than graph classification. So it would be better to show comparision with expressive baselines on regression datasets such as QM9 and ZINC, instead of Table 4, if time permits.\n\nThanks for your suggestion. We may need some time to implement and evaluate it for graph regression problem. We are planning for future work to evaluate GraphSNN on regression datasets such as QM9 and ZINC.\n""}, {'title': 'Author Responses to Reviewer FAad', 'comment': 'Thanks for the comments. The reason why our method GraphSNN can alleviate over-smoothing is because structural coefficients capture structural connectivity between a target vertex and its neighbors. Thus, a neighbor whose structural connectivity is weak would pass little messages to the target vertex, whereas a neighbor whose structural connectivity is strong would pass a strong message to the target vertex. We have added the explanation into Section 5.4 in the revised version.\n\nFor the minor comments, below are our responses:\n\nQ1. We have revised the writing in the abstract and introduction.\n\nQ2. Thanks for pointing out the spelling inconsistencies of ""Lehman"" and ""Leman"". We have checked the paperand the revised version sticks to Lehman throughout.\n\nQ3. I don\'t believe that the citation of ""Provably Powerful Graph Networks\' of Maron et al. is accurate. It is cited as a powerful (3 WL) method that is expensive, however, I think it is an efficient method. The authors might want to verify this claim. \n\nWe have revised the claim to make it more accurate.\n\nQ4. For the typos on Pages 1 and 2, thanks for pointing them out. We have fixed them in the revised version.\n\nQ5. If no domain knowledge is required, then various simple sub-structures would need to be considered, for example, triangles, 4-cliques, 5-cliques, ..., paths of length 2, length 3, ..., cycles of length 4, length 5, ..., 3-star, 4-star, ... or simply subgraphs with n vertices. This would lead to inefficiency and affect effectiveness as well. Thus, domain expert knowledge is often needed by these methods so as to choose only certain substructures of interest. Below are some examples:\n\nLRP [1] counts the number of times a given pattern such as 3-star, triangle, tailed triangle, chordal cycle and attributed triangle appears as a subgraph or induced subgraph in the original graph.\n\nGSN [2] incorporates handcrafted topological features such as the presence of cliques or cycles, which requires expert knowledge on what features are relevant for a given task [3].\n\n[1] Can Graph Neural Networks Count Substructures? Chen et al., NeurIPS 2020.\n\n[2] Improving Graph Neural Network Expressivity via Subgraph\nIsomorphism Counting, Bouritsas et al., arXiv.\n\n[3] Building powerful and equivariant graph neural networks with structural message-passing, Vignac et al., NeurIPS 2020.'}, {'title': 'Author Responses to Reviewer csQY', 'comment': ""Thanks for the suggestions on improving the presentation. We have revised Fig 2 to make it more comprehensive, and also added some explanation for $\\omega$ in Eq. 4.\n\nFor the minor comments, below are our responses:\n\nQ1. '0.05 level of significance': how do you define it? Please elaborate.\n \nIn our work, '0.05 level of significance' corresponds to 95\\% confidence level, which is commonly used in the literature (e.g. [1,2,3]) to ensure the statistical robustness of experimental results. \n\n[1] Adaptive Universal Generalized PageRank Graph Neural Network, Chien et al., ICLR 2020.\n\n[2] Learning from the Past: Continual Meta-Learning with Bayesian Graph Neural Networks, Luo et al., AAAI 2020.\n\n[3] Predict then Propagate: Graph Neural Networks meet Personalized PageRank, Klicpera et al., ICLR 2019.\n\n\nQ2. Any intuition on what the performance on the MUTAG dataset is not that great?\n\nMUTAG dataset is the smallest dataset that we use in our experiments on graph classification tasks. We don't have concrete ideas but some intuition may include: (1) small number of available graphs in total, i.e., only 188 graphs with 18 nodes and 20 edges on average for each graph; (2) small number of graphs for validation, i.e., only 19 graphs are used for validation in a single fold; (3) the classes of graphs are unbalanced, i.e., the majority class is twice as the minority class. \n\nQ3. Please double check the References to make them complete and consistent (e.g., Waiss Azizian et al..., Cosro....'33'?,2020)\n\nWe have checked the References and fixed it in the revised version.""}, {'title': 'Related Work', 'comment': 'Interesting paper! We would like to point the authors to our related work on random node initialization: https://arxiv.org/pdf/2010.01179.pdf'}, {'summary_of_the_paper': 'This paper deals with the very challenging and important problem of designing Graph Neural Networks (GNNs) that are more expressive. The authors propose a new GNN framework, that injects structural information into a message-passing aggregation scheme. The proposed architecture (GraphSNN) is shown to be more expressive than the 1-WL in distinguishing graph structure. Finally, the framework is validated in state-of-the-art node classification and graph classification benchmarks. ', 'main_review': ""The contribution of the work is definitely relevant and significant. The framework is solid, well-justified, and supported by proofs. The experimental results are quite convincing as they confirm the good performance of the method in classical datasets. \n\nI would however encourage the authors to work a bit more on the presentation of the paper. While the paper is generally well-written, there are parts that are difficult to follow for someone who is familiar with GNNs, but not working exactly on aspects of expressivity. For example, the notions of subgraph-isomorphic, overall-isomorphic, and subtree-isomorphic are very technical, and hard to follow. I would appreciate it if the authors could make them more clear, by for example, explaining better Fig. 2. \n\nSimilarly, the proposed choice of \\omega, in Eq. 4, should be better explained/motivated.  \n\nSome additional minor comments:\n   - '0.05 level of significance': how do you define it? Please elaborate. \n   - Any intuition on what the performance on the MUTAG dataset is not that great?\n   - Please double check the References to make them complete and consistent (e.g., Waiss Azizian et al..., Cosro....'33'?,2020)\n\n"", 'summary_of_the_review': 'This is a good paper, with some aspects of the presentation that should be improved. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes an efficient method for message passing that can incorporate structural information (that of neighborhood subgraphs) that is provably stronger than 1-WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the oft-quoted ""over smoothing problem"". The benefit of better expressivity coupled with the simplicity of the method is borne out in extensive experimental results and a satisfactory ablation. ', 'main_review': 'This paper proposes an efficient method for message passing that can incorporate structural information (that of neighborhood subgraphs) that is provably more expressive than 1-WL. As compared to three strands of provably powerful (more than 1 WL) GNNs, the method has limited additional computational overhead, and can also show encouraging results on the oft-quoted ""over smoothing"" problem. This is borne out in extensive experimental results and a satisfactory ablation. \n\nIn the recent literature on GNNs, there have emerged three directions of research that aim to construct procedures more expressive than 1 WL:\n1. Higher-order WL-based methods, which often can get prohibitively expensive.\n2. Using features generated from substructures (most of the papers taking this approach claim this information is available through domain knowledge, but often just use triangles and the likes). \n3. Augmenting node features with identifying information to improve expressive power. \n\nThe central idea of the paper is grounded in the common observation that treating the neighborhood as a multiset of features ignores rich topological information, limiting the expressivity of message passing procedures that use such a representation. If the neighborhood is represented as a neighborhood subgraph, then 1-WL is only as powerful as distinguishing neighborhood subgraphs in terms of subtree structures.  The question then becomes if structural information can be incorporated in a way that can go beyond neighborhood subtree isomorphism. Towards this end, the authors show that there exists a class of isomorphic graphs that lie in between neighborhood subgraph isomorphism and neighborhood subtree isomorphism, which they call overlap subgraph isomorphism. It is shown that by incorporating structural information that can solve overlap subgraph isomorphism, one gets a message-passing network that is more expressive than WL. \n\nTo be more specific, section 2 provides the hierarchy of local isomorphism mentioned above. Theorem 1 states that subgraph isomorphism implies overlap subgraph isomorphism but not vice-versa, and that overlap subgraph isomorphism implies subtree-isomorphism but not vice-versa. 1-WL can only distinguish those graphs that can solve for sub-tree isomorphism at each layer. In order to go beyond 1-WL, the authors focus on overlap-subgraph isomorphism and define a set of structural coefficients for each vertex based on overlap subgraphs. These coefficients depend on reasonable notions of closeness, density, and invariance. The exact form of such coefficients is shown in section 4, and it is also shown that incorporating such information can give a method strictly more powerful than 1 WL. Since it just admits the same message passing paradigm, the computational overhead is limited. \n\nExtensive experiments on node classification (Cora, CIteseer, Pubmed, NELL, ogbn-arxiv), graph classification for the commonly used small graph datasets, and large graphs show across-the-board improvement compared to the competition (including the higher-order methods). The ablation shows the importance of the structural coefficients. Further, another set of experiments show that the proposed method is able to avoid the so-called oversmoothing problem (however, the results are presented without comment -- it would be beneficial to share some intuition on why this is the case). \n\nIn summary, I think the paper makes a solid contribution. It proposes a well-motivated method and validates it by extensive experimentation that leaves little doubt on its efficacy. \n\nMinor comments: \n- The paper will benefit from sharpening the writing in the abstract and intro -- it feels a little bit wayward and has some convoluted sentences. \n- The paper title, and most of the paper itself, uses the spelling ""Lehman."" The actual spelling is Leman. Somewhere after the intro, when the original WL paper is cited (and in some following sentences), the correct spelling is used. I would suggest either using the correct spelling throughout, or sticking to Lehman throughout (since it is widely used, and Leman himself did not mind it https://www.iti.zcu.cz/wl2018/pdf/leman.pdf)\n- I don\'t believe that the citation of ""Provably Powerful Graph Networks\' of Maron et al. is accurate. It is cited as a powerful (3 WL) method that is expensive, however, I think it is an efficient method. The authors might want to verify this claim. \n- Line 4 from the bottom of page 1: typo ""This solution enables GNNs to provably more expressive"": -> ""This solution enables GNNs to provably be more expressive""\n- Line 2 from the bottom of page 1: ""which require high computational overheads and are impractical"" -> ""which require high computational overhead and are impractical""\n- Line 1 page 2: ""(2), our method does not require any domain"" I am not sure it is accurate to say that these methods require domain knowledge. They can certainly benefit from it, and this claim is made in those papers, but they just stick to simple sub-structures such as triangles that are easy to treat. This repeats on page 3. \n- Line 3 of the second paragraph, page 2: ""capacity of a model"" --> ""capacity of the model""\n\nDisclaimer: I have not verified the proofs for correctness. However, based on the statements of the theorems and the general idea, I can buy them to be true and base my review on that assumption (since I expect these statements to be true). ', 'summary_of_the_review': 'Well motivated method that is provably more powerful than 1-WL, which incorporates structural information in a principled manner using the notion of overlapping-subgraph ismorphism. Extensive experimentation shows strong performance compared to the competition (including the higher-order WL procedures). ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'To overcome expressive weakness of message passing neural networks and computational efficiency of expressive GNNs, this paper proposes a new perspective by introducing overlap subgraphs and overlap isomorphism which is between subgraph isomorphism and subtree isomorphism. Based on that, it carefully design GraphSNN with local structural coefficients to control message passing to obtain more expressive power than 1-WL GNNs. Experiments are conducted on node-level and graph-level classification tasks with an abalation study on a key hyperparameter.', 'main_review': ""* Pros:\n\n1. Overlap isomorphism is a nice point to find a balance between expressive power and computational efficiency. Based on that, it is persuasive to encorporate the three properties into structural coefficients.\n2. The setting of $\\lambda$ is very interesting to me. It brings the coefficients some flexibility so that different $\\lambda$'s may capture inherent key information in different learning tasks, as shown in experiments.\n3. GraphSNN shows impressive experiment results. On node classification, it improves the counterparts of traditional GNNs by considerable boost, but is also very easy to use. On graph classification, it outperforms baselines by an intriguingly large margin. The performance drops slowly when it stacks more layers.\n\n* Concerns:\n\n1. In the beginning of page 2, the sentence ``compared with the methods of augmenting node identifiers ...'' needs more explanation. If I understand correctly, the latter claim is from the ablation study of $\\lambda$ in Section 5.3. But I do not see its relationship with node id/random feature models.\n2. According to the definition of $\\tilde{A}_{vu}$ with normalization above (5), can the first summation over $u\\in \\mathcal{N}(v)$ be simplified as 1?\n3. Since structural coefficients emphasize strongly connected neighborhood, I am wondering whether it would hurt the performance when there is a task requiring long-range information [1] and the path is, to some extent, adversarially going through a path with weaker connectivity.\n4. With respect to oversmoothing on node classification, I am wondering whether the graph operator (can be derived from (5)) with different $\\lambda, \\gamma$ has dominating subspace [3] that aligns well with the labels. A possible experiment is to send features through leading eigenvectors and then do pure MLP, following [2].\n5. Expressive GNNs typically show better performance on graph regression tasks than graph classification. So it would be better to show comparision with expressive baselines on regression datasets such as QM9 and ZINC, instead of Table 4, if time permits.\n\nReference:\n\n[1] On the bottleneck of graph neural networks and its practical implications. U Alon, E Yahav.\n\n[2] Revisiting Graph Neural Networks: All We Have is Low-Pass Filters. NT Hoang, T Maehara.\n\n[3] Graph Neural Networks Exponentially Lose Expressive Power for Node Classification. Kenta Oono, Taiji Suzuki."", 'summary_of_the_review': 'I would like to recommend to accept this paper, for its expressive, efficient and easy-to-use GNN component with intriguing performance.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper introduces a general framework for designing message-passing neural networks (MPNNs) stronger than the 1-WL distinguishing power via structural information injection to the aggregation scheme in the message passing framework. Unlike common MPNNs which perform neighborhood aggregation based on a pre-defined fixed function (which does not depend on the neighborhood subgraph structure) or based on a learned function of the neighborhood node features, the paper suggests a weighting function in the aggregation step of MPNNs which depends on the structure of the neighborhood subgraph. The authors show that under reasonable conditions the resulting architecture has a superior expressive power than the 1-WL test. This also implies superiority in expressive power over common MPNNs while maintaining the same memory and time complexity. The paper introduces a hierarchy of 3 levels of neighborhood isomorphisms: subtree, overlap, and subgraph; which facilitate the proofs and theoretical guarantees. Furthermore, as a by-product, the authors show improved robustness to the notorious over smoothing problem in MPNNs.', 'main_review': '1. **Originality**  \n\nThe paper belongs to a body of recent works which aim to break the expressive power limit of MPNNs in a sub-quadratic memory complexity in the number of nodes. As far as I’m aware, the aggregation scheme suggested in the paper is novel, providing a constructive way to design more powerful architectures than the 1-WL test. \n\n3. **Experimental evaluation** \n\n   ***Strengths*** - the paper demonstrates a boost in performance on node and graph classification. \n\n   ***Weaknesses*** -\n\n   1. Expressive power experiments  - also an experimental evaluation showing what types of structures the proposed architecture can distinguish would be interesting to see (e.g., cycles, d-regular graphs). \n   2. Over-smoothing - the empirical results are not explained. Do the authors have a conjecture as to why their proposed method circumvents over smoothing?\n\n   ***Suggestions***\n\n   - Further discussion on generalization to unseen graphs (With larger / smaller average neighborhoods)? Since the aggregation coefficients suggested are normalized (edges/vertices) it might be that the suggested model is more robust to size differences in graphs. It would be interesting to see how it performs. For example, a simple experiment like the ones performed in [1].\n\n   \n- **Clarity** - paper is well written; claims are well supported by theoretical proofs and guarantees.\n\n\n[1] Yehudai, G., Fetaya, E., Meirom, E., Chechik, G., & Maron, H. (2020). On Size Generalization in Graph Neural Networks. ArXiv, abs/2010.08853.\n\n\n\n', 'summary_of_the_review': 'A well written paper with strong theoretical exposition and results. \n\n\n---\n**Post Rebuttal**: I am satisfied with the authors response and keep my original score. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'A New Perspective on ""How Graph Neural Networks Go Beyond Weisfeiler-Lehman?""', 'authorids': ['~Asiri_Wijesinghe1', '~Qing_Wang14'], 'authors': ['Asiri Wijesinghe', 'Qing Wang'], 'keywords': ['Graph Neural Networks', 'Graph Isomorphism', 'Weisfeiler Lehman'], 'abstract': 'We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wijesinghe|a_new_perspective_on_how_graph_neural_networks_go_beyond_weisfeilerlehman', 'pdf': '/pdf/376e7da3d7f86a2bd40cd51fadfc278e94372443.pdf', 'supplementary_material': '/attachment/933ba75f12d12c8e12bde0edcad6fc41e3aaf1a4.zip', 'data': '', '_bibtex': ""@inproceedings{\nwijesinghe2022a,\ntitle={A New Perspective on ''How Graph Neural Networks Go Beyond Weisfeiler-Lehman?''},\nauthor={Asiri Wijesinghe and Qing Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uxgg9o7bI_3}\n}"", 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Ye Yuan', 'Yuda Song', 'Zhengyi Luo', 'Wen Sun', 'Kris Kitani']",ICLR,Transform2Act_ Learning a Transform-and-Control Policy for Efficient Agent Design,https://iclr.cc/virtual/2022/oral/6197,2022," An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=UcDUxjPYWSr,https://openreview.net/forum?id=UcDUxjPYWSr,UcDUxjPYWSr,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""The paper considers the problem of learning both the physical design (morphology and parameters) of a robot together with the corresponding control policy to optimize performance at a target task. Unlike several contemporary methods that formulate this as two separate, but coupled, optimization problems, the paper unifies these decisions into a single decision-making framework. More specifically, a conditional policy learns to first change an agent's physical design (i.e., the morphology/skeletal structure and its associated parameters), and then to control the design. The policy is formulated as a graph neural network, enabling a single policy to simultaneously control robots with different morphologies (and, in turn, different action spaces). Experimental results demonstrate that the approach outperforms recent baselines on a variety of simulated control tasks.\n\nThe paper considers an interesting and challenging problem, that of jointly optimizing an agent's physical design and its control policy, an area of research that has received renewed attention of-late. As the reviewers note, the idea of treating design and control in the context of a single decision-making process is novel. The approach is principled and the experimental results largely justify the significance of the contributions. The reviewers agree that the approach is described clearly and that the paper is well written. The reviewers initially raised a few concerns regarding the experimental evaluation, including the desire for more in-depth evaluations and the need for more random seeds. They also questioned some of the claims made in the initial submission. The authors provided a detailed response to each of these points and made changes to the paper to resolve most of the concerns.\n\nIn summary, the paper proposes a novel approach to an interesting problem with convincing results.""}, {'title': 'Response to author rebuttal', 'comment': ""Thanks for the response! \n\nIt's interesting to know that the design takes 6 steps in total only. I guess it makes sense since it can grow the robot exponentially. It'd be nice to make this clear explicitly in the paper.\n\nI'll keep my current score of 8.""}, {'title': 'Response to Authors', 'comment': ""Thank you for the detailed clarifications. I think now it's ready for publication and I have updated my score to 8.\n\nHowever, I am still not satisfied with only 4 environments, which makes results overall less exciting. Also, I strongly encourage authors to incorporate more discussions on the limitations.""}, {'title': 'Good enough', 'comment': ""Thanks to the authors for their response and changes. \n- The response to Q1 is not 100% convincing, as there are ways to overcome the limits of the original (expert-) design by increasing all bones' length and adding more joints to increase the limbs. However, the reviewer acknowledges that the comparison between morphology construction methods is still fair and that the resulting morphologies do not seem to exploit this (which raises the question why not).\n- While the authors very convincingly argued for asymmetry in Q3, it is still unclear why their JSMLP method provides a robust framework to induce this (e.g. how it generalizes when when nodes are deleted). This raises serious questions on how general this method is, e.g. when applied to another GNN task. However, it is worth mentioning that the authors are not alone in this (see https://openreview.net/forum?id=fy_XRVHqly for another approach that remains similarly unclear). \n- Some comments seem to have been misunderstood: the background section must clarify that $r_t = R(s_t, a_t)$ and that $D \\in \\mathcal D$ does not only change the transition model (as in the current version), but also the state and the action space.\n\nWhile the response does not answer 100% of this reviewers questions, the resulting paper is nonetheless interesting enough, and the results clear enough, to merit publication.  The reviewer is therefore raising the score to 8.""}, {'title': 'Response to Reviewer 42PW [2/2]', 'comment': '> **Q8:** *""...PPO on some standard morphology in each environment...""*\n\nIn Appendix A of the revised version, we perform experiments of training expert designs (provided by Gym) with PPO. By examining both Fig. 3 and 6, we can see that the designs obtained by our approach from scratch do outperform the expert designs in all environments. \n\n---\n\n### **Reference**\n\n[1] Kurin et al., ""My Body Is A Cage: The Role of Morphology in Graph-based Incompatible Control."" ICLR 2021.\n'}, {'title': 'Response to Reviewer 42PW [1/2]', 'comment': 'Thank you for the constructive feedback, suggestions, and pointers. We also appreciate the acknowledgment of our approach\'s novelty. We have revised the paper to include experiments with more seeds and baseline comparison of continuous design optimization for finetuning expert designs (please see the revision summary post for all changes). Here, we aim to address the questions raised in the review.\n\n---\n\n> **Q1:** *""The reward of the tested environments is not scale invariant...""*\n\nFor all the environments, there is an upper bound on the bone length, which is close to the initial bone length. For example, for 2D Locomotion and Gap Crosser, the upper bound is 0.6 while the initial bone length is 0.4. Therefore, there is not much room for the agent to exploit the bone length. We also have a torque penalty built in for 3D Locomotion and Swimmer. We’d also like to note that all the methods in the paper use the same reward function and design space for fair comparison.\n\n---\n\n> **Q2:** *""The results are based on 3 random seeds...""*\n\nWe follow the suggestion and run additional experiments to report results with a total of 6 seeds in the revised version. We hope the reviewer can understand that these new experiments cost a significant amount of compute, i.e., 100+ machine days in total, which limits the number of experiments we can run.\n\n---\n\n> **Q3:** *""The JSMLP method is complicated and poorly explained. It is also not clear to the reviewer why asymmetry is inherently favorable for robotics design. Even if one would like to include the possibility, the presented JSMLP method seems very ad-hoc and might have some unintended side effects...""*\n\n- We’d like to first motivate the need for asymmetry using the giraffe-like design obtained by our method for 2D Locomotion. If we look at the 3 grandchildren of the root joint, i.e., the two front legs and the torso, they are asymmetric. Without allowing any asymmetry in the design, it would be **impossible** to grow these different joints (torso and front legs) from the root since they are topologically equivariant, where the transform policy will treat these joints the same and output the same design changes. Similarly, if we want to design a humanoid from a chest joint, we would also need to grow asymmetric joints to develop various branches of body parts (arms, neck, and lower body).\n- For the JSMLPs, we\'d like to politely argue that its core idea is very simple and not ad-hoc, i.e., combining weight-sharing layers (GNNs, or attention layers as suggested) with non-weight-sharing layers (MLPs) together to balance the generalization and specialization across joints for the policies. With more (6) random seeds, we also empirically show that this solution is both simple and effective in boosting the performance and learning stability of our method. We also believe the problem of generalization vs. specification raised in JSMLPs in these node weight sharing settings would be worth considering for future work.\n- For the special case mentioned by the reviewer, where removing sibling joints could change a joint’s index, we can easily prevent that case by only allowing to remove the sibling joint with the largest index so that it won’t affect other joints.\n\n---\n\n> **Q4:** *""It is unclear whether GNN actually generalize well (in particular out-of-distribution) over different robot morphologies. Kurin et al. (2021) have shown some very convincing counter-examples and suggest to use attention instead of GNN layers...""*\n\nThank you for pointing out this alternative, which we believe is a promising future direction. We have discussed this option in the related work of the revised version. We’d like to mention that our approach by design is not limited to GNNs and can use other node weight sharing mechanisms such as the suggested attention layers from [1]. Due to limited amount of computational resources, we have prioritized other experiments (e.g., more random seeds and continuous design optimization), but we will try attention layers too to see if they can yield better performance than GNNs.\n\n---\n\n### Answers to detailed comments\nThank you for all the detailed comments which are very useful! Many comments are directly addressed in the revised paper. Here we provide answers to those that need explanations.\n\n---\n\n> **Q5:** *""when you introduce the design D, doesn\'t it also affect the state and action space?""*\n\nThe design $D$ will indeed affect the state and action space, and we have redefined the MDP in the ""MDP with Design"" paragraph in the initial submission.\n\n---\n\n> **Q6:** *""it is not clear why you allow the removal of joints, as it could lead to cycles""*\n\nAllowing the removal of joints is mainly for use cases where we want to finetune a given initial design.\n\n---\n\n> **Q7:** *""it would be good to have a \'stop changing the morphology\' action""*\n\nThis is a brilliant idea and would be interesting for future work to further improve efficiency.\n\n\n\n'}, {'title': 'Response to Reviewer ZAQQ', 'comment': 'Thank you for the constructive feedback and appreciation of our approach\'s novelty and potential impact. We have revised the paper to include experiments with more seeds and baseline comparison of continuous design optimization for finetuning expert designs (please see the revision summary post for all changes). Here, we aim to address the questions raised in the review.\n\n---\n\n> **Q1:** *""Formulations of some claims could be made more neutral...""*\n\nThank you for the suggestion, we have modified the claims accordingly.\n\n---\n\n> **Q2:** *""The paper refers to policy gradients (PG) as \'first-order\' optimization methods. This may not be exactly technically correct as these methods still only use zero order information about the objective function.""*\n\nWe acknowledge that this statement is a bit confusing, and we have modified it in the revised version. What we intended to say by “first-order” is that our method uses a parametrized transform policy to output design-changing actions, which enables us to use policy gradients methods for policy optimization, and policy gradients use the first-order information in the policy by taking the exact gradient of the policy w.r.t. its parameters. Our method, using a parametrized transform policy to output design changes, is more efficient because the policy allows better generalization and experience sharing among joints, where joints in similar states choose similar transform actions using the policy. Please see Appendix B of the revised version for a detailed discussion of our approach vs. ES-based methods.\n\n---\n\n> **Q3:** *""Q Table 1 in Appendix E shows that N_s=5 skeleton transforms and N_z=1 attribute transform were selected using hyperparameter search. What qualitative results does one obtain if there are more than 1 attribute transform? Why allowing more skeleton transforms performs worse? In principle, it should provide more flexibility. Adding a discussion explaining the effects of these numbers to the main body of the paper would be illuminating.""*\n\nThe reason for $N_\\mathrm{z}=1$ attribute transform is that one step of attribute transform (adding a residual attribute) can already explore the whole space of the continuous design parameters. In our experiments, we found that using more attribute transforms ($N_\\mathrm{z}>1$) yields quite similar performance. Therefore, we use $N_\\mathrm{z}=1$ for efficiency.\n\n---\n\n> **Q4:** *""Table 1 in Appendix E shows values -2.3 and 0.0 are selected for diagonal values of covariance matrices...""*\n\nThese are indeed typos and thank you for spotting them. -2.3 and 0.0 are actually the log standard deviation, so the diagonal values of the covariance matrices should be 0.01 and 1.0 respectively.'}, {'title': 'Response to Reviewer mvHK [2/2]', 'comment': '> **Q4:** *""Enriching ablation studies by training with skeleton transformation disabled and attribute transformation disabled respectively.""\n> ""...probably we already have a decent hand-designed agent at the beginning. If your approach can also effectively improve upon that...""\n> ""...show your method even beat the previous works on continuous design optimization...""*\n\nIn Appendix A of the revised version, we have also evaluated our approach\'s ability to finetune a given expert design and compared it against a popular continuous design optimization baseline [4]. We perform experiments with the skeleton transform stage disabled in our approach and use the expert design as the initial design. As shown in Appendix A, our approach outperforms the expert design and baseline significantly for all four environments. This demonstrates that, in addition to combinatorial design optimization, our approach also enjoys superior performance in continuous design optimization.\n\n---\n\n> **Q5:** *""I can\'t entirely agree with the argument that this approach enables first-order optimization of agent design. Technically, both this approach and ES-based methods do not have access to the ground-truth gradient and estimate first-order gradients based on the gathered experiences. So, this approach is still zeroth-order optimization, and it\'s not appropriate to claim that the sample efficiency comes from the first-order nature of the method.""*\n\nWe acknowledge that this statement is a bit confusing, and we have modified it in the revised version. What we intended to say by “first-order” is that our method uses a parametrized transform policy to output design-changing actions, which enables us to use policy gradients methods for policy optimization, and policy gradients use the first-order information in the policy by taking the exact gradient of the policy w.r.t. its parameters.\n\n\n---\n\n> **Q6:** *""While I agree that ES-based methods have a high-dimensional search space for design, your approach does not essentially reduce that search space...""*\n\nWe intend to say that all methods (NGE and ours) face the same problem of high-dimensional design space, but our method, using a parametrized transform policy to output design changes, is more efficient because the policy allows better generalization and experience sharing among joints (as explained in the answer to Q3). We are not claiming that we reduce the design search space.\n\n---\n\n> **Q7:** *""skeptical about the reported performance of NGE...""*\n\nWe believe that our results, especially with more random seeds, are consistent with the findings in the NGE paper [1]. In particular, we find that NGE performs much better than RGS (1.5\\~2x rewards) for 2D Locomotion, Swimmer, and 3D Locomotion. This agrees with the results in the NGE paper. Similar to prior work [5], we use our own implementation of NGE to have a fair comparison and avoid any performance differences caused by implementation details such as deep learning frameworks, RL algorithm implementation, etc.\n\n---\n\n### **Reference**\n\n[1] Wang et al., ""Neural Graph Evolution: Towards Efficient Automatic Robot Design."" ICLR 2019.  \n[2] Huang et al., ""One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control."" ICML 2020.  \n[3] Kurin et al., ""My Body Is A Cage: The Role of Morphology in Graph-based Incompatible Control."" ICLR 2021.  \n[4] Ha, ""Reinforcement Learning For Improving Agent Design."" Artificial Life, 2019.  \n[5] Hejna et al., ""Task-Agnostic Morphology Evolution."" ICLR 2021.'}, {'title': 'Response to Reviewer mvHK [1/2]', 'comment': 'Thank you for the constructive feedback and detailed review. We also appreciate the acknowledgment of our approach\'s novelty. We have revised the paper to include experiments with more seeds and baseline comparison of continuous design optimization for finetuning expert designs (please see the revision summary post for all changes). In the following, we aim to address your questions and concerns.\n\n---\n\n> **Q1:** *""Experiments are only conducted on four custom environments...""*\n\nWe’d like to mention that four environments are actually not few considering that NGE [1] has only used two environments. Our 2D Locomotion and Swimmer environments are also quite similar to the Walker and Fish environment in NGE despite using different morphology parametrization (we use capsules only). Furthermore, our 3D Locomotion and Gap Crosser environments cover other uses cases including 3D multi-legged robots and challenging terrains.\n\n---\n\n> **Q2:** *""three random seeds are far below the standard""*\n \nWe have rerun all the experiments with more seeds (6 seeds in total now) in the revised version. We hope the reviewer can understand that these new experiments cost a significant amount of compute, i.e., 100+ machine days in total, which limits the number of experiments we can run. We note that 6 seeds are also already more than many prior works in this area (e.g., 4 seeds in [2] and 3 seeds in [3]).\n\n---\n\n> **Q3:** *""Discussing comparison with NGE (probably the strongest baseline) about similarities and differences and how those differences lead to a huge performance increase""\n> ""Could you provide more intuition on the comparison against NGE?""*\n\nThere are three main advantages of our method over NGE:\n1. **NGE does not allow experience sharing among species in a generation.** As shown in Algorithm 1 of NGE [1], each species inside a generation $j$ has its own set of weights $\\theta_i^j$ and is trained independently without sharing experiences among different species. The experience sharing in NGE is only enabled through weighting sharing between a species and its parent species from the previous generation. This means that if there are $N$ species in a generation, in every epoch, each species is only trained with $M/N$ number of experience samples where $M$ is the sample budget for each epoch. At the end of the training, each species has only used $EM/N$ samples for training where $E$ is the number of epochs. In contrast, in our method, every design shares the same control policy, so the policy can use all $EM$ samples. Therefore, our method allows better experience sharing across different designs, which improves sample efficiency.\n2. **Our method uses a transform policy to change designs instead of random mutation.** Our transform policy takes the current design as input to output the transform actions (design changes). Through training, the policy learns to store information about which design to prefer and which design to avoid. This information is also shared among different joints via the use of GNNs, where joints in similar states choose similar transform actions (which is also balanced by JSMLPs for joint specialization). Additionally, the policy also allows every joint to simultaneously change its design (e.g., add a child joint, change joint attributes). For example, in 3D Locomotion, the agent can simultaneously grow its four feet in a single timestep, while ES-based methods such as NGE will take four different mutations to obtain four feet. Therefore, our method with the transform policy allows better generalization and experience sharing among joints, compared to ES-based methods that perform random mutation.\n5. **Our method allows more exploration.** Our transform-and-control policy tries out a new design every episode, which means our policy can try $M/H_\\text{avg}$ designs every epoch, where $M$ is the total number of sample timesteps and $H_\\text{avg}$ is the average episode length. There is also more exploration for our approach at the start of the training when $H_\\text{avg}$ is small. On the other hand, ES-based methods such as NGE only try $N$ (num of species) different designs every epoch. If NGE uses too many species (large $N$), each species will have few samples to train as mentioned in point 1. Therefore, $N$ is typically set to be $\\ll M/H_\\text{avg}$. For example, $N$ is set from 16 to 100 in NGE, while $M/H_\\text{avg}$ in our method can be more than 2000.\n\n\n\n\n'}, {'title': 'Response to Reviewer 487L', 'comment': 'Thank you for the constructive feedback and appreciation of our approach\'s novelty. We have revised the paper to include experiments with more seeds and baseline comparison of continuous design optimization (please see the revision summary post for all changes). Here, we aim to address the questions raised in the review.\n\n---\n\n> **Q1:** *""Not sure how much control a user has over the design space. For example one may want the design to mimic certain animals, or be symmetrical.""*\n\nThere are two ways one can control the design space in our method:\n1. **Use design-related rewards in the transform stage.** For example, we can add rewards to encourage symmetry or certain topology in the design.\n2. **Use certain design and action parametrization.** For example, to enforce symmetry, we can parametrize the design and transform actions with only one side of the agent\'s body, and if we change the design with transform actions, the other side of the agent\'s body is also symmetrically updated.\n\n---\n\n> **Q2:** *""The training for the first two stages would involve a delayed sparse reward signal. Is there any intuition why PPO handles this okay in the presented case? Does the horizon of the first two stages, where no reward is given, impact the learning effectiveness?""*\n\nThe main reason is that the first two transform stages are relatively short in our cases, i.e., only $N_\\mathrm{s} + N_\\mathrm{z} = 6$ timesteps, compared to the much longer horizon of the execution stage (> 100 timesteps). Therefore, the transform sub-policy can quickly receive reward signals after taking the transform actions.\n\n---\n\n> **Q3:** *""It’d be interesting to analyze individual components of the proposed algorithm. E.g. comparing to [1] or [2] assuming a fixed morphology. This will help compare the condition policy approach to a bi-level optimization approach.""*\n\nIn Appendix A of the revised version, we have also evaluated our approach\'s ability to finetune a given expert design and compared it against a popular continuous design optimization baseline [1]. We perform experiments with the skeleton transform stage disabled in our approach and use the expert design as the initial design. As shown in Appendix A, our approach outperforms the expert design and baseline significantly for all four environments. This demonstrates that, in addition to combinatorial design optimization, our approach also enjoys superior performance in continuous design optimization.\n\n---\n\n### **Reference**\n\n[1] Ha, ""Reinforcement Learning For Improving Agent Design."" Artificial Life, 2019.'}, {'title': 'Revision Summary', 'comment': 'We thank the reviewers for their detailed and useful feedback. We are very grateful that all reviewers appreciate the novelty of our approach. We have revised the paper based on the feedback and comments. Here, we list the main changes in the revised paper (highlighted in blue):\n\n1.  **More random seeds for all experiments.** We have increased the number of seeds to 6 for all the experiments in the revised paper.\n2.  **Baseline comparison of continuous design optimization for finetuning expert designs**. We have added these experiments in *Appendix A* of the revised version. The results show that our approach outperforms the expert design and baseline significantly for all the environments.\n3.  **Detailed discussions of our approach\'s advantages over ES-based methods such as NGE [1].** We have added these discussions in the introduction and *Appendix B* of the revised version as requested by Reviewer mvHK.\n4.  **Discussion of attention-based policy [2]**. We have added a short discussion of [2] in the related work to address the comments of Reviewer 42PW.\n\nIf the reviewers still have remaining questions after reading the revision and responses, we are happy to provide additional responses.\n\n---\n\n### **Reference**\n\n[1] Wang et al., ""Neural Graph Evolution: Towards Efficient Automatic Robot Design."" ICLR 2019.  \n[2] Kurin et al., ""My Body Is A Cage: The Role of Morphology in Graph-based Incompatible Control."" ICLR 2021.  '}, {'summary_of_the_paper': 'The paper introduced a reinforcement learning algorithm that simultaneously optimizes the design as well as the controller of a simulated robot to perform locomotion tasks. The core idea is to train a conditioned policy that performs the task in three stages: 1) morphology design of the robot, 2) design parameter adjustment, and 3) controlling of the robot to perform the task. By integrating the design process into the policy learning framework, they are able to design novel and effective agents to complete a variety of tasks. To support the proposed algorithm, graph neural networks is heavily used to support different morphologies. They further propose a joint-specific architecture to improve flexibility of the network, which improves the performance of the algorithm.', 'main_review': 'Strengths:\n- The resulting morphologies of the algorithm seem interesting and effective.\n- The idea of breaking the training into multiple stages where policy first designs the agent and then controls it is novel.\n- Paper is in general well written.\n\nWeaknesses/Questions:\n- Not sure how much control a user has over the design space. For example one may want the design to mimic certain animals, or be symmetrical.\n- The training for the first two stages would involve a delayed sparse reward signal. Is there any intuition why PPO handles this okay in the presented case? Does the horizon of the first two stages, where no reward is given, impact the learning effectiveness?\n- It’d be interested to analyze individual components of the proposed algorithm. E.g. comparing to [1] or [2] assuming a fixed morphology. This will help compare the condition policy approach to a bi-level optimization approach.\n\n[1]. Reinforcement Learning for Improving Agent Design. Ha.\n[2]. Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning. Schaff et al.\n', 'summary_of_the_review': 'The paper presents a concrete algorithm with good results in general. It would be helpful to provide some more details and insights in what makes the method work and what are some limitations of the current work.\n\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""This paper proposes a transform-and-control policy to optimize the robotic agents' designs. Contributions include:\n- A novel perspective on agent design: rather than formulating agent design as a bi-level optimization, this paper embeds both design generation and control into a single decision-making process such that both design and control are optimized by the same RL algorithm.\n- In this formulation, the training experience from different designs is shared to improve sample efficiency.\n- Joint-specialized MLP on top of the GNN policy that further finetunes the control of individual joints."", 'main_review': 'Strengths:\n- Formulating design and control co-optimization as one sequential decision-making problem is novel.\n- The paper\'s ideas all make sense (transform-and-control policy, skeleton and attribute transform, JSMLP).\n- The empirical results look stronger than existing baselines.\n- The paper is written to be easy to understand.\n\nWeaknesses:\n- Experiments are only conducted on four custom environments. Why not use existing environments from NGE or [1] (see references below)? Also, three random seeds are far below the standard. \n- Little analysis on the empirical results, given no theoretical justification of the algorithm. The analysis can be further enhanced from several aspects: 1) Discussing comparison with NGE (probably the strongest baseline) about similarities and differences and how those differences lead to a huge performance increase; 2) Enriching ablation studies by training with skeleton transformation disabled and attribute transformation disabled respectively.\n\nConcerns:\n- I can\'t entirely agree with the argument that this approach enables first-order optimization of agent design. Technically, both this approach and ES-based methods do not have access to the ground-truth gradient and estimate first-order gradients based on the gathered experiences. So, this approach is still zeroth-order optimization, and it\'s not appropriate to claim that the sample efficiency comes from the first-order nature of the method.\n- While I agree that ES-based methods have a high-dimensional search space for design, your approach does not essentially reduce that search space. Instead, the search space for policy is much larger in your formulation, i.e., the dimension of MDP becomes much higher.\n- Could you provide more intuition on the comparison against NGE? By looking at the performance curves, it outperforms NGE by a large margin even without all JSMLPs. Is it because of the new formulation of the design optimization, or the attribute transform is optimized (unlike sampling from uniform distributions in NGE), or other reasons?\n- I\'m skeptical about the reported performance of NGE because of several reasons: 1) In the original NGE\'s paper, it outperforms RGS by a large margin, while in Figure 3 of this paper, the improvement is marginal; 2) I believe NGE should be much better than RGS is because NGE also uses GNN policies that allow experience sharing across different designs; 3) If experience sharing via GNN is not effective in NGE, why this is effective in your approach as you claimed? A good way to address my concerns is to probably run NGE\'s original implementation besides your own implementation and report the performance.\n\nOther suggestions:\n- Section 4 can be shortened to include more analysis in Section 5.\n- In many practical use cases, probably we already have a decent hand-designed agent at the beginning. If your approach can also effectively improve upon that and is better than other baseline methods, the results will be more solid.\n- The result will look even stronger if you can show your method even beat the previous works on continuous design optimization [2,3] (probably with attribute transform only and no skeleton transform).\n\nTypos:\n- JSMPLs -> JSMLPs in the caption of Figure 4.\n\nReferences:\n- [1] Gupta, Agrim, et al. ""Embodied Intelligence via Learning and Evolution."" arXiv preprint arXiv:2102.02202 (2021).\n- [2] Luck, Kevin Sebastian, Heni Ben Amor, and Roberto Calandra. ""Data-efficient co-adaptation of morphology and behaviour with deep reinforcement learning."" Conference on Robot Learning. PMLR, 2020.\n- [3] Schaff, Charles, et al. ""Jointly learning to construct and control agents using deep reinforcement learning."" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.\n\n--------- Post Rebuttal Update ----------\n\nThe authors adequately addressed my main concerns through extra experiments and analysis. \n', 'summary_of_the_review': 'This paper provides an interesting perspective on efficient agent design with reasonable technical approaches, and the results look empirically good. I recommend acceptance.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The paper proposes an algorithm for simultaneous agent design and policy optimization. The choice of the body structure is treated as another action available to the agent. Therefore, the policy is parameterized by graph neural networks (GNNs), and it outputs i) the skeleton structure, ii) node attributes such as bone length, size, motor strength, and iii) motor control commands. Thanks to the parameterization via GNNs, the policy can be trained with PPO. Experiments show that the proposed method outperforms prior approaches, which mainly employ evolutionary methods for optimization, whereas the proposed method leverages more sample efficient policy gradient algorithms.', 'main_review': 'Strengths: clear idea, described well, convincing experiments\n\nWeaknesses:\n1) Formulations of some claims could be made more neutral. E.g., ""improves sample efficiency tremendously"" -> substantially, considerably, significantly\n2) The paper refers to policy gradients (PG) as ""first-order"" optimization methods. This may not be exactly technically correct as these methods still only use zero order information about the objective function. The authors are encouraged to consult the recent literature on the analysis of PG methods, e.g., [1].\n3) Table 1 in Appendix E shows that N_s=5 skeleton transforms and N_z=1 attribute transform were selected using hyperparameter search. What qualitative results does one obtain if there are more than 1 attribute transform? Why allowing more skeleton transforms performs worse? In principle, it should provide more flexibility. Adding a discussion explaining the effects of these numbers to the main body of the paper would be illuminating.\n4) Table 1 in Appendix E shows values -2.3 and 0.0 are selected for diagonal values of covariance matrices Sigma^z and Sigma^e. This seems to be a mistake, because diagonal values should be greater than zero.\n\n[1] Agarwal, A., Henaff, M., Kakade, S., & Sun, W. (2020). Pc-pg: Policy cover directed exploration for provable policy gradient learning. arXiv preprint arXiv:2007.08459.\n\n', 'summary_of_the_review': 'Strong paper. The proposed approach is novel and can be impactful in other discrete-continuous optimization domains. Experiments validate the advantages of the proposed method. The paper is clear and easy to follow.\n\n====\n\nI am satisfied with the response of the authors and I maintain my score ""8: accept, good paper"".', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""The paper poses the problem of morphology design for robots as RL training of one joint GNN policy. Their policy first generates the robot's morphology and then evaluates the design with a common behavior policy that conditions on it. The authors also introduce a technique called JSMPL to allow asymmetric morphologies. Experiments in the Mujoco simulator demonstrate a large improvement over evolutionary methods in terms of sample efficiency and final performance (although the latter is less clear, as neither method has clearly converged). Ablation studies show that the GNN architecture is essential, but are less clear about the impact of JSMPL.\n"", 'main_review': '**POSITIVES**\n\nTo the reviewers knowledge (which is admittedly limited), this is the first attempt to learn the morphology of a robot this straight forward with RL. The method is easy to understand and improvements over evolutionary baselines appear very significant (even for 3 seeds).\n\n**NEGATIVES**\n\nThe reviewer has 4 major complaints:\n\n(1) The reward of the tested environments is not scale invariant. When the agent is allowed to construct a robot with twice the dimensions and torque, the robot will move (approximately) twice as fast and therefore get twice the reward. The reviewer assumes that there is a build in penalty for using large torques, but this is a delicate balancing act that the authors need to at least discuss. An additional figure that shows how the robots\' size changes during training would reveal a systematic growth of the robot and put this reviewers mind at ease if that is not the case.\n\n(2) The results are based on 3 random seeds. While this is (sadly) all to common in RL, it severely limits the conclusions one can draw with any certainty from the results. The performance gap to the baselines appears large enough to be significant, and the same holds for the ""ours w/o GNNs"" ablation, but the ablations without JSMLP have barely non-overlapping standard deviations, which is an insufficient measure of statistical significance for 3 seeds anyway. To make the presented claims about the ablations, the authors need at least (!) twice the number of seeds (if not more).\n\n(3) The JSMLP method is complicated and poorly explained. It is also not clear to the reviewer why asymmetry is inherently favorable for robotics design. Even if one would like to include the possibility, the presented JSMLP method seems very ad-hoc and might have some unintended side effects. For example, the decision to remove one node can change the node index (e.g. from 31 to 21 in Figure 6) without changing anything about the node. The authors are encouraged to look into Relational GCN as an alternative to represent asymmetric morphologies.\n \n(4) It is unclear whether GNN actually generalize well (in particular out-of-distribution) over different robot morphologies. Kurin et al. (2021) have shown some very convincing counter-examples and suggest to use attention instead of GNN layers. The authors must discuss this alternative and are encouraged to evaluate their experiments with attention layers to see whether this yields any improvement. \n\n**DETAILED COMMENTS**\n\n- $r_t$ must be defined\n- clarify that in the definition of J, H is a variable (due to episodic environment)\n- when you introduce the design D, doesn\'t it also affect the state and action space?\n- you do not explicitly state in the main paper which variant of GNN you use, e.g. which aggregation function\n- (eq.7) looks as if $a^e$ and $a^d$ can be executed simultaneously instead of alternatively. Better select one $a$ (over the union of the action spaces).\n- it is not clear why you allow the removal of joints, as it could lead to cycles\n- it would be good to have a ""stop changing the morphology"" action instead of always running for $N_s$ and $N_z$ actions\n- the ""Policy Update"" line in Algorithm 1 must be in the first while loop, as PPO first collects the data set M and then uses it to update the model (currently it is called after every episode)\n- you should mention before page 7 that your graph has a root node\n- it would be nice to run in Figure 3 PPO on some standard morphology in each environment, for example HalfCheetah, Ant, Swimmer and Hopper, to see that your algorithm is a ""super-human"" designer. In particular Ant would make for an interesting comparison\n\n**REFERENCES**\n\nKurin, V.; Igl, M.; Rocktaeschel, T.; Boehmer, W., and Whiteson, S. My body is a cage: the role of morphology in graph-based incompatible control. In International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=N3zUDGN5lO.', 'summary_of_the_review': 'The paper introduces the (to the reviewers knowledge) novel concept of learning the morphology with RL instead of evolution. The experiments need more seeds, but the advantage of the presented method appears significant. The method to allow asymmetric morphologies appears less significant and one might find a more elegant way to ensure this property (if it is at all needed). \n\nThe reviewer would be willing to increase the score if the authors could substantiate point (1) and promise to discuss (or convincingly argue against) the other points of criticism.\n\n**POST-REBUTTAL**\n\nWhile the response does not answer 100% of this reviewers questions, the resulting paper is nonetheless interesting enough, and the results clear enough, to merit publication.  The reviewer is therefore raising the score to 8.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design', 'authorids': ['~Ye_Yuan5', '~Yuda_Song2', '~Zhengyi_Luo1', '~Wen_Sun1', '~Kris_M._Kitani1'], 'authors': ['Ye Yuan', 'Yuda Song', 'Zhengyi Luo', 'Wen Sun', 'Kris M. Kitani'], 'keywords': ['Agent Design', 'Morphology Optimization', 'Reinforcement Learning'], 'abstract': ""An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially.  Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'yuan|transform2act_learning_a_transformandcontrol_policy_for_efficient_agent_design', 'pdf': '/pdf/511a5c95afacad18125605721a8d1e530c07018b.pdf', 'one-sentence_summary': 'We learn a transform-and-control policy to both design and control an agent.', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/transform2act-learning-a-transform-and/code)', '_bibtex': '@inproceedings{\nyuan2022transformact,\ntitle={Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design},\nauthor={Ye Yuan and Yuda Song and Zhengyi Luo and Wen Sun and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=UcDUxjPYWSr}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shuxiao Chen', 'Koby Crammer', 'Hangfeng He', 'Dan Roth', 'Weijie J Su']",ICLR,Weighted Training for Cross-Task Learning,https://iclr.cc/virtual/2022/oral/7204,2022," In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=ltM1RMZntpu,https://openreview.net/forum?id=ltM1RMZntpu,ltM1RMZntpu,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper proposes an approach to learn the task-specific weights in pretraining or mutli-task learning. It provides theoretical guarantees to the algorithm, as well as strong empirical results on several NLP problems. All the reviewers agreed that the work is interesting and the paper is well written. During the discussion period, the authors committed to address in the revised version (relatively minor) concerns raised by reviewers, including providing additional clarifications and additional comparisons to related methods. Overall, this is a strong paper that merits an acceptance.'}, {'title': 'Thanks for your response.', 'comment': 'Thanks for your response.'}, {'title': 'Response to Reviewer HcJj', 'comment': 'Thank you for your valuable feedback!\n\n**Reply to “Dynamic weights analysis”:** Thanks for your suggestion! We will highlight it in the main text in the revised version. \n\n**Reply to “Normalized joint training”:** Thanks for your suggestion! We will make it more clear in the revised version.  We originally chose to compare with joint training mainly because mainstream models use the joint training instead of normalized joining training, such as BERT [1] and MT-DNN [2].\n\n**Reply to “minor typos”:** Thanks, we will fix them.\n\n[1] Jacob Delvin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding."" In NAACL-HLT, 2019.\\\n[2] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. ""Multi-Task Deep Neural Networks for Natural Language Understanding."" In ACL, 2019.\n\n'}, {'title': 'Response to Reviewer jFGp ', 'comment': 'Thank you for your insightful comments!\n\n**Reply to “Related Work”:** Thanks for your suggestion! We will add it in the revised version. \n\n**Reply to “Comparison with other methods”:** Thanks for your suggestion! We will add the comparison in the revised version. We originally chose to compare with vanilla pre-training and joint training for the following two reasons: 1) Our goal is to design a practical algorithm with **theoretical guarantees** instead of simply state-of-the-art (SOTA). In this sense, it’s unfair to compare TAWT with other weighted training methods **without theoretical guarantees**. 2) Current mainstream models in this area still use the vanilla pre-training or joint training, such as BERT [1] and MT-DNN [2]. \n\n[1] Jacob Delvin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. ""BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding."" In NAACL-HLT, 2019.\\\n[2] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. ""Multi-Task Deep Neural Networks for Natural Language Understanding."" In ACL, 2019.\n'}, {'title': 'Response to Reviewer xsx7', 'comment': 'Thank you for your kind review!\n\n**Reply to “representation-based task distance”:** We would like to point out that the notion of representation-based task distance (Definition 3.1) *appears explicitly* in Equation (3.8). The upper bound in Equation (3.8) is a superposition of three terms, and the last term is precisely the task distance. This task distance is **a function of the weight vector**. Thus, with proper weighting, this term becomes negligible compared to the other two terms, demonstrating the utility of weighted training (See Appendix A.2 for an illustration).\n\n**Reply to “weight vector”:** The illustration of weight vectors can be found in **Table 7** in Appendix B. More discussion on weights can be found in **dynamic weights analysis** in Appendix B. We will make it more clear in the revised version.\n\n**Reply to “additional experiments”:** Thanks for your suggestion on sentiment analysis and masked language model.  We also plan to further evaluate TAWT in more settings. \n'}, {'summary_of_the_paper': 'The paper proposes a new weighted training algorithm, TAWT, to learn the task-aware weights on tasks for better using the cross-task signals.  The author theoretically and empirically verified the effectiveness of TAWT. \nThe paper can bring a new research interest for multi-task learning and transfer learning.', 'main_review': ""The paper proposes a new weighted training algorithm, TAWT, to learn the task-aware weights on tasks for better using the cross-task signals.  The author theoretically and empirically verified the effectiveness of TAWT. \nStrength:\n1. The paper was clearly written and give a good formal definition for the problem of weighted training.\n2. The paper can bring a new research interest for multi-task learning and transfer learning. \n3. The experiment designs are very good,which show the affects on different training data size, impact of the ratio between source tasks training samples and target task training samples, etc.\n\nWeakness:\n1. The paper introduces a representation-based task distance, but this distance is neglected in the analysis of Eq.(3.8). I think it could not be negligible. Otherwise, the upper bound become task-agnostic. \n2. The used four NLP tasks are closely related.  It's better to add an irrelevant task, such as sentiment analysis, to show the effect of weighted training.\n3. An illustration of weight vector should be provided.\n\nOthers:\n1. It's interesting to extend this idea for incorporating some unsupervised tasks, such as Masked language model.\n"", 'summary_of_the_review': 'The paper proposes a new weighted training algorithm, TAWT, to learn the task-aware weights on tasks for better using the cross-task signals.  The proposed method is quite effective for transfer learning and multi-task learning on small target data. \nThe weighted training is very important but there lacks good work in this direction. Therefore, this paper is great to give an attempt for task-aware weighted training.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper discusses an approach that learns to weight data from different tasks in pretraining or mutli-task learning. It also gives a VC/empirical-processes style analysis, which gives guarantees for the algorithm and insights about sample complexity. Finally it describes experiments on a number of NLP problems.\n', 'main_review': 'I enjoyed this paper a great deal. The presentation is very clean and the underlying algorithm and theory is interesting. The results look quite promising for the method.\n\nA few comments:\n\n[1] Are there connections to work that learns data weighting using multi-arm bandits? Specifically:\n\nAutomated Curriculum Learning for Neural Networks\nAlex Graves, Marc G. Bellemare, Jacob Menick, Remi Munos, Koray Kavukcuoglu\n\nIt would be interesting to discuss connections to this work - the ideas are different but there does seem to be some connection.\n\n[2] The choice of mirror descent is not critical, correct? For example projected gradient methods could be used instead? It would be useful to note this.\n\n[3] Given Assumption B, it seems clear that \\phi must be a vector, correct? This is a minor thing, but I think that was not specified earlier in the paper?\n\n[4] Definition 3.2 (transferability) could benefit from much more explanation. I’m also not sure if it’s quite correct as written. Should there be a \\forall quantifier, for example \\forall \\phi \\in \\Phi before Eq. 3.5? Also I’m not sure how this equation implies that the loss is “controlled by a polynomial…”. Finally, is there a reason that the expression (\\sum …) is positive? And if it can become negative, how can we take a power to 1/p? Finally, I have very little intuition about what values p will take in practice, or what p intuitively corresponds to.\n', 'summary_of_the_review': 'An interesting algorithm, theory, and results. A few parts of the paper need clarification.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper introduces a novel cross-task training method that weights different tasks by optimizing a representation-based task distance between the source and target tasks. They provide both theoretical guarantee and empirical evidence to corroborate the proposed method.', 'main_review': 'Strengths\n- The proposed method is novel and well-motived.\n- The authors provide a theoretical guarantee of the proposed method.\n- Empirical results indicate that the proposed model greatly outperforms the vanilla methods in both pre-training and joint learning. The authors also open-source their implementation.\n\n\nWeaknesses\n- Add a section that introduces ""Related Works"" would be very helpful for readers to understand the background and the novelty of this work. For example, what\'re the most popular/advanced methods in cross-task learning? How is this method related to and different from them? What\'s the relationship between cross-task learning, joint learning, and multi-task learning?\n- The empirical evidence would be much more convincing if authors compare their model with other methods besides the vanilla pre-training and joint training. Many existing works that utilize different strategies to calculate the weight of different task has been introduced in the area of multi-task learning. Although this work is designed for ""cross-task learning"" instead of ""multi-task learning"", the existing methods can be straightforwardly used to address the same problem. So it would be great if the authors can either compare the proposed methods with existing works or explain why other existing works cannot be directly adapted to this problem. There are many great survey papers that summarize the related works (e.g., [A Comparison of Loss Weighting Strategies for Multi task Learning in Deep Neural Networks](https://ieeexplore.ieee.org/document/8848395) and [A Survey on Multi-Task Learning](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392366&tag=1). )', 'summary_of_the_review': 'This is a good paper with novel ideas, but it can be further improved by adding more backgrounds, related works, and comparisons with other state-of-the-art methods.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper introduces Target-Aware Weighted Training (TAWT), a cross-task learning algorithm. In the two-step procedure popular for cross-task training, the weight vector $\\omega$ defining the relative weight of each source task is usually exogenous (i.e: a hyperparameter). In contrast, in TAWT it is part of the optimization procedure, with the weights depending on the proximity (throughout training) of each source task to the target task (hence the “target aware” name).\n\nThe authors first derive an algorithm that enables learning when $ \\omega $ is made endogenous in the case of (weighted) pre-training. They then show how it can naturally be extended for weighted joint-training by considering the source tasks as one of the source tasks we learn representations from. They then provide theoretical performance guarantees for TAWT.\n\nThe authors apply TAWT on 4 NLP tasks, using BERT as their base model. They evaluate their method on both full-data and limited-data settings on each target task, using the 3 other tasks as source tasks. They show that TAWT-pretraining and TAWT-joint training significantly outperform their unweighted counterparts across tasks. These improvements are more marked when target task data is scarce.\n\nThey also show that weight initialization can be particularly important when the different source datasets have different # examples (e.g: when training jointly on data abundant source tasks and the data-scarce target task). This specific finding is not very novel, but it is great to see TAWT-training also helping when initialization weights are chosen correctly. \n\nCritically, the authors also show the importance of varying task weights throughout training. Indeed, fixing the task weights to be the final weights from TAWT leads to worse results than training with TAWT. This suggests that it is important that task-weights vary throughout the training process.\n', 'main_review': 'Review note: As noted in the initial review assessment, I am less comfortable with the mathematics underlying section 3 and Appendix A. I will focus more on the empirical results and I hope other reviewers will help review those. This explains the lower confidence score. \n\nPros:\n\n- The method is well-motivated. It is clear what the method is aiming for and how it differs from existing approaches. The added theoretical guarantees are welcome.\n- The experimental setup is convincing and the results confirm the soundness of the approach. The settings are well described and I appreciated Section B.1. describing some of the rationale behind these choices.\n- The writing is very good.\n- The appendix is very rich in additional interesting experiments.\n\nCons:\n\n- Dynamic weights analysis not present in main text:\n\nTAWT is more complex than prior methods. Seeing the importance of dynamic weights throughout training (Table 8) is very relevant as it shows that even a strong choice of initial weights for $\\omega$ (e.g through hyperparam opt) may not be as good as TAWT.\nCurrently, this feels like it is missing from the main body and deserves to be emphasized earlier. \n\n- Weight initialization for joint training\nMore of a suggestion than a con: For joint training, I believe it would be better to use normalized joint training as the baseline. Indeed, initialization with task weights that depend on the sample size is a stronger baseline and more closely aligned with approaches used in practice. \nAn added benefit is that table 3 could then be removed (since it’d be the later rows of Table 1 and Table 2). The non normalized results could still be kept in appendix for completeness. This would also allow Table 8 and its discussion to be moved into the main body.\n\n- Minor typos: The writing is good overall, but there are still some minor typos:\ne.g: p20 Pre-training first pre-train**s** and then fine-tune**s**\n', 'summary_of_the_review': 'A rare mix of well-motivated method, theoretical guarantees, solid experimental work and convincing results. There are minor presentation improvements to be done but I think this is a strong paper otherwise. As noted earlier, I am not able to review carefully Sec 3/Appendix A so this the caveat to my review.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Weighted Training for Cross-Task Learning', 'authorids': ['~Shuxiao_Chen1', '~Koby_Crammer1', '~Hangfeng_He3', '~Dan_Roth3', '~Weijie_J_Su1'], 'authors': ['Shuxiao Chen', 'Koby Crammer', 'Hangfeng He', 'Dan Roth', 'Weijie J Su'], 'keywords': ['Cross-task learning', 'Natural language processing', 'Representation learning'], 'abstract': 'In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning.', 'one-sentence_summary': 'We introduce a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'chen|weighted_training_for_crosstask_learning', 'pdf': '/pdf/579ed2f74ecc130396039eae33e13de66b8de08b.pdf', 'supplementary_material': '/attachment/4554ccf3b03ed539601ba4727c22f5b6646fff7a.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/weighted-training-for-cross-task-learning/code)', '_bibtex': '@inproceedings{\nchen2022weighted,\ntitle={Weighted Training for Cross-Task Learning},\nauthor={Shuxiao Chen and Koby Crammer and Hangfeng He and Dan Roth and Weijie J Su},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=ltM1RMZntpu}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Xuechen Li', 'Florian Tramer', 'Percy Liang', 'Tatsunori Hashimoto']",ICLR,Large Language Models Can Be Strong Differentially Private Learners,https://iclr.cc/virtual/2022/oral/6895,2022," Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=bVuP3ltATMz,https://openreview.net/forum?id=bVuP3ltATMz,bVuP3ltATMz,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This work adapts the widely used DP learning algorithm to language models. Reviewers all agreed that this work tackles an important problem with clear motivation and thorough experiments, and achieved strong performance (memory reduction and effectiveness) on NLP tasks.  Thus, we recommend an acceptance.'}, {'title': 'Rebuttal revision', 'comment': ""We have updated our draft based on suggestions from reviewers. \n\nTo summarize the most important aspects:\n- We provided more background about DP-Adam in Appendix A and included results comparing DP-SGD against DP-Adam in the new section Appendix S.\n- We included additional results showing that varying $\\delta$ only affects performance marginally in the new section Appendix R. This section also has results of the effect on performance when varying $\\epsilon$.\n- We included non-private numbers of GPT-2 and RoBERTa in Figure 1. \n- We outlined subtleties in DP mixed-precision training in the new section Appendix T. \n- We included experimental results comparing RGP with and without using the text-infilling objective in the new section Appendix U. \n- We included additional discussion on difficulties in fine-tuning when the embedding and language modeling heads aren't updated for dialog generation in the new section Appendix V. \n- We modified the writing of the abstract and introduction to be more precise. We provided more details about the implementation of the memory-saving technique in Section 4. \n\nWe thank the reviewers again for their time and valuable suggestions!\n""}, {'title': 'Author response (part 2)', 'comment': '> Figure 1 is a bit mis-leading. Please show and compare with RoBERTa (non-private) on 1(a) and non-private GPT-2 on 1(b). Those are the baselines to compare. Otherwise you are comparing private RoBERTa with non-private BERT or non-private T-GEN(non Transformer). Statement in the abstract and introduction are over-stated.\n\nWe thank the reviewer for these suggestions. We’ll include the numbers for non-private RoBERTa and GPT-2 in the next revision.\n\nMeasuring the performance drop of the method of full fine-tuning private vs non-private is important. However, this wasn’t meant to be the main message of Figure 1. Our original intent was not to suggest that private learning has no cost on utility. Rather, we intended to convey the idea that **direct DP full fine-tuning using increasingly better pretrained models eventually attains the level of non-private performance that was considered state-of-the-art a few years ago**. This is very encouraging from a practical standpoint, especially considering that enforcing DP in NLP problems was considered quite impractical by many in the field. The appeal also lies in the simplicity of our demonstrated approach -- the most basic method works well, and more progress can be expected in the future as pretraining consistently improves from existing developments. \n\nPerhaps another way to state the above is that the “introducing new methodology” aspect is only one part of our paper among many others. We believe that our paper is a combination of empirical analyses (e.g., Section 3.1), empirical evaluation/benchmarking (e.g., Section 5.2 & Table 2), and new methodology (e.g., Section 4). We also believe that all aspects are indispensable components of the paper, each of which either provides explanations of why things should work or provides guidelines on how to make things run efficiently in practice. \n\nIn general, we also don’t believe that every paper in the machine learning community has been or should be an “introduce-a-new-methodology-type” paper. Recent works on empirical evaluation and analysis, and benchmarking have revealed great insights that pushed the field forward and led to tangible progress [1, 2, 3, 4, 5, 6]. \n\n> The organization of the paper could be improved. Ghost-Cliping is your main method, which could be moved earlier.\n\nWe share the sentiment with the reviewer that there’s room for improvement in terms of organization. Overall, we think that ghost clipping is only one part of our paper. How to get full fine-tuning to work well (Section 3), and does dimensionality degrade performance (Section 5) are important aspects that we deem necessary to gain a better understanding of DP language model fine-tuning. \n\nWe will improve the organization by updating parts of the introduction. \n\n[1] Schmidt, Robin M., Frank Schneider, and Philipp Hennig. ""Descending through a crowded valley-benchmarking deep learning optimizers."" International Conference on Machine Learning. PMLR, 2021.\n\n[2] Choi, D., Shallue, C. J., Nado, Z., Lee, J., Maddison, C. J., & Dahl, G. E. (2019). On empirical comparisons of optimizers for deep learning. arXiv preprint arXiv:1910.05446.\n\n[3] Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, S., Dahl, G., ... & Grosse, R. B. (2019). Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32, 8196-8207.\n\n[4] Shallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., & Dahl, G. E. (2018). Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research 20 (2019) 1-49. \n\n[5] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\n\n[6] \u200b\u200bBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n'}, {'title': 'Author response (part 1)', 'comment': 'We thank the reviewer for their thoughtful comments. Below we address each concern.\n> If RGP is augmented with larger batch/proper learning rate, and masked prediction for fine-tuning, it may be the best in terms of accuracy.\nThe results we include for RGP are from [their released codebase](https://github.com/dayu11/Differentially-Private-Deep-Learning/tree/main/language), and using the tuned hyperparameters from their original work.\n\nWe stress that we **did not** tune hyperparameters for any of the classification tasks, but rather used hyperparameters transferred from the E2E data-to-text generation task. This was to avoid incurring extra privacy leakage due to hyperparameter tuning. This was documented in Appendix B and Appendix L of our initial draft (“In addition, model selection from hyperparameter tuning on private training data could incur extra privacy leakage. We skip the step of private hyperparameter tuning (Liu & Talwar, 2019; Chaudhuri & Vinterbo, 2013) and instead perform tuning only on the E2E task and reuse almost the exact hyperparameters for all remaining tasks.”). \n\nWhile it may seem fair to compare well-tuned full vs well-tuned RGP, we generally prefer to avoid large-scale tuning due to the potential of increased privacy leakage. Note that this matter of privacy cost incurred by tuning with private training (and potentially private validation) data has also been raised by other reviewers (a9ro, uqBn). \n\nIn addition, it’s important to note that the tuning strategies that we outlined in Section 3.1 do not necessarily transfer to the RGP setting. For instance, using small clipping norms (under mixed-precision) typically isn’t better for RGP from our experiments. This is likely an artifact of RGP not directly privatizing gradients -- rather, for a parameter weight matrix, the method runs power iteration to obtain rank-k tiles (k=1 in most cases), for which gradients are computed. \n\nTo test if the mask-infilling objective improves performance for RGP, we ran additional ablation studies comparing RGP with and without the mask-infilling objective. On SST-2 for $\\epsilon=8$ with RoBERTa base, validation accuracy improves from $90.64\\pm 0.26$ to $92.91\\pm 0.40$ over 5 independent runs. \n\nSince the released code for RGP fails when running in full precision, we re-tuned hyperparameters of our re-implementation in full precision with our mask-infilling objective (for training stability purposes). The above results we report were obtained with this tuned hyperparameter set (thus the absolute numbers aren’t strictly comparable to our non-tuned full fine-tuning numbers).\n\nWe thank the reviewer again for raising these points. We will include additional results and discussions based on this thread in the next revision. '}, {'title': 'Thanks', 'comment': 'We thank the reviewer for spending their time reading through our comments. \n\nLink to our code will be posted after the double-blind review period. \n'}, {'title': ""Thanks for authors' response"", 'comment': ""The response from the authors solved most of my concerns. I hope the authors make those points more clear in the next version. I've also read the comments from other reviewers. Thanks for the authors' rebuttal and other reviewers' comments. I prefer to increase my score.\n\nBTW, when will the source code be available for the public?""}, {'title': 'Author response (part 3)', 'comment': ""> 4) However, tuning hyper-parameters requires additional information about the private information (accessing the validation set or testing set), which leads to private leakage.\n\nWe thank the reviewer for pointing this out. We are fully aware of the fact that tuning hyperparameter with a private training set (and possibly private validation set) could lead to additional privacy leakage. That’s why we only tuned hyperparameters on the E2E task, and reused these hyperparameters for all other tasks. While there’s extra privacy leakage on the E2E dataset, there isn’t any for the other tasks (which is the majority of tasks considered in the paper). In general, we find that good hyperparameters tend to transfer well.\n\nIn our initial draft, this was carefully documented in Appendix B (“In addition, model selection from hyperparameter tuning on private training data could incur extra privacy leakage. We skip the step of private hyperparameter tuning (Liu & Talwar, 2019; Chaudhuri & Vinterbo, 2013; Papernot & Steinke, 2021) and instead perform tuning only on the E2E task and reuse almost the exact hyperparameters for all remaining tasks.”) and Appendix L. \n\nWe will put this discussion in the main text for the next revision. \n\n> 5) Is the DP-Adam proposed by yourself? If not, the corresponding reference and explanation are needed. Does it come from [Wang 2019]? If it is proposed by yourself, I suggest the author make it clear as it's one of the contributions.\n\nAs mentioned in our response to 3), DP-Adam is well-known in the literature, and implementations/usages of it occur in most of the popular private machine learning libraries. More clarification is given in our response to 3). We will make these points clearer in the next revision. \n\n[1] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016, October). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (pp. 308-318).\n\n[2] Tramèr, F., & Boneh, D. (2020). Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660.\n\n[3] Yu, D., Zhang, H., Chen, W., & Liu, T. Y. (2021). Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677.\n\n[4] Wang, B., Gu, Q., Boedihardjo, M., Barekat, F., & Osher, S. J. (2019). DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM.\n\n[5] Mironov, I. (2017, August). Rényi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium (CSF) (pp. 263-275). IEEE.\n\n[6] Yu, D., Zhang, H., Chen, W., Yin, J., & Liu, T. Y. (2021). Large Scale Private Learning via Low-rank Reparametrization. arXiv preprint arXiv:2106.09352.\n""}, {'title': 'Author response (part 2)', 'comment': ""> 3) It would be better to conduct an ablation study to verify the promotion of the proposed methods (ghost clipping technique, fine-tuning on only a part of the parameters, and DP-adam). It's better to make it clear how much does each strategy contribute.\n\nNote that ghost clipping is a computational technique that enables clipping per-example gradients with low (GPU) memory cost -- this trick does not alter the privacy-utility tradeoff of the underlying DP optimization algorithm; one gets no more and no less privacy leakage with this technique compared to without it. We believe we made this very clear in our initial draft, but would revise to make it even clearer in the next draft. \n\nAs for different ways of fine-tuning, we note that Table 2 already includes many of the established fine-tuning approaches, some of which optimizes parts of the model. For instance, `top2` optimizes the last 2 Transformer blocks of a large Transformer model, and `LoRA` only optimizes low-rank matrices which are added to the dense matrices of linear layers in a Transformer model. While we could include comparisons against more bizarre fine-tuning approaches (e.g., optimize only the first weight of each weight matrix), we have opted to only test against the most prominent examples in the literature (prefix-tuning, top2, LoRA, and RGP). \n\nNote that DP-Adam is quite well-known in the literature, and implementations/usages of it appear in almost all the popular differentially private machine learning libraries (e.g., DP-Adam in [tf-privacy](https://github.com/tensorflow/privacy/blob/7c4f5bab0964bd32b7ceafa009d9488920856440/tensorflow_privacy/privacy/optimizers/dp_optimizer.py#L385), [CIFAR-10 private training example in Opacus](https://github.com/pytorch/opacus/blob/9cda8072e52049a06afba7ab524276bb6613a727/examples/cifar10.py#L358), [DCGAN private training example in Opacus](https://github.com/pytorch/opacus/blob/9cda8072e52049a06afba7ab524276bb6613a727/examples/dcgan.py#L295)). We don’t claim this to be part of our contribution. Our contribution rather is applying this algorithm to a new setting (language model *full* fine-tuning), and showing that it works well when applied appropriately (our memory trick is one addition that makes the overall procedure computationally efficient). Note, even though we’re applying a known algorithm, we believe that applying it appropriately is highly non-trivial and that our empirical results are significant. \n\nNote that DP-Adam has the same gradient privatization procedure as DP-SGD (subsampled Gaussian mechanism). Any manipulation of the privatized gradient has the same privacy guarantee due to post-processing. Similarly, one may construct privatized versions of any known non-private optimizer, by first performing gradient privatization and then performing the usual update rule. \n\nOriginally, the reason that we experimented with DP-Adam as opposed to DP-SGD was due to the popularity of Adam for non-private language model fine-tuning (one would agree to this claim if one has fine-tuned any language model with an established codebase, e.g., [HuggingFace’s tutorials](https://github.com/pytorch/opacus/blob/9cda8072e52049a06afba7ab524276bb6613a727/examples/dcgan.py#L295), [fairseq’s tutorial](https://github.com/pytorch/fairseq/blob/main/examples/rxf/README.md)). As a side note, we’d also want to emphasize that a past work that fine-tunes language models with DP-Adam falsely claims they fine-tune with DP-SGD [6] ([their code](https://github.com/dayu11/Differentially-Private-Deep-Learning/blob/d52db8acb1c9fa83cd23457bdc7240a3ee5310a0/language/bert/bert_code/run_exp.py#L104) uses Adam as the optimizer).\n\nAs mentioned in the previous comment, we have performed experiments with DP-SGD during the response period and will include these results in the next revision. \n""}, {'title': 'Authors response (part 1)', 'comment': 'We thank the reviewer for thoughtful comments. Below we address each concern separately. \n\n> 1) The \\delta in (\\epsilon, \\detla)-DP is too large for training a DP model with a meaningful level of privacy protection.\n\nWe agree with the reviewer that setting $\\delta$ requires care. We’d like to emphasize that we use a $\\delta$ value of $1 / (2 * |D_\\text{train}|)$ across all experiments (see Section 2 of our initial draft). We recap several points we made while addressing reviewer E6VG’s concerns. \n\nFirst, a value as such is quite common in the existing literature. For instance, the seminal work by Abadi et al. [1] sets $\\delta=1e-5$ for MNIST, where $|D_\\text{train}|=60000$. Here, $\\delta \\approx 1 / (1.67 * |D_\\text{train}|)$. Thus, compared to Abadi et al. [1], our values of $\\delta$ are already much more conservative. Similar values of $\\delta$ can also be found in more recent works [2, 3]. Note that the reference provided by the reviewer [4] also sets $\\delta=1e-5$ for MNIST, which is much less conservative than our values. \n\nSecond, we stress that the privacy guarantee for DP optimizers vary smoothly, i.e., approximate-DP holds for DP-SGD (or any DP optimizer) for a collection of $(\\epsilon(\\delta), \\delta)$ pairs, where $\\epsilon(\\delta)$ is always finite and approaches infinity when $\\delta\\to 0$. This fact follows from the Renyi-DP to approximate-DP conversion [5]. The catastrophic failure mode with the name-and-shame mechanism -- an instance where the privacy loss random variable is consistently unbounded on the failure set of non-zero measure -- does not occur with DP-SGD (or any DP optimizer). \n\nFinally, we stress that the value of $\\delta$ affects the noise multiplier in DP optimizers in only minor ways. We ran additional experiments to test this. For instance, on E2E, with epsilon=3, the noise multiplier goes from 1.225 with $\\delta=1e-7$ to 1.156 with $\\delta=1e-5$. The performance change is also minor. For instance, on E2E, test set BLEU is $58.01\\pm1.88$ for $(\\epsilon, \\delta)=(3, 1e-7)$, and $58.95\\pm1.76$ for $(\\epsilon, \\delta)=(3, 1e-5)$ over 5 independent runs. \n\nWe thank the reviewer for raising this point, and will provide more clarification in the main text as well as include the new results in the Appendix. \n\n> 2) This paper did not compare to other related papers in the experiments. Comparing with the vanilla DP-SGD is a necessary baseline.\n\nThe careful reader should see that for sentence classification tasks, we compared to the only previous paper [6] that attempted this task (results in Table 1). For language generation tasks E2E and DART, since we’re the first to perform experiments on these tasks under differential privacy, we compared to possible ways one could combine existing fine-tuning approaches with DP optimization (results in Table 2). The setup of these experiments are detailed carefully in Section 5 and Appendix K. \n\nDuring the author response period, we ran experiments with DP-SGD. We tuned the learning rate hyperparameter afresh for DP-SGD. On E2E, the best performing setup for $\\epsilon=8$ (learning rate = 3, clipping norm = 0.1) attains a BLEU score of 63.17 (averaged over 3 seeds). **This is on par with the performance that DP-Adam obtains on this task.**\n\nWe will include these results in the Appendix in the next revision. '}, {'title': 'Authors response (part 3)', 'comment': '[1] Yu, D., Zhang, H., Chen, W., Yin, J., & Liu, T. Y. (2021). Large Scale Private Learning via Low-rank Reparametrization. arXiv preprint arXiv:2106.09352.\n\n[2] Kerrigan, G., Slack, D., & Tuyls, J. (2020). Differentially private language models benefit from public pre-training. arXiv preprint arXiv:2009.05886.\n\n[3] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016, October). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (pp. 308-318).\n\n[4] Tramèr, F., & Boneh, D. (2020). Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660.\\\n\n[5] Yu, D., Zhang, H., Chen, W., & Liu, T. Y. (2021). Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677.\n\n[6] Mironov, I. (2017, August). Rényi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium (CSF) (pp. 263-275). IEEE.\n'}, {'title': 'Author response (part 2)', 'comment': ""> \u200b\u200bHowever, the choice of \\delta in experiments may cause some problems in privacy leakage. The value of \\delta on the order of 1/|D_train| is very dangerous according to [2]. In that level of \\delta, they permit “preserving privacy” by publishing the complete records of a small number of database participants. It's better to choose the \\delta as 1/( |D_train| *100), which may reduce the utility but ensure the privacy is well protected.\n\nWhile we agree that how $\\delta$ should be set requires much thought, we’d like to emphasize that we use a $\\delta$ value of $1 / (2 * |D_\\text{train}|)$ across all experiments (see Section 2 of our draft). \n\nFirst, a value as such is quite common in the existing literature. For instance, the seminal work by Abadi et al. [3] sets $\\delta=1e-5$ for MNIST, where $|D_\\text{train}|=60000$. Here, $\\delta \\approx 1 / (1.67 * |D_\\text{train}|)$. Thus, compared to Abadi et al. [3], our values of $\\delta$ are already much more conservative. Similar values of $\\delta$ can also be found in more recent works [4, 5]. \n\nSecond, we stress that the privacy guarantee for DP optimizers vary smoothly, i.e., approximate-DP holds for DP-SGD (or any DP optimizer) for a collection of $(\\epsilon(\\delta), \\delta)$ pairs, where $\\epsilon(\\delta)$ is always finite and approaches infinity when $\\delta\\to 0$. This fact follows from the Renyi-DP to approximate-DP conversion [6]. The catastrophic failure mode with the name-and-shame mechanism -- an instance where the privacy loss random variable is unbounded on the failure set -- does not occur with DP-SGD (or any DP optimizer). \n\nFinally, we stress that the value of $\\delta$ affects the noise multiplier in DP optimizers in only minor ways. We ran additional experiments to test this. For instance, on E2E, with epsilon=3, the noise multiplier goes from 1.225 with $\\delta=1e-7$ to 1.156 with $\\delta=1e-5$. The performance change is also minor. For instance, on E2E, test set BLEU is $58.01\\pm1.88$ for $(\\epsilon, \\delta)=(3, 1e-7)$, and $58.95\\pm1.76$ for $(\\epsilon, \\delta)=(3, 1e-5)$ over 5 independent runs. \n\n> The choice of \\epsilon makes sense because the \\epsilon in a range of 0.1~5 provides meaningful protection.\n\nWe want to stress that there are many real-world deployments of DP with $\\epsilon$ much larger than 5, with the US census being a prime example (see [these notes](https://www.census.gov/newsroom/press-releases/2021/2020-census-key-parameters.html)).\n\n> Will the ghost clipping increases the utility or privacy protection?\n\nAs described in the paper, ghost clipping is a memory-saving trick for clipping gradients -- it does not alter the core algorithmic procedure of DP optimization. Assuming one has unlimited GPU memory, one should get similar results with and without ghost clipping. One should get the same privacy guarantee with and without ghost clipping, once all other hyperparameters are fixed (e.g., sampling rate, noise multiplier, number of updates). \n\nIn practice, ghost clipping enables us to use larger (and higher quality) pretrained models that typically wouldn’t fit in memory otherwise. In this sense, ghost clipping helps us achieve better performance. \n\n> It would be better to conduct some key experiments with a range of \\espilon from 0.1 to 10 (e.g. 0.1, 0.5, 1, 2, 3, 5, 8).\n\nWe thank the reviewer for raising this point. We have run additional experiments on E2E. When $\\epsilon < 2$, we obtain considerably worse results (BLEU score below 30). The test set performance change from $\\epsilon=2$ to $\\epsilon=8$ is more gradual. We will include these additional results in the Appendix. \n\n> This paper did a good job on the empirical study, but the technical novelty is limited.\n\nWhile papers that propose fancy techniques may seem interesting and eyecatching, we believe that the simplicity demonstrated in our work is a boon rather than a bane (not to mention it works well!). We believe that the simplicity of the approach is especially desirable in the realm of private learning since any additional design decision would imply more tuning -- hyperparameter tuning incurs privacy cost unless good hyperparameters can be transferred across tasks and domains. \n\nWe believe we have adequately addressed the concerns raised by the reviewer, so we’d like to politely ask the reviewer to reconsider their assessment of our work.\n""}, {'title': 'Author response (part 1)', 'comment': ""We thank the reviewer for their thoughtful comments. Below we address each concern separately. \n\n> The experimental results in table 1~3 compared to the non-private seems amazing, however, the comparison with the baseline methods is also required, which would show the contribution of your proposed methods. There're several possible baselines: directly apply DP-SGD and an application of DP-SGD on language models (GPT-2) [1].\n\nRecall that we perform full fine-tuning with DP-Adam. Note that DP-Adam is quite well-known in the literature, and implementations/usages of it appear in almost all the popular differentially private machine learning libraries (e.g., DP-Adam in [tf-privacy]( https://github.com/tensorflow/privacy/blob/7c4f5bab0964bd32b7ceafa009d9488920856440/tensorflow_privacy/privacy/optimizers/dp_optimizer.py#L385), [CIFAR-10 private training example in Opacus](https://github.com/pytorch/opacus/blob/9cda8072e52049a06afba7ab524276bb6613a727/examples/cifar10.py#L358), [DCGAN private training example in Opacus](https://github.com/pytorch/opacus/blob/9cda8072e52049a06afba7ab524276bb6613a727/examples/dcgan.py#L295)). We don’t claim this to be part of our contribution. Our contribution rather is applying this algorithm to a new setting (language model *full* fine-tuning), and showing that it works well when applied appropriately (our memory trick is one addition that makes the overall procedure computationally efficient). Note, even though we’re applying a known algorithm, we believe that applying it appropriately is highly non-trivial and that our empirical results are significant. \n\nNote that DP-Adam has the same gradient privatization procedure as DP-SGD (subsampled Gaussian mechanism). Any manipulation of the privatized gradient has the same privacy guarantee due to post-processing. Similarly, one may construct privatized versions of any known non-private optimizer, by first performing gradient privatization and then performing the usual update rule. \n\nOriginally, the reason that we experimented with DP-Adam as opposed to DP-SGD was due to the popularity of Adam for non-private language model fine-tuning (one would agree to this claim if one has fine-tuned any language model with an established codebase, e.g., [HuggingFace’s tutorials](https://huggingface.co/transformers/training.html), [fairseq’s tutorial](https://github.com/pytorch/fairseq/blob/main/examples/rxf/README.md)). As a side note, we’d also want to emphasize that a past work that fine-tunes language models with DP-Adam falsely claims they fine-tune with DP-SGD [1] ([their code](https://github.com/dayu11/Differentially-Private-Deep-Learning/blob/d52db8acb1c9fa83cd23457bdc7240a3ee5310a0/language/bert/bert_code/run_exp.py#L104) uses Adam as the optimizer). \n\nTo directly address the reviewer’s concern, we ran additional experiments with DP-SGD. We tuned the learning rate hyperparameter afresh for DP-SGD. On E2E, the best performing setup for $\\epsilon=8$ (learning rate = 3, clipping norm = 0.1) attains a BLEU score of 63.17 (averaged over 3 seeds). **This is on par with the performance that DP-Adam obtains on this task.**\n\nWe stress that the reference which the reviewer cited [2] does not at all fine-tune GPT-2 with DP-SGD. The careful reader should notice that they only fine-tune GPT-2 **non-privately**. For their private experiments, they fine-tuned a feedforward network that **they pretrained themselves** (see their Section 4.3 for the details). Since they didn’t privately fine-tune a high quality pretrained model like GPT-2, their empirical finding was that even with fine-tuning, DP models are orders of magnitude worse than state-of-the-art non-private models (see Section 5 -- “The perplexity scores for both the small and large feedforward language models are orders of magnitude worse than the GPT-2 models indicating that they are not competitive with state of the art language models.”). This empirical finding is much the opposite of our conclusions. \n\nWe thank the reviewer again for raising these subtle points, and we will make all of these points clear in the next revision. ""}, {'title': 'Response', 'comment': ""We thank the reviewer for their thoughtful comments and their positive sentiment of our work. Below, we address the concern of the reviewer. \n\n> My only complaint is that I wish to see more explanations and more principled approach to selecting the hyper-parameters in fine-tuning with DP-SGD, but I guess it is out of this paper's scope (the authors did provide some empirical discussions in the paper and the appendix, which I appreciate).\n\nWe share the view with the reviewer that perhaps more can be done with regards to hyperparameter selection. While we believe that hyperparameter tuning and good hyperparameters are ultimately task-dependent, experiments in our paper have demonstrated very encouraging results with hyperparameter transfer. \n\nNotably, we only tuned hyperparameters on the E2E task in the paper and reused almost the exact hyperparameter set for other tasks (e.g., sentence classification, dialog generation, and DART).  The details of how we transferred the hyperparameters are documented in Appendix B and Appendix L. \n\nWe believe that hyperparameter transfer (with tuning performed on public data) is a practical approach to bypassing the compute cost associated with per-task tuning and the privacy leakage caused by tuning on private training data. \n\nWe will include the discussion on hyperparameter transfer in the main text in the next revision.\n""}, {'summary_of_the_paper': 'The paper propose a faster algorithm to learn approximate differentially private NLP models. Pretrained NLP models are often very large. Practical procedure involves fine-tuning NLP model on private data, which may leak private information. To avoid leakage, DP-SGD (and DP-AGAGRAD, DP-ADAM) uses norm clipping on each sample’s gradient, and then add isotropic noise to aggregated gradients for samples in a batch. Then the normal update steps of SGD, ADAGRAD, or ADAM are performed. This will ensure privacy under differential privacy definition. However, this procedure requires computing per-sample gradient and keep them in the memory so that the norm of the gradient can be calculated, and the per-sample gradient clipping can be performed. This will introduce memory overhead proportional to the batch_size$\\times$#params, which is impossible for very large NLP models. \n\nThis paper propose GhostClipping method to save the memory, without the need of per-sample gradient instantiated. The idea is compute the partial sum of gradient element-wise square using two small matrices of the size of $T\\times T$, where $T$ is sequence length (<=1024 in practice), and aggregate them to obtain the per-sample norm (just a scale for each). And then it performs a second back-propagation to compute aggregation on the clipped gradient. It will uses almost the same memory as standard SGD (or ADAM), with one forward pass and two backward passes. \n\nThis paper also introduce two additional techniques to improve, one is choose a larger batch size, and the other is to introduce multi-task finetuning (fine-tuning includes masked prediction task on the target dataset). These two are important in boost the performance of the final models. \n\nThe paper evaluates on text classification, data-to-text generation, and dialog generation tasks. The results shows it gains prior DP methods. \n\n\n\n', 'main_review': 'Strength of the paper:\n1. The paper is clearly written and easy to understand. It presents the DP training problem very well and demonstrates the challenge for large pretrained NLP models. \n2. The proposed GhostClipping is effective in reduce memory consumption, at (not too big) cost of an extra backpropagation. The method is very clear and also simple to implement. \n3. The experiments show strong results on both classification and text generation tasks, better than the prior DP methods in terms of memory consumption and accuracy/generation quality. \n\nWeakness of the paper:\n1. There are three aspects involved. GhostClipping, large batch/proper learning rate, fine-tuning with masked prediction. It seems the accuracy on text classification comes from the later two. Table 1, comparing full(RoBERTa-large) with RGP(RoBERTa-large) does not show improvement (please list the average score here). Full is actually worse. If RGP is augmented with larger batch/proper learning rate, and masked prediction for fine-tuning, it may be the best in terms of accuracy. \n2. Figure 1 is a bit mis-leading. Please show and compare with RoBERTa (non-private) on 1(a) and non-private GPT-2 on 1(b). Those are the baselines to compare. Otherwise you are comparing private RoBERTa with non-private BERT or non-private T-GEN(non Transformer). Statement in the abstract and introduction are over-stated. \n3. The organization of the paper could be improved. Ghost-Cliping is your main method, which could be moved earlier. \n', 'summary_of_the_review': 'Reason to accept:\nSimple DP algorithm with demonstrated reduction in memory consumption and effectiveness. It enables large pre-trained NLP models to be further fine-tuned on private data under approximate DP. Clear presentation. \n\nReason to reject:\nBetter to identify the cause of improvement and make a fair comparison to baseline models (in particular for text classification tasks). Over-statement and misleading in abstract and introduction about comparable to strong non-private baselines.\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'This paper investigated the problem of privately fine-tuning large language models for downstream NLP tasks, including sentence classification and language generation. The authors showed that by appropriately selecting hyper-parameters (including batch size, learning rate, training epochs, and clipping norm) and making the fine-tuning task aligned with pretraining tasks, directly fine-tuning large language models with DP-SGD yields strong performance, and provided an empirical guideline for setting a good training configuration. The authors also proposed ghost clipping trick for further memory saving when fine-tuning large language models. Finally the authors showed through experiments that low dimensional updates do not necessarily lead to better performance.', 'main_review': ""Overall, this is a fairly empirical paper on an important problem and shows good performance. The authors presented a set of thorough experiments investigating the impact of various hyper-parameters, which are widely known as sensitive and difficult to tune, providing a nice and informative guidelines for other researchers and practitioners who work in this area. The ghost clipping trick proposed in this paper for memory reducing in DP-SGD is simple yet effective, greatly reducing the memory cost when applying DP-SGD to large models, especially the popular large-scale pre-trained language models, potentially encouraging more research effort in the DP learning area.\n\nThe experiments presented in this paper are quite solid and clear, both well-designed and documented, and most of the claims made in the paper are reasonable and well-supported. My only complaint is that I wish to see more explanations and more principled approach to selecting the hyper-parameters in fine-tuning with DP-SGD, but I guess it is out of this paper's scope (the authors did provide some empirical discussions in the paper and the appendix, which I appreciate)."", 'summary_of_the_review': 'This paper presented a detailed investigation on the training configurations of fine-tuning large language models with DP-SGD through a set of extensive experiments, which are beneficial to researchers and practitioners working on private NLP, and the proposed ghost clipping trick for DP-SGD greatly reduces memory when applied to large language models, making private NLP research more feasible. I believe both the insights and the proposed method in this paper will bring value to the private NLP community.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'n/a', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper adapts the widely used DP learning algorithm, DP-SGD, to language models. It achieves to fine-tune the dataset while protecting the private information in the dataset. In this paper, the authors conduct some empirical studies on language models and find some useful conclusions (e.g. fine-tuning on a part of parameters with DP is enough). The authors verify the model on sentence classification, table-to-text generation, and dialog generation tasks, using various pre-trained language models (e.g. GPT, Bert).', 'main_review': ""Pros,\n1) It achieves a remarkable performance of DP algorithms on NLP tasks with a satisfactory level of privacy protection.\n2) There will be a number of potential applications since this paper verify the model on multiple NLU and NLG tasks.\n3) It works on several pre-trained language models (e.g. GPT, BERT, Roberta).\n4) The experiments part is quite solid and covers many necessary details.\n\nCons,\n1) The experimental results in table 1~3 compared to the non-private seems amazing, however, the comparison with the baseline methods is also required, which would show the contribution of your proposed methods. There're several possible baselines: directly apply DP-SGD and an application of DP-SGD on language models (GPT-2) [1].\n\n2) The choice of \\epsilon makes sense because the \\epsilon in a range of 0.1~5 provides meaningful protection. However, the choice of \\delta in experiments may cause some problems in privacy leakage. The value of \\delta on the order of 1/|D_train| is very dangerous according to [2]. In that level of \\delta, they permit “preserving privacy” by publishing the complete records of a small number of database participants. It's better to choose the \\delta as 1/( |D_train| *100), which may reduce the utility but ensure the privacy is well protected. \n\n3) Will the ghost clipping increases the utility or privacy protection? Could you please provide some insights (or even quantitative analysis) about it?\n\n4) It would be better to conduct some key experiments with a range of \\espilon from 0.1 to 10 (e.g. 0.1, 0.5, 1, 2, 3, 5, 8). Many DP learning algorithms use a curve to show the variation tendency of utilities for the different \\epsilon.\n\n5) This paper did a good job on the empirical study, but the technical novelty is limited.\n\n[1] Differentially Private Language Models Benefit from Public Pre-training. 2020\n\n[2] The Algorithmic Foundations of Differential Privacy, 2014\n\n\nAfter rebuttal:\nThanks for your explanation! I believe it's a good paper and I'm happy to see its acceptance.\n"", 'summary_of_the_review': 'The paper has a clear motivation and the main idea is also interesting, but still suffers from some issues.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'In this paper, the authors propose a method that applies DP-SGD to NLP tasks. DP-SGD protects the privacy of the model training against that the individual information about the training samples is detected or inferred. The method is applied to the fine-tuning phase of the pre-trained language models (e.g. bert, gpt), thus it achieves good performances for many applications. To adapt DP-SGD to NLP models, this paper proposes ghost clipping that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model.', 'main_review': ""Strengths:\n1. As a DP algorithm on a deep learning model, this algorithm achieves good performance on fine-tuning the language models and several real-world applications.\n2. The experimental results are sufficient and solid. \n3. The performance gap between the proposed method and the non-private method is quite small, which shows applying the proposed DP algorithm would not decrease the utility so much.\n4. Some of the conclusions from the experiments seem to be adaptive to other applications. For example, the relations among parameter size (for fine-tuning), privacy leakage, and performance.\n\nWeaknesses:\n1. The \\delta in (\\epsilon, \\detla)-DP is too large for training a DP model with a meaningful level of privacy protection. Normally, \\detla should be much smaller than the inversed dataset size (cannot be in the same scale of the inversed dataset size), some researchers choose \\detla = 1/(|D| * log(|D|)), where |D| stands for the dataset size.\n\n2. It would be better to conduct an ablation study to verify the promotion of the proposed methods (ghost clipping technique, fine-tuning on only a part of the parameters, and DP-adam). It's better to make it clear how much does each strategy contribute.\n\n3. In this paper, the authors introduce a lot about the selection of hyper-parameters, thus hyper-parameters play an important role in model training. However, tuning hyper-parameters requires additional information about the private information (accessing the validation set or testing set), which leads to private leakage. So, how to count the information leakages of users? How to avoid those private leakages.\n\n4. Is the DP-Adam proposed by yourself? If not, the corresponding reference and explanation are needed. Does it come from [Wang 2019]? If it is proposed by yourself, I suggest the author make it clear as it's one of the contributions. \n\n[Wang 2019] DP-LSSGD: An Optimization Method to Lift the Utility in Privacy-Preserving ERM."", 'summary_of_the_review': 'The paper is quite good on experiments and their possible applications but faces some questions.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Large Language Models Can Be Strong Differentially Private Learners', 'authorids': ['~Xuechen_Li1', '~Florian_Tramer1', '~Percy_Liang1', '~Tatsunori_Hashimoto1'], 'authors': ['Xuechen Li', 'Florian Tramer', 'Percy Liang', 'Tatsunori Hashimoto'], 'keywords': ['language model', 'differential privacy', 'language generation', 'fine-tuning', 'NLP'], 'abstract': 'Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead.\nWe show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure.\nWith the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines---by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. \nTo address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. \nThe technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. \nContrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation.\nCode to reproduce results can be found at https://github.com/lxuechen/private-transformers.\n', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'li|large_language_models_can_be_strong_differentially_private_learners', 'pdf': '/pdf/d88e1e721c4085b8a6403837f45b8c483ad0225b.pdf', 'one-sentence_summary': 'We show how to build highly performant differentially private NLP models by fine-tuning large pretrained models.', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 9 code implementations](https://www.catalyzex.com/paper/large-language-models-can-be-strong/code)', '_bibtex': '@inproceedings{\nli2022large,\ntitle={Large Language Models Can Be Strong Differentially Private Learners},\nauthor={Xuechen Li and Florian Tramer and Percy Liang and Tatsunori Hashimoto},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=bVuP3ltATMz}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['António Farinhas', 'Wilker Aziz', 'Vlad Niculae', 'Andre Martins']",ICLR,Sparse Communication via Mixed Distributions,https://iclr.cc/virtual/2022/oral/5896,2022," Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call ""mixed random variables.'' Our starting point is a new ""direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (""sample-and-project'’) and an intrinsic one (based on face stratification). We experiment with both approaches on an  emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=WAid50QschI,https://openreview.net/forum?id=WAid50QschI,WAid50QschI,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes mixed distributions over convex polytopes, and provides theory for mixed distributions that is relevant to the machine learning community. All of the reviewers were positive, and agree that this is a solid contribution. I agree, and I believe that this paper stands a chance of being a foundational paper for future work in probabilistic ML and structured learning.'}, {'title': 'Response to authors', 'comment': 'Dear Authors,\n\nMany thanks for your detailed response\n\nMy two major concerns regarding:\n1) how would you treat more than one tuples of T discrete outcomes, and ;\n\n2) the algorithmic complexity; \n\nwhere clearly explained by your response.\n\nOn (2) I was mainly worried about the the exact computation of the continuous part which costs as you said O(2^K) but, you responded that for large K, you can by-pass the exponential problem by using an MC estimate that costs O(K) (it is important though to acknowledge that this is merely an estimate).\n\nI thank the authors for their detailed response; my evaluation will be changed accordingly \n\nGood job.'}, {'title': 'Author response', 'comment': 'Thank you for your positive reviews and suggestions!\n\nIn the manuscript we focused on learning sparse representations, but the direction you suggest is indeed very interesting, thanks for pointing it out. We have now reproduced one of the experiments reported by [1] comparing three likelihood functions in a generalized linear model: Dirichlet, the Continuous Categorical [1], and our Mixed Dirichlet. For each observation, the model regresses from 4 predictors to a 5-dimensional probability vector (proportions of votes towards one of 5 parties in an election), you can find the results in Appendix G.4. As you expected, the Mixed Dirichlet too addresses the pathologies of the Dirichlet in this setting, and it also shows a slight advantage over the Continuous Categorical, likely due to the fact that Mixed Dirichlet samples are often sparse.\n\n> Section 3.1, when defining a face, I believe (...) should be replaced by ""A face P is any intersection of P with a closed halfspace such that none of the interior points of P lie on the boundary of the halfspace"" for added clarity.\n\nThank you for the suggestion! We have updated the manuscript.\n'}, {'title': 'Author response', 'comment': 'Thank you for your positive review!\n\nWhile our paper has indeed a strong theoretical component, we do not see this as a weakness. Our paper addresses the problem of learning sparse representations, a topic of wide interest to the ICLR audience, by carefully building a sound mathematical framework for handling mixed distributions. Other ICLR papers which ally theory and practice for learning sparse representations include the paper which proposed the Concrete distribution (ICLR 17, https://arxiv.org/pdf/1611.00712.pdf), D-VAE (ICLR17, https://arxiv.org/pdf/1609.02200.pdf), and Hard Concrete (ICLR 18, https://arxiv.org/pdf/1712.01312.pdf), among several others, all of which a strong inspiration to our own work. Alongside our theoretical contributions, we demonstrate with a set of simple experiments that our framework can be useful in practice, with mixed distributions being a promising way to address some of the limitations of latent variable models. Having said that, we welcome any suggestions that could make our paper more accessible to an audience less familiar with measure/probability theory.'}, {'title': 'Author response', 'comment': 'Thank you for your positive review and suggestions! Our paper currently mentions some mathematical and probability literature that can be helpful to the reader, including Ziegler (1995), Conway (2019), and we added a new  reference to Halmos’ “Measure Theory” book in the revised version. Any further recommendations are very welcome! '}, {'title': 'Author response', 'comment': 'Thank you for the review! We will now try to address your concerns about our paper.\n\n> Table 1 is quite unclear to me.\n\nTable 1 is a summary table comparing the properties of the discrete, continuous, and mixed distributions considered in our paper. The table distinguishes distributions according to three questions: \n\n1. Does the distribution assign probability mass to all the faces of the simplex? (for visualization, see Figure 1, which highlights two 3D distributions with opposite answers);\n2. Is it restricted to binary variables or does it support multiple categories (K >= 2)?;\n3. For mixed distributions, is it characterized extrinsically (using the “sample-and-project” approach discussed in Section 3.3) or intrinsically (based on face stratification; a mixture of distributions is specified directly over the faces of the probability simplex)? \n\nPlease let us know if this clarifies your question. \n\n> do not provide information regarding the case of having more than one discrete outcomes with different alphabets;\n\nDo you mean predicting a tuple of T discrete outcomes instead of a single one? Our method handles this case as other methods would, namely, we can give the T variables independent treatment, or factorize their joint probability via the chain rule, in which case each factor is a conditional over a single mixed variable. Our techniques apply to both cases without special treatment. \n\nIf the T variables are treated independently, this corresponds to a latent space which is a product of simplices of (possibly) different dimensions -- in this case the total direct sum entropy/KL is the sum of entropies/KL as expected. The binary vector case that we address in our paper for the bit-vector VAE (Section 5) is a particular case of a product of simplices with |V|=2. You can check Section 3.1: “the hypercube can be regarded as a product of simplices...”\n\nIf the T variables are not treated independently, we can have multiple random variables and use a joint distribution via the chain rule $p(x_1, ..., x_N) = prod_{i=1}^N p(x_i|x_{<i})$, where the distribution of each $x_i$ is a mixed distribution parameterized given $x_{<i}$.\n\nIn summary, there is nothing specific to our mixed distributions that would provide any limitation when extending to multiple discrete outcomes. Does this answer your question?\n\n> what is the algorithmic complexity with respect to the number of observations, the rate of continuous vs discrete outcomes;\n\nDo you mean the algorithm complexity of sampling a face? This is relevant for our proposed intrinsic mixed distribution, the Mixed Dirichlet. In that case, sampling is done with the dynamic program described in App. C (to rule out the empty face), and the complexity is O(K). You can check our App. C for details about the computation complexity of other quantities of interest. For instance, both the log-normalizer and the gradient of KL(P_F||Q_F) can be computed in a single pass through a DAG of size O(K); the exact computation of the continuous part in H(Y|F) costs O(2^K) but, for large K, we can use an MC estimate that costs O(K).\n\nFor the multivariate Gaussian-Sparsemax, the complexity is that of sampling K i.i.d. numbers from a Gaussian and projecting them to the simplex (which is the asymptotic cost of sparsemax, O(K), or O(K log K) with a sorting algorithm [1]).\n\nThe algorithmic complexity is linear in the number of observations, for a fixed number of epochs.\n\nRelated to your previous question, this is what happens if we had a tuple of T mixed random variables. With independent mixed random variables, the sampling is independent, thus the time complexity is unchanged on GPUs, the space grows linearly with the number of random variables. With a chain rule factorization, the complexity is linear because we sample the $i$-th random variable given an assignment of the preceding ones.\n\n[1] André FT Martins and Ramón Fernandez Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In ICML, 2016.\n\n> they still work on the code and they do not provide it unless the paper is accepted\n\nPlease note that we included all the code and instructions to reproduce our experimental results as supplementary material (please check the submitted zip file). This is described in our reproducibility statement (see page 10). The code is finalized and it will be publicly released on github after the anonymity period ends. We hope this clarifies your concern.\n'}, {'summary_of_the_paper': 'This paper proposes mixed distributions over convex polytopes such as the probability simplex. The proposed distributions are a discrete mixture over the faces of the polytope of ""continuous"" distributions on the corresponding face (formally, absolutely continuous wrt the Lebesgue measure on the face). For example, for the 2-simplex there is a distribution over the triangle, over each edge, and over each vertex. The authors formalize the dominating measure of these distributions, which they call the direct sum measure (given by the mixture over the counting measure over faces and Lebesgue measures on the faces); and derive formulas for entropy and KL divergences between such distributions, as well as characterizing maximum-entropy distributions.', 'main_review': 'This paper was a pleasure to read. The idea is simple and intuitive, and it addresses a recurring issue with commonly-used simplex-valued distributions, allowing to better model sparcity while avoiding diverging likelihoods in the presence of 0s. The paper is clearly written, and the mathematical exposition is formal and well presented. I also believe the idea presented in this paper will be easily used to propose new simplex-valued distributions and be valuable to the community: while the authors propose several instances of mixed distributions, one can easily think of potential alternatives.\n\nMy only complaint about the paper is that the experiments focus mostly on sparcity, and not on avoiding ill-defined log-likelihoods, which I actually believe is another benefit of the proposed distributions. For example, the distribution proposed in [1], which is discussed in the paper, addresses learning $p(y|x)$ when $y$ is simplex-valued in the presence of 0s in the data; which can be handled in a more principled way through mixed distributions. Finally, the experiments are carried out against the Gumbel-Softmax, but not more recent improvements upon it, e.g. [2,3,4]. Nevertheless, I believe the contributions of the paper are enough to warrant publication.\n\nMinor things:\n\n-Section 3.1, when defining a face, I believe ""A face P is any intersection of P with a halfspace such that none of the interior points of P lie on the boundary of the halfspace"" should be replaced by ""A face P is any intersection of P with a closed halfspace such that none of the interior points of P lie on the boundary of the halfspace"" for added clarity.\n\n\n[1] The continuous categorial: A novel simplex-valued exponential family, Gordon-Rodriguez et al., ICML 2020\n\n[2] Estimating Gradients for Discrete Random Variables by Sampling Without Replacement, Kool et al., ICLR 2020\n\n[3] Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax, Potapczyski et al., NeurIPS 2020\n\n[4] Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator, Paulus et al., ICLR 2021\n\n========================================================================================================\n\nUPDATE 1 AFTER REBUTTAL\n\n========================================================================================================\n\nI have read the author\'s rebuttal as well as the other reviews, and my opinion on the paper remains that it should be accepted. I particularly appreciate the authors adding the suggested additional experiment to an already strong paper.', 'summary_of_the_review': 'This paper proposes a well-motivated and mathematically elegant family of distributions on convex polytopes, allowing to place positive probability not only on the interior of the polytope, but on its faces as well. While there is room for improving the experiments, I believe the presented experiments and theoretical developments -- which can easily enable future work -- should be accepted.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'In this paper, the authors build rigorous theoretical foundations for mixed random variables. They first define a natural measure for mixed random variables, which looks like a direct sum. Then they define the entropy and KL divergence based on the proposed measure. They also give two strategies for representing and sampling mixed random variables. At last, they conduct some experiments to illustrate the usefulness of their framework.', 'main_review': 'Strengths:\n\n1. The technique in this paper is very solid. I believe the theoretical foundations built in this paper will be useful in the future research on mixed random variables. \n\n2. The idea of this paper makes sense, and this paper is well written and easy to read. \n\n3. The experimental results demonstrate the usefulness of the proposed framework. \n\n\nWeaknesses:\n1. My only concern is that whether ICLR is a proper conference to publish this paper, since it involves some concepts about measure theory and probability theory.  \n', 'summary_of_the_review': 'This paper is very solid. It builds some important theoretical foundations for the mixed random variables. It seems that this will be useful for the future research on this topic. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper presents multidimensional extensions for mixed random variables originating from discrete-continuous hybrids based on truncation and rectification, which have been proposed for univariate distributions. The proposed extension replaces truncation by sparse projections to the simplex. The authors also propose a direct sum base measure definition on the face lattice of the probability simplex and intrinsic sampling strategies motivated by “manifold stratification”. Based on these introductions, new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality, are presented. ', 'main_review': ""**Reasons to accept:**\n- The paper is well-written and appears relevant to recent developments. \n- The motivation of the paper in relevance to bridging the continuous representations computed by neural networks and other machine learning models with the discrete representations that characterize humans is interesting. Table 1 nicely summarizes the contributions of this work.\n- The mathematical coverage also seems reasonable. \n- Extensive experiments on three different tasks, including an emergent communication benchmark. Supplementary material also provides qualitative examples.\n\n**Suggested improvements:**\n- Mixed random variables are a standard topic in probability theory. Definitions 1,2,3 appear to be natural in this theoretical context and the same applies to Propositions 1,2. More references in the mathematical and probability literature would be helpful due to the coverage of mixed random variables and associated theory in this literature. \n\n**Update after rebuttal:** I have read the reviews and the author's responses. This is solid well-written work, my review remains unchanged. "", 'summary_of_the_review': 'This is a paper with mathematical rigor and connections to emergent communication, which is a current topic of interest for specific machine learning communities. Albeit the submission of this paper would better fit a probability and statistics venue, to the best of my knowledge, the definitions 1-3 and propositions 1,2 appear to be novel. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'None', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The authors wish to introduce a new kind of random variables that are consisted of both continuous and categorical data.\nThey also provide the corresponding theory and take into account information theoretical aspects.', 'main_review': 'The authors are encouraged for their good work.\nThough they do try to introduce ""mixed variables"" (ie random variables consisted by both categorical and continuous data) there are a few unclear points: for example, they consider a (ie one) categorical variable with alphabet K. What is the case when there are more than one categorical points, with different alphabet (eg ordinal data with alphabet X) ?\nAlso, what is the algorithmic complexity with respect to the number of observations, the number/rate of continuous and discrete outcomes and the number of variables (in the case of multivariate mixed variable?\nFinally, Table 1 is quite unclear to me.', 'summary_of_the_review': 'My main point is that the authors\n1) do not provide information regarding the case of having more than one discrete outcomes with different alphabets; \n2) what is the algorithmic complexity with respect to the number of observations, the rate of continuous vs discrete outcomes;\n3) they still work on the code and they do not provide it unless the paper is accepted', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Sparse Communication via Mixed Distributions', 'authorids': ['~António_Farinhas1', '~Wilker_Aziz1', '~Vlad_Niculae2', '~Andre_Martins1'], 'authors': ['António Farinhas', 'Wilker Aziz', 'Vlad Niculae', 'Andre Martins'], 'keywords': [], 'abstract': 'Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call ""mixed random variables.\'\' Our starting point is a new ""direct sum\'\' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (""sample-and-project\'’) and an intrinsic one (based on face stratification). We experiment with both approaches on an  emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables.', 'pdf': '/pdf/f8c966f98befffb0bfbd9af921a4e4dd831d549f.pdf', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'farinhas|sparse_communication_via_mixed_distributions', 'supplementary_material': '/attachment/92722c31871512945eb4d2c789e94cd2ba73b3b0.zip', 'code': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/sparse-communication-via-mixed-distributions/code)', '_bibtex': ""@inproceedings{\nfarinhas2022sparse,\ntitle={Sparse Communication via Mixed Distributions},\nauthor={Ant{\\'o}nio Farinhas and Wilker Aziz and Vlad Niculae and Andre Martins},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=WAid50QschI}\n}"", 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Sagar Vaze', 'Kai Han', 'Andrea Vedaldi', 'Andrew Zisserman']",ICLR,Open-Set Recognition_ A Good Closed-Set Classifier is All You Need,https://iclr.cc/virtual/2022/oral/6728,2022," The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr closed set all you_need.",Oral 1: AI Applications,https://openreview.net/pdf?id=5hLP5JY9S2d,https://openreview.net/forum?id=5hLP5JY9S2d,5hLP5JY9S2d,"[{'title': 'Thanks', 'comment': 'Thanks for the info in brief.'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper provides well-written and thorough analysis demonstrating that closed-set recognition performance correlates with open-set recognition performance, and that simply making the close-set model strong via augmentation, label smoothing, etc. along with small scoring changes (using logits rather than softmax probabilities) can get close to (or better than in some cases) performance than much more complicated methods. The authors also propose a large-scale benchmark that varies the semantic similarity across classes, allowing for a more fine-grained analysis of this problem. \n\nOverall, all of the reviewers thoughts that the paper provides very thorough validation of an insight that would be very interesting to the community. Reviewer HAFU had some concerns about novelty, since a number of papers have shown closed-set classifier improvements (and therefore better embeddings) benefit related problems such as few-shot learning and generalization to novel domains, as well as proposed large-scale experiments. The rebuttal convinced this reviewer, however, that some of the contributions and findings are unique and provide additional evidence to the community, and the new setting provides more fine-grained analysis. Reviewer dw7J had a number of suggestions in terms of additional evaluations, and the rebuttal either clarified why it is not possible or added them. As a result, after the discussion the reviewers all supported acceptance of this paper. \n\nGiven the above discussion, and rebuttal/changes to the paper, I recommend acceptance. It is a very well-done empirical paper, provides interesting findings, stronger baselines, and thorough experimentation. Further, some of the smaller findings (ViT correlation experiment) as well as larger relationship between open-set recognition and out-of-distribution detection are valuable contributions to the community. Finally, I would recommend this paper as oral, given that it may garner a good discussion of these contributions.'}, {'title': 'Final evaluation', 'comment': 'The updates by the authors addressed my concerns well, also consider the comments from other reviewers, I will keep my previous rating of this paper and suggest to accept this paper. '}, {'title': 'Final Evaluation', 'comment': 'I read the other reviews and I think there is a clear agreement that the paper is well written, clarify some important points about open-set recognition and out of distribution and shows that well tuned baselines based on cross-entropy are often very close (or even better in certain cases) to the performance of more complex and adapted methods.\nI also read authors answers to my questions/comments and in most of the cases they give a clear answer about the impossibility to add some additional experiments or where possible they added the additional evaluation.\nThus, I keep my positive evaluation of this work and I recommend it for publication.'}, {'title': ""Updating rating to '8'"", 'comment': ""I appreciate author’s detailed feedback and updates to paper on various issues.\n\nI have updated my rating to ‘8’, I was concerned for novelty but contribution of paper is very useful for both researchers and practitioners. As pointed by reviewer 'dw7J' it is important to highlight simplicity. The take home message of this paper is very useful, that by leveraging augmentations, data, architectures (ViTs) if we improve the quality of representation on closed set then it reduces the gap between complex open-set detection mechanisms or even outperform them. But when the closed set accuracy is poor may be more complex methods compensate for quality of representation.  \n\nI think this work is another evidence that a good representation (closed set performance, invariant/robust, etc.) is the crux of open set detection, few-shot learning, generalization to novel domains, similar to [3],[4] and other works.\n\nOne additional comment would be, if the visualization plot can be updated from CIFAR10 to CIFAR100 (closed set) or ImageNet i.e. when the number of closed set classes are large.""}, {'title': 'Response to Reviewer dw7j (1/2)', 'comment': 'We thank the reviewer for their kind comments on our paper and for succinctly summarising our take-home message: “at this stage of research for improving open-set recognition, the best that we can do (or almost) is to use stronger models for close-set recognition”. We also concur that it is important for the research community (and practitioners) to understand when simple algorithms work as well as complex architectures, and that other similar papers are important! We also appreciate the comment on the importance of clarifying the tasks of OSR and OoD, and refer to the general response (above) for comments on experiments in the OoD setting.\n\n>“In my understanding the distinction between open-set recognition and out-of-distribution is ficticial and the proposed cross-entropy baseline should evaluated also on out-of-distribution settings”: \n\nWe provided experiments in the OoD setting in Appendix F. We have now further augmented this section with results on the open-set/closed-set correlation in an OoD setting (please refer to the global response for details). To summarize, we found that:\n - Taking a strong closed-set classifier on CIFAR100 can outperform a popular OoD baseline (Outlier Exposure) on common OoD evaluations.\n - We could identify a similar correlation to Fig 3b in the main paper on the CIFAR100 → CIFAR10 OoD benchmark. We found a strong correlation between the closed-set accuracy of ResNets trained on C100, and their OoD performance on C10.\n\n*Regarding the OoD vs OSR distinction more broadly:* we agree with the reviewer that there is currently significant overlap in the research question tackled by the two fields. Hence we propose specific OSR benchmarks in Sec. 5, which isolate semantic novelty from general distribution shifts. In theory, we believe that OSR (semantic shift, e.g image of a bird → image of a different species of bird) is a subset of OoD (any distribution shift, including e.g image of a bird → Gaussian noise). As such, it may be the case that there are algorithms which work specifically well for the OSR problem; for instance those based on class-prototypes or class attributes. In fact, our results in Appendix F (Tab. 7) give some evidence that this is the case. This difference could be considered analogous to, for example, Fine-grained Image Recognition (CUB, FGVC-Aircraft) vs. General Image Recognition (ImageNet, CIFAR). We hope the proposed benchmarks will help study this distinction. \n\n>“Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits  … However, as previous works use the softmax scores ... I would like to see this comparison at least for the models in Tab. 1”: \n\nOverall, due to their open-set scoring mechanisms, the other models in Tab. 1 are not suitable for performing an ablation on the ‘Softmax’/‘Logits’ scoring rule. Specifically:\n - ARPL + CS uses the maximum distance with a set of learned ‘reciprocal points’ in feature space to provide the open-set score. We note that this could be seen as similar to logits (if one considers the reciprocal points as weights of a linear classifier).\n - OpenHybrid does not use the classification vector for OSR scoring at all, but rather directly uses the scalar output of a density model as an open-set score.\n - With OSRCI, the open-set score is computed by appending ‘0’ to the logit vector and normalizing with Softmax, thus creating a dummy probability for a ‘K+1’ class. \n\n>“It is still important to consider also the [performance of] other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes … I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3)”: \n\nOverall, we found methods from Tab. 1, other than ARPL, to be too computationally expensive for us to train in the new setting in Tab. 3. We hope that, given that ARPL is a SOTA model in OSR research and the highly competitive performance of Cross-Entropy, our experiments will serve as sufficient baselines for future research. Specifically:\n\n - We attempted to implement OpenHyrbid (after requesting code from the authors) but underperformed the reported numbers. We believe this to be due to the very large batch size (1024 in theirs vs 128 in ours) and large number of epochs (10k in theirs vs 600 in ours) required to stabilize the model. \n - OSRCI requires training: a classifier and GAN concurrently; generating an auxiliary dataset with the GAN; and then training another classifier. Thus, it is expensive to train on large-scale datasets like ImageNet under a constrained hardware budget. Given the clarity of the ranking in Tab.1, we believe it still holds true in our new setting. \n'}, {'title': 'Response to Reviewer dw7j (2/2)', 'comment': '>“In Fig. 2 I would expect to see also results for OpenHybrid”: \n\nWe did not include OpenHybrid results in this figure for the same reasons as mentioned above (we could not recreate their results with our computational resources). However, we could include OSRCI results in this figure, as we found this model tractable to train on the old (smaller scale) OSR benchmarks. We thank the reviewer for this suggestion and have included this version of the figure in Appendix A (Fig. 6).\n\n>“In the figure [Fig. 2], for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance”: \n\nThis is an interesting point. It is indeed the case that the trend may behave differently when closed-set accuracies become very high or saturated. We have updated the manuscript to include this nuance in Appendix A.\n\n>“In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds”: \n\nWe agree that it would be interesting to further analyze the reasons why ViT models perform well on OSR. Similar experiments were conducted in [1] (as pointed out by Reviewer HAFU). It is worth noting that all models in Fig 3, including ViT, were trained on the same data (ImageNet-1K). '}, {'title': 'Response to Reviewer PPFy', 'comment': 'We thank the reviewer for the time they spent on our work, and for commenting on the value and clarity of the paper!\n\n>“there are not more in depth analysis the behind reason for these findings [closed-set / open-set correlation]”: \n\nThis point was also raised by Reviewer ‘HAFU’, hence we address it in the general response (above). To summarize, we provide some intuitions for the empirical correlation in Sec 3.1 ‘Discussion’, as well as more in-depth analysis in Appendix B by looking at feature visualizations of the cross-entropy baseline.'}, {'title': 'Response to Reviewer HAFU (1/2)', 'comment': 'We thank the reviewer for their detailed feedback on our paper, and appreciate their acknowledgement of the practical utility of our findings (that the simple baseline performs comparably to SOTA), as well as our comprehensive experimental results. We refer the reviewer to the general response (top) for comments on CIFAR100 → CIFAR10 experiments and analysis on the closed-set/open-set correlation. Note: Analysis of the correlation is provided mainly in Appendix B, where we further provide feature space visualizations of weak / strong classifiers.\n\n>“[closed-set / open-set correlation] is known for researchers working on open-set detection, though not highlighted enough”: \n\nAlthough this correlation may be known to some researchers, we noted in Sec. 4 that increasingly sophisticated methods are proposed for OSR with carefully tuned training strategies and hyper-parameters. Meanwhile, the closed-set accuracy of the methods is often unreported in the literature, making it difficult to delineate what proportion of the open-set performance gains come from increases in closed-set accuracy. We thus perform controlled experiments to explore the correlation and further use our findings to demonstrate SOTA (or almost) open-set results with the simple cross-entropy baseline. We further note that:\n - To our knowledge, few papers comment on both the closed-set and open-set performance. In fact, when the two are mentioned together, it is often highlighted that good OSR performance is achieved ‘without harming’ the closed-set accuracy [1][2]. \n - Though some methods proposed in the CVPR 2021 ‘Open World Vision’ workshop (mentioned in the review) discuss methods to improve the backbone, they do not rigorously explore how the improved accuracy affects the OSR performance.\n\n>“[3],[4] highlight the fact that good representation is all you need for meta learning or out of distribution generalization which though orthogonal to open-set detection, informs the research community that ‘quality of representation & performance on closed set’ is useful”:\n\nWe believe our findings are different and complementary to those from [3] and [4]. \n\nSpecifically, [3] does not show that **better** closed-set representations transfer better to the few-shot task. Instead, they demonstrate that closed-set representations are good enough to provide few-shot features compared with bespoke few-shot algorithms. They also demonstrate that good self-supervised features work almost as well. In contrast, we specifically demonstrate that the better the closed-set representation, the better the open-set performance (i.e we demonstrate an explicit correlation between the two metrics).\n\nSecondly, [4] show that the better features can discriminate between a set of classes, the better those features can discriminate between the *same classes* under a distribution shift. This makes sense as these features have learned more invariances to spurious factors which benefit both in-distribution and out-of-distribution classification performance. In contrast, we show that these same features - which have learned more invariance to spurious factors - are better at detecting completely new classes.\n\n>“[5] makes similar claims that vision transformers result in better open-set detection"": \n\n[5] shows that vision transformers outperform convolution models on an OoD benchmark. However we further show that ViTs not only give better open-set detection results, but also outperform the closed-set / open-set trend. Furthermore, this observation is not one of our key contributions, but an incidental finding amongst a larger study of the closed-set / open-set correlation over a wide range of architectures. \n\nThank you for bringing this paper to our attention, we have now acknowledged this work in our manuscript (Sec 3.2, ‘Discussion’). '}, {'title': 'Response to Reviewer HAFU (2/2 + citations)', 'comment': '>“Another key contribution of this paper is proposing large scale ImageNet benchmark, [6] is a CVPR’21 competition and existing benchmark along the similar lines”: \n\nPrevious works [6][7] have constructed open-set splits from the larger ImageNet database (we had cited [7] in Sec. 1 and Sec 5.1). Our contribution is to carefully structure the open-set classes for semantic similarity with the (closed-set) ImageNet-1K classes, thereby creating OSR splits of varying difficulty and allowing better understanding of the open-set problem. Our experiments suggest that this structuring is important. Specifically, we note that:\n - Randomly sampling 10x more open-set classes from the larger ImageNet database reduces open-set performance by only 0.6%. \n - In contrast, with our method, choosing open-set classes which are semantically more similar to ImageNet-1K (while keeping the number of open-set classes the same), reduces open-set performance by 6% (a 10x difference). \n - We also propose two other benchmarks, based on FGVC datasets, which isolate semantic novelty from other low-level distributional shifts. In this case, harder splits reduce AUROC by as much as 19%.\n\nThese observations are highlighted in Sec 5.2 ‘Results’ in our manuscript. We thank the reviewer for pointing out [6] and we have now cited it in our manuscript (Sec 5.1 ‘ImageNet for open-set recognition’).\n\n[1] D. Zhou et al.: Learning Placeholders for Open-Set Recognition \\\n[2] R. Yoshihashi et al.: Classification-Reconstruction Learning for Open-Set Recognition \\\n[3] Y. Tian et al.: Rethinking Few-shot Image Classification: A Good Embedding is All You Need? \\\n[4] J. Miller et al.: Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. \\\n[5] Fort et al.: Exploring the Limits of Out-of-Distribution Detection \\\n[6] Open World Vision CVPR’21 Competition and workshop \\\n[7] Bendale & Boult: Towards Open Set Deep Networks'}, {'title': 'General Response', 'comment': 'We thank all reviewers for the constructive comments on our work. We appreciate the reviewers acknowledging the practical value of our findings for the research field and commenting on the comprehensive experimental evaluations. We found two comments which were common amongst more than one reviewer, hence we highlight them here. \n\n>“lacks sufficient commentary on why a good closed set accuracy would result in better open-set detection”[HAFU] \\\n“there are not more in depth analysis the behind reason for these findings [open-set/closed-set correlation]”[PPFy]: \n\nWe included commentary on the closed-set/open-set correlation in the ‘Discussion’ of Sec 3.1 of the main paper and, particularly, in Appendix B. Specifically, we suggest that:\n - Taking results from the model robustness literature, classifiers which have a lower generalization error are likely to be better calibrated and hence be better OSR detectors (Sec 3.1).\n - Stronger cross-entropy classifiers map ‘seen’ class points further from the origin than weak classifiers, while mapping ‘unseen’ points near the origin. Thus, stronger closed-set classifiers have a stronger signal for the OSR decision (Appendix B).\n\n>“Please consider including CIFAR100 (close set) vs CIFAR10 (open set)”[HAFU] \\\n“the proposed cross-entropy baseline should evaluated also on out-of-distribution settings”[dw7j]: \n\nExperiments on an OoD benchmark were included in Appendix F (specifically, training on CIFAR100 as ‘in-distribution’ and testing on other datasets as ‘out-of-distribution’). We found that taking a strong closed-set classifier trained on CIFAR100 could outperform a common OoD baseline (Outlier Exposure). In general, however, it is difficult to perform comparisons between the fields as (unlike OSR) OoD allows access to extra data as examples of ‘OoD’ during training. We highlight this in our Related Work section (Sec. 2). \n\nWe have also expanded on this analysis in the updated manuscript. Notably, we have conducted further experiments in the OoD setting, conducting similar tests to those in Fig. 3b of the paper. Specifically, on the CIFAR100 → CIFAR10 OoD experiment, we find a correlation between the accuracy and OoD performance of a number of ResNet models. For ResNet-(20,32,44,56), we find a Pearson correlation coefficient of 0.97 between their closed-set accuracy and OoD detection performance. This plot is now included in Appendix F (Fig 8).\n'}, {'summary_of_the_paper': 'This paper makes an observation that good representations for open set detection would be correlated with high closed set accuracy.\nTo validate and illustrate this observation they improve closed set accuracy on existing open set benchmarks with different architectures and demonstrate  improvement in open set detection performance. \nThis paper also proposes new benchmarks for openest detection with fine grain details, which haven’t been studied in the setting of openest detection.', 'main_review': 'The core technical contribution or claim of this paper is that closed set accuracy is important for openest detection. This is known for researchers working on openest detection, though not highlighted enough. For any representation one key challenge of open set detection is in distinguishing confusing instances (incorrect predictions) within closed set vs novel category instances along the decision boundary. So as the quality of closed set classification improves this overlap could decrease and hence would enable easier detection of open-set instances.\n\n[1] makes similar claims that vision transformers result in better open-set detection without complex detection mechanisms. [3],[4] highlight the fact that good representation is all you need for meta learning or out of distribution generalization which though orthogonal to open-set detection, informs the research community that ‘quality of representation & performance on closed set’ is useful for auxiliary and relevant tasks of OOD, few-shot learning.  \n\nSimilar to the observation of this paper of closed set accuracy, many works in ‘openworld’ competition [5] at CVPR has taken various strategies to improve closed set accuracy in pre-training phase to improve closed set accuracy and leverage it for open-set performance boost.  \nAnother key contribution of this paper is proposing large scale ImageNet benchmark, [5] is a CVPR’21 competition and existing benchmark along the similar lines which unfortunately makes benchmarking contribution not significant enough. \n\nStrengths:\n\nThis paper focuses on an important problem and highlights an observation and it’s simplicity is especially relevant for practical settings and safety-critical applications.\nThis paper is well written and has great empirical evaluation methodology and demonstrate SOTA compared to other methods with additional components.\nThis paper proposes additional benchmarks for openset detection leveraging fine-grained classification datasets, and this is adds a valuable dimension to openset benchmarks.\n\nWeakness:\n\nThe core technical contribution(s) does not seem convincing to warranty a conference acceptance, especially given previous works making similar conclusions and contributions. \nThis lacks sufficient commentary on why a good closed set accuracy would result in better open-set detection.\n\n\nSuggestion to Authors:\n        Visualization of representations (albeit noisy) with and without augmentations/modified training could be insightful. May be including few examples of how hard novel instances/samples map onto different representations.\n\tFor completeness please consider including CIFAR100 (close set) vs CIFAR10(open set), as the number of closed set classes increase the task of openset detection becomes more challenging, this version of experiment is included in [2] which this paper includes as baseline and [1].\n\nReferences:\n\n[1]  Exploring the Limits of Out-of-Distribution Detection. http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-001.pdf\n \n[2] H. Zhang et al. Hybrid Models for Open Set Recognition https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480103.pdf \n\n[3] Y. Tian et al. Rethinking Few-shot Image Classification: A Good Embedding is All You Need?  \nhttps://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf\n\n[4] J. Miller et al. Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. https://proceedings.mlr.press/v139/miller21b/miller21b.pdf\n\n[5] Open World Vision CVPR’21 Competition and workshop https://www.cs.cmu.edu/~shuk/open-world-vision.html, https://eval.ai/web/challenges/challenge-page/1041/overview \n', 'summary_of_the_review': 'This paper focuses on an important problem of open-set detection and demonstrate that a good closed-set representation is a very strong baseline and competitive with more complex methods for open-set detection, which is valuable for many practical settings.\nThough empirical evaluation is comprehensive and illustrate performance on large scale benchmarks, given existing prior works novelty of contributions seems rather limited', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper first analyzed the correlation between closed set classifier and open set classifier. Then based on this finding, cross entropy closed set classifier is enhanced with a few recent accuracy improvement methods, then it is transferred to a open set classifier and achieve good performance. And also experimentation is conducted to compare with other SOTA close set classifiers, and that the proposed enhanced OSR classifier based on cross entropy baseline can achieve similar accuracy. Finally a new benchmark dataset is generated. ', 'main_review': 'This paper is dealing with the open set recognition problem which is a challenging research problem. Based on some findings about the correlation of closed set recognizer and open set recognizer, this paper found a way to achieve a strong OSR through an enhanced close set recognizer. This is a valuable finding for future open set recognition research. \nThe weakness of this paper is that there are not more in depth analysis the behind reason for these findings, thus the value of this paper is not that significant. So this is possibly a direction to make this paper even stronger.', 'summary_of_the_review': 'This paper proposed interesting finding about the correlation between closed set classifier and open set classifier, and leveraging this finding, a new SOTA open set classifier is generated from a baseline classifier, which has similar performance as  other SOTA close set classifiers. Also a new benchmark dataset is generated.\nOverall it is a valuable paper, which is clearly written, and with good experimentation results to support. The weakness of this paper is lacking of more in depth analysis, and providing more insights to the behind rationale, which lower the value of this paper.  \nSo overall I will recommend marginal acceptance of this paper. \n ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper considers the problem of open-set recognition, in which a visual classifier should be able to distinguish images of the trained categories from images of other different categories.\nThe authors show that there is a strong correlation between results on open-set scenario and the close set-scenario (the classic problem in which the model is trained and tested on the same semantic categories). Then, they show that a baseline based on ranking the logits of a model trained on standard cross-entropy training can be very competitive with more complex methods when trained with strong data augmentation and other improvements. \nFinally, they propose new benchmarks for open-set recognition emphasising the fact that in contrast to other similar tasks, in open set scenario, the images that should be classified as other classes not seen during training should belong to a different semantic category, and not to other kind of distributional shifts.\nThe take home message of this paper is that at this stage of research for improving open-set recognition, the best that we can do (or almost) is to use stronger models for close-set recognition (as the title suggests).', 'main_review': 'Strengths:\n- The paper is well written, clear and straight to the point.\n- The appendix add more experiments and details to further understand the main paper statements.\n- Up to my knowledge the most important references in the field have been considered\n- The paper belongs to the category of very important papers that show that sometimes in research we consider very complex solutions, whereas a properly tuned standard approach can do the job as well.\n- The authors propose a clear distinction between open-set recognition (in which the none-of-the above class comes form well defined classes) and out-of-distribution (in which the none-of-the-above can be any kind of image). It is important to clarify the different tasks.\n- It is interesting to see that in new and not overfitted datasets, cross-entropy baseline seems to perform better than the best method for open-set scenario. \n\nWeaknesses:\n- Authors compare with only ARPL and OpenHybrid approaches because they say that the other approaches perform lower on open-set benchmarks. I think it is still important to consider also the other methods especially because the authors propose new benchmarks and on these benchmarks the ranking between the methods changes. So it could be for the other methods. I see that a more complete evaluation is performed in the appendix E (Table 6), however this is not done for the new datasets (Table 3).\n- In Fig.2 I would expect to see also results for OpenHybrid. Also, in the figure, for the simplest datasets (with performance close to 100) there is not much correlation between open-set can close-set performance as ARPL does not improve over cross-entropy on the close-set scenario. It is something that should be mentioned.\n- Results in Table 1 for the cross-entropy are presented with a ranking based on the classifier logits, which in my opinion makes more sense. However, as previous works use the softmax scores, it is important to compare the two ways. I see a comparison in Appendix B (Fig. 6c), but this is only for a model. I would like to see this comparison at least for the models in tab. 1.\n\nAdditional questions/comments:\n- Fig.1a presents ARPL in the legend, without really having yet introduced the method.\n- The authors propose a clear distinction between open-set recognition and out-of-distribution. However, is this distinction really necessary. Could not the two fields be unified? It would be more interesting to know if the same conclusions of this paper are valid also for out-of-distribution problems.\n- The ""more theoretical"" justification of the results at bottom of page 4 does not add much, but I do not have suggestions on how to improve it.\n- In Fig.3 it is interesting to see that ViT seems to have a better generalization to the open-set scenario on ImageNet. Is it due to the fact that it has been trained on more data or it is the reduced inductive bias (no convolutions)? It would be interesting to see if with different sizes of ViT, the correlation between open and close-set scenario still holds.', 'summary_of_the_review': 'The paper is well written and introduces interesting results that can change the understanding of the open-set recognition. Experimental results prove the main conclusion of the paper, but some additional experiments are needed (see above) to further understanding the problem. Also, in my understanding the distinction between open-set recognition and out-of-distribution is ficticial and the proposed cross-entropy baseline should evaluated also on out-of-distribution settings.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Open-Set Recognition: A Good Closed-Set Classifier is All You Need', 'authorids': ['~Sagar_Vaze1', '~Kai_Han1', '~Andrea_Vedaldi1', '~Andrew_Zisserman1'], 'authors': ['Sagar Vaze', 'Kai Han', 'Andrea Vedaldi', 'Andrew Zisserman'], 'keywords': ['open set recognition', 'image recognition', 'computer vision'], 'abstract': ""The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr_closed_set_all_you_need."", 'one-sentence_summary': 'We show that the baseline method for open-set recognition can achieve state-of-the-art performance and introduce new benchmark settings', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'vaze|openset_recognition_a_good_closedset_classifier_is_all_you_need', 'pdf': '/pdf/a9e422d293a936fe65575b5e1ea6a86549b84bca.pdf', 'supplementary_material': '/attachment/77b537399e49faa19e972ac00c03fc55fab82141.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/open-set-recognition-a-good-closed-set/code)', '_bibtex': '@inproceedings{\nvaze2022openset,\ntitle={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},\nauthor={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=5hLP5JY9S2d}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Pingchuan Ma', 'Tao Du', 'Joshua B Tenenbaum', 'Wojciech Matusik', 'Chuang Gan']",ICLR,RISP_ Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation,https://iclr.cc/virtual/2022/oral/6922,2022," This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=uSE03demja,https://openreview.net/forum?id=uSE03demja,uSE03demja,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""This paper proposes a method to solve the inverse problem of identifying parameters of a dynamic physical system from image observations. The main idea is to train a rendering-invariant state-prediction (RISP), which estimates the inverse mapping from the pixel to the state domain. The authors introduce a new loss to this end, and an efficient gradient computation of the loss.\n\nThe paper received three clear accept recommendations. The reviewers discussed the potential improvement of RISP when combined to disentanglement methods, and also raise several concerns regarding experiments, e.g. rendering conditions during training and testing, or evaluation on real data. The rebuttal did a good job in answering reviewers' concerns, and the reviewers especially appreciated the new results on real videos. Eventually, all reviewers recommended a clear acceptance of the paper.\n\nThe AC's own readings confirmed the reviewers' recommendations. The paper is introduces very solid contributions for solving the complex task of physical parameter identification in the unobservable setting. The paper is also clear and well written, and validated with convincing experimental results. Therefore, the AC recommends acceptance.""}, {'title': 'Response to reviewer paRJ', 'comment': 'Dear reviewer paRJ,\n\nThank you for your detailed comments. We are glad to see you appreciate our additional real-world experiment. We agree with the suggestion about the discussion to add and will incorporate it into our final manuscript.'}, {'title': 'Real-world experiment adds value', 'comment': 'Dear authors,\n\nThank you for addressing some of my (minor) concerns with this additional experiment. I believe this additional experiment brings in a lot more value to the paper, particularly as most work dealing with differentiable simulation has required precise dynamics and/or rendering configuration match. This portrays one advantage of RISP in being fairly robust to such configuration changes (while the current result is only qualitative, it is indeed a challenging setting).\n\nOne minor remark that could potentially be discussed in a revision: in typical (kinematic) control and planning settings, quadrotors end up being modeled as rigid bodies nonetheless, without considering propeller-blade effects and such. So I believe differentiable simulation can get around by making such an assumption too (which is vindicated in the presented qualitative result).'}, {'title': 'Response to Reviewer qnnY ', 'comment': 'We thank the reviewer for the suggestion.\n\n**Q1:** ""I expect the authors to evaluate the method on more datasets, especially real datasets.""\n\n**A1:** We agree that applying our approach to real-world problems is definitely an exciting direction. Therefore, we have added a real-world experiment that imitates a quadrotor\'s motion from a video clip. Please refer to our general response above [[response]](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw) and our website [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor) for more details.\n\n---\n\nReference links:\n\n- general response of the real-world experiment: [https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw)\n- website of the real-world experiment: [https://sites.google.com/view/risp-iclr-2022/real-quadrotor](https://sites.google.com/view/risp-iclr-2022/real-quadrotor)'}, {'title': 'Response to Reviewer UwVk', 'comment': 'We thank the reviewer for the valuable feedback and positive comments. We are pleasant that the reviewer found the paper to be novel and interesting.\n\n**Q1:** ""It would be interesting to see results when the train and test rendering parameter distributions are different.""\n\n**A1:** This is a brilliant question indicating the key problem RISP attempts to solve: adaptation between various rendering configurations. The rendering configurations in our training and test environments are indeed from different distributions. In fact, some of the material models we used in our test environment (pbrt) are not even available in our training environment (mitsuba). Our real-world experiment [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor) is another example of using rendering configurations from different distributions, and RISP still achieves reasonable performance.\n\n---\n\n**Q2:** ""It would be interesting to see how the various methods would perform on real videos.""\n\n**A2:** Thank you for this suggestion. We have provided a new real-world experiment. Please find its details in our general response above [[response]](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw) and in our website [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor).\n\n---\n\nReference links:\n\n- general response of the real-world experiment: [https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw)\n- website of the real-world experiment: [https://sites.google.com/view/risp-iclr-2022/real-quadrotor](https://sites.google.com/view/risp-iclr-2022/real-quadrotor)'}, {'title': 'Response to Reviewer paRJ ', 'comment': 'We thank the reviewer for providing additional clarifications. Below we provide a detailed response to that as well as other concerns.\n\n**Q1:** ""Will RISP generalize to scenarios where the underlying dynamics change too?"" (**W1** in the review)\n\n**A1:** This is a great point that our new real-world experiment [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor) can help answer. In that experiment, we not only had no access to the ground-truth system parameters (we used publicly available empirical numbers) but also omitted many real-world quadrotor dynamics in our simulation, e.g., ground effects, motor responses, propeller dynamics, and battery models. Still, our method reconstructed similar motions from the real-world video, showing its potential generalizability among different dynamic models.\n\n---\n\n**Q2:** ""how dense (in terms of time) the loss needs to be."" (**W2** in the review)\n\n**A2:** We thank the reviewer for the clarification, and we agree that this ablation study helps better understand how RISP works. We added this ablation study in our appendix using a setup similar to Fig. 12 in the $\\nabla$Sim paper. Specifically, we use the system identification task in our **rod** environment and present the loss-vs-Young\'s modulus curves with various temporal sampling rates. Note that unlike in $\\nabla$Sim, the loss is now computed through our RISP network. The figure [[figure]](https://drive.google.com/file/d/1TNe8z3sXVvGlb4QZL_1qoBv__HbtXXSr/view?usp=sharing) shows that changes in temporal sampling rate do not seem to affect the loss landscape substantially: most of these curves, except for the first-and-last-frames one, are unimodal functions with one local minimum. Therefore, we can expect RISP plus a standard gradient-based optimizer to succeed under various temporal sampling rates.\n\n---\n\n**Q3:** ""Disentanglement/Compositionality of the learned RISP"" (**W3** in the review)\n\n**A3:** We agree that it would be exciting to introduce disentanglement or compositionality into RISP for better explainability. For example, in the training phase, one might introduce Variational Information Maximization [Barber et al., 2004] into the framework to encourage spontaneous interpretation. Additionally, to analyze a trained RISP, a clustering algorithm might be helpful to understand any emerging behaviors behind the representation. These enhancements and analyses are definitely excellent additions to RISP, which we will leave as future work.\n\n---\n\n**Q4:** ""the paper will be well-rounded if the limitations of the work are discussed upfront.""\n\n**A4:** Thank you for the suggestion. We plan to add a limitation section in our manuscript, which we briefly summarize below:\n\n- We require knowledge of the intrinsic and extrinsic parameters of the camera, which is not always accessible in a real-world scenario. We expect this limitation can be resolved easily by incorporating in RISP the gradients for camera parameters, which are available in some modern differentiable renderers.\n- We also require a moderately accurate object geometry. A potential direction is to combine recent advances in Neural Radiance Fields (NeRF) with our approach and infer the geometry without omniscient information of cameras.\n- We require a differentiable simulator that can capture the dynamic model of the object. Such a differentiable simulator may not be available for complex, real-world scenes with intricate dynamics, e.g., fluidic systems, deformable objects with rich contact, or multi-physics systems.\n\n---\n\nReference:\n\n- [Barber et al., 2004] D. Barber and F. V. Agakov, ""The IM algorithm: A variational approach to information maximization"" in NIPS, 2003.\n\nReference Links:\n\n- website of the real-world experiment: [https://sites.google.com/view/risp-iclr-2022/real-quadrotor](https://sites.google.com/view/risp-iclr-2022/real-quadrotor)\n- figure in A2: [https://drive.google.com/file/d/1TNe8z3sXVvGlb4QZL_1qoBv__HbtXXSr/view?usp=sharing](https://drive.google.com/file/d/1TNe8z3sXVvGlb4QZL_1qoBv__HbtXXSr/view?usp=sharing)'}, {'title': 'General Response', 'comment': 'Dear reviewers,\n\nWe thank all reviewers for their feedback and comments on our manuscript. We are glad to see that the reviewers have found that\n\n- the problem we propose to solve is challenging and crucial (reviewer paRJ, qnnY),\n- our idea is novel and neat (reviewer UwVk, qnnY),\n- the choice of baselines is appropriate (reviewer UwVk),\n- our method achieves strong performance (reviewer paRJ, UwVk, qnnY),\n- our manuscript is well-written (reviewer paRJ, UwVk, qnnY).\n\nTo address concerns raised by the reviewers, we have made the following changes in our work and updated the manuscript (textual revisions marked in red) accordingly:\n\n**Additional experiments**\n\n- We added a real-world experiment [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor) that imitates the trajectory of a quadrotor from a video, which we will elaborate in the next post [[response]](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw) (reviewer paRJ, UwVk, qnnY).\n- We added an ablation study on the influence of temporal sampling rate (reviewer paRJ).\n\n**Clarifications and discussions**\n\n- We discussed the impact of dynamic model discrepancies, the disentanglement/compositionality of RISP, and the limitations of our method (reviewer paRJ).\n- We clarified the differences between the training and testing environments (reviewer UwVk).\n\nWe hope our responses have addressed all concerns from the reviewers adequately. We thank all reviewers again for their time and feedback, and please feel free to let us know if there are other clarifications or experiments we can offer. We would really appreciate it if the reviewers could consider raising their scores after evaluating our updates.\n\n---\n\nReference link:\n\n- general response of the real-world experiment: [https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw](https://openreview.net/forum?id=uSE03demja&noteId=C9ZB2AOsUrw)\n- website of the real-world experiment: [https://sites.google.com/view/risp-iclr-2022/real-quadrotor](https://sites.google.com/view/risp-iclr-2022/real-quadrotor)'}, {'title': 'A Real-World Experiment', 'comment': ""**Problem setup**\n\nWe consider imitating a quadrotor's motion from a real-world video clip.\n\n- **Input:** A real-world quadrotor video clip, intrinsic and extrinsic parameters of the camera, empirical values of the quadrotor's system parameters (obtained from the vendor's website), and the geometry of the quadrotor represented as a mesh file.\n- **Output:** An action sequence so that when applied to the quadrotor in simulation, we expect its motion to resemble the motion in the input video as closely as possible.\n\n---\n\n**Challenges**\n\nOur task requires matching information between simulation and reality, a challenging open problem in computer vision, robotics, and graphics. Therefore, solving this task perfectly would require tremendous efforts:\n\n- The real-world quadrotor contains hard-to-measure parameters and hard-to-model dynamics, e.g., ground effects, motor responses, propeller dynamics, and battery models.\n- A real-world video typically contains complex textures, materials, and lighting conditions, requiring substantial manual work to reconstruct them in a realistic renderer.\n- The quadrotor's sensing, control, and actuation modules are often polluted by environmental noises, which are challenging to model and infer in a simulated environment.\n\nDespite these challenges, our method achieved a qualitatively good result with a standard differentiable rigid-body simulator and renderer, showing its generalizability across moderate discrepancies in dynamic models and rendering configurations.\n\n---\n\n**Method**\n\n- **Video recording and pre-processing:** We flew a quadrotor in an indoor environment with a clean background and clear lighting. We recorded its motion using a static camera whose parameters were calibrated beforehand. We removed the takeoff and landing motions from the beginning and end of the video and downsampled the video to 360x640 resolution at 10 FPS. The final video is 14.4 seconds long.\n- **Training the RISP network:** We trained the network the same way as the virtual **quadrotor** example described in our manuscript before.\n- **Solving the task:** We first ran RISP on the input video to generate a target trajectory of the quadrotor. Next, we minimized a loss function defined as the difference between the target trajectory and a simulated trajectory from a differentiable quadrotor simulator, with the action at each video frame as the decision variables. Because this optimization involves many degrees of freedom and a long time horizon, we found a good initial guess crucial. We initialized the action sequence using the output of a handcrafted controller that attempts to follow the target trajectory. Such an initial guess ensures the quadrotor stays in the camera's view and is used by our method and the baselines.\n\n---\n\n**Results**\n\nWe show three results on our website [[website]](https://sites.google.com/view/risp-iclr-2022/real-quadrotor): one from our method (**RISP**) and two from the state-of-the-art approach with variations (**GradSim** and **GradSim-Enhanced**). Each result is a side-by-side video comparison between the reconstructed motion and the input video.\n\n- **RISP:** Our method reconstructs a similar dynamic motion without knowing the rendering configuration and the exact dynamic model in the input video.\n- **GradSim:** The setup is identical to **RISP** except that the loss function is replaced with the pixel-wise difference defined in the original $\\nabla$Sim paper. The reconstructed motion fails to resemble the motion in the input video. We believe one major reason is that **GradSim** assumes the rendering configuration in the target domain is known. However, such information is generally difficult to access from a real-world video.\n- **GradSim-Enhanced:** We improved the performance of **GradSim** by 1) manually tuning the renderer's configuration to match the video input as closely as possible and 2) providing a good initial guess of the action sequence computed based on the ground-truth trajectory recorded from a motion capture system. Note that such a good initial guess from motion capture data is not used in **RISP** and **GradSim** results above and is generally inaccessible in real-world applications. With this additional help, **GradSim-Enhanced** manages to mimic the motion at the beginning of the video but still fails to replicate the full trajectory reliably.\n\nWe believe that RISP's encouraging result shows the potential of applying differentiable physics plus differentiable rendering techniques in real-world applications. We refer the reviewers to our appendix for a full description of our results and the details of our method in this real-world experiment.\n\n---\n\nReference link:\n\n- website of the real-world experiment: [https://sites.google.com/view/risp-iclr-2022/real-quadrotor](https://sites.google.com/view/risp-iclr-2022/real-quadrotor)""}, {'title': 'Clarification on W2', 'comment': 'With **W2**, I am trying to get a sense of how dense (in terms of time) the loss needs to be. For instance, if one were to use only a sparse subset of (time-synchronized) frames from within a trajectory (in the limiting case, a single image as used in $\\nabla$Sim), it would be interesting to note the impact on performance. For dense pixelwise MSE losses, computing these quantities over an entire trajectory offers worse performance as opposed to computing MSE over just the ""first and last"" frames, as reported in $\\nabla$Sim.'}, {'title': 'Looking for clarification', 'comment': 'Thank you very much for your positive comments and constructive suggestions. Could you please clarify **W2**? If you question whether all baselines use full trajectory for the sysid and imitation learning tasks, the answer is yes. We believe this is a pretty common setting and also consistent with $\\nabla$Sim. Please do not hesitate to contact us if there is any additional clarification or experiment we can offer. Thank you.'}, {'summary_of_the_paper': 'This paper proposes a general approach to leveraging differentiable simulator for the downstream tasks of system identification and visuomotor control WITHOUT requiring access to the true underlying states. They do so by predicting a ""rendering-invariant state"" that results in a much stabler loss landscape and reduces domain gap / mismatch. Experiments over 4 environments indicate the merits of this approach over current state-of-the-art.', 'main_review': ""### Strengths\n\n* **S1** This paper tackles the challenging (inverse) problem of directly recovering object properties (system identification) or control parameters (visuomotor control) from image/video observations. The current best approach to this ($\\nabla$Sim) proposes to compose differentiable physics simulation and differentiable rendering to result in a computation graph where ground-truth physical parameters are no longer required. However, this approach has a crucial shortcoming: it works well only when the reference and target trajectories are drawn from the same distribution (i.e., identical physics and rendering engines used across both). This work proposes RISP to bridge that gap (bridging this gap is crucial to enable several downstream, including real-world, applications).\n\n* **S2** The paper is very well-presented. The core ideas are easy to follow, and the rationale for incorporating the rendering gradients into a regularization term is well laid-out. Of the three contributions, I believe this to be the more significant one (as also corroborated by Fig. in section 4.6)\n\n* **S3** This approach (RISP) demonstrates strong performance over current art -- this is over multiple environment settings (rigid, articulated, deformable object identification; control).\n\nIn general, this is well-executed work and I would as such recommend acceptance. However, there are a few issues I hope to see discusses/addressed over the rebuttal phase.\n\n### Weaknesses\n\n* **W1** Clarifying the impact of differentiable physics simulation vs rendering: Are the train sets generated using a different differentiable physics engine? (the manuscript mentions they're generated using a different rendering engine -- but, arguably, generating them using a different physics engine as well would create a wider domain gap) This also raises several interesting questions such as: will RISP generalize to scenarios where the underlying dynamics change too? If not, are there other domain randomization / regularization methods that can assist?\n\n* **W2** Baseline choices: Do all baselines use all frames in the reference trajectory to compute the loss. Just as dense pixelwise loss baselines are ablated upon by bringing in other baselines that treat the image as a whole in loss computation; a similar analogy may be drawn at the sequence level (i.e., computing the full pixelwise loss across all frames in a sequence, vs. using pixelwise loss across select frames in a sequence)\n\n* **W3** Disentanglement/Compositionality of the learned RISP: The current training strategies do not seem to explicity focus on disentangling state representations or improving compositionality of the learned representation. (This seems clearly out of scope for the current work, but I'd like to bring this discusison point here to perhaps prompt addition of clarifying statements in the manuscript). Does this have an imact on e.g., where the object may lie within an image, and perhaps impact the extension of RISP to simulations with multiple objects?\n\n\n### Minor comments\n\nThese comments are nitpicks/typos and are easily addressed in a minor revision. The authors needn’t respond to these\n\n* “Articulate-body” -> articulated body\n* “Simulates” -> simulate"", 'summary_of_the_review': 'This is a well-written paper describing an idea of substantial interest to the physical reasoning and the visual reasoning communitties. I only have clarifying concerns with this work and feel the paper will be well-rounded if the limitations of the work are discussed upfront.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper focuses on the problem of estimating dynamic parameters of a physical system from videos under unknown rendering conditions.\nIt presents a novel idea of using a rendering-invariant state-prediction (RISP) network that predicts the state from a rendered image and can be integrated into a framework with a differentiable simulation and rendering engine for parameter estimation. Training this network is done using domain randomization technique with synthetic data. Additionally, the paper introduces a novel gradient loss that further pushes the network to be invariant to rendering parameters.', 'main_review': 'Strengths:\n\n- The paper is very well written and contributions are clear.\n- The idea of using a rendering-invariant state-prediction as a pre-step before the gradsim-like framework is novel and sensible.\n- The idea of the using the gradient with respect to the rendering parameters as a regularization is novel and interesting. It could additionally be very used in many other fields where training on synthetic data is used with random rendering parameters.\n- The Combination of weak/strong/oracle baselines chosen in the experiments are very appropriate for comparison.\n- The paper performs a large range of experiments using several environment and tasks to study the effectiveness of the proposed method compared to baselines.\n- The ablation study shows the impact of the gradient loss on the state prediction model performance.\n\nWeaknesses:\n\n- It is not clear if the training and testing rendering environments are the same. This means that the parameter distribution in both training (for the RISP) and testing cases come from the same distribution. In reality, often the test parameter distribution comes from a different distribution. It would be interesting to see results when the train and test rendering parameter distributions are different.\n- It would be interesting to see how the various methods would perform on real videos.', 'summary_of_the_review': 'The paper presents mainly two new ideas, the RISP network for state estimation and the gradient loss for regularization. Both are novel and can be interesting and impactful for the community and push forward the state-of-the-art in using synthetic data for parameter estimation. The quantitative and qualitative experimental results on synthetic data show clear improvements compared to the previous methods.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper presents a new method of predicting physics simulation parameters and rendering configurations from an RGB video. Unlike previous methods that calculate the loss function in image space, this work proposes to calculate in the simulation state space to avoid the issues (e.g. being stuck in a local minimum) when the reference video is very different from the generated video. Specifically, a rendering-invariant state-prediction (RISP) network is pretrained with the generated data from a differentiable renderer under various rendering conditions.  The RIST network is then appended to the output of a differentiable renderer to make the pipeline from simulation and rendering parameters to states predicted from images fully differentiable. Besides a state prediction loss term for training this pipeline, a novel regularization term is used to enforce the RISP network rendering invariant. In addition, a new training strategy to efficiently calculate the gradient for the loss function is proposed. The experiments have shown that the proposed method significantly outperforms the state-of-the-arts and the proposed components contribute to the final results with big improvements. ', 'main_review': 'This work addressed a crutial problem. The proposed method is novel and neat. The paper is well-written. I like the main idea and technically novel components proposed, such as the rendering invariant regularization term and efficiently calculating the second-order gradient. The results have demonstrated the importance of these components and the improvements over previous methods. \nFor the weakness of this paper, it is not clear how this method works on real datasets. I suggest that the proposed method be evaluated on more datasets, especially real datasets. ', 'summary_of_the_review': 'This paper is a solid submission, which solves an important problem and proposes some novel ideas. I expect the authors to evaluate the method on more datasets, especially real datasets. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation', 'authorids': ['~Pingchuan_Ma3', '~Tao_Du1', '~Joshua_B._Tenenbaum1', '~Wojciech_Matusik2', '~Chuang_Gan1'], 'authors': ['Pingchuan Ma', 'Tao Du', 'Joshua B. Tenenbaum', 'Wojciech Matusik', 'Chuang Gan'], 'keywords': ['differentiable rendering', 'differentiable simulation', 'system identification'], 'abstract': ""This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'ma|risp_renderinginvariant_state_predictor_with_differentiable_simulation_and_rendering_for_crossdomain_parameter_estimation', 'pdf': '/pdf/999353870633727a2d50bc5b4ee873b50401eba7.pdf', 'one-sentence_summary': ""We propose a novel approach to address the problem of identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible."", '_bibtex': '@inproceedings{\nma2022risp,\ntitle={{RISP}: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation},\nauthor={Pingchuan Ma and Tao Du and Joshua B. Tenenbaum and Wojciech Matusik and Chuang Gan},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=uSE03demja}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Chulhee Yun', 'Shashank Rajput', 'Suvrit Sra']",ICLR,Minibatch vs Local SGD with Shuffling_ Tight Convergence Bounds and Beyond,https://iclr.cc/virtual/2022/oral/7031,2022," In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-Łojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=LdlwbBP2mlq,https://openreview.net/forum?id=LdlwbBP2mlq,LdlwbBP2mlq,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper analyzes local SGD under the random reshuffling data selection setting. As is the case for standard random reshuffling, better rates are shown for local SGD when random reshuffling is used. This would already be a nice contribution to a line of work on random shuffling methods—but the paper goes beyond that by showing a matching lower bound and designing a (theoretically) better variant algorithm. The reviewers were all in agreement that this paper should be accepted (as a result not much further discussion happened after the original reviews), and I agree with this consensus. The modification seems to improve the paper, although I did not look through it in detail.'}, {'title': 'Response to authors', 'comment': 'I thank the authors for clarifying the questions. They have improved the paper according to the comments and suggestions provided by all reviewers. I believe that this paper has good quality and it deserves high evaluation. I would like to keep my evaluation unchanged, which is ""8: accept, good paper"". '}, {'title': 'Response to Reviewer 6QN8', 'comment': 'We sincerely appreciate the reviewer for the thoughtful feedback. Below, we reply to the concerns and questions raised by the reviewer.\n\n1. Assumption 2: Is it not too strict to have a uniform bound for this difference?\n- For Assumption 2 to hold with small $\\nu$ in a real distributed learning scenario, each local machine should have local data points that are similar to each other; for example, a device having many dog images, another having many cat images, etc. To allow devices to have diverse images, we would have to make $\\nu$ larger.\n\n- As we briefly discussed in Appendix A, we agree that the uniform bound required by Assumption 2 is stronger than the common ""bounded variance"" assumption, i.e., for all $x \\in \\mathbb R^d$ and $m \\in [M]$, $\\frac{1}{N}\\sum_{i=1}^N || \\nabla f^m_i(x) - \\nabla F^m(x) ||^2 \\leq \\sigma^2$. In many existing results, this assumption is employed to prove in-expectation convergence bounds. In contrast, our assumption requires that the deviation for each $i \\in [N]$, not the average, is bounded by a constant. However, we would like to emphasize that we use this stronger assumption to prove *with-high-probability* upper bounds, which is a departure from the in-expectation bounds in the literature. In a nutshell, we make a stronger assumption to prove stronger results. \n\n- In our revision, we also added Appendix D.7 to discuss how to circumvent the uniform bound over the whole $\\mathbb R^d$. There, we show that by modifying the theorem statements a bit, we can show the same bounds without the ""entire $\\mathbb R^d$"" requirement.\n\n2. Useful to have a table with a comparison with rates of other methods.\n- In our initial version of Appendix A, we had already presented comparisons to existing assumptions and convergence results, mainly focusing on with-replacement local SGD. If the paper gets accepted, in the camera-ready version we will update Appendix A with up-to-date results and also a summary table. Thank you for the helpful suggestion.\n\n3. Comparison with results from https://arxiv.org/pdf/2102.06704.pdf.\n- Thank you for kindly linking the paper (Mishchenko et al. (2021)). In our initial version, we had already discussed and compared our results against this paper. Please see the discussion after Proposition 5 in Section 4.2. Also, the last part of Appendix A provides further details.\n\n4. Simple and small experiments on toy models.\n- As per the reviewers\' suggestions, we performed numerical experiments to evaluate the performance of the algorithms and support our theoretical findings. Please see Appendix C of the updated manuscript. We also believe that the paper will benefit from additional real data experiments. Since these experiments are a bit more time-consuming to run, we plan to add them into the camera-ready version of the document if the paper gets accepted.\n\n5. The synchronized shuffling method is not clearly described. \n- We agree that the notation is not easy to digest at first glance. In order to provide clearer motivation and illustration for the technique, we newly added Appendix B in the revised version. Please see if the description in Appendix B clarifies your concerns.\n\nWe appreciate the reviewer again for the insightful comments and questions. Please let us know if you have any remaining questions/comments.\n\nBest,\n\nAuthors\n'}, {'title': 'Response to Reviewer 9cRr', 'comment': 'We thank the reviewer for the detailed review and insightful comments. Thank you also for pointing out the strengths of our paper. Below, we address the comments raised by the reviewer:\n\n1. The gap of $\\kappa^2$ between upper bounds and lower bounds.\n- We agree that the $\\kappa^2$ gap is important, and we believe that closing this gap is an important future direction. We tried to be clear about the gap in the beginning of Section 4 and also in the discussion paragraphs after Theorems 3 and 4. In the revision, we have added further clarifications to Section 1.1 that our bounds are loose by $\\kappa^2$.\n\n2. The lower bounds and upper bounds are in different notions of convergence.\n- The high-probability upper bounds shown in the paper can be used to prove matching in-expectation bounds. The upper bounds we prove are hence stronger than in-expectation bounds common in the literature. In the revision, we added a proof of this extension at the end of Appendix D.2.\n\n3. Dependence of $c_1$, $c_2$, and $\\kappa$ in Theorem 3.\n- We believe there may be a slight confusion here. Allow us to first clarify on the ""dependence"": the constants $c_1$ and $c_2$ as well as $c_3$ and $c_4$ that appear in Theorem 4 are numerical constants that do *not* depend on any problem parameters. In Theorem 3, the condition number $\\kappa$ and $c_1$ are indeed related by the requirement $\\kappa \\geq c_1$. This means that the theorem holds for the function class $\\mathcal F_{\\rm cmp} (L, \\mu, \\nu, 0)$ if the ratio between $L$ and $\\mu$ is above a certain numerical threshold. In other words, for our theorem to hold, the function class must have large enough $\\kappa$ to allow sufficiently ill-conditioned functions. Please let us know if this clarifies your question; we are happy to elaborate more.\n\n4. The assumption $K \\ge MB/N$ in Theorem 4 and its relation to parameter regimes.\n- The requirement $K \\ge MB/N$ we put is to make sure that the second term in the large-epoch bound, $\\frac{\\nu^2B}{\\mu N^2 K^2}$, is smaller than the small-epoch bound $\\frac{\\nu^2}{\\mu MNK}$. Therefore, after making the assumption explicit, the two ""if"" cases in Eq (8) become $K < \\max ( c_4 \\kappa, \\frac{MB}{N} )$ and $K \\geq \\max ( c_4 \\kappa, \\frac{MB}{N} )$. \nTherefore, if $\\kappa \\gtrsim \\frac{MB}{N}$, then the ""phase transition"" threshold in Eq (8) becomes $\\Theta(\\kappa)$. In the revised version, we have updated the theorem statement as well as the discussion.\n\n5. Some experiments to validate the analysis. \n- As per the reviewers\' suggestions, we performed numerical experiments to evaluate the performance of the algorithms and support our theoretical findings. Please see Appendix C of the updated manuscript. We also believe that the paper will benefit from additional real data experiments. Since these experiments are a bit more time-consuming to run, we plan to add them into the camera-ready version of the document if the paper gets accepted.\n\nThank you again for the valuable feedback. Please let us know if you have any other comments/questions.\n\nBest,\n\nAuthors'}, {'title': 'Response to Reviewer dmha', 'comment': ""We thank the reviewer for their time and efforts, as well as their valuable comments. \n\nAs per the reviewers' suggestions, we performed numerical experiments to evaluate the performance of the algorithms and support our theoretical findings. Please see Appendix C of the updated manuscript. We also believe that the paper will benefit from additional real data experiments. Since these experiments are a bit more time-consuming to run, we plan to add them into the camera-ready version of the document if the paper gets accepted.\n\nAgain, we appreciate the reviewer for the feedback and the positive evaluation. Please let us know if you have any other comments/questions.\n\nBest,\n\nAuthors\n""}, {'title': 'Revision of paper', 'comment': 'Dear all,\n\nWe would like to announce that we made a revision to our submission, according to the reviewers\' valuable comments. We have marked newly added sentences and paragraphs in green.\n\nSome noteworthy changes include:\n- We added Appendix C with some simple experiments on the 7 algorithms (minibatch/local RR with and without synchronized shuffling, single-machine RR, with-replacement minibatch/local SGD) considered in this paper. We use the ""hard instance"" constructed in Theorems 3 and 4 to examine the convergence speed of these algorithms. We are excited to see that the experimental results align quite nicely with our theoretical findings.\n- We newly added Appendix B which contains a more detailed description of synchronized shuffling proposed in Section 5. We discuss why it is needed and illustrate how it works.\n- At the end of Appendix D.2 (previously B.2), we added a discussion on how we can use the high-probability upper bounds in our paper to prove matching in-expectation upper bounds.\n- We newly added Appendix D.7 to discuss ways we can prove our upper bounds without having to assume that Assumptions 2, 3, and 4 hold for the *entire* $\\mathbb R^d$ space. We hope that this can help alleviate concerns regarding our intra- and inter-machine deviation assumptions.\n\nWe also plan to add the following to the camera-ready version, if the paper gets accepted:\n- Additional experiments on real data.\n- Up-to-date coverage of existing results, including a summary table.\n\nThanks,\n\nAuthors'}, {'summary_of_the_paper': 'This paper proposes two variants of stochastic gradient algorithms without replacement. For smooth functions satisfying the PŁ condition, the authors showed that the proposed shuffling-based variants converge faster than their with-replacement counterparts (for the case with large number of epochs). Moreover, the authors showed that their provided convergence analysis is tight for the case with not large number of epochs. ', 'main_review': 'The majority of the manuscript is well-written and easy to understand. This paper is solely theoretical with promising guarantees. The authors showed that if $K \\geq c_2\\kappa$ where $c_2>0$ and $\\kappa = \\dfrac{L}{\\mu}$, then their convergence rates are faster that the with-replacement counterpart. However, for the ill-conditioned case, the number of epochs (theoretically) will be huge to satisfy the mentioned assumptions. \n\nIt would be ideal if the authors could also provide some numerical results to demonstrate how well their proposed methods performs in practice.\n\n', 'summary_of_the_review': 'This paper provides some promising theoretical results for the variants of stochastic gradient algorithms without replacement.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This work analyzes the convergence rate of local and mini-batch Random Reshuffling. For \\mu-PL and smooth objectives, It provides high probability upper bounds, and matching expected lower bounds, and a special variant of the random reshuffling that outperforms the previous tight bounds with \\sqrt{M} speed up in some regimes. \n', 'main_review': 'Strength. 1. The analyzed problems have significant importance in practice and lack a thorough theoretical understanding yet. 2. The technical contribution for analyzing gradient sampling with dependence is original, the technique for bounding gradient noise in local RR is involved and interesting. 3. The provided bounds are tight in some sense and variants with better performance (linear speedup) are analyzed. 4. The lower bound for mini batch RR in small-epoch regime is informative in that it concludes the weakness of mini batch RR for K \\lesssim \\kappa.\n\nSome other points. 1. There exists a gap of \\kappa^2 between upper bounds and lower bounds, which can be important given that \\kappa itself is relevant in the considered regime. It may help to explain more on parameter dependence when claiming tightness. 2. The lower bounds and upper bounds are in different notions of convergence, it may help understanding if there are some explanations there. 3. In theorem 3, it might be useful to explain on the connections among c_1, c_2 and \\kappa, does it affect the regime K \\ge c_2 \\kappa, if c_2 has dependence on \\kappa given that c_1 has dependence on \\kappa. 4. In theorem 4, it also assumes that K \\ge MB/N, does it has influence on the followed discussions on the choice of B. It seems to me the cutoff between parameter regimes is not very clear, could you explain more on it, like, similarly, does c_4 has relationship with \\kappa? 5. Although this is a theory paper, giving some experiments to validate the analysis especially for the discussions on different regimes would be appreciated. \n', 'summary_of_the_review': 'I think this is a good paper with solid theoretical analysis and contribution, the analyzed problem is of great importance and needs more thorough understanding like this work.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper studies two popular algorithms: local SGD and minibatch SGD using the data shuffling technique, which means sampling without replacement. Authors provide analysis under PL condition and show that in some cases these methods with shuffling outperform classical local SGD and minibatch SGD. Additionally, this paper provides lower bounds for minibatch RR and local RR in homogeneous and heterogeneous settings. At the end of the paper, a new technique of using synchronized permutations is proposed. In an almost homogeneous setting, this approach can improve rates.', 'main_review': ""The paper presents a deep theoretical study of popular-in-practice methods that utilize shuffling of the data points. This paper has a good structure, all sections are well-written and fully described. All assumptions and statements of theorems are clear and understandable. \n\nHowever, assumption 2 is questionable:\n\nAssumption 2 (Intra-machine deviation). There exists $\\nu \\geq 0$ such that for all $m \\in[M]$ and $i \\in[N]$,\n$$\n\\left\\|\\nabla f_{i}^{m}(\\boldsymbol{x})-\\nabla F^{m}(\\boldsymbol{x})\\right\\| \\leq \\nu, \\text { for all } \\boldsymbol{x} \\in \\mathbb{R}^{d}\n$$ \n\nDoes this assumption hold in practice? Is it not too strict to have a uniform bound for this difference? \n\nThe theory is clear and it is easy to follow the proofs. The theoretical part is solid and the importance and novelty of obtained results are significant. Upper and lower bounds explain methods' behaviors in detail. These results should be interesting for the optimization community and the machine learning community in general. However, it might be useful to have a table with a comparison with rates of other methods for Federated Learning. Also, it is interesting to compare these results with results from https://arxiv.org/pdf/2102.06704.pdf. In this paper, local methods with shuffling are also considered. \n\nThis paper does not have any experimental results. I understand that this work is theoretical, but simple and small experiments on toy models are desirable. It can be useful for readers to have graphical illustrations of methods' behavior. \n\nThe synchronized shuffling method is not clearly described. The notation $\\sigma_{k}^{m}(i):=\\sigma\\left(\\left(i+\\frac{N}{M} \\pi(m)\\right) \\bmod N\\right)$ is quite confusing. Can you explain why you shift the permutation that way?  Despite the main idea of this approach being understandable, some technical details are not easy to get. It might be useful to add some clarifications in this part. \n\n\n\n"", 'summary_of_the_review': 'This paper introduces a deep and wide theoretical study of permutation-based variants of local SGD and minibatch SGD. Authors provide upper bounds, as well as lower bounds, which makes the contribution solid. The novelty and tightness of results are significant, so the paper should be accepted. My grade is ""accept, good paper"". ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond', 'authorids': ['~Chulhee_Yun1', '~Shashank_Rajput1', '~Suvrit_Sra1'], 'authors': ['Chulhee Yun', 'Shashank Rajput', 'Suvrit Sra'], 'keywords': ['Local SGD', 'Minibatch SGD', 'Shuffling', 'Without-replacement', 'Convex Optimization', 'Stochastic Optimization', 'Federated Learning', 'Large Scale Learning', 'Distributed Learning'], 'abstract': 'In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-Łojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'yun|minibatch_vs_local_sgd_with_shuffling_tight_convergence_bounds_and_beyond', 'pdf': '/pdf/1669f6cc32c853b0d69068b7ed1a230ce3f321d0.pdf', 'one-sentence_summary': 'We provide tight upper and lower bounds on convergence rates of shuffling-based minibatch SGD and local SGD, and propose an algorithmic modification that improves convergence rates beyond our lower bounds.', 'supplementary_material': '/attachment/214714c214de248a5a59d905312cfe380de10aa4.zip', '_bibtex': '@inproceedings{\nyun2022minibatch,\ntitle={Minibatch vs Local {SGD} with Shuffling: Tight Convergence Bounds and Beyond},\nauthor={Chulhee Yun and Shashank Rajput and Suvrit Sra},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=LdlwbBP2mlq}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
['Alex Rogozhnikov'],ICLR,Einops_ Clear and Reliable Tensor Manipulations with Einstein-like Notation,https://iclr.cc/virtual/2022/oral/6603,2022," Tensor computations underlie modern scientific computing and deep learning.A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc.However, tensor operations in all frameworks follow the same paradigm.Recent neural network architectures demonstrate demand for higher expressiveness of tensor operations.The current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. Moreover, some commonly used operations do not provide sufficient checks and can break a tensor structure.These mistakes are elusive as no tools or tests can detect them.Independently, API discrepancies complicate code transfer between frameworks.We propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors.We implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.",Oral 2: AI applications,https://openreview.net/pdf?id=oapKSVM2bcj,https://openreview.net/forum?id=oapKSVM2bcj,oapKSVM2bcj,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'All reviewers agree that this paper is a useful and valuable contributions to ML engineering.\n - insightful analysis .. highly user friendly operator design\n - ""useful and I can see it having large adoption in the community of scientific computing"" ... ""\n - ""Personally I tend to buy these advantages of einops"" ... ""However, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design""\n - ""a useful and appealing new coding tool.""\n\nThe negative reviewers appear fixated on the (true) observation that the paper does not look like a conventional ICLR paper, thati it ""reads like a technical blog"", and ""lacks rigour"".\nI belive it is fair and measured to state that these reviews may be considered to exhibit aspects of gatekeeping: requiring more ""mathiness"" that does not help the paper, or more ""rigour"" through user studies that are in fact less valuable than the reviewers\' own observations ""I could see myself..."", ""I tend to buy..."".\n\nThis is a paper about design, not about models or algorithms (although the algorithmic work is good).  It is about the design of tools that we all use, and about the decisions and thought processes that led to that design.  A reviewer decries ""many non-rigorous claims"".  These are claims about the usability of existing systems, and mostly appear in the discussion and footnotes, as the authors note in rebuttal.  Of course, one could have run user studies to back up each claim, but I am just as convinced by the examples shown in the paper.  It matters not to me what some users corralled into a user study thought.  It matters what I and my colleagues will think, and I am now sure to recommend einops to colleagues.  I would not have met it had the paper not been submitted to ICLR, and hence I am certain it should be accepted, so more can see that we care not just about mathiness, but actually enabling progress in our field.\n\nThe job of a conference like ICLR is to expose researchers and practitioners in machine learning to ideas and techniques that may advance their research and practice.  Programming, and the translation of mathematical ideas to efficient computer code, are fundamental to all of machine learning, and hence programming models are very much suitable for presentation to an ICLR audience.  I am certain that this paper, and the technology it describes, are more important to ICLR readers than knowing that if module A is co-trained with module B, then combined with compression C, the SOTA on some arbitrary benchmark is increased by 0.31 +/- 1.04. \n\nReviewer gRMH says ""there is no code"", but the code has been in the open for three years; it is an accident of our misapplication of the principles of blind review that the reviewer felt they could not search for the code, and that the authors felt they could not bring to bear the evidence that three years of real-world usage have brought.  \n\nReviewers say the work is just an extension of einsum, while noting that the extension is useful and nontrivial.  Yes, it is an extension, and the paper\'s examples show how it yields more compact code that is also more readable and maintainable.\n\nI could add more examples, but in short, I tend to side with the authors\' response at almost every point.  At the same time, the final version of the paper has been strengthened by this dialectic, and I expect further strenghtening through exposure to the ICLR community.\n\nTo the authors: Listing 1 is useful, but should be in an appendix.  Instead, add examples of ellipses on P5, and show more inline examples in general.  The paper would be strengthened by another pass over the English -- after the decision is made I would be happy to volunteer to help.'}, {'title': 'Rebuttal revision', 'comment': ""Dear Reviewers and Area Chairs,\n\nThanks again for your constructive and specific feedback which helps us to improve clarity and presentation of the paper.\n\nWe have incorporated fruitful suggestions on improving the paper's text and included additional information requested by reviewers. The main changes made in the text are:\n- The formal grammar of proposed notation is given in Section 4 along with its detailed description and features\n- As the issue on relation between einops and einsum was raised by several reviewers, we add it in Sections 2 and 4 with detailed comparison in Appendix B\n- Due to space constraints we included a user study for the prototype stage of einops in Appendix C, which demonstrates intuitiveness and readability of the main design choice for the notation\n- Einops overhead on top of the DL frameworks is provided in Appendix F for 3 use cases and different input sizes\n- Details on caching mechanism and why it is important is given in Appendix G\n- We moved discussion of related work on labeled tensors approach and convolutions into Appendix A\n- We describe initial confirmation of einops applicability to a wide range of DL applications and architectures in Appendix H\n- Finally, validation of einops suitability by adoption is covered in Appendix I\n- We also revisited Discussion section \n- Multiple minor changes addressing reviewers concerns\n\nLet us know if you have any other questions. Thank you!\n\n""}, {'title': 'Response to reviewer gRMH', 'comment': 'Dear reviewer gRMH,\nThank you for your feedback and time. We would like to clarify some of your questions and points as follows:\n> JMLR OSS would be a better venue (due to code review).\n\nNotation is the core of our approach, and the main novelty. This is not the subject of JMLR OSS, but ICLR is the right venue.\nWe added the proper formal syntax and constraints for the notation in the revised version in Section 4 to make it clear that notation is the main novelty.\nOn the other hand, notation without implementation, and integration into existing software design stack is not a viable thing.\nWe made this (quite a significant bunch of) engineering work to make the notation practically useful and universal.\nWe do not see this a software (or mainly-software) contribution, as well as we don\'t see this as two independent contributions. \n\n\n> Some design choices could lead to unexpected problems when using the library, and the authors chose to dismiss them as a non-concern (which I don\'t believe counts as a real solution).\n\nIn the previous discussion:\n- We pointed to the correctness of the result.\n- We pointed to a remedy (type hinting). We can point to other (e.g. layers).\n- We pointed to an existing operation in TF (a@b@c, where ""a"" and ""b"" are numpy tensors, and then numpy multiplication will be used, not tf on first multiplication)  that has way higher computational overhead but not alarming anyone.\n- We reconfirm our assessment that this hypothetical situation is not an issue. \nPositive changes of backend-polymorphic code should not be ignored or omitted in this discussion.\n\n> The readability is not what is being discussed here.\n\nWe don\'t see discussion of code that ignores readability as a worthy discussion.\nWe provided other points in that response too.\n\n> I assume that this discussion about convolution should somehow tie to the ""neural layers"" mentioned on page 9.\n\nIt is not, as we pointed in our previous answer. \n\n> … If not, how is this discussion related to einops?\n\nWe expect that giving discussion around ""why this approach did not work for DL code"" is valuable for context since no post-mortems or good discussion of issues were published. \nTwo years ago the most common questions for einops were ""how does it compete with labeled tensors (LT)"", ""can you integrate LT"" or ""can you integrate into LT"". \nOur answer ""this thing is not complete and it is unlikely that it will ever be"" rarely satisfied and long unwrapping was required.\n\nStrangely, this same review demands a detailed comparison with einsum in the main text, which is not a competitor of einops (we got this question too, but rarely - if a person ""speaks"" einsum, similarities and differences are rather obvious. If not - this discussion would bring no value at this point. For a novelty statement this may be different, and we\'ll try to cover that).\n\nWe rearranged the text and gave high-level comparisons with einsum in Sections 2 and 4 while having detailed discussion in Appendix B. Additionally we moved discussion about convolutions and labeled tensors into Appendix A.\n\n> The impact of these libraries is not comparable to that of einops. \n\nIf impact is high, then is ""just wrapping"" fine? On well-known examples we pointed to the lack of argument and that ""wrapping"" is a common practice in software.\n\n> any permutation of shape elements provides a valid reshape and does not fail\n> Isn\'t that true also in einops? That example only makes sense if one hardcodes the output shape\n\nNo, it is not: change of output order does not have the same consequences in einops. As we pointed, the following reshape can\'t be written in rearrange.  \nAssume that we have torch.zeros(H, W, color, time).reshape(W, H, color * time) and analogue with einops rearrange(x, ""h w c t -> w h (c t)""). In both cases we ""made a mistake"" in ordering ""w"" and ""h"" but einops does transpose on these axes while in case of pytorch reshape mixing between these axes happens and there will be a mess with the tensor similar to  Figure 1 (c) .\nIn einops resulting w and h axes still correspond to w and h, as output shape and output pattern suggest.\nIn addition, users can provide axes sizes to ensure exact dimensionalities of inputs rearrange(x, ""h w c t -> w h (c t)"", w=W, h=H).\n\n\n> My question was: ""Can the authors provide an example of how they improved the exception message for broadcasting w.r.t. the one from Numpy? How is it easier to understand?""\n\nEinops does not provide broadcasting, so this seems like misinterpretation. \nWhen a user does not use einops and makes a mistake in reshape/transpose/..., in majority of cases after wrong reshape they receive messages about broadcasting - not because broadcasting is used, but because numpy (and other packages) try to apply broadcasting when shapes don\'t coincide and report broadcasting error (which is an indicator of mistake, but does not point where mistake is). \n\nWe removed in the revised version mention of broadcasting to avoid confusion.'}, {'title': 'Reply', 'comment': 'I thank the authors for their response. It was indeed my goal to help them improve the paper.\n\n>considering it as an implementation/documentation thing\n\nI consider it neither a documentation paper nor one that introduces the notation, as I have stated before. I agree with reviewer p5Km that the paper ""reads like a technical blog"". Please refer to my previous comments for a justification of why I think this is not good.\n\nFurther proof of this is that the authors have relegated the discussion of the notation to the appendix, indicating that they don\'t see the need for formal discussion in the paper. \nThe same could be said for the comparison with einsum. \n\nAlso, code review is required for a paper like this and therefore JMLR OSS is a better venue.\n\n> Users never reported issues with it.\n\nIs the notation a contribution of the paper? If so, asking that the readers infer the notation from the examples is not sufficient.   \nI don\'t consider this a satisfactory reply to my comment. \n\n> Tensorflow is merely an API to CuDNN/CuBLAS/CuFFT/... functions...\n\nThe impact of these libraries is not comparable to that of einops. \nAlso: how less verbose is PyTorch compared to the CUDA equivalent? How less verbose is einops compared to the PyTorch equivalent?\n\n> Example on page 4\n\nI still don\'t get what that passage is supposed to convey. One unclear sentence is: \n\n""Should convolution stick to existing behavior and rely on axes order not names? If so, we get no additional checks, but create potential problems since **user is expected to use axes names in other operations** (e.g. reductions), but has to drop the names before every convolution, and reorder axes if necessary""\n\nI assume that this discussion about convolution should somehow tie to the ""neural layers"" mentioned on page 9. If not, how is this discussion related to einops?\nAs I said, this is just unclear to me, I\'m not objecting to anything in particular.\n\n> Resulting construction becomes not readable\n\nThe readability is not what is being discussed here. \n\n> any permutation of shape elements provides a valid reshape and does not fail\n\nIsn\'t that true also in einops? That example only makes sense if one hardcodes the output shape. \n\n> Page 4, third bullet point\n\nI had misinterpreted the example, I have re-read it after your reply and I retract that comment. \n\n> Given type hinting, correctness of result and TF-specificity, we don\'t see this as a concern.\n\nDiscussing the design choices of TF is outside of the scope of my review.\nIf the authors decide to dismiss this issue as a non-concern, I still advise them to highlight this behaviour clearly, because it is in contradiction to the way TF works.\n\n> Einops complains on: ...\n\nMy question was: ""Can the authors provide an example of how they improved the exception message for broadcasting w.r.t. the one from Numpy? How is it easier to understand?""\n\n----\n\nI think that we could go on unproductively discussing the finer details ad infinitum, which is why I have not replied point-by-point to many additional things that the authors have written in their reply. Overall, I think that none of the following major points was addressed: \n\n- JMLR OSS would be a better venue (due to code review).\n\n- The paper is neither a good description of the software nor a good introduction to the notation. \n\n- The presentation is, at times, confusing.\n\n- There are still many non-rigorous claims that cannot be tolerated in a scientific publication (waiting for the authors to update the paper).\n\n- Some design choices could lead to unexpected problems when using the library, and the authors chose to dismiss them as a non-concern (which I don\'t believe counts as a real solution). '}, {'title': 'Response to all reviewers', 'comment': '### Einops and Einsum\n\nAs we mention in the paper, einops notation was largely inspired by np.einsum and we tried to align interfaces for simultaneous usage. Appendix A demonstrates how those should interact together (einops documentation contains multiple examples).\n\nOn pages 6-7 there is numpy/einops mapping. Only 2 operations of 16 (transpose and swap axes) can be implemented with einsum.\n\nEinops deviates from einsum in the following ways:\n\n- Arbitrary reductions. Einops supports max-, min-, sum-, mean-, logaddexp- reductions (as well as callables to support any other reduction). Einsum can be used to perform sum-reduction only.\n- Axes composition/decomposition. This core change is discussed in the main text with multiple examples\nMulti-character names. Digits and underscores are allowed, arbitrary capitalization is allowed), space is used as a delimiter for axes names. In einsum ""ij k -> k ij"" transposes 3-dim tensor, while this is an operation on 2-dim tensors for einops.\n- Unitary axes (in any pattern user can use ""1"" to reflect axis of size one)\n- Specification of axes size, verification of shapes and divisibility.\n- Anonymous axes. Axes that are present only on lhs or rhs can be specified with just size in the pattern. Example: repeat(x, ""h w -> h w c"", c=3) can be repeat(x, ""h w -> h w 3""). Unitary axes are not subcase of anonymous axes.\neinops.repeat (introduction of new axes on rhs by repeating elements along the axis). We demonstrate that it allows einops to support repeats and tiles. \n- Backend-independence of einops notation. Einsum implementations suffer from non-aligned support for spaces between names, and capitals.\n- List inputs. Einops can accept a list of tensors with identical shapes, which are stacked. E.g. rearrange([r, g, b, alpha], \'channel h w -> h w channel\'). This extends einops to support tensor stacking and concatenations.\n- Layers. einops functions have layer counterparts, e.g. einops.layers.torch.Rearrange(\'b c h w -> b (c h w)\') can be used inside Sequential as one of the modules. In examples from pytorch documentation (otherwise sequential) modules are implemented using custom ""forward"" to accomplish this transformation with x.view or x.flatten.\n\nWe do not implement following features of np.einsum:\n- No axes repeats on lhs. Einsum allows writing e.g. \'ii->\' to get a matrix trace. To prevent coding mistakes, we dropped this. No user feedback argued for bringing it back.\n- Multi-tensor operations. Einops focuses on single-tensor transformations. While there is an einops layer that operates on two tensors, the core functionality used by most users is one-tensor transforms.\nEinops novelties (compositions, new axes, anonymous axes, unitary axes, specification of axes sizes) can readily be applied to extend e.g. einsum. \n- Implicit indexing. It relies on letter sorting, does not encourage descriptive axes names as those define output axes order, and makes output implicit, not explicit; it also conflicts with einops.reduce. Github search shows negligible usage (<1%) of list-inputs syntax, and it was not added to einops.\n\n### Venue placement\n\nFormally, einops satisfies two topics mentioned in call for papers: implementation issues, software platforms.\nEinops was submitted specifically for this track. \n\nICLR community is the right auditory for einops as these are (mostly) researchers heavily operating with tensors, and implementing novel models and algorithms. \nOur solution would be most beneficial to them and can help in research on a daily basis.\n\n### Validation of notation and implementation\n\nReviewers p5Km and gRMH push on statistical validation (user-study), which has extremely limited scope and value: neither it can guide long-term design of a set of dependent features, nor it helps to verify ability to smoothly integrate into existing codebase, nor it shows maintainability of resulting solutions, nor even proves ultimate usefulness.   \n\nPytorch claims to be user-focused. Recent keynote talk by Pytorch lead (https://soumith.ch/posts/2021/02/growing-opensource/) confirms our position about poor relevance of artificial tests and ultimate validation by tracking usage dynamics, collecting and analyzing feedback.\nWe believe a couple of user studies will add little to no value to the reader, and can\'t be considered any stronger argument than adoption.\n\nEinops similarly has taken a ""test by time"" approach. Open-sourced more than 3 years ago, it got adoption in public projects of most major AI labs, which is a strong indicator of novelty, usefulness and reliability.\nWhile einops used studies on small user groups to validate its design, there is little value that can be added to adoption evidence at this stage.\nWe want to highlight the paper\'s focus: analyze previous ideas in detail, describe our approach, technical challenges and solutions, and explain which problems a researcher can expect to be resolved. The latter is supported by examples and implementations for real-world cases.\n\n'}, {'title': 'Response to reviewer gRMH, part 4. ', 'comment': '\n> Since TensorFlow knows to automatically convert arrays to tensors at runtime\n\nNot automatically, the right word is implicitly. ""Explicit is better than implicit"" if you find Zen to be an argument.\nSo, why is w = x @ y @ z, which possesses in this context the same behavior as rearrange, not prohibited in TF?\nTF\'s design to make explicit operations, but not conversions, is very controversial and neither supported by its predecessors (theano, mxnet), nor by google\'s jax, nor by any other popular package. Though it makes sense for the production setting when only TF is approved.\n\nGiven type hinting, correctness of result and TF-specificity, we don\'t see this as a concern.\n\n> Can the authors provide an example of how they improved the exception message for broadcasting w.r.t. the one from Numpy? How  is it easier to understand?\n\nEinops complains on:\n- wrong input dimensionality (e.g. reshape doesn\'t check it)\n- malformed pattern with missing on one side/repeating axes. E.g. incorrect rearrange from this review would receive (after fixing order of arguments):\n\n```\nError while processing rearrange-reduction pattern ""ab->()"".\n Input tensor shape: (3, 5). Additional info: {}.\n Identifiers only on one side of expression (should be on both): {\'ab\'}\n```\nThis error 1) points to exact pattern to search for 2) provides full information about inputs 3) points to exact mismatch between operation semantics and usage 4) even shows some information about parsing: ""ab"" is a single axis.\n\n- missing information about axes introduced on RHS\n- mismatch on anonymous axes size, mismatch on unitary (length 1) axes\n- mismatch on composite axes size if all components are defined, e.g. (a b c), a=2, b=3, c=4; if only part of them are provided with explicit size (e.g. b and c) - divisibility is checked\n- insufficient information to infer all axes sizes \n- sizes for identifiers not used in the expression\n- multiple syntax/semantic errors (brackets, incorrect identifiers, parenthesized ellipsis on lhs, improper usage of anonymous non-unitary axes, invalid names, double usage of identifier, double usage of ellipsis, etc)\n\n\n### To sum up:\n\nReview misses value, placement, related works, and historical context of our contribution. We can be blamed for not discussing the historical context in the paper.\n\nThere are several misattributions/misinterpretations which result in wrong claims.\n\nSoftware (implementation/API) remarks and recommendations are questionable and not grounded. Pointed concerns and recommendations were never brought to our attention by community or other experts since einops was open-sourced more than 3 years ago. Einops currently is used by >1000 public repos/projects, and was independently endorsed by multiple core contributors to large OS projects, so API design and implementation were extensively tested by the community.\n\nWe accept comments on limited comparison with einsum, insufficiently formal exposure, lack of benchmarks and formal notation.\n\nComparisons with other frameworks made by the reviewer unexpectedly confirmed strong sides of our work.\n\nOnce again, thanks to the reviewer gRMH for being specific and detailed, we value this. We believe review would also benefit from being more weighted and justified.'}, {'title': 'Response to reviewer gRMH, part 3. ', 'comment': '> Figure 1: the example assumes that the reshape is done by hardcoding the dimensions and that the programmer will make a typo that ""luckily"" does not crash. First, we don\'t know how often this kind of bug happens and, second, this issue can be easily bypassed by accessing the actual shapes (like the authors do on page 6, line 2-3 of the code block at the bottom). I wouldn\'t use this as a primary motivation for the rearranging function.\n\nThis issue can\'t be bypassed, because \n- Resulting construction becomes not readable even for a 4-dimensional tensor. This becomes insanely long when the name of the tensor is not x, but something like attn_logits. \nreshaped = attn_logits.reshape(attn_logits.shape[0], attn_logits.shape[1] * attn_logits.shape[2], head, attn_logits.shape[3] // head), and we did not transpose result yet\n- Reshape of this kind requires having a name for the tensor and can\'t be applied to intermediate results of other operations.\n- There is nothing ""lucky"" in not crashing, any permutation of shape elements provides a valid reshape and does not fail. Sufficient to use WH not HW for a non-square matrix. Wide usage of -1 in reshapes and powers of two as size dimensions increases chances of being ""lucky"".\n\n> Page 4, third bullet point: the authors imply that the code should crash, but in fact it is a perfectly well-formed expression that should not crash. Again, the authors should not assume the likelihood of a bug or what the users want to do.\n\nThis recommendation contradicts the established practice of software engineering. Design of programming languages (that reviewer refers to) is largely based on pulling out/discouraging/substituting language features with highest likelihood of bug.\n\nWe want to highlight for the reviewer that a single typo changes semantics of all downstream operations without providing any notice. Non-alignment between any two packages/functions in names is not caught, but propagates.\n \n> Page 7: ""Einops alleviates necessity to introduce a function, as arguments describe input, output, and the transformation itself"". This is a claim in support of the readability of einops, although it holds for einsum already. \n\nSee general comment (once again). This operation can\'t be rewritten with einsum.\n\n> While I partially agree, einsum/ops notation could also be seen as less beginner-friendly because it requires the users to know the syntax. For example arr.flatten() seems (subjectively) more descriptive than rearrange(""ab->()"", arr), especially if one doesn\'t already know einsum notation.\n\nArr.flatten is objectively less descriptive because in numpy and e.g. keras/tensorflow v1 this operation works differently and either returns 1d tensor or 2d tensor depending on the framework.\nIn pytorch flatten uses axis indexing that conflicts with standard python range indexing.\n\n> than rearrange(""ab->()"", arr)\n\nThis is not a valid einops operation. In rearrange neither of dimensions can be reduced, while reviewer reduced all (first point in section ""4. einops""). Correct analogue of flatten can be found on page 7, second line.\nOnce again, einops is not einsum, and does not use one-letter names.\nPattern\'s syntax is correct, though.\n\n> The choice of automatically detecting the backend from the input is arguable. Quoting from the famous Zen of Python by Tim Peters: ""Explicit is better than implicit"" and ""In the face of ambiguity, refuse the temptation to guess.""\nFor example, a situation could happen in which a Numpy array is given as input to a TensorFlow-based user-defined function. Since TensorFlow knows to automatically convert arrays to tensors at runtime, this is standard and expected behaviour. Replacing the initial call of this imaginary function with an einops-based one would result in the first operation happening in Numpy and not TensorFlow.\nI advise the authors to adopt a paradigm like that of multi-backend projects like Keras in which the users decide explicitly what backend to use (through a config file or by making the backend explicit at import time -- e.g., from einops.torch import ...).\n\n\nWe do not agree with these suggestions.\n\nKeras\'s config file to define which framework it will work with is a catastrophe in the long term: two or more systems/models implemented over different backends can\'t coexist in the same process. Keras didn\'t live long enough to face these consequences. \n\nUser that can work with either numpy, or tensorflow, or pytorch, but not two - design is a clear no go. \nEinops layers use syntax `from einops.layers.torch import ....` because layers (weight management/hooks/etc) are fundamentally incompatible. \n\nThe value of writing tensor backend-polymorphic code was long underestimated (e.g. by tensorflow but not jax; and see data-apis consortium that we mention). Introduction of specific functions complicates backend-polymorphic coding.\n'}, {'title': 'Response to reviewer gRMH, part 2. ', 'comment': '\n> In this regard, throughout the paper the authors seem to imply that einops solves problems (like that of rearranging) that were previously unsolvable (e.g., second paragraph of page 6). This is also misleading since einops is merely an interface/API to existing functions.\n\nTensorflow is merely an API to CuDNN/CuBLAS/CuFFT/... functions. Numpy is merely an API to C/FORTRAN functions. Pytorch is (was) an interface/API for lua-torch cpp/cuda backend. OpenBLAS is merely … etcetc. This comment seems to argue with the concept of software stacks.\n\nSecond paragraph of page 6 does not claim, or implies, or reads that it was impossible. Exact phrase from the paragraph:  ""... particularly helpful to leverage existing operations ..."". \n\nMoreover, einops is not just interface, but a novel notation (that was already ported e.g. to rust independently of us).\n\n> The example on page 4 concerning convolution is unclear. For example: when convolutions rely on axes order, the user is not ""expected"" at any time to use named axes for other operations. It is also not clear what kind of ""name checks"" would be prevented by assigning fixed names to the axes in convolution, and how einops overcomes this issue. I might have misunderstood this whole paragraph, so please correct me if I\'m wrong.\n\nThe paragraph discusses issues with integrating named tensors, and this \'the user is not ""expected"" at any time to use named axes for other operations\' just straight claim to not use them anywhere.\n\nEinops does not introduce tensor-attached labels and does not face this problem.\n\n> In this regard, if the interaction with ""neural layers"" is a primary motivation for developing the library, the feature should be discussed more in depth (it is only briefly mentioned once, towards the end, with no explanation).\n\nIssues of labelled tensors should be resolved by labelled tensors, not einops, which takes completely orthogonal design.\n\n> There are many non-rigorous/subjective claims. I have found at least five:\n\nThank you for being specific here, we value this. \nWe should point out that 4 of them are in the discussion section, and the last one is a footnote.\nWe will rewrite statements to highlight observational character.\n\n> Since there are no performance benchmarks, how is performance quantified here? I assume that the performance of einops is equal to or worse than the backend, given the extra overhead. Is it faster than equivalent einsum operations? Can the authors quantify the ""important role"" of caching?\n\nThanks for the feedback. For practical use-cases the overhead is negligible. We will add details on this comparison in the revised version.\n\n> Page 9: ""Design of such system is much harder than it sounds: previous ideas failed"" -- What does it mean that designing is hard and how can the authors be sure that it wasn\'t due to their own limitations? Whose previous ideas have failed and in what context? I am obviously not implying anything about the programming and engineering abilities of the authors, I am merely pointing out that claims like this should not be made in a scientific paper.\n\nSection 3 discusses an alternative approach (labelled tensors) that was in different ways independently explored by several groups.\nYears after the inception and despite significant informational support they did not reach adoption, and two were already shut.\n\nE.g. Yann Lecun promoted one of these solutions https://twitter.com/ylecun/status/1080974471689687040?lang=en, others are/were included in the most widespread frameworks. Multiple researchers, not only LeCun (see thread) were convinced that more reliable tensor ops are almost ready and need some polishing.\n\nWe see this as an important point that einops was adopted, while seemingly reasonable approaches did not pass ""testing by time"". \n""Much harder than it sounds"" is a very mild and tempered way of reflecting the drastic difference between promises/expectations and results of these alternative designs.\nThis controversy is a valuable lesson of software/notation design, that highlights the importance and non-obviousness of our work. \nIt is unprofessional to refer to our capabilities, specially in the light of discussing work done not by us. During einops design we were aware of design traps of labelled tensors, however the only confirmation one can give to support a claim is reference to systematic work of other researchers that did not succeed - that\'s what we observe and report. \n\n'}, {'title': 'Response to reviewer gRMH, part 1. ', 'comment': 'First of all we want to say thanks to reviewer gRMH for an extremely detailed review. However, we disagree on most points and will delve in details later.\nSpecific suggestions and detailed points reviewer gRMH left make it easy for us to improve the paper and/or keep discussion focused. \n\nGlobal comment:\n\nThe review largely misses placement of the paper and its value for ICLR participants, instead considering it as an implementation/documentation thing.\nML/DL researchers experiments in most cases involve tensor manipulation, and claim that improvement to this process ""is not suitable for publication at ICLR"" is ... weird.\n\nReviewer gRMH also pushes on statistical validation, please see our discussion of its insufficiency in comment to all reviewers. \n\nComments in this review which oppose einsum and einops are mostly irrelevant (see detailed comparison in the comment to all reviewers).\nHistorically, einops was open-sourced before proper implementation of einsum appeared in most of the DL frameworks (Example: in TF supported only two arguments, did not support ellipsis, did not support repeated indices; keras, gluon, mxnet did not provide this function at all). \nEinops documentation and examples (one example is given in Appendix A) argue for usage of einsum along with einops, and likely contributed to the process of following einsum adoption, as there is a large intersection between the communities, advocating for usage of these two in DL.\nEinops always cited its inspiration einsum (including paper)\n\n> I realize that libraries like TensorFlow and PyTorch had a paper published at some top conferences like ICLR, but it\'s clear that their novelty and impact on the community was incomparably larger at the time of publication.\n\nTensorflow paper claims no novelty and makes insufficient attribution to previous works they build on. \nPytorch paper focuses on design choices and implementation, and does not claim any novelty. \nNeither of the papers mentions any user studies on API design.\nTensorflow paper has a list of recommendations based on experience of porting a single deep learning network (inception).\nPytorch uses a number of arxiv papers (not compared to anything) as a proof of design and adoption.\n\nWith all respect, in our paper we claim a novel notation for tensor manipulation (which is different from einsum, see their comparison ""einsum and einops"") compared to the examples you provided.\n\nFurther, pytorch claims to be user-focused. And in a much-later talk (https://soumith.ch/posts/2021/02/growing-opensource/), pytorch lead confirms our points, why and how they ""measured"" design choices. Pytorch project is at the stage when choices wouldn\'t be frowned upon, which made such (keynote) talk possible.\n\n> repeating axes;\n\nThe way it is written in the review is incorrect, there is no such operation as ""repeat axis"", but ""repeat tensor elements along new axis""\n\n> Users have to remember axes order: this is largely solved by einsum, not einops exclusively.\n> Einops is declarative and self-documenting: einsum too, einops simply extends the paradigm.\n\nPlease see a general comment and response to all reviewers. Most of einops operations can\'t be expressed with einsum, so einsum provides very limited remedy, basically only for sum-reduction and transposition, and only with one-letter names.\n\n> Control over data layout: einops does not solve the issue, it just gives a simpler interface to control it.\n\nSo ... this is not valuable? Not useful? Paper states the opposite? Point is missed.\n\n> The notation is never formally described, for instance as a grammar,\n\nThanks for the feedback. We will add formal syntax in Appendix in the revised version.\n\n>  and one must look at examples to infer the form of a correct string.\n\nUsers never reported issues with it.\n\n> Many features like anonymous axes and ellipsis indexing are essentially ignored, even if they are a key part of the notation.\n\nThanks, we will add details in Appendix in the revised version.\n\n> A comparison with einsum is missing, which confuses the reader and does not highlight some features that are present in einsum but not einops (for example, computing the trace and the implicit notation).\n> Also, the authors claim that they ""align the interface with einsum to allow smooth simultaneous usage"" but this is misleading. Einops is clearly inspired from einsum and simply adds some features (while also removing others, as I said above).\n> And, in any case, did einops notation get picked up more easily than einsum notation?\n\nWe will add a detailed comparison in Appendix. Again, see a general comment and comment to all reviewers.\n\n'}, {'title': 'Response to reviewer p5Km', 'comment': 'Thank you for the review, p5Km\n> … which is an important intermediate layer for modern ML systems ...\n\n> These claims sound very promising and valuable as a ML toolkit.\n\nWe appreciate this evaluation.\n\n> this paper reads like a technical blog, which tries to introduce and advertise a Python library\n\nThis is a generic statement that we expect to be detailed, at least accompanied by a set of examples.\n\nPaper states the problem, discusses context and previous approaches, introduces notation, demonstrates proposed solution (package), describes technical challenges and (sometimes unique) decisions that made this solution possible and practical. \nWe support our work with analysis of code examples. If there is something that is redundant and does not line up, please be specific and point this out.\n\n\n> ""We intentionally omit discussion of user conveniences provided by einops package: anonymous axes, ellipsis, list inputs and neural layers."" […] this sentence sounds a little ungrounded\n\n9 pages is the hard limit, we will rephrase and provide additional information in Appendix.\n\n> To be specific, I do not get why Section 2 & Section 3 are split, it seems that both sections are talking about the issues and limitations of STOA systems.\n\nSection 2 focuses on the currently widely accepted solution that we build upon and integrate into; also we discuss its flaws. However, einops does not completely replace existing operations (see Appendix A).\nIn contrary, section 3 describes alternatives to einops, and how those aim to resolve named flaws.\n\n> There is a lack of reasonable amount of empirical study to justify the statements about the contribution of the work. For example, the author claims that einops ""significantly improves code readability and flexibility"" in the abstract, I would expect some user-study (which could invite a group of participants to accomplish some programming tasks about tenor manipulation with and without einops and measure some objective or subjective metrics for evaluation) to justify this claim---note that this is pretty standard in first-tier PL/SE volumes, it would be easy to find hundreds of such papers and adopt such methodology for evaluation. The current Section 5 can be considered as good motivating examples, but it is not sufficient to support this claim following the principle of scientific study. \n\nPlease check the general comment to all reviewers.\n\n>  The efficiency evaluation of the implementation is also missing, e.g., it is important to learn how much overhead is introduced by imposing einops over the backend systems, e.g., what is the runtime gap between einops implementations and direct usage of the backend systems\' interfaces.\n\nThanks for the feedback. For practical use-cases the overhead is negligible. We will add details on this comparison in the revised version.\n\n> Lastly, I feel lost about the discussion about the difference between einops and numpy einsum. As far as I learn, the interface looks similar (at least from coding examples). As so, I would expect some clear and concrete statements about the distinguish components of einops (different from einsum).\n\nWe appreciate the feedback and will make this clear in the revised version. Detailed explanation we give in the general comment to all reviewers. \n'}, {'title': 'Response to reviewer ntk4', 'comment': 'Thank you for the review, ntk4.\n\n\n> I found the tool very useful for any application involving tensors with Python. The paper is focused on deep learning applications. However, I am sure it is also very useful for dealing with Tensor Networks (TNs), where many core tensors are interconnected. The paper lack of a description of its application to TN contractions, for example.\n\nWe agree with your judgement and we agree TNs can benefit from einops, e.g. einops test suite has tensor train as one of the cases.\nEinops + TNs require detailed discussion with examples, but we are unlikely to accommodate this in the paper (and we believe that the right exposure of TN community to einops and analysis of benefits/cases/practices/potential costs is of its own value, and better be done by the experts in TN)\n\n> It seems that current implementation only consider operations on single tensors. I would suggest to include also operations on two or more tensors, similarly to einsum in numpy and Pytorch, but using a more flexible indices labelling.\n\nThank you for this suggestion.\n\nWhile not mentioned in the paper, einops has one layer implementing generalized einsum for two tensors, one of which is learnable ""weight"". It covers a number of practical cases (e.g. MLP, MLP-mixer). \nWe demonstrate in examples (page 6 last row and page 7 first row) that stack and concatenate of multiple tensors can be implemented in einops.\nWe consider adding generalization of einsum as a part of einops, however inconsistent support by backends delays this feature. \nOther multi-tensor operations are a subject of future research.\n\n> A comparison with previously available solutions, via numpy or Pytorch, for example, in terms of computation cost is missing in the paper, making it difficult to evaluate if there is some price to pay.\n\nThanks for the feedback. For practical use-cases the overhead is negligible. We will add details on this comparison in the revised version.\n'}, {'title': 'Response to reviewer c6LK', 'comment': ""Thank you for this neat review.\n\nSummary and strength are so well-written that it feels tempting to steal some phrases.\n\n> With the proposed notation, compiler-based approaches, e.g. TVM, MLIR, haven't been explored to further boost performance, although such opportunities are real and tangible. For example in TVM, the Tensor Expression (TE) can be considered as a generalized form of EinOps, and it could bring extra performance gain to the users. Therefore, it would be desirable to integrate with those compiler backends to assist with potential performance.\n\nAgree, we'll mention this in the discussion section. \nThanks for a specific pointer to TE, we'll investigate this opportunity for speeding up further.\n\n> The normal definition of the EinOps notation, while demonstrated in many examples, are not carried out formally. For example, brackets () in the notation may serve different purposes under rearange, reduce and repeat, on the left- and right-hand side of the notation. It may refer to tiling one dimension, or composition of new dimensions, or removing a dimension, etc. Another example that needs more clarity is stack/concatenate where there are multiple tensors involved, and might be desirable to state formally on the constraints implied in this particular case.\n\nThanks for the feedback. We will add formal syntax and constraints in the revised version.\n\n> Last, while the notation is very flexible in expressing layout-related operations, it is yet under-explored on expressing the common operators that deep learning researchers may care about, namely, many variants of convolution.\n\nWe agree with this. \n\nResearchers tend to use hardware-optimized operations (mostly, CuDNN) as they provide a solid combination of flexibility and performance.\nThus, one either wraps CuDNN (and yet-another-way to describe convolutions is interesting as a concept, but not that valuable), or introduces notation for out-of-the-box operations (as e.g. Halide/TensorComprehension). Due to the low ratio of value over long-term engineering costs, we are unlikely to explore that direction.\nSome other DL operations (not convolutions) are a subject of future research.""}, {'summary_of_the_paper': 'Operators are key elements in deep learning (TensorFlow, PyTorch, etc) or scientific computing (NumPy) frameworks.\nThis paper introduces a generic, convenient and elegant way of massively describing large portion of frequently used operators\nunder a unified framework, EinOps, inspired by einsum, or the Einstein summation convention.\nUnder this convention, key components like multi-head attention in modern neural networks can be expressed\nwith fewer lines of code. The major contribution of this work includes:\n- Presents a set of novel and highly expressive notations to represent hundreds of operators\nin modern deep learning frameworks;\n- Develops a library that interprets the proposed notations and offloads them to existing implementations\nin NumPy or TensorFlow, PyTorch;\n- Demonstrates significantly improved readability and debuggability with the proposed notations\n', 'main_review': '**Strength of the work.** First of all, the paper provides a handful of case studies with insightful analysis on\nuser experience on the existing design of operators, and then based on those studies it derives a series\nof notations for highly user friendly operator design. By providing various comparison between\nexisting ones on real-world, the paper leads to a convincing proof of the strength of\nthe proposed notation for deep learning researchers\' daily usecases, in terms of expressiveness,\nelegance, usability and debuggability.\n\nSecond, the paper comes with solid engineering efforts that offload the computation to underlying\nlibraries, including NumPy, TensorFlow, PyTorch, MXNet, Keras etc. Effectively, with the solid engineering\neffort, the library serves as a meta framework that can be reused across backends with close-to-zero\nengineering overhead. The paper also explores some techniques like caching to reduce the execution overload\nof runtime dispatching.\n\nThird, by the ""stringly-typed"" abstraction of operators, the proposed notation enforces more explicit\nprogramming of the semantics of tensor axes, as well as more runtime checking to catch correctness issues.\nThis is particularly helpful among deep learning practitioners who are already exposed to a set of conventions\n(e.g. ""NCHW"") to write the right code. The typed operators, instead of typed tensors (e.g. NamedTensor),\nprovides the guarantee that users only need to type important part of their code as a drop-in replacement of the existing\ncode fragments without having to worry about refactoring the entire codebase. This is again particularly helpful\nfor deep learning practitioners\n\n**Weakness.** With the proposed notation, compiler-based approaches, e.g. TVM, MLIR, haven\'t been explored to further boost performance,\nalthough such opportunities are real and tangible. For example in TVM, the Tensor Expression (TE) can be considered as \na generalized form of EinOps, and it could bring extra performance gain to the users. Therefore, it would be desirable to integrate\nwith those compiler backends to assist with potential performance.\n\nThe normal definition of the EinOps notation, while demonstrated in many examples, are not carried out formally.\nFor example, brackets `()` in the notation may serve different purposes under `rearange`, `reduce` and `repeat`,\non the left- and right-hand side of the notation. It may refer to tiling one dimension, or composition of new dimensions,\nor removing a dimension, etc. Another example that needs more clarity is stack/concatenate where there are multiple\ntensors involved, and might be desirable to state formally on the constraints implied in this particular case.\n\nLast, while the notation is very flexible in expressing layout-related operations, it is yet under-explored on\nexpressing the common operators that deep learning researchers may care about, namely, many variants of convolution.\n\n**Correctness.** The reviewer is not aware of any correctness issue in all demos and figures in this paper.\n\n**Clarity.** The paper is overall rich in detailed and insightful explanation. There is some minor clarity issues as pointed out in the weakness section.\n\n', 'summary_of_the_review': 'In summary, the paper demonstrates clearly an elegant and novel notation to represent a large portion of operators in deep learning workloads, and developed a meta-framework with solid engineering. The reviewer recommends to consider the acceptance of the paper as a novel good deep learning meta-framework.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'The reviewer is not aware of any ethical concerns in this paper', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper extends the Einstein summation notation (einsum) of Numpy to introduce some additional features: \n\n- naming axes arbitrarily (i.e., with more than a single character);\n- controlling how groups of axes are flattened (called ""rearranging"" in the paper);\n- repeating axes;\n- reducing axes with different operations other than the sum.\n\nThe notation is implemented in a Python library called ""einops"" that works with different backends for tensor computation (PyTorch, TensorFlow, Jax, etc.). ', 'main_review': 'There are numerous issues with the paper, which I will try to summarize below. \n\nI should mention that, since no code was included in the submission, my assessment of this work is based entirely on the paper and the ideas expressed therein. I am not judging the work based on technical aspects related to the library except for those described in the paper.\nHowever, a complete assessment of the paper should also include a code review (see my first comment in the ""Weaknesses"" section).\n\n**Strenghts**\n\n1. I am a great fan of einsum notation for expressing tensor operations and I agree with the authors on the value of extending the notation as they did. I could see myself, as a practitioner, using the ""rearrange"" function (I thought that the example of reshaping an array of images into a 4x4 grid on page 6 was nice).\nAs an engineering feature, the einops package could be useful.\n\n2. Using the library seems to make the code less verbose in the examples provided in the paper. This gives a reasonable advantage in terms of the readability and maintainability of the code (although this is a purely subjective assessment, as I argue below).\n\n3. The fact that the library supports multiple backends is a plus, although I have some concerns about some design choices discussed in the paper (see below).\n\n**Weaknesses**\n\nUnfortunately, I think that there are considerable flaws in the paper, the motivation behind it, and the described implementation of the library. \nI will try to summarize everything in the following points.\n\n1. While I agree that much of machine learning is based on tensor manipulation, this paper is not suitable for publication at ICLR. \nEven if we were to focus only on tensor manipulation as a machine learning exclusive, better venues exist for this kind of paper, like the JMLR Open Source Software track. There, the paper could be reviewed along with the code (which was not included in this submission even if it is a crucial part of the authors\' contributions).\nI realize that libraries like TensorFlow and PyTorch had a paper published at some top conferences like ICLR, but it\'s clear that their novelty and impact on the community was incomparably larger at the time of publication. \n\n2. The motivation for the paper is weak. I will give some examples of claims made by the authors to motivate einops that I don\'t think hold up:\n    - Users have to remember axes order: this is largely solved by einsum, not einops exclusively.\n    - Control over data layout: einops does not solve the issue, it just gives a simpler interface to control it. \n    - Einops is declarative and self-documenting: einsum too, einops simply extends the paradigm. \n\n3. The contributions of the paper are not enough to consider a publication. \n\n    The paper should describe the notation in detail, but it doesn\'t. Instead, it is halfway between a description of the notation and a documentation for the package. The result is that it works badly as both. Some examples: \n\n    - The notation is never formally described, for instance as a grammar, and one must look at examples to infer the form of a correct string.\n    - Many features like anonymous axes and ellipsis indexing are essentially ignored, even if they are a key part of the notation.\n    - A comparison with einsum is missing, which confuses the reader and does not highlight some features that are present in einsum but not einops (for example, computing the trace and the implicit notation).\n\n    Also, the authors claim that they ""align the interface with einsum to allow smooth simultaneous usage"" but this is misleading. Einops is clearly inspired from einsum and simply adds some features (while also removing others, as I said above).\nIn this regard, throughout the paper the authors seem to imply that einops solves problems (like that of rearranging) that were previously unsolvable (e.g., second paragraph of page 6). This is also misleading since einops is merely an interface/API to existing functions. \n\n4. The example on page 4 concerning convolution is unclear.\nFor example: when convolutions rely on axes order, the user is not ""expected"" at any time to use named axes for other operations.\nIt is also not clear what kind of ""name checks"" would be prevented by assigning fixed names to the axes in convolution, and how einops overcomes this issue. \nI might have misunderstood this whole paragraph, so please correct me if I\'m wrong.\n\n    In this regard, if the interaction with ""neural layers"" is a primary motivation for developing the library, the feature should be discussed more in depth (it is only briefly mentioned once, towards the end, with no explanation).\n\n5. There are many non-rigorous/subjective claims. I have found at least five: \n\n    - Footnote 4: ""New users frequently continue trying to imagine the tensors layout in memory."" -- On what grounds are the authors making this claim without any references? Have the authors conducted user studies? \n    - Page 9: ""Caching plays an important role in einops high performance"" -- Since there are no performance benchmarks, how is performance quantified here? I assume that the performance of einops is equal to or worse than the backend, given the extra overhead. Is it faster than equivalent einsum operations? Can the authors quantify the ""important role"" of caching?\n    - Page 9: ""einops was also referred once as a good intermediate solution"" -- By whom? How is this whole reported conversation meaningful to the reader?\n    - Page 9: ""Design of such system is much harder than it sounds: previous ideas failed"" -- What does it mean that designing is hard and how can the authors be sure that it wasn\'t due to their own limitations? Whose previous ideas have failed and in what context? \nI am obviously not implying anything about the programming and engineering abilities of the authors, I am merely pointing out that claims like this should not be made in a scientific paper.\n    - Page 9: ""We observed that einops notation gets picked up for describing tensors with packed dimensions"" -- Again, did the authors perform a user study to make this claim? And, in any case, did einops notation get picked up more easily than einsum notation? If so, why?\n\n**Other minor issues**\n\n6. Figure 1: the example assumes that the reshape is done by hardcoding the dimensions and that the programmer will make a typo that ""luckily"" does not crash. First, we don\'t know how often this kind of bug happens and, second, this issue can be easily bypassed by accessing the actual shapes (like the authors do on page 6, line 2-3 of the code block at the bottom). I wouldn\'t use this as a primary motivation for the rearranging function.\n\n7. Page 4, third bullet point: the authors imply that the code should crash, but in fact it is a perfectly well-formed expression that should not crash. Again, the authors should not assume the likelihood of a bug or what the users want to do.\n\n8. Page 7: ""Einops alleviates necessity to introduce a function, as arguments describe input, output, and the transformation itself"". This is a claim in support of the readability of einops, although it holds for einsum already. While I partially agree, einsum/ops notation could also be seen as less beginner-friendly because it requires the users to know the syntax. For example `arr.flatten()` seems (subjectively) more descriptive than `rearrange(""ab->()"", arr)`, especially if one doesn\'t already know einsum notation.\n\n**Comments on the implementation details**\n\n9. The choice of automatically detecting the backend from the input is arguable. \nQuoting from the famous Zen of Python by Tim Peters: ""Explicit is better than implicit"" and ""In the face of ambiguity, refuse the temptation to guess.""\n\n    For example, a situation could happen in which a Numpy array is given as input to a TensorFlow-based user-defined function. \nSince TensorFlow knows to automatically convert arrays to tensors at runtime, this is standard and expected behaviour. \nReplacing the initial call of this imaginary function with an einops-based one would result in the first operation happening in Numpy and not TensorFlow. \n\n    I advise the authors to adopt a paradigm like that of multi-backend projects like Keras in which the users decide explicitly what backend to use (through a config file or by making the backend explicit at import time -- e.g., `from einops.torch import ...`). \n\n10. Can the authors provide an example of how they improved the exception message for broadcasting w.r.t. the one from Numpy? How is it easier to understand?\n\n**Suggestions**\n\nAfter revising the paper, the authors should consider submitting it to an appropriate venue for open source contributions. \n\nThe revision should make it more clear that einops is an extension of einsum and should highlight the concrete additions and limitations of the proposed einops. \n\nThe authors should make the paper more rigorous: the notation should be formalized, the claims about usability should be removed unless backed up by user studies, and the motivation of the paper/library should be rethought. \nThe paper could also focus less on examples (which are more suitable for the documentation).', 'summary_of_the_review': '**Positive aspects**\n\n- The proposed extension of einsum is useful and I can see it having large adoption in the community of scientific computing.\n\n**Negative aspects**\n\n- I don\'t believe that ICLR is the correct venue for a paper like this. \n\n- The novelty and quality of the paper are rather limited. \n\n- The motivation for the paper is weak and much of the advantages brought by the proposed ""einops"" were already brought by the original ""einsum"" notation. \n\n- The presentation is, at times, confusing.\n\n- There are many non-rigorous claims that cannot be tolerated in a scientific publication.\n\n- There are some design choices that could lead to unexpected problems when using the library. ', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '3: reject, not good enough', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'In this paper, a new Python toolbox called EINOPS (Einstein Operations) is introduced. The proposed toolbox allows for applying tensor operations on a single tensor data, such as reshaping, reducing (max, mean, etc), repeating, permuting, and others; in such a way that it improves code readability and flexibility. The paper illustrates about current issues on available solutions, typically in numpy and shows how EINOPS solves those issues. Also, the toolbox supports widely used frameworks such as Pytorch, TensorFlow and others providing framework-independent minimalist API for tensor structure manipulations.', 'main_review': 'Strengths:\n-\tThe proposed toolbox allows for simple and easy to read code tensor manipulation operations and operates with most popular Python frameworks.\n-\tThe usage of indices names is more flexible compared to already existing ones in the einsum operation in numpy and Pytorch, which are known to have some limitations, e.g. number of dimensions is limited by the number of letters.\n-\tIt is very useful for using Tensor Networks where number of interacting tensors are usually big and can have a large number of dimensions. \n\nWeaknesses:\n-\tThe paper does not provide any advance in theory or new algorithm for machine learning. It is limited to introduce a useful and appealing new coding tool.\n-\tThe paper does not mention its application for computing and manipulating Tensor Networks, missing a very important usage for which there is a growing audience eager to have such convenient tool.\n-\tEINOPS does not consider operations involving two or more tensors\n-\tA comparison in terms of computation cost is missing in the paper\n', 'summary_of_the_review': 'I found the tool very useful for any application involving tensors with Python. The paper is focused on deep learning applications. However, I am sure it is also very useful for dealing with Tensor Networks (TNs), where many core tensors are interconnected. The paper lack of a description of its application to TN contractions, for example. \n\nIt seems that current implementation only consider operations on single tensors. I would suggest to include also operations on two or more tensors, similarly to einsum in numpy and Pytorch, but using a more flexible indices labelling.\n\nA comparison with previously available solutions, via numpy or Pytorch, for example, in terms of computation cost is missing in the paper, making it difficult to evaluate if there is some price to pay.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper introduces einops, a tensor manipulating library in Python, to efficiently support multidimensional tensor manipulations widely adopted in deep learning. \nHighlighted advantages of einops include:  \n+ providing semantic check for tensor operations; \n+ including high expressiveness and flexibility for tensor manipulating interfaces;\n+ supporting multiple backend runtimes efficiently; \n+ making tensor manipulating code more readable and reliable, and etc..', 'main_review': 'First, this paper studies efficient programming paradigms for multi-dimensional array operation, which is an important intermediate layer for modern ML systems, especially for deep learning systems. Personally I tend to buy these advantages of einops claimed by the author, including:\n\n+ providing semantic check for tensor operations; \n+ including high expressiveness and flexibility for tensor manipulating interfaces;\n+ supporting multiple backend runtimes efficiently; \n+ making tensor manipulating code more readable and reliable, and etc..\n\nThese claims sound very promising and valuable as a ML toolkit. \n\nHowever, there are some fundamental concerns I have for the paper: \n\n+ The writing is problematic as an academic paper. Comprehensively, this paper reads like a technical blog, which tries to introduce and advertise a Python library; some statements in this paper are casual, i.e., in the end of Section 7, the author says ""We intentionally omit discussion of user conveniences provided by einops package: anonymous axes, ellipsis, list inputs and neural layers."", although I would not doubt about it if reasonable amount of evidence (e.g., empirical study or analysis) was presented, this sentence sounds a little ungrounded. Further, the organization of the paper can be polished as well. To be specific, I do not get why Section 2 & Section 3 are split, it seems that both sections are talking about the issues and limitations of STOA systems. \n+ There is a lack of reasonable amount of empirical study to justify the statements about the contribution of the work. For example, the author claims that einops ""significantly improves code readability and flexibility"" in the abstract, I would expect some user-study (which could invite a group of participants to accomplish some programming tasks about tenor manipulation with and without einops and measure some objective or subjective metrics for evaluation) to justify this claim---note that this is pretty standard in first-tier PL/SE volumes, it would be easy to find hundreds of such papers and adopt such methodology for evaluation. The current Section 5 can be considered as good motivating examples, but it is not sufficient to support this claim following the principle of scientific study. The efficiency evaluation of the implementation is also missing, e.g., it is important to learn how much overhead is introduced by imposing einops over the backend systems, e.g., what is the runtime gap between einops implementations and direct usage of the backend systems\' interfaces.\n+ Lastly, I feel lost about the discussion about the difference between einops and numpy einsum. As far as I learn, the interface looks similar (at least from coding examples). As so, I would expect some clear and concrete statements about the distinguish components of einops (different from einsum).', 'summary_of_the_review': 'This paper introduces an interesting tensor manipulating library in Python, supporting Einstein-notation style operations over multi-dimensional arrays for deep learning. The library manages to run the convenient interface over different popular deep learning frameworks. \n\nHowever, there is a lack of solid empirical study to validate the effectiveness and efficiency of the design and clear discussion about the difference from the existing tool, i.e., einsum. ', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '3: reject, not good enough', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation', 'authorids': ['~Alex_Rogozhnikov1'], 'authors': ['Alex Rogozhnikov'], 'keywords': ['tensor manipulations', 'tensor transformation', 'einops', 'einstein notation', 'einsum'], 'abstract': 'Tensor computations underlie modern scientific computing and deep learning.\nA number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc.\nHowever, tensor operations in all frameworks follow the same paradigm.\nRecent neural network architectures demonstrate demand for higher expressiveness of tensor operations.\nThe current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. \nMoreover, some commonly used operations do not provide sufficient checks and can break a tensor structure.\nThese mistakes are elusive as no tools or tests can detect them.\nIndependently, API discrepancies complicate code transfer between frameworks.\nWe propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors.\nWe implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations.', 'one-sentence_summary': 'We propose a notation for clear and reliable tensor manipulations; we implented notation in Python package to handle multiple frameworks', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'rogozhnikov|einops_clear_and_reliable_tensor_manipulations_with_einsteinlike_notation', 'pdf': '/pdf/d568f6e36eaa377888611b8e0d84076777edc330.pdf', '_bibtex': '@inproceedings{\nrogozhnikov2022einops,\ntitle={Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},\nauthor={Alex Rogozhnikov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=oapKSVM2bcj}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shoufa Chen', 'Enze Xie', 'Chongjian GE', 'Runjian Chen', 'Ding Liang', 'Ping Luo']",ICLR,CycleMLP_ A MLP-like Architecture for Dense Prediction,https://iclr.cc/virtual/2022/oral/6274,2022," This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can copewith various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.",Oral 2: Structured learning,https://openreview.net/pdf?id=NMEceG4v69Y,https://openreview.net/forum?id=NMEceG4v69Y,NMEceG4v69Y,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The authors propose a new MLP-Mixer-like architecture called Cycle MLP which has two main advantages with respect to MLP-Mixer: (i) it’s applicable to varying input image sizes, and (ii) linear computational complexity. The authors present competitive results on image classification, object detection and segmentation.\n\nThe reviewers felt that both (i) and (ii) are key issues in the current MLP-Mixer-based models. The reviewers also appreciated the simplicity of the idea and the execution of the empirical evaluation. During the rebuttal and discussion phase the authors provided compelling evidence for the issues pointed out in the initial review. \n\nGiven that MLP-Mixer-based architectures are becoming increasingly popular, I believe that these contributions will be of great interest to the ICLR community and I will recommend acceptance.'}, {'summary_of_the_paper': 'The paper proposes Cycle MLP architecture, the idea is to bring spatial context into Channel FC and increase its receptive field.  The main objective of the paper is to address the challenges faced by the current MLP-Architectures. Cycle MLP allows flexible image resolution and avoids quadratic computational complexity in dense architectures. The paper presents results in a variety of tasks and shows demonstrates promising performance. \n', 'main_review': 'The method modifies the existing 1x1 convolution (channel FC in paper), to sample points in a cyclical manner inside neighbourhood window along the channel dimension. It has the same weight shape as 1x1 convolution, ie: C_in x C_out. Stepsize is used to control the receptive field and can be thought of as analogous to kernel size in convolution. Overall Cycle MLP mimics the effect of sparse convolution kernel where each element along channel dimension is sampled at a different spatial position in a cyclical manner.\n\nStrengths: \n- Simple idea with promising results.\n- Readily adaptable to existing architectures with vanilla MLP.\n- Increases the receptive field without increasing the number of  parameters\n\nWeakness/suggested improvements:\n- There are no ablations or explanations on how/why the induced sparsity is helping in performance.\n- It is not evident that why cyclical sampling is better than random sampling or what is the significance of cyclical sampling. Some discussion and experiments around this point would be useful. \n- A minor typo: MHSA is written as MSHA below eqn3\n\n', 'summary_of_the_review': 'Overall a simple idea and seems to work well. However, some more intuition around cyclic sampling would be worthwhile and might improve the paper. Exploring alternate sampling strategies would be interesting (random sampling or some other pattern can be explored). ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': ""This paper proposes an improvement to existing MLP style models for image tasks that extends the general MLP competitive work compared to CNN models while attempting to capture some desirable CNN properties such as applicability to varying image sizes.  In addition the method has linear computational complexity as compared to previous MLP work which have quadratic complexity.  The basic idea is a variant of previous channel and spatial FC ideas by having a node's receptive field sample across spatial and channel domains rather than being fixed to either just channel or spatial aggregation.  The overall architecture uses the previous work on patch embeddings and idea of stages with stacked blocks.  Comprehensive experimental results are show improvement over previous MLP models and competitive results to other methods such as CNNs and transformers on multiple tasks including classification and segmentation."", 'main_review': ""The paper is strong in the sense that it proposes a simple yet effective idea to extend previous MLP work for image tasks.  In a sense the basic idea of a sparse sampling across spatial and channel space is a natural variation to the other extremes in previous work such as channel and spatial FC.  Thus the methodology follows the basic pattern as previous work so doesn't require much explanation in terms of details.  In any case, the primary contribution here is the extensive sense of experimental results.  Figure 2 and Table 2 already provide decent support for CycleMLP, and the results showing FLOPs and parameter counts further strengthens the results.  Also as CycleMLP should be generic in terms of tasks the additional results on detection and segmentation make the idea more convincing.  Overall the basic contribution of improving MLP models to have linear complexity and applicability to varying image sizes is solid.\n\nAlthough the main work here is experimental in proving out the idea, it would be useful to have a deeper analysis of why the method works well.  In particular, the strided sampling of CycleMLP seems a bit motivated by ease and computational complexity, but seems it doesn't quite have the basic property of CNNs in being adapted to local correlations in natural images.  Presumably the patch embedding part is critical here as it already explicitly enforces spatial locality before CycleMLP is applied?  Do the previous MLP methods use patch embedding, and if they do, would they also have similar performance?  In other words, how much of the desired properties of linear computational complexity and applicability to varying image sizes comes from the patch embedding itself?"", 'summary_of_the_review': 'Overall the comprehensive and varied experimental results give the paper its strength.  The results are either competitive, such as on segmentation, or better than the previous MLP methods.  The idea is simple and a natural extension of previous ideas for fully connected layers.  There is a question on the relative contribution of the patch embedding when comparing to previous MLP methods, but generally no major weaknesses in the paper.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper presents a simple MLP-like architecture, CycleMLP for image classification, object detection and segmentation. It can cope with various image sizes and achieves linear computational complexity to image size. CycleMLP achieves competitive results on object detection, instance segmentation and semantic segmentation. Further, it outperforms Swin-Tiny on the ADE20K dataset with fewer FLOPs. ', 'main_review': 'Pros:\n(1)  This paper designs a new MLP-like architecture that can cope with various image sizes and achieves linear computational complexity. \n(2)  The authors contribute amounts of experiments to show the comparable performance even higher performance in semantic segmentation. \n\nCons:\n(1)  It is a little difficult to understand Figure 1. In Figure 1(c), you implement cycle operation in the HW dimension, but in Figure 1(d), it seems that you only implement cycle operation in the H dimension. It needs to be clarified.\n(2)  In Table 4 and Table 5, the authors conduct ablation studies about stepsize. It is implemented when stepsize is 1x7 and 7x1, what if the stepsize is 2x7 or 7x2? Does this situation exist? Another, can the stepsize be an even number? \n(3)  Besides, ViP uses the 3-branch structure and AS-MLP seems to be the concurrent work. What are the specific differences between CycleMLP and these architectures?\n(4)  About Figure 4, what means the effective receptive field? And what does the higher color value mean? From this figure, Swin has the sharp ERF and CycleMLP has smoother ERF. Can you explain it?\n', 'summary_of_the_review': 'This paper shows competitive results of MLP-like architecture on image classification, object detection and segmentation. However, there exist some unclear descriptions. Currently I rate it as a borderline paper and the authors need to consider the above questions.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents a  MLP-like architecture, CycleMLP. The proposed CycleMLP can be used for dense prediction, which is more suitable for object detection or image segmentation tasks.  The experiments show the effectiveness of the proposed model. The proposed Cycle FC can reduce the amount of network parameters and calculation, and is insensitive to image resolution.', 'main_review': 'Strengths: 1. The Cycle FC is capable of dealing with various image scales, and has linear computational complexity the same as channel FC and a larger receptive field than Channel FC. 2. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. \n\nWeaknesses: 1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size. 2. CycleMLP is slightly insufficient in discussion of the design and ablation studies.', 'summary_of_the_review': 'The proposed method is simple and interesting. Compared to modern MLP architectures, the CycleMLP can cope with various image sizes and achieves linear computational complexity. The benefits of each component and the performance difference are shown. All experiments of this paper is detailed and the overall content is sufficient. The proposed method is applicable to many vision tasks in the future.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'CycleMLP: A MLP-like Architecture for Dense Prediction', 'authorids': ['~Shoufa_Chen1', '~Enze_Xie1', '~Chongjian_GE1', '~Runjian_Chen1', '~Ding_Liang1', '~Ping_Luo2'], 'authors': ['Shoufa Chen', 'Enze Xie', 'Chongjian GE', 'Runjian Chen', 'Ding Liang', 'Ping Luo'], 'keywords': ['MLP', 'Dense Prediction'], 'abstract': 'This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope\nwith various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models’ applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset.', 'one-sentence_summary': 'A versatile MLP-like architecture for both recognition and dense prediction.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'chen|cyclemlp_a_mlplike_architecture_for_dense_prediction', 'pdf': '/pdf/0ff0f728cbc430b36ea84288793e887e216cff59.pdf', 'supplementary_material': '/attachment/fd8f09073416a32a523ca794fda894d24f943625.zip', 'code': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/cyclemlp-a-mlp-like-architecture-for-dense/code)', '_bibtex': '@inproceedings{\nchen2022cyclemlp,\ntitle={Cycle{MLP}: A {MLP}-like Architecture for Dense Prediction},\nauthor={Shoufa Chen and Enze Xie and Chongjian GE and Runjian Chen and Ding Liang and Ping Luo},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=NMEceG4v69Y}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Benjamin Eysenbach', 'Ruslan Salakhutdinov', 'Sergey Levine']",ICLR,The Information Geometry of Unsupervised Reinforcement Learning,https://iclr.cc/virtual/2022/oral/6207,2022," How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=3wU2UX0voE,https://openreview.net/forum?id=3wU2UX0voE,3wU2UX0voE,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Strong submission that analyses the unsupervised skill discovery setting from the perspective of information geometry, which leads to some interesting conclusions. In particular, it is shown that this does not lead to skills that are optimal for all reward functions, but does provide a good initialization for methods that aim to find optimal policies.\n \nAcross the board, the reviewers believe the analysis provided by this work is both important and novel. And while there were some initial concerns raised, such as lack of empirical confirmation of some of the claims and some questions about the analysis, the authors have addressed all of these concerns convincingly.\n \nHence, I strongly recommend acceptance of this submission.'}, {'summary_of_the_paper': 'This paper is trying to analyze whether unsupervised skill discovery is useful for more easily solving any possible downstream tasks in an MDP. It does so by adapting the idea of the value function polytope to a state visitation distribution polytope. It also specifies possible reward functions in this geometric setting and analyzes the connection between points on this polytope and returns with respect to the reward function.\n\nNext, it casts the mutual information based skill discovery problem in this geometric space and tries to analyze the skills learned. From their analysis, the paper suggests ways to infer how many skills can be learned, and whether those skills are optimal with respect to some downstream tasks.\n\nIt seems that these skills can be guaranteed to be the vertex of the above polytope, but not to be optimal with respect to all downstream reward functions. They then suggest that the skills learned might be useful for an adaptation procedure that ignores the dynamics of the environment.', 'main_review': '## Strengths:\n* This paper attempts to answer the fundamental question of why RL practitioners should care about unsupervised skill discovery. Providing a satisfying answer to this question should attract more attention to learning skills in this setting, as well as test and clarify the fundamentals on which the problem is based.\n* The high level results this paper presents also seem interesting.\n* Adapting the value function polytope to a polytope over state visitation frequencies and analyzing the problem from a geometric perspective is also a novel approach that deserves to be studied further.\n* The reframing of the unsupervised skill discovery problem as a vertex discovery problem is very interesting.\n\n## Weaknesses:\n* I am not completely convinced of the correctness of all the results. The paper introduces a lot of tools for its analysis. Specifically, the state marginal distribution polytope is an entirely new concept and its use in the analysis in this paper was nontrivial to follow. My questions in this regard are below.\n* The adaptation procedure that this paper refers to when it asserts that the mutual information maximizing skills are optimal initializations ignores the dynamics of the environment. As such, I am not sure if this adaptation procedure should even be considered feasible. The paper should reframe its statement to make the _strong_ assumption on this adaptation clear, rather than merely stating that the initialization is optimal ""under some assumption about how the adaptation is performed"".\n\n## Detailed Comments and Questions:\n* The problem setup specifies $\\gamma \\in [0, 1]$. Doesn\'t $\\gamma=1$ not work for the state marginal distribution and the subsequent RL objective?\n* The value function polytope is a closely related to the value function polytope approach (Dadashi et al., 2019). This related work seems almost hidden in section 2, and the contrast between the two approaches beyond the fact that one looks at value functions and the other at state marginals is not clarified. I felt like a paragraph clarifying the difference so that a reader familiar with that work might be able to grasp the state visitation polytope better would be useful. \n* In the 3-state MDP example considered for this paper, the start state distribution and the dynamics of the environment are not illustrated sufficiently. Does the agent start at all three states with equal probability?\n* When analysing this polytope in terms of state-only reward functions, does the origin indicate reward functions that do not prefer any particular state over the other? Are the rewards adjusted to be positive, so that they lie in this quadrant that is being analyzed?\n* In general, section 5 seems very condensed. I understand that the limits of the paper might be forcing the authors to be succinct, but I come away from this section feeling like I haven\'t completely understood the geometric view the paper is utilizing.\n* In Section 6, the notion of policies being closer or further in terms of the KL divergence is mentioned when analyzing the skills learned via mutual information maximization. I\'m having a hard time wrapping my head around this concept, and perhaps it stems from not grasping the polytope in the previous section completely. Do points further away from each other on the polytope have higher a higher value of KL divergence? But if it was possible for two policies to spend all their time at individual vertices of the outer triangle (if the dynamics of the MDP allowed it), wouldn\'t the KL divergence be infinite? The notion of treating distance in this polytope in terms of KL divergence is something that I am not convinced of, and leads to me being unconvinced of the rest of the results of this paper.\n* In Lemma 6.2, does the probability of sampling a skill ($p(z)$) have to be uniform? I am unsure how skills with different probabilities could lead to state marginals that have the same KL divergence from the average. Do the skills with lower probability of being sampled travel farther?\n* For theorem 6.4, if the agent is able to learn $|\\mathcal{S}|$ skills that each maximize the time spent at individual states, then one of these skills should maximize the return for any state based reward function. What am I missing here? Why do we need to consider all $|\\mathcal{A}|^|\\mathcal{S}|$ policies?\n* On page 7, after the proof of Lemma 6.3, the paper mentions a regularity assumption. What is this assumption?\n* ICLR allows 10 pages of content this year. Given the density of Section 5, I feel like this paper wasted some space that could have been used for some clarifying exposition on the geometric view that the paper proposes.\n\n## Other Comments:\n* After Equation (3): ""... can be written is the  average state ..."", _is_ should be _as_.\n* Section 6.3: ""Maximizing mutual information pushes skills away from one another, not as finding a skill or policy that is close to every other policy"". I\'m not sure what this sentence is trying to say. It is hard to parse.', 'summary_of_the_review': 'The problems that this paper attempts to solve are interesting and would add value to the setting of unsupervised skill discovery. Specifically, it considers the question of what the objective of such skill discovery should be.\nHowever, the geometric tools introduced in the paper and the subsequent analysis does not seem polished enough and is hard to follow. I am unconvinced of the analysis and thus scoring the paper conservatively.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors provide an in-depth study of unsupervised skill discovery from the RL literature. In particular, the authors analyze such approaches from the perspective of ""information geometry,"" and show that these algorithms _can not_ learn skills that are optimal for all possible reward functions, but _can_ provide a good initialization for online learning approaches that seek to adapt the initial skills to find optimal policies for new reward functions.', 'main_review': ""STRENGTHS\n\n(S1) The paper provides a very nice analysis for a growing class of methods that seek to solve a problem of growing interest in the community. The results are interesting--they say what unsupervised skill discovery methods _can_ and _can not_ do--and the discussion is timely as more and more researchers start to work in the space of unsupervised reinforcement learning.\n\n(S2) The geometric perspective presented is intuitive and does a nice job of communicating the main results of the paper.\n\nWEAKNESSES\n\n(W1) The paper lacks a good empirical confirmation of the claims made here. For example, if unsupervised skill discovery indeed hits the theoretical upper bound that the authors claim, shouldn't there be even a very small toy domain on which the authors could do some experiments to validate that claim? That said, I understand the contribution of the paper here is mostly theoretical in nature."", 'summary_of_the_review': 'Overall, I really like the paper and think that the results are important and should be presented to the community. However, I do think the paper could be improved with even a small empirical study.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper treats the problem of unsupervised RL, which it defines as the problem of pretraining a system, without having access to a reward function, to learn a collection of policies, that are labeled skills. The idea is that when the reward function is presented, the target policy can be assembled as a combination or composition of these skills. Such policies had been previously proposed to be learned using mutual information maximization approaches. \n\nThe primary contribution of this paper that it analyzes the space of policies/skills that are learnable using mutual information based approaches. The authors present this analysis through a geometric lens on the probability simplex of the possible states of the system. A given policy or skill can be associated to a point in this space by associating with a skill or policy its discounted state probability distribution. \n\nUsing these analytical tools, the authors prove several interesting results. For instance, they show that by maximizing mutual information alone, one cannot learn sufficient skills to cover all the set of optimal policies - the number of unique skills learned through this approach is bounded by the dimensionality of the state space. ', 'main_review': 'Overall, the paper presents an insightful analysis of some of the challenges of unsupervised RL. In general, I believe that the primary impact of this paper would be through its geometric approach of analyzing the space of learnable policies. The concrete theorems proven by the paper, while interesting, appear to have a relatively large set of conditions. It would be interesting to see how the analysis can be applied to more general settings.', 'summary_of_the_review': 'The paper analyzes some of the challenges of unsupervised RL through a geometric approach of analyzing the space of learnable policies. The approach allows insights about the characteristics and certain optimality criteria that applies to skills learned through mutual information type approaches. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'The Information Geometry of Unsupervised Reinforcement Learning', 'authorids': ['~Benjamin_Eysenbach1', '~Ruslan_Salakhutdinov1', '~Sergey_Levine1'], 'authors': ['Benjamin Eysenbach', 'Ruslan Salakhutdinov', 'Sergey Levine'], 'keywords': ['unsupervised skill learning', 'reward-free RL', 'mutual information', 'DIAYN'], 'abstract': 'How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods.', 'one-sentence_summary': 'We show that mutual information skill learning is optimal in one sense but not optimal in another sense.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'eysenbach|the_information_geometry_of_unsupervised_reinforcement_learning', 'pdf': '/pdf/4709236cdf10497a057511e94fe99f87770c5bf6.pdf', 'supplementary_material': '/attachment/70a8d05ec738c35895783c0cb17f52610301f6df.zip', 'code': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/the-information-geometry-of-unsupervised/code)', '_bibtex': '@inproceedings{\neysenbach2022the,\ntitle={The Information Geometry of Unsupervised Reinforcement Learning},\nauthor={Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=3wU2UX0voE}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Sabri Eyuboglu', 'Maya Varma', 'Khaled Saab', 'Jean-Benoit Delbrouck', 'Christopher Lee-Messer', 'Jared Dunnmon', 'James Y Zou', 'Christopher Re']",ICLR,Domino_ Discovering Systematic Errors with Cross-Modal Embeddings,https://iclr.cc/virtual/2022/oral/6149,2022," Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data).Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=FPCMqjI0jXN,https://openreview.net/forum?id=FPCMqjI0jXN,FPCMqjI0jXN,"[{'title': 'Thanks', 'comment': ""We appreciate Reviewer HQRB's detailed feedback and have clarified the evaluation framework, Domino's applicability, and improved paper organization based on suggestions https://thatsnotmy-neighbor.io to enhance clarity and justification.""}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""Three experts reviewed this paper and all recommended acceptance. The reviewers liked that the work addressed a common problem in prior related work that it is hard to quantitatively evaluate slide discovery methods. Moreover, the proposed method achieves superior performance over prior arts. Based on the reviewers' feedback, the decision is to recommend the paper for acceptance. The reviewers did raise some valuable concerns, such as paper clarity, significance of the textual descriptions, that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes to the best of their ability. We congratulate the authors on the acceptance of their paper!""}, {'title': 'Updated scores', 'comment': 'I would like to thank the authors for addressing most of my concerns. I have adjusted my scores.'}, {'title': 'Thank you for your response', 'comment': 'I now feel more strongly towards accept. Best of luck!'}, {'title': ""Dear reviewer hQrb: we'd love to know if you have any more questions after our response"", 'comment': 'Dear reviewer hQrb\n\nThank you very much for your helpful feedback and suggestions, they helped us to improve the paper. We tried to carefully address all of your comments in our response and the updated paper. Please let us know if you have any further questions, and we are very happy to follow up!\n\nThank you for your time!'}, {'title': 'Response to Reviewer hQrb (2/2)', 'comment': '**Terms and Definitions.** \n> *There is a lack of definitions for many of the terminology... I find the term ‘slice discovery method’ misleading, it is not commonly used term in the field... The definition of the slice discovery problem (Section 2) is rather imprecise...”*\n\nWe have modified Sections 1, 2, 3, and 4 to provide additional details about the slice discovery problem setting. Specifically, we have made the following changes:\n\n- We have added a definition of the term *slice* to the Introduction.\n- We have expanded the Related Work section to provide additional context on prior slice discovery approaches.\n- We have edited Section 3 to provide a clearer description of the slice discovery problem setting.\n- The exact definition of “degraded performance” will vary by problem setting. In our experiments, we define degraded performance as a 10% drop in classification performance across examples in the slice when compared to examples outside the slice. This has been updated in Section 4.1.2.\n\nSince most slice discovery methods have been proposed only recently, prior work uses a number of terms to refer to such methods, and there is no consensus on a name. Slice discovery methods have been referred to as “slice finders” [5],\xa0 “methods for discovering systematic errors” [1], “multiaccuracy auditors” [3] and “subclass label estimation” [2]. We use the term “slice discovery” because it is concise and intuitive, and it reconciles the diverse names found in the literature. We note that the term “slice” is standard and has been used in several recent works to refer to data subgroups united by a shared characteristic [6,7]. We would appreciate a comment if there are specific ways in which Reviewer HQRB found the name misleading.\n\n**Clarification.** \n> *“It is not clear to me where the actual model predictions are trained...No precise definition is given what ""degraded"" means in this context”*\n\nWe describe the model training procedure in Section 4.1.2. The implementation of $h_\\theta$ will depend on the application setting. For our natural image and medical image settings, $h_\\theta$ takes the form of a ResNet-18 model trained across the provided dataset $D$. For our medical time-series data, predictions are generated using a Dense Inception CNN. We ensure that models exhibit degraded performance across the specified slice, which we define as at least a 10% drop in classification performance across examples in the slice when compared to examples outside the slice (the performance metric we use is either recall or AUROC depending on the dataset and task).\n\n**Clarification**\n> *""After reading the definition of the slice discovery problem (Section 2), I am still not clear whether a slice can refer to whole images only or part of the images (or frames in a video).""*\n\nThe term *slice* refers to a group of data examples united by a shared attribute (see section 3). For example, if we consider the CelebA dataset which includes images of celebrity faces, some examples of data slices include images with blonde hair, individuals with beards, and individuals with earrings. We have clarified the definition of the term *slice* in the updated manuscript in Section 1.'}, {'title': 'Response to Reviewer hQrb (1/2)', 'comment': 'We thank Reviewer HQRB for reviewing our paper and providing helpful feedback on our work. We address many of Reviewer HQRB’s concerns in the general response above, and we provide additional details on specific comments below.\n\n**Justification of the evaluation framework.** \n> *“The proposed evaluation framework seems rather ad-hoc and poorly justified.”*\n\nOur evaluation framework enables the first large-scale, quantitative evaluation of slice discovery methods (SDMs). Prior works (see references [1,2,3]) either use (1) qualitative evaluations (*i.e.* subjective interpretations of discovered slices) or (2) include fewer than ten slice discovery settings, all from a single slice category. In our work, we propose a framework for programmatically generating a large number of realistic slice discovery settings. We use our framework to instantiate over 1,000 settings across diverse slice types and application domains, which we will release publicly.\n\nThe design of our evaluation framework was motivated by an extensive review of real-world slices documented in prior works (this literature review is included in Section A.1 of the revised submission). Below we provide justification for each aspect of our evaluation framework, and provide pointers to the sections in the manuscript where these design choices are discussed:\n\n- **Slice Categories.** Our framework includes three slice categories: rare slices, correlation slices, and noisy label slices. All of the slices that we found in our literature review fall into one of these three categories (see Section A.1). Prior work on hidden stratification highlights a similar set of slice categories [4]. Other slice categories could easily be incorporated into the framework in future work. Additionally, within each category, we generate slices with varying strengths in order to reflect the diversity of slices likely to occur in real-world datasets. This is discussed in further detail in Section 4.1.1.\n- **Metrics.** In order to evaluate the quality of a predicted slice, practitioners typically analyze the top-$k$ examples in the slice [1,2,3]. Our primary metric precision-at-$k$ aligns closely with this intended use case.\n- **Application domains.** To capture the diversity of machine learning tasks, we generate slices in three diverse domains: natural images (CelebA and ImageNet), medical images (MIMIC), and medical time-series data (EEG). Since our evaluation framework is programmable, it can be easily expanded to cover additional domains. This is discussed in further detail in Section 4.2.\n\nIf there remain aspects of the framework that feel poorly justified to Reviewer HQRD, we would appreciate additional feedback.\n\n**Clarification.**\n> *""The Domino approach seems limited to images with captions.""*\n\nDomino is not limited to images with captions. In our experiments, we apply Domino to ImageNet and CelebA, two datasets without captions. Domino does use *pretrained* cross-modal embeddings trained on available corpora of paired data, but it does not require the slice discovery dataset $\\mathcal{D}$ to be captioned. In many domains (*e.g.* natural images and medical images), cross-modal embeddings are readily available for download [8,9], so the average Domino user would not need access to any captioned data. In more specialized domains where pretrained cross-modal embeddings are unavailable (such as non-image inputs), paired input-text data may still be abundantly available, allowing the practitioner to train a domain-specific set of cross-modal embeddings. In our work, we utilize this procedure for our medical time-series (EEG) dataset, where we train a set of custom cross-modal embeddings that align representations of time-series signals with text from physician reports.\n\n**Paper Organization.**\n> *""The paper is written in a confusing manner.”*\n\nWe thank Reviewer HQRB for the suggestion to improve paper organization. We have modified the manuscript and refer the reviewer to the general response above, where we detail the structural changes that were made.'}, {'title': 'Response to Reviewer LuSN (2/2)', 'comment': '**Discussion and future work.**\xa0 \n> *“In the experiments section, the results are described as-is without further interpretation. Do the results with the evaluation framework indicate specific trends? If so, why? Maybe further investigation on this would provide some ideas for future exploration - which are also missing in the paper.”*\n\nWe thank the reviewer for raising this point.\xa0 Below, we include additional interpretation of trends from our experiments. We provide a detailed analysis of trends in Appendix Section A.4 in our updated manuscript, and we have updated the conclusion to include some ideas for future exploration.\n\nIn Section 6.1, we explore the effect of embedding type on slice discovery performance:\n\n1. We find that uni-modal embeddings (BiT, ImageNet, trained activations of $h_\\theta$) are particularly effective for correlation slices, exhibiting only a small decrease in performance when compared to cross-modal embeddings. This makes sense because a model that relies on a spurious correlate to make predictions will likely capture information about the correlate in its activations.\n2. We note that slice discovery performance is generally lower across rare slices when compared to correlation and noisy label slices. This trend is visible for both model types (synthetic and trained) and all embedding types (unimodal and cross-modal). This is likely due to the nature of the rare slice setting; since a rare subclass occurs in the dataset with very low frequency, it is difficult for SDMs to identify the error slice.\n3. Slice discovery performance on synthetic models is often higher than performance on trained models. This is expected because the use of synthetic models allows us to explicitly control performance on our labeled ground-truth slices, and as a result, Domino can more effectively recover the slice. On the other hand, trained models are likely to include underperforming, coherent slices that are not labeled as “ground-truth”, which may limit the ability of Domino to recover the labeled slice. Trained models are also likely to exhibit lower slice performance degradations than synthetic models. We discuss these trade-offs in Section 4.1.2.\n\nIn Section 6.2, we explore the effect of slicing algorithms on performance:\n\n1. We note that the naive Confusion SDM demonstrates high performance on correlation slices across all three datasets, even outperforming our error-aware mixture model in some cases. This finding suggests that when a strong correlation exists, simply inspecting the confusion matrix may be sufficient for effective slice discovery.\n2. Our error-aware mixture model demonstrates significantly higher performance on rare slices than prior SDMs; this is especially visible in trained model results. This is likely because our error-aware mixture model jointly models the input embeddings, class labels, and model predictions, allowing for better identification of rare slices when compared to existing methods.\n\nWe also outline some directions for future work:\n\n- Slice discovery is particularly difficult when the strength of the slice ($\\alpha$) is low. In the future, we aim to explore strategies for improving slice discovery and explanation generation in this scenario.\n- We hope to explore strategies for generating informative input embeddings when access to paired input-text data is limited.\n- In our response to Reviewer si46, we provide a preliminary user evaluation in order to determine if generated slice descriptions are actionable. We plan to expand this in the future with a controlled user study in order to better understand when explanations are actionable.'}, {'title': 'Response to Reviewer LuSN (1/2)', 'comment': 'We thank Reviewer LuSN for their positive comments and for providing thoughtful feedback on our work. We address many of Reviewer LuSN’s comments in our general response above, and we provide additional details on specific comments below.\n\n**Paper Organization.** \n>*""Though the idea introduced in the paper is quite interesting, the paper itself is organised in a very confusing manner. Related work is only introduced in section 6 on page 9 and seems incomplete. The Domino method is shown in Figure 1 on page 2 but the actual text describing it is on page 6.""*\n\nWe appreciate the reviewer’s thoughtful suggestions for improving the organization of our paper. We refer Reviewer LuSN to our general response where we describe a significant reorganization of the paper.\n\n**Technical Novelty.** \n> *""Though the framework is novel, the individual models are not. Given the new evaluation framework, it would have been great to introduce some further technical novelty.”*\n\nWe thank Reviewer LUSN for noting the novelty of our evaluation framework. We clarify that our manuscript does introduce a novel technical method for performing slice discovery. Specifically, we introduce Domino, which is an SDM with the following novel technical contributions:\n\n1. ***Embed:*** Motivated by the insight that cross-modal embeddings produce semantically meaningful representations, we leverage cross-modal embeddings to discover coherent slices. Our work is the first to use cross-modal embeddings for slice discovery and demonstrate empirically that they outperform uni-modal embeddings at this task. Additionally, while cross-modal embeddings have been trained for natural images and medical images, to the best of our knowledge our work is the first to train cross-modal embeddings for continuous medical time-series (EEG) data.\n2. ***Slice:*** We propose a novel error-aware mixture model that jointly models the input embeddings, class labels, and model predictions to find slices in embedding space. We demonstrate empirically that it outperforms existing methods by 12 percentage points via detailed ablation studies. To the best of our knowledge, such a mixture model has not previously been proposed.\n3. ***Describe:*** Our approach to generating natural language descriptions for discovered slices is novel. Prior work has not introduced any other methods for describing discovered slices with natural language.\n\nTo summarize, in addition to our evaluation framework, we propose a slice discovery method that leverages multiple novel components to effectively identify and describe slices.\n\n**Framing language generation as a retrieval task.**\n> *""The part about generating natural language explanation is not as convincing as it\'s ultimately using the text embeddings on a large corpus in a retrieval setting...Currently the text seems to be restricted to single words from Fig 5.”*\n\nAlthough the version of Figure 5 included in our original submission included text descriptions consisting of only one word, our approach can also be used to generate full-sentence descriptions, which we demonstrate in the updated manuscript with a set of new experiments. To do so, we apply pre-trained masked-language models to domain-specific templates in order to generate a corpus of candidate full-sentence descriptions. For example, when using the CelebA dataset, we use pre-trained BERT to generate thousands of candidate descriptions by filling in templates of the form “A photo of a person [MASK] [MASK] [MASK].”\xa0 We opted for this approach rather than unconstrained generation because it affords the practitioner greater control over the generation process due to the predefined templates. In our updated manuscript, we have added Section A.3.2, which describes this approach in detail.\n\nFor our medical applications, we also generate full-sentence natural language descriptions using a corpus of complete physician reports. Descriptions accurately characterize identified slices. Examples are displayed in Section A.4.3.\n\nWe agree with the reviewer that exploring the feasibility of unconstrained generation from the embeddings would be an interesting direction to explore in future work.'}, {'title': 'Response to Reviewer si46', 'comment': 'We thank Reviewer si46 for positive comments and helpful feedback on our work. We address many of Reviewer si46’s suggestions in the general response above and respond to specific comments below.\n\n**Paper Organization.**\n>*""I would rather the literature review section be in the intro rather than in the conclusion for flow.”*\n\nWe thank Reviewer si46 for the suggestion to move the literature review section and expand the conclusion. We have moved the literature review to Section 2 and modified the manuscript. We refer the reviewer to the general response above, where we detail the structural changes that were made.\n\n**Actionable Natural Language Descriptions.** \n> “*Are textual descriptions of the slices actually actionable for real life experts?”*\n\nWe thank Reviewer si46 for this question. We believe a thorough user study where we provide textual descriptions to experts for interpretation would be a promising direction for future work. Although conducting such an analysis is out of scope for this study, we provide a preliminary version below that seems promising:\n\nWe displayed the top 10 EEG slice descriptions generated by Domino to a board-certified neurologist who regularly administers EEGs. We selected a correlation slice setting, where the presence of seizures (target variable) was strongly correlated (alpha=0.8) with young patients less than 1 year in age (slice). We asked the neurologist two questions: (1) can you identify a coherent concept that is shared among the 10 descriptions? (2) What actions would you take after being aware of this systematic error?\n\n1. *Finding a coherent concept*: The neurologist explained that the sentences are mainly describing patients that previously had, currently have, or are likely to develop seizures, where a specific seizure type (focal) was more prominent. He also found that the majority of the descriptions make it clear that the patients are young.\n2. *Taking actions*. The neurologist explained that they would expect the bias caused by age to be reflected in how the model interprets the EEG signal, since younger patients exhibit higher amplitudes and slower waveforms than adults. Moreover, the neurologist would expect the model to perform differently on specific seizure types since older patients have more focal seizures, while pediatric patients might have more myoclonic, migraine, and genetic seizures. As a result, the neurologist would take action to further evaluate model performance on seizure subtypes and sample more EEGs from older patients.\n\nIn the future, we hope to conduct controlled user studies to directly test the real-world effectiveness of Domino. We believe a comprehensive, well-executed user study to be out of scope for this paper.\n\nWe again thank Reviewer si46 for their review of our manuscript, and we hope that the above responses adequately address all concerns.'}, {'title': 'Response to all reviewers (2/2)', 'comment': 'In summary, as machine learning models become increasingly widespread in use, slice discovery will become critical for building robust models, especially in safety-critical settings where the presence of hidden underperforming slices could result in significant consequences. We hope that our evaluation framework will facilitate the development of effective systems for this task and that future work will build on our proposed method, Domino.\n\nWe would again like to thank all reviewers for their time and feedback, and we hope that our changes adequately address all concerns.\n\n**References**\n\n[1] Greg d’Eon, Jason d’Eon, James R. Wright, and Kevin Leyton-Brown. The spotlight: A general method for discovering systematic errors in deep learning models, 2021.\n\n[2] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No sub- ´ class left behind: Fine-grained robustness in coarse-grained classification problems. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19339–19352. Curran Associates, Inc., 2020.\n\n[3] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-Box Post-Processing for Fairness in Classification. arXiv:1805. 12317 [cs, stat], August 2018.\n\n[4] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. September 2019.\n\n[5] Chung, Yeounoh, Tim Kraska, Neoklis Polyzotis, Ki Hyun Tae, and Steven Euijong Whang. 2019. “Slice Finder: Automated Data Slicing for Model Validation.” In *2019 IEEE 35th International Conference on Data Engineering (ICDE)*, 1550–53. Ieeexplore.ieee.org.\n\n[6] Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher Re. Robustness Gym: Unifying the NLP Evaluation Landscape. ´ arXiv:2101. 04840 [cs], January 2021\n\n[7] Mayee Chen, Karan Goel, Nimit Sohoni, Fait Poms, Kayvon Fatahalian, Christopher Re. Mandoline: Model Evaluation under Distribution Shift. *International Conference on Machine Learning*, 2021.\n\n[8] Alec\xa0 Radford,\xa0 Jong\xa0 Wook\xa0 Kim,\xa0 Chris\xa0 Hallacy,\xa0 Aditya\xa0 Ramesh,\xa0 Gabriel\xa0 Goh,\xa0 Sandhini\xa0 Agarwal,\xa0 Girish\xa0 Sastry,\xa0 Amanda\xa0 Askell,\xa0 Pamela\xa0 Mishkin,\xa0 Jack\xa0 Clark,\xa0 Gretchen\xa0 Krueger,\xa0 and\xa0 Ilya Sutskever. \xa0 Learning\xa0 transferable\xa0 visual\xa0 models\xa0 from\xa0 natural\xa0 language\xa0 supervision. \xa0 February 2021.\n\n[9] Yuhao\xa0 Zhang,\xa0 Hang\xa0 Jiang,\xa0 Yasuhide\xa0 Miura,\xa0 Christopher\xa0 D.\xa0 Manning,\xa0 and\xa0 Curtis\xa0 P.\xa0 Langlotz. Contrastive\xa0 learning\xa0 of\xa0 medical\xa0 visual\xa0 representations\xa0 from\xa0 paired\xa0 images\xa0 and\xa0 text. 2020.'}, {'title': 'Response to all reviewers (1/2)', 'comment': 'We thank the reviewers for their thoughtful and constructive review of our manuscript. We were encouraged to hear the reviewers found the slice discovery problem we present to be both interesting (Reviewers LuSN, hQrb) and well-motivated (Reviewer si46) and that they view our methodology as useful (Reviewer si46), novel (Reviewer si46), and effective (Reviewers si46, LuSN). In response to feedback, we provide a general response here to points raised by multiple reviewers, individual responses below to address each reviewer’s concerns, and an updated manuscript.\n\nRegarding questions from Reviewers LuSN and hQrb about **novelty and technical contributions**, we emphasize that the two key contributions of this work are (1) designing a novel quantitative evaluation framework for rigorously evaluating slice discovery methods (SDMs), which we instantiated with over 1,000 diverse slice settings, and (2) introducing *Domino*, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to both identify slices and provide natural language explanations.\n\n1. Prior works [1,2,3] have proposed slice discovery methods for unstructured inputs, and these approaches are evaluated either qualitatively or with a small selection of tasks and slices. To the best of our knowledge, our paper is the first to introduce a large-scale quantitative evaluation framework for rigorously evaluating SDMs.\n2. Prior approaches often extract incoherent slices, where slices are not unified by a single, actionable concept. We address this by utilizing cross-modal representations (i.e. input embeddings that are aligned with textual phrases), which have not been previously used for slice discovery. Our work is also the first to generate natural language explanations for slices. We demonstrate through experiments with our evaluation framework that Domino often outperforms prior SDMs across a range of classification tasks, application domains, and slice categories.\n\nAs summarized by Reviewer si46, we perform a first-of-its-kind comparison of existing SDMs and introduce Domino, an SDM powered by a novel application of cross-modal embeddings and a novel error-aware mixture model.\n\nIn response to comments from Reviewers si46 and LuSN about **natural language descriptions**, we have updated our manuscript to include a larger, more diverse set of explanations for our natural image, medical image, and medical time-series datasets. As demonstrated in Section A.4.3, our approach can successfully generate explanations for a range of slice types and slice settings. We also ran a set of additional experiments in order to demonstrate that full-sentence descriptions can be generated with our method. We include details on these new experiments in Sections A.3.2 and A.4.3 of the revised manuscript.\n\nRegarding feedback from Reviewers LuSN, si46, and hQrb about the **organization** of our manuscript, we have made the following changes.\n\n- Introduction (Section 1): We have added definitions for some terminology and clarified our problem setting.\n- Figure 1 (Section 1): Figure 1 provides an overview of our two key contributions: (1) the evaluation framework, and (2) Domino, our novel SDM. We have edited the figure and caption to better communicate our contributions.\n- Related Work (Section 2): We have significantly expanded our literature review section to provide additional background on slices, the slice discovery task, and cross-modal embeddings, and we have moved the related work to Section 2 as suggested.\n- Slice Discovery Preliminaries (Section 3): We have modified the formal definition of the slice discovery task in order to improve clarity.\n- Slice Discovery Evaluation Framework (Section 4): We have added additional details to the slice category descriptions in 4.1.1 and added further justification for our evaluation framework.\n- Domino (Section 5): We have updated section 5.1 to clarify assumptions under which paired input-text data is necessary. We have also updated section 5.3 and added Appendix Sections A.3.2 and A.4.3 to provide details on generating sentence-level explanations.\n- Experiments (Section 6): We have added additional information on experiments, observed trends, and explanations in Appendix Section A.4.\n- Conclusion (Section 7): We have updated the conclusion to include directions for future work. We also provide an extended discussion on future work in Appendix Section A.4.4.\n- Appendix: We have updated the appendix to include a survey of slices in the wild (A.1), additional details on generating natural language descriptions (A.3.2), extended evaluation on the trends observed in Section 6 (A.4.1 and A.4.2), an extended analysis of the natural language descriptions generated by Domino (A.4.3), and possible directions for future work (A.4.4).'}, {'summary_of_the_paper': 'Recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices, or important subgroups of data, on which a model performs poorly.\n\nAn ideal SDM should automatically identify: \n1. Slices that contain examples on which the model underperforms, or has a high error rate.\n2. Slices that contain examples that are coherent, or align closely with a human-understandable concept.\n\nThis is difficult because:\n1: No quantitative evaluation framework exists for measuring performance of SDMs; Existing SDM evaluations are either qualitative, performed on synthetic data, or consider only a small subset.\n2. Prior qualitative evaluations have demonstrated that existing SDMs often identify slices that are incoherent, even though they may satisfy the first ideal case.\n\nDomino:\nThe authors preset Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices using natural language descriptions. The proposed method could also quantitatively compare SDMs, which has not been done before.\n\nStep 1. Embed: Encode inputs in a cross-modal embedding space with a function.\n\nStep 2. Slice:  identify underperforming regions in the cross-modal embedding space using an error-aware mixture model fit on the input embeddings, model predictions, true class labels using expectation maximization. I.e. input embeddings, class labels, and model predictions as independent based on slice. \n\nStep 3. Describe: Use the text embedding function ψtext learned in step (1) to generate a set of k natural language descriptions of the discovered slice.\n\nEvaluation Approach of 3 popular slide types:\nRare slice: To generate settings with rare slices, Construct a skewed dataset such that for a given class label Y, elements in subclass C occur with proportion α, where 0.01 < α < 0.1.\nCorrelation slice: Construct a dataset such that a linear correlation α exists between the target variable and other class labels, where 0.2 < α < 0.8.\nNoisy label slice. Construct dataset such that for each given class label Y, the elements in subclass C exhibit label noise with probability α, where 0.01 < α < 0.3.\n\nExperiments show that when cross-modal embeddings are provided as input, the error-aware mixture model often outperforms previously-designed SDMs.', 'main_review': 'Pros:\n\nI believe that there is well backed motivation for work based off of the plentiful literature review.\nThere is novel integration of CLIP as well as SDMs previously not combined before\nA variety of datasets seem to show that the proposed method is useful.\nCode is available for reproducibility.\n\nCons:\n\nI would rather the the literature review section be in the intro rather than in the conclusion for flow. Then, the authors could summarize their work in the conclusion instead.\n\nAre textual descriptions of the Slices actually actionable for real life experts? I would like if physicians found the textual descriptions of the MIMIC and EEG dataset slices useful.\n', 'summary_of_the_review': 'I would tend to accept this paper as it is novel enough and supported by empirical experiments.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper introduces a new approach for slice discovery by leveraging the advances in cross-modal embeddings. The paper also suggests a new evaluation framework to quantitively evaluate SDMs. ', 'main_review': ""The paper addresses interesting problems in the area of slice discovery, particularly underperforming clusters. Overall the Domino approach seems to work well for this task. There are ablation studies on the embeddings used and clustering algorithm chosen. The part about generating natural language explanation is not as convincing as it's ultimately using the text embeddings on a large corpus in a retrieval setting. Actually generating text from the given slice embeddings would have been more convincing. Currently the text seems to be restricted to single words from Fig 5. \nThough the idea introduced in the paper is quite interesting, the paper itself is organised in a very confusing manner. Related work is only introduced in section 6 on page 9 and seems incomplete. The Domino method is shown in Figure 1 on page 2 but the actual text describing it is on page 6. Another weakness is the novelty. Though the framework is novel, the individual models are not. Given the new evaluation framework, it would have been great to introduce some further technical novelty. \nIn the experiments section, the results are described as-is without further interpretation. Do the results with the evaluation framework indicate specific trends? If so, why? Maybe further investigation on this would provide some ideas for future exploration - which are also missing in the paper. "", 'summary_of_the_review': 'The paper introduces novel frameworks to evaluate and perform slice discovery. Though technical novelty in the individual parts are limited, the overall framework seem to be interesting and perform well according to the experimental results. The understanding of the paper suffers from the way how it is organised and should be further improved. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper propose a framework for identifying on which subsets of data machine learning models make systematic errors. The problem is cast in two parts: (1) identify a model that can be identify a subset of data and predict degraded performance of the machine learning model for this subset and (2) ensure that the identified subset is ""coherent"". The framework is evaluated on a number of classification tasks in computer vision and medicine.', 'main_review': 'General comments:\n\nThe paper addresses the problem of identifying on which subsets of data machine learning models make systematic errors. The authors term this as a ""slice discovery method"" (SDM). Two desirable properties for a SDM are outlined: First, providing a quantitative evaluation framework for measuring performance of SDMs and secondly, ensuring a solution that is ""coherent"". Here coherence is defined as being understandable by a domain experts, The proposed evaluation framework seems rather ad-hoc and poorly justified. The main idea, the DOMINO approach seems limited to images with captions. In the approach images and captions are separately embedded while preserving their similarity. This is followed by a mixture model which identifies errors in the model predictions, but it is not clear to me where the actual model predictions are trained. Also, the paper is often written in a confusing manner that makes it difficult to follow and understand the the contributions of the paper. Furthermore there is a lack of definitions for many of the terminology used in the paper.\n\nSpecific comments:\n\n- I find the term ""slice discovery method"" misleading, it is not commonly used term in this field.\n- The definition of the slice discovery problem (section 2) is rather imprecise and uses formulations such as ""exhibits degraded performance"". No precise definition is given what ""degraded"" means in this context. \n- After reading the definition of the slice discovery problem (section 2), I am still not clear whether a slice can refer to whole images only or part of the images (or frames in a video).\n', 'summary_of_the_review': 'The paper addresses an interesting problem but is hampered by the ad-hoc nature of the approach and the lack of clarity in the problem formulation and writing.', 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'n/a', 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Domino: Discovering Systematic Errors with Cross-Modal Embeddings', 'authorids': ['~Sabri_Eyuboglu1', '~Maya_Varma1', '~Khaled_Kamal_Saab1', '~Jean-Benoit_Delbrouck1', '~Christopher_Lee-Messer1', '~Jared_Dunnmon1', '~James_Zou1', '~Christopher_Re1'], 'authors': ['Sabri Eyuboglu', 'Maya Varma', 'Khaled Kamal Saab', 'Jean-Benoit Delbrouck', 'Christopher Lee-Messer', 'Jared Dunnmon', 'James Zou', 'Christopher Re'], 'keywords': ['robustness', 'subgroup analysis', 'error analysis', 'multimodal', 'slice discovery'], 'abstract': 'Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data).\nThen, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'eyuboglu|domino_discovering_systematic_errors_with_crossmodal_embeddings', 'pdf': '/pdf/a5ca838a35d810400cfa090453cd85abe02ab6b0.pdf', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/domino-discovering-systematic-errors-with/code)', '_bibtex': '@inproceedings{\neyuboglu2022domino,\ntitle={Domino: Discovering Systematic Errors with Cross-Modal Embeddings},\nauthor={Sabri Eyuboglu and Maya Varma and Khaled Kamal Saab and Jean-Benoit Delbrouck and Christopher Lee-Messer and Jared Dunnmon and James Zou and Christopher Re},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=FPCMqjI0jXN}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Minkai Xu', 'Lantao Yu', 'Yang Song', 'Chence Shi', 'Stefano Ermon', 'Jian Tang']",ICLR,GeoDiff_ A Geometric Diffusion Model for Molecular Conformation Generation,https://iclr.cc/virtual/2022/oral/7029,2022," Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=PzcvxEMzvQC,https://openreview.net/forum?id=PzcvxEMzvQC,PzcvxEMzvQC,"[{'title': 'Got it. Thanks.', 'comment': 'Got it. Thanks.'}, {'title': 'Yes you are right', 'comment': ""Yes, you are right. Since every $T_g$ can be represented by a SO(3) matrix and thus det($T_g$) is well-defined. In our work, | det ($T_g^{-1}$) | is manifest to be 1 so this is omitted in our proof.\n\nMore generally, let's consider the case $T_g$ be a general representation of any group, actually we always have the similar conclusion. Let p(x) still be an invariant density:\n\n$$\n1 =\\int p(x) d x \n=\\int \\rho(T_g(y))|\\operatorname{det} T_g| d y \n=\\int \\rho(y)|\\operatorname{det} T_g| d y \n=|\\operatorname{det} T_g| \\underbrace{\\int \\rho(y) d y}_{=1} \n=|\\operatorname{det} T_g|\n$$\n\nHope this could address your questions!""}, {'title': 'Regarding the proof of Proposition 1', 'comment': 'Thank you for nice work. There seems to be an integration-by-substitution step in the proof of Proposition 1.\n\nIn the first line of Equ. (14), some new random variables (T_g(x_0), T_g(x_1), ..., T_g(x_T)) are introduced.  Correspondingly, there should be a volume change |det (T_g)^{-1}| multiplying each infinitesimal d x_i in the integration. But in this work, T_g is a rotation matrix, so |det (T_g)^{-1}| happens to be 1. Therefore, although the above-mentioned volume change factor is omitted in the proof, the coclusion still holds. \n\nIs my understanding correct?'}, {'title': 'Thank you for your kind comments!', 'comment': 'Hi Octavian-Eugen,\n\nThanks for your comments here! We will follow your suggestion and rewrite the part about the concurrent work GeoMol in the final version!! The Jupyter notebook clearly shows the data was not filtered much by the dihedral pattern.\n\nIn our experiments, we just observed a fraction of filtering in the data-loader and thus make the statement. You clearly show that this is not because of the pattern matching, and we will re-study this observation carefully.\n\nGeoMol indeed presents a great contribution to this community. Thanks again for your kind comment!'}, {'title': 'Regarding excluding the SOTA baseline GeoMol (Ganea et al, NeurIPS 2021)', 'comment': 'First of all, congratulations for the acceptance of your great work!\n\nWe would like to reply to your decision of not including the SOTA baseline GeoMol (Ganea et al, NeurIPS 2021) among your baselines. We noted your reason stated below that GeoMol uses the dihedral pattern \'[ * ] ~ [ * ] ~ [ * ] ~ [ * ]\') which you claim to ""filter out about one-third of the dataset"".  We show that the percentage of errors from GeoMol is, in fact, below 0.7%. First, the above pattern is simply saying that the molecule must have at least 4 atoms, 4 bonds and have one rotatable bond, otherwise the molecule is almost trivial to predict even without any ML model. Second, we added the following jupyter notebook [1] to our repo to count the actual number of SMILES that cannot be processed by GeoMol from both GEOM-QM9 and GEOM-DRUGS. As you can see, GeoMol cannot generate conformes for only 62 out of 10K random QM9 SMILES (sampled randomly from the full 133258 sized dataset), and for only 1 out of 10K random DRUGS SMILES (randomly from the full 304466 sized dataset).\n\nWe kindly ask you to correct your main paper\'s text regarding the reason of not using GeoMol as a valid baseline, or, to add GeoMol as a baseline. We hope that future research papers will use both GeoDiff and GeoMol as strong baselines to improve upon! Thanks for your understanding.\n\n[1] https://github.com/PattanaikL/GeoMol/blob/main/count_geomol_failures.ipynb '}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""The authors focus on the conditional generation of molecular conformations (i.e. 3D cartesian atom positions) from a given molecular graph. They formulate the generation via diffusion probabilistic models.  Conformations are generated by a reverse diffusion process from isotropic Gaussian noise to molecular conformations. This diffusion process is learned from data using a SE(3) invariant formulation of the diffusion process. The authors work directly with atomic positions (i.e. a point cloud) instead of interatomic distances or an intermediate bond geometry representation. Experimental evaluations show state-of-the-art results according to COV/MAT metrics on GEOM-Drugs and GEOM-QM9 datasets.\n\nStrengths:\n\n- High technical novelty: first generative model for molecular conformation generation based on a diffusion framework\n- Very clearly written paper.\n- Impressive empirical results with state-of-the-art results on GEOM-Drugs and GEOM-QM9 datasets.\n\nWeaknesses:\n\n- Most of the weaknesses reported by the reviewers seem to have been addressed in the rebuttal.\n\nThe idea of the work is highly novel. The authors propose the first generative model for molecular conformation generation based on a diffusion framework. This paper brings together recent ideas and methods (e.g. diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. All the reviewers agree on acceptance with high scores. I recommend the authors to look at the reviewers' comments to improve the paper for the camera-ready version""}, {'title': 'Thank you for answering questions and providing additional experiments.', 'comment': 'The authors clarified all unclear parts, accelerated the sampling process, and provided an estimate of the sampling time. I will raise my score from 6 to 8.'}, {'title': 'General response to all reviewers and public readers', 'comment': 'We would like to thank all the reviewers and public readers for your constructive feedback. We’ve updated the paper according to your suggestions. More specifically, we have made the following changes:\n\n- We add several additional contents to the paper, including extra experiments and related works. We have also added several additional experiment details. These contents are all marked in blue color. Due to the page limits, most contents are added to the Appendix part.\n- We have updated several typos.\n\n*We hope the above response and the updated draft could address your questions!*'}, {'title': 'Thank you very much for your comments! The response to your questions are listed below:', 'comment': 'Thank you very much for your comments! The response to your questions are listed below:\n\n**Q1: How $\\hat{\\rho}(C)$ ensures translation invariance?**\n\n- Note that, in Sec 4.2 we stated that we consider the CoM-free system. As illustrated in the “invariant density” paragraph, we will first move $C$ to zero CoM for calculating the likelihood. Therefore, structures with different translations will always be of the same probability.\n- Then, let’s consider your example. First you assume $x \\in U$. As in Appendix A.5, $U$ is defined as zero-CoM subspace. Therefore, if you have $\\hat{\\rho}(x+g)=0$, then $x+g$ must be a zero-CoM system, and therefore $\\hat{\\rho}(x)=\\hat{\\rho}(x+g)=0$.\n\n**Q2: How Markov kernels ensure translation equivariance?**\n\n- Again, we would highlight that as stated in the paper, we also always consider the CoM-free system in equivariant kernels. Therefore, just as your speculation that “the only valid case is that $C_t$ is translation-invariant”, $C_t$ indeed is translationally invariant since we always first move them to zero-CoM, and therefore we hold the equivariance. \n- Furthermore, in practice, this condition is also very easy to implement. As long as the initial structure $C_T$ is sampled as a zero-CoM system, along the way all intermediate structures $C_t$ will still enjoy a zero-CoM with the equivariant updates and therefore naturally meet the above requirement. \n\n**Q3: About alignment approach.**\n\nYes, here actually we implicitly define the function and variable. You can view this as a function $f(C_t; C_0)$ that outputs a new coordinate as $\\hat{C}_0$ that is equivariant with the input $C_t$.\n\n**Q4: About chain-rule approach.**\n\n- As shown in ConfGF, when computing $\\nabla_{d^t} p(C^t|C^0)$ they just compute it as $\\nabla_{d^t} p(d^t|C^0)$. Here, in our formulation, \\$p(d^t|C^0)$ are intractable distributions so we cannot calculate the exact gradient. We just follow the convention of machine learning to take this distribution as Gaussians and compute the $L^2$ distance as gradients. \n- For the question of whether the Gaussian assumption is a good approximation, I think this is a more foundational problem of ML. This is more related to the empirical results, and for this work in practice this approximation works pretty well and leads to competitive performance.\n\n**Q5: About shapes of matrices in Eq.20.**\n\nThank you very much for your check and this is a typo! $x$ should be in $N \\cdot 3$ instead of $N \\times 3$. We have updated it in the paper. \n\n**Q6: About the theoretical analysis of the invariant Gaussian.**\n\n*Q6.1: Why $\\rho$ is defined on $R^3$ instead of $R^{3N}$?*\n\n- Thank you very much and there are some typos here. $\\rho$ should lie in $R^{3N}$. We have updated the paper and you can check it now.\n\n*Q6.2: Why $\\hat{\\rho}(x)=\\rho(x)$?*\n\n- As illustrated in the paper, we hold this equation under the condition that $x \\in U$. This is used to show that we can just calculate the likelihood of a CoM-free system by normal Gaussian. More specifically, since $||x||^2_2 = ||Qx||^2_2$, we have that ${\\rho}(x) = \\hat\\rho(Qx) = \\hat\\rho(x)$. So this equation didn’t means that $\\hat{\\rho}(x)$ and $\\rho(x)$ are two same distribution.\n\n*We hope the above response and the updated draft could address your questions!*'}, {'title': 'Thank you for your constructive comments and suggestions! The response to your questions are listed below:', 'comment': 'Thank you for your constructive comments and suggestions! The response to your questions are listed below:\n\n**Q1: The computational time for generation.**\n- Our generation time is in line with the most competitive existing method ConfGF (which also requires 5000 sampling steps), and requires around 8500 and 11500 seconds to decode the entire QM9 and Drugs test sets respectively. The sampling time varies among different molecules, so here we report the sampling time of the whole test set instead. This sampling time is around 100x times compared to more previous models as well as RDKit.\n- We admit that sampling inefficiency is a typical bottleneck for diffusion-based models. And as stated in the conclusion section, we leave it as an important future work about utilizing the most recent progress of diffusion methods [1] to accelerate the sampling process.\nFollowing this concern, we also further test an additional setting with fewer diffusion steps. Please refer to our response to your Q3 for details.\n\n**Q2: About the structures at intermediate stages.**\n\nThanks for your suggestion and we have evaluated the intermediate structures. The results are as below:\n\n| Number of steps | COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|4000 | 89.33 | 97.62 | 0.8542 | 0.8251 | 61.63 | 64.33 | 1.1709 | 1.1358|\n|3000 | 71.34 | 85.39 | 1.1055 | 1.0901 | 33.57 | 25.98 | 1.5256 | 1.4822|\n|2000 | 32.40 | 22.49 | 1.3812 | 1.3756| 9.75 | 3.78| 1.8977 | 1.8495|\n|1000 | 0.65 | 0.00 | 1.9854 | 1.9965 | 0.09 | 0.00 | 2.4904 | 2.4629|\n\nBy comparing with Tab.1, we can observe that running 4000 steps is already good enough and in the last 1000 steps (from 4000 to 5000) the performance only changes slightly. This interesting result indicates 4000 is enough to reach the desired performance, and we leave more detailed studies of the decoding process as future work.\n\n**Q3: Tests with fewer diffusion steps.**\n\nThanks for your suggestion and we tested the setting with T=1000 on GEOM-Drugs. We set $\\beta_1$ as 1e-7 and $\\beta_T$ as 9e-3. The results are:\n\n|COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median)|\n|--- |--- | --- | --- | --- | --- | --- | --- |\n|82.96 | 96.29 | 0.9525 | 0.9334 | 48.27 |  46.03 | 1.3205 | 1.2724|\n\nCompared with Tab.1, we can see T=1000 shows slightly weaker results than T=5000, but already outperforms all existing methods. Note that, the most competitive baseline ConfGF also requires 5000 sampling steps, which indicates that our model can achieve better performance with fewer computational costs compared with the state-of-the-art method.\n\n**Q4: What if the training data are not invariant?**\n\nSince our model and objective functions preserve the invariance of transformations, any arbitrary orientation will have no effect on the trained model. Therefore, even when the training data is not invariant, our model can still enjoy the invariance property.\n\n**Q5: Which threshold $\\delta$ did you use?**\n\nThanks a lot and we have missed this important information! We follow existing works (CGCF, ConfVAE and ConfGF methods mentioned in the paper) to set $\\delta=0.5$ for QM9 and $\\delta=1.25$ for drugs. We have added this to the paper.\n\n**Q6: The total number of model parameters.**\n\nOur current configuration holds 795858 parameters, which lies in the typical shape of graph neural networks for molecular geometric systems.\n\n**Q7: How is GeoDiff +FF implemented? What is GeoDiff\'s performance in property prediction without FF?**\n\nGeoDiff+FF is implemented by first drawing samples from GeoDiff as illustrated in Sec 4.4, and then further optimizing the structures with an empirical force field. Specifically, we use the API from RDKit that “from rdkit.Chem.rdForceFieldHelpers import MMFFOptimizeMolecule”.\n\n*We hope the above response and the updated draft could address your questions!*\n\n**References**: [1] Nichol, Alex, and Prafulla Dhariwal. ""Improved denoising diffusion probabilistic models."" arXiv preprint arXiv:2102.09672 (2021).\n'}, {'title': 'Thank you for your constructive comments and suggestions! The response to your questions are listed below:', 'comment': 'Thank you for your constructive comments and suggestions! The response to your questions are listed below:\n\n**Q1: About generating stable conformations.**\n\n- First of all, one could note that the GEOM dataset provides stable conformations with local minimum energy for all molecules. Therefore, by saying “generating stable conformations”, we mean our model can fit the distribution of the dataset and generate conformations close to the dataset, which is verified by experiments in Sec 5.2.\n- Besides, one should notice that the energy prediction task is not used to test “whether generated conformations are stable with low enough”, but to verify “whether the energy ensemble of generated ones is near to reference ones”. Therefore, the energy metrics are not directly related to the structure stability. \n- **For the dataset setup of energy prediction task**: We just directly follow previous work ConfGF to compare over 30 molecules of GEOM-QM9, which is a standard benchmark. We have also followed your suggestion to test on GEOM-Drugs, but the results turned out to be meaningless with extremely high variance. We think this is because small changes of large molecules can lead to a significant difference of energy ensemble, and thus not suitable for this benchmark.\n\n**Q2: Detailed explanation of GeoMol limitation.**\n\nGeoMol requires that the molecular graph should contain a special substructure of the dihedral pattern. Specifically, in the official implementation, the pattern is set with open-sourced RDKit tool as “dihedral_pattern = Chem.MolFromSmarts(\'[\\*]\\~[\\*]\\~[\\*]\\~[\\*]\')”, which filtered out about one-third of the dataset and made it incomparable with other models.\n\n**Q3: An estimation of the generation time.**\n\n- Our generation time is in line with the most competitive existing method ConfGF (which also requires 5000 sampling steps), and requires around 8500 and 11500 seconds to decode the entire QM9 and Drugs test sets respectively. The sampling time varies among different molecules, so here we report the sampling time of the whole test set instead.\n- We admit that sampling inefficiency is a typical bottleneck for diffusion-based models. And as stated in the conclusion section, we leave it as an important future work about utilizing the most recent progress of diffusion methods [1] to accelerate the sampling process.\n- Following your concern, we also test an additional setting with T=1000 on GEOM-Drugs. We set $\\beta_1$ as 1e-7 and $\\beta_T$ as 9e-3. The results are:\n|COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median)|\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 82.96 | 96.29 | 0.9525 | 0.9334 | 48.27 |  46.03 | 1.3205 | 1.2724 |\n- This can further speed up GeoDiff by 5 times. Compared with Tab.1, we can see T=1000 shows slightly weaker results than T=5000, but already outperforms all existing methods. Note that, the most competitive baseline ConfGF also requires 5000 sampling steps, which indicates that our model can achieve better performance with fewer computational costs compared with the state-of-the-art method.\n\nWe hope the above response and the updated draft could address your questions!\n\n**Reference**: [1] Nichol, Alex, and Prafulla Dhariwal. ""Improved denoising diffusion probabilistic models."" arXiv preprint arXiv:2102.09672 (2021).\n'}, {'title': 'Thank you for your constructive comments and suggestions! The response to your comments are listed below:', 'comment': 'Thank you for your constructive comments and suggestions! The response to your comments are listed below:\n\n**Q1: Not convinced about the equivariant setup and it would be nice to include a baseline without SE(3) invariance.**\n\nThanks for your suggestion, and we have designed an additional non-SE(3) equivariant baseline. We use the same networks architecture to obtain atom embeddings $h$ (as in Eq.6), and then directly output the atomic $\\epsilon$ from $h$ by MLPs. Therefore, $\\epsilon$ is non-equivariant w.r.t $C$. The numerical results on the challenging GEOM-Drugs dataset are as below:\n\n|COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median)|\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|0.0 | 0.0 | 54.8588 | 55.0188 | 0.0 | 0.0 | 634.4928 | 625.7850 |\n\nAs expected, the non-equivariant model shows extremely poor performance, which is in line with the observation in previous works [1]. \n\n**Q2: How to select the hyper-parameters. How about lower diffusion steps T like 1000.**\n\n- The majority of hyper-parameters ($T$, $\\tau$, batch size, and training iters) are selected by following the previous work ConfGF. For the beta scheduler, our strategy is to ensure the variances from 0 to T go exponentially from around 0.01 to 10, which has also shown effectiveness in ConfGF. \n- The most competitive baseline ConfGF also uses 5000 sampling steps, where they set 50 noise scales and conduct 100 optimization steps for each noise scale. Therefore we also choose T=5000. We have followed your suggestion to test the setting with T=1000 on GEOM-Drugs. We set $\\beta_1$ as 1e-7 and $\\beta_T$ as 9e-3. The results are:\n|COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median)|\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 82.96 | 96.29 | 0.9525 | 0.9334 | 48.27 |  46.03 | 1.3205 | 1.2724 |\n- Compared with Tab.1, we can see when T=1000, though slightly weaker than T=5000, it already outperforms all existing methods. Note that, the most competitive baseline ConfGF also requires 5000 sampling steps, which indicates that our model can achieve better performance with fewer computational costs compared with the state-of-the-art method.\n\n**Q3: About the generation time.**\n\nOur generation time is in line with the most competitive existing method ConfGF, and it requires around 8500 and 11500 seconds to decode QM9 and Drugs test sets respectively. This is a typical bottleneck for diffusion-based models. And as stated in the conclusion section, we leave it as an important future work about utilizing the most recent progress of diffusion methods [2] to accelerate the sampling process.\n\n**Q4: About other methods on Tab.4.**\n\nThis table mainly aims to compare GeoDiff with RDKit, and thus we omit other models. We have additionally tested the performance of several other methods. The results are as follows:\n\n|Method | COV-R(Mean) | COV-R(Median) | MAT-R(Mean) | MAT-R(Median) | COV-P(Mean) | COV-P(Median) | MAT-P(Mean) | MAT-P(Median)|\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|GraphDG+FF | 90.71 | 100.00 | 0.8839 | 0.8514 | 57.21 | 60.81 | 1.4549 | 1.4268|\n|CGCF+FF | 90.56 | 100.00 | 0.7827 | 0.7573 | 74.06 | 81.88 | 1.1736 | 1.1230|\n|ConfVAE+FF | 91.43 | 100.00 | 0.7749 | 0.7575 | 75.74 | 83.32 | 1.1391 | 1.1007|\n|ConfGF+FF | 92.54 | 100.00 | 0.7696 | 0.7528 | 76.34 | 85.86 | 1.1374 | 1.0886|\n\nCompared with the results in Tab.3,  we can see that when further optimized with FF, some baselines can achieve similar performance considering the Recall (-R) metrics, but still behind GeoDiff with a large margin for the Precision (-P) metrics.\n\n**Q5: Mention related work for the point cloud generation.**\n\nThanks for the suggestion. We agree that the problem is related and have added them now. We would highlight that for existing point cloud generative models,  typically the data is not in graph structures with atom and bond information, and the equivariance is not considered, making them fundamentally different from our method.\n\n**Q6: What are sigmas?**\n\nIn this work we set $\\sigma_t = 1 - \\frac{\\alpha_t}{\\alpha_{t-1}}$.\n\n------------------------------------------------------------\n\nWe hope the above response and the updated draft could address your questions!\n\n**References:**\n\n- [1] Mansimov, Elman, et al. ""Molecular geometry prediction using a deep generative graph neural network."" Scientific reports 9.1 (2019): 1-13.\n- [2] Nichol, Alex, and Prafulla Dhariwal. ""Improved denoising diffusion probabilistic models."" arXiv preprint arXiv:2102.09672 (2021).\n'}, {'title': 'Some questions about the theoretical analysis provided in this paper', 'comment': 'Thanks for the submitted work. Here are some questions about the key theoretical analysis provided in this paper. We think these following questions are necessary for authors to respond. \n\nQuestion 1:\n\nIn page 5, a claim states that “$\\hat{\\rho}(C)$ is translation-invariant”. However, for $x \\in U$, if $\\hat{\\rho}(x+g) = 0$ while $\\hat{\\rho}(x) \\not= 0$, how could $\\hat{\\rho}(C)$ be translation invariant? Looking forward to authors\' response.\n\nQuestion 2:\n\nIn page 5, for showing Markov Kernels equivariant, $\\epsilon_\\theta$ is clearly roto-translation equivariant, however, $\\mu_{\\theta}(RC+g, t) = R\\mu_{\\theta}(C, t) + g/\\sqrt{\\alpha}$ is not equivariant. So is Markov Kernel. \n\n$\\mu_{\\theta}(RC^{t}+g, t) = \\frac{1}{\\sqrt{\\alpha_{t}}}(RC^{t}+g - \\frac{\\beta_{t}}{\\sqrt{1-\\overline{\\alpha_{t}}}}\\epsilon_{\\theta}(\\mathcal{G}, RC^{t}+g, t))$\n\n$= \\frac{1}{\\sqrt{\\alpha_{t}}}(RC^{t}+g - \\frac{\\beta_{t}}{\\sqrt{1-\\overline{\\alpha_{t}}}}(R\\epsilon_{\\theta}(\\mathcal{G}, C^{t}, t)))$\n\n$= \\frac{1}{\\sqrt{\\alpha_{t}}}R(C^{t}+\\frac{g}{R} - \\frac{\\beta_{t}}{\\sqrt{1-\\overline{\\alpha_{t}}}}(\\epsilon_{\\theta}(\\mathcal{G}, C^{t}, t))))$\n\n$= \\frac{1}{\\sqrt{\\alpha_{t}}}R(C^{t} - (\\frac{\\beta_{t}}{\\sqrt{1-\\overline{\\alpha_{t}}}}\\epsilon_{\\theta}(\\mathcal{G}, C^{t}, t))) + \\frac{1}{\\sqrt{\\alpha_{t}}}g$\n\n$\\not= \\frac{1}{\\sqrt{\\alpha_{t}}}R(C^{t} - (\\frac{\\beta_{t}}{\\sqrt{1-\\overline{\\alpha_{t}}}}\\epsilon_{\\theta}(\\mathcal{G}, C^{t}, t))) + g$\n\n$= R\\mu_{\\theta}(C^{t}, t) + g$\n\nHow could $\\mu_{\\theta}$ be roto-translation equivariant? The only valid case is that $C^{t}$ is translation-invariant. However, $C^{t}$ is just coordinate. Could authors provide a formal proof of this claim?\n\nQuestion 3:\n\nIn page 6, section ""Alignment approach"", you mention that ""conformation $C^{0}$ is equivariant with $C^{t}$, the processed $\\hat{\\epsilon}$ will also enjoy equivariance"". This claim is not clear. When you mention equivariance, please also provide the function and the variable?\n\nQuestion 4:\n\nIn page 7, section ""Chain rule approach"", you mention that ""our practical implementation is to first approximately calculate $\\nabla_{d^{t}}q(C^{t}|C^{0})$ as $\\frac{d^{t}-\\sqrt{\\overline{\\alpha_{t}}}d^{0}}{1-\\overline{\\alpha_{t}}}$"". What is the exact gradient? Is the approximation error small enough? Looking forward to further clarifications. \n\nQuestion 5:\n\nBased on equation (20) in page 15, Q is in $R^{3N\\times 3N}$, x is in $R^{N\\times 3}$. So how can $Qy = y$? The equation (20) is clearly incorrect. \n\nQuestion 6:\n\nIn page 16, an isotropic normal distribution $\\rho = N(0, I_{3})$. Why $\\rho$ is defined on $R^{3}$, not $R^{3N}$? Since a sample should be a conformer, not a single coordinate? You also mention that $\\hat{\\rho}(y) = \\rho(y)$, why is this case? In this case, $QQ^{T} = I_{3}$, but clearly this is not correct. So $\\hat{\\rho}(y) = c\\rho(y)$ for some constant $c$? Quite confused about these claims. Looking forward to authors\' response. \n\nWe are really expecting the authors to provide formal proof and explanations to the above questions. Sincerely thanks!'}, {'summary_of_the_paper': 'This paper tackles the problem of conditional generation of molecular conformations (i.e. 3D cartesian atom positions) given a molecular graph. The authors formulate the generation process via diffusion probabilistic models; Conformations are generated by learning a reverse diffusion process from isotropic gaussian noise to molecular conformations. They use a SE(3) invariant formulation of the diffusion process based on Kohler et al 2020, and they operate directly on atomic positions (i.e. a point cloud) instead of interatomic distances or an intermediate bond geometry representation. The authors show state of the art results evaluated by COV/MAT metrics on GEOM-Drugs and GEOM-QM9 datasets.', 'main_review': 'This paper brings together recent ideas and methods (e.g. diffusion, SE(3) equivariance) to the established task of molecular conformation generation with impressive empirical results. A few more experiments are necessary. The paper is well written. I am only tangentially in this area so I am not intimately familiar with the prior works and benchmarks, and may have missed something in the empirical evaluation.\n\nComments:\n* The authors state that roto-translation invariance is a critical inductive bias for this problem. But I am not convinced that it is critical for this setup and model class. It would be nice to include a baseline of the diffusion model without SE(3) invariance.\n* I am curious about the sensitivity of the performance of the model to the hyperparameters of the diffusion process. Can the authors describe how they reached the current settings given in Table 5. In particular, T=5000 steps seems quite high and I was curious about the performance with lower T say around 1000. As a minor note, there appears to be an inconsistency in the reported batch size (128 in the text vs. 32/64 in Table 5)\n* What is the generation time with 5000 diffusion steps? Does that become a bottleneck in conformation generation?\n* What happened to the other methods on the geom-drugs dataset for Table 3?\n* The authors should mention recent point cloud diffusion papers in their related works as it is addressing a very similar problem\n* What are the reverse process sigmas set to?\n\nMinor comments:\n* Typo: translatoinal\n* Typo: sophisticate\n* I noticed a reference that seemed incorrect: Xu et al 2021a as the reference for molecular generation with VAEs in the second paragraph. The authors should double check their references\n', 'summary_of_the_review': 'Overall a nice paper with technical novelty and good empirical results. I have some questions about the evaluation since some apt baselines are missing. I will raise my score if the authors can address my comments.\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The work introduces a novel\xa0**GeoDiff**\xa0model for conformation generation task, based on a promising diffusion model approach that shows state-of-the-art results in other domains. The authors improved and adapted the concept of the diffusion model for the new domain so that it can work with rotation and translation invariant objects, such as conformations - authors introduced an architecture based on SE(3)-equivariant Graph Field Network (GFN) for parameterizing Markov kernels, also used roto-translational invariant starter density.\n\nThe main contributions of the paper are: the authors are the first who propose an architecture based on principal novel generative diffusion framework for molecular conformation generation and explore suitable roto-translational invariant architecture to parameterize the kernel of the method. Experiments show that the proposed model outperforms recent state-of-the-art approaches with a huge gap on\xa0**Conformation Generation**\xa0and\xa0**Property Prediction**\xa0tasks.', 'main_review': ""**Originality:**\xa0The idea of the work is novel -\xa0**GeoDiff**\xa0is the first generative model for molecular conformation generation based on a diffusion framework.\n\n**Clarity and Quality:**\xa0The authors clearly provide motivation and state the differences of the proposed model from other approaches. The architecture is described in an easy-to-follow and intuitive way. The paper provides good\xa0**Related Work**\xa0and\xa0**Results & Discussion**\xa0sections. Still, it would be great if authors add a clear description of contributions and additional figures of architecture and an aligned cloud of conformations to visually evaluate the diversity of conformations.\n\n**Significance:**\xa0The contribution of the paper to the field is significant. The experiments show a significant metrics gap between the proposed method and baselines, still, it would be great if authors will add standard deviation of metrics to evaluate the stability of the method.\n\n**Drawbacks / questions**:\n\n1. In 3.1 Authors say, they are interested in generating stable conformations. Authors provide energy metrics for the generated ensemble on GEOM-QM9 split, which covers only 30 molecules. Still, they don't provide energy metrics for medium size molecules from GEOM-DRUGS dataset.\n2. Despite the proposed model being compared against a variety of baselines and that the authors mentioned the reason why they are not compared with the GeoMol, it requires a more detailed explanation of GeoMol limitations, since the GeoMol is the current state of the art approach in conformation generation task.\n3. Diffusion models suffer from high computational complexity. Based on the parameters provided by the authors in appendix B, it's logical to assume that 5000 iterations per molecule are very computationally expensive. The article lacks an estimate of the generation time for one molecule."", 'summary_of_the_review': 'The idea of the paper is novel. The text of the paper is clear and easy to follow. The experiments show a significant gap in metrics values between the proposed model and baselines. Still, there are several minor drawbacks that can be fixed in the next revision of the paper.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'This paper describes and tests GeoDiff, an end-to-end denoising diffusion model for generating molecular conformations from a molecular graph. The idea is to learn how to denoise a random conformation so as to generate a realistic structure of a small molecule. To do so, a molecular structure is first transformed into a random configuration from an uncorrelated multivariate normal distribution via a diffusion process (forward process). By training a backwards model that inverts the diffusion process, molecular conformations can then be generated from white noise. The reverse process is modeled as a sequence of Markov kernels where each kernel is a Gaussian whose means are neural networks that are trained based on the forward  trajectories. Molecular conformations are directly parameterized in Cartesian coordinates. Therefore, an important property that should be imposed on the reverse process is *equivariance*. Equivariance refers to the following behavior of a mapping under the action of a group: Transforming the input and then mapping it to some output is the same as applying the mapping first and then transforming the output. Here, the relevant group, the special Euclidean group $SE(3)$, is formed by the rigid transformations in $\\mathbb R^3$. Equivariance is implemented by drawing the initial conformation from a distribution that is invariant under rigid transformations (isotropic Gaussian that removes the center of mass) and by using equivariant Markov kernels for the reverse process. The equivariance of the Markov kernels is implemented with equivariant convolutional layers (graph field network (GFN)). Two invariant objective functions for training are derived (""alignment approach"" and ""chain-rule approach""). GeoDiff is tested on two standard benchmarks and shows a superior performance compared to other molecular configuration generators. Also property predictions made by GeoDiff are better than with concurrent methods. \n', 'main_review': '## Pros\n\n* GeoDiff is an end-to-end model for generating Cartesian coordinates from a molecular graph. \n\n* GeoDiff shows very good performance on standard test sets.\n\n* GeoDiff outperforms various state-of-the-art methods for generating molecular conformations. \n\n## Cons\n\n* GeoDiff\'s model seems to involve a large number of parameters. \n\n## Comments and Questions\n\n* What is the computation time for generating a structure with GeoDiff? How does this compare to other ML approaches or RDKit? \n\n* What is the accuracy of the structures at intermediate stages of the diffusion process? Perhaps this could provide some insight into whether $T=5000$ diffusion steps are needed to reach the reported performance. \n\n* Have you also run tests with a smaller number of diffusion steps? \n\n* You show that GeoDiff\'s model and objective functions are invariant under rigid transformations. However, what if the training data are not invariant? Doesn\'t this still fix some arbitrary orientation in the trained model? \n\n* Which threshold $\\delta$ did you use in the computation of the precision and recall measures? \n\n* What is the total number of model parameters that are trained? \n\n* How is GeoDiff +FF implemented? What is GeoDiff\'s performance in property prediction without FF? \n\n## Language\n\n* Page 3: ""It has been shown effective""\n\n* Page 9: ""can stay the superior diversity""\n\n* Page 9: ""yeild"" instead of ""yield""\n\n', 'summary_of_the_review': ' GeoDiff is an end-to-end model for generating Cartesian coordinates from a molecular graph. GeoDiff outperforms various state-of-the-art methods for generating molecular conformations. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation', 'authorids': ['~Minkai_Xu1', '~Lantao_Yu2', '~Yang_Song1', '~Chence_Shi1', '~Stefano_Ermon1', '~Jian_Tang1'], 'authors': ['Minkai Xu', 'Lantao Yu', 'Yang Song', 'Chence Shi', 'Stefano Ermon', 'Jian Tang'], 'keywords': ['molecular conformation generation', 'deep generative models', 'diffusion probabilistic models'], 'abstract': 'Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'xu|geodiff_a_geometric_diffusion_model_for_molecular_conformation_generation', 'pdf': '/pdf/d6be0299d7f2d2bf947d450fffe98c8395458c75.pdf', 'one-sentence_summary': 'A novel probabilistic diffusion framework to generate accurate and diverse molecular conformations, achieving state-of-the-art results on conformation generation and property prediction', 'supplementary_material': '/attachment/26791ee1444d8190c207fe60b1cb08e3eded683d.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/geodiff-a-geometric-diffusion-model-for/code)', '_bibtex': '@inproceedings{\nxu2022geodiff,\ntitle={GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation},\nauthor={Minkai Xu and Lantao Yu and Yang Song and Chence Shi and Stefano Ermon and Jian Tang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=PzcvxEMzvQC}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Omri Puny', 'Matan Atzmon', 'Edward Smith', 'Ishan Misra', 'Aditya Grover', 'Heli Ben-Hamu', 'Yaron Lipman']",ICLR,Frame Averaging for Invariant and Equivariant Network Design,https://iclr.cc/virtual/2022/oral/6190,2022," Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=zIUyj55nXR,https://openreview.net/forum?id=zIUyj55nXR,zIUyj55nXR,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The submission proposes a method to make a pre-existing model equivariant to desired symmetries: frame averaging. The strategy relies on a significant reduction of the number of symmetries to average over (with respect to the Reynolds operator) and uniform subsampling. The paper also demonstrates the usefulness of this method theoretically (universal approximation result) and practically (competitive performance). The contributions are clear and the core idea is simple.\nI recommend this paper for acceptance with spotlight.'}, {'title': 'Very satisfied.', 'comment': 'Sorry for the late reply.\nI think the proposed text has been improved.\n\nI believe that your model of the point cloud is successful in adding invariance efficiently because the group is decomposed into a direct product.\nHowever, as the $A_n$ example above shows, I thought that the frame-like set should change depending on the base model as well as the input.\nIn any case, I think we had a good discussion.\n\nI change my score to 8.'}, {'title': 'Clarifying the non-normal case, and another suggestion for additional sentences', 'comment': 'We agree that averaging over representatives from **all** cosets lead to an efficient computation of the group averaging (Reynolds operator), and hence to an invariant function. However, the number of cosets can be large (or even infinite), and our point is that we are not sure there exists a **subset** of the representatives, which are not input dependent like a frame, that leads to an invariant function in all cases. \nWe also want to make the point that efficient computation of the Reynolds operator (when it is possible) is a different approach to invariance than Frame Averaging and deserves an independent treatment. If the reviewer agrees we therefore suggest the following *two* sentences:  \n “Another interesting open question for function symmetrization is extending the invariance (or equivariance) of a model $\\phi$ (or $\\Phi$) from a (not necessary normal) subgroup $H$ of $G$, to $G$. A different approach for building efficient invariant/equivariant operators is to come up with an efficient computation to the full group averaging operator.”\n'}, {'title': 'Thank you for your response.', 'comment': 'Thank you for your reply. First of all, your generalization is correct (It is not necessary that $H$ be normal).\n\nHowever, I have some doubts about the proposed sentence. This is because, in general, the set of representatives does not have a group structure, as in the case where $H$ is not a normal subgroup.\n\nTherefore, the proposed sentence should be ""Another interesting open question for function symmetrization is extending the invariance (equivariance) of a model $ \\phi (\\Phi)$ from a minimal subset $H_{\\phi (\\Phi)}$ of $G$.""\n\nPlease let me know what you think.'}, {'title': 'Interesting observation, suggested change', 'comment': 'Thank you for your response and for acknowledging the merits of the suggested approach.  \nRegarding your concern: If we understand correctly, the example you are describing can be formulated in the following general terms: consider a backbone model $\\phi$ that is invariant to $H$, which is a normal subgroup of $G$, namely, $H\\triangleleft G$. Then, making $\\phi$ invariant to $G$ can be done by averaging over arbitrarily chosen representatives from the quotient $G/H$. In some cases, e.g., $A_n \\triangleleft S_n$ such quotient is small and full group averaging is tractable. We agree and think this is an interesting observation, thank you!  \nNote that in some other cases the quotient group can still be too large for full averaging and could benefit from FA, as well.  \nTo address your request, we suggest the following addition to the conclusion/future work part of the paper:  \n“Another interesting open question for function symmetrization is extending the invariance (equivariance) of a model $\\phi$ ($\\Phi$) from a subgroup $H$ of $G$, namely $H<G$, to $G$.”  \n**Please note:** Since replacing the revision PDF is no longer possible in the system at this time, we will incorporate such a sentence in the next revision cycle. \n\n'}, {'title': 'Thank you for your reply.', 'comment': 'Thank you for your reply. Many of my questions have been answered.\n\nMy main concern is that because the Frame is tied to an action on the input space, you may be missing cases where a smaller Frame is possible depending on the model.\nFor example, let $f$ be an $A_n$-invariant function,\nthen Reynolds operator can be computed over two elements, id and (12).\nIn this way, when the model is fixed, it is possible to get smaller frames.\n\nThis approach has its merits in that it can be adapted to all models, but I would like you to write a Question in your paper so that the community is aware of this point.\nQuestion: Can model-dependent Frames be found?\n\nAfter checking this part, I am thinking of raising my score too.'}, {'title': 'The authors have solved the technical issues I raised', 'comment': ""Thank you very much for the authors' responses! The responses are quite helpful and clear. The technical issues I raised have been clarified and fixed. Therefore, I increase the score to 8.""}, {'title': 'All my questions are answered, I keep my original score.', 'comment': 'I would like to thank the authors for their response. All the raised questions are properly addressed in their rebuttal and I have no further concerns. \n\nThis is great solid work, and I will keep my original rating of 8. '}, {'title': 'Very satisfied', 'comment': ""Thanks for your response and extensive revisions. My questions have been excellently answered and I'll adjust my score.""}, {'title': 'Official Response to Reviewer 5HAS', 'comment': 'Thank you for your detailed review. Below we address the main comments and \nquestions expressed in this review. All changes refer to the uploaded revision. \n\n**Q:** My concern is mostly about the incompleteness of the framework. It may happen that for some $X$, $F(X)$  is a small set, but for some other $X$, $F(X)$  is large or even infinity. In this case, a deeper analysis on how to separate these two cases is necessary. I am curious to know why ""generically the frame would consist of $2^d$  elements [...], while for rare inputs $X$  the frame can be an infinite set""?  \n**A:** Thank you for this comment. First, although the infinite frame case if of real interest this paper deals exclusively with **finite** frames. We made that clear in the revision: We added a clear statement at Section 2.1 after Theorem 1, and in Section 2.2 dealing with expressive power, we explicitly state we work in domains where the frame is finite. Furthermore, we have rewritten two paragraphs in Section 3.1 explicitly defining the frame for simple spectrum covariance matrices. We hope it is clearer now why it has $2^d$ elements in this case, and also provide a citation for the genericity claim [1]. In Appendix B (“repeating eigenvalue”) we provide an empirical validation to the genericity claim, namely that all covariance matrices that we encounter have a simple spectrum. Note that in this case the frame size is **constant** where it is defined. In the case of discrete groups (i.e., permutations) there is no natural notion of continuity and indeed frame sizes can change. This seems necessary in this case since, for example, different input graphs could have different self symmetries (automorphisms).\n\n[1] On the geometry of the set of symmetric matrices with repeated eigenvalues, \nBreiding, Kozhasov, Lerario, Arnold Mathematical Journal 4(3), 2018, Springer. \n\n**Q:** It is also not clear how the FA is adapted in the FA-Local-PointNet. A detailed explanation here would be helpful.  \n**A:** Thank you for this comment. We added more details about the FA-Local-Pointnet construction in the appendix, including a visualisation. Please see the appendix, section C.1.   \n\n\n**Q:** No comparison with the previous equivariant architectures are presented.   \n**A:** This is actually not true. We have compared to **4** state-of-the-art equivariant architectures. In more detail:\nIn 5.1 we compared to the Vector Neurons model [1].\nIn 5.2 we compared to SE(3) Transformer [2], TFN [3], and EGNN [4].\nWe strongly believe that this collection of previous works is a solid representation of \ncurrent group equivariant architectures.\n\n[1] Vector neurons: A general framework for so (3)-equivariant networks. Deng et al, ICCV 2021.  \n[2] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks, Fuchs et al, 34th Conference on Neural Information Processing Systems (NeurIPS 2020).   \n[3]  Tensor field networks: Rotation- and translation-equivariant neural networks for 3D  point clouds, Thomas et al, 32th Conference on Neural Information Processing Systems (NeurIPS 2018).  \n[4] E(n) Equivariant Graph Neural Networks, Satorras et al, International Conference on Machine Learning 2021.\n\n**Q:** The definition of the frame $F(X)$  for the case of point clouds and $G=E(d)$  in page 5 is not clear. Why "" $F(X)$  [...] is the collection of $E(3)$ Euclidean transformations defined by [...]"", while in this case, I think,  $F(X)$  must be a subset of  $E(d)$ ?  \n**A:** We have realized the previous frame definition was unclear and rewritten that part in Section 3.1.\n\n\n**Q:** I think the proof of Proposition 1 (Appendix A.6) has some flaws:   \nWhat is $\\Lambda$?  \n (b) you need to prove that $RO$ consists of eigenvectors of $[(R,t)X]^T⋅(I−\\frac{1}{n}11^T)⋅[(R,t)X]$.  \nwhich is - $(R+t1^T)X^T⋅(I−\\frac{1}{n}11^T)⋅X(R+t1^T)^T$   \nrather than - $C=RX^T⋅(I−\\frac{1}{n}11^T)⋅XR^T$   \n...I do not know how it can be true.  \n**A:** For (a), $\\Lambda$ is defined to be the eigenvalues (diagonal) matrix in the eigen-decomposition of $C$ (updated in the revision). For (b), we had a typo in the definition of the group action (corrected in the revision) -   \nOld version was $\\rho_1(g)X = X(R^T+1t^T)$  \nInstead of $\\rho_1(g)X = XR^T+1t^T$.  \nWhich is now fixed and consistent with the proof of proposition 1.\n'}, {'title': 'Official Response to Reviewer BhKC 2/2', 'comment': ""**Q:** Point cloud models: The $S_n\\times E(d)$ or $SE(d)$-invariant model is constructed by computing a frame for $E(d)$ or $SE(d)$  and transforming the model of the Deep sets with that frame. The simple question is why don't you construct a frame for $S_n\\times E(d) $ or $ SE(d)$? If this method is good enough, the point permutation action should also be subject to the frame averaging model. I would like you to explain the rationale reason for not doing so.  \n**A:** Although frame averaging is general and can be used to construct invariant/equivariant models to any given group G including $S_n \\times E(d)$ it should be used wisely. Frame averaging provides a remedy to the fact that for certain group representations (e.g., for graph functions, and to an extent, point clouds), it is not clear how to construct efficient architectures which are both expressive (i.e., universal) and invariant/equivariant. However, it has a cost: it does not use parameter sharing and requires more function evaluations (as a function of the sizes of the frames). In the specific case of sets, efficient universal invariant/equivariant architectures do exist, i.e., DeepSets. It is therefore redundant to apply frame averaging over the permutation representation for sets. Using Frame Averaging to **add** additional symmetry (e.g., to build a universal E(3) invariant/equivariant point cloud network) is therefore a virtue. \n\n\n\n**Q:** The problem with MLP+FA is that it uses the adjacency matrix itself as input, so when the number of nodes is large, the input space is also large, and the number of parameters is significantly larger than, for example, the model in Maron et al. Is it possible to train MLP+FA with, say, 50 nodes?   \n**A:** First, Maron et al. model is known to be not universal [1]. To make it universal, [2] shows a construction that requires high dimensional tensors, presenting a worse (i.e., exponential) tradeoff between expressiveness and complexity than Frame Averaging. To the best of our knowledge the best expressiveness achieved so far with a quadratic model (i.e., using the adjacency as input) is 3-WL [3], which is also not universal. In fact, as far as we can tell, FA provides the **first** quadratic universal model for graphs. Second, the EXP dataset in Section 5.2 contains graphs with up to 64 nodes, larger than 50, and therefore it is indeed feasible to train models on graphs of that size.\n\n[1] Expressive Power of Invariant and Equivariant Graph Neural Networks,  Azizian et al., International Conference on Learning Representations (ICLR) 2021  \n[2] On the Universality of Invariant Networks,  Maron et al., International Conference on  Machine Learning (ICML) 2019  \n[3] Provably Powerful Graph Networks,  Maron et al., Conference on Neural Information Processing Systems (NeurIPS 2019)\t\n\n**Q:** In a real task, the number of nodes in a graph can take many different values, is it possible to train MLP+FA for such a case?  \n**A:** The solution for such a case is to pad the input according to the size of the largest graph in the dataset. In the EXP dataset (Section 5.2) the sizes of the graphs change from 32 and to 64 and we use padding to process input for the MLP+FA model.  (This is mentioned in appendix C.2.)\n\n**Q:** In GNN+FA, the problem seems to be that invariance is calculated redundantly as described above, and the contribution of this method is not clear.  \n**A:** We respectfully disagree. There are two variants of our implementation to GNN+FA and in both of them the FA procedure is **not** redundant:  \n1. In 5.2 the addressed symmetry is E(3) (the particles’ positions and velocities as node features), which GNN is not equivariant to, and therefore the frame averaging is required.  \n2.  In 5.3 we used the GNN equipped with node identifiers (GIN+ID), an universal but not permutation equivariant model. The additional identifiers ruin the equivariance properties of GNN and in order to obtain an equivariant model FA is necessary. \n\n**Q:** it gives good results, but this could be achieved by, for example, concatting two GNNs and then transforming them with FNN, so we decided that it is not worthy of evaluation.  \n**A:** We are not clear what was the intention here and we would be thankful if the reviewer could clarify.\n\n**Q:** Since calculating the specific frame itself involves mathematical difficulties, I believe that the evaluation should be done on the model for which the frame has been calculated, i.e., for which the experiment has been conducted.  \n**A:** We didn’t understand this comment and would be thankful if the reviewer can rephrase so we can properly respond.\n""}, {'title': 'Official Response to Reviewer BhKC 1/2', 'comment': 'We want to thank the reviewer for the review. Unfortunately, we feel that the reviewer has misunderstood a core concept of the paper, which we try to clarify below. \n\n**Q:** This paper proposes a method to transform a model into an invariant/equivariant model using the Reynolds operator. In addition, they define a notion called the equivariant frame to compute the Reynolds operator efficiently.   \n**A:** Our paper does **not** try to provide efficient computation, nor an approximation to the Reynolds operator (group averaging).  Instead, we define a new operator, the Frame Averaging (FA) operator, that has two crucial differences to the Reynolds operator: (i) FA averages on a **subset** of the group; and (ii) FA is input dependent. The FA operator can yield arbitrary different results than the Reynolds operator that averages over the entire group. As mentioned after Theorem 1, FA is a generalization of group averaging. In fact, we even compare with the Reynolds operator (called GA, acronym for Group Averaging) in Section 5.2, showing superiority of FA in performance and computational complexity. \n\n\n**Q:** I would like to point out that the idea of using Reynolds operators to transform a model into an invariant model is found in Kicki et al. 2021, where the construction is exactly the same except for the use of frames. The authors should be cited for this paper. (Kicky et al. A New Approach to Design Symmetry Invariant Neural Networks International Joint Conference on Neural Networks (IJCNN) 2021).   \n**A:** First, let us note that our paper cites **earlier** papers that suggest and use the Reynolds operator than Kicki et al. 2021. This is done before equation 1 in the paper: Murphy ey al. 2018, and Yarotsky who’s original preprint dates back to 2018.  Second, regarding Kicki et al., note that this work considers subgroups of $S_n$, therefore restricts to the finite group case (cannot handle infinite groups such as $E(3)$), and the number of terms in the sum is much larger than required for Frame averaging. In Section 5.2 we compare FA to averaging of $S_n$ using the Reynold operator.\n\n**Q**. The name ""frame"" is confusing with the concept of frame in differential geometry and should be given a different name.  \n**A:** Actually, our frames do relate to frames in differential geometry. Let us try to explain: The method of moving frames, and in particular Darboux frames, are chosen on a surface in $\\mathbb{R}^3$ in an $E(3)$ equivariant manner by aligning them with principal directions and the normal to the surface. If the surface rotates and/or translates the Darboux frame changes accordingly. Our frames can be seen as a generalization of this principle. \nFurther note that also Darboux frames are defined everywhere on the surface **except** at umbilical points, which are defined by repeating eigenvalues of the shape operator, similar to the situation with our $E(d)$ frame construction. \n\n\n**Q:** Theorem 1: It is proved that if a frame is an equivariant function, then a partial sum over the frame gives a transformation to an invariant/equivariant function, but the fact that the frame is defined depending only on the input space is not appropriate. For example, if we have an invariant model F, the proposed method will sum over frames to convert it to an invariant function, which will increase the computational complexity. The transformation should be done without any transformation for the invariant model F. This leads to the problem of overlapping invariants when combined with other models in the experimental section.  \n**A:** We didn’t fully understand this comment. We would like to point out a few aspects that perhaps were misunderstood:  \n1. Our goal in this work is to transform non-invariant/equivariant (backbone) architectures to become invariant or equivariant to new symmetry types. Therefore we **never** apply Frame Averaging to an already invariant/equivariant mode.   \n2. We are not using a partial sum. As mentioned above, our goal is not to compute (or approximate) the full group average but to average over an (equivariant) subset of the group. This operator could have an arbitrarily different value than the full group average.\n\n\n\n'}, {'title': 'Official Response to Reviewer 57mq', 'comment': 'Thank you for your detailed review. Below we address the main comments and \nquestions expressed in this review. All changes refer to the uploaded revision. \n\n**Q:** Simple examples. It takes me some time to form an intuition of what is proposed in this paper. It would help me a lot if the authors could provide a simple example at the beginning to give readers some intuition. For example, it might be good to work through an example to make MLP translation equivariant?  \n**A:** Added a simple running example in Section 2.1.\n\n**Q:** The construction of frames in Section 3.1 seems to come out of nowhere. While I could check they are indeed equivariant, I don’t think I understand the motivation or thinking process behind such designs.  \n**A:** The role of frames is to normalize the data so that the network always “sees” symmetric versions of the input in the same way. This is why choosing a frame requires finding a property of the data that is equivariant and using that to define the frame. One intuition comes from differential geometry where moving frames, and in particular Darboux frames are set on a surface by aligning with principal directions and the surface’s normal. If the surface rotates and/or translates the Darboux frame changes accordingly. Our frames can be seen as a generalization of this principle. \n\n\n**Q:** Can the frames be simplified? As an example, if we let the input $X$  be a function on groups, i.e. $X=f(g), g∈G$ . Can we let $F(f)=\\arg\u2061max_g||f(g)||$ ? I might be wrong, but I think this simple construction is also equivariant?   \n**A:** The reviewer is right, the frame suggested is indeed equivariant. As the reviewer witnessed, frame design is a very flexible framework that could be constructed with different approaches, optimization (e.g., as you suggested), or by spectral properties (as we did in this paper), and probably also in other ways. Regarding simplicity, we assume it depends on how we choose to define “simple” - is it by the cardinality of the frame? or maybe the computational complexity of computing the frame? We opted for the spectral method since it is relatively efficient to compute.\nWe find these questions very interesting and believe they could lead to further developments in equivariant deep learning in future research. \n\n\n\n**Q:** Are the number of elements output by frames the smaller the better, or is there a balance between performance and computational efficiency?  \n**A:** In terms of expressive power, the size of the frame does not matter. This can be seen by our expressive power theorem and corollaries. Another thing that is clear is that large frames are more computationally demanding than small frames. Regarding generalization performance, this is less obvious; we do believe different frame choices will inject different inductive bias and generalize differently. We leave this important and non-trivial question to future work. \n\n \n**Q:** How stable are the proposed frames? That is to say, if I add noise to the inputs, will the output subset of groups be significantly different?  \n**A:** Thank you for this question. The stability of the frames depends of course on the particular definition of a frame, and as this reviewer noticed many constructions of equivariant frames are indeed possible. In this paper we mostly use spectral properties of matrices, in particular eigenvectors, to define frames. On the theoretical level we can analyze the stability of such frames for the generic case of simple spectrum (i.e., all eigenvalues have multiplicity one). The fact that this is indeed the generic case is justified in [1]. Next, stability results such as [2] (Theorem 8.1.12) can bound the change in frame as a function of the perturbation (i.e., noise) and the distance to the nearest eigenvalue. We added the relevant discussion in Section 3.1 (immediately after proposition 1). Furthermore, in Appendix B we added an empirical analysis of the stability of frames.  Our main observation is that the frames behave in a stable manner, i.e., small noise added to the input results in a small change of the frame. Please see the added experiment for more details. \n\n\n[1] On the geometry of the set of symmetric matrices with repeated eigenvalues, Breiding, Kozhasov, Lerario, Arnold Mathematical Journal 4(3), 2018, Springer. \n\n[2] Matrix computations, Gene H Golub, and Charles F Van Loan, 1996, Johns Hopkins University Press, Baltimore, MD.\n'}, {'title': 'Official Response to Reviewer CToH 2/2', 'comment': '**Q:** In the proof of theorem 5, which norm is used for $||\\rho_2(g)||$ ? It can’t be the max K-norm because K is a subset of the input of phi, not the output. Is it the operator norm?  \n**A:** It is the operator norm, we clarified this and the other norms using specific notation (see Definition 1 and Appendix A.5). \n\n\n**Q:** I think a citation would be appropriate to Finzi et al 2020, “Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data”, as they also consider sampling from the group to build equivariant networks.  \n**A:** citation was added in the related work section.\n\n**Q:** Why have the authors chosen the name “frame” for $F(X)$? I know frame as a set of \nvectors or in the context of a frame bundle.  \n**A:** As shortly alluded to above, our intuition comes from differential geometry where the \nmethod of moving frames, and in particular Darboux frames, are chosen in an $E(3)$ equivariant manner by aligning them with principal directions and the normal to the surface. Further note that also Darboux frames are defined everywhere except at umbilical points, which are defined by repeating eigenvalues of the shape operator, similar to the situation with our $E(d)$ frame construction. \n'}, {'title': 'Official Response to Reviewer CToH  1/2', 'comment': '\nThank you for your detailed review. Below we address the main comments and questions expressed in this review. All changes refer to the uploaded revision. \n\n**Q:** Theorem 4 is unintuitive (behavior of $\\epsilon,k$) and unclear (bounds probability of  \none subsample at a time), please remove from paper.   \n**A:** The idea behind Theorem 4 was to compare the lower bound for a **fixed** $k$ and $\\epsilon$, which in this case show that smaller $m_F$ leads to a better lower bound. The counter-intuitive behavior of this bound w.r.t.~$k$ and $\\epsilon$ stems from the fact that the size of the set of ""good"" empirical distributions $\\hat{\\mu}$ is increasing with $k$ and $\\epsilon$. Nevertheless, we agree with the reviewer that this theorem does not provide the full picture and moved it to supplementary, adding the above disclaimers. We do feel it can be used in the future as a step to provide a full proof of approximation quality of the FA approximator, and therefore believe it\'s a good idea to keep it in the supplementary. \n\n\n**Q:** “It is a bit unclear when the results apply to finite and infinite groups and frames $F(X)$. Everywhere, a summation symbol is used, but in some places, $F(X)$ is infinite. In the infinite cases, which measure should then be used? Can one always use some canonical Haar-like measure? In particular, in the proofs of theorem 1 and theorem 4 this should be discussed.”  \n**A:** In this paper we only treat the finite frame case. We added a clear statement at Section 2.1 that in this paper we only consider $X, F$ so that $F(X)$ are finite sets. We also added related comments in other relevant places in the paper: In Section 2.2 dealing with expressive power, we explicitly state we work in domains where the frame is finite. The definition of frames in 3.1 are defined only for simple spectrum covariance matrices. In Corollary 2 we also added the finite frame assumption. \nRegarding the general (infinite) case we believe this is an important future direction. Yes, Haar measure can be used but it won’t cover all interesting cases (e.g., think of 1-dimensional rotations embedded in 3-dimensional rotations). The most general treatment seems to require data dependent measures, but this is out of scope for this paper and will be treated in a future publication. \n\n\n**Q:** “I don’t follow the choice of $F(X)$ for the $E(d)$ case. Which are the $2^d$ $O(d)$ matrices?”  \n**A:** We rewrote this part in the paper (see Section 3.1), hopefully it is clearer. \n\n**Q:** I would like some more discussion about the choice of $F(X)$. Does the choice of $F(X)$ affect the output? If so, how? Is $F(X)$ required to be continuous / does a continuous (non-trivial) $F(X)$ always exist? How does this affect the continuity of the symmetrized function? If it is discontinuous, does that affect the universality?  \n**A:** The choice of frame affects the output: the frame sets a “normalized form” of the input to be injected into the network. Therefore different frame choices inject different inductive biases.  Regarding continuity: in the continuous group case (i.e., $E(d)$) the frames are continuous where they are defined, see discussion in Section 3.1, after Proposition 1, and Appendix B. In the discrete group setting (e.g., $S_n$) we are not sure what continuity means. Globally continuous frames are not always defined. A similar situation is in differential geometry where choosing continuous frame fields can fail due to topological obstructions or geometrical (e.g., umbilical points). Universality is not affected by discontinuous frames; we have simplified and generalized the universality claim and proof, and as can be checked, it does not require continuity of the frame, see Section 2.2. \n\n\n**Q:** Why does GA-MLP and GA-GIN+ID only get 50% score on EXP-classify? Are you there using a finite subsample of G or $F(X)$? And could you give any insight into why we’d expect then complete failure for a G subsample and complete success for a $F(X)$ subsample?  \n**A:** The core reason for the difference between the FA and GA sampling method lies in Figure 2 (left) in section 5.2. The plot describes how well each sample method is able to approximate its average as a function of the number of samples. As can be seen from the plot, when $k=1$ (which is the same number of samples used in the EXP experiment) the variance of the frame sampling is smaller than the variance of the group sampling. The difference in variance explains the accuracy gap in the EXP task. We can expect the same phenomenon with GIN+ID.\n\n\n'}, {'summary_of_the_paper': 'The paper proposes to make any neural network equivariant by symmetrizing over a subset of the group, rather than over whole group. If the subset selection F(X), depending on input X, is equivariant (gFX=FgX), then the symmetrization is equivariant.\nThe authors furthermore prove:\n1)\tWhen interested in invariant prediction, the subset can be chosen in the quotient G/G_X, where G_X is the stabilizer subgroup of X.\n2)\tWhen symmetrizing with a random subsample of F(X), the probability of a particular subsample that deviates from symmetrizing with all of F(X) by less than some epsilon, is bounded below.\n3)\tWhen using the symmetrization of a universal model, the resulting model class is universal in the class of equivariant functions.\n', 'main_review': 'Strengths:\n-\tThe paper proposes a very practical strategy of building equivariant nets\n-\tThe universality proof helps convince the reader to use this method\n-\tThe paper considers and experiments on three different instantiations of their method, showing wide applicability.\n-\tThe experimental results show the method performs competitively.\n\nWeaknesses:\n-\tI don’t understand what’s happening in theorem 4. It considers a subsample $\\hat\\mu$ of F(X) to be ‘good’ when the symmetrizer that uses the subsample is epsilon-close to the full F(X) symmetrizer. Then it says that the probability of *one particular* good subsample is bounded below.\nHowever, that bound seems vacuus, as plugging any reasonable number brings the bound quickly close to 0. Also, it’s counter-intuitive why the bound should become looser as epsilon grows or as k grows.\nWhat one would want instead is giving a lower bound of the probability that we get *any* $\\hat\\mu$ that is epsilon-close to the full F(X) symmetrizer. And we want this bound to get higher when epsilon or k increases.\nThe line below theorem 4 draws a conclusion that would follow from a theorem as I propose it above, not from the theorem in the paper.\nAs it is currently stated – and I’m not completely misunderstanding – theorem 4 can best be removed from the paper. \n-\tIt is a bit unclear when the results apply to finite and infinite groups and frames F(X). Everywhere, a summation symbol is used, but in some places, F(X) is infinite. In the infinite cases, which measure should then be used? Can one always use some canonical Haar-like measure? In particular, in the proofs of theorem 1 and theorem 4 this should be discussed.\n-\tThe writing of the paper can be improved. I don’t follow the choice of F(X) for the E(d) case. Which are the 2^d O(d) matrices? Perhaps the authors can elaborate in more detail one of the examples how to construct F(X) in the main paper, and then do the other two in the appendix. \n-\tI would like some more theoretical discussion about the choice of F(X). Does the choice of F(X) affect the output? If so, how? Is F(X) required to be continuous / does a continuous (non-trivial) F(X) always exist? What does the topology on 2^G look like? How does this affect the continuity of the symmetrized function? If it is discontinuous, does that affect the universality? When can F(X) be chosen to be finite?\n \nOther comments:\n-\tWhy does GA-MLP and GA-GIN+ID only get 50% score on EXP-classify? Are you there using a finite subsample of G or F(X)? And could you give any insight into why we’d expect then complete failure for a G subsample and complete success for a F(X) subsample?\n-\tIn the proof of theorem 5, which norm is used for ||rho_2(g)|| ? It can’t be the max K-norm because K is a subset of the input of phi, not the output. Is it the operator norm?\n-\tI would like theorem 1 to be put in the main paper, as it shows why the key construction is correct.\n-\tI think a citation would be appropriate to Finzi et al 2020, “Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data”, as they also consider sampling from the group to build equivariant networks.\n-\tWhy have the authors chosen the name “frame” for F(X)? I know frame as a set of vectors or in the context of a frame bundle.\n', 'summary_of_the_review': ""I think this is a great paper, as it proposes a new practical method for building equivariant networks which is broadly applicable, universal and performs well in practice. I have serious concerns about theorem 4. If the authors convince me why it makes sense, or if they remove it, I will increase my score.\n\nI've updated my score after the response and revision."", 'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': '**Summary and Contributions**:\n\nThe paper introduces a framework called Frame Averaging (FA) that can adapt existing backbone architectures to become invariant/equivariant to new symmetry types. It achieves this by averaging over an input-dependent frame which outputs a subset of groups. Frame averaging is often much more efficient to compute than averaging over the entire group, while at the same time, guarantees exact invariance/equivariance.\n\nOn the technical side, the paper also proves that FA-based models have the same expressive power as the original backbone architectures.\n\nOn the empirical side, the paper provides new classes of models using FA such as universal Euclidean motion invariant point cloud networks / Message Passing GNNs, and demonstrates their practical effectiveness on several tasks.\n', 'main_review': '**Strengths**:\n\n- *FA is a very simple framework yet it is potentially very useful*. Group equivariance is an important form of inductive bias in deep learning architectures, but designing architectures that has such equivariance is challenging. It would be very useful if we could adapt any back architecture to become invariant/equivariant to a certain group. However, the previously studied group averaging is computationally infeasible when the group is large, so this paper, which greatly reduces the computational cost of group averaging, could be potentially very useful. I really like this work, and will be looking forward to seeing future development of this work.\n- *Technical statements are all sound and proved*. Mathematical statements in this paper all look correct to me, and they have provided proofs. These statements are clear and rigorous.\n- *Impressive results*. Using the simple idea of frame averaging, the paper demonstrates state-of-the-art results on several tasks. The results are impressive, which suggests that the FA framework can be very useful in practice.\n\n**Weaknesses**:\n- *Lack of simple examples*. While it is not hard to check the correctness of all these statements, it takes me some time to form an intuition of what is proposed in this paper. It would help me a lot if the authors could provide a simple example at the beginning to give readers some intuition. For example, it might be good to work through an example to make MLP translation equivariant (with the simplest possible construction of frames)?  \n- *Insufficient study and explanation of the proposed frames*. \n  - The construction of frames in Section 3.1 seems to come out of nowhere. While I could check they are indeed equivariant, I don’t think I understand the motivation or thinking process behind such designs. \n  - Furthermore, can the frames be simplified? As an example, if we let the input $X$ be a function on groups, i.e. $X=f(g),g\\in G$. Can we let $F(f) = \\arg\\max_{g} ||f(g)||$? I might be wrong, but I think this simple construction is also equivariant? \n  - Are the number of elements output by frames the smaller the better, or is there a balance between performance and computational efficiency?\n\n**Questions**:\n*How stable are the proposed frames*? For the proposed frames, I would be interested to know how stable they are? That is to say, if I add noise to the inputs, will the output subset of groups be significantly different? \n', 'summary_of_the_review': 'The paper studies an important problem that could potentially have great impact: How to adapt existing architectures to become invariant/equivariant to a certain group while maintaining the expressive power and computational efficiency of the original backbone model? The paper provides a simple yet effective solution. The technical statements are sound and the empirical results are impressive.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'No ethics concerns', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper proposes a method to transform a model into an invariant/equivariant model using the Reynolds operator.\nIn addition, they define a notion called the equivariant frame to compute the Reynolds operator efficiently.', 'main_review': 'Strengths \nThis paper proposes a general method for transforming existing models into invariant/equivariant models using Reynolds operators.\nCombined with other methods, it provides state-of-the-art experimental results.\n\nWeaknesses & Questions\nFirst of all, I would like to point out that the idea of using Reynolds operators to transform a model into an invariant model is found in Kicki et al. 2021, where the construction is exactly the same except for the use of frames.\nThe authors should be cited for this paper.\nAlso, the name ""frame"" is confusing with the concept of frame in differential geometry and should be given a different name.\n\nTheorem 1: It is proved that if a frame is an equivariant function, then a partial sum over the frame gives a transformation to an invariant/equivariant function, but the fact that the frame is defined depending only on the input space is not appropriate.\nFor example, if we have an invariant model F, the proposed method will sum over frames to convert it to an invariant function, which will increase the computational complexity. The transformation should be done without any transformation for the invariant model F.\nThis leads to the problem of overlapping invariants when combined with other models in the experimental section.\n\nAlso, since calculating the specific frame itself involves mathematical difficulties, I believe that the evaluation should be done on the model for which the frame has been calculated, i.e., for which the experiment has been conducted.\n\nPoint cloud models:\nThe $S_n \\times E(d)$ or $SE(d)$-invariant model is constructed by computing a frame for $E(d)$ or $SE(d)$ and transforming the model of the Deep sets with that frame.\nThe simple question is why don\'t you construct a frame for $S_n \\times E(d)$ or $SE(d)$?\nIf this method is good enough, the point permutation action should also be subject to the frame averaging model.\nI would like you to explain the rationale reason for not doing so.\n\nGraph models:\nTwo types of models have been proposed: MLP+FA and GNN+FA.\nThe problem with MLP+FA is that it uses the adjacency matrix itself as input, so when the number of nodes is large, the input space is also large, and the number of parameters is significantly larger than, for example, the model in Maron et al.\nIs it possible to train MLP+FA with, say, 50 nodes?\nAlso, in a real task, the number of nodes in a graph can take many different values, is it possible to train MLP+FA for such a case?\nIn GNN+FA, the problem seems to be that invariance is calculated redundantly as described above, and the contribution of this method is not clear.\n\nReference\n\nZaheer et al.\nDeep sets\nNeural Information Processing Systems (NeurIPS) 2017.\n\nMaron et al. Invariant and Equivariant Graph Networks\nInternational Conference on Learning Representations (ICLR) 2019\n\nKicky et al.\nA New Approach to Design Symmetry Invariant Neural Networks\nInternational Joint Conference on Neural Networks (IJCNN) 2021\n\n\n\n', 'summary_of_the_review': 'The structure of the proposed model has already been seen in Kicki et al. 2021 except for the use of frames, and even if frames are used, MLP+FA for example does not seem to be able to handle changes in inputs such as changes in the number of nodes in the graph.\nWhen combined with the existing strong methods, it gives good results, but this could be achieved by, for example, concatting two GNNs and then transforming them with FNN, so we decided that it is not worthy of evaluation.\n\nAfter some discussion, the score was raised because the theoretical uncertainties and doubts were resolved.\n\n\n\n\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'There are no ethical issues in this paper.', 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The authors introduce Frame Averaging (FA), a general framework for adapting known architectures to become invariant or equivariant with respect to a general group by using group averaging operator. The idea of FA is to replace the averaging operator over the entire group by the averaging over a smaller set of group elements but still achieve the full invariant/equivariant property.', 'main_review': '*Strengths*\n- The idea of replacing the averaging operator over the entire group by the FA is innovative. FA is simpler to compute and has less complexity in comparison with the averaging operator over the entire group.\n\n- The authors prove that the FA-based models preserve the universality property of their backbone architectures. \n\n- The FA framework is then applied to design several invariant and equivariant architectures for point clouds and graphs.\n\n*Weaknesses*\n\n- My concern is mostly about the incompleteness of the framework. It may happen the case that for some $\\mathbf{X}$, $\\mathcal{F}(\\mathbf{X})$ is a small set, but for some other $\\mathbf{X}$, $\\mathcal{F}(\\mathbf{X})$ is large or even infinity. (For example the frame choice in Subsection 3.1 is in this case). In this case, a deeper analysis on how to separate these two cases and how to deal when $\\mathcal{F}(\\mathbf{X})$ is large or even infinity is necessary.\n\n- It is also not clear how the FA is adapted in the DA-Local-PointNet. A detailed explanation here would be helpful.\n\n- No comparison with the previous equivariant architectures are presented. Therefore, it is hard to estimate how novel and efficient the framework is in the world of current group equivariant architectures.\n\n- Some technical parts are quite compact and not easy to read. Maybe the reason is that the paper includes rich contents and the authors tried to fix all of them in 9 pages.', 'summary_of_the_review': 'In addition, I have some comments on the technical parts of the paper:\n\n- The definition of the frame $\\mathcal{F}(\\mathbf{X})$ for the case of point clouds and $G=E(d)$ in page 5 is not clear. Why ""$\\mathcal{F}(\\mathbf{X})$ [...] is the collection of E(3) Euclidean transformations defined by [...]"", while in this case, I think, $\\mathcal{F}(\\mathbf{X})$ must be a subset of $E(d)$?\n\n- I am curious to know why ""generically the frame would consist of $2^d$ elements [...], while for rare inputs $\\mathbf{X}$ the frame can be an infinite set""? I think this remark is crucial for the proposed method as it affects the size of the FA framework.\n\n- I think the proof of Proposition 1 (Appendix A.6) has some flaws:\n\n     -- Line 3: What is $\\Lambda$? It is not defined yet.\n\n     -- Line 6-7: It seems to me that, you need to prove that $\\mathbf{RO}$ consists of eigenvectors of \n$$[(\\mathbf{R},\\mathbf{t})\\mathbf{X}]^T \\cdot (I-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^T) \\cdot [(\\mathbf{R},\\mathbf{t})\\mathbf{X}]$$\nwhich is \n$$(\\mathbf{R}+\\mathbf{t} \\mathbf{1}^T) \\mathbf{X}^T \\cdot (I-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^T) \\cdot \\mathbf{X} (\\mathbf{R}+\\mathbf{t} \\mathbf{1}^T)^T$$\nrather than \n$$C = \\mathbf{R} \\mathbf{X}^T \\cdot (I-\\frac{1}{n} \\mathbf{1} \\mathbf{1}^T) \\cdot \\mathbf{X} \\mathbf{R}^T$$\nas you claimed. If this is exactly the case, I do not know how it can be true.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Frame Averaging for Invariant and Equivariant Network Design', 'authorids': ['~Omri_Puny1', '~Matan_Atzmon1', '~Edward_J._Smith1', '~Ishan_Misra2', '~Aditya_Grover1', '~Heli_Ben-Hamu1', '~Yaron_Lipman1'], 'authors': ['Omri Puny', 'Matan Atzmon', 'Edward J. Smith', 'Ishan Misra', 'Aditya Grover', 'Heli Ben-Hamu', 'Yaron Lipman'], 'keywords': ['Invariant and equivariant neural network', 'expressive power'], 'abstract': 'Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. \nWe introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks.', 'one-sentence_summary': 'Introducing a general methodology for building expressive and efficient invariant and equivariant networks.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'puny|frame_averaging_for_invariant_and_equivariant_network_design', 'pdf': '/pdf/d7849f0ef0f911d06889785dc7116564d5342442.pdf', 'data': '', '_bibtex': '@inproceedings{\npuny2022frame,\ntitle={Frame Averaging for Invariant and Equivariant Network Design},\nauthor={Omri Puny and Matan Atzmon and Edward J. Smith and Ishan Misra and Aditya Grover and Heli Ben-Hamu and Yaron Lipman},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=zIUyj55nXR}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Hangbo Bao', 'Li Dong', 'Songhao Piao', 'Furu Wei']",ICLR,BEiT_ BERT Pre-Training of Image Transformers,https://iclr.cc/virtual/2022/oral/6324,2022," We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=p-BhZSz59o4,https://openreview.net/forum?id=p-BhZSz59o4,p-BhZSz59o4,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Inspired by BERT and the corresponding masked language modeling objective, this paper proposes masked image modeling as a pre-training technique for vision transformer. More precisely, the image is tokenized using a pre-trained tokenizer, and the goal is to predict the token indices corresponding to masked patches of the image. As noted by the reviewers, the proposed method is simple, works very well in practice and the paper is well written. Since this work potentially opens a whole new research direction, my recommendation is to accept with oral presentation.'}, {'title': 'Response to Reviewer Vvct', 'comment': 'The suggested checkpoints have been added to our release plan. Thank you!'}, {'title': 'thanks', 'comment': 'I thank the authors for answering my review and for following my suggestions. However, I still think authors should release the model at lower epochs to help low-budgets groups to compare BEiT with other papers.\n\n> A: Previous work used different settings. For example, dino-vit/base/16 used 400 epochs (as shown in their configuration file and training log), iGPT was trained by 1M steps with 128 batch size (~105 epochs; because of their extremely large model size), and MoCo v3 reported 300/600 in their paper. If we consider the training throughput and cost, our method tends to be cheaper because we do not use multi-crop augmentation or two copies of Transformer parameters for self-distillation and contrastive learning. So the results are comparable in the sense of training cost. Moreover, as reported in MoCo v3\'s paper, ""the gain of training longer is diminishing on ViT-B"" for MoCo v3 (i.e., 600 epochs vs. 300 epochs). In contrast, our method can still benefit from training more epochs, which is also another advantage of our method, especially for self-supervised learning. We will add a curve to show the performance along with the number of training epochs in the camera-ready version as suggested.\n\nThank you, but I also strongly suggest to the authors to release the model at 300 epochs, to allow other practitioners to compare the model at lower epochs\n\n> A: We used 100 for ImageNet-1k fine-tuning. Because the data size of CIFAR is much smaller, we enlarge its epoch number to 150. For a reference, DINO used 1000 epochs for CIFAR10 fine-tuning as shown in their repo.\n\nI strongly suggest the authors to show and release the fine-tuning at 100 epochs.\n'}, {'title': ""Response to the authors' rebuttal"", 'comment': 'Overall, I am satisfied with the rebuttal. I suggest the authors add the key information ""We trained the tokenizer with 50 epochs (batch size=512; lr=5e-4), ... which is only about 1/33 of ViT-B pre-training ..."" into the paper.'}, {'title': 'Response to Reviewer GYF1', 'comment': 'Thanks for your feedback!\n\nFirst, I would like to clarify that the pre-training and the tokenizer do **not** rely on any image-text pairs, i.e., only unannotated images are used. So, the proposed framework is fairly compared. More details are described as follows.\n\nQ: the tokenizers are borrowed from DALL-E, which means that a large number of image-text pairs have been used. ... This is not considered self-supervised learning, as the texts can contain vast amount of semantics (according to the CLIP paper, after training on image-text pairs, the model can achieve good performance on zero-shot ImageNet classification)\n\nA: The [DALL-E tokenizer](https://arxiv.org/abs/2102.12092) does not use any image-text pair, which is learned by unsupervised learning with reconstruction loss. So, the whole framework is still self-supervised learning.\n\n\nQ: The detailed setting of the DALL-E tokenizer and ImageNet re-trained tokenizer. In particular, please specify what kind of external information has been used, and the cost of pre-training such tokenizers.\n\nA: The reimplemented tokenizer in Appendix C is re-trained on only ImageNet-1k images (without labels). We trained the tokenizer with 50 epochs (batch size=512; lr=5e-4) based on the [open-sourced repo](https://github.com/lucidrains/DALLE-pytorch) using 4 v100 GPU cards. The training cost of the above tokenizer is only about 1/33 of ViT-B pre-training, which is relatively cheap compared with the whole run. Moreover, the DALL-E tokenizer also does not use external information. The training of tokenizers uses reconstruction loss, which only needs images.\n\n\nQ: If external information is used\n\nA: No, we do not use any external information for pre-training. Please refer to the above answers.\n\n\nQ: a discussion on the relationship between this approach and knowledge distillation is necessary\n\nA: As shown in Table 4, the ablation study ""-Masking +Recover 100% visual tokens\'\' (i.e., without masking and distill from the tokenizer) is equal to knowledge distillation with the tokenizer, which performs worse than fine-tuning from scratch (i.e., without pre-training). So, the key ingredient is masking and then recovering the original image from its corrupted version. Sec 2.4 helps to understand the proposed framework from the perspective of variational autoencoder.\n'}, {'title': 'Response to Reviewer pM4q', 'comment': 'Thank you for the constructive comments.\n\nQ: recommend the authors add the relation with BEiT and VLP models in the related work section\n\nA: We will mention the explorations of VLP in the related work section as suggested.\n\n\nQ: several ways (1. use it as a one-hot label; 2. use KL-div; 3. regress the features that are used to infer the class labels) to exploit the discrete VAE tokens. all three approaches are also available for BEiT and the discrete VAE; thus, I wonder whether they can further boost the performance of BEiT.\n\nA: We tried KL-div (i.e., the second way) in our experiments, and the results became slightly worse compared with one-hot labels. My guess is that in UNITER such ways can learn object detection (OD) knowledge from the prediction distribution of the OD model, which is particularly important for vision-language tasks. We would try more settings as you suggested. Thanks for the suggestion!'}, {'title': 'Response to Reviewer Vvct', 'comment': 'Thanks for your feedback!\n\nQ: Are authors going to release the code?\n\nA: All the code, pretrained checkpoints, and scripts are available for reimplementation.\n\n\nQ: pre-training is run for 800 epochs, which is fine for large ML groups but it might be very demanding for smaller groups.\n\nA: Our ablation studies used 300 epochs for faster iterations, which is hardware-friendly for resource-demanding settings. Moreover, we do not store two copies of Transformer parameters for self-distillation/contrast, and do not use multi-crop augmentation. So our method is much more memory-efficient and faster than previous work.\n\n\nQ: are pre-training results obtained with 800 epochs for all the models? I mean, are they comparable? practitioners would really like to see the training curve of BEiT, to understand what budget should they invest to have the desired accuracy.\n\nA: Previous work used different settings. For example, dino-vit/base/16 used 400 epochs (as shown in their [configuration file](https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/args.txt) and [training log](https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain_log.txt)),  iGPT was trained by 1M steps with 128 batch size (~105 epochs; because of their extremely large model size), and MoCo v3 reported 300/600 in their paper. If we consider the training throughput and cost, our method tends to be cheaper because we do not use multi-crop augmentation or two copies of Transformer parameters for self-distillation and contrastive learning. So the results are comparable in the sense of training cost. Moreover, as reported in MoCo v3\'s paper, ""the gain of training longer is diminishing on ViT-B"" for MoCo v3 (i.e., 600 epochs vs. 300 epochs). In contrast, our method can still benefit from training more epochs, which is also another advantage of our method, especially for self-supervised learning. We will add a curve to show the performance along with the number of training epochs in the camera-ready version as suggested.\n\n\nQ: Since BEiT is somewhat based on BERT, why do you replace 40% of tokens? Is there a motivation behind this number?\n\nA: The ""information density"" of a patch seems lower than one word in a text. So the masking ratio would be larger than 15% that is used for language data.\n\n\nQ: BERT task was to replace the token 80% of the time with a [MASK] token, 10% of the time with a random token, and 10% of the time keeping it as it was. Did you consider this strategy? As far as I know, this is beneficial for fine-tuning tasks.\n\nA: Thanks for your valuable suggestion! It makes sense to me. We would like to try this improvement.\n\n\nQ: (CIFAR-100) Why 150 epochs? I believe that the standard is 100.\n\nA: We used 100 for ImageNet-1k fine-tuning. Because the data size of CIFAR is much smaller, we enlarge its epoch number to 150. For a reference, DINO used 1000 epochs for CIFAR10 fine-tuning as shown in their [repo](https://github.com/facebookresearch/dino/issues/81#issuecomment-884935778).\n\n\nQ: Why is DINO in Table 10 with 400 epochs? why not 300 as standard?\n\nA: We report the DINO results by directly using their officially released checkpoints. As shown in the files of [training arguments](https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/args.txt) and [logs](https://dl.fbaipublicfiles.com/dino/dino_vitbase16_pretrain/dino_vitbase16_pretrain_log.txt), the dino-vit/base/16 model was trained with 400 epochs. As there are too many hyperparams of DINO, we directly follow their best-reported checkpoint.\n\n\nQ: cite this recent paper in the intro talking about how Vision Transformers are data-hungry https://arxiv.org/abs/2106.03746 and some tricks for small datasets\n\nA: We added the citation as suggested. Thanks for the pointer.\n\n\nQ: In Section 2.3 ""pixel-level auto-encoding"" might be confusing. I suggest rephrasing it and explaining it better (as it is in the intro).\n\nA: We added the explanation of the term in Sec 2.3.\n\n\nQ: I would like to see a comment on the throughput of DINO vs your paper in the main manuscript\n\nA: We added it in the related work section.\n\n'}, {'title': 'Response to Reviewer 7cfB', 'comment': 'Thanks for your feedback!\n\nQ: The dependence on DALL-E to is a pretty hefty one (unsupervised pretraining on 250M images). This can potentially be lifted, but the authors made no attempt in this direction.\n\nA: As shown in Appendix C, we re-train the image tokenizer on ImageNet-1K and compare it with the DALL-E tokenizer. Table 8 shows that our reimplemented tokenizer obtains comparable reconstruction loss and ImageNet fine-tuning performance compared with the off-the-shelf DALL-E tokenizer.\n\n\nQ: (linear probes) I wonder if this comes from the relatively small batch size (and/or lack of hyperparameters search) compared to other papers, or if there is a more fundamental reason for BEiT to be worse than contrastive methods there.\n\nA: The contrastive learning methods typically learn to aggregate the image-level features into a global vector, which is relatively suitable for linear probing. In contrast, the generative self-supervised learning methods, such as iGPT and ours, usually do not pretrain such global feature aggregation, which tends to make linear probes difficult.\n'}, {'summary_of_the_paper': 'This paper is one of the first to present state-of-the-art results for masked image modeling (BERT-style) self-supervised learning on images (contrastive approaches held the SOTA before). The whole system is based on a ViT encoder (e.g. with 16x16 pixels patches as input) to produce visual tokens. The pretraining objective consists in matching the visual tokens (one per image patch) from a provided visual tokenizer, with the ViT encoder. The tokenizer is a discrete VAE (with 8192 token types) from prior work called DALL-E (Ramesh et al., 2021). The results on ImageNet-1K are SOTA for comparable model sizes and pretraining data and settings. Idem for segmentation on ADE20K.', 'main_review': '(+) The paper is clear and easy to follow.  \n(+) There are proper ablations for most of the design decisions.  \n(-)  The dependence on DALL-E to is a pretty hefty one (unsupervised pretraining on 250M images). This can potentially be lifted, but the authors made no attempt in this direction.  \n(+/-) To be fair with the above point, the authors did at least the ablation of predicting masked pixels directly (instead of visual tokens from DALL-E) in Table 4, and the performance drops by ""only"" ~1.7% top1 on ImageNet in the downstream task. This is still not great as this brings the accuracy below supervised training only, which is what is done as fine-tuning on this network in this case. This ablation should have been done in linear probing also.  \n(-) Regarding linear probes, the results (in Table 9) are somewhat disappointing. I wonder if this comes from the relatively small batch size (and/or lack of hyperparameters search) compared to other papers, or if there is a more fundamental reason for BEiT to be worse than contrastive methods there.  \n(=) No pretraining on larger than ImageNet-1K.  \n(+) The paper provides enough details for reproducing the results.\n\n', 'summary_of_the_review': 'Overall, this is a strong paper presenting a significant improvement in unsupervised pretraining for images, that should be presented at ICLR. More analysis (e.g. linear probing) could make the paper stronger. The limits of the paper are fair to have, and can potentially be addressed in follow-ups.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents a novel task called Masked Image Modeling (MIM), inspired by the more famous Masked Language Model task proposed by BERT in NLP. For this reason, the paper is called BEiT. BEiT relies on a pre-pre-trained tokenizer that transforms image patches into discrete tokens, which are then masked and predicted. Extensive experiments show that this self-supervised pre-training improve SoTA in various downstream tasks such as image classification and semantic segmentation.', 'main_review': 'PROS:\n- The task is interesting per se, as it brings the concept of BERT into Vision Transformers\n- Well written paper and simple idea\n- Good analysis\n\nCONS:\n- Some important details for practitioners are missing (e.g. choices on the masking)\n- experiments of different models are sometimes difficult to compare\n\nIn general, I liked the paper a lot. However, I have some issues that should be answered and addressed before publication. I\'ll number them for convenience\n\n1. Authors says that pre-training is run for 800 epochs, which is fine for large ML groups but it might be very demanding for smaller groups. Moreover, it does not help the comparison with all other SoTA methods. For example, DiNO is trained for 300 epochs. Moco v3 is an experimental paper, but the main results are obtained for 300 epochs (although they report 600 epochs as well). Now, I wonder two things: 1) are pre-training results in Table 1 obtained with 800 epochs for all the models? I mean, are they comparable? 2) I\'d have liked to see a comparison between models at 300 epochs (as standard) in Table 1 or in another additional table. Appendix A might be in that direction but I did not understand it well as it is not well explained; 3) practitioners would really like to see the training curve of BEiT, to understand what budget should they invest to have the desired accuracy.\n\n2. Since BEiT is somewhat based on BERT, why do you replace 40% of tokens? Is there a motivation behind this number? then, BERT task was to replace the token 80% of the time with a [MASK] token, 10% of the time with a random token, and 10% of the time keeping it as it was. Did you consider this strategy? As far as I know, this is beneficial for fine-tuning tasks.\n\n3. It would be good to see the standard (small) datasets for downstream tasks such as CIFAR-10, Oxford Flowers-102 and Oxford Pets or cars alongside CIFAR-100. This would have made the paper more comparable. Moreover, why 150 epochs? I believe that the standard is 100.\n\n4. Why is DINO in Table 10 with 400 epochs? why not 300 as standard?\n\n5. It\'s good to have the appendix, but authors should refer to them and comment on the (interesting!) results in the main paper. E.g. Appendix E, F.\n\n6. Are authors going to release the code?\n\nMINOR:\n- I would change the special token [S] to [CLS] as in BERT\n- I would cite this recent paper in the intro talking about how Vision Transformers are data-hungry https://arxiv.org/abs/2106.03746 and some tricks for small datasets\n- In Section 2.3 ""pixel-level auto-encoding"" might be confusing. I suggest rephrasing it and explaining it better (as it is in the intro).\n- If you have time, for the camera ready, I\'d love to see BeIT applied to some convolution transformers such as Swin or CvT\n- I would like to see a comment on the throughput of DINO vs your paper in the main manuscript', 'summary_of_the_review': 'The paper is well written, the idea is somewhat novel (novel in computer vision, less novel in general because of BERT). Experiments are good but improvable. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper presents a new objective called masked image modeling (MIM) to pre-train vision transformers, making the model predict a visual token from the masked patches.  \nThe visual tokens are obtained by discrete VAE, which is trained on 250M images. (Though, the authors showed that training discrete VAE with only 1M images (imagenet-1K) is enough to demonstrate the power of the proposed objective.)  \nThe authors pre-train the ViT with MIM and fine-tune the pre-trained weights (BEiT) on two visual downstream tasks: image classification and semantic segmentation, and showed superior performances compared to previous methods, including DINO and MoCo v3.  \nThe authors also propose an additional trick called blockwise masking to improve BEiT further.', 'main_review': 'I think the MIM objective resembles the masked region modeling (MRM) objective, which is widely used in the vision-and-language pretraining (VLP) models.  \nVLP models often mask several visual regions and make their contextualized outputs predict their corresponding object class to guide visual inputs in tandem with the textual inputs\' masked language modeling.  \nBy removing textual inputs from VLP models and switching detection-based region features to patches, we have a BEiT-like structure.  \nRecently, ViLT[^1] proposed this type of VLP model but failed to make appropriate objectives for the patches, reporting that regressing masked pixels (masked patch prediction objective) deteriorate downstream vision-and-language tasks.  \nIn my view, as MRM and MIM are similar enough to be noted, I recommend the authors add the relation with BEiT and VLP models in the related work section.\n\nUsing argmax-ed visual tokens is inevitable for DALL.E since they had to plug the discrete tokens into the decoder.  \nHowever, since BEiT only uses the discrete tokens as ingredients of the MIM objective, I think argmax-ing the visual tokens is unnecessary, and the token distribution can be immediately used for the objective (e.g., KL-divergence).  \nActually, UNITER[^2], which used detection-based regional inferred class for the MRM objective to train VLP model, has tested three types of using the class information:  \n1.  Use it as a one-hot label (as BEiT did).\n2.  Use KL-div.\n3.  Regress the features that are used to infer the class labels.\n\nUNITER showed that the combination of (2) + (3) yields better performance than solely using (1).  \nI believe all three approaches are also available for BEiT and the discrete VAE; thus, I wonder whether they can further boost the performance of BEiT.\n\nI believe using visual tokenizers such as discrete VAE can be a silver lining for the community and those seeking self-supervisable images\' objectives, including the VLP community.  \nI think this paper showed rigorous and solid empirical results and well contributes to the community by providing valuable tools.\n\n[^1] Kim, Wonjae et al. ""ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision."" _ICML_ (2021).  \n[^2] Chen, Yen-Chun et al. ""UNITER: UNiversal Image-TExt Representation Learning."" _ECCV_ (2020).', 'summary_of_the_review': ""- Consider adding a VLP subsection to the related work section.\n- There are several ways to exploit the discrete VAE tokens.\n- The reviewer thinks the paper's results are solid."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents a self-supervised learning framework named BEIT, in which the input image can be masked in some regions, and the task is to recover the token of the masked region. Pre-trained on ImageNet, the model shows good performance on a series of downstream tasks.', 'main_review': 'This paper presents a self-supervised learning framework named BEIT, in which the input image can be masked in some regions, and the task is to recover the token of the masked region. Pre-trained on ImageNet, the model shows good performance on a series of downstream tasks.\n\nI shall say that I very much like the idea of this paper. Self-supervised learning in computer vision seems far behind that in natural language processing, mostly because the proxy task is not good enough. This is highly related to the original data format -- masked language modeling is a perfect proxy for texts, but it seems very difficult to replicate it on images -- because image signals are mostly continuous in the space, and semantic signals are sparse, etc. This paper makes a good trial along this direction. I think the mask image modeling task is much more elegant than the existing contrastive learning (e.g. SimCLR & MoCo) or predictive learning (e.g. BYOL) counterparts, because those methods are mostly relying on data augmentation to provide the prior-to-be-learned, but data augmentation not strong enough. More importantly, data augmentation can bring conflicts, e.g. one needs to enlarge the intensity of data augmentation to improve the difficulty of learning, but strong data augmentation is risky and may generate duals with very different semantics (so that they are bad for SSL).\n\nOK, I have expressed my opinions that this new direction is promising. Also, the experiments on ImageNet and ADE20K are good (though I expect to see more including ImageNet linear-tests, MS-COCO tests, etc.), showing the strong ability of the pre-trained networks. However, I have one major concern that avoids me from giving a higher score to this paper. **That is, I am not sure if the proposed framework is fairly compared against the prior methods.**\n\nThe key lies in the tokenizer, which delivers almost all priors in the BEIT algorithm. However, the tokenizers are borrowed from DALL-E, which means that a large number of image-text pairs have been used -- the authors did not specify which model has been used, but at least, the CC3M (if not JFT300M) dataset is included in the pre-training part. This is not considered self-supervised learning, as the texts can contain vast amount of semantics (according to the CLIP paper, after training on image-text pairs, the model can achieve good performance on zero-shot ImageNet classification). I do not believe that using this tokenizer as the ""teacher"" to ""distill"" the target ""student"" model is ""self-supervised learning"". In Appendix C, the authors claimed that a re-implemented tokenizer on ImageNet shows similar performance, but technical details are missing. Even in the unsupervised setting, if the tokenizer is trained for sufficiently long, it can offer powerful guidance to the target network, so that some statements (e.g. Section 2.5, The pre-training runs for about 500k steps (i.e., 800 epochs)) are not strictly meaningful.\n\nI hope the authors can answer the following questions.\n\n1. The detailed setting of the DALL-E tokenizer and ImageNet re-trained tokenizer. In particular, please specify what kind of external information has been used, and the cost of pre-training such tokenizers.\n\n2. If indeed external information is used (e.g. ImageNet labels to the worst case), is it possible to re-implement a counterpart without any such information and re-test the performance? Also, I am very interested in the performance of a random tokenizer (i.e. without pre-training, just clustering the responses of the randomly initialized networks).\n\nBy any means, I think a discussion on the relationship between this approach and knowledge distillation is necessary. I shall re-evaluate this paper after seeing answers to the above questions.', 'summary_of_the_review': '1. The idea is good, going through an important direction of self-supervised learning.\n2. Downstream tests show good performance.\n3. BUT, whether the improvement comes from the designed framework is questionable.\n\nI need additional details from the authors to make the final decision.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'title': 'BEiT: BERT Pre-Training of Image Transformers', 'authorids': ['~Hangbo_Bao1', '~Li_Dong1', '~Songhao_Piao1', '~Furu_Wei1'], 'authors': ['Hangbo Bao', 'Li Dong', 'Songhao Piao', 'Furu Wei'], 'keywords': ['self-supervised learning', 'pre-training', 'vision Transformer'], 'abstract': ""We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods."", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'bao|beit_bert_pretraining_of_image_transformers', 'pdf': '/pdf/1be2cb0e0edf9af45f8ef450b802b459897cec3d.pdf', 'one-sentence_summary': 'We propose a masked image modeling task to pretrain vision Transformers.', 'code': '', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/beit-bert-pre-training-of-image-transformers/code)', '_bibtex': '@inproceedings{\nbao2022beit,\ntitle={{BE}iT: {BERT} Pre-Training of Image Transformers},\nauthor={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=p-BhZSz59o4}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Huiqi Deng', 'Qihan Ren', 'Hao Zhang', 'Quanshi Zhang']",ICLR,DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS,https://iclr.cc/virtual/2022/oral/6623,2022," This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=iRCUlgmdfHJ,https://openreview.net/forum?id=iRCUlgmdfHJ,iRCUlgmdfHJ,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low-order and high-order interaction patterns are easy to be learned, while middle-order interaction patterns are difficult to be learned. They also propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle-order interaction. All reviews are positive.'}, {'summary_of_the_paper': 'This paper discovers and theoretically proves the representation bottleneck phenomenon that indicates cognition gap between DNNs and humans. This paper proposes losses to encourage or penalize the DNN to control interactions of specific complexities, and analyze the representation capacities of interactions.', 'main_review': ""Strengths\n- The paper is well-written and easy to follow. In particular, Figure 1 and 2 support the concept of representation bottleneck reasonably well.\n- Two questions the authors raise about DNNs are rational, and prove convincingly that this phenomenon is a common problem of DNNs.\n- The idea of describing the representation of the DNN using multi-order interaction utility is good and easy to understand.\n\nWeaknesses\n- The authors propose two losses to encourage/penalize DNNs to learn interactions of specific orders and show the results in Figure 4. This loss can stimulate interactions of specific orders well. High-order DNN seems to have been fully explained and experimented, but not on middle-order and low-order interaction. I wonder why the authors didn't explain it in detail.\n- The authors conduct several experiments, but these show similar results, making it difficult to gain new insights. And it is difficult to understand what Table 1 means and shows. \n- Although this paper shows good enough insight to others, it would be good to present directions for solving the problem of representation bottleneck or providing a clear problem definition for future works.\n- I know it’s due to the limitations of the amount, but putting too many formulas and terms into the sentence. Instead of including all the formulas in the sentence, it would be better to reorganize it and make it easier to legible.\n"", 'summary_of_the_review': 'This paper introduces and proves the representation bottleneck, which is a common  phenomenon in the DNNs. This is well-written and easy to understand. There seems to be some shortcomings in the experiments and conclusion, but the paper provides good insights to others.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper studies the representation ability of DNNs from the perspective of interactions (the multi-order interaction). The authors discovered an interesting representation bottleneck phenomenon, i.e., in a normally trained DNN, low-order and high-order interaction patterns are easy to be learned, while middle-order interaction patterns are difficult to be learned. Moreover, they give a theoretical explanation that the bottleneck origins from that different interaction patterns have different learning strengths (gradients). To relieve the bottleneck problem, the authors propose two novel loss functions, which allow the model to encode interactions of specific orders, including middle-order interactions. The experiments have validated the effectiveness of the proposed losses. Finally, the authors explored some properties of DNNs encoding different-order interactions. ', 'main_review': '[Strength]\n1. This paper discovers a very interesting tendency (called representation bottleneck) in existing DNNs, i.e., a DNN is usually more likely to encode low-order and high-order interactions, but difficult to encode middle-order interactions. $\\\\\\\\$\n2. This paper analyzes theoretically the reason behind the bottleneck. Moreover, the simulated results based on Theorem 1 well match the real cases, which validates the theoretical reason. $\\\\\\\\$\n3. From the perspective of the bottleneck, the authors explore the difference between DNN learning and human learning, which provides the key idea of the proposed two losses. $\\\\\\\\$\n4. To relieve the bottleneck phenomenon, the authors propose two novel losses to enable DNNs to learn interactions of any specific orders. $\\\\\\\\$\n5. Experimental results demonstrate the effectiveness of the two losses, and show that middle-order interactions can provide useful information for classification. $\\\\\\\\$\n\n[Weakness]\n1. The explanation for the representation bottleneck is not intuitive enough.  $\\\\\\\\$\n2. There is a lack of discussion why the trained DNNs do not achieve a significant improvement than a normally trained DNN.$\\\\\\\\$\n3. The experiments on adversarial robustness can not well support the conclusions, since they were only conducted on tabular datasets. $\\\\\\\\$\n\n[Suggestions]\n1. I would suggest the authors give an additional illustration of the intuition behind the proof for the bottleneck. $\\\\\\\\$\n2. It’s expected to give guidance on how we can use the two losses to boost the classification performance. $\\\\\\\\$\n3. It’s not clear why the output change in Eq.(5) can represent interactions of specific orders. $\\\\\\\\$\n4. The authors are suggested to report results in terms of adversarial accuracy on more datasets such as image data.  $\\\\\\\\$\n5. The authors are suggested to clarify whether the proposed two losses will increase the time complexity.  $\\\\\\\\$\n\n***\nAfter carefully reading the response, other reviews, and the revised version of the manuscript. \n\n1. The added proof skeleton makes the theory more clear and intuitive. \n2. New experimental results show a strong connection between representation capacity and interaction orders, which may inspire follow-up studies. \n\nOverall, I think this work is insightful and may make a great impact on both training and explaining of deep neural networks. I have modified the score accordingly.\n\n\n\n', 'summary_of_the_review': 'This paper discovers and theoretically explains an interesting representation bottleneck in existing DNNs, and relieves the bottleneck by proposing two novel losses. As the discovered bottleneck reflects the common difficulty to learn middle-order interactions, this work may provide a new direction to train DNNs. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper looks at representation bottleneck of deep neural networks, from the multi-order interaction (within subsets of pixels) point of view. Specifically, the paper provides a novel definition on the relative interaction strength, and then analyzes this quantity both theoretically and numerically. The paper next proposes two loss functions that encourage or penalyze interactions of certain orders, and conducts multiple experiments on different combinations of these loss functions. ', 'main_review': ""The research topic of this paper is significant in understanding the representation power of deep neural networks, and is interesting to the entire community of deep learning. The writing of this paper is clear enough, and the main contents are easy to follow.\n\nStrengths:\n- The definition of the relative interaction strength is elegant.\n- The theoretical and numerical analysis in Section 3.2 is novel and interesting. \n- The loss functions to encourage or penalize interactions of certain orders are also novel and very natural.\n- There are experiments showing the effects of these loss functions and connecting them to properties such as accuracy and robustness.\n\nHowever, there are several concerns of this paper. The following are about theory.\n- Regarding Thm 1, how reasonable is the assumption that $\\frac{\\partial}{\\partial W} \\Delta v(i,j,S)$ is a Gaussian? At least some numerical validations should be made here. \n- Regarding the simulation in Fig 3, there is still a tiny gap between the blue and orange curves, which does not look like a random pattern. Can you explain where this gap comes from? \n- Regarding equation 5, there are two issues. First, it is confusing to reload the notation $\\Delta v$. I would recommend using another symbol. Second, are you missing an expectation in this equation?\n- In equation 7, it is clearer to write $L^+(r_1,r_2)$ rather than $L^+$. Simimlar in equation 8. \n- Regarding equation 9 and simulations in Fig 4, it is possible to use multiple $L^+$ or $L^-$ with different $(r_1,r_2)$ pairs? If so, why are you not using it? I feel this can provide more flexibility and is therefore potentially more useful. \n\nThere are also concerns regarding experiments. In summary, the experiments are not extensive and cannot well support the findings.\n- Regarding accuracy, what can we read from Table 1 (left)? The numbers seem random.\n- Regarding structural representations, Fig 5 & 6 make some sense, but what about other masking methods? \n- Regarding robustness, Table 1 (right) tells us that penalizing low-order and boosting high-order interactions harm robustness. What about doing the opposite, i.e. penalizing high-order and boosting low-order interactions? Does it improve robustness?\n- In general, the experiments are not extensive. In order to better understand the effects of the proposed loss functions, there should be experiments carefully adjusting the $\\lambda$'s and $(r_1,r_2)$'s so that we can extract general patterns of the effects. \n- In addition, I do not see any significant improvement by using the proposed loss functions. Are there scenarios where using the proposed loss functions outperforms standard training in any of the metric (accuracy, robustness)? \n- Finally, figures and tables are a bit small so hard to read. \n\nScores can be modified based on authors' feedback and additional experiments. \n\n----------------------\nAfter rebuttal: I have carefully read the response and added experiments.\n\n- The revised theory with milder assumption is great.\n- The additional experiments are excellent and provide a much more complete picture of the proposed theory. \n\nI have modified the score accordingly. "", 'summary_of_the_review': 'Theory is excellent but there is space for improvement in the experiments. \n\n----------------------\nAfter rebuttal: experiments are extensive and insightful enough. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': ""The authors study the complexity of interactions between input variables in deep neural networks (DNNs). They use a multi-order interaction utility between pairs of variables to represent the interaction complexity between input variables encoded in DNNs. The results can be summarized as follows:\n\n1) DNNs are more likely to encode simple and complex interactions between pairs of input variables but aren't good enough to encode the intermediate ones. The authors define the simplicity of interaction depending on its contextual complexity.\n\n2) The authors develop novel loss functions to encourage/penalize the interactions of specific complexities and then, investigate the representation capacities of the learned DNNs.\n\n"", 'main_review': ""Strengths:\n\n1) The novelty of the paper lies in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs. \n\n2) The major strength of the paper lies in the extensive study of different neural networks and datasets to back the claim. Also, the effect of the proposed loss functions $L^{+}$ and $L^{-}$ on deeper networks like AlexNet, VGG-16, and ResNet-18/20 is quite interesting.\n\n\nSuggestions:\n\n1) The number of sets $S$ s.t. $|S| = m$ is given by $n \\choose m$. This function increases from $m = 1 \\ldots \\frac{n}{2}$ and decreases thereafter till $n$. Thus, for achieving large $I^{(m)}(i, j)$ at $m = \\frac{n}{2}$, the model needs to capture dependence between the variables $i$ and $j$ for significantly higher number of contexts, compared to the cases when $m=3$ (local context) or $m=n-2$ (global context). One may not need the dependence to be large for all possible contexts in the intermediate context length case.  Hence, can the authors give an example to explain why the increasing number of contexts in the intermediate contextual interactions doesn't necessarily lead to a small $I^{(m)}(i, j)$ always?\n\n2) Since, the authors consider $v(S|x) = \\log \\frac{1 - \\Pr[\\hat{y} = y^{\\star}]}{  \\Pr[\\hat{y} = y^{\\star} }$ for computing $J^{(m)}$ (which depends on the probability scores of a data example), I believe the magnitudes of $v(S|x)$ aren't necessarily comparable for different examples. Then, isn't the following definition of $J^{(m)}$ a proper relative interaction strength to measure?\n\\begin{equation*}\n    J^{(m)} = \\mathbb{E}_{x \\in \\Omega} \\frac{ \\mathbb{ E }   I^{(m)} (i, j \\mid x)   }{\\mathbb{E_k } \\mathbb{E} |I^{(k)} (i, j | x) | \n }\n\\end{equation*}\n\nThe above interaction measure measures the strength of the contextual utility for each example.\n\n3) In the adversarial robustness experiments, are the low-order and intermediate-order interactions more robust to adversarial attacks? It will be great to have some experiments for intermediate-order interactions, to possibly showcase the fact that neural networks are susceptible to adversarial attacks because of the cognition gap.\n\n\n\n\n"", 'summary_of_the_review': 'Overall, the paper makes an important evaluation study to further the understanding of the representation capacity of DNNs. The novelty is in using the multi-order interaction proposed in Zhang et al. (2020) to understand the complexity of interactions in DNNs and the proposal of different loss functions to encode/penalize interactions of specific complexities. Hence, I am leaning towards a positive score for the paper.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS', 'authorids': ['~Huiqi_Deng1', '~Qihan_Ren1', '~Hao_Zhang22', '~Quanshi_Zhang1'], 'authors': ['Huiqi Deng', 'Qihan Ren', 'Hao Zhang', 'Quanshi Zhang'], 'keywords': ['representation bottleneck', 'representation ability', 'interaction', 'explanation'], 'abstract': 'This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'deng|discovering_and_explaining_the_representation_bottleneck_of_dnns', 'pdf': '/pdf/e470657e4d47a20411713a973ed0282f87c9f9a9.pdf', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/discovering-and-explaining-the-representation/code)', '_bibtex': '@inproceedings{\ndeng2022discovering,\ntitle={{DISCOVERING} {AND} {EXPLAINING} {THE} {REPRESENTATION} {BOTTLENECK} {OF} {DNNS}},\nauthor={Huiqi Deng and Qihan Ren and Hao Zhang and Quanshi Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=iRCUlgmdfHJ}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Zongze Wu', 'Yotam Nitzan', 'Eli Shechtman', 'Dani Lischinski']",ICLR,StyleAlign_ Analysis and Applications of Aligned StyleGAN Models,https://iclr.cc/virtual/2022/oral/6949,2022," In this paper, we perform an in-depth study of the properties and applications of aligned generative models.We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion.",Oral 2: AI applications,https://openreview.net/pdf?id=Qg2vi4ZbHM9,https://openreview.net/forum?id=Qg2vi4ZbHM9,Qg2vi4ZbHM9,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper provides an interesting analysis of aligned GAN models. The paper shows that when a model is obtained (fine-tuned) from another, then the corresponding hidden semantic spaces are aligned. The paper uses this property to show that without any additional architecture or training, the models can perform diverse tasks such as image translation and morphing. The paper also demonstrates that zero-shot tasks can be performed by learning in the parent domain and transferring to the child domain.\n\nAll reviewers agree that the paper presents an interesting analysis and findings and will make a valuable contribution to the field. The reviewers raised some particular concerns, which were addressed by the authors in their response.'}, {'title': 'Thank you for the reply', 'comment': 'I maintain my stance that the paper should be accepted - this is a valuable study'}, {'title': ""Thanks for the authors' reply"", 'comment': 'Thanks for your reply. All my concerns have been addressed. While the fine-tuning method is the same as prior works, I agree with other viewers that the analysis of the latent space is interesting and can contribute to other related researches.'}, {'title': ""Thanks for the authors' reply"", 'comment': '- As I pointed out in my previous reply, the subtle detailed differences are not the key factors to me. I think given that [R1] shows aligned model intepolation achieves continuous cross-domain (different styles, different gender) translation, and that [R3] shows layer swapping (a kind of hard interpolation) can adjust the degree of the shape deformation between cats and dogs and the well known StyleGAN latent interpolation to achieve continuous changes in geometry and pose, I do not think the cross-domain morphing in this paper brings me much insights in terms of research values. As also pointed out by Reviewer x6dT, `The authors show abundant results in Figures 23, 24, 25, and 26. Similar results are also being met in the StyleGAN-based method. I do not fully understand the key challenge in such a situation.` It maybe a challenge for traditional image morphing frameworks, but given the StyleGAN framework and related works as well as [R1], the solution to this task is intuitive and simple.\n- What I mean in my previous reply is that your claim of `This was discussed by AgileGAN but was not demonstrated` is too absolute. Although not supported in a perfect way, is it fair to claim that AgileGAN does not demonstrate the conclusion (and this is even one of the contributions claimed by AgileGAN) with its analysis, visual comparison and comparison of the latent code distributions? '}, {'title': 'Image morphing, AgileGAN, and paper structure', 'comment': 'Thanks for these comments and suggestions,\n\n- We do consider it insightful to demonstrate that aligned models may be readily applied to **cross-domain morphing**. Previous (fairly recent) works tackling this task [2,3] resorted to sophisticated machinery for coping with such morphs. As discussed earlier, [1] is related yet not directly comparable, because it does not really **morph** between two real images, but rather interpolates the “effects” of two neural networks. To the best of our knowledge, our method is the first to demonstrate cross-domain image morphing with significant changes in geometry and pose, without explicitly accounting for these factors. We consider the fact that a traditionally challenging task can be solved with such ease, insightful. Furthermore, we also observe at the end of section 4.2 (and demonstrate in Figures 24-27) that there are multiple feasible morphing trajectories in the 2D interpolation space. A creative artist may choose different trajectories depending on the desired transition effect.\n\n- AgileGAN discusses that Z+ is superior to W+ for face-to-cartoon translation. Although we arrive at the same conclusion, this claim was not fully supported in the AgileGAN paper, since all of their experiments comparing Z+ to W+ had other varying factors besides the latent space. Namely, to represent W+, they used other inversion methods (pSp and Image2StyleGAN) - differing significantly from their hVAE multi-path encoder. These methods differ in optimization strategy, architecture, losses, etc. These differences make it difficult to attribute the superiority entirely to the choice of latent space.\n\n- We understand your point that in regards to image-to-image translation, the analysis part may be more insightful than the comparison to other methods. We consider both important, and ideally would have liked to include both in the main paper, but haven’t been able to do so due to space constraints. We are open to suggestions regarding what should be placed in the paper and what in the appendix, assuming the reviewers are in agreement on this and space constraints permit.\n\n[1] 2019 CVPR Deep Network Interpolation for Continuous Imagery Effect Transition\n\n[2] K. Aberman, J. Liao, M. Shi, D. Lischinski, B. Chen, and D. Cohen-Or. Neural best-buddies:  Sparse cross-domain correspondence. SIGGRAPH 2018.\n\n[3] N. Fish, R. Zhang, L. Perry, D. Cohen-Or, E. Shechtman, and C. Barnes.   Image morphing with perceptual constraints and STN alignment. Computer Graphics Forum 2020.\n'}, {'title': ""Thanks for the authors' reply"", 'comment': '- I agree with the authors that this paper provides new insights for the choice of inversion methods and spaces for image-to-image translation. However, this content is mainly included in the APPENDIX part, while Sec. 4 mainly describes a comparison with I2I translation methods and image morphing (which is not very new to me), making the readers easily miss the new part. I would suggest the authors to refine and even rephrase Introduction and Sec. 4 accordingly. For example, `Next, we explore additional tasks, for which aligned models have not been used before. In Section 4.2 we describe a simple method for fully automatic image morphing between fairly dissimilar domains, such as human to dog faces` in the introduction is not very precise and is not the key contribution of this paper. \n\n- It seems my concern that the application of image morphing is less insightful is not responded. If the authors agree with my concern, maybe the introduction and experiment of this part could be moderately reduced to leave more room to those really exciting parts of this paper.\n\n- `This was discussed by AgileGAN but was not demonstrated`. AgileGAN verifies that Z+ is better than W+ on face to cartoon translation task by extensive experiments of ablation studies and visualization of the latent code distributions. Why it is not demonstrated?\n'}, {'title': 'Insights regarding image translation', 'comment': 'Thanks for responding to our rebuttal.\n\n* As we acknowledge several times throughout our paper, our I2I workflow is indeed similar to several recent methods [1,2,3]. Our main novel contributions for I2I and multi-modal image translation tasks lie in the analysis of different inversion methods and spaces and the insights resulting from it. Specifically, we decompose the workflow to real image inversion and image translation, and compare multiple alternatives for each of these steps in order to decide which latent spaces should be used in different scenarios. Previous I2I methods [1,2,3] did not provide extensive evaluation of the different components in the process and therefore perhaps unsurprisingly chose different inversion methods and latent spaces. The original Toonify paper [1] only translated generated images (though their github implementation uses direct optimization in W+), [3] uses a different latent optimization objective in W+, while AgileGAN [2] trains an hVAE to Z+ space. In contrast, we perform an extensive evaluation regarding the choice of the latent space and the inversion method, which is briefly summarized below.\n\n    Specifically,\n    1. In Figures 16 and 17, we show that the W+ space, which is good for reconstruction, does not necessarily lead to good translations (wrong color palette). This was discussed by AgileGAN but was not demonstrated. Additionally, we provide a possible explanation - the change in mapping function (shown in Figures 1 and 8). For nearby domains (FFHQ2Cartoon), the mapping function only changes slightly, and the translation results for W+ and Z are compatible (shown in Figure 18 and 19). For more distant domains (AFHQ), the mapping function changes a lot, and the translation results for Z space are significantly better than W+ space (shown in Figures 16 and 17). \n    2. Figures 17 and 19 show that inversions using an encoder and using latent optimization achieve images with similar attributes, with latent optimization results having more diversity and better visual quality, as can be seen from the FID and KID results reported in Table 5. This may be because the encoder method effectively compromises over a large set of training images, at the price of losing some image specific features.\n\n* Regarding whether freezing the mapping (Freeze-FC) works or not, this is not a question that can be easily answered. The difficulty comes from the fact that the current I2I task is not fully defined. Rather than translating the input image from source domain to target domain, users typically wish to translate the source image to a domain that is **between** source and target. For example, the Metface dataset barely contains smiling faces. If we wish to translate an image from FFHQ to Metface, it is reasonable to obtain a non-smiling result. However, this is probably not what the user would consider a satisfactory result. \nTo create images that mix between source and target domains, Toonify-like methods limit the capacity of the child network through freezing the mapping function [3], or create a hybrid model between parent and child through layer swapping [1,3]. The task is not well defined in the sense that we do not explicitly define which attributes should be maintained and which attributes should be translated. \n\n    We believe the next generation of image translation methods should allow users to more explicitly choose what to translate and what to maintain. Once the goal of image translation is explicitly defined, we can test and compare the performance of different modifications of the network, such as freezing the mapping function and layer swapping.\n\n**References:**\n\n[1] Pinkney, Justin NM, and Doron Adler. ""Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains."" arXiv preprint arXiv:2010.05334 (2020).\n\n[2] Song, Guoxian, et al. ""AgileGAN: stylizing portraits by inversion-consistent transfer learning."" ACM Transactions on Graphics (TOG) 40.4 (2021): 1-13.\n\n[3] Jialu Huang, Jing Liao, Sam Kwong. ""Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network."" IEEE Transactions on Multimedia (2021).\n'}, {'title': ""Thanks for the author's reply"", 'comment': ""Thanks for the author's reply. Part of my concerns are solved. However, I still feel that the applications of image translation and image morphing are less insightful to me. \n\n- [R3] has shown the Toonify based method can work for more distant domains (dog2cat).\n\n- [R3] has shown the Toonify based method can work for multi-modal image translation tasks.\n\n- The authors clarify the paper’s difference from [R1] in terms of implementation details, applicable domains, and channel analysis. From my perspective, I would like to see something new in the applications, which could provide me with some insights. However, since I’m familiar with StyleGAN, Toonify, [R1][R2][R3], I think image-to-image translation and image morphing in this paper are not new to me. For example, it is true that this paper interpolate both model weights and latent codes, while [R1] only interpolates model and StyleGAN only interpolate codes. But that is still not new to me in terms of research values. By comparison, I think obtaining a low-resolution child model from a high-resolution parent model is new to me. (And the property study part is fairly new to me, which I really appreciate.)\n\n- [R3] has shown that `Freezing the FC layers (freeze-FC) preserves the mapping relation from a latent code to an input feature vector. Therefore, some semantic information like age and facial expression can be preserved`. The claim is very different from the experiment in authors’ response. Maybe the authors could analyze the reasons why freeze-FC works or does not work to provide more insights.\n\n- Thank you for providing this strange application of illegal use detection. I think Figure 34 is very interesting. The grandchild model recovers most of the feature from the parent model, which might have some potential good applications.\n\n`[R1] 2019 CVPR Deep Network Interpolation for Continuous Imagery Effect Transition`\n\n`[R2] https://github.com/justinpinkney/toonify/blob/master/StyleGAN-blending-example.ipynb`\n\n`[R3] 2021 TMM Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network`""}, {'title': 'Reply to reviewer x6dT', 'comment': 'We thank the reviewer for time and thoughtful feedback. Below we address the main points raised in this review.\n\n1. Indeed, our work focuses on simply fine-tuning StyleGAN. Nevertheless, as pointed out in most of the reviews, our main novel contribution comes from the extensive empirical analysis of aligned models and their use for downstream tasks. As we discuss in the paper at length, several previous works [1,2,3], including AgileGAN [4], have used “fine-tuning” for image-to-image translation. Nevertheless, we go further than that: we analyze several aspects of aligned models and discover novel insights regarding their nature. We further apply aligned models for additional tasks and demonstrate good performance. \nWe also agree that MUNIT [5], as well as others [6,7], have discussed shared attributes in latent space for image-to-image translation (we mention it in second paragraph on page 4). However, we note that such works have devised a dedicated architecture to encourage a shared latent space. Conversely, in our framework this phenomenon occurs without any task-specific encouragement or intervention. \n\n2. We observed that the mapping network changes very little during fine-tuning. This implies that a single z code is mapped to similar w codes. While the correspondence of images is demonstrated in the paper (Figure 1), we agree that the similarity of w codes is not explicitly shown. According to your feedback, we have now performed an additional experiment in order to show this. We measure the L1 distance between w codes inferred by different mapping networks from the same z code. The results (reported in a new Table 7, end of appendix) clearly demonstrate that indeed the w codes are very close in latent space for models that are closely related. We refer to this as “point-wise alignment”, but indeed perhaps a better term could be used - we’re open to suggestions.\n\n3. We consider working *only* with aligned shapes a limitation of several existing I2I methods. We thus believe comparison on domains with different shapes to be crucial. Nevertheless, we agree a comparison on aligned shapes would improve our evaluation more complete. We used our approach to translate edge maps to shoe images (see new Figure 37). We note that we do not rely on the paired information (like pix2pix and similar). \nIt may be seen that our method maintains the structure well, and the generated texture is realistic. Due to time constraints, we were not able to train CUT or F-LSeSim yet, but will include the comparison in a future revision.\n\n4. As previously mentioned, a known limitation [8] of several I2I methods is that they are unable to change shape and instead change mostly the texture. This indeed causes StarGAN-v2’s results to be more similar to the source image. However, this comes as the cost of generating images that are less realistic for the target domain. For example, in Figure 6, on the second and third row on the right, the generated images have a structure of a wild animal and not of a dog, detracting from their realism. Unfortunately, the task of image-to-image is ill-defined in the first place. We believe our method, StarGAN-v2 and OverLord, each achieve different aspects. Please see our response to reviewer nPWN, where we discuss this in more detail.\n\n5. Figures 24-27 demonstrate cross-domain image morphing. StyleGAN inherently supports in-domain image morphing through interpolation of latent codes. However, a single StyleGAN can only generate images in a limited domain. Training a single StyleGAN on both FFHQ and AFHQ dogs will not result in high quality generated images. Therefore, traditional StyleGAN-based methods can morph well between human faces or between dog faces, but not between humans and dogs. In contrast, we perform cross-domain image morphing (human face and dog face) using aligned models. Specifically, we interpolate the model\'s weights and latent codes simultaneously. We show a smooth transition from dog2cat in Figure 24 and 25, dog2human in Figure 26 and 27.\n\n\n[1] Pinkney/Adler. ""Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains.""\n\n[2] Kwong et al. ""Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network.""\n\n[3] Wang et al. ""Deep network interpolation for continuous imagery effect transition.""\n\n[4] Song et al. ""AgileGAN: stylizing portraits by inversion-consistent transfer learning."" \n\n[5] Huang et al. ""Multimodal unsupervised image-to-image translation.""\n\n[6] Liu et al. ""Few-shot unsupervised image-to-image translation.""\n\n[7] Liu et al. ""Unsupervised image-to-image translation networks.""\n\n[8] Gabbay/Hoshen. ""Scaling-up Disentanglement for Image Translation.""\n\n\n'}, {'title': 'reply to reviewer nPWN ', 'comment': 'We thank the reviewer for time and thoughtful feedback. Below we address the main points raised in this review.\n\n* Novelty: indeed, as pointed out in most of the reviews, the main novel contribution of our paper lies in the extensive empirical analysis of transfer learning in the case of the StyleGAN architecture and the applications that become possible due to the semantic alignment of the latent spaces. \n    1. Specifically, we show that multiple channels maintain their function after fine tuning, for both nearby and distant domains. This single channel level analysis is more fine grained than previous works. \n    1. We propose a way to quantitatively measure the channel alignment between parent and child.\n    1. We show the knowledge from the parent model that is not utilized in the child model remains inactive in the latent space, rather than forgotten.  \n\n* We agree that OverLORD’s results sometimes resemble the reference image more closely than our results. On the other hand, they don’t always capture well enough the pose or the structure of the source image (for example, the eyes of the source dog are open in the top left example, but in OverLORD’s result the eyes are closed, as in the reference; in the bottom right the ears of the source are erect, but OverLORD’s result takes the shape of the ears from the reference). We believe our method, StarGAN-v2 and OverLord, win in different aspects. This is due to the ambiguity of content-style separation. We believe a more fine-grained version of disentanglement should be (pose, structure, texture, color) separation, where the different aspects are explicitly. A good disentanglement method should allow users to create images based on these 4 explicit aspects (pose, structure, texture, color), rather than the ambiguous (content, style) aspects.   \n    1. StarGAN-v2 takes the (pose, structure) from source images, and creates unrealistic structures in the reference domain (1st example in dog2cat, all examples in wild2dog). \n    1. OverLORD takes mainly the pose from source images (for wild2dog case, part of pose is taken from reference), therefore it preserve the appearance of the reference well, but sometimes fail to capture the pose and structure (e.g., ear shape) from the source image (2nd and 3rd examples in wild2dog). \n    1. Our method takes the pose from the source domain, and adapts the source structure to the reference domain (therefore it will not create unrealistic structures in the reference domain), the texture and color are from the reference domain. Our method thus attempts to create a balance between source and reference domain.\n'}, {'title': 'reply to reviewer fnkZ ', 'comment': 'We thank the reviewer for time and thoughtful feedback. Below we address the main points raised in this review.\n\n* While the focus of our work is on the extensive empirical analysis of transfer learning in the case of the StyleGAN architecture and the applications that become enabled by semantic alignment of latent spaces, we do believe that there are also several contributions in the applications themselves: \n    1. We show the Toonify based method can work for more distant domains (dog2cat), rather than just nearby domains (ffhq2cartoon).\n    1. The Toonify based methods are mainly used in I2I tasks, we show its ability for multi-modal image translation tasks.  \n    1. In supplementary A.4 and Figure 30, we propose a way to obtain a low-resolution child model from a high-resolution parent model, while keeping the semantics of each channel. This saves considerable computation time, compared to training and analyzing an unrelated low-resolution model.\n\n* Thanks for referring us to [R1]. We were not familiar with this work at the time of writing and have now revised the text to cite it properly (See page 8). In addition to the revised text, we want to explicitly point out that [R1] discusses a transition between the effects of two different networks. For example, the neworks might do style-transfer for different styles, and their method is able to create styles in between. We, on the other hand, morph two real images, possibly from distant domains. For example, creating a hybrid between a person and a dog. For this end, beyond weights interpolation we also invert the real images and interpolate the latent codes.\n\n * Although [R1] shows the filter patterns of the same channel remain similar after fine tuning, we show the actual function of the channel remains the same. We believe that examining the channel function is more important than its filter pattern, since two channels may have the same filter pattern but completely different function. \n[R1] also shows weight interpolation between aligned models, but mainly for nearby domains (day2night, image style transfer), which only requires changes in color and texture.  We show results for distant domains (dog2cat, ffhq2dog), which require changes in both structure and texture. It is worth mentioning that we interpolate model weights and latent codes simultaneously, which allows us to interpolate two unrelated images (a black cat and a yellow dog with different pose). \n\n* Can we just finetune the conv part and fix all other parts for better translations?\nThis essentially would result in a StyleGAN model custom-tailored to the translation task. We did a related experiment, but the performance was unsatisfactory. Specifically, when using aligned models to do real image translation, there are two issues: \n    1. The latent space (W+) of the parent and child model may not be well aligned, since the mapping function may change during fine tuning. Inverting the real image into Z space may alleviate this problem, but at the cost of reconstruction quality (the translated image might not resemble input image well enough). \n    1. The generative W+ distribution may not be the same as the W+ distribution of inverted images. For example,  [1] show that latent optimization in W+ space results in latent codes that lie outside the generative distribution. Manipulating these latent codes creates significant artifacts in image space. \nWe have attempted to resolve these issues for cartoonization, but the resulting model suffered from overfitting the noise present in the child domain (Mega). We leave further experiments to future research. \n\n* Application of “the hidden latent semantics” between parent, child and grandchild\nWe believe this property could be used as a “watermark” for a trained model. If someone uses without permission a trained model (parent) to train another model (child) on some other dataset, it can be shown that the child model is inherited from that parent model through retraining it to the parent’s dataset (grandchild) and measuring the alignment.  \n\n* In Figure 34, we add corresponding images between FFHQ parent LSUN church child. More results could be found in Figure 35 and 36. \n\n* We were not aware of the ‘blend_width’ parameter in Pinkney’s implementation, thank you for pointing it out to us! In the Toonify paper (https://arxiv.org/pdf/2010.05334.pdf), equation 2, the interpolation parameter is defined as either 0 or 1, which results in a discrete layer swap, so there seems to be a mismatch between the paper and the implementation. \n\n[1].  Wu, Zongze, Dani Lischinski, and Eli Shechtman. ""Stylespace analysis: Disentangled controls for stylegan image generation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n'}, {'title': 'reply to Reviewer 2pb8', 'comment': 'We thank the reviewer for time and thoughtful feedback. Below we address the main points raised in this review.\n\n* Why does the Age example in Figure 2 change identity, etc.\nThe latent direction for Age is obtained through InterfaceGAN [1], which operates in W space. It is well-known that some of the directions discovered by InterfaceGAN are somewhat entangled. The point we’re trying to make in Figure 2 is that the changes induced by different latent directions are similar between the parent (FFHQ) and the children models (Mega and Metface).\n\n* How do we perform the editing in Figure 2 and other figures, like Figure 14?\nFor the first 3 attributes in figure 2 (Bangs, Smile, Gaze), we simply manipulate a single channel in S space [2] (the layer and channel numbers are shown under each column). The same layer/channel is manipulated in all three models, and the results appear similar because of alignment. For example, 3_169 (layer 3, channel 169) consistently controls bangs in all three models. For the last 3 attributes (Pose, Age, Gender), we use InterfaceGAN [1] to get the manipulation direction in W space. These directions modify the entire W vectors, which affects all layers and all channels (through affine transformations), but the same manipulation direction is used for each manipulation for all three models. In Figure 14, exactly in the same manner, the latent direction in W that is known to control pose of faces (obtained using InterfaceGAN on FFHQ model), is used *as is* in models fine-tuned to churches (child) and then to bedrooms (grandchild) and still results in a change of pose in these models.\nThe channel overlaps reported in Table 1 are unrelated to the manipulation results, they only serve as quantitative evidence of semantic alignment.\n\n* Does the claim of “the hidden latent semantics” hold for the case of (parent FFHQ, child Church, grandchild FFHQ)?\nWe believe the answer is yes.\nDue to time limitations, for this rebuttal we could only train the grandchild FFHQ model from the child church model. Extracting all the localized channels reported in Table 1 requires more than a week of computation. Nevertheless, we added a new figure (Figure 34), where we show that the parent and grandchild FFHQ models generate face images with highly similar attributes and identity, given the same input noise vector z. \nIn another new figure (Figure 36), we show the same channel still controls the same attribute between parent and grandchild model, even though it had no visible effect in the child Church model. \n\n* Further discussion on the “real distant domain”, e.g., face VS church. For example, is it helpful to transfer learning from face to church? Are there any other shared semantics between face and church except pose?\nTransferring from FFHQ to church increases the model convergence speed, but does not help to improve the FID. As for additional shared semantics, in a new figure (Figure 35), we show that the “beard” direction from FFHQ affects the amount of trees in the church model, and the black hair direction from FFHQ controls the darkness of the building in the church model.\n\n\n1. Shen, Yujun, et al. ""InterFaceGAN: Interpreting the disentangled face representation learned by GANs."" IEEE transactions on pattern analysis and machine intelligence (2020).\n1. Wu, Zongze, Dani Lischinski, and Eli Shechtman. ""Stylespace analysis: Disentangled controls for stylegan image generation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n'}, {'summary_of_the_paper': ' \nThe paper provides interesting analysis and leveraging of GAN’s model alignment (i.e., transfer learning). Without custom architectures and losses, it demonstrates impressive performance in a diverse set of tasks (image translation and image morphing). It also demonstrates promising results for zero-shot image recognition by leveraging the shared latent space of aligned models. \n', 'main_review': '##########################################################################\n\nPros:  \n1. The paper performs the first detailed exploration of model alignment based on StyleGANs. Considering the wide-range applications of StyleGAN, I think the analysis is timely, insightful, and interesting!\n2. To analyze the model alignment during transfer learning from the source domain to similar/distant target domains, the paper uses weight resetting and quantitative channel alignment measurement. These techniques probably inspire others to analyze their GAN models.\n3. The paper demonstrates impressive and promising experimental results on image translation and image morphing between different domains. Besides, it demonstrates the benefits of the shared latent space by zero-shot recognition/regression. \n \n##########################################################################\n\nCons: \n1. I’m confused about the example of “Age” in Figure 2. Why does the “Age” change the identity, background, and color largely? It looks like another totally different face.\n2. Many details of the results are missing. Take Figure 2 as an example, does the semantic control apply in overlap channels (e.g., 15 for eyebrow in FFHQ2MetFace) or all the distinct channels of the child model (e.g., eyebrow 35)? Are they in the same layer? How do the authors transfer the pose control from face to church and bedrooms in Figure 14? Please check all the results and provide the necessary details. \n3. Does the claim of “the hidden latent semantics” hold for the case of (parent FFHQ, child Church, grandchild FFHQ)?\n4. I think most of the successful results can be attributed to the well-structured properties of the human face, animal face, and mega face. It would be helpful to provide further discussion on the “real distant domain”, e.g., face VS church. For example, is it helpful to transfer learning from face to church? Are there any other shared semantics between face and church except pose?\n', 'summary_of_the_review': ' \nOverall, I vote for accepting. I like the analysis of aligned GANs’ models. The paper also demonstrates impressive experimental results by leveraging the properties of aligned models for image translation, image morphing, and zero-shot classification. My major concern is about the clarity of the paper and some missing details. Hopefully, the authors can address my concern in the rebuttal period. \n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper provides an in-depth study of the properties and applications of the semantic alignment between the original parent StyleGAN model and its finetuned child model on another dataset. Specifically, the paper empirically demonstrates the semantical alignment of the two models. Then, based on the properties, the paper solves serval tasks like image translation, image morphing, zero-shot image editing and attribute classification. ', 'main_review': '**Strengths:**\n+ There are many works using aligned StyleGAN models but without deep analyses on the properties. This paper provides an in-depth study of the properties, which provides some insights on the aligned models and might inspire following more complex researches. \n+ The organization of the paper is good and the analyses are comprehensive. Several observations are studied with quantitative/visual analyses to prove key properties, followed by corresponding applications utilizing the properties, making the paper easy to follow and concrete.\n\n**Weaknesses:**\n+ While the part of the property study is comprehensive and interesting and provides insights, the part of the application is less insightful. The application involves the image translation, image morphing and zero-shot vision tasks. \n  - Image translation and image morphing are intuitive and well-studied in previous studies, as also pointed out in the paper, which might provide limited insights. Image morphing by interpolating aligned models has been studies in `[R1]`.\n  - The zero-shot vision tasks involve utilizing the label of the parent domain to edit/classify the child domain, which gives some novel ideas but alone are not very thorough to me.  \n  - I think it will be valuable to investigate the applications in terms of the specific properties. For example, only the conv parts of network change greatly for close domains, which supports translations. Can we just finetune the conv part and fix all other parts for better translations? (which is also discussed in `[R3]`) The paper discusses that the latent semantics are hidden rather than forgotten during finetune. It will be valuable to also discuss the potential application of this property.\n\n**Some small issues:**\n+ In Fig. 14, the authors demonstrate the semantic alignment between the human face and the church. However, only the church images are shown. The corresponding face images using the same latent z are suggested to be added to help the readers better find their correspondences.\n+ In Sec. 4.2, the paper claims that the layer swapping performs the transition as a series of discrete steps, rather than continuously. In the implementation of Pinkney `[R2]`, the layers can be smoothly swapped through the parameter ` blend_width` instead of hard swap, which might be able to perform the transition continuously.\n\n`[R1] 2019 CVPR Deep Network Interpolation for Continuous Imagery Effect Transition`\n\n`[R2] https://github.com/justinpinkney/toonify/blob/master/StyleGAN-blending-example.ipynb`\n\n`[R3] 2021 TMM Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network`\n', 'summary_of_the_review': 'This paper investigates an important topic of the semantic alignment of the parent model and finetuned child model. Despite the application part could be further enriched to better match the analyzed properties, the comprehensive analyses on the properties provides many insights and might arouse following researches. Therefore, I am positive.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This work is about the task of transfer learning to tame a new ""child"" network using a pre-trained ""parent"" network. While the model and fine-tuning technology lack novelty, the shared semantic information in the generation network are interesting. Finally, the authors applied the proposed aligned model to multiple tasks, including image-to-image translation, cross-domain image morphing, and zero-shot classification and regression. The impressive results with shared semantic information are achieved. ', 'main_review': 'Pros:\n1. The shared latent space for ""child"" and ""parent"" networks is interesting, and matches the learning representation goal. \n\n2. The overall idea and the proposed fine-tuning method towards analyzing the shared latent space is reasonable. The StyleGAN-based architecture is a powerful structure, which consists of a mapping network, an affine translation, and a generator. This is exactly suitable for exploring feature attributes at different levels.  \n\n3. The paper provides many interesting visual results, such as in Figures 1, 2, 3, and 4, to show the effectiveness of shared information in different networks. These interesting conclusions may contribute to the generation community to design better generator works. \n\n4. The wide applications are being met by subtly using the shared semantic information between different domains. \n\nCons:\n\nWhile the impressive results are achieved, I believe some parts need more clarification (even after considering the supplementary material):\n\n1. The key limitation of this work is the novelty. If I understand correctly, the authors just fine-tuning the different parts of the StyleGAN. This is very similar to the existing works AgileGAN and the shared attributes have also been mentioned in prior MUNIT. It takes me a hard time to buy the novelty of models, techniques, and theoretical insights. \n\n2. The authors claimed the ""two W spaces are point-wise aligned"" on page 4. While they showed some shared attributes in Figure 2 and Table 1, and they also demonstrated some attributes are hidden, I do not believe these attributes are ""point-wise"" aligned. Is this conclusion correct? How to demonstrate it?\n\n3. A quantitative and qualitative comparison with the latest CUT and F-LSeSim is given in Figure 5. While the improved results are provided, the original CUT and F-LSeSim work for aligned shapes. Why not provide some aligned examples, for example, horse2zebra, night2day, apple2orange, every two domains share a similar shape, but different appearances? This would be more robust to demonstrate the effectiveness of aligned style on both aligned examples (Figure 5) and unaligned examples (Figure 6).\n\n4. In Figure 6, the StarGAN2 seems to be able to provide more consistent results to the source image, such as the smiling mouth in the first row, and the same head poses for the third row. Except for the FID score, it seems the StratGAN2 provides better translated results (better shared content, such as pose and expression). \n\n5. The authors show abundant results in Figures 23, 24, 25, and 26. Similar results are also being met in the StyleGAN-based method. I do not fully understand the key challenge in such a situation.  ', 'summary_of_the_review': 'As can be inferred from the balance in strengths/weaknesses, my preliminary rating for this submission is borderline accept. While the interesting results are shown in the paper and supplementary material, the key contribution for models, techniques or theoretical insights is unclear. Furthermore, some comparisons seem unfair in the paper. The authors should clearly interpret them during the rebuttal. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The paper undertakes extensive experimentation in the adaptability of latent space modifications from a base model, and a fine-tuned model for a secondary dataset. They demonstrate which areas of the models change most, and which information remains trained in the parameters, despite re-training. They go on to use their findings in downstream experiments, reaching state-of-the-art quality.', 'main_review': ""Weaknesses\n\n\tThe main weakness for this paper is likely the limited novelty in the technical contributions, given that transfer learning is a fundamental, established technique. However, this paper is more of a study of this technique, applied to the particular domain of image GANs, and how their editability changes during the process.\n\n\tThe qualitative results do seem high quality, but it is sometimes quite difficult to see where the resemblance to the reference images are (especially with Figure 6, for the dogs dataset). Perhaps the translation is not focused strongly enough on the reference images. Despite more artefacts being present, I'd say OverLORD images are a better transfer, for both cats and dogs.\n\n\n\nStrengths\n\n\tThe primary contribution of the paper is the extensive experimentation, around transfer learning in GANs, and they perform this very well. Their approach is very well documented, is very thorough, and convincing. The supplementary videos are helpful in further displaying their results.\n\n\n\tAlthough there are aspects that are both better and worse, the samples from their models compared to other models, are mostly good. The benefits arise mostly from the editability of the output, given their findings that the channels maintain their editing effects from previous datasets with more labels."", 'summary_of_the_review': 'The paper presents some novel analysis in a popular field of research, with implications that can help drive the field further, with minimal changes.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'StyleAlign: Analysis and Applications of Aligned StyleGAN Models', 'authorids': ['~Zongze_Wu2', '~Yotam_Nitzan1', '~Eli_Shechtman3', '~Dani_Lischinski2'], 'authors': ['Zongze Wu', 'Yotam Nitzan', 'Eli Shechtman', 'Dani Lischinski'], 'keywords': ['StyleGAN', 'transfer learning', 'fine tuning', 'model alignment', 'image-to-image translation', 'image morphing'], 'abstract': ""In this paper, we perform an in-depth study of the properties and applications of aligned generative models.\nWe refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion. "", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'wu|stylealign_analysis_and_applications_of_aligned_stylegan_models', 'pdf': '/pdf/a75f48f49713ac38baaaee51cb3273177975f96b.pdf', 'one-sentence_summary': 'Analysis and applications of aligned generative models', 'supplementary_material': '/attachment/feb237822a8f823627b38f1d6c8b71432d2daf0e.zip', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/stylealign-analysis-and-applications-of/code)', '_bibtex': '@inproceedings{\nwu2022stylealign,\ntitle={StyleAlign: Analysis and Applications of Aligned Style{GAN} Models},\nauthor={Zongze Wu and Yotam Nitzan and Eli Shechtman and Dani Lischinski},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=Qg2vi4ZbHM9}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Kohei Miyaguchi', 'Takayuki Katsuki', 'Akira Koseki', 'Toshiya Iwamori']",ICLR,Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion,https://iclr.cc/virtual/2022/oral/6534,2022," We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. In this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically.",Oral 2: Structured learning,https://openreview.net/pdf?id=qnQN4yr6FJz,https://openreview.net/forum?id=qnQN4yr6FJz,qnQN4yr6FJz,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'While generative model can be used to input data, this work propose to a novel discriminative learning approach to optimize this data imputation phase by deriving a discriminative version of the traditional variational lower bound (ELBO). The resulting bound can be estimated without bias with Monte Carlo estimation leads to a practical approach, leading to encouraging experimental performances.\n\nThe reviewers recognised the novelty and suggest that this approach, given its novelty and wide applicability, could be considered for an oral presentation.'}, {'title': 'On additional experiment', 'comment': 'We have decided to leave the MNAR experiment as future work\nconsidering the necessity and the time constraint.\n\nNote that MCAR is already suitable for demonstrating \nan advantage of vDIG since it can be captured only with the generative approach and the DIG approach,\nnot with the discriminative approach.\nOn the other hand, MNAR in general can be captured with all of these approaches\nand we need to make careful design choices to specify subclasses of MNAR meaningful for the comparison.\n\nThus, it is an interesting direction for future study\nto identify specific types of the feature corruption process relevant to some applications\nand see if vDIG can be useful to solve the associated prediction problems.\n'}, {'title': 'On generative process and dataset scales', 'comment': ""Thank you for additional feedbacks and questions.\n\n> The modification about adding the generative process for MCAR, MNAR and MAR is very helpful. I'm wondering why the conditional distribution of label $y$ is always irrelevant of the observed features $x$ in all the three cases. Is this part of the constrains you set in the assumption of the generative process?\n\nIn our formulation, the dependence between $x$ and $y$ is captured through $z$. Note that Equation 5 and 6 are *joint* distribution of ($x$, $y$, $z$). The distribution of $y$ is relavant to the feature $x$ after $z$ is marginalized. This kind of independence among varialbes in the $z$-*joint* density is common in the VAE literature, as in the original paper (Kingma & Welling, 2013).\n\n> From the authors' response, it only took less than a minute to train on the largest dataset (YearPred) used in the submission. Why the algorithm was not tested on any larger datasets? The datasets in table 3 are all at very small scale.\n\nThe reason is that\nthe scalability of vDIG is not the primary subject of our experiment.\nIn particular, the scalability of DVAE is inferable from those of VAE and CVAE\nsince they are all SGD-based and the numbers of the parameters are not that different owing to DVAE's parameter sharing among $\\phi$, $\\psi$ and $\\xi$.\nSee below for more information.\n\n### Computation time comparison\n\nTo demonstrate how the scalability of DVAE is inferable,\nwe report the training time (only gradient computation and parameter update are accounted, the saving/evaluation times are ignored)\nof each method for YearPred.\nIt is seen that DVAE costs around 2.25x CVAE or 1.2x VAE.\nThe reason of DVAE being almost twice as expensive as CVAE is, supposedly, because\nthere are two encoding-decoding computation paths (i.e., one with $z_\\phi$ and one with $z_\\psi$).\nIt is slightly more expensive than twice (2.25x > 2x), which is attributed to the unsharable parameters of $\\psi$ and $\\xi$.\nOn the other hand, the reason of VAE being almost twice as expensive as CVAE is also because\nthere are two paths corresponding to the doubled minibatches for learning the imputation of $y$ (see Section 4.1).\nNote that the training time of MICE is not reported\nsince we did not measure the time of imputation with MICE (we only measured the time for the gradient computation and parameter updates *after* MICE, which is the same as Simple).\n\n|       |     Time [sec] |\n|:------|---------:|\n| DVAE  | 35.9171  |\n| DVAE* | 35.1979  |\n| CVAE  | 16.3529  |\n| VAE   | 30.6778  |\n| VAE*  | 29.8359  |\n| Simple |  7.51111 |\n\n### Total execution time of the current setup\n\nThe current experimental setup already costs ~10 hours to execute in total.\nThere are multiple factors that blow up the total execution time.\nFirst, the bottleneck of our experiment is in fact not the training time,\nbut the time to save/evaluate the model for each 20 checkpoints per single run (the time we have just reported, 39.5 sec, does not include this).\nNote that placing 20 checkpoints are rather overdoing for a single algorithm,\nbut necessary for fair comparison among different algorithms with different optimal early-stopping timings.\nMoreover, if we adopt larger datasets, then probably we need more checkpoints.\nAnother factor is the training time of MICE, whose scalability is the worst supposedly because of its CPU-based implementation.\nFinally, recall that the total time of the experiment is (roughly) multiplied with\nthe number of configurations, namely, (7 methods) x (6 datasets) x (5 random seeds) x (2 missing-value ratios) = 420 configurations.\n""}, {'title': 'Questions on the assumptions in the generative process and the dataset scales', 'comment': ""Thank you for clarifying all my questions. \n\nThe modification about adding the generative process for MCAR, MNAR and MAR is very helpful. I'm wondering why the conditional distribution of label $y$ is always irrelevant of the observed features $x$ in all the three cases. Is this part of the constrains you set in the assumption of the generative process?\n\nFrom the authors' response, it only took less than a minute to train on the largest dataset (YearPred) used in the submission. Why the algorithm was not tested on any larger datasets? The datasets in table 3 are all at very small scale.""}, {'title': 'Further response', 'comment': '> Right, the condition of the exact tightness is not likely to be achieved in practice with complex parameter spaces such as those induced by neural nets. However, we still believe it is sufficient to give some justification on the use of CELBO-SP as a surrogate of CELBO. Please refer to Section H, which we have added for the clarification and additional discussion on this point.\n\nThank you for clarifying this and adding Section H in the supplementary material.\n\n> The models $p(y,x,z|m)$ can be more general than exponential families. Specifically, in the example of VAE, $y$ and $x$ depends on an NN-based transformation of $z$ with weight parameters , which implies it is not exponential family in general. In fact, both Ghahramani & Jordan, 1994 and Smola et al., 2005 solve the optimization of the mixture of exponential families either with the EM or DCA algorithm.\n\nIt seems to me that standard VAEs assume both the decoder distribution $p(x|z;\\theta)$ and the encoder distribution $q(z|x;\\phi)$ to be exponential families. If we denote e.g. the decoder network by $d(z,\\theta)$, we have\n$$ p(x|z;\\theta) = \\exp[<\\psi(x),d(z,\\theta)> - A(x,\\theta)]$$\nwhere $\\psi(x)$ denotes the sufficient statistics of the family. Thus, the decoder output represents the natural parameter vector as a function of its parameters $\\theta$. I understand that this ""re-parametrisation"" prevents a direct application of the methods from  Ghahramani & Jordan, 1994 and Smola et al., 2005. However, I would not exclude that something can be done here.\n\nThank you for your comments and clarifications. I will keep my score (accept).'}, {'title': 'Thank you', 'comment': 'Thank you for the helpful comments and suggestions.\nSee below for the answers to your questions and comments\n(before proceeding to the updated manuscript, note that we have simplified a part of the notation, denoting the pair of observable features $(\\tilde{x}, m)$ with $u$).\n\n\n> The topic of this paper is to focus on missing data. However, this paper does not put enough efforts on learning various cases of data missing patterns. As in the experiments, the authors only test the case where the data are missing completely at random (MCAR), which may not be the most common case in reality. MNAR case might be a more interesting situation to study with. The proposed method is mostly focusing on solving the variational upper bound, while overlooks modeling the missing patterns. Suggest the authors could add some experiments with MNAR data.\n\nWe are willing to conduct additional experiments with MNAR datasets.\nPlease wait until we figure out if we can carry it out within the discussion period.\n\n\n> Also it would be better if the authors could add some modeling part on the missing pattern into the loss. For example, let the mask $m$ to depends on $(x, y, z)$. This will make the proposed model more useful in practice.\n\nWe have generalized the notation to allow such flexible modeling of missing data.\nWe have also added some explicit examples of missing data modeling.\nPlease refer to Section 2.2.\n\n\n> From Proposition 2, we know that the surrogate parameterization can make the optimal solution of CELBO remains under the zero gap case. However, it is nearly impossible to reach the zero gap case in reality since it is unlikely to select variational distributions (i.e. $q(\\cdot)$\'s) to perfectly estimate the model posterior distributions. What about the ""sub-optimal"" cases? Is the CELBO-SP optimal solution close to the CELBO optimal solution in a small gap (but not zero gap) case? I understand this might not be easy, but it will be better if the authors could add some theoretical analysis on this.\n\nRight, the condition of the exact tightness is not likely to be achieved in practice with\ncomplex parameter spaces such as those induced by neural nets.\nHowever, we still believe it is sufficient to give some justification on the use of CELBO-SP as a surrogate of CELBO.\nPlease refer to Section H, which\nwe have added for the clarification and additional discussion on this point.'}, {'title': 'Thank you', 'comment': ""Thank you for the helpful comments and suggestions.\nSee below for the answers to your questions and comments\n(before proceeding to the updated manuscript, note that we have simplified a part of the notation, denoting the pair of observable features $(\\tilde{x}, m)$ with $u$).\n\n\n> Sec. 3.1: More detailed explanation of the exponential divergence would be beneficial. Is there a reference for it? What role does $f(u;\\xi)$ play? If it can be any real-valued function, why was it chosen to be a Gaussian pdf, as shown in the appendix?\n\nThe name 'the exponential divergence' does not refer to specific existing divergences.\nIt is just a function used to derive EUBO.\nGiven your feedback, we think giving it a name is confusing and decided to drop it.\n\nThe role of $\\Psi_\\alpha$ can be understood as giving linear upper bound approximation of likelihood.\nIn this context, the role of $f(u;\\xi)$ is then understood as a parameter of the tangent point,\n$x_0=e^{\\alpha f(u;\\xi)}$.\n\nAs for taking $f(u;\\xi)$ as Gaussian pdf in the appendix, it is a typo.\nIt should be a linear layer with a single output, not a Gaussian layer.\nSorry for making confusion. We have fixed it.\n\n\n> Sec. 3.2: Which standard automatic differentiation library was used? The submission mentioned both the reparameterization trick and the REINFORCE trick - which one was actually used in the experiments?\n\nWe used the reparametrization trick (updated Section B, First-order optimization).\n\n\n> Sec. 3.3: I don't quite understand how this part works. ... But why $G$ was defined in that math format? What does $\\vee$ mean? Why does $G$ represent the ratio before and after the transform (equation between proposition 2 and 3)?\n\nThe specific form of $G$ is chosen to obtain the desired properties (i.e., Proposition 2 and 3).\nWe design it to be smooth and cause minimal, yet sufficient change in the density ratio $w$ to derive Proposition~3.\n$a\\vee b$ denotes the pairwise maximum of $a$ and $b$ (sorry for the lack of the definition, we have fixed it).\n$G$ representing the ratio before and after the transform is shown by plugging the transformed parameters $(\\theta',\\psi,\\xi')$ into the definition of the density ratio $w_{\\theta,\\psi,\\xi}(u,z)$.\n\n> Sec. 4.1: Missing value processes (MCAR and MNAR). What do they mean? Are they different ways to decide what values are missing in the feature, and thus leading to different versions of a dataset? If so, shouldn't we also add CVAE\\*, Simple\\* and MICE\\*? Could you give more explanation for the last sentence of Section 4 (saying DVAE\\* is more robust than DVAE)?\n\nSorry, we mistakenly dropped the citation there (added in the new manuscript: see Section 2.2).\nThey are standard types of the assumptions on the missing value mechanism given by Rubin 1976.\nIn short, MNAR is no assumption while MCAR is a strong assumption that missing value is independent of $x,y$.\nSince MCAR is an assumption on the process of feature corruption,\nit is impossible (at least in a trivial way) to incorporate it in discriminative methods such as CVAE and Simple (note that this is the major motivation behind the DIG method).\nOn the other hand, MICE is derived under another assumption, namely MAR,\nso there is no MCAR and MNAR variants.\n\nThe last sentence of Section 4 saying the DVAE\\* is more robust than DVAE\nis reflecting the fact that MCAR is a stronger and correct assumption (in this setting) than MNAR.\n\n\n> Size of the test datasets. ... In the appendix it's said the minibatch size is 521 -- what if the entire training set is smaller than 512? How long did the algorithm take to run on YearPred? Is the algorithm able to be easily extended to larger datasets?\n\nIf the data size is smaller than 512, then the minibatch size is reduced to the data size.\nThe training time of DVAE for YearPred was 35.9 seconds.\n\n\n> How does DIG works compared with other more recent imputation baselines such as MIWAE (Mattei & Frellsen, 2019) and GAIN (Yoon et al., 2018)?\n\nThanks for the pointers to the reference.\n\nThe contribution of variational algorithms for likelihood-based generative models, including MIWAE, is orthogonal to ours.\nIn particular, since MIWAE is an algorithm that gives tighter lower bound for missing-data likelihoods,\nit can be combined with vDIG at least to improve the first variational posterior $q(z|y,\\tilde{x},m,\\phi)$.\nThe applicability to the second posterior $q(z|\\tilde{x},m,\\psi)$ is more intricate since\n$\\psi$ is a parameter for the upper bound approximation, which is out of MIWAE's (and most of other VI methods') scope.\n\nOn the other hand, the comparison/combination with GAIN is less straightforward\nsince GAIN's objective function is not the log likelihood, but the adversarial error.\nWe would like to left it as future work.""}, {'title': 'Thank you', 'comment': 'Thank you for the helpful comments and suggestions.\nSee below for the answers to your questions and comments\n(before proceeding to the updated manuscript, note that we have simplified a part of the notation, denoting the pair of observable features $(\\tilde{x}, m)$ with $u$).\n\n> The authors prove that the transformation used for the reparametrisation preserves the effective parameter subset, i.e. the subset of parameter combinations for which the overall bound is tight. This is indeed a desirable property, but is in my view not sufficient. The reason is, that this effective subset can be very small and cover a subset of simple models only. Moreover, there is no guarantee that the respective gap will become small during learning.\n\nRight, the condition of the exact tightness is not likely to be achieved in practice with\ncomplex parameter spaces such as those induced by neural nets.\nHowever, we still believe it is sufficient to give some justification on the use of CELBO-SP as a surrogate of CELBO.\nPlease refer to Section H, which\nwe have added for the clarification and additional discussion on this point.\n\n\n> You mention earlier works (Ghahramani & Jordan, 1994; Smola et al., 2005), noting that their applicability is restricted to exponential families. Please explain whether these approaches are / are not applicable for the model class analysed in your work. As I understand it, the models p(y,x,z|m) considered by you are exponential families, but of course after marginalising over z, the resulting mixture model p(y,x|m) is not any more. It remains however unclear to me, whether a DCA (difference of convex functions algorithm) for learning p(y|x,z,m) can be somehow generalised for learning p(y|x,m).\n\nThe models $p(y,x,z|m,\\theta)$ can be more general than exponential families.\nSpecifically, in the example of VAE, $y$ and $x$ depends on an NN-based transformation of $z$ with weight parameters $\\theta$,\nwhich implies it is not exponential family in general.\nIn fact, both Ghahramani & Jordan, 1994 and Smola et al., 2005 solve the optimization of the mixture of exponential families\neither with the EM or DCA algorithm.\n\n\n> I would suggest to drop the data instance superscript earlier in the text, e.g. starting from subsection 2.2. the latest. This would in my view improve readability and reduce clutter.\n\nThanks for the suggestion. We have updated the manuscript accordingly.'}, {'title': 'Thank you', 'comment': 'Thank you for the helpful comments and suggestions.\nWe have updated the manuscript to make it explicit on the intentions and intuitions.\nSee below for the answers to your questions and comments\n(before proceeding to the updated manuscript, note that we have simplified a part of the notation, denoting the pair of observable features $(\\tilde{x}, m)$ with $u$).\n\n\n> last para of section 2 - the optimization of negative L ""is relatively difficult"". Why? What makes it difficult?\n\nThere is a typo. The \'and\' just after the phrase must be \'since\'.\nThus, the reason is because there have been no upper bound approximation of the evidence comparable\nto the VI for lower bounds in terms of scalability and flexibility.\n\n\n> last para of section 2 - ""... have been no equivalent of VI ... "" What about CUBO and its variants pick up from in your work? These have some specific flaws for which they do not qualify here?\n\nWe discussed it in Section 5. We have placed a pointer there.\n\n\n> before equation 3 - ""exponential divergence"". You mean the Bregman exponential divergence? A citation to help the reader?\n\nThe name \'the exponential divergence\' does not refer to specific existing divergences.\nIt is just a function used to derive EUBO.\nGiven your feedback, we think giving it a name is confusing and decided to drop it.\n\n\n> $p(u,z|\\theta)$ in equations (11) and (12) seem to use the same parameters $\\theta$ [...]. Is this in practice the same network with two outputs?\n\nIt could be. In the experiment, we employed the network structure you mentioned (see Appendix D).\nHowever, the proposed algorithm is designed without such structural assumptions.\nFor example, (11) and (12) (in the updated manuscript, (10) and (11)) can be implemented with different networks without any parameter sharing.\nIn this case, $\\theta$ represents the union of the parameters of both networks.\n\n\n> But then in equation (15) these use different $z_\\phi$ and $z_\\psi$ samples. How is this designed and trained in practice?\n\nIt is trained by any stochastic gradient-based optimization algorithms.\nWe believe there is no ambiguity in the procedure with/without the aforementioned parameter sharing.\n\n\n> page 5 - clarify notation for and explain the gain function; what is the intuition / purpose for it?\n\nIt is detailed in the next paragraph. We have added a pointer there.\nThe intuition is to suppress the divergence of the problematic density ratio.\n\n\n> Def 1 - effective parameters are those with gradient zero. "".. i.e. the set of parameters inducing tight variational approximation."" How zero gradient achieves this in a complex non-convex problem, i.e. can\'t this be a local non-tight extremum?\n\nIt is not the gradient.\nIt denotes the gap of the variational approximation (defined along with CELBO, see the paragraph just after equation (11)).\n\n\n> page 7 DVAE/ DVAE\\* - you say these are MNAR and MCAR model variants as in Collier at al. 2020. Can you clarify how these translate into your rather more complex model formulation and what specifically changes in the loss (especially the EUBO part)?\n\nWe have added explanation on this point in Section 2.2. (and placed pointers in Secion 4)\nThe difference between the MNAR and MCAR variants is the same between VAE-based and DVAE-based formulations,\nsince both VAE and DVAE has the same decoder structure (the difference is in encoders)\nand the missing-value processes are modeled with the decoder. \n\n\n> Why do you condition $y$ and $x$ on $m$ in equation (1). These are the complete $x$ data so they should not depend on the masking so that $p(y,x|m)=p(y,x)$. Or is this not true? Or is it the $y$ that depends on $m$? Or is it rather the $m$ which depends on $x$? (As in some values being more likely to be masked?)\n\nIn the general case of MNAR, both $x$ and $y$ can depend on $m$.\nWe have added explanation on this point in Section 2.2.\n\n\n> You introduce two latent variable models in equations (7) and (8). ... There is currently no link between the two (approximate) posteriors. Would it make sense to somehow link them? (Sorry, I don\'t know how and may not be possible, or not easily.)\n\nYour suggestion is a possible extension of our formulation.\nNote, however, that these posterior approximations are already liked (edit: typo - *linked*) together indirectly\nas these are designed to approximate different aspects of the same generative model.\n\n> What is the effect on the predictive conditional $p(y|x)$? Is it somehow sandwiched or not really due to splitting and modelling the two non-conditional log likelihoods separately?\n\nThere is no sandwich effect:\nThe effect is that the predictive conditional log likelihood is bounded from below.'}, {'summary_of_the_paper': 'The paper addresses the problem of predictive modelling with missing input features. The authors formulate the problem as a latent variable model, and in addition to the standard variational lower bound (ELBO) propose to use a variational upper bound based on CUBO (Dieng at al 2017) modified by an exponential divergence to solve the MC estimation of CUBO. They further propose surrogate parametrization to reduce the variance in the gradients. The experimental evaluation over standard regression UCI datasets with randomly dropped features shows marginal improvements over existing baselines.', 'main_review': '(+) pros / (-) cons\n-------------------\n\n(+) Predictive modelling over inputs with missing features is an important problem arising often in many application domains. This paper contributes this somewhat underexplored field.\n\n(-) The rather low documented performance benefits over simpler baselines do not justify the use of the complex model (combining 5? networks) proposed here as opposed to simpler VAE or CVAE variants.\n\n(+) The method and the various bounds introduced are mathematically intriguing, well motivated and potentially useful in follow-up research, however, ... \n(-) the paper is difficult to  follow and at places the reader is left guessing what the authors meant. This should be improved. Concretely\n1. last para of section 2 - the optimization of negative L ""is relatively difficult"". Why? What makes it difficult?\n2. last para of section 2 - ""... have been no equivalent of VI ... "" What about CUBO and its variants pick up from in your work? These have some specific flaws for which they do not qualify here?\n3. before equation 3 - ""exponential divergence"". You mean the Bregman exponential divergence? A citation to help the reader?\n4. $p(u, z | \\theta)$ in equations (11) and (12) seem to use the same parameters $\\theta$ though for (11) $u = (y, \\tilde{x})$ and for (12) it is $u = \\tilde{x}$. Is this in practice the same network with two outputs?\n5. But then in equation (15) these use different $z_{\\theta}$ and $z_{\\psi}$ samples. How is this designed and trained in practice?\n6. page 5 - clarify notation for and explain the gain function; what is the intuition / purpose for it?\n7. Def 1 - effective parameters are those with gradient zero. "".. i.e. the set of parameters inducing tight variational approximation."" How zero gradient achieves this in a complex non-convex problem, i.e. can\'t this be a local non-tight extremum?\n8. page 7 DVAE/ DVAE* - you say these are MNAR and MCAR model variants as in Collier at al. 2020. Can you clarify how these translate into your rather more complex model formulation and what specifically changes in the loss (especially the EUBO part)?\n\nFurther questions for clarification/discussion\n-----------------------------------------------\n\n1) Why do you condition $y$ and $x$ on $m$ in equation (1). These are the complete $x$ data so they should not depend on the masking so that $p(y, x | m) = p(y, x)$. Or is this not true? Or is it the $y$ that depends on $m$? Or is it rather the $m$ which depends on $x$? (As in some values being more likely to be masked?)\n\n2) You introduce two latent variable models in equations (7) and (8). My understanding is that the latent $z$ is shared as (8) is just a marginalization of (7) over $y$. You then formulate to approximate posteriors $q(z| y, \\tilde{x})$ and $q(z| \\tilde{x})$ the first learned through ELBO maximization, the 2nd through EUBO minimization. There is currently no link between the two (approximate) posteriors. Would it make sense to somehow link them? (Sorry, I don\'t know how and may not be possible, or not easily.)\n\n3) You used the Bayes rule to decompose the predictive conditional log likelihood into two terms in equation (4) of which one you are bounding from the bottom (ELBO) and the oher from top (EUBO). What is the effect on the predictive conditional $p(y | x)$? Is it somehow sandwiched or not really due to splitting and modelling the two non-conditional log likelihoods separately? \n\n\nMinor text problems / typos\n---------------------------\n\n1. This first proposition in in page 6 is numbered 2 (not 1) - confusing', 'summary_of_the_review': 'The paper contributes to practically very important yet relatively little explored area of research - that of predictive modelling with missing data. The proposed method is rather complex, composed of multiple steps adding onto each other to solve a problem arising in the previous steps. These are all well motivated, however, overall the current presentation of the method is difficult to follow and should be improved to help the reader (see main review). Moreover, the documented performance benefits seem to be rather little to justify the use of such a complex method over simpler baseline. These two (lack of clarity, low performance given the complexity of method) are for me the reasons not to consider the paper for this conference. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '1: The contributions are neither significant nor novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '5: marginally below the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper considers the problem of prediction with missing (incomplete) features. The authors propose a class of generative models that includes missingness of features, and develop a discriminative learning algorithm that maximises the conditional (posterior) log-likelihood of the training data approximately. Experiments show that the method is competitive compared with existing approaches, and in particular with approaches based on VAEs. ', 'main_review': 'The ab-initio generative model class proposed by the authors for handling predictions with missing features is convincing. It has the advantage that the involved distributions are simple (factorising), however at the price of introducing latent variables. To learn it discriminatively requires to maximise a difference of concave functions. The first term is lower bounded by ELBO as in VAEs. The second term requires a tractable upper bound. The authors develop a  novel upper bound (starting from alpha-Rényi divergence) that admits a stochastic gradient estimator.  They further introduce a data dependent surrogate reparametrisation in order to achieve an estimator with low variance. The technical part of the paper is concisely written and correct. \n\nThe authors prove that the transformation used for the reparametrisation preserves the effective parameter subset, i.e. the subset of parameter combinations for which the overall bound is tight. This is indeed a desirable property, but is in my view not sufficient. The reason is, that this effective subset can be very small and cover a subset of simple models only. Moreover, there is no guarantee that the respective gap will become small during learning.\n\nThe experimental section first analyses the learning properties of the method in an ablation study. The authors then show competitiveness of their method by comparing it with existing approaches on a subset of tasks taken from the  UCI Machine Learning Repository. The description of the experiments is clear and reproducible. The experiments are however not fully convincing w.r.t. the scalability of the approach. All networks used for the model and bound construction have only one fully connected hidden layer. This seems to be sufficient for the considered tasks from the  UCI repository. However, this would be not sufficient e.g. for image classification tasks where the involved networks are usually deep CNNs.\n\nFurther comments:\n- You mention earlier works  (Ghahramani & Jordan, 1994; Smola et al., 2005), noting that their applicability is restricted to exponential families. Please explain whether these approaches are / are not applicable for the model class analysed in your work. As I understand it, the models p(y,x,z|m) considered by you are exponential families, but of course after marginalising over z, the resulting mixture model p(y,x|m) is not any more. It remains however unclear to me, whether a DCA (difference of convex functions algorithm) for learning p(y|x,z,m) can be somehow generalised for learning p(y|x,m).\n\n- I would suggest to drop the data instance superscript earlier in the text, e.g. starting from subsection 2.2. the latest. This would in my view improve readability and reduce clutter.', 'summary_of_the_review': 'The conceptual part, i.e. the model and the proposed learning approach are in my view concise and sufficiently novel. This outweighs the missing scalability analysis in the experimental part. I would however expect the authors to clearly address the raised conceptual questions.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors propose a new method, DIG, for discriminative tasks with missing input features. It uses latent variable models to marginalize out the label and the missing part of the features given the latent variable, in order to compute the objective, the conditional log likelihood of the label given the corrupted features. As this objective is intractable, the paper builds a conditional evidence lower bound (CELBO) that can be unbiasedly approximated using Monte Carlo samples. CELBO consists of the regular ELBO as the lower bound for the log joint probability of the label and the observed features, and an evidence upper bound (EUBO) that bounds the log marginal probability of the observed features. The derivation of EUBO involves the alpha-renyi divergence and the exponential divergence. The stochastic CELBO contains a density ratio that can lead to large variance in the stochastic gradients during optimization, so the authors propose a surrogate parameterization to bound the gradient norm. Experiments on real datasets justify the effectiveness of the variational approximations to stabilize the optimization. When compared with VAE, CVAE, and MICE, the DIG algorithm shows better or comparable predictive performance and robustness against feature corruption.', 'main_review': ""**Strengths**\nThe paper is overall clearly written. The issue it tries to solve, discriminative tasks with missing input features, has great impact for a wide range of practical machine learning problems in real life. Technically, the paper has quite some novelty including the creation of a rigorous lower bound to the true objective using recent advances in the variational inference area, and designed an effective surrogate parameterization to stabilize the optimization.\n\n**Weaknesses and Questions**\n1. Sec. 3.1: More detailed explanation of the exponential divergence would be beneficial. Is there a reference for it? What role does $f(\\boldsymbol{u}; \\xi)$ play? If it can be any real-valued function, why was it chosen to be a Gaussian pdf, as shown in the appendix?\n2. Sec. 3.2: Which standard automatic differentiation library was used? The submission mentioned both the reparameterization trick and the REINFORCE trick - which one was actually used in the experiments?\n3. Sec. 3.3: I don't quite understand how this part works. All I can see that in Eq(17) the problematic ratio term is multiplied by $G$, which is always smaller than 1 and non-increasing based on Figure 3. But why $G$ was defined in that math format? What does $\\vee$ mean? Why does $G$ represent the ratio before and after the transform (equation between proposition 2 and 3)?\n4. Sec. 4.1: Missing value processes (MCAR and MNAR). What do they mean? Are they different ways to decide what values are missing in the feature, and thus leading to different versions of a dataset? If so, shouldn't we also add CVAE*, Simple* and MICE*? Could you give more explanation for the last sentence of Section 4 (saying DVAE* is more robust than DVAE)?\n5. Size of the test datasets. Based on Table 3, the datasets are all quite small, ranging from 353 to 10k data points. And we further split these points into training and test, which makes the training sets even smaller. In the appendix it's said the minibatch size is 521 -- what if the entire training set is smaller than 512? How long did the algorithm take to run on YearPred? Is the algorithm able to be easily extended to larger datasets?\n6. How does DIG works compared with other more recent imputation baselines such as MIWAE (Mattei & Frellsen, 2019) and GAIN (Yoon et al., 2018)?"", 'summary_of_the_review': 'Given the strengths of the paper listed above, I would recommend acceptance for this paper, if the authors can figure out a clear feedback for the questions I summarized when reading the paper.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper proposes a new method for learning with missing data. Compared with previous approaches, the authors choose to perform discriminative learning with generative modeling so as to borrow the benefits from these two types of methods. To optimize with the underlying intractable loss function, the authors start from the traditional variational lower bound ELBO and one upper bound CUBO from a previous work (the $\\chi$-divergence lower bound [1]) and derives a lower bound for the original loss function. To solve the issue with the estimation bias as well as the potential huge variance, the authors change the divergence function in CUBO as well as add the surrogate parameterization so that the Monte Carlo estimation of the loss can be unbiased and (potentially) with smaller variances. Experiment results show the proposed methods run stably and perform comparably or better compared to baseline methods.\n\nReferences:\n\n[1] Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational inference via χ upper bound minimization. In Advances in Neural Information Processing Systems, pp. 2732–2741. 2017.', 'main_review': 'Strengths:\n\n1. The idea of learning missing data using discriminative learning together with generative modeling is interesting. As mentioned in the paper, performing such kind of learning will resulting a loss function as a subtraction on two integrals with respect to the latent variables, which makes it harder to derive a lower bound compared to the traditional variational inference cases. To solve the term being subtracted, the authors found a upper bound that could be estimated in an unbiased way with Monte Carlo methods.\n\n2. The exponential function in the first version of CELBO will potentially has a bigger variance when estimated using Monte Carlo. To solve this issue, the authors propose adding a regularization to the loss function while remain the optimal solution unchanged under the zero gap case. This also helps a lot in making training more stable, as shown in the experiments.\n\nWeakness:\n\n1. The topic of this paper is to focus on missing data. However, this paper does not put enough efforts on learning various cases of data missing patterns. As in the experiments, the authors only test the case where the data are missing completely at random (MCAR), which may not be the most common case in reality. MNAR case might be a more interesting situation to study with. The proposed method is mostly focusing on solving the variational upper bound, while overlooks modeling the missing patterns. Suggest the authors could add some experiments with MNAR data. Also it would be better if the authors could add some modeling part on the missing pattern into the loss. For example, let the mask $m$ to depends on $(x, y, z)$. This will make the proposed model more useful in practice.\n\n2. From Proposition 2, we know that the surrogate parameterization can make the optimal solution of CELBO remains under the zero gap case. However, it is nearly impossible to reach the zero gap case in reality since it is unlikely to select variational distributions (i.e. $q(\\cdot)$\'s) to perfectly estimate the model posterior distributions. What about the ""sub-optimal"" cases? Is the CELBO-SP optimal solution close to the CELBO optimal solution in a small gap (but not zero gap) case? I understand this might not be easy, but it will be better if the authors could add some theoretical analysis on this.\n\n3. The performance metrics in Table 1 is not showing the proposed methods can outperform the baselines with big gaps, meaning that the proposed methods is not much better compared to previous approaches empirically. However, I think there are many ways that the authors could try to improve the performances. For example, the authors could try a different divergence function, a better way to add the regularization, etc.', 'summary_of_the_review': 'The author propose an interesting discriminative learning approach with generative modeling to solve the missing data modeling problem, by extending the traditional variational lower bound (ELBO), with a novel and stable upper bound that can be estimated without bias with Monte Carlo estimation. It is better if the author can study more on the missing data pattern (both empirically and theoretically) and the optimal solution preservation (under sub-optimal cases). Also, the empirical performance still has some space to improve.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion', 'authorids': ['~Kohei_Miyaguchi1', '~Takayuki_Katsuki2', '~Akira_Koseki1', '~Toshiya_Iwamori1'], 'authors': ['Kohei Miyaguchi', 'Takayuki Katsuki', 'Akira Koseki', 'Toshiya Iwamori'], 'keywords': ['Black-box variational inference', 'missing values', 'evidence upper bound'], 'abstract': 'We are concerned with the problem of distributional prediction with incomplete features: The goal is to estimate the distribution of target variables given feature vectors with some of the elements missing. A typical approach to this problem is to perform missing-value imputation and regression, simultaneously or sequentially, which we call the generative approach. Another approach is to perform regression after appropriately encoding missing values into the feature, which we call the discriminative approach. In comparison, the generative approach is more robust to the feature corruption while the discriminative approach is more favorable to maximize the performance of prediction. \nIn this study, we propose a hybrid method to take the best of both worlds. Our method utilizes the black-box variational inference framework so that it can be applied to a wide variety of modern machine learning models, including the variational autoencoders. We also confirmed the effectiveness of the proposed method empirically.\n', 'one-sentence_summary': 'A new variational approximation and algorithm are proposed for discriminative inference with missing features.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'miyaguchi|variational_inference_for_discriminative_learning_with_generative_modeling_of_feature_incompletion', 'pdf': '/pdf/537474668e8264be0d7e7963ad009564621ad25e.pdf', '_bibtex': '@inproceedings{\nmiyaguchi2022variational,\ntitle={Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion},\nauthor={Kohei Miyaguchi and Takayuki Katsuki and Akira Koseki and Toshiya Iwamori},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=qnQN4yr6FJz}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Yonathan Efroni', 'Dipendra Kumar Misra', 'Akshay Krishnamurthy', 'Alekh Agarwal', 'John Langford']",ICLR,Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics,https://iclr.cc/virtual/2022/oral/6734,2022," Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically.","Oral 1: Learning in the wild,  Reinforcement learning",https://openreview.net/pdf?id=RQLLzMCefQu,https://openreview.net/forum?id=RQLLzMCefQu,RQLLzMCefQu,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The paper presents a new technique that infers the endogenous states of an RL problem, as well as the corresponding model and optimal policy.  A bound is derived that shows that the amount of data needed depends only on the number of endogenous states, while being independent of the number of exogenous states and the complexity of the observation space.  This is remarkable since this is the first technique that is shown to have a complexity that depends only on the number of endogenous states.  Furthermore, the bound derived is not just a theoretical bound.  It is a practical bound in the sense that it is used in the associated algorithm, which is demonstrated effectively on two problems.  Perhaps the main weakness of the paper is that no intuition is provided in the main paper to explain why the sample complexity can be made independent of the number of exogenous states and the complexity of the observation space.  The reader has to look at the proof in the supplementary material.  Nevertheless, this is remarkable work.'}, {'title': 'Thanks for the rapid response!', 'comment': 'Thanks for the change! Score updated 6-->8'}, {'title': 'Thanks for your feedback', 'comment': ""Thank you for the response. \n\nWe have revised the description of VIC to incorporate your comment (changes concerning your comment are in red color in Section 1 and Section 7). We have moved the discussion of these algorithms early into the introduction (see text in red color in Section 1). We understand that when an observation-dependent policy is used and jointly learned, our counter-example doesn't hold. We do think that analysis of VIC in the presence of exogenous noise is an interesting future work direction. We have also added a line explicitly in the conclusion section (see text in red color in Section 7) to consider VIC as a future work direction. Please let us know if you have any further comments, or suggestions for improvement.""}, {'title': 'Remaining VIC concerns', 'comment': 'Thanks for the response!\n\nI\'m generally happy with the explanations and changes to the paper.\n\nThat said, I very much dispute your characterisation of variational intrinsic control (VIC). In VIC algo 1, the policy over actions is learned jointly with the rest of the objective, so your statement that the uniform random policy is ""the only reasonable choice"" doesn\'t make sense. Indeed, this policy will learn to ignore the exogenous states as they don\'t allow transmitting any information about which option is currently being executed. While the option space could certainly be made to have excessive entropy without harming the total mutual information, I would claim that the minimum entropy option distribution (for a set level of mutual information) would only contain the reachable endogenous states (and this can be approximated by known methods e.g. VALOR).\n\nThat said, I wasn\'t really expecting you to refute this line of work in the timeframe of a rebuttal. I merely wanted it acknowledged as an alternative to be explored in future work. If the main text is changed to reflect this, then I\'ll raise my score to an 8 in light of your other substantial improvements.'}, {'title': 'Thanks', 'comment': 'Thank you for the response. The updates have addressed the majority of my concerns about the paper and I updated my review.'}, {'title': 'Thanks!', 'comment': 'We have updated the paper to add results on bisimulation. We have also added the discussion of ""Variational Intrinsic Control"" Gregor et al., in related work. See text in blue in Section 3 and Section 6. As expected, bisimulation is unable to solve combination lock which has a sparse-reward structure. We have also added a conclusion section that discusses future work on the more general setting. If you have any further comments or questions, then we are happy to answer them by the Monday deadline for author discussion. However, if our comments resolve your doubts, then we will appreciate it if you can update your review.'}, {'title': 'Thanks!', 'comment': 'We have updated the paper to add discussion of Paster, et al. We also now state how the state decoder is learned and how planning is done in greater detail. See text in blue in Section 3 and 4 and also in Appendix D.2.. Pseudocode with all details, as was run in our experiments, is stated in Appendix F.4. If you have any further comments or questions, then we are happy to answer them by the Monday deadline for author discussion. However, if our comments resolve your doubts, then we will appreciate it if you can update your review.'}, {'title': 'Thanks!', 'comment': 'We have updated the paper to add baseline results on the visual grid world. If you have any further comments or questions, then we are happy to answer them by the Monday deadline for author discussion. If our comments resolve your doubts, then we will appreciate it if you can update your review.\n\n'}, {'title': 'Thanks for your response', 'comment': 'Thank you for your response and suggestion!\n\nWe have updated the paper and added a discussion for point W2 at the end of page 3, as you suggested. We emphasize that our goal (which PPE achieves) is to learn a policy cover on the endogenous state space with cardinality which is independent of both the observation space and exogenous space. If you have any further comments or questions, then we are happy to answer them by the Monday deadline for author discussion. If our comments resolve your doubts, then we will appreciate it if you can update your review.'}, {'title': 'Thanks', 'comment': 'Thanks to the authors for responding to my comments. I believe (W1) and (W3) have been adequately addressed. For (W2), I greatly appreciate the clarification in the discussion here (i.e., the point about seeking a cover for the endogenous state space only)--it seems to me that the authors ought to make this more clear in the paper as well. In particular, it feels like something to this effect could easily be said somewhere at the bottom of p3.'}, {'title': 'Comment of review of fNfe ', 'comment': 'We thank the reviewer for the detailed comments. We address the comments below.\n\n**1. The assumption on near deterministic endogenous dynamics.** We acknowledge the limitation of this assumption. However, we believe that the PPE algorithm is an important first step towards a general solution of the EX-BMDP model (and, more generally, to RL with exogenous noise).  The importance of the result stems from the fact PPE is the first provably efficient algorithm for such a setting. Furthermore, we believe that generalizing it to the fully stochastic setting is an important next step to take, and is an open problem posed by our work.\n\n$\\quad$ We also note that contrastive learning and inverse dynamics approaches fail even in deterministic settings. Further, we believe that the failure of these approaches is quite common. E.g., the contrastive learning approaches get confused with the presence of any exogenous noise, whereas the aliasing of states for inverse dynamics as explained later in this response is quite common. Therefore, we believe that issues with the alternative approaches are quite serious and the fact that PPE can surmount them even in a deterministic setting provides an important lesson. \n\n$\\quad$ Furthermore, we do want to stress the simplicity of the PPE algorithm. PPE is not only provably efficient but very simple to implement and run. It only requires access to a multi-class classification oracle, which is routinely used in practice. PPE is also adaptive to the number of latent states (it does not require an upper bound on the number of latent states, but calculates this quantity adaptively), and, lastly, PPE is very fast in practice. This makes PPE a strong algorithm to consider in cases where the near deterministic assumptions hold.\n\n\n**2. Reference to Variational Intrinsic Control.** We thank the reviewer for pointing these references. Indeed, _“Variational Intrinsic Control”_ is a form of multi-step inverse dynamics. We will cite and discuss it in the revision. However, the algorithm in that paper may fail in the presence of an exogenous distractor. One way to see a failure case is when the set $\\Omega$ contains only exogenous states. That is, in the initialization step, the option-set may include only the exogenous states at time step t=2. If the sampling policy is uniform $\\pi = U(A)$ (without prior knowledge that is the only reasonable choice), then it can be shown that: $I(\\Omega, s_f | s_0) =  I( \\Omega, \\phi_{exo}^*(s_f) | \\phi_{exo}^*(s_0))$ where $\\phi_{exo}^*(s)$ maps a state to its exogenous state. This can be proved using the fact that $P_{a \\sim U(A)}(s) = q(s| \\phi_{endo}^*(s), \\phi_{exo}^*(s))P_{a\\sim U(A)}( \\phi_{endo}^*(s)) P_{a\\sim U(A)}( \\phi_{exo}^*(s)),$ and the definition of mutual information.\n\n$\\quad$ Thus, a fixed-point solution of algorithm 1 of _""Variational Intrinsic Control""_ is to set $\\Omega$ as the set of exogenous states. This counterexample holds in the exact case, i.e., when we have access to exact computations. Nevertheless, it highlights the problematic nature of algorithm 1 of _""Variational Intrinsic Control""_ applied to the EX-BMDP model.\n\n**3. Comparison with Bisimulation.** Previous work has shown that bisimulation can fail even in the presence of reward. See [Modi et al. 2019](https://arxiv.org/pdf/1910.10597.pdf), Proposition B.1 and also see discussion of it in Section 3 in our paper. In particular, this happens in sparse-reward settings such as the combination lock experiments. Intuitively, the agent will not receive a reward for a long time in these settings. Hence, a degenerate solution that maps all observations to the same abstract state will receive optimal bisimulation loss. However, this degenerate solution results in poor exploration because of which the agent will not receive the reward. We will revise the paper to include empirical results on bisimulation on the combination lock.\n\nReference: Sample Complexity of Reinforcement Learning using Linearly Combined Model Ensembles, Modi et al., 2019\n\n**4. State aliasing in inverse dynamics.** You are correct that inverse dynamics can merge state $s_{h, a}$ and $s_{h, b}$ for different values of h in the combination lock experiment. However, the reason for failure is more serious than accidental cases where a_{h-1} = a\'_{h-1}. More generally, anytime two states have a disjoint set of _parent states_, then inverse dynamics can merge them. This happens since we can rely on the parent state to predict the action. As the parent states are disjoint, therefore, the parent information also uniquely identifies which of the two states is being addressed. E.g., even if we merge s_3a = s_3b to an abstract state $\\bar{s}$, we can still predict $p(. $| s_2a, $\\bar{s})$ and $p(. $| s\\_2b, $\\bar{s})$ correctly. This makes the failure of inverse dynamics quite common. We are happy to explain this in more detail if desired.\n\n'}, {'title': 'Comment of review of yfQT ', 'comment': ""We thank the reviewer for the detailed comments. We address them below.\n\n**1. How is the policy learning done in the experiment section?** PPE returns a policy cover over the endogenous state space. PPE also learns an endogenous state decoder and estimates the latent endogenous transition dynamics. This gives us two existing dynamic programming-based approaches to do policy planning. When we have a general reward function, we can use PSDP to optimize the reward function (see Appendix D.1). PSDP can be run off the shelf using the policy cover returned by PPE. However, as PSDP is computationally expensive, we also discuss a value iteration based approach that can be employed when the reward only depends on the endogenous state (see Appendix D.2). For experiments in this paper (Figure 2 and 3), we use the value iteration algorithm. Due to the space limit, and since we felt that an application of value-iteration given a policy cover is quite standard, we chose to defer this part to the appendix. We will revise the paper to make this point clearer. We will also release the source code and along with detailed single pseudocode in the Appendix to provide the most clarity.\n\n**2. How are representations found in the decoding accuracy plot?** We discuss this in the paragraph called _“Recovering latent transition dynamics.”_ We extract a decoder for the endogenous state $\\hat{\\phi}_h$ for each time step $h$ from the learned classifier $\\hat{f}_h$. This is the decoder we evaluate in Figure 2c. \n\n**3. How does this paper relate to prior works on inverse dynamics (Paster, Keiran, et al.)?** Thanks for the reference to Paster, Keiran, et al. We will cite and discuss this paper in the revision. We want to emphasize important differences between our work and theirs:\n\n- The algorithm of Paster, Keiran, et al., can recover goal states that depend on the exogenous state since they don't use any filtering mechanism to remove exogenous noise. Of course, they considered a different problem than the EX-BMDP setting, and therefore this issue was not problematic in their setting. When the goal states depend on such exogenous noise, the size of the latent state may significantly grow and scale with the cardinality of the exogenous state space instead of just the endogenous state space as for PPE.\n\n- The planning oracle used in their paper (see section 3.2 in their paper) is not computationally efficient, whereas ours is (the path elimination process we offer is computationally efficient and is polynomial in the number of latent states, actions and horizon).\n\n**4. Conclusion section.** We have added a conclusion in the revision.""}, {'title': 'Comment on review of B1Cs ', 'comment': 'We thank the reviewer for the thorough comment. We will address the comments point-by-point.\n\n1. **Inverse mapping $\\phi^\\star_e$:** We both use and learn this mapping. For example, we use it when expressing the Bayes optimal classifier in the main text (see the paragraph on *Why does PPE work?*), and we use it many times in the Appendix. One key feature of PPE is that it can learn $\\phi^\\star_e$. This is done implicitly in the Algorithm 1 pseudocode due to brevity. Basically, we can recover a learned decoder $\\hat{\\phi}_e$ from $\\hat{f}$. We describe how to do this in the paragraph _""Recovering latent transition dynamics""_ (Section 4)  after the Algorithm 1 pseudocode. In this paragraph, the notation $\\hat{\\phi}_h$ represents the learned endogenous state decoder for time step $h$. We will change the notation to $\\hat{\\phi}^e_h$ to make this connection clearer. We also evaluate this decoder in Figure 2c. PPE is implicitly relying on this decoding since it is relying on $\\hat{f}_h$ to make planning decisions. Learning this decoder has other uses such as debugging or visualizing the latent state.\n\n\n2. **Tractability of PPE (important clarification).** We would like to clarify an important property and goal of our research. PPE is designed to achieve covering in the endogenous state space and so the sample and computational complexity are _bounded in terms of the size of endogenous state space_ which is finite, rather than the size of observation space or even the size of exogenous state space. Hence, PPE is ideally suited for very challenging high dimensional observation problems such as the visual grid world in Figure 3. In this case, the observation space is a 56x56x3 RGB image and each channel takes 255 values, hence the size of the observation space is bounded by $255^{56 \\times 56 \\times 3} \\approx 10^{22000}$. In contrast, the size of endogenous state space is bounded by 25 x 4 = 100 (25 due to the position of agent in one of the 5x5 grid squares and 4 for each of the four directions). PPE’s sample and computational complexity are dependent on the size of endogenous state space (100) rather than the size of observation space ($10^{22000}$) or even the exogenous space. Further, note that PPE reduces RL to solving a sequence of $H$ classification tasks which are routinely performed in practice.\n\n\n3. **Planning details**. We thank you for this comment. Once we have policy cover ($\\Psi_h$) we can directly use existing methods such as PSDP to learn an optimal policy for any given reward function. We discuss how to do this in Appendix D.1 and provide pseudocode of PSDP. PSDP only relies on policy cover which is returned by PPE. In the special case, when the reward function is only dependent on the endogenous state, we can use a more efficient value iteration algorithm (see Appendix D.2). In light of this comment, we added a paragraph in section 4 (see _“learning a near optimal policy given a policy cover”_) and improved the writing in Appendix D.2. The value iteration algorithm is described in Algorithm 4. It tries to estimate the reward function $r(s, a)$ and, then returns the optimal policy with respect to this reward function and the recovered endogenous transition dynamics $T_D$. We will also release the source code and provide a detailed single pseudocode in the Appendix to provide more clarity. Please don’t hesitate to ask us for further clarification. \n'}, {'title': 'Comment on review of Zamv ', 'comment': 'We thank the reviewer for the detailed and kind comment! \n\n-) We would like to emphasize that RND fails on the combination lock problem and underperforms PPE which not only can solve problem with longer horizon (H=40 for PPE vs H=10 for PPO+RND), but also that PPE uses fewer samples for H=10 than RND. Note that in Figure b, the number of episodes are on the y-axis and horizon is on the x-axis and the flatter the plot, the better it is. Regarding theoretical failures, it is difficult to provide a concise theoretical counterexample to RND. Our experiments though show that it is unlikely that PPO+RND enjoys the PAC-RL guarantees that we derive for PPE.\n\n-) While visual gridworld are more challenging in terms of representation, they are not as challenging as combination lock in terms of exploration. Therefore, our focus on this task is to recover the latent dynamics for which this task presents a non-trivial challenge due to its representation complexity. PPO and PPO+RND do not output latent model as a side-product, nor do they learn a state decoder. Hence, they are not valid baselines. We will revise the paper to add Homer and ID baselines on this task.\n'}, {'title': 'Comment for all reviewers', 'comment': 'Based on reviewer feedback, we have updated the paper to (new text is in blue colour for ease of reading):\n\n1. Include extra experimental details. In particular, we report baseline results in the visual-grid world for state decoding (Figure 3d is new) as well as reporting a bisimulation-based baseline in Figure 2b.\n\n2. Adding more details on how planning for a given reward is done, and how a transition model is extracted. These are in Section 4. \n\n3. Related work discussion of prior work on multi-step inverse dynamics (Paster et al., and Gregory et al.). As discussed, these works cannot provably solve exogenous noise, unlike our algorithm PPE. \n\n4. Adding a conclusion section that also discusses future work. Section 7.'}, {'summary_of_the_paper': 'This paper proposes a new class of decision problems, the exogenous block-MDP (EX-BMDP), and an algorithm for acquiring minimal state representations for EX-BMDPs. EX-BMDPs are an extension of block MDPs that allows for additional ""exogenous"" state components that may have arbitrary Markovian dynamics, but cannot be influenced by actions, including through the actions\' effects on ""endogenous"" state. The proposed algorithm, PPE, is able to recover a mapping from observed states to endogenous states (i.e. ignoring the latent distractor variables) in the setting where the endogenous state dynamics and initial state distribution are near-deterministic. It does this by comparing the set of policies that can reach each pair of states: informally, if $p(\\pi \\mid s_1) = p(\\pi \\mid s_2)$ for all policies $\\pi$ (assuming a uniform $p(\\pi)$), then $s_1$ and $s_2$ are treated as equivalent. In the deterministic case, policies reduce to action sequences (""paths""). Illustrative experiments on simple domains show that the proposed approach is able to quickly recover a state representation that depends mostly on the endogeneous component.', 'main_review': 'Strengths:\n\n- EX-BMDPs seem to capture several popular simulated distractors that are currently being used in applied RL papers, such as random video backgrounds in MuJoCo tasks. I suspect the fact that the exogenous dynamics are relatively unconstrained will also make them a good model for practical tasks. The requirement for near-determinism in the endogenous dynamics is unfortunate, particularly given that it applies to the initial state distribution as well (often you want to perform well over a distribution of similar-but-not-identical tasks). However, I think this is not a major concern for exploration methods: so long as the possible initial states are reachable from some common starting state within $H$ steps, PPE will eventually explore them.\n- The fact that it is possible to obtain a sample complexity bound completely independent of the complexity of $\\Xi$ was surprising to me given the relatively complex form of exogenous noise in an EX-BMDP, and shows that the EX-BMDP class is at least tractable.\n- The clarity of writing was appreciated, particularly when stating assumptions and comparing against past algorithms.\n\nWeaknesses:\n\n- I noticed that RND is missing from Section 3/Appendix A (shortcomings of existing methods), but does surprisingly well on the combination lock problem. Is it possible to give some intuition for where RND fails in general?\n- The visual gridworld experiments are a little confusing. The takeaway seems to be that PPE drives accuracy to \\~100% eventually on a less-toy problem. However, there are no baselines, so it is hard to determine how significant this achievement is. It would be useful to see how the baselines from Figure 2 fare in this setting, particularly in terms of sample efficiency.', 'summary_of_the_review': ""This paper proposes a general but tractable class of MDPs with exogenous distractors, and proposes a novel algorithm which provably obtains representations that exclude the spurious latent dimensions of the state. Given the technical significance of the work, I am in favour of acceptance.\n\n-----\n\n**Update:** the author response resolves the issue with visual gridworld experiments. I'm still in favour of acceptance."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The authors consider the problem of reinforcement learning using high-dimensional observations (eg, images) that may contain both exogenous and endogenous state information. Seeking to remedy the issues with learning that arise due to the exogenous state information, the authors propose a new model called an Exogenous Block MDP (EX-BMDP) and a new algorithm called Predictive Path Elimination (PPE) to learn a generalization of the inverse dynamics of the EX-BMDP. Additionally, the authors present some experimental evidence that PPE performs well in the EX-BMDP setting compared to alternatives.', 'main_review': 'STRENGTHS\n\n(S1) The problem of efficiently learning in high-dimensional observation spaces is a good one, and the authors provided an excellent discussion of the technical components of this problem in the early parts of the paper. I thought the definition and presentation of the EX-BMDP was particularly good.\n\n(S2) I appreciate the authors providing experimental results to show that PPE performs well in some regards.\n\nWEAKNESSES\n\n(W1) I\'m afraid I found the discussion of PPE to be a bit difficult to decode. Even looking at Algorithm 1, I find myself with a number of important questions unanswered, chief among them being ""what about $\\phi^*_e$""? It strikes me as odd that nowhere in this algorithm are the inverse mappings from observation to exogenous/endogenous state represented or used. Why is this?\n\n(W2) I\'m a little concerned with the computational tractability of the algorithm. My understanding from the paper is that one seeks some sort of ""covering"" of the state space--in what sense is it practical to obtain and/or store this covering? If the problem setting consists of high-dimensional observations this seems especially challenging.\n\n(W3) From my naive perspective, it seems that the ultimate goal here is to use PPE and then actually perform reinforcement learning. However, I didn\'t understand from the brief discussion at the end of Section 4 (or, frankly, from reading the Appendix) how exactly that would be accomplished. I feel this is important enough that it should appear in the main paper.\n\nPOST-DISCUSSION COMMENTS\n\nDuring the discussion, I feel as though the authors adequately addressed each of the issues I raised above, and so I\'m happy to raise my score to accept.', 'summary_of_the_review': ""While I appreciate the importance of the problem setting and think I understand the general thrust of the paper here, I overall left the paper with a lot of confusion. I'm hoping the authors can provide clarity here--both in their response and by revising the manuscript--during the discussion phase. Indeed, after some discussion and edits to the paper, I feel that the paper is much more clear and recommend acceptance."", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'This paper introduces a model called the Exogenous Block MDP (EX-BMDP) where the latent state contains both controllable (endogenous) and uncontrollable (exogenous) elements. The paper proposes an algorithm to find a policy cover with sample complexity that depends only on the size of the endogenous state rather than the observation or exogenous state. The algorithm works by training a classifier to predict the actions that were taken to get to a state. Since the exogenous state is not affected by actions, states for which the classifier returns similar results are likely the same endogenous state. The algorithm builds up a set of policies (sequences of actions are sufficient in the near-deterministic setting) which visit unique endogenous states by using the classifier to deduplicate redundant action sequences. Besides proving the sample efficiency of their approach, experiments are provided which show that this algorithm performs better than baseline approaches in terms of performance as well as in ability to decode the state from its representation in a simple combination lock environment as well as a grid world with distractors.', 'main_review': 'Strengths:\n\n- The problem setting is important and seems to be relevant to many real-world problems.\n- While the idea of using inverse dynamics and even multi-step inverse dynamics [1] to learn representations in these types of environments to learn controllable representations is not novel, this paper provides a theoretical basis for this choice, which I think is valuable.\n- I found sections 1 through 5 to be very well written.\n\nWeaknesses:\n\n- In the experiment section, it seems as though you are testing the planning/policy learning abilities with PPE in both of the environments. However, as far as I can tell, there is no indication of how you perform this planning in PPE in the main text. The paper simply switches from talking about learning policy covers to showing regret for environments with only a small mention of the fact that you could plan using the data collected with PPE. The way the experiments are run and the choice to test the planning performance / how the state representations are obtained in experiments should be explained in more detail in my opinion.\n- The paper discusses prior works with inverse dynamics seemingly only in the context of their applications in representation learning in two prior works on exploration. I think the application of multi-step inverse dynamics in goal-conditioned environments described in [1] is worth mentioning, since for a fixed goal this algorithm seems like it is doing something similar to PPL. Related is the lack of a ""related works"" section in the paper, which would be helpful for framing PPL in the context of prior works.\n- There is no conclusion section, which I feel would improve the paper.\n\nMy major questions are:\n\n1. How is the policy learning done in the experiment section? How are representations found in the decoding accuracy plot?\n2. How does this paper relate to prior works on inverse dynamics?\n\n[1] Paster, Keiran, et al. Planning from Pixels Using Inverse Dynamics Models. 2020.\xa0openreview.net,\xa0https://openreview.net/forum?id=V6BjBgku7Ro.', 'summary_of_the_review': 'In my opinion, the contribution of the paper is strong since it analyzes an important problem setting, presents an analysis of representations learned using inverse dynamics, and shows that it works in practice. The weaknesses, specifically little discussion of prior work and problems with the experiment section seem fixable, and under the condition that these points improve, I will recommend acceptance.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The authors propose a novel algorithm PPE, which they prove efficiently eliminates exogenous noise under certain assumptions (e.g. near deterministic dynamics). PPE works by growing a set of open-loop policies (action sequences) sufficient to reach all possible states for an increasing horizon. This is done by predicting the index of the policy from its final state, and eliminating states that are not sufficiently predictable. They show that both in theory and in practice that popular alternatives (noise contrastive and inverse-dynamics approaches) either fail to ignore exogenous noise or fail to distinguish between actually different states.', 'main_review': 'The paper is very well motivated. It has convinced me that being invariant to exogenous noise is important and that the EX-BMDP is useful abstraction for making this problem concrete. It is also very clearly presented. The argumentation is clear, and the flow of the paper is quite natural. Even the lengthy appendix is fairly easy to parse, with straightforward proofs and relevant details (e.g. specific claims about the unsuitability of alternative approaches).\n\nOne shortcoming is potential impact / applicability. I\'m not convinced that environments with single starting states and near-deterministic dynamics is a very rich problem class. Or more specifically, I\'m not convinced that restricting myself to this class would be preferable to working in the full space of MDPs with e.g. the possibility of aliasing a few states with a single-step inverse-dynamics approach. I am aware that impact is very hard to predict in advance, so I won\'t let this aspect unduly affect my scoring. And I\'m convinced that even if PPE is never used in practice, this paper should still be accepted for its useful problem formulation and theoretical results. But a quick read would leave one with the impression that the alternatives (e.g. contrastive learning, inverse dynamics, bisimulation metrics) flaws outweigh their considerable advantages (e.g. applicability with stochastic dynamics) --  explicitly noting where alternative approaches are preferable would be appreciated.\n\nI also noticed what I believe to be an omission in your related works: mutual information / empowerment based methods. ""Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning"" seems particularly related in its search for predicable action sequences. And the follow-up work ""Variational Intrinsic Control"" appears to be a multi-step inverse dynamics analog that doesn\'t require open-loop policies. If you can show that these approaches are flawed in a way that PPE is not, then that would considerably raise my assessment of this work. And even if not, I believe this field (if not these specific works) should be acknowledged, as they similarly attempt to learn what in the environment is controllable.\n\nWhile I agree that PPE is applicable in no-reward situations whereas bi-simulation metrics are not, your experiments all involve rewards, so I\'m surprised a bi-simulation metric method was not included as a baseline.\n\nWhile this isn\'t necessary for acceptance, it is worth noting that prior work has established much more challenging benchmarks for evaluating the representation of exogenous noise, and utilizing a pre-existing benchmark would make your empirical results considerably more impressive.\n\nThis is a small point, but it is not initially obvious why inverse dynamics fails on the combination lock problem. Does alliasing occur just when e.g. s_2a --> s_3a and s_2b-->s_3b have the same action?', 'summary_of_the_review': ""A well-argued paper that sets up an important problem and introduces a novel algorithm (PPE) to solve it. This is somewhat undercut by PPE only working in a restrictive setting. Empirical results would benefit from an additional baseline (bisimulation) and (optionally) a previously published benchmark. Some discussion of mutual information approaches should be made. But some reasonable attempt is made towards these improvements, I'd happily see this work accepted."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'title': 'Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics', 'authorids': ['~Yonathan_Efroni2', '~Dipendra_Misra1', '~Akshay_Krishnamurthy1', '~Alekh_Agarwal2', '~John_Langford1'], 'authors': ['Yonathan Efroni', 'Dipendra Misra', 'Akshay Krishnamurthy', 'Alekh Agarwal', 'John Langford'], 'keywords': ['Reinforcement Learning Theory', 'Invariant Representation', 'Rich Observation Reinforcement Learning', 'Exogenous Noise', 'Inverse Dynamics'], 'abstract': 'Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically. ', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'efroni|provably_filtering_exogenous_distractors_using_multistep_inverse_dynamics', 'pdf': '/pdf/310151127bcaaee206f6987dfe48a6f9a49ae848.pdf', 'supplementary_material': '/attachment/f967ff5acd44c135ebd15ad1bb92a5d01c70c077.zip', '_bibtex': '@inproceedings{\nefroni2022provably,\ntitle={Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics},\nauthor={Yonathan Efroni and Dipendra Misra and Akshay Krishnamurthy and Alekh Agarwal and John Langford},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RQLLzMCefQu}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shiori Sagawa', 'Pang Wei Koh', 'Tony Lee', 'Irena Gao', 'Sang Michael Xie', 'Kendrick Shen', 'Ananya Kumar', 'Weihua Hu', 'Michihiro Yasunaga', 'Henrik Marklund', 'Sara Beery', 'Etienne David', 'Ian Stavness', 'Wei Guo', 'Jure Leskovec', 'Kate Saenko', 'Tatsunori Hashimoto', 'Sergey Levine', 'Chelsea Finn', 'Percy Liang']",ICLR,Extending the WILDS Benchmark for Unsupervised Adaptation,https://iclr.cc/virtual/2022/oral/6964,2022," Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.",Oral 3: Meta-learning and adaptation,https://openreview.net/pdf?id=z7p2V6KROOV,https://openreview.net/forum?id=z7p2V6KROOV,z7p2V6KROOV,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper presents U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. The authors propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain-shift problems. The vision behind sounds quite ambitious and convincing to me, namely, the proposed U-WILDS benchmark would be a useful and well-motivated resource for the ML community, and their experiments were very comprehensive. Although they did not introduce any new methods in this paper, U-WILDS significantly expands the range of modalities, applications, and shifts available for studying and benchmarking real-world unsupervised adaptation.\n\nThe clarity, vision and significance are clearly above the bar of ICLR. While the reviewers had some concerns on the novelty, the authors did a particularly good job in their rebuttal. Thus, all of us have agreed to strongly accept this paper for publication! Please include the additional rebuttal discussion in the next version.'}, {'title': 'Thank you', 'comment': 'Great, thank you for engaging with us on this and for helping to improve our paper!'}, {'title': 'Thank you', 'comment': ""Thank you! That's a good suggestion. We can't edit the rebuttal revision right now, but we will do that for the subsequent version of this paper.""}, {'title': 'Response to authors', 'comment': 'All your responses answer my questions! Thanks!'}, {'title': 'Response to Authors', 'comment': 'This was very helpful.  Thank you.  I would encourage the authors to have the pointer to Appendix A in the main body of the paper provide a little more insight into what specifically they discuss about the data sets in the appendix.  Other than that, I feel my questions were addressed.'}, {'title': 'Thank you', 'comment': ""That's great to hear. Thank you for all of the suggestions; they have been very helpful for us to improve the paper.""}, {'title': 'Thank you for the clarifications. ', 'comment': 'I thank the authors for the detailed response. All of my concerns have been addressed.'}, {'title': 'Thank you', 'comment': 'Thank you very much. We appreciate it!'}, {'title': 'Correction', 'comment': 'I apologize for having misclicked the ""correctness"" setting. I naturally meant to select the option 4.\n\nMy main concerns have all been adressed, and the paper is now clearer. I commend the authors for their work in editing the paper and responding to the reviews.'}, {'title': 'Clarifying self-supervised algorithms', 'comment': 'Thank you for the quick reply! \n> I still fail to understand how can domain adaptation and self-supervised algorithms work with similar assumptions about labeled and unlabeled data? Self-supervised algorithms like SwAV treat a source domain as unlabeled in the first phase of training, but finetune on some amount of labeled data on a downstream task. Domain adaptation treats source domain as labeled and target domain as unlabeled. Perhaps the authors could clarify this a bit more. ""In particular, none of them use any target labels"". Can you explain how is SwAV trained in this setting?\n\nWe apologize for the lack of clarity about self-supervised algorithms like SwAV. The reviewer is correct that such algorithms are typically pre-trained on unlabeled data and then finetuned on labeled data on a downstream task, which might or might not come from the same data distribution as the unlabeled data. In our case, we are similarly treating self-supervision as a means of pre-training, except that we pre-train on the target unlabeled data, and then finetune on the source labeled data (since that is the only labeled data that we have). As a concrete illustration using the DomainNet real -> sketch experiment, training a standard ERM model would involve the following:\n\n1. Initialize the model using ImageNet-pretrained weights\n2. Finetune the model on labeled data from the source (“real”) domain\n3. Evaluate on held-out data from the target (“sketch”) domain\n\nwhile using SwAV would involve adding the second step in the following (all other steps are identical):\n\n1. Initialize the model using ImageNet-pretrained weights\n2. Continue pre-training the model with SwAV on unlabeled data from the target (“sketch”) domain\n3. Finetune the model on labeled data from the source (“real”) domain\n4. Evaluate on held-out data from the target (“sketch”) domain\n\nThe DomainNet experiments that we added in the revision (Table 15) show that this procedure can improve performance on the target domain:\n\n|                 | In-dist (real) | Out-of-dist (sketch) |\n|-----------------|----------------|----------------------|\n| ERM             | 82.5 (0.3)     | 35.9 (0.3)           |\n| ...             | ...            | ...                  |\n| SwAV            | 79.0 (0.3)     | 38.2 (0.4)           |\n\nWe use the same procedure for self-supervised training using masked language modeling as well: we first initialize a pre-trained BERT model; then do continued pre-training using the unlabeled (target) data in the U-WILDS dataset; and finally finetune it on the labeled (source) data. \n\nWe apologize for not having described this procedure clearly in the text. We have uploaded a new version of our revision that includes this in Section 5 (Algorithms) and Appendix B.4 (Self-supervision methods). Could we check if that answers your questions? We appreciate these comments and questions; thank you for helping us clarify the paper.\n\n---\n\n> nevertheless, it is indeed surprising to know that semi-supervised and self-supervised models are quite good OOD generalizers. In that light, I would like to change my statements and agree that the experiment results are indeed significant. you should definitely add this discussion to the paper.\n\nThank you! In our revision, as we’ve noted above, we’ve added the DomainNet results (which help to support this point); clarified the semi-supervised and self-supervised methods and why we’re evaluating them; and emphasized this discussion in the text. We appreciate the constructive suggestions and feedback.\n'}, {'title': 'Response to the rebuttal', 'comment': 'Thanks for your detailed rebuttal.\n\nI still fail to understand how can domain adaptation and self-supervised algorithms work with similar assumptions about labeled and unlabeled data? Self-supervised algorithms like SwAV treat a source domain as unlabeled in the first phase of training, but finetune on some amount of labeled data on a downstream task. Domain adaptation treats source domain as labeled and target domain as unlabeled. Perhaps the authors could clarify this a bit more. \n\n""In particular, none of them use any target labels"". Can you explain how is SwAV trained in this setting?\n\nnevertheless, it is indeed surprising to know that semi-supervised and self-supervised models are quite good OOD generalizers. In that light, I would like to change my statements and agree that the experiment results are indeed significant.  you should definitely add this discussion to the paper.'}, {'title': 'Overall response, part 2/2', 'comment': 'Furthermore, we have added two new sets of experimental results in response to reviewer suggestions:\n\n1. In response to a question from Reviewer RpU9 concerning the effectiveness of self-training and self-supervised methods on prior benchmarks, we have added **DomainNet results** for all of the methods that we had tested on U-WILDS. DomainNet is a standard domain adaptation benchmark for object recognition, and this is a replication experiment to show that the changes we made to the methods (e.g., standardizing the set of augmentations used) did not adversely affect their performance. Consistent with prior work, all of the methods we tested indeed outperformed ERM on our DomainNet experiments. Despite their success on DomainNet, these methods do not fare as well on the shifts in U-WILDS, which shows that **the success of these methods need not transfer across benchmarks**. In our revision, we mention these results in Section 6, and describe the setup and results in more detail in Appendix E. \n\n2. On Reviewer Dxvt’s suggestion, we have added results on **fully-labeled ERM models** to four datasets (Amazon, CivilComments, FMoW, iWildCam). These models use ground truth labels on the “unlabeled” data, and can therefore provide an informal “upper bound” on how well a standard pseudo-labeling approach might do on these datasets. As we had created the unlabeled splits for these four datasets by acquiring additional data and then hiding their labels, we could train these fully-labeled ERM models by revealing the hidden labels on the “unlabeled” data and then training a model jointly on both the labeled and “unlabeled” data with ground truth labels. \n\n    On FMoW and iWildCam, where the challenge is primarily in performing well on unseen domains (that are not in the labeled training set), the fully-labeled ERM models do substantially better than the other methods which do not observe those ground truth labels. It is promising that this “upper bound” estimate is high, though we use the term “upper bound” loosely---it might also be possible to surpass this “upper bound” (even without access to ground truth labels) with methods that do more than standard ERM.\n\n    On Amazon and CivilComments, where the challenge is primarily in performing uniformly well over subpopulations of the training distribution, the fully-labeled ERM baselines show a more modest improvement. This is consistent with prior work on other datasets showing that even with a large amount of labeled data, ERM models can still obtain poor subpopulation performance compared to specialized methods such as distributionally robust optimization (DRO) techniques. However, these techniques have yet to be successfully adapted to the Amazon and CivilComments settings, and it remains an open challenge to develop similar methods that might work well on these datasets.\n\n    In our revision, we discuss these new results in Section 6 and Table 2, and describe the experimental setup in more detail in Appendix F.\n\nWe are grateful to all of our reviewers for all of these suggestions. Please do not hesitate to contact us for any further questions or clarifications. Thank you!\n'}, {'title': 'Overall response, part 1/2', 'comment': 'We thank all of our reviewers for their thoughtful reviews, and we appreciate all of their encouragement and constructive feedback. Overall, reviewers found that the U-WILDS benchmark would be a useful and well-motivated resource for the community, and that the experiments were comprehensive. While we did not introduce any new methods in this paper, U-WILDS significantly expands the range of modalities, applications, and shifts available for studying unsupervised adaptation, which leads to the central conclusion of our work--- success on prior unsupervised adaptation benchmarks need not transfer to success on other benchmarks that reflect different real-world conditions. We believe that this is an important point for the ML research community to know about.\n\nHowever, parts of the paper were unclear, which led to some questions from the reviewers. In this revision, we have edited the paper throughout to clarify and expand on the points of confusion that they raised, including:\n\n- **Section 4, Datasets.** We have emphasized that U-WILDS uses exactly the same labeled datasets as in WILDS, and provides additional unlabeled data on top of that.\n- **Section 5, Algorithms.** We have provided further context behind the use of self-training and self-supervision for domain adaptation tasks.\n- **Section 6, Experiments.** We have expanded the experimental setup in the main text to clarify model selection, the use of unlabeled data, and other details. We have also included discussions of the new experiments below. \n- **Section 7, Discussion.** We have elaborated on methods that can use domain annotations, such as multi-source domain adaptation algorithms. \n- **Ethics statement.** We have expanded the Ethics statement to verify that the datasets have the appropriate licenses and have been adequately anonymized, as well as to further discuss potential fairness concerns.\n- **Appendix A, Additional dataset details.** We have expanded on the discussion of the labeled vs. unlabeled data for the iWildCam, MolPCBA, and Amazon datasets. \n- **Appendix B, Algorithm details.** We have elaborated on our discussion of domain-invariant methods to add more citations and discuss multi-source/multi-target methods, as well as to detail how we use self-supervision.\n- **Appendix D, Experimental details.** We have clarified our model selection procedure.\n\nFor convenience, we have highlighted all of the substantive changes in the revision in green. We also made smaller line edits elsewhere in the paper in order to tighten the prose and fit the new additions within the page limit, but for clarity, we have omitted those highlights. In the individual responses below, we respond to each reviewer’s questions in more detail.\n'}, {'title': 'Response to Reviewer NvLs', 'comment': 'We thank the reviewer for their encouraging feedback and suggestions, and we are glad that the reviewer appreciated our attempt to explain each method and our implementation in a unified manner. Below, we respond to their questions in turn.\n\n---\n\n> Correctness: 1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.\n\nMay we politely ask if this rating was made in error, as the text of the review does not seem to match the rating? If so, would the reviewer mind changing the rating? If not, please let us know which claims are unsupported and we would be glad to clarify or edit them. Thank you!\n\n---\n\n> ""Models are trained on labeled data from the source domains, as well as unlabeled data from one or more of the other sources, depending on what is realistic for the application."" I couldn\'t understand which problems were allowed to use OOD unlabelled data for training and which ones were not. From my understanding, unlabelled data from all domains (source, val, target, extra) are merged and used for training; is this correct? And what would be the use of unlabelled data if not for helping the training?\n\nWe apologize for the lack of clarity. Other than the ERM models, all of the other methods used OOD unlabeled data for training. For our experiments, we picked a single type of unlabeled data (source, val, target, extra) for each dataset -- this is marked just below each dataset name in Table 2 -- and did not use the other types of unlabeled data. We did this for simplicity, but our public codebase will allow researchers to use whichever types of unlabeled data they would like, including merging and using all of them, or using them separately to compare the relative benefits of training with one type of unlabeled data versus another. We have clarified this in Section 6.1.\n\n---\n\n> As a follow-up, are unlabelled data from all domains (ID/OOD) used identically by all methods? DANN, for example, may work better if only using target domain unlabelled data, if available.\n\nYes, that is right: the unlabeled data is used identically by all methods (except ERM, which does not use any unlabeled data). For example, for iWildCam, all methods use unlabeled data from the extra domains; while for Camelyon, all methods use unlabeled data from the target domains. We agree with the reviewer that different types of unlabeled data (e.g., extra vs. target) might work better for some methods than others. In general, for our experiments, we picked the unlabeled target data whenever available, because we thought that this was likely to lead to stronger results. We have also clarified this in Section 6.1.\n\n---\n\n> The justification to not use augmentation on GlobalWheat are weak: translating and rotating bounding boxes seems easy enough?\n\nWe agree with the reviewer that it is straightforward to apply standard transformations, like translation and rotation, to bounding boxes. Our concern was that doing so would break the consistency assumption made by algorithms like FixMatch and NoisyStudent. For example, those algorithms assume that the predictions made on any example should be invariant under data augmentation, but if the data augmentation also changes the labels (e.g., by translating the image in such a way as to remove a bounding box), then this assumption would no longer be true. It did not seem straightforward to us to modify these algorithms accordingly. Another possibility would have been to define a custom set of augmentations that didn’t change the bounding boxes (e.g., just changing the color of the image, or adding some sort of random noise), but to our knowledge, that is not a common approach taken in detection tasks. To be clear, we believe that it would be an interesting research direction to experiment with methods that can exploit label-changing augmentations, but for the reasons above, we decided that it was out of the scope of the current paper. We have clarified this in the GlobalWheat paragraph in Section 6. Thank you for bringing this up.\n\n---\n\n> It seems that you did not try approaches that explicitly use the domain as weak labels or adversarially, such as ""Adversarial Multiple Source Domain Adaptation, Zhao et al. NeurIPS2018. I am not suggesting that you do (the comparison is already more than sufficient), but you should mention these approaches. Note that these would only work when the domains are not too numerous and meaningful.\n\nWe agree with the reviewer that approaches that explicitly use domain annotations are promising, and we thank them for the pointer! We have edited the last paragraph of our discussion (Section 7) to expand on this point accordingly.\n\n---\n\n> Details: The domains of PovertyMap are missing in Fig2\n\nThank you! We apologize for the omission and have fixed it in the revision.\n'}, {'title': 'Response to Reviewer RpU9, part 2/2', 'comment': ""> Self-supervised, semi-supervised and DA methods make different assumptions about the availability of target data. Self-supervised requires target labels for fine-tuning, while DA methods only require unlabeled target data during training and nothing more. So, comparing both of them together using a common yard-stick is, in my opinion, not appropriate.\n\nIn our benchmark, all of our methods use the exact same labeled and unlabeled data for each dataset. In particular, none of them use any target labels. We apologize for the lack of clarity, and we have edited the setup in Section 6.1 to clarify this. As we discussed above, prior work has shown that self-supervised and semi-supervised methods can perform well on domain adaptation benchmarks with nothing more than labeled source data and unlabeled target data. \n\n---\n\n> The DA methods considered are quite old and primitive. The authors are encouraged to consider more recent benchmarks like CDAN, CAN, AFN etc. The authors are also encouraged to use multi-source/multi-domain adaptation benchmarks [1,2,3] which seem most appropriate for the dataset proposed. at least for the visual recognition datasets. While self-supervised and semi-supervised methods aren't particularly designed keeping in mind cross-domain transfer, UDA methods like DANN and CORAL aren't designed to handle multi-domains.\n\nWe agree with the reviewer that newer and more sophisticated domain adaptation methods, and in particular those that can exploit multiple domains, might perform better on U-WILDS. In our revision, we have expanded on the discussion of other domain invariance methods in Appendix B.2, including citations to CDAN, CAN, and AFN, and a discussion of multi-source methods. We have also edited the main text in Section 5 to point to this explicitly. We hope that by releasing the U-WILDS open-source package and public leaderboard, we can encourage the broader research community to test out other existing methods and develop new methods on the U-WILDS datasets.\n\nAt least on benchmarks like DomainNet and VisDA-2017, we note that the prior work cited above [1-5] suggest that the self-training and self-supervised methods we study can outperform existing domain-invariance methods. However, we agree with the reviewer that newer domain-invariance methods would still be interesting to try.\n\n---\n\n> The authors are also encouraged to compare and contrast with [1] in terms of the datasets proposed in both papers (although these can be considered contemporary works).\n\nWe thank the reviewer for the pointer. The Geo-YFCC dataset introduced in that paper seems like a great dataset for domain generalization in the context of object recognition, and we have added it to the related work (Section 2). The U-WILDS datasets differ from Geo-YFCC in two ways. First, the U-WILDS data splits contain both labeled and unlabeled data, which allows them to be used to evaluate domain adaptation algorithms. In contrast, Geo-YFCC is geared for the transductive test-time adaptation setting, where they assume that models do not have access to any unlabeled data at training time, but are allowed to adapt on-the-fly to a small number of unlabeled test samples after training. Second, Geo-YFCC focuses on object recognition with ImageNet classes, which is an important but comparatively well-studied problem. For U-WILDS, we have deliberately aimed to broaden the range of modalities and applications studied. \n\n---\n\n > The key takeaways from sec 7 are not that surprising, and things like lack of augmentation strategies for many modalities and model selection for DA are already well known to the community, while ineffectiveness of pre-training seems interesting.\n\nIn addition to the ineffectiveness of pre-training, we believe that one of the more surprising results is how methods that can obtain state-of-the-art performance on standard domain adaptation benchmarks (such as FixMatch on DomainNet) can nonetheless fail to improve performance on the U-WILDS datasets, whereas other methods that are conceptually quite similar, like Noisy Student, can help in some settings.\n\n---\n\nWe appreciate the detailed clarifications requested by the reviewer, and we hope that our answers and revisions have adequately addressed them. Please let us know if there are any other questions or comments that we might be able to get to in the discussion period. Thank you.\n""}, {'title': 'Response to Reviewer RpU9, part 1/2', 'comment': ""We thank the reviewer for their detailed and thoughtful feedback. Below, we address them in turn.\n\n---\n\n> The models chosen for benchmarking the methods are not suitable. While domain adaptation methods like DANN and CORAL work strictly with an assumption of single source to target domains, semi-supervised and self-supervised works are well-known to work only for within-domain samples. In this respect, the observations found with respect to the benchmarking, although useful, aren't surprising.  \n\nWe agree with the reviewer that many of the semi-supervised and self-supervised methods were originally developed for within-domain samples. However, recent work has shown that semi-supervised and self-supervised methods can actually be highly effective on standard domain adaptation benchmarks. For example, Zhang et al. [2] and Berthelot et al. [3] show that FixMatch and NoisyStudent (which we benchmark on U-WILDS) can outperform strong UDA methods like MCD on domain adaptation benchmarks like Digit-Five and DomainNet. As further examples, on the semi-supervised / self-training side:\n- Saito et al. [1] introduce a pseudo-labeling technique that can outperform methods like DANN and MMD on digits datasets. \n- Zhang et al. [2] show that methods such as FixMatch and pseudo-labeling (which are included in the methods we tested for U-WILDS) can outperform UDA methods such as DANN, CDAN, AFN, and MDD on benchmarks like VisDA-2017 and DomainNet.\n- Berthelot et al. [3] show that self-training methods like AdaMatch, FixMatch, and NoisyStudent can substantially outperform UDA methods like MCD on digits datasets as well as DomainNet. \n\nOn the self-supervised side, for example:\n- Wang et al. [4] show that self-supervised contrastive learning approaches can outperform UDA approaches on VisDA-2017 and Office-31.\n- Tsai et al. [5] show that self-supervised contrastive learning approaches can help to train models to be robust to spurious domain-specific correlations, enabling them to extrapolate out-of-domain more reliably.\n\n\nIn our revision, we also ran additional experiments on the real -> sketch split in DomainNet to confirm that the changes we made to the methods we tested (for standardization and consistency) did not affect their performance. As in prior work, our results show that **all of the methods we benchmark on U-WILDS outperform ERM on this DomainNet split.**\n\n|                 | In-dist (real) | Out-of-dist (sketch) |\n|-----------------|----------------|----------------------|\n| ERM (-data aug) | 82.6 (0.0)     | 34.9 (0.2)           |\n| ERM             | 82.5 (0.3)     | 35.9 (0.3)           |\n| CORAL           | 78.0 (0.5)     | 37.3 (0.4)           |\n| DANN            | 77.8 (0.2)     | 39.4 (0.8)           |\n| Pseudo-Label    | 79.9 (0.2)     | 36.1 (0.4)           |\n| FixMatch        | 80.8 (0.2)     | **50.2 (0.4)**       |\n| Noisy Student   | 82.0 (0.3)     | 39.7 (0.2)           |\n| SwAV            | 79.0 (0.3)     | 38.2 (0.4)           |\n\n**However, the benchmarked methods do not do as well on U-WILDS.** We believe that our observations are interesting in light of the strong prior results on these methods on other benchmarks. For example, FixMatch does well on VisDA-2017, DomainNet, and Digit-Five [2,3], and it does extremely well on the DomainNet experiments we ran above, but its performance is starkly worse on the U-WILDS datasets, where it does not improve performance over ERM.\n\nWe have edited Sections 5 and 6 to highlight our reasoning and the prior work cited above, as we agree with the reviewer that it is otherwise unclear. We have also described the DomainNet setup and results in more detail in Appendix F. We thank the reviewer for bringing this point up.\n\n[1] Saito, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-training for unsupervised domain adaptation. In International Conference on Machine Learning (pp. 2988-2997). PMLR.\n\n[2] Zhang, Y., Zhang, H., Deng, B., Li, S., Jia, K., & Zhang, L. (2021). Semi-supervised Models are Strong Unsupervised Domain Adaptation Learners. arXiv preprint arXiv:2106.00417.\n\n[3] Berthelot, D., Roelofs, R., Sohn, K., Carlini, N., & Kurakin, A. (2021). AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation. arXiv preprint arXiv:2106.04732.\n\n[4] Wang, R., Wu, Z., Weng, Z., Chen, J., Qi, G. J., & Jiang, Y. G. (2021). Cross-domain Contrastive Learning for Unsupervised Domain Adaptation. arXiv preprint arXiv:2106.05528.\n\n[5] Tsai, Y. H. H., Ma, M. Q., Zhao, H., Zhang, K., Morency, L. P., & Salakhutdinov, R. (2021). Conditional Contrastive Learning: Removing Undesirable Information in Self-Supervised Representations. arXiv preprint arXiv:2106.02866.\n""}, {'title': 'Response to Reviewer 3NYa', 'comment': 'We thank the reviewer for their feedback and suggestions. Below, we respond to the reviewer’s questions. \n\n---\n\n> The descriptions of the data sets are relatively shallow. Besides raw increase in instances, not much else is reported about the data sets. Is there anything interesting to say about the instances that were chosen to be labeled by the original WILDS authors versus the ones added by U-WILDS? Namely, is there anything unique about the new instances? Can you characterize and measure how different the new instances are from the original ones?\n\nWe agree with the reviewer on the importance of characterizing the unlabeled vs. labeled data, so that algorithm developers have a good understanding of the kinds of structure and leverage they might be able to use. In general, we strove to choose unlabeled data that was qualitatively similar to the existing labeled data: for example, in iWildCam, we obtained unlabeled data from camera traps that are drawn from essentially the same distribution as the labeled camera traps. \n\nWe have attempted to provide these details in Appendix A, including describing the sources of the new unlabeled data; characterizing how it differs from the original labeled data in terms of domains; and describing the data processing. In our revision, we have edited Appendix A to add more detail on the differences between the unlabeled vs. labeled datasets for iWildCam (Appendix A.1), FMoW (Appendix A.3), MolPCBA (Appendix A.6), and Amazon (Appendix A.8). We thank the reviewer for bringing this important point up.\n\n---\n\n> Data preparation was not discussed at all. The major practical concern I have about this endeavor is standardization across new and old instances. If the original WILDS data set was released with some effort to process the data before repackaging and release, the U-WILDS data set would need to undertake the same process. I would like to see some comments on whether effort was taken to ensure that the U-WILDS instances were preprocessed for consumption in the same way the WILDS data set were, or why that was not a necessary step.\n\nWe also agree with the reviewer that it is important to be consistent in data preparation and to have a standardized protocol. We discuss our data preparation protocols for each dataset in Appendix A. In all cases, we made sure that the U-WILDS instances were preprocessed in the exact same way as the original WILDS instances. The only exception was for the Camelyon17 dataset, where the original labeled dataset was processed to be class-balanced; because we do not know the classes for the unlabeled data, we instead uniformly sampled patches at random for the unlabeled data.\n\n---\n\nIf the reviewer has any additional questions or concerns about the unlabeled vs. labeled data, data processing, or standardization that might be preventing them from providing a stronger recommendation, please let us know and we will do our best to answer them in this discussion period. Thank you.\n'}, {'title': 'Response to Reviewer Dxvt, part 2/2', 'comment': '> The authors should clarify the model selection procedure for each of the algorithms. For example, how does each method make use of the Validation (ID) labelled examples versus the Validation (OOD) unlabelled examples for model selection?\n\nWe apologize for the lack of clarity. None of our experiments use the Validation (ID) labeled examples or Validation (OOD) unlabeled examples for model selection; instead, they use only the Validation (OOD) *labeled* examples for model selection. This model selection procedure (using the Validation (OOD) labeled examples) is the same as in the original WILDS benchmark, which allows for direct comparisons to the results therein. We note that all of the following datasets are drawn from different distributions:\n- Training examples\n- Validation (OOD) examples\n- Target (OOD) examples\n\nFor example, in iWildCam, where the examples are photos and the domains are camera traps, the training domains, Validation (OOD) domains, and Target (OOD) domains are all disjoint sets of camera traps. This means that hyperparameter tuning on the Validation (OOD) data does not leak information on the Target (OOD) distribution. We have clarified this in Section 6.1 and Appendix D.3. \n\nIn Section 7, we discuss how the availability of Validation (OOD) unlabeled examples in U-WILDS expands the range of potential model selection procedures and is an interesting avenue for future work. Another potential option would be to do unsupervised hyperparameter tuning directly on the Target (OOD) unlabeled examples, such as in [3].\n\n[3] Saito, K., Kim, D., Teterwak, P., Sclaroff, S., Darrell, T., & Saenko, K. (2021). Tune it the Right Way: Unsupervised Validation of Domain Adaptation via Soft Neighborhood Density. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9184-9193).\n\n---\n\n> It would be good to: Verify that all of the datasets used have licenses that allow for public release, and that all datasets have been adequately anonymized/deidentified. \n\nWe can confirm that all of the datasets (both labeled and unlabeled) used have licenses that allow for public release. All datasets have been anonymized/deidentified. We have also ensured that the satellite image datasets (FMoW and PovertyMap) appropriately protect privacy. We have edited our Ethics statement to clarify these. We thank the reviewer for checking.\n\n---\n\n> Flag any potential fairness concerns with using these datasets for the benchmarking and selection of developed algorithms, e.g. as has been found in ImageNet [1].\n\nIn our Ethics statement, we have also attempted to discuss potential fairness concerns, especially with the CivilComments dataset, which deals with biases against mentions of particular demographic groups. We thank the reviewer for the pointer to the relevant reference; we have edited our Ethics statement to expand on this, and have included the reference as well as others.\n'}, {'title': 'Response to Reviewer Dxvt, part 1/2', 'comment': 'We thank the reviewer for their feedback and suggestions. Below, we respond to each in turn.\n\n---\n\n> In order to present an informal upper bound for the performance of algorithms on the OOD domain, the authors should consider adding in two additional ""oracle"" baselines which have access to labelled OOD-domain data: 1) training an ERM model only on the OOD domain, 2) training an ERM model on all available data.\n\nThank you for the suggestion! For four of the U-WILDS datasets (Amazon, CivilComments, iWildCam, FMoW), we created the unlabeled splits by acquiring additional data and then hiding their labels, so we were able to run fully-labeled ERM experiments using their ground truth labels. Specifically, we trained ERM models on all available training data (the original labeled data, as well as the “unlabeled” data with ground truth labels revealed), using the exact same held-out test set and evaluation protocol. \n\nFor FMoW and iWildCam, where the challenge is primarily in performing well on unseen domains (that are not in the labeled training set), the fully-labeled ERM models obtained significantly higher OOD test performance, as shown in this table:\n\n|                         | ERM (original) | ERM (fully-labeled) |\n|-------------------------|----------------|---------------------|\n| FMoW (worst-region acc) | 34.8%          | 58.7%               |\n| iWildCam (macro F1)     | 32.2%          | 44.0%               |\n\nThese results suggest that leveraging the unlabeled data could be helpful for these datasets.\n\nOn Amazon and CivilComments, where the challenge is primarily in performing uniformly well over subpopulations of the training distribution, the fully-labeled ERM baselines show a more modest improvement in test OOD performance:\n\n|                                  | ERM (original) | ERM (fully-labeled) |\n|----------------------------------|----------------|---------------------|\n| Amazon (10th percentile acc)     | 54.2%          | 56.4%               |\n| CivilComments (worst-group acc)  | 66.6%          | 69.6%               |\n\nThese results are consistent with prior observations that ERM models can have poor subpopulation performance even on large labeled training sets [1], necessitating other training procedures besides ERM for dealing with subpopulation shifts. We note that these are not quite oracle experiments, in the sense that the unlabeled data comes from the overall population, whereas the test performance is evaluated only on a particular subpopulation (like the worst 10th percentile group in Amazon, or the lowest-performing demographic group in CivilComments).\n\nIn our revision, we have added these fully-labeled ERM results to Section 6 and Table 2, and we have also added Appendix E, which describes their setup in more detail.\n\nWe note that the original WILDS paper [2] also contains oracle results for each of the datasets. The main difference is that the original WILDS paper did not use the data introduced in U-WILDS, which allows us to run fully-labeled ERM experiments using ground truth labels on the new data in U-WILDS.\n\n[1] Sagawa, S., Koh, P. W., Hashimoto, T. B., & Liang, P. (2019). Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731.\n\n[2] Koh, P. W., Sagawa, S., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., ... & Liang, P. (2021). Wilds: A benchmark of in-the-wild distribution shifts. International Conference on Machine Learning (pp. 5637-5664). PMLR.\n\n---\n\n> The authors should justify why the OOD domain was the one selected in each dataset. If possible, the authors could consider allowing for the ability to swap around the various training/validation/test domains, though it might be impractical if some domains do not have sufficient data.\n\nFor consistency, we kept the exact same test split (and therefore, the choice of OOD domains) as the original WILDS benchmark. We have edited Sections 3 and 4 to clarify this, and we have also emphasized it in Section 6.1. As described in the WILDS paper [2], the different datasets have different rationales for the choice of splits. In some cases, there were natural splits (e.g., for FMoW, where the domain shift is partially over time, the appropriate split would be to train on data from earlier time periods and test on data from later time periods). In other cases, the assignment of domains was simply done at random. For the PovertyMap dataset, the evaluation uses five folds, with each fold having a different training/validation/test split; however, PovertyMap is a relatively small dataset, and the original WILDS paper reported that doing something similar for the other datasets would have been too computationally expensive. \n'}, {'summary_of_the_paper': 'The authors propose U-WILDS, which extends the WILDS benchmark (typically used for domain generalization or subpopulation shift) to the unsupervised domain adaptation scenario. They select eight datasets from WILDS, spanning a variety of data modalities, and add in additional unlabelled examples from a variety of data sources. They benchmark a comprehensive set of algorithms which make use of unlabelled data, and find that most methods do not significantly outperform ERM, except for some limited cases which the authors characterize in detail.', 'main_review': 'The problem is well-motivated, and the use of realistic problems to benchmark unsupervised adaptation methods presents a major gain over prior datasets which rely on different stylized images (e.g. PACS). The datasets used span a wild variety of modalities and domains, and are fairly robust with large sample sizes. The paper is easy to read and easy to follow, and the experimental evaluations are robust.\n\nI would suggest the following improvements:\n\n- In order to present an informal upper bound for the performance of algorithms on the OOD domain, the authors should consider adding in two additional ""oracle"" baselines which have access to labelled OOD-domain data: 1) training an ERM model only on the OOD domain, 2) training an ERM model on all available data. \n\n- The authors should justify why the OOD domain was the one selected in each dataset. If possible, the authors could consider allowing for the ability to swap around the various training/validation/test domains, though it might be impractical if some domains do not have sufficient data.\n\n- The authors should clarify the model selection procedure for each of the algorithms. For example, how does each method make use of the Validation (ID) labelled examples versus the Validation (OOD) unlabelled examples for model selection?', 'summary_of_the_review': 'Though the paper does not propose any novel methodology, I believe that it is a solid step towards the use of real-world datasets for benchmarking unsupervised domain adaptation algorithms, and would be a valuable contribution to the conference. ', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '1: The contributions are neither significant nor novel.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['Yes, Discrimination / bias / fairness concerns', 'Yes, Privacy, security and safety', 'Yes, Responsible research practice (e.g., human subjects, data release)'], 'details_of_ethics_concerns': 'It would be good to:\n- Verify that all of the datasets used have licenses that allow for public release, and that all datasets have been adequately anonymized/deidentified.\n- Flag any potential fairness concerns with using these datasets for the benchmarking and selection of developed algorithms, e.g. as has been found in ImageNet [1].\n\n[1] https://arxiv.org/pdf/2010.15052.pdf', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors introduce an extension to the new, but popular, data shift benchmark WILDS data sets called U-WILDS.  U-WILDS increases the number of instances in these data sets significantly (by a factor 3.5-14x times, depending on the data set), but includes no additional labels.  Such an extension allows unsupervised domain adaptation techniques to now be evaluated on the various WILDS data sets.  The authors evaluate a sampling of the state of the art in unsupervised domain adaptation on these new data sets.  The main finding is that most of the techniques performed poorly, except data augmentation in the image problems.  The authors suggest that this could motivate the need for better data augmentation techniques for other modalities.', 'main_review': ""Strengths:\n1) The problem of unsupervised domain adaptation is well-grounded in a practical learning setting where labels are scarce, but observations are not.  This makes the extension of the WILDS data set to this setting a very useful contribution that can facilitate impactful future work.\n2) The empirical evaluation was fairly extensive in it's inclusion of a variety of unsupervised domain adaptation techniques.  I think it is important to validate the trend established in prior work that these techniques typically perform poorly in more realistic domain adaptation problems.  I also think the finding that data augmentation techniques work relatively well in image domains to be an interesting one that can motivate future work.\n3) Perhaps a necessary consequence of this kind of work is that the paper serves as a nice survey of modern unsupervised domain adaptation, both in terms of data sets and methods.  Someone interested in the topic could use this paper to begin a literature search.\n\nWeaknesses:\n1) Ultimately, the novelty of this work is low.  The main novel contributions are the extension to the existing WILDS data set (using already established data sets used by the original WILDS authors) and some new empirical results using previously published results.  However, I do not think for this kind of work that novelty is paramount.  \n2) The descriptions of the data sets are relatively shallow.  Besides raw increase in instances, not much else is reported about the data sets.  Is there anything interesting to say about the instances that were chosen to be labeled by the original WILDS authors versus the ones added by U-WILDS?  Namely, is there anything unique about the new instances?  Can you characterize and measure how different the new instances are from the original ones?\n3) Data preparation was not discussed at all.  The major practical concern I have about this endeavor is standardization across new and old instances.  If the original WILDS data set was released with some effort to process the data before repackaging and release, the U-WILDS data set would need to undertake the same process.  I would like to see some comments on whether effort was taken to ensure that the U-WILDS instances were preprocessed for consumption in the same way the WILDS data set were, or why that was not a necessary step."", 'summary_of_the_review': 'While I recognize the low novelty of the work, I believe the contribution of the extended WILDS data set as well as the insights provided in the empirical evaluation are of significant enough value to the machine learning community to warrant acceptance.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': '### New large scale dataset for unsupervised transfer learning\n\nThe paper proposed an extension to the popular WILDS benchmark dataset by augmenting the different domain data with additional unlabeled examples. The dataset consists of data of various modalities including images, graph and text from various domains. Additionally, many recent methods that leverage unlabeled images to achieve generalization are bench marked on the proposed datasets. ', 'main_review': '### Strengths \n\n- The dataset is a useful addition to the WILDS dataset, and the provided unlabeled data would be very useful to design many unsupervised generalization algorithms.\n- The paper is very well written, easy to understand and well organized, although the experimental evaluation could be explained in greater detail in main paper.\n\n### Required Clarifications\n\n- The models chosen for benchmarking the methods are not suitable. While domain adaptation methods like DANN and CORAL work strictly with an assumption of single source to target domains, semi-supervised and self-supervised works are well-known to work only for within-domain samples. In this respect, the observations found with respect to the benchmarking, although useful, aren\'t surprising. \n- self-supervised, semi-supervised and DA methods make different assumptions about the availability of target data. self-supervised requires target labels for fine-tuning, while DA methods only require unlabeled target data during training and nothing more. So, comparing both of them together using a common yard-stick is, in my opinion, not appropriate.  \n- The main paper needs to have more experimental detail such as what are the exact labeled and unlabeled data from source and target for each of the experiments.\n- The DA methods considered are quite old and primitive. The authors are encouraged to consider more recent benchmarks like CDAN, CAN, AFN etc.\n- The authors are also encouraged to use multi-source/multi-domain adaptation benchmarks [1,2,3] which seem most appropriate for the dataset proposed. at least for the visual recognition datasets. While self-supervised and semi-supervised methods aren\'t particularly designed keeping in mind cross-domain transfer, UDA methods like DANN and CORAL aren\'t designed to handle multi-domains. \n- The authors are also encouraged to compare and contrast with [1] in terms of the datasets proposed in both papers (although these can be considered contemporary works).\n- The key takeaways from sec 7 are not that surprising, and things like lack of augmentation strategies for many modalities and model selection for DA  are already well known to the community, while ineffectiveness of pre-training seems interesting.\n\n1. Dubey, Abhimanyu, et al. ""Adaptive Methods for Real-World Domain Generalization."" _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2021.\n2. Li, Ya, et al. ""Deep domain generalization via conditional invariant adversarial networks."" _Proceedings of the European Conference on Computer Vision (ECCV)_. 2018.\n3. Li, Da, et al. ""Deeper, broader and artier domain generalization."" _Proceedings of the IEEE international conference on computer vision_. 2017.', 'summary_of_the_review': ""Although the dataset itself is very useful and important, the analysis and bench marking needs some improvement. Nevertheless, this paper definitely has merit, and if the authors could clarify the questions raised, I would be happy to update the score. I haven't gone through the supplementary material in detail. If any of the questions above have a direct answer in the suppl. material, the authors can directly point to that and I would be happy to update my comments. \n\nPost Rebuttal \n****************\n\nI thank the authors for answering all my queries patiently, which answered most of the questions I had regarding suitability of self-supervised and semi-supervised algorithms to the task. I would like to raise my score, and suggest the authors to include these discussions in the main paper."", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors present U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. They provide a large quantity of unlabelled data complementing 8 of the existing multidomain labeled datasets in WILDS. They propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain-shift. They present reasoned conclusions, and open-source datasets and implementations.\n', 'main_review': 'Strength:\n- The work behind the data collection and baseline computation appears sizeable and very valuable\n- The authors implemented many of the top-performing algorithms meant to address domain shift with unlabelled data. They provide their implementation, as well as a unified, didactic, and detailed explanation.\n- The conclusions are both measured and essential: we lack a definitive answer to domain shift in the wild, and unlabelled only provides a very partial solution.\n- The writing is remarkably clear and to the point.\n\nWeakness:\nI am struggling to fault the paper.  The extensive appendix answered most of my questions.\n- The paper does not have a methodological contribution per se, but the answers provided from the meta-analysis are new - at least to me, even if the fact that current methods would not hold too well for in-the-wild data was suspected\n- I have some questions about missing details, detailed below\n\nQuestions:\n- ""Models are trained on labeled data from the source domains, as well as unlabeled data from one or more of the other sources, depending on what is realistic for the application."" I couldn\'t understand which problems were allowed to use OOD unlabelled data for training and which ones were not. From my understanding, unlabelled data from all domains (source, val, target, extra) are merged and used for training; is this correct? And what would be the use of unlabelled data if not for helping the training?\n- As a follow-up, are unlabelled data from all domains (ID/OOD) used identically by all methods? DANN, for example, may work better if only using target domain unlabelled data, if available.\n- The justification to not use augmentation on GlobalWheat are weak: translating and rotating bounding boxes seems easy enough?\n- It seems that you did not try approaches that explicitly use the domain as weak labels or adversarially, such as ""Adversarial Multiple Source Domain Adaptation, Zhao et al. NeurIPS2018. I am not suggesting that you do (the comparison is already more than sufficient), but you should mention these approaches. Note that these would only work when the domains are not too numerous and meaningful.\n\nDetails:\nThe domains of PovertyMap are missing in Fig2', 'summary_of_the_review': 'An impressive and exemplary dataset paper. The data collection and baseline implementation could be an important stepping stone for the ML community to address its biggest challenge yet: domain shift in the Wild. This paper highlights that we are not there yet, and that unlabelled data give an encouraging venue that has not reached maturity yet.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'N/A', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Extending the WILDS Benchmark for Unsupervised Adaptation', 'authorids': ['~Shiori_Sagawa1', '~Pang_Wei_Koh1', 'tonyhlee@stanford.edu', '~Irena_Gao1', '~Sang_Michael_Xie1', '~Kendrick_Shen1', '~Ananya_Kumar1', '~Weihua_Hu1', '~Michihiro_Yasunaga1', '~Henrik_Marklund2', '~Sara_Beery1', 'etienne.david@inrae.fr', '~Ian_Stavness1', 'guowei@g.ecc.u-tokyo.ac.jp', '~Jure_Leskovec1', '~Kate_Saenko1', '~Tatsunori_Hashimoto1', '~Sergey_Levine1', '~Chelsea_Finn1', '~Percy_Liang1'], 'authors': ['Shiori Sagawa', 'Pang Wei Koh', 'Tony Lee', 'Irena Gao', 'Sang Michael Xie', 'Kendrick Shen', 'Ananya Kumar', 'Weihua Hu', 'Michihiro Yasunaga', 'Henrik Marklund', 'Sara Beery', 'Etienne David', 'Ian Stavness', 'Wei Guo', 'Jure Leskovec', 'Kate Saenko', 'Tatsunori Hashimoto', 'Sergey Levine', 'Chelsea Finn', 'Percy Liang'], 'keywords': ['distribution shifts', 'adaptation', 'unlabeled data'], 'abstract': 'Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'sagawa|extending_the_wilds_benchmark_for_unsupervised_adaptation', 'pdf': '/pdf/16bc69d47c7ff67867bfc50009d6b9fc5043a00f.pdf', 'one-sentence_summary': 'We introduce U-WILDS, which augments the WILDS distribution shift benchmark with realistic unlabeled data, and benchmark existing methods for unlabeled data on these in-the-wild distribution shifts.', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/extending-the-wilds-benchmark-for/code)', '_bibtex': '@inproceedings{\nsagawa2022extending,\ntitle={Extending the {WILDS} Benchmark for Unsupervised Adaptation},\nauthor={Shiori Sagawa and Pang Wei Koh and Tony Lee and Irena Gao and Sang Michael Xie and Kendrick Shen and Ananya Kumar and Weihua Hu and Michihiro Yasunaga and Henrik Marklund and Sara Beery and Etienne David and Ian Stavness and Wei Guo and Jure Leskovec and Kate Saenko and Tatsunori Hashimoto and Sergey Levine and Chelsea Finn and Percy Liang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=z7p2V6KROOV}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Kyle Hsu', 'Moo Kim', 'Rafael Rafailov', 'Jiajun Wu', 'Chelsea Finn']",ICLR,Vision-Based Manipulators Need to Also See from Their Hands,https://iclr.cc/virtual/2022/oral/6103,2022," We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.",Oral 1: AI Applications,https://openreview.net/pdf?id=RJkAHKp7kNZ,https://openreview.net/forum?id=RJkAHKp7kNZ,RJkAHKp7kNZ,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'All reviewers consistently agree on the high quality of the research presented in this paper, such that it the paper clearly is significantly above the acceptance threshold of ICLR.'}, {'title': 'Acknowledgement', 'comment': 'Thank you very much for your engagement.'}, {'title': 'score confirmed', 'comment': 'Thank you for addressing my comments. '}, {'title': 'improved the review score', 'comment': 'The score now reads 8 which I forgot to mention in my previous message --- sorry about that. '}, {'title': 'final remarks', 'comment': 'Hi there, \n\nThank you for your rebuttal. It more than answers my concerns and thanks for your explanations. I am very supportive of this work and as I said before it is a useful case study to understand the design choices we make when doing any image based robot control. Thank you also for Appendix B.6. '}, {'title': 'Additional Revision', 'comment': '### Writing\n- [u4WJ] We add clarifications on the novelty of our work to the abstract (currently visible in the pdf but not on this webpage).'}, {'title': 'Re: Your Concern About Novelty', 'comment': 'Dear Reviewer u4WJ,\n\nWe notice that your main concern regarding our work is novelty. You mention in your summary that “The ideas presented in the paper are not something that’s unknown”, and you give scores of 2 for “Novelty And Significance”. It seems that you are largely satisfied with significance, as you deem the work “a good case study … a good investigation”. We agree that our ideas are not completely novel — people have put cameras in robot hands. We have better clarified the novelty of our work by adding the following sentence to the abstract:\n\n> While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving vision-based robotic manipulation.\n\nWe hope this improves your opinion of our work. Thank you again for your reviewing efforts.'}, {'title': 'Summary of Revisions', 'comment': ""We thank the reviewers again for their thoughtful reviews and their appreciation of the paper's contributions, experimental execution, and writing. We were eager to incorporate their constructive feedback as revisions to the paper (highlighted in magenta in the pdf), which we now summarize:\n\n### Experiments\n- [RmQW] In Appendix A.5, we study whether the data augmentation in DrQ is still necessary with the hand-centric perspective. The results indicate that this is indeed the case.\n- [u4WJ] In Appendix B.6, we study whether orientation control could change the relative performance between choices of perspective. We find that the three agents that used a hand-centric perspective significantly outperformed the one that didn't. We also find that VIB regularization results in improvement when using both perspectives. These results are in line with what we observed in the previous experiments.\n- [z6Z4] In Appendix B.5, we study to what extent the Meta-World tasks we consider require vision by running a proprioception-only agent. This agent fails to consistently solve the tasks on the training distribution, demonstrating the importance of vision.\n\n### Writing\n- [RmQW] We add discussion on the choice of memory-augmented policies vs. global observations to resolve partial observability to our related works section.\n- [u4WJ] We add discussion on the applicability of our study to more complex, longer-horizon manipulation tasks in the conclusion.\n- [z6Z4] We make explicit the possible relevance of our findings on the role of perspective in manipulation to robotic systems that use tactile sensing in the conclusion.\n- [RmQW] We add an explanation of $z_\\text{shift}$ in the main paper (caption of Figure 2).\n\n### Additional References\n- [RmQW] We add discussion of Szot et al. (2021) to our related works section.\n- [u4WJ] We add Zhan et al. (2020) to our related works section.\n\nWe hope that the additional experimental results and text have addressed the reviewers' concerns, and we are grateful for their help in improving the paper. Please let us know if you have any further comments!""}, {'title': 'Author Response to Reviewer u4WJ', 'comment': ""Thank you for your thoughtful review and constructive feedback. We have revised the paper with new discussion and experiments based on your feedback, and we respond to individual points below.\n\n>  Specifics about the training … The experimental set up to show that hand perspective is better than third perspective involves picking up a cube (which is somewhat trivial task and naturally favours hand perspective).\n\nThis description and critique of the cube grasping task (Section 3) is fair, but this is only one part of our empirical study. The Meta-World results in Section 4 also contain head-to-head comparisons between the two perspectives ($\\pi \\circ O_{h+p}$ and $\\pi \\circ O_{3+p}$) for a variety of manipulation tasks, most of which are much less trivial than picking up a cube. Indeed, the “reach-hard” and “peg-insert-side-hard” tasks were specifically designed to be hard for the hand-centric perspective. As you correctly suggest, such tasks demonstrate that the hand-centric perspective alone is not sufficient in general, which we ultimately address via the $\\pi \\circ O_{h+3+p} + \\mathrm{VIB}(z_3)$ agent.\n\n> My first impression was that it's certainly helpful to have hand perspective as it always gets a close up / zoomed-in view of the object free of any occlusions etc. so it makes sense that the hand perspective performs better. However, if you have a more complicated manipulator e.g. hands the self occlusions from fingers and as the hand starts to cage the object, you'd need a third person view to get a better view of the object.\n\nDextrous manipulation from vision is typically done palm-up to avoid occlusions. Tactile sensing becomes necessary otherwise; we don’t think that vision is in general sufficient for manipulation.  The relevance of our work to tactile sensing is that it supports the idea that tactile sensing not only provides modality-specific advantages (e.g. no issue with occlusions) but also affords generalization through locality/symmetry. \n\n> I feel adding rotation in the end-effector target (which the current output parameterisation doesn't include) could make things even for third perspective too. Is there a reason why the network only outputs just 3D end-effector position? For this particular task I believe you don't need rotation but any general task you'd need to also parameterise rotations as rotations will bring occlusions for hand perspective too.\n\nThe reason is as the reviewer suggests -- all the tasks we considered are solvable without changes in orientation. To probe this, we have added Appendix B.6 to the paper, which features additional experiments on two variants of peg-insert-side that require an additional 1-DoF end-effector orientation control. On both variants, the hand-centric perspective still affords faster training and better generalization than the third-person perspective, and both perspectives with VIB regularization dominates. We leave experiments with 6-DoF pose control to future work.\n\n> Though most (if not all) of these tasks involve decision making that has more to do with the object they are reaching to and less about doing any long horizon interaction with scene or other objects in the scene after interaction with the object they are reaching to.\n\nThank you for pointing this out. It is fair to be skeptical of how applicable our study is to long-horizon tasks and scene-level reasoning. However, it should be noted that long-horizon manipulation tasks will involve sub-tasks that are analogous to those we experiment with, so the insights from our study should at least apply in that respect. We have added this discussion to Section 6 of our paper.\n\n> Although this is not the first paper to show that e.g. https://arxiv.org/pdf/2012.07975.pdf have also shown that adding hand perspective helps. This paper was also not cited.\n\nThank you for the relevant reference. We have added it to the first paragraph of the related works section. Like the other works we cited (e.g. Wu et al. (2021), Mandelkar et al. (2021)), Zhan et al. (2020) do not consider out-of-distribution generalization, which is a large focus of our empirical study.\n""}, {'title': 'Author response to Reviewer z6Z4', 'comment': 'Thank you for your thorough feedback. We appreciate that you took the time to comment on the aspects of the paper you liked. We have revised the paper with new discussion and experiments based on your feedback, and we respond to individual points below.\n\n> You mention tactile signals as a good example in robot manipulation of local streams of information. What is the applicability of your study to the tactile sensor modality?\n\nThis is an excellent question. Our study is broadly related to the ideas of tactile/multi-modality sensing: we study the effects of sensor embodiment and how to properly fuse information from multiple sensors for the sake of generalization. We view both hand-centric cameras and tactile sensing as instances of hand-centric sensing. Since our work operates in just one modality (modulo proprioception), we are able to provide controlled insight on the effect of perspective that may apply to other forms of hand-centric sensing. For example, our work supports the idea that tactile sensing not only provides modality-specific advantages but also affords generalization through locality/symmetry. We have revised Section 6 of the paper to include more discussion on this.\n\n> It may be that some of the tasks could be solved using proprioceptive information only - do you have results using proprioception only as your observation space? Comparing the results presented in the paper with results obtained with proprioception only would be instrumental to understand the effect/contribution of vision (especially in the case of O_{h+p})\n\nThank you for the insightful suggestion. We have run a proprioception-only variant of the DrQ-v2 agent on the Meta-World tasks, and have revised the paper to include a pointer to the results in Appendix B.5. Since the proprioception-only agent performs poorly during training for all of the tasks, this experiment concretely establishes that vision is necessary to consistently solve the tasks. Indeed, all of these tasks have wide distributions of initial object positions. Thus, a manipulator operating from proprioception only without privileged information is forced to do blind physical search to accomplish task instances. '}, {'title': 'Author Response to Reviewer RmQW', 'comment': 'Thank you for your precise review and constructive feedback. We have revised the paper with new discussion and experiments based on your feedback, and we respond to individual points below.\n\n### Weaknesses \n> $z_\\mathrm{shift}$ is only explained in the appendix, it should be explained in the main paper.\n\nThank you for pointing this out. Our updated draft explains this in the caption of Figure 2. \n\n> The information bottleneck technique harms performance initially.\n\nIndeed, incorporating the VIB does slow training (but does not lower asymptotic performance) for some tasks. In the discussion on p. 8 (under the heading “Effect of regularizing the third-person information stream via a VIB.”), we conjecture based on work studying how neural networks learn spurious correlations that this slowdown may be hard to avoid, at least in the single-task setting. In future work, it would be interesting to explore ways to combat this, e.g. via an inductive bias, side information, multi-task learning, etc.\n\n### Suggestions for improvement\n\n> In concurrent work, Szot et al. (NuerIPS 2021) also used a hand/arm camera to learn manipulation policies and found similar trends. It may be worth citing as additional support for these finding.\n\nThank you for the relevant reference. We have added discussion on Szot et al. to the second paragraph of our related works section. It is encouraging to see complementary results on the significant effect of sensor embodiment on generalization. One key difference between our setting and theirs is that we consider physical manipulation tasks, whereas their picking task uses “abstracted grasping” and hence asks the policy to focus on learning to navigate and move the manipulator in free space. \n\n> How useful is the data-aug in DrQ for the hand camera? Part of the argument for DrQ is to reduce overfitting of the Q function during training. Perhaps with the hand camera you no longer need aug?\n\nThis is an intriguing suggestion. We experiment with removing the data augmentation in DrQ (essentially recovering SAC) for both $\\pi \\circ O_h$ and $\\pi \\circ O_3$ and have revised the paper to point to new results in Appendix A.5. The results show that data augmentation remains important even with the hand-centric perspective: removing the data augmentation severely compromises training for both agents, though $\\pi \\circ O_h$ still performs better.\n\n> What about recurrent policies instead of the third person camera?\n\nThis is a good suggestion: memory could certainly be used in place of a third-person camera as a strategy to resolve partial observability. However, recurrent networks are known to be difficult to train in an RL setting, and their use in the literature seems to correspond to when observations that would resolve the partial observability cannot be conveniently instrumented. Since this is not the case in tabletop manipulation, we follow prior work in choosing to use third-person observations with feedforward policies. In contrast, recurrence is standard practice for tasks involving navigation as global third-person observations are less natural: Hill et al. (2019) and Szot et al. (2021) follow this paradigm. In our updated draft, we have added this discussion to the second paragraph of our related works section. It would be interesting to study the performance trade-offs between the two strategies in future work.'}, {'summary_of_the_paper': ""This paper presents an analysis of camera placement for vision-based manipulators. Specifically it compares the performance of a disembodied third person camera vs. place the camera on the robot's hand/gripper.\n\nThe authors find that the hand camera improves generalization and training performance in the cases where a hand camera still reveals enough information to complete the task.\n\nWhen the hand camera does not reveal enough information to perform the task, the third person camera is still needed and the authors propose to use an information bottleneck to reduce the amount of information used from the third person camera, thereby improving generalization even when it's needed.\n"", 'main_review': '### Strengths\n- Well motivated idea\n- Compelling results\n- Overall clearly written\n- Good ablations and the idea is shown for multiple algorithms and in a wide variety of settings\n- Interesting approach to generalize the hand camera to settings with a higher degree of partial observability\n\n### Weakness\n\n- $$z_\\text{shift}$$ is only explained in the appendix, it should be explained in the main paper.\n- The information bottleneck technique harms performance initially\n\n\n\n### Suggestions for improvement\n\n- In concurrent work, Szot et al (NuerIPS 2021) also used a hand/arm camera to learn manipulation policies and found similar trends. It may be worth citing as additional support for these finding.\n\n- How useful is the data-aug in DrQ for the hand camera? Part of the argument for DrQ is to reduce overfitting of the Q function during training. Perhaps with the hand camera you no longer need aug?\n\n- What about recurrent policies instead of the third person camera?\n\n\nSzot et al: https://arxiv.org/abs/2106.14405', 'summary_of_the_review': 'This paper presents thorough analysis of using a 3rd person camera vs. a hand camera and finds that hand cameras generalize better. I believe this work presents a useful contribution.\n\n\n### Post Rebuttal Update\n\nI thank the authors for their response. I continue to think this is a good paper that should be accepted for publication.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper studies the effect of visual perspective by using the images coming from a camera installed on the hand of the robot in the specific context of robot manipulation from raw observations.  Results demonstrate that such a choice of visual perspective requires no algorithmic changes but can improve OOD generalisation and training efficiency. Of course this does not mean that we abandon altogether the traditional third person perspective as in many cases just having a close-up view of one object may not be enough when making decisions about the scene as a whole. Therefore, they also show that combined with third person perspective with information bottleneck regularisation can improve the OOD generalisation. They show results on six different manipulation tasks adapted from Meta-World and their choice of perspective improves OOD generalisation. ', 'main_review': ""Specifics about the training \n- 84x84 RGB input images. \n- outputs 3D end-effector position relative to the robot base, 1D gripper width, and a boolean contact flag for each of two gripper fingers. No rotation.\n- Three learning algorithms: Dagger, DrQ, DAC. \n\nThe experimental set up to show that hand perspective is better than third perspective involves picking up a cube (which is somewhat trivial task and naturally favours hand perspective). My first impression was that it's certainly helpful to have hand perspective as it always gets a close up / zoomed-in view of the object free of any occlusions etc. so it makes sense that the hand perspective performs better. However, if you have a more complicated manipulator e.g. hands the self occlusions from fingers and as the hand starts to cage the object, you'd need a third person view to get a better view of the object. Secondly, I feel adding rotation in the end-effector target (which the current output parameterisation doesn't include) could make things even for third perspective too. Is there a reason why the network only outputs just 3D end-effector position? For this particular task I believe you don't need rotation but any general task you'd need to also parameterise rotations as rotations will bring occlusions for hand perspective too. \n\nIn the 6 meta-world tasks, they show that combining a third perspective together with information bottleneck regularisation leads to better generalisation. Though most (if not all) of these tasks involve decision making that has more to do with the object they are reaching to and less about doing any long horizon interaction with scene or other objects in the scene after interaction with the object they are reaching to. \n\nThe paper seems to suggest that a zoomed-in view of the object by using a hand perspective almost always helps which I agree with. It seems to highlight the design choices that we regularly make when doing manipulation with raw observations need to be carefully looked into. Although this is not the first paper to show that e.g. https://arxiv.org/pdf/2012.07975.pdf have also shown that adding hand perspective helps. This paper was also not cited. \n\n\n"", 'summary_of_the_review': ""The paper is a good case study. The ideas presented in the paper are not something that's unknown but this paper does a good investigation on the choice of perspectives.  \n\n\n"", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper presents an interesting empirical evaluation of the role of visual perspective in learning and generalization in the context of physical manipulation. The paper compares performances using third-person and in-hand visual perspectives, showing that hand-centric vision consistently improves training and out-of-distribution generalization. Authors also explore a combination of the two perspectives by proposing to regularize the third-person information stream to maintain generalization performance.', 'main_review': 'The paper is well motivated, clearly written and coherently structured. The exposition of the main ideas is linear and easy to follow. The contribution is clear, well presented and well motivated. The notation and the formulation of the proposed method are clearly presented. Claims are supported by thorough experimental results. Figures and tables are presented in a nice and easily readable way, and help grasping the contribution of the study. Videos are also helpful in understanding the experimental setup and the results in a qualitative way.\nYou mention tactile signals as a good example in robot manipulation of local streams of information. What is the applicability of your study to the tactile sensor modality?\nIt may be that some of the tasks could be solved using proprioceptive information only - do you have results using proprioception only as your observation space? Comparing the results presented in the paper with results obtained with proprioception only would be instrumental to understand the effect/contribution of vision (especially in the case of O_{h+p})\n', 'summary_of_the_review': 'The paper is well written, the contribution is clearly presented and the experimental results are thoroughly executed. An evaluation comparing the presented results with the proprioception-only case could further improve the analysis.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'N/A', 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Vision-Based Manipulators Need to Also See from Their Hands', 'authorids': ['~Kyle_Hsu1', '~Moo_Jin_Kim1', '~Rafael_Rafailov1', '~Jiajun_Wu1', '~Chelsea_Finn1'], 'authors': ['Kyle Hsu', 'Moo Jin Kim', 'Rafael Rafailov', 'Jiajun Wu', 'Chelsea Finn'], 'keywords': ['reinforcement learning', 'observation space', 'out-of-distribution generalization', 'visuomotor control', 'robotics', 'manipulation'], 'abstract': 'We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation.', 'one-sentence_summary': 'Appropriately designing the observation space of a vision-based manipulator and regularizing its representations leads to clear gains in learning stability and out-of-distribution generalization.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'hsu|visionbased_manipulators_need_to_also_see_from_their_hands', 'pdf': '/pdf/bf5308ad68220347e7cbf2dcbedbf7bb4e0a21b1.pdf', 'data': '', '_bibtex': '@inproceedings{\nhsu2022visionbased,\ntitle={Vision-Based Manipulators Need to Also See from Their Hands},\nauthor={Kyle Hsu and Moo Jin Kim and Rafael Rafailov and Jiajun Wu and Chelsea Finn},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=RJkAHKp7kNZ}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shi Zhan Liu', 'Hang Yu', 'Cong Liao', 'Jianguo Li', 'Weiyao Lin', 'Alex Liu ·']",ICLR,Pyraformer_ Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting,https://iclr.cc/virtual/2022/oral/6828,2022," Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.",Oral 4: Sequence modeling,https://openreview.net/pdf?id=0EXmFzUn5I,https://openreview.net/forum?id=0EXmFzUn5I,0EXmFzUn5I,"[{'title': 'Noted with thanks!', 'comment': 'Thanks for pointing this out. The subscript of $\\mathbb{N}$ should be $\\ell$, as you wrote.'}, {'title': 'There may be a tiny-mistake in your formula', 'comment': 'In formula 3, the subscript of the summation symbol in the denominator seems to be problematic.\n\nthe original is:\n$\\sum\\limits_{ ℓ \\in {\\scriptstyle \\mathbb{N}}_{\\scriptstyle l}^{\\scriptstyle (s)}}$\n\ni believe is:\n$\\sum\\limits_{ ℓ \\in {\\scriptstyle \\mathbb{N}}_{\\scriptstyle ℓ}^{\\scriptstyle (s)}}$\n\nIf there are something that i mis-understand, please point out, tks~\n\n\n'}, {'title': 'Noted with thanks!', 'comment': ""Thank you for your attention to our work and valuable suggestions! We'll take a closer look at the literature you have listed.""}, {'title': 'Potentially helpful work from the time series forecasting literature', 'comment': 'Interesting work and glad to see its good performance! \n\nThere is a similar idea that has been developing in the time series forecasting literature for the past years. It may help you in connecting some of the intuitions there with your work. Here is the paper that introduced the modelling idea:\n- Kourentzes, N., Petropoulos, F., & Trapero, J. R. (2014). Improving forecasting by estimating time series structural components across multiple frequencies. International Journal of Forecasting, 30(2), 291-302.\n\nAnd a follow up work that made the theory quite a bit more solid:\n- Athanasopoulos, G., Hyndman, R. J., Kourentzes, N., & Petropoulos, F. (2017). Forecasting with temporal hierarchies. European Journal of Operational Research, 262(1), 60-74.\n\nThere are quite a few papers building on these, but I think these are a good start, if you find them useful. It is always difficult to keep track of what is happening in adjacent fields!'}, {'title': 'Response to Yi Rao (2)', 'comment': ""I don't think shuffling the training data will cause future information leakage. \n\nFirst, operations in the network are parallel in the 'batch' dimension and do not model the correlations between the different time series within the batch. Thus, even if the future time series is in the same batch as the past time series, the future information is not leaked to the past. \n\nSecond, assuming that the network trained on the training set predicts the future based on future information leakage, then it should do poorly on the test set, because there is no shuffle in the test set. However, many methods perform better on the training set with shuffle than without shuffle. \n\nThird, if information leakage occurs when the future time series and the past time series are in the same batch, then training without shuffle will cause more leakage than that with shuffle. Since the time series in the same batch are closer in time and have higher correlation when no shuffle is done. \n\nFinally, shuffling the training dataset did not start with Informer. For example, as early as in 2017, DeepAR [1] was not trained on the unshuffled training set.\n\n[1] Salinas, D., Flunkert, V., & Gasthaus, J. (2017). DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks. arXiv preprint arXiv:1704.04110.""}, {'title': 'Response to Shizhan Liu', 'comment': ""Thank you for your reply, so it's clear that the complexity per layer & maximum path length of Pyraformer is the same as that of dilated CNN.\n\nAnd I noticed that in your code you shuffle the training data, I know that this is the setting used by Informer and its followers, but it doesn't mean that this is the correct setting. In fact, in practice, we should not shuffle the data when training for a time series prediction problem, because it can lead to serious leakage (using future data to predict the past). Not to mention that shuffle makes the PositionalEmbedding  in your code meaningless during training, because the order you use as position is actually disordered. If you compared the performance of Pyraformer with Wavenet or other models under this setting, the comparison may not be correct, as some models may overfit the training set and resulting in worse test results due to this leakage.""}, {'title': 'Response to Yi Rao', 'comment': ""Thank you for your valuable comment. What we refer to in Table 1 is the general convolution. As mentioned in [1], the maximum path length for the general convolution is $O(\\frac{L}{k})$, while that for dilated convolution is $O(log_k\u2061 L)$:\n\n    Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n    or O(log_k(n)) in the case of dilated convolutions, increasing the length of the longest paths\n    between any two positions in the network.\n\nFor the dilated convolution, we have also compared the performance of Pyraformer with Wavenet in the experiment, see 'Response to reviewer WMVh, part2'. Pyraformer performs a little bit better than Wavenet. This may be because Pyraformer passes information through the pyramidal graph in each layer, while the dilated convolution loses some local information in order to expand the receptive field.\n\nIn addition, the CSCM of Pyraformer can not only use convolution, but also use parameterless operators such as average pooling, Max pooling, etc. This may help to alleviate the problem of large parameters and hard to optimize when the convolution kernel is large.\n\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\n""}, {'title': 'The comparison with CNN is not fair', 'comment': 'The maximum path length of CNN is $ O(log_kL) $ instead of $ O(L) $ as written in Table 1, where k is the kernel width. if $ k ∝ \\sqrt[S-1]{L} $ is chosen (the same way as C is chosen in the paper), then the maximum path length of CNN should also be $ O(1) $. The complexity per layer & maximum path length of Pyraformer is actually the same as that of CNN, and Pyraformer is not proven to be superior to CNN in the paper.'}, {'title': 'Noted with thanks!', 'comment': 'Thanks for pointing this out! We will correct the argument in the camera-ready version.'}, {'title': 'A suggestion on related work.', 'comment': 'Dear authors,\n\nCongratulations on the acceptance, I was the author of BP-Transformer and I noticed you mentioned ""Note that BP-Transformer initializes the nodes at each scale by embedding the corresponding sequences"" in the related work section. This argument is not true because BP-Transformer initializes span nodes with all zeros and leaf nodes with corresponding word embedding. I hope you can consider revising this argument.\n\nThanks\n\n'}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'The authors propose a multi-resolution pyramidal attention mechanism to capture long-range dependencies in time series forecasting, achieving linear time and space complexity. The authors conduced an extensive set of experiments and ablation studies demonstrating that  the proposed method consistently outperforms the state-of-the-art and provided evidence for the various components of the architecture. They also provided a proof guarantee the linear complexity of long sequence encoding and adequately addressed the concerns raised by the reviewers. The additional additional benchmarks conducted by the author further demonstrated the strong performance of the method. All reviewers agreed that this work makes a solid contribution to the field.'}, {'title': 'Thanks for the Response of Reviewer 17as', 'comment': ""Many thanks to Reviewer 17as for providing an impressively insightful pre-rebuttal review. Your detailed suggestions help us a lot in paper revision.\n\nWe'd also thank your dedication for carefully judging our feedback and raising the score! Your constructive suggestions are very helpful for us to improve the paper in a better shape.""}, {'title': 'Thanks for the Response of Reviewer WMVh', 'comment': ""Many thanks to Reviewer WMVh again for providing a detailed valuable pre-rebuttal review. Your detailed suggestions help us a lot in paper revision.\n\nWe'd also thank you for carefully judging our feedback and raising the score! Your constructive suggestions are very helpful for us to improve the paper in a better shape.\n""}, {'title': 'Thanks for the Response of Reviewer f9f9', 'comment': ""We'd like to thank Reviewer f9f9 again for providing an insightful pre-rebuttal review, which has enabled us to make an effective response. Your detailed suggestions help us a lot.\n\nWe'd also thank you for carefully judging our feedback and raising the score! Your constructive suggestions are very helpful for us to improve the paper in a better shape.\n""}, {'title': 'Thanks for the response', 'comment': 'I have upgraded my score to 8. I think the rebuttal addresses most of my concerns.'}, {'title': 'Raising my recommendation score base on rebuttal', 'comment': ""I want to thank the authors for providing a rebuttal discussing my concerns on the paper. Overall I believe the rebuttal is thorough and resolved most of my concerns. Therefore, I'm raising my recommendation score.""}, {'title': 'Thank you for your response', 'comment': 'Thank you for your detailed response, which has convincingly addressed my initial concerns. I think that this paper makes a strong contribution and will be updating my review to recommend acceptance.'}, {'title': 'Response to reviewer 17as, part1', 'comment': ""We appreciate your thoughtful and insightful comments. We believe that addressing the reviewer's comments has resulted in improving the clarity and presentation of the paper’s contributions and has brought the paper to a higher standard. Below we address your concerns point by point. Corresponding modifications in the paper are **highlighted in blue**. Unless otherwise stated in our response, all pages, equations, sections and bibliographical references refer to those in the revised paper.\n\n> *1. It would be better to formally define Signal Traversing Path and forward reference it from the introduction.*\n\nWe have added the definition of the signal traversing path in **Proposition 2** and **forward referenced it in the second paragraph in the introduction.**\n\n> *2. It would be better if one states upfront that some external mechanism is used to extract the initial states of the coarser nodes. The clarity of section 3.2 can be improved.*\n\nWe have briefly introduced what the nodes at each scale represent and further mentioned that the coarser-scale nodes are initialized using the coarser-scale construction module in the third paragraph in the introduction as follows:\n\n''The inter-scale connections build a multiresolution representation of the original sequence: nodes at the finest scale correspond to the time points in the original time series (e.g., hourly observations), while nodes in the coarser scales represent features with lower resolutions (e.g., daily, weekly, and monthly patterns). **Such latent coarser-scale nodes are initially introduced via a coarser-scale construction module.**''\n\nTo make **Section 3.2** more clear, we have added another sentence in this section:\n\n''Specifically, the coarse-scale nodes are introduced scale by scale from bottom to top by performing convolutions on the corresponding children nodes $\\mathbb C_\\ell^{(s)}$.''\n\nWe cannot add further explanations due to the page limit and hope the above one-sentence summary helps to clarify this section.\n\n> *3. Where is N defined? What is the relation between N and S ? I would suggest defining A, L, C, N, S clearly in one place with some indentation in Section 3.1, for ease of reading.*\n\n$N$ is defined under Eq. (3) in the paper. Note that $N$ is the number of attention layers and $S$ is the number of scales. We treat these two parameters as hyper-parameters. They can be chosen independently. We recommend to determine the number of attention layers $N$ based on the available computing resources, as this number is directly related to the model complexity. On the other hand, the number of scales $S$ can be determined by the granularity of the time series. For example, for hourly observations, we typically assume that it may also have daily, weekly and monthly periods. Therefore, we can set $S$ to be 4. We have added a discussion on hyper parameters selection in **Appendix K**.\n\nIn addition, we notice that the number of notations used in this paper is relatively large. To avoid any further confusion, we have added a table in the Appendix that defines all notations (see **Table 4** in the paper).\n\n> *4. Please be slightly more clear in the second paragraph of section 3.3. I would like to verify my understanding of the multi-step forecasting modules. In the first module, we just have a fixed output layer which is a dense mapping to F outputs, where F is the future prediction length. In the second mapping, there is a decoder that sequentially generated the multi-step output and the decoder is equivalent to the decoder in the original transformer paper.*\n\nThanks for pointing this out!\n\nIn the first module, yes, we did apply a fixed fully-connected layer to map the outputs of the encoder to the $F$ time points to be predicted.\n\nThe second prediction module is different from the one in the original Transformer though. In Transformer, the decoder outputs the predictions one by one in an autoregressive manner. By contrast, in Pyraformer, we output the multi-step prediction results in a batch.\n\nTo solve your confusion, we have modified the second paragraph in **Section 3.3** as follows: (see the next part)""}, {'title': 'Response to reviewer 17as, part2', 'comment': ""''For multi-step forecasting, we propose two prediction modules. The first one is the same with the single-step forecasting module, but **it maps the last nodes at all scales to all $M$ future time steps in a batch**. The second one, on the other hand, resorts to a decoder with two full attention layers. Specifically, similar to the original Transformer (Vaswani et al., 2017), we replace the observations at the future $M$ time steps with 0, embed them in the same manner with the historical observations, and refer to the summation of the observation, covariate, and positional embedding as the ''prediction token'' $\\mathbf{F_p}$. The first attention layer then takes the prediction tokens $\\mathbf{F_p}$ as the query and the output of the encoder $\\mathbf{F_e}$ (i.e., all nodes in the PAM) as the key and the value, and yields $\\mathbf{F_{d1}}$. The second layer takes $\\mathbf{F_{d1}}$ as the query, but takes the concatenated $\\mathbf{F_{d1}}$ and $\\mathbf{F_e}$ as the key and the value. The historical information $\\mathbf{F_e}$ is fed directly into both attention layers, since such information is vital for accurate long-range forecasting. The final prediction is then obtained through a fully connected layer across the dimension of channels. **Again, we output all future predictions together to avoid the problem of error accumulation in the autoregressive decoder of Transformer.**''\n\n> *5. Clarify the information of the datasets and the experimental setup. In multi-step forecasting what is the length of the future prediction window. Also, is the task rolling prediction over the test set?*\n\nWe have added relevant statements in **Appendix F**. In multi-step forecasting, the prediction length is shown in Table 3, and we have added the corresponding historical length for each prediction length in **Appendix F**. \n\nFor the prediction on the test set, we follow the scheme in Informer [1]. More concretely, we partition the testing data into sliding windows with stride 24. The width of each window equals the historical length plus the prediction length. We then use the model trained on the training data and forecast the prediction part in each window given the history part. \n\n> *6. Is there a validation set?*\n\nWe agree that it is better to use a validation set to tune the hyper-parameters. We have included a discussion on how to choose the hyper-parameters in **Appendix K**.\n\nFor the sake of simplicity, we did not use a validation set in our experiments. Instead, for single-step forecasting, we fixed $A = 3$ and $C = 4$. On the other hand, for long-range multi-step forecasting, we tested four combinations of $A$ and $C$ in each experiment, and the best results were selected. Specifically, when the prediction length is smaller than 600, we tested $A = 3,5$ and $C = 4,5$. When the prediction length is larger than 600, we tested $A = 3,5$ and $C = 5,6$. We have made this point clear in **Appendix F**.\n\nIn addition, we have also followed your suggestion and partitioned the ETTh1 dataset into a training and a validation set, and then tuned $A$ and $C$ on the validation set. As expected, the order stays the same.""}, {'title': 'Response to reviewer 17as, part3', 'comment': '> *7. Is there a relation between A, C vs the granularity of the dataset? For example, if the data is hourly, then one could build a resolution graph such that groups of 24 are constructed in the first level (24 hours in a day), then groups of 7 (7 days in a week), then 4 weeks in a month. Such an irregular pyramidal structure might be very useful.*\n\nWe fully acknowledge that setting $A$ and $C$ based on the granularity of the dataset is an interesting idea, and we try to explore the potential of the non-uniform pyramidal graph with different $C$ for each scale with a synthetic dataset in **Appendix I**.\n\nSpecifically, each time series in the synthetic dataset is a linear combination of three sine functions of different periods: 24, 168 and 720, that is, \n\n\\begin{equation}\n    f(t)= \\beta_0 + \\beta_1 \\sin(\\frac{2\\pi}{24}t) + \\beta_2 \\sin(\\frac{2\\pi}{168}t) + \\beta_3 \\sin(\\frac{2\\pi}{720}t).\n\\end{equation}\n\nIn the above equation, the coefficients of the three sine functions $\\beta_1$, $\\beta_2$, and $\\beta_3$ for each time series are uniformly sampled from $[5, 10]$. $\\beta_0$ is a Gaussian process with a covariance function $\\Sigma_{t_1, t_2} = |t_1 - t_2|^{-1}$ and $\\Sigma_{t_1} = \\Sigma_{t_2} = 1$, where $t_1$ and $t_2$ denote two arbitrary time stamps. Such polynomially decaying covariance functions are known to have long-range dependence [2]. We first generate 60 time series of length 14400, and then split each time series into sliding windows of width 1440 with a stride of 24. In our experiments, we use the historical 720 time points to predict the future 720 points. With this long-range correlated noise and sine functions of different periods, the model needs to be able to capture multi-resolution temporal dependencies to accurately predict the next 720 points. The results are summarized below. The subscripts of Pyraformer correspond to the $C$ in each scale from bottom to top.\n\n| Method         |    MSE   |   MAE   |\n| ----------------- | ---------- | ---------- |\n| Full attention|    3.550  |   1.477  |\n| LogTrans     |   3.007   |   1.366  |\n| ETC             |   4.742   |   5.509  |\n| Informer       |    7.546  |    2.092 |\n| Longformer  |    2.032 | 1.116 |\n| Reformer     | 1.538 | 3.069 |\n| Pyraformer-[6,6,6]   | 1.258 | 0.877 |\n| Pyraformer-[24,7,4] | 1.776 | 1.033 |\n| Pyraformer-[12,7,4] | 1.176 | 0.849 |\n\nIt can be observed that under the regular pyramidal structure (choose $C=6$ for all scales), Pyraformer already outperforms other methods by a large margin. Moreover, we set different $C$ for different scales, and find that Pyraformer-[12,7,4] performs even better than that of Pyraformer-[6, 6, 6]. In this setting, nodes at scale 2, 3 and 4 represents half a day, half a week, and half a month, respectively, and the pyramidal attention successfully utilizes prior period knowledge to further improve the prediction accuracy. On the other hand, we find that the performance of Pyraformer-[24,7,4] is not as good as Pyraformer-[6, 6, 6}], probably because the convolution layer with a kernel size of 24 is difficult to train. \n\nWe have also added visualizations of multi-resolution features in **Appendix I**. It can be observed that the coarser-scale features are approximately a summary of the finer-scale features.\n\n> *8. Generally, it would be better if we could rank pyramidal attention vs state of the art models on the well known benchmarks used in the DCRNN paper and newer papers like https://arxiv.org/abs/2103.07719. For instance one could just do the evaluation of pyramidal attention in the same tasks as the linked paper and report the numbers.*\n\nWe have checked DCRNN and the paper in the link carefully, and noticed that both of them focus on capturing the interactions between different time series under the scenario of multivariate time series forecasting. Moreover, the historical length in this two models is typically very short. As a result, we think it is difficult to apply Pyraformer in this scenario, since the proposed model typically requires a long history to extract the long-range dependencies and is devoted to modeling the complicated temporal dependencies within a time series instead of the spatial dependencies between different time series. Due to the limited discussion time, we decide not to explore the potential use of the pyramidal attention in this direction.\n\n### Reference\n\n[1] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. ""Informer: Beyond efficient transformer for long sequence time-series forecasting."" In Proceedings of AAAI, 2021.\n\n[2] Choi, Myung Jin, Venkat Chandrasekaran, and Alan S. Willsky. ""Exploiting sparse Markov and co-variance structure in multiresolution models."" In Proceedings of the 26th Annual International Conference on Machine Learning.'}, {'title': 'Response to reviewer WMVh, part1', 'comment': ""Thanks very much for your detailed review and constructive feedback. We believe that addressing the reviewer's comments has resulted in improving the clarity and presentation of the paper’s contributions and has brought the paper to a higher standard. Below we address your points individually. Corresponding modifications in the paper are **highlighted in blue**. Unless otherwise stated in our response, all pages, equations, sections and bibliographical references refer to those in the revised paper.\n\n> *Some key architectural details can be clarified further for full reproducibility and analysis.*\n\nThanks for your constructive suggestion. We have answered your questions about reproducibility below. We have also added more experimental details in the **Appendix F**.\n\n> *1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)?*\n\nRecall that we have two prediction modules in the paper. Depending on the prediction module we use, the input to the CSCM is different. \n\nWhen using the first prediction module (i.e., the fully connected layer), we add an ''end token'' to the end of the historical observation sequence $z_{t-L+1:t}$. The value of the end token is 0 and the corresponding covariate is $x_{t+1}$. Thus, the embedding of $x_{t-L+1:t+1}$ is added with the observation embedding of $[z_{t-L+1:t}, 0]$ and positional embedding, and the resulting embedding with length $L+1$ is fed into the CSCM.\n\nWhen using the second prediction module (i.e., the attention-based decoder), the covariate sequence is first split into two parts: $x_{t-L+1:t}$ and $x_{t+1:t+M}$. The embedding of $x_{t-L+1:t}$ is added with the associated observation embedding and positional embedding, and the resulting embedding with length $L$ is fed into the CSCM. The remaining part of the covariate embedding $x_{t+1:t+M}$ is added with the corresponding positional embedding to construct the ''prediction tokens''. The prediction tokens are then fed into the decoder.\n\nWe have added definitions of the end token and the prediction tokens in **Section 3.3**.\n\n> *2. Can each node attend to its own lower-level representation?*\n\nYes, it can. In the definition of $A_l^{(s)}$ in Equation (2), we can see that each node can attend to the nodes that have a distance less than $(A-1)/2$ with it on the same scale. The distance between each node and itself is 0, so each node can attend to its own lower-level representation.\n\nWe have explicitly explain this point in **the second paragraph on Page 5** as:\n\n''each node in the graph can attend to a set of neighboring nodes $\\mathbb N_\\ell^{(s)}$ at three scales: the adjacent $A$ nodes at the same scale **including the node itself** (denoted as $\\mathbb A_\\ell^{(s)}$) ...''""}, {'title': 'Response to reviewer WMVh, part2', 'comment': '> *3. Do the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L?*\n\nWe have added a discussion on hyper-parameter selection in **Appendix K**:\n\n\'\'We recommend to first determine the number of attention layers $N$ based on the available computing resources, as this number is directly related to the model size. Next, the number of scales $S$ can be determined by the granularity of the time series. For example, for hourly observations, we typically assume that it may also have daily, weekly and monthly periods. Therefore, we can set $S$ to be 4. We then focus on the selection of $A$ and $C$. According to the ablation study, we typically prefer a small $A$, such as 3 and 5. Lastly, in order to ensure the network has a receptive field of $L$, we can select a $C$ that satisfies Equation (5). In practice, we can use a validation set to choose $C$ from its candidates that satisfies (5). It is also worthwhile to check whether choosing different $C$ for different scales based on the granularity of the time series can further improve the performance as we did in Appendix I.\'\'\n\n> *It would be good to evaluate the base performance without the PAM to determine the value added by attention.*\n\nFollowing your advice, we have added another ablation study in **Appendix J** to show the importance of the PAM and also checked the performance of Wavenet on the ETTm1 dataset. The experimental results are summarized as follows:\n\n| Method | Metrics | 96 | 288 | 672 |\n| ---------- | ---------- | --- | ----- | ----- |\n| CSCM Only | MSE | 0.576 | 0.782 | 0.883 |\n| CSCM Only | MAE | 0.544 | 0.683 | 0.752 |\n| Wavenet      | MSE | 0.571 | 0.766 | 0.937 |\n| Wavenet      | MAE | 0.526 | 0.649 | 0.747 |\n| Pyraformer  | MSE | **0.480** | **0.754** | **0.857** |\n| Pyraformer  | MAE | **0.486** | **0.659** | **0.707** |\n\nFor a fair comparison, the number of parameters of the three methods were controlled to be within the same order of magnitude. In particular, we increased the bottleneck dimension of ""Conv. w/bottleneck"". We stacked 18 basic Wavenet blocks, and the resulting Wavenet has a receptive field of 190. Our experimental results show that the Pyraformer with the PAM yields much more accurate predictions than the remaining two methods, indicating the importance of the PAM. In addition, the CSCM itself performs slightly worse than Wavenet when the prediction length is 96 and 288, since the CSCM only stacks three convolution layers. However, when the prediction length is 672, the performance of CSCM exceeds that of Wavenet, since the receptive field of Wavenet is only 190 whereas that of the CSCM is 672. \n\n> *Finally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.*\n\nWe have provided more information regarding the experiment settings in the section of ablation study in **Apendix J**. **Note that the results are presented in Tables 7-9 in the updated version of this paper**.\n\nThe ablation study on the impact of $A$ and $C$ (Table 7) was done on the ETTh1 dataset with the prediction length $M = 720$. On the other hand, in Table 3, $A = 5$ and $C = 4$ was our initial setting. However, in order to better show the impact of $A$, we did not include $A=5$ in Table 7. Therefore, the results in Table 3 cannot be found in Table 7.\n\nThe ablation study on the impact of the CSCM architecture (Table 8) was also done on the ETTh1 dataset. To reduce the number of parameters, we used convolution with bottleneck in our long-range forecasting experiments. Thus, the performance of Pyraformer on ETTh1 with prediction length 168 in Table 3 is the same with the last row in Table 8.\n\nThe ablation study on the impact of history length (Table 9) was done on the ETTm1 dataset, since its granularity is minute and contains more long-range dependencies. However, we are very sorry that we misstated the prediction length in Table 9. In order to better study the impact of history length, our prediction length is 1344. Therefore, both MSE and MAE in Table 9 are larger than that of Pyraformer on ETTm1 with the prediction length 672. We have corrected this mistake in the paper.\n\nTo facilitate the reproduction of the results in the paper, we will also provide shell scripts corresponding to the experiments done in the paper when the code is made public.'}, {'title': 'Response to reviewer f9f9, part1', 'comment': ""We value your constructive and thoughtful comments. We believe that addressing the reviewer's comments has resulted in improving the clarity and presentation of the paper’s contributions and has brought the paper to a higher standard. Below we address your concerns point by point. Corresponding modifications in the paper are **highlighted in blue**. Unless otherwise stated in our response, all pages, equations, sections and bibliographical references refer to those in the revised paper.\n\n> *1. Hierarchical structure illustrated in Figure 1 (d) has been explored before in the context of LSTM/RNN.*\n\nThanks for pointing this out! We have added a brief review on related RNNs and LSTMs in the **Appendix A** and discussed their differences from Pyraformer as follows. **We further refer the readers to Appendix A in the second paragraph in Section 2.1**.\n\n''In this section, we provide a brief review on the related RNN-based models. Multiscale temporal dependencies are successfully captured in HRNN (Costa-juss`a \\& Fonollosa, 2016) and HM-RNN (Chung et al., 2019).  The former requires expert knowledge to partition the sequence into different resolutions, while the latter learns the partition automatically from the data. Note that the theoretical maximum length of the signal traversing path in both models is still $\\mathcal O(L)$.   Another line of works aim to shorten the signal traversing path by adding residual connections (Kim et al., 2017) or dilated connections to LSTMs (Chang et al., 2017). However, they do not consider the multiresolution temporal dependencies explicitly. Furthermore, all aforementioned RNNs only propagate information in one direction from the past to the future.  An appealing approach that allows bidirectional information exchange is Bi-LSTM (Schuster, 1996). The forward and backward propagation is realized through two different LSTMs though, and so still incurs a long signal traversing path.\n\nAs opposed to the above mentioned LSTM/RNN models, the proposed Pyraformer enables bidirectional information exchange that can better model the temporal dependencies, while providing a multiresolution representation of the observed sequence at the same time. We also notice that due to the unidirectional property of RNNs, it is difficult the realize the pyramidal graph in Figure 1d based on RNNs.''\n\n> *2. It would be more convincing if the evaluation could also illustrate that Pyraformer simultaneously capture temporal dependencies of different ranges in a compact multi resolution fashion.*\n\nWe completely agree that it is better if we can show that Pyraformer indeed learns multiresolution temporal dependencies. To move forward to this goal, we have depicted the extracted features for an arbitrary chosen channel in **Figure 7**. It can be observed from the figure that features at the coarser scales can be regarded as a lower resolution version of the features at the finer scales.""}, {'title': 'Response to reviewer f9f9, part2', 'comment': ""> *3. It would be better if the authors could illustrate a bit more on why these datasets can be used to evaluate long-term dependency modeling. If the real-world dataset does not include the long-term dependency in it, some studies on a synthetic dataset would help readers understand the benefit of the proposed model.*\n\nThanks a lot for your constructive suggestion! \n\nThe three datasets used for long-range forecasting in this paper, i.e., ETTh1, ETTm1 and Electricity, are also used in Informer (Zhou et.al, 2021), a recent work on long-range forecasting, to show the superiority of Informer in long-range forecasting tasks. Hence, we believe it is appropriate to use these datasets to evaluate the proposed Pyraformer. \n\nOn the other hand, we agree that long-term dependency modeling is a bit different from long-term prediction. However, we would like to argue that capturing long-term dependency helps to yield accurate long-term forecasting results. \n\nIn addition, all benchmark methods implemented in this paper obtain multi-step long-range prediction results together in a single forward operation instead of in an autoregressive step-by-step manner. Therefore, there is no error accumulation over time. We can safely assume that the prediction error mainly arises from the lack of long-term temporal dependency modeling.\n\nSince it is difficult to tell whether long-term dependency exists in real-world datasets, we follow your suggestion and generate a synthetic dataset with long-range dependencies. Detailed synthesis formula and experimental results are shown in **Appendix I**. Here we briefly illustrate how we generate the synthetic dataset and show the main experimental results.\n\nSpecifically, each time series in the synthetic dataset is a linear combination of three sine functions of different periods: 24, 168 and 720, that is, \n\n\\begin{equation}\n    f(t)= \\beta_0 + \\beta_1 \\sin(\\frac{2\\pi}{24}t) + \\beta_2 \\sin(\\frac{2\\pi}{168}t) + \\beta_3 \\sin(\\frac{2\\pi}{720}t).\n\\end{equation}\n\nIn the above equation, the coefficients of the three sine functions $\\beta_1$, $\\beta_2$, and $\\beta_3$ for each time series are uniformly sampled from $[5, 10]$. $\\beta_0$ is a Gaussian process with a covariance function $\\Sigma_{t_1, t_2} = |t_1 - t_2|^{-1}$ and $\\Sigma_{t_1} = \\Sigma_{t_2} = 1$, where $t_1$ and $t_2$ denote two arbitrary time stamps. Such polynomially decaying covariance functions are known to have long-range dependence. In our experiments, we use the historical 720 time points to predict the future 720 points. **Since both the deterministic and stochastic parts of the synthetic time series have long-range correlations, such dependencies should be well captured in the model in order to yield accurate predictions of the next 720 points.** The results are summarized below.\n\n| Method         |    MSE   |   MAE   |\n| ----------------- | ---------- | ---------- |\n| Full attention|    3.550  |   1.477  |\n| LogTrans     |   3.007   |   1.366  |\n| ETC             |   4.742   |   5.509  |\n| Informer       |    7.546  |    2.092 |\n| Longformer  |    2.032 | 1.116 |\n| Reformer     | 1.538 | 3.069 |\n| Pyraformer   | 1.258 | 0.877 |\n| Pyraformer-period | 1.176 | 0.849 |\n\nIt can be observed that Pyraformer outperforms the benchmark methods by a large margin. In particular, the MSE given by Pyraformer is decreased by **18.2%** compared with Reformer, which produces the smallest MSE among the existing variants of Transformer. The performance of Pyraformer-period will be discussed in the next reply.\n\n> *4. The factor to construct the hierarchy.*\n\nWe did not use a binary tree in our experiments. The binary tree in Figure 1 is schematic only. In **Section 3.1**, we denote the number of children for each node in the coarser scales as $C$ and further provide some guidelines to choose $C$ in Proposition 2 in order to obtain a short signal traversing path. In practice, we can choose $C$ using two strategies. First, we can regard $C$ as a hyper parameter, and determine its value from the candidates that satisfy Equation (5) using a validation set. Second, we can choose $C$ based on the granularity of the observations. For instance, for hourly observations, we typically assume that it may also have daily, weekly and monthly periods. As a result, we can set $C = 12, 7, 4$ sequentially for each scale from bottom to top, and so the nodes in coarser scales represent half a day, half a week, and half a month, respectively. The result of this configuration is referred to as ''**Pyraformer-period**'' in the above table. In comparison with the Pyraformer with the same $C$ for all scales (i.e., Pyraformer in the above table), the performance of Pyraformer-period can be further improved. We have included this discussion in **Appendix I**. Besides, we also added a discussion about the selection of hyper-parameters in **Appendix K**.""}, {'title': 'Response to reviewer cdAe', 'comment': ""Many thanks for your positive review and valuable feedback!  We believe that addressing the reviewer’s comments has resulted in improving the clarity and presentation of the paper’s contributions and has brought the paper to a higher standard. Below we address your comments individually. The corresponding modifications in the paper are **highlighted in blue**. Unless otherwise stated in our response, all pages, equations, sections, and bibliographical references refer to those in the revised paper.\n\n> *1. More datasets for multi-step forecasting would be helpful in evaluating the method.*\n\nThank you for your suggestion. We further synthesized a dataset with long-range dependence and carried out experiments on it to verify the long-range modeling capability of the proposed method. We presented the details about the dataset and experimental results in **Appendix I**. Here we briefly illustrate how we generate the synthetic dataset and show the main experimental results.\n\nSpecifically, each time series in the synthetic dataset is a linear combination of three sine functions of different periods: 24, 168 and 720, that is, \n\n\\begin{equation}\n    f(t)= \\beta_0 + \\beta_1 \\sin(\\frac{2\\pi}{24}t) + \\beta_2 \\sin(\\frac{2\\pi}{168}t) + \\beta_3 \\sin(\\frac{2\\pi}{720}t).\n\\end{equation}\n\nIn the above equation, the coefficients of the three sine functions $\\beta_1$, $\\beta_2$, and $\\beta_3$ for each time series are uniformly sampled from $[5, 10]$. $\\beta_0$ is a Gaussian process with a covariance function $\\Sigma_{t_1, t_2} = |t_1 - t_2|^{-1}$ and $\\Sigma_{t_1} = \\Sigma_{t_2} = 1$, where $t_1$ and $t_2$ denote two arbitrary time stamps. Such polynomially decaying covariance functions are known to have long-range dependence. In our experiments, we use the historical 720 time points to predict the future 720 points. Since both the deterministic and stochastic parts of the synthetic time series have long-range correlations, such dependencies should be well captured in the model in order to yield accurate predictions of the next 720 points. The results are summarized below.\n\n| Method         |    MSE   |   MAE   |\n| ----------------- | ---------- | ---------- |\n| Full attention|    3.550  |   1.477  |\n| LogTrans     |   3.007   |   1.366  |\n| ETC             |   4.742   |   5.509  |\n| Informer       |    7.546  |    2.092 |\n| Longformer  |    2.032 | 1.116 |\n| Reformer     | 1.538 | 3.069 |\n| Pyraformer   | 1.258 | 0.877 |\n\nIt can be observed that Pyraformer outperforms the benchmark methods by a large margin. In particular, the MSE given by Pyraformer is decreased by **18.2%** compared with Reformer, which produces the smallest MSE among the existing variants of Transformer. For more details about the synthetic dataset and visualization results, please refer to **Appendix I**.\n\n> *2. The reason of using the second prediction module.*\n\nWe concatenate $\\mathbf F_e$ and $\\mathbf F_{d1}$ in the second layer instead of only using $\\mathbf F_{d1}$ since we believe that the historical information $\\mathbf F_e$ is essential for accurate long-range forecasting.\n\nWe have added more justifications for the second prediction module in **Section 3.3** as follows:\n\n''The second one, on the other hand, resorts to a decoder with two full attention layers. Specifically, **similar to the original Transformer (Vaswani et al., 2017)**, we replace the observations at the future $M$ time steps with 0, embed them in the same manner with the historical observations, and refer to the summation of the observation, covariate, and positional embedding as the ''prediction token'' $\\mathbf{F_p}$. The first attention layer then takes the prediction tokens $\\mathbf{F_p}$ as the query and the output of the encoder $\\mathbf{F_e}$ (i.e., all nodes in the PAM) as the key and the value, and yields $\\mathbf F_{d1}$. The second layer takes $\\mathbf{F_{d1}}$ as the query, but takes the concatenated $\\mathbf{F_{d1}}$ and $\\mathbf F_e$ as the key and the value. **The historical information $\\mathbf F_e$ is fed directly into both attention layers, since such information is vital for accurate long-range forecasting.** The final prediction is then obtained through a fully connected layer across the dimension of channels. **Again, we output all future predictions together to avoid the problem of error accumulation in the autoregressive decoder of Transformer.**''\n""}, {'summary_of_the_paper': 'This paper presents a new hierarchical transformer architecture with constant connection path length and linear time and space complexity for long-range time series modeling. The module at core is a pyramidal attention network that makes multi-resolution representations in a tree structure and perform attention operations on the tree. A stack of convolutions is used to initialize the pyramidal tree. Experiments show the proposed method is able to make more accurate predictions with significantly fewer attention operations and, as a result, less time and memory expenses.', 'main_review': '## Strengths\n1. The paper is well-written and well-motivated with sufficient technical details\n2. The extensive empirical results demonstrate the effectiveness and efficiency of the proposed method\n3. A proof is provided to guarantee the linear complexity of long sequence encoding\n\n## Comments\n1. More datasets for multi-step forecasting would be helpful in evaluating the method.\n2. The reason of using the second prediction module in multi-step forecasting need more justification, e.g. why take the concatenation as key/value in the second layer.', 'summary_of_the_review': 'Overall I find this paper quite interesting with great potential contribution to the community, and recommend an accept.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper proposes Pyraformer, a low-complexity pyramidal attention model for long-range time-series modelling and forecasting. The proposed architecture is build upon a pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighbouring connections model the temporal dependencies of different ranges. The proposed framework harnesses the benefit of both transformer and RNN. Evaluation on Electricity, wind, App Flow, and ETT dataset against baseline models including Informer., LogTrans, Longformer, Reformer, ETC shows superior performance.', 'main_review': ""Strength:\n1. The paper is well written and the proposed architecture is easy to understand. I believe the equations are correct, and the included model illustrations makes the model design very intuitive.\n2. Extensive evaluation is provided, and the proposed model consistently outperforms baseline models.\n3. A ablation study is provided to justify the effectiveness of each component in the proposed architecture.\n\nWeakness:\n1. Although I couldn't find any specific publication, I believe hierarchical structure illustrated in Figure 1 (d) has been explored before in the context of LSTM/RNN. In addition, another line of work trying to apply the residual connections from ResNet to RNNs (e.g., https://arxiv.org/pdf/1701.03360.pdf) also provides a means for long-range time-series modelling. \n2. The paper states that Pyraformer simultaneously capture temporal dependencies of different ranges in a compact multi resolution fashion. This is intuitively understandable. However, it would be more convincing if the evaluation could also illustrate this perspective. To be more specific, how do we know from the current evaluation setting that the model indeed learns multi resolution temporal dependencies?\n3. Connected to comment 2, it would be better if the authors could illustrate a bit more on why these datasets can be used to evaluate long-term dependency modelling. In my understanding, long-term dependency modelling could be a bit different from long-term prediction. The former focuses on extract meaningful information from long-term previous steps, and the latter focuses on accurate prediction into the far future. If the real-world dataset does not include the long-term dependency in it, some study on a synthetic dataset would help readers understand the benefit of the proposed model. In other words, inaccuracy in long-term prediction could be resulted from (1) accumulated noise in predictions along time, and (2) lack of long-term temporal dependency modelling. How does the authors distinguish the result from these two perspectives? \n4. The proposed method uses a factor of 2 to construct the hierarchy. This number seems a bit arbitrary. I'm wondering if this is a design choice with some thoughts behind it, or could this number be treated as a hyper parameter? Basically this number will decide the number of hierarchies in the structure."", 'summary_of_the_review': ""Overall it is a solid paper. Please refer to the above session for the detailed discussion. I had a few concerns regarding evaluation setup and novelty. If the authors could reply accordingly, I'm willing to reevaluate the paper."", 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The authors propose a new architecture to tackle the problem of long sequence temporal forecasting (LSTF) – which looks at capturing long-range dependencies in time-series data by providing more direct paths between the output and distant history. \n\nThe Pyraformer takes an interesting spin on the current state-of-the-art sparse transformers, consisting of 2 main components:\n1.\tA dilated CNN encoder to learn coarse-scale representations at multiple resolutions.\n2.\tA decoder with a pyramid structure that applies attentions masks to a limited subset of nearest neighbours (i.e. parents, adjacent nodes and children) – effectively sparsifying fully connected attention patterns by imposing a prior structure onto attention patterns.\n', 'main_review': 'Strengths\n---\n1.\tThe architecture is well-motivated, tackles the important problem of LSTF, and improves both forecasting performance and computational efficiency of state-of-the-art baselines.\n2.\tPaper is well written and easy to follow – making motivations and contributions clear.\n\nWeaknesses\n---\nHowever, some key architectural details can be clarified further for full reproducibility and analysis. Specifically:\n1.\tHow are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed.\n2.\tCan each node attend to its own lower-level representation? From equation 2, it seems to be that only neighbouring nodes are attended to, based on the description of N_l^(s).\n3.\tDo the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L? \n\nIn addition, while the ablation analysis tests the impact of changing CSCM architectures, it would be good to evaluate the base performance without the PAM to determine the value added by attention. This would also provide a simple comparison vs dilated CNNs which have been used successfully in time series forecasting applications  (e.g. WaveNet). \n\nFinally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.\n', 'summary_of_the_review': 'The paper makes a strong contribution to long sequence temporal forecasting, although there are some aspects that need to be verified before it is ready for publication.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes a variant of the transformer architecture called pyramidal attention. In this architecture one forms a pyramid graph over the sequence at different resolutions and then applies attention among the neighbors of that graph. This leads to a network that can attend to long sequences of length L with O(L) computation but at the same time attention path between two sequence positions is O(1). This is especially relevant for time-series forecasting as it has potential to summarize the time-series at different scales like hourly, daily, monthly etc. Some experiments are performed on 4 datasets to show the advantage of this architecture over other transformer variants.', 'main_review': '__Pros__\n\n1. Overall I like the idea conceptually. As mentioned in the summary it makes a lot of sense for time-series data as it has an implicit bias of summarizing the past at different resolutions. \n\n2. This architecture as claimed, can handle long sequences with computational complexity O(L) where $L$ is the number of elements in the sequence. It can also maintain short attention path distance between any node. This distance is O(1) w.r.t L. \n\n3. The experiments are good enough to demonstrate the advantage of this network over other attention mechanisms.\n\n4. The implementation has been done efficiently using TVM and when released the code base would be of value. I hope the paper is reproducible even though no code has been provided.\n\n5. The empirical speed and memory consumption graphs are useful.\n\n__Cons__\n\nI have certain clarifying questions. i would be happy to raise my scores if these are answered. Further I have some comments that would hopefully make the paper better.\n\n1. It would be better to formally define Signal Traversing Path and forward reference it from the introduction.\n\n2. While reading the initial description in the introduction, it is not fully clear what the nodes at each resolution represent. This is only evident to me after reading section 3.2 which states that convolutional layers with specific strides are applied repeatedly to get the coarser levels. It would be better if one states upfront that some external mechanism is used to extract the initial states of the coarser nodes. Related comment: I think the clarity of section 3,2 can be improved.\n\n3. I might be wrong but I believe N is not defined in section 3.1 before it is used. Please check this (ignore if I missed this somehow).What is the relation between N and S ? I would suggest defining A, L, C, N, S clearly in one place with some indentation in Section 3.1, for ease of reading.\n\n4. Please be slightly more clear in the second paragraph of section 3. 3. I would like to verify my understanding of the multi-step forecasting modules. In the first module, we just have a fixed output layer which is a dense mapping to F outputs, where F is the future prediction length. In the second mapping, there is a decoder that sequentially generated the multi-step output and the decoder is equivalent to the decoder in the original transformer paper. Is my understanding correct?\n\n5. In the experiments the information of the datasets and the experimental setup is not 100% clear to me even after reading the appendix. Please clarify or point me to the relevant places in the paper. In multi-step forecasting what is the length of the future prediction window. Also, is the task rolling prediction over the test set? These should be clearly defined. \n\n6. is there a validation set? Typically I would prefer if A and C are tuned on a validation set per dataset and the best results are reported. the ordering would stay the same I guess, because fixed set of A, C are used in all experiments.\n\n7. Is there a relation between A, C vs the granularity of the dataset. For example if the data is hourly, then one could build a resolution graph such that groups of 24 are constructed in the first level (24 hours in a day), then groups of 7 (7 days in a week), then 4 weeks in a month. Such a non-uniform pyramidal structure might be very useful. Can the authors comment on this or better yet try this? -- not absolutely essential but it would be interesting.\n\n8. Generally, it would be better if we could rank pyramidal attention vs state of the art models on the well known benchmarks used in the DCRNN paper and newer papers like https://arxiv.org/abs/2103.07719. For instance one could just do the evaluation of pyramidal attention in the same tasks as the linked paper and report the numbers. This would reveal the rank of pyramidal attention in the SOTA table and would be a useful signal for the community. Again this is not required but a nice to have.\n\n\n', 'summary_of_the_review': 'I like the overall idea and the execution is decent (please see the pros and cons above). In my mind the pros outweigh the cons and I am currently rating it as above the acceptance threshold. I can strengthen my reviews if the authors answer the questions asked in the cons section.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting', 'authorids': ['~Shizhan_Liu1', '~Hang_Yu1', '~Cong_Liao1', '~Jianguo_Li2', '~Weiyao_Lin1', '~Alex_X._Liu1', 'dustdar@dsg.tuwien.ac.at'], 'authors': ['Shizhan Liu', 'Hang Yu', 'Cong Liao', 'Jianguo Li', 'Weiyao Lin', 'Alex X. Liu', 'Schahram Dustdar'], 'keywords': ['sparse attention', 'pyramidal graph', 'Transformer', 'time series forecasting', 'long-range dependence', 'multiresolution'], 'abstract': 'Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long.', 'one-sentence_summary': 'We propose a multiresolution pyramidal attention mechanism for long-range dependence modeling and time series forecasting, successfully reducing the maximum length of the signal traversing path to O(1) while achieving linear time and space complexity', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'liu|pyraformer_lowcomplexity_pyramidal_attention_for_longrange_time_series_modeling_and_forecasting', 'pdf': '/pdf/2ac159853cd001bbca6a8a12da497c8013914b31.pdf', '_bibtex': '@inproceedings{\nliu2022pyraformer,\ntitle={Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting},\nauthor={Shizhan Liu and Hang Yu and Cong Liao and Jianguo Li and Weiyao Lin and Alex X. Liu and Schahram Dustdar},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=0EXmFzUn5I}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Qing Jin', 'Jian Ren', 'Richard Zhuang', 'Sumant Hanumante', 'Zhengang Li', 'Zhiyu Chen', 'Yanzhi Wang', 'Kaiyuan Yang', 'Sergey Tulyakov']",ICLR,F8Net_ Fixed-Point 8-bit Only Multiplication for Network Quantization,https://iclr.cc/virtual/2022/oral/5944,2022," Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only ﬁxed-point 8-bit multiplication. To derive our method, we ﬁrst discuss the advantages of ﬁxed-point multiplication with different formats of ﬁxed-point numbers and study the statistical behavior of the associated ﬁxed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different ﬁxed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm—parameterized clipping activation (PACT)—and reformulate it using ﬁxed-point arithmetic. Finally, we unify the recently proposed method for quantization ﬁne-tuning and our ﬁxed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or ﬂoating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance.",Oral 3: Learning from distribution shift,https://openreview.net/pdf?id=_CfpJazzXT2,https://openreview.net/forum?id=_CfpJazzXT2,_CfpJazzXT2,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'This paper proposes an approach for 8-bit fixed point training of NNs, based on a careful analysis of quantization error in fixed-point methods. They present convincing and thorough empirical results in addition to a detailed analysis providing insights about their method. Reviews for this paper were quite split. One reviewer was a strong advocate, asserting that the paper will have substantial impact in the area, and that the authors’ approach of minimizing quantization error for fixed-point training is of substantial practical interest. Other reviewers were concerned that the proposed method was not novel enough, and that the proposed approach was not practical enough to work in realistic hardware use cases. The authors provided substantial detailed responses addressing the majority of reviewers’ concerns, and after following the discussion in detail I agree with the reviewer advocating for the paper, that the paper presents a practical, novel approach with valuable insights for the field from their analysis and results. \n\nI indicated I am certain about this decision, but I would be ok with the paper being bumped down from oral to poster.'}, {'title': 'Follow-up with Clarification on XNNPACK', 'comment': ""Dear Reviewer xQht,\n\nWe hope our provided additional information about XNNPACK in the latest response could help answer why XNNPACK is a suitable library for testing the integer quantization models. \n*XNNPACK is designed for speeding up not only for floating-point models, but also integer quantized models.* \n\nWe sincerely hope that our explanation could help alleviate the reviewer's major concern about using XNNPACK for latency analysis. \n\nBesides the latency, we also provide quantitative analysis for energy and area cost saving in our previous response. We hope that the provided hardware evidence, though not the focus of our paper, could help strengthen our work. We would really appreciate it if the reviewer would see the value of our work and the suggested experiments we run.\n\nThanks again for your time and feedback.\n\nBest,\n\nAuthors\n\n""}, {'title': 'Response to Reviewer xQht ', 'comment': 'Dear Reviewer xQht,\n\nAs mentioned by the reviewer: *If we only consider the quantization error, we have better numerical format options than the fixed point. The benefit of fixed-point numerical format is more on the hardware perspective rather than quantization error*.  This is a well-known fact, and we are also aware of that. This is the reason why we focus on accuracy instead of hardware efficiency, as it is a more challenging problem for fixed-point quantization. \n\nMeanwhile, per the reviewer’s request, we also provide hardware evidence to show the advantages of our method, including latency efficiency and energy-saving.\n\nThanks,\n\nAuthors\n'}, {'title': 'Clarification on XNNPACK and the Reference', 'comment': 'Dear Reviewer xQht\n\nWe really appreciate your timely response.\n\nOur understanding of your main concern is that we utilize XNNPACK [1] for latency comparison. We want to explain more reasons for choosing XNNPACK.\n\nFirst, we would like to kindly emphasize that **XNNPACK is an industry-level library that is highly optimized for integer quantization**. XNNPACK is built upon QNNPACK [2], and QNNPACK is a highly optimized library for low-precision, high-performance network inference on mobile.  XNNPACK can even achieve faster inference for integer quantized networks than QNNPACK. More details can be found in [3].\n\nSecond, we humbly think the comparison experiments using XNNPACK are more convincing than simply running the comparison results using other libraries, such as PyTorch, because the latency improvement can be verified on an integer-quantization optimized library.\n\nThird, we would like to kindly mention that XNNPACK is not optimized for fixed-point quantization. Still, we can observe latency improvements of our approach over the integer quantization ([4]).\n\nFinally, as for [5], the reviewer mentioned *The result mentioned in Table 2 is obtained within 1% degradation of accuracy of FP32*. However, we do not find the accuracy in Tab. 2, as mentioned by the reviewer. We are wondering how to use Tab. 2 to get the accuracy for different models, or any other information in [5] that can obtain the accuracy on ImageNet directly.\n\nThanks again for your time.\n\nBest,\n\nAuthors\n\n\n[1] https://github.com/google/XNNPACK\n\n[2] https://github.com/pytorch/QNNPACK\n\n[3] https://blog.tensorflow.org/2021/09/faster-quantized-inference-with-xnnpack.html\n\n[4] Zhao et al., ""Hawq-v3: Dyadic neural network quantization."" ICML. PMLR, 2021.\n\n[5] Judd, Patrick, et al. ""Proteus: Exploiting numerical precision variability in deep neural networks."" Proceedings of the 2016 International Conference on Supercomputing. 2016.\n'}, {'title': 'Response to Authors', 'comment': ""I appreciate the authors' responses to my comments. The paper that I mentioned are consider both hardware and accuracy. The result mentioned in Reference 5 is also mentioned accuracy ( The result mentioned in Table 2 is obtained within 1% degradation of accuracy of FP32). It is important to consider both accuracy and hardware metric when the paper highlighting removing INT32 as one of its contributions. The experiments on hardware that you mentioned are not satisfactory since you run two fixed-point quantization ( your method and [1] ) in the XNNPACk which is optimized for floating-point quantization. It is no doubt that the paper has a contribution but still, in my opinion, it needs to be completed by adding a meaningful comparison with previous works with considering both accuracy and hardware metrics. Therefore, I decide to keep my original score (5).""}, {'title': 'Response to Reviewer 7TRc', 'comment': 'Dear Reviewer 7TRc,\n\nThanks for your comments. If we only consider the quantization error, we have better numerical format options than the fixed point. The benefit of fixed-point numerical format is more on the hardware perspective rather than quantization error.  Therefore considering both hardware metrics and accuracy are important. '}, {'title': 'Follow-up with Further Response to Reviewer 1v2J', 'comment': 'Dear Reviewer 1v2J,\n\nWe would like to thank you again for your suggestions and feedback to improve our work.\n\nWe provide additional explanations to help clarify our work. Given the deadline for response is today, we would sincerely like to use this opportunity to see if our responses are sufficient and any concern remains.\n\nThanks again for your time.\n\nBest,\n\nAuthors\n'}, {'title': 'Follow-up with Further Response to Reviewer xQht ', 'comment': 'Dear Reviewer  xQht,\n\nWe would like to thank you again for your suggestions and feedback to improve our work.\n\nWe provide additional results and comparisons to help alleviate your most recent concerns. Given the deadline for response is today, we would sincerely like to use this opportunity to see if our responses are sufficient and any concern remains.\n\nThanks again for your time.\n\nBest,\n\nAuthors'}, {'title': 'This is convining', 'comment': 'I find the presented data and additional experiments convincing and, for me, they alleviate any concerns raised by reviewer xQht.'}, {'title': 'I think your perspective is too hardware oriented.', 'comment': 'While I agree partially with reviewer xQht concerns, I feel that the response misses the point of the paper. One can take a hardware perspective and a quantization error perspective. \n\nWhile the hardware perspective is very important to apply 8-bit training to computer vision networks such methods utterly fail for more complicated models like transformers. The second perspective, that this paper is taking, is that of relative quantization errors and how these can be minimized for fixed-point data types. This perspective is very important to make the first steps to approach the more difficult problems that lie ahead, such as 8-bit training of transformers. \n\nA counterpoint might be, that this paper should then either show 8-bit training results for transformers or a comprehensive analysis for all 8-bit data types (INT8). To that, I would say, that these problems are so complex that intermediate studies like presented here are necessary to gather the proper insights to be able to tackle such problems. It is unreasonable to ask for a more extensive comparison when the comparison presented in the paper is already extensive.\n\nI learned a lot from this paper and I will apply its insights directly to my research. While some insights are found also in some of the papers that you mentioned, I have never seen a such clearly presented perspective on relative quantization errors for fixed-point data types.'}, {'title': 'Further Response to Reviewer 1v2J', 'comment': 'Dear Reviewer 1v2J,\n\nThank you for your feedback.\n\nFirst of all, **our models are fully quantized neural networks.**  There is no floating operation required in our models during inference time. We can directly deploy our quantized networks on integer accelerators. For example, we test MobileNet V1 and V2 on CPU with only integer operation involved, and we can get top-1 accuracy of 72.8% and 72.6%, which are the same as the one we get from GPU. *We humbly think that how to implement floating-point operations on integer-only hardware is not related to our work.* Also, the implementation of floating-point operations on integer-only hardware might not fully utilize the integer accelerators.\n\nSecond, as mentioned by the reviewer, [a2] shows fully quantized models. We further reduce the computation from [a2] by eliminating the 32-bit multiplication (Fig. 1 (c) *vs.* Fig. 1 (d)). We conduct an extensive comparison with [a2] on ImageNet for ResNet 18 / 50 (shown in Tab. 2 of our paper) and demonstrate better results. For example, the performance of [a2] on ResNet 50 using layer-wise quantization is 76.7%, while ours is 78.1%.\n\nThird, we provide the table (Table A in our response) to show the better latency efficiency of our method (8-bit multiplication only fixed-point quantization) compared with [a2] on hardware.\n\nFourth, our method achieves better classification accuracy than [a1] on ResNet 18. We will add the comparison and the reference paper to the revised version.\n\nWe would like to kindly note that our models are fully quantized, and do not require 32-bit multiplication as used by other integer-only quantization methods (such as [a2]). We sincerely hope that the implementation and optimization of floating-point operations on integer-only hardware (which is not related to our work) will not affect the reviewer for the review and rating.\n\nThanks again for your time.\n\nBest,\n\nAuthors\n\n[a1] Peng, Peng, et al. ""Fully integer-based quantization for mobile convolutional neural network inference."" Neurocomputing 432 (2021): 194-205. \n\n[a2] Yao, Zhewei, et al. ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021.\n'}, {'title': 'Post rebuttal', 'comment': 'Thanks for your detailed response. For Q1, I think there are already some papers to describe the fully-quantized DNN model (e.g., [a1], [a2]). \nI am not if F8Net can outperform these solutions. In addition, I do not think floating-point batchnorm and activation is that expensive to implement. For example, we could use table lookup to prestore the results of the nonliner functions in the buffer, so no computing hardware is required. \n \nTherefore, I decide to keep my original score (5). \n\n[a1] Peng, Peng, et al. ""Fully integer-based quantization for mobile convolutional neural network inference."" Neurocomputing 432 (2021): 194-205.\n[a2] Yao, Zhewei, et al. ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021.'}, {'title': 'Further Response to Reviewer jedd', 'comment': 'Dear Reviewer jedd,\n\nThank you for checking our responses and raising the score. We appreciate your time and efforts. It is our great pleasure to know our efforts help address your concerns. \n\nWe agree that using the standard deviation (or dynamic range) to determine the fractional length (or quantization range) for fixed-point quantization might be discussed in other literature (maybe the studies in digital communication or digital signal processing). However, based on our knowledge, we are the first to incorporate the standard deviation for choosing the fixed-point format into the training of quantized neural networks. Our method can avoid more complicated learning-based methods and achieve better results (as also mentioned by Reviewer 7TRc). \n\nThanks again for your time.\n\nBest,\n\nAuthors\n\n'}, {'title': 'Changing my score to a weak accept', 'comment': 'The authors have addressed most of my concerns and I will raise my score to a 6. I still think that the formulas to compute the fractional length are not novel. The idea of estimating the dynamic range of the tensor and using that to determine the quantization range is well known.'}, {'title': 'Further Response to Reviewer xQht', 'comment': 'Dear Reviewer xQht,\n\nWe thank the reviewer for feedback. \n\nGenerally, the focus of our work is **how to train the quantized neural network using fixed-point formats to achieve good performance** (*e.g.*, high accuracy on ImageNet), instead of focusing on providing a solution that can dramatically improve hardware efficiency. Most references provided by the reviewer are from the domain of Computer Architecture or Electronic Design Automation (EDA), which are other domains that mainly focus on how to improve hardware efficiency. Some of these references (*e.g.*, [5]) even do not provide any result on classification accuracy. We would like to kindly emphasize that these are **orthogonal research domains**, and it is not easy to provide a thorough and fair comparison between papers from different domains.\n\nMore specifically, in the following, we would like to provide more facts, explanations, and comparisons with the provided references to help understand the advantages of our approach.\n\n- First, the network performance (*e.g.*, the classification accuracy) on [2-6] is not as good as the recent works. Also, [2,3,5,6] did not report the performance of large quantized models on ImageNet. Specifically, only results on small datasets are reported in [2, 3, 6] mostly with significant performance degradation, and [6] only reports error rate of the top-5 accuracy for a shallow network (with 5 conv layers and 2 fc layers). It is hard for us to implement all their methods on ImageNet.\n\n- Second, the work that has the best performance among all the references provided by the reviewer is [1]. We conduct an extensive comparison with [1] on ImageNet for ResNet 18 / 50 (shown in Tab.2 of our paper). For example, the performance of [1] on ResNet 50 using layer-wise quantization is 76.7%, while ours is 78.1%.\n\n- Third, we conduct the experiments on hardware to show the advantages of our method over the integer-only quantization method [1] for one convolution layer. We run some simple tests using iPhone XR CPU with XNNPACK [7], which is a widely-used library. As shown in the following table, our method can achieve faster inference speed than [1] for the convolution operation. We want to kindly note that an industry-standard library to fully leverage the efficiency of our fixed-point quantization is beyond the scope of this paper.\n\n  **Table A. Latency improvement on iPhone XR CPU with XNNPACK.**\n> | Input Channels | Output Channels | Kernel Size | Groups | Height | Width | Latency Improvement (%) |\n> |:--------------:|:---------------:|:-----------:|:------:|:------:|:-----:|:-----------------------:|\n> |        3       |        8        |      3      |    1   |   160  |  160  |           6.57          |\n> |        3       |        16       |      3      |    1   |   160  |  160  |          12.34          |\n> |       480      |       480       |      3      |   480  |   10   |   10  |           6.33          |\n> |       480      |       480       |      3      |   480  |   16   |   16  |           8.36          |\n\n- Fourth, as demonstrated in Fig.7 in [8] (Page 12), latency and energy consumption for 8-bit integer multiplications are much less than 32-bit floating-point or integer multiplications. For instance, 32-bit integer multiplication consumes $15\\times$ more energy than 8-bit integer multiplication, and the area cost saving (or equivalently, roughly proportional to the number of gates required) is around $12.4\\times$.\n\nOverall, we would again thank the reviewer for providing feedback.\n\nBest,\n\nAuthors\n\n[1] Zhao et al., ""Hawq-v3: Dyadic neural network quantization."" ICML. PMLR, 2021. \n\n[2] Langroudi, Hamed F., et al. ""Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge."" arXiv preprint arXiv:1908.02386, 2019. \n\n[3] Hashemi, Soheil, et al. ""Understanding the impact of precision quantization on the accuracy and energy of neural networks."" Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017. IEEE, 2017. \n\n[4] Gysel, Philipp, et al. ""Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks."" IEEE transactions on neural networks and learning systems 29.11 (2018): 5784-5789. \n\n[5] Judd, Patrick, et al. ""Proteus: Exploiting numerical precision variability in deep neural networks."" Proceedings of the 2016 International Conference on Supercomputing. 2016. \n\n[6] Lin, Darryl, Sachin Talathi, and Sreekanth Annapureddy. ""Fixed point quantization of deep convolutional networks."" ICML. PMLR, 2016.\n\n[7] https://github.com/google/XNNPACK\n\n[8] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.'}, {'title': 'Post Rebuttal', 'comment': 'I appreciate the authors\' responses to my comments. After carefully reading your responses and other reviewers’ comments, I decide not to change the score. The response to Q4 is not satisfactory. As mentioned by other reviewers and I suggested, the author needs to compare (quantitively) their approach in terms of hardware metrics and accuracy with references [1,2,3,4,5,6] that used an 8-bit fixed point. The hardware evidence mentioned by the author is not convincing. \n\n[1] Zhao et al., ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021.\n[2] Langroudi, Hamed F., et al. ""Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge."" arXiv preprint arXiv:1908.02386, 2019\n[3] Hashemi, Soheil, et al. ""Understanding the impact of precision quantization on the accuracy and energy of neural networks."" Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017. IEEE, 2017.\n[4] Gysel, Philipp, et al. ""Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks."" IEEE transactions on neural networks and learning systems 29.11 (2018): 5784-5789.\n[5] Judd, Patrick, et al. ""Proteus: Exploiting numerical precision variability in deep neural networks."" Proceedings of the 2016 International Conference on Supercomputing. 2016.\n[6] Lin, Darryl, Sachin Talathi, and Sreekanth Annapureddy. ""Fixed point quantization of deep convolutional networks."" International conference on machine learning. PMLR, 2016.'}, {'title': 'Author Response to Reviewer xQht', 'comment': 'Dear Reviewer xQht,\n\nThank you again for your thoughtful feedback to improve our work. \n\nWe update the paper with additional results (Appendix 7.7, 7.8, 7.9 in the revision) to explain the reason for using fixed-point numbers for quantization, and that all 8-bit fraction lengths are useful. We hope our response can help further demonstrate that our approach is crucial for achieving further accelerated quantized neural networks with high performance by eliminating the unnecessary 32-bit multiplication widely-adopted in conventional integer-only quantization techniques.\n\nAs the discussion deadline is approaching, we would sincerely appreciate it if you could kindly let us know whether our response is satisfactory and has addressed your concerns. It will be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors'}, {'title': 'Author Response to Reviewer 1v2J', 'comment': 'Dear Reviewer 1v2J,\n\nThank you again for your reviewing efforts to improve our work. \n\nWe provide additional hardware evidence, including latency and energy efficiency, that helps clarify the motivation of our 8-bit fixed-point quantization. Your other questions are also answered in our response, and the suggestions are reflected in the revised paper. We hope our response can help further demonstrate that our approach is crucial for achieving further accelerated quantized neural networks with high performance by eliminating the unnecessary 32-bit multiplication widely-adopted in conventional integer-only quantization techniques.\n\nAs the discussion deadline is approaching, we would sincerely appreciate it if you could kindly let us know whether our response is satisfactory and has addressed your concerns. It will be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors'}, {'title': 'Author Response to Reviewer jedd', 'comment': 'Dear Reviewer jedd,\n\nWe appreciate your reviewing efforts to improve our work. \n\nWe follow your initial comments to provide additional results, specifically how each of our proposals improve over existing standard. For example, we conducted the suggested small experiment using the scaling method proposed by the reviewer for quantization, which could not converge even though we spent several days working on it. We also provide more hardware evidence, including latency and energy efficiency, to help clarify the motivation of our 8-bit fixed-point quantization. We hope our response can help further demonstrate that our approach is crucial for achieving further accelerated quantized neural networks with high performance by eliminating the unnecessary 32-bit multiplication widely-adopted in conventional integer-only quantization techniques.\n\nAs the discussion deadline is approaching, we would sincerely appreciate it if you could kindly let us know whether our response is satisfactory and has addressed your concerns. It will be our great pleasure if you would consider updating your review or score.\n\nBest,\n\nAuthors'}, {'title': 'Author Responses to Reviewer jedd (Part 2/2)', 'comment': '**Q4: PACT followed by quantization; the benefits gained by combining PACT with fixed-point quantization; small experiments comparing these two approaches.**\n\nWe agree that PACT is meant to be used for quantization and is a powerful quantization technique, especially for activations. This is the reason why we focus on PACT and try to formulate PACT with fixed-point quantization.\n\nWe assume that the reviewer asked the *conventional quantization method that uses PACT for activation quantization, *i.e.*, clipping the values followed by quantization*. Suppose we use PACT followed by quantization, which is the conventional approach (*e.g.*, in [1]). In that case, it requires scaling before quantization and rescaling after quantization, which introduces extra computation with high precision (*e.g.*, 32-bit) and leads to more latency and energy consumption. This belongs to simulated quantization (Figure 1 (b)). Also, implementing these with floating-point values can not fully leverage the integer accelerators. On the other hand, using fixed-point, we only need bit-shifting, as illustrated in Figure 1 (d) in the paper, to save energy and latency.\n\nTo extensively examine the advantages of fixed-point quantization over other quantization methods (including simulated and 32-bit multiplication-based integer-only quantization), we conduct some small experiments as suggested by the reviewer, and compare the two approaches from three different aspects, *i.e.*, latency efficiency, energy efficiency, and accuracy performance.\n\n- **Small experiments on latency comparison.** For small experiments, here we list the results of latency comparison on CPU between convolution with 8-bit integer multiplication and that additionally using 32-bit integer multiplication for scaling. We run some simple tests (batch size is 1) using iPhone XR CPU on some toy modules to get the hardware evidence of the efficiency of our method. We use XNNPACK [2], which is a highly optimized library. We find that 8-bit multiplication is able to accelerate the convolution by around 6~8%.\n\n  **Table A. Latency improvement on iPhone XR CPU with XNNPACK.**\n> | Input Channels | Output Channels | Kernel Size | Groups | Height | Width | Latency Improvement (%) |\n> |:--------------:|:---------------:|:-----------:|:------:|:------:|:-----:|:-----------------------:|\n> |        3       |        8        |      3      |    1   |   160  |  160  |           6.57          |\n> |        3       |        16       |      3      |    1   |   160  |  160  |          12.34          |\n> |       480      |       480       |      3      |   480  |   10   |   10  |           6.33          |\n> |       480      |       480       |      3      |   480  |   16   |   16  |           8.36          |\n\n- **Evidence for energy and area cost saving.** As demonstrated in Fig.7 in [3] (Page 12), latency and energy consumption for 8-bit integer multiplications are much less than the 32-bit floating-point or integer multiplications. For instance, the 32-bit integer multiplication consumes 15x more energy than 8-bit integer multiplication, and the area cost saving (or equivalently, roughly proportional to the number of gates required) is around 12.4x.\n\n- **Accuracy comparison.** In Tables 1 & 2, we give accuracy comparisons for PACT followed by quantization and demonstrate that our method can give better results. For example, on MobileNetV1, we get 72.8 while PACT gets 71.3, and on MobileNetV2, we get 72.7 while PACT gets 71.1.\n\nWe hope our explanation can help make this clearer.\n\n---\n\n**References**\n\n[1] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. PACT: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.\n\n[2] https://github.com/google/XNNPACK\n\n[3] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\n\n'}, {'title': 'Author Responses to Reviewer jedd (Part 1/2)', 'comment': ""**We would like to thank you for your thoughtful review and valuable feedback. We have modified the paper based on the reviewer's suggestion. In the following, we provide more details on how to use fixed-point quantization during inference (Q1), clarification of novelty (Q2), reasons for using different fixed-point formants (Q3), and the comparison between our work and PACT (Q4). We hope our response can help address the reviewer's concern.**\n\n---\n\n**Q1: Clarification on standard deviation, two-pass method for batch norm, and accelerators for fixed-point.**\n\n- **The goal of this paper.** We work on quantization aware training, instead of quantized training. Our goal is to get a quantized model for accelerated inference after deployment. Reducing the training efforts is not the goal of our work.\n\n- We agree calculating standard deviation and batch normalization running statistics requires 32-bit floating-point operations. However, those steps are only **conducted during training**, and **none of them are required for inference**. When the training is done, all fractional lengths for both weights and inputs of all layers have already been determined. We can directly run the model with 8-bit fixed-point arithmetic without any necessity to calculate the standard deviations during inference. We have clarified this important point following Eqn. 1 in Section 3.3 in the revised paper.\n\n- Special accelerators designed specifically for fixed-point are not required to run our quantized model. Compared with other integer only quantization, we remove the 32-bit multiplication without changing other quantization pipelines (Figure 1 (c) vs. Figure 1 (d)). We can directly deploy our quantized networks on integer accelerators. For example, we test MobileNet V1 and V2 on CPU with only integer operation involved, and we can get top-1 accuracy of 72.8% and 72.6%, which are the same as the one we get from GPU.\n\n---\n\n**Q2: Clarification on novelty of our approach.**\n\nAs mentioned by Reviewer 7TRc, our approach for 8-bit only multiplication with fixed-point numbers is creative, and the insights from our work will inform research on network quantization for quite some time. As shown in the final experiments in the paper, our theory can help to guide the practice, unify fixed-point quantization with PACT, and lead to state-of-the-art performance.\n\n---\n\n**Q3: Scaling tensor into [-1, 1] and mapping with fixed-point representation without integer portion, the benefits gained by using standard deviation to estimate the fractional lengths, and small experiments comparing these two approaches.**\n\nWe appreciate the reviewer’s discussion on scaling and directly using fixed-point numbers without the integer part. The method proposed by the reviewer is innovative but might have some problems as follows.\n\n- 32-bit scaling operations are still required to scale the values into [-1, 1], which are what we are trying to eliminate in our work. We are not sure how to merge them to avoid these calculations and only use fixed-point numbers without integer part (namely, without determining the format for the fixed-point numbers). \n\n- We are not sure if we understand clearly what the reviewer wants us to compare with and what small experiments to conduct, but we do the experiment as follows: scale (with 32-bit floating-point division) the weights and inputs with their maximum values to map them into [-1, 1] and [0, 1] respectively, and quantize the values directly with fractional lengths of 7 and 8, respectively (namely, we have something like +/-0.1010110 and 0.11011100), and we finetune the model. We find the ResNet50 does not converge correctly, and the top-1 accuracy is only 1%. We guess the main reason is that there are some specific requirements on the weights and activation ranges for the model to converge during training.""}, {'title': 'Author Responses to Reviewer 1v2J (Part 2/2)', 'comment': '**Q4: Handling of other types of nonlinear operations.**\n\nWe agree that more complicated nonlinear activations such as PReLU or GELU might require more attention to deal with instead of naive application of our method directly. Actually, based on our knowledge, PReLU is less frequently adopted in quantized networks. It has been used in binary models, which as described above, is a different research topic of our work. Nevertheless, if we indeed need to use it, it might be a better practice to separate that layer as an individual layer (although not as efficient as ReLU) and use 8-bit fixed-point numbers to approximate the parameter in such layers. For other more complicated functions, approximating them with polynomial function in 8-bit fixed-point arithmetic might be a good option, following the method proposed in [9], or we might be able to directly quantize the input of these activation functions with 8-bit fixed-point values, and use look-up-table to calculate it more efficiently, as we only need to store 255 different values for each of this (and totally $8\\times255$ values for all different formats for all layers).\n\n---\n\n**Q5: Meaning of previously stored activation fractional lengths.**\n\nWe appreciate the reviewer for pointing out this point. As mentioned in the paper on the last sentence of Section 3, we determine the fractional length of input for each layer from its standard deviation. Also, when we calculate the effective weight in Eqn. 6, we need the fix scaling factor from the next layer that relies on its input fractional length via Eqn. 5, which has not been updated. Thus, for such calculation during training, we use the input fractional lengths stored in the buffer, instead of the one used when quantizing the input of the next layer. We have made minor modifications in the revised paper.\n\n---\n\n**Q6: Observations 1 and 2 should be described very briefly.**\n\nWe appreciate the suggestion from the reviewer and agree that the description can be abbreviated. We include them mainly to motivate our method and to make the analysis clear to readers who are not familiar with fixed-point quantization. We rephrase the two observations in the revised paper as suggested by the reviewer.\n\n---\n\n**References**\n\n[1] Oh, Sangyun, et al. ""Automated Log-Scale Quantization for Low-Cost Deep Neural Networks."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[2] Darvish Rouhani, Bita, et al. ""Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point."" Advances in Neural Information Processing Systems 33 (2020).\n\n[3] Rastegari et al., “XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks.” European Conference on Computer Vision. 2016.\n\n[4] Liu et al., “ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions.” European Conference on Computer Vision. 2020.\n\n[5] Zhang et al., “Dynamic Binary Neural Network by Learning Channel-Wise Thresholds”, arXiv preprint arXiv: 2110.05185, 2021.\n\n[6] https://github.com/google/XNNPACK\n\n[7] Steven W Smith et al. The scientist and engineer’s guide to digital signal processing. 1997.\n\n[8] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630, 2021.\n\n[9] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer only bert quantization. arXiv preprint arXiv:2101.01321, 2021.\n\n'}, {'title': 'Author Responses to Reviewer 1v2J (Part 1/2)', 'comment': ""**We appreciate the thoughtful review and comments from the reviewer. We have modified the paper based on the reviewer's suggestion, clarified the meaning of previously stored activation fractional lengths (Q5), and simplified the description of Observations 1 & 2 (Q6). Other concerns are also addressed in the following. We hope our efforts can help further reveal the potential and significance of 8-bit multiplication only fixed-point quantization.**\n\n---\n\n**Q1: 8-bit fixed point vs other numeric formats.**\n\nWe agree that binarization and other numeric formats such as logarithm quantization [1] and block floating point [2] are also able to accelerate neural networks. However, to the authors’ best knowledge, current work of binarization that achieves comparable performance than the full-precision counterparts only binarizes on weights and activations in convolutional and fully-connected layers, but not all layers implemented with binary operation [3-5]. For example, typically they require 32-bit floating-point scaling or batch normalization layers, which are not friendly to hardware platforms that only support integer operations. Also, such a combination of binary operation and floating-point operations (scaling) are less efficient, mainly because the accumulation results are stored with 32-bit integers and thus an extra type conversion is necessary. For network compression, our work is orthogonal to these branches of research and can be combined with these methods. However, we would like to appreciate the suggestion of the reviewer and have included both works [1-2] in our revision as references.\n\n---\n\n**Q2: Hardware evidence that motivates 8-bit fixed-point format.**\n\nBetter efficiency and less energy consumption is the main motivation for our proposal to eliminate the inefficient 32-bit multiplication in deep networks and use only 8-bit multiplication.\n\n- **Evidence for better latency efficiency.** We run some simple tests using iPhone XR CPU on some toy modules to get the hardware evidence of the efficiency of our method. We use XNNPACK [6], which is a highly optimized library. We compare our method with previous integer only quantization as shown in the following (batch size is 1).\n\n  **Table A. Latency improvement on iPhone XR CPU with XNNPACK.**\n> | Input Channels | Output Channels | Kernel Size | Groups | Height | Width | Latency Improvement (%) |\n> |:--------------:|:---------------:|:-----------:|:------:|:------:|:-----:|:-----------------------:|\n> |        3       |        8        |      3      |    1   |   160  |  160  |           6.57          |\n> |        3       |        16       |      3      |    1   |   160  |  160  |          12.34          |\n> |       480      |       480       |      3      |   480  |   10   |   10  |           6.33          |\n> |       480      |       480       |      3      |   480  |   16   |   16  |           8.36          |\n\n  It can be seen that our method is able to achieve acceleration for several different settings, even under such a simple testing environment, which is not optimal for our implementation using fixed-point arithmetic.  Actually, to fully employ the advantage of our technique, it might be better to develop a thoroughly new library, which is not easy to finish immediately. As discussed in [7], DSP engineering with fixed-point arithmetic involves a large amount of tradeoff between complexity, efficiency and accuracy, making fixed-point values more promising than other implementations. An industry-level library to fully leverage the efficiency of our fixed-point quantization is beyond the scope of this paper.\n\n- **Evidence for energy and area cost saving.** As demonstrated in Fig.7 in [8] (Page 12), latency and energy consumption for 8-bit integer multiplications are much less than 32-bit floating-point or integer multiplications. For instance, 32-bit integer multiplication consumes 15x more energy than 8-bit integer multiplication, and the area cost saving (or equivalently, roughly proportional to the number of gates required) is around 12.4x.\n\n---\n\n**Q3: Justify the correctness of Equation 1 with real DNN trace.**\n\nWe want to emphasize that **we indeed use real DNN traces** in all our experiments. Actually, the motivation of using Gaussian for theoretical analysis to guide practice is that typical DNN models have weights distributed as Gaussian, due to the Central Limit Theorem. Also, Eqn. (1) is a theoretical analysis to guide practice, and our latter experiments demonstrate that such theoretical analysis leads to good results. We humbly think that using this theoretical analysis is an advantage of our method.""}, {'title': 'Author Responses to Reviewer xQht (Part 2/2)', 'comment': '**Q5: Performance comparison with different 32-bit baseline.**\n\n- We follow the practice of the most recent literature [1] to use models with the highest accuracy to perform quantization, since weak models could lead to misleading accuracy for the quantized models. Using strong baselines are specifically mentioned and suggested in [1] (Page 7, Section 4, the first paragraph, and the last two sentences).\n\n- We use the same baseline on ResNet50 as the one used in [1]. We achieve more improvement than [1], especially for performing layer-wise scaling (Ours 77.6 vs. [1]’s 77.1).\n\n- We also verify our results on ResNet18 using the same baseline model from HAWQ-V3 (top-1 accuracy of 71.5%) with layer-wise quantization, and we can get the same result (70.9%).\n\n- As suggested by the reviewer, we include the accuracy difference in the revised paper.\n\n---\n\n**Q6: Extending our methods for 4-bit quantization.**\n\nThank you for mentioning this. Extending our work to INT4 is a good direction, and we leave it as future work, mainly because INT4 is beyond the scope of our work as we focus on using INT8 multiplication only in this paper. We study INT8 multiplication only quantization mainly because INT8 is the most widely supported and optimized low-precision integer format for general purpose devices such as CPU and GPU. For INT4 quantization, we need some further extension of our method to support it, such as clipping suggested in [3]. Note that in [3] the authors also need to use INT8 for activation quantization. Using INT4 only to quantize both weights and activations still requires further in-depth study to achieve acceptable performance.\n\n---\n\n**References**\n\n[1] Zhao et al., ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021.\n\n[2] Langroudi, Hamed F., et al. ""Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge."" arXiv preprint arXiv:1908.02386, 2019.\n\n[3] Sambhav R Jain, Albert Gural, Michael Wu, and Chris H Dick. Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks. arXiv preprint arXiv:1903.08066, 2019.'}, {'title': 'Author Responses to Reviewer xQht (Part 1/2)', 'comment': '**Thanks for your valuable feedback and comments. We have reflected them in the revised paper to improve our work. In the following, we clarify more about the motivation of our approach including empirical analysis and explain differences from existing methods (Q1-Q4), provide additional experimental evidence as per reviewer’s feedback (Q5), and discuss the future direction of extending to INT4 case (Q6). We hope our response can further demonstrate the strengths of our method.**\n\n---\n\n**Q1: Dynamic range vs. standard deviation for determining fractional length.**\n\nThanks for proposing the relationship between dynamic range and standard deviation. We agree their relationship suggested by the reviewer could be a good explanation of why using standard deviation for determining the fractional length is an effective way. We can indeed try to use the dynamic range or other statistics (such as mean absolute error which we have also tried in our early exploration) to determine the fractional length.\n\nWe finally decided to use standard deviation because it is a more **robust statistics** than dynamic range and is an easily-estimated parameter for Gaussian distributed weights and pre-activations. Considering depth-wise convolution layers that contain much fewer weights and inputs, using robust statistics becomes essential as these layers might include weights or inputs with strange behavior, *e.g.*, the pre-activation values of some channels become all negative with large magnitude. Therefore, the standard deviation is more suitable and robust than the dynamic range. We have included the explanation relating the standard deviation and dynamic range in the revised paper (Appendix 7.7) to give a more intuitive description.\n\n---\n\n**Q2: Most of the standard deviations in Figure 3 are not useful and the small dynamic range of parameters explains why most of the layer parameters are using 6 to 8 bit fractions in Figure 2.**\n\n- The reason that we use 8 fractional lengths is that the 8-bit fixed-point value has 8 options. Using all the 8 options can enlarge the searching space and lead to better performance.\n\n- We would like to emphasize that the fractional lengths shown in Figure 2 **cannot** represent the fraction lengths used in other neural networks. Different fractional lengths besides 6 or 8 can be useful for other neural networks. For example, in Appendix 7.8 of the revised paper, we illustrate the fractional lengths with respect to the layers for ResNet50, and we can find that over 85% of layers have an input fractional length that is less or equal to 4.\n\n- To further verify that all the 8 fractional lengths can be useful, we run experiments by training models with fractional lengths **only from 6 to 8**. The comparison result for ResNet50 is listed below (also included in Appendix 7.9 of the revised paper). We can find that restricting the fractional length can significantly impair the accuracy of the final model.\n\n  **Table A. Analysis of the impact of the searching space for fractional length (ResNet50 on ImageNet).**\n> | Fractional Length Range | Top1 Acc (%) |\n> |:-----------------------:|:------------:|\n> |           {6, 7, 8}           |     72.4     |\n> |          {0, 1, 2, 3, 4, 5, 6, 7, 8}          |     **77.6**     |\n\n---\n\n**Q3: Motivation of using fixed-point instead of INT8.**\n\nWe are not sure if we understand the question or if the reviewer can kindly give more explanation on the point. Based on our understanding, the question is *why using fixed-point quantization instead of INT8 quantization*.\n\nTo answer the above question, we first note that our approach is indeed **INT8 quantization for neural networks**. Other INT8 quantization methods still require an additional scaling operation implemented with INT32 multiplication and bit-shifting, as illustrated in Figure 1 (c) in the paper. Our approach belongs to INT8 quantization methods for neural networks and is more advanced than others in a way that **we do not need additional INT32 multiplication** (Figure 1 (d) in the paper).\n\n---\n\n**Q4: Difference between our approach and previous work of dyadic quantization [1] and linear quantization with bit-shift [2].**\n\n- Ref. [1] employs 32-bit multiplication during dequantization (Page 3, Figure 1 in [1]), while our work does not require 32-bit multiplication (Figure 1 of our paper). Additionally, we unify PACT into fixed-point quantization without extra clipping operation.\n\n- Thank you for mentioning Ref. [2]. It studies the hardware-software co-design and uses three numeric types (namely floating, fixed, and posit numbers) to represent weights and activations, providing an extensive study on various models and datasets. The operation in the paper still requires some 32-bit multiplication, *e.g.*, the scaling in Eqn. (5). Therefore, the paper is orthogonal to our work. It is a good paper, and we have referenced it in the revision.'}, {'title': 'Author Responses to Reviewer 7TRc', 'comment': 'Thank you for identifying the significance of our work and your strong recommendation for acceptance. We appreciate your acknowledgement that our approach for 8-bit only multiplication with fixed-point numbers is creative, our results are outstanding and reliable, and the insights from our work will inform research on network quantization for quite some time. In addition, our work has the potential application in innovative computing systems, such as computing-in-memory (CIM) systems where 32-bit integer multiplication is too bulky and inefficient that cannot be affordable, yet 8-bit integer multiplication only is a promising solution.\n\nWe agree that the linear model found from the theoretical analysis of relative error and standard deviation distributions can empirically help decide the fixed-point format, instead of using learning-based approaches. Through building PACT and fixed-point quantization under one forward pass, unnecessary clip operation used in PACT can be abandoned to make the whole quantization formula much neater.\n\nAs suggested, we make some revisions on the paper to give more details on pre-estimating fractional length and clipping level sharing, and simplify the conclusion. We hope our revision can make these points clearer and would like to appreciate your constructive suggestions to improve our paper.'}, {'summary_of_the_paper': 'The paper analyzes the relationship between relative quantization errors and fixed-point formats for zero-centered normal distributions and finds a linear model which fits the best exponent length of the fixed-point data type given a standard deviation. These insights are then unified with parameterized clipping activation (PACT) to normalize incoming floating point data into the desired fixed point range. To handle the network with sole 8-bit multiplications a forward pass in floating precision is used to compute batch norm statistics for the main 8-bit forward/backward pass. Additional adjustments are made between successive layers and residual layers which rely on reusing some statistics of the previous layer.\n', 'main_review': 'Pro: \nThis is a very strong paper and I have not seen any paper of such quality in this space for some time. The analysis of relative error and normal distributions gives a strong empirical and theoretical basis for the approach used in this work and provides deep insight into what the optimal fixed-point data type in each layer would look like. This analysis alone will inform future research in this area. Usually, such data types would be learned. It is very creative to abandon this approach and instead start from the analysis. Later this approach is merged with learned quantization through unification with PACT.\n\nOverall, the results are outstanding and appear to be reliable given the extensive analysis of the approach.\n\nCon:\nSome details might not be accessible to readers that do not have an extensive background, especially pre-estimation of the fraction length and the sharing of the clipping thresholds for residual connections. It might be beneficial to trim the conclusion and add more context for these sections.\n\n\n==============================UPDATE===================================\n\nAll the other reviewers take a very hardware-oriented perspective. I feel like they are not seeing the broader picture of this work which goes much beyond its initial applications. Many insights of this work are directly applicable for mixed-precision hardware where most, but not all operations are done in some 8-bit data type. From my own experience, I know that most 8-bit training methods fail when scaled up to models with billions of parameters.  For that area, quantization precision is the most important unsolved problem. This is a very important problem since transformer models keep getting larger and larger and accelerating training through 8-bit methods can make it more feasible to train these models. This work is one of the only ones in this space that makes some headway by its excellent analysis and impressive results. Many of the insights of the paperw are immediately applicable to my research. \n\nOverall, I disagree with the perspective of the other reviewers and I still believe this is an outstanding paper. As such, I keep my score of 10.', 'summary_of_the_review': 'The approach taken in this paper is very creative and the insights gained from this paper will inform research in this space for quite some time. As such, I strongly recommend acceptance of this work. I would be happy for this work to be selected as an oral presentation.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '10: strong accept, should be highlighted at the conference', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The paper proposes a low-precision DNN inference models with 8-bit fixed point. To realize the number of fraction bits, the author uses the variance of DNN parameters and combines it with PACT approach in QAT. The new approach is evaluated in various neural networks such as MobileNet V1/V2 and ResNet18/50 on ImageNet for image classification and the result are mostly par with the state of the art approaches. ', 'main_review': 'Strengths:\n1- The proposed numerical format is evaluated in variant of benchmarks which mean the proposed approach can be deployed and generalized for different models\n2- Combining the PACT and fixed point is interesting approach.\n\nWeaknesses:\n1- The correlation between standard division and fraction bit-width is interesting but it is obvious and in my opinion it is not required. The fraction length can be selected based on dynamic range and I think the reason behind the constant in the equation 1 is the correlation between standard division and dynamic range (DR=3ST). If we closely look at the figure 3, we find most of these standard divisions ( more than 1 ) is not useful for your case study, since dynamic range of most parameters is less than one for most of the layers. This small dynamic range of parameters explains why most of the layers parameters are using 6 to 8 bit fractions in figure 2. This brings a doubt on the motivation of why using the fixed point instead of INT8 approach?\n2- The state of approach like dyadic quantization [1] and linear quantization with bit shift [2] performs DNN inference with 8-bit quantization without 32-bit INT multiplication. How is your approach different compared to these studies?\n3- The author claims the performance of his approach is better than state of the art. However, the 32-bit baseline is different and the degradation accuracy should be reported in Tables 1 and 2. For instance, in ResNet18 result the degradation of accuracy in HAWQ-V3 is less than the proposed approach. \n4- The new approach needs to compare with pervious work for 4-bit quantization. It is difficult to understand the advantages of this approach in compared to pervious work in 8-bit? \n[1] Yao, Zhewei, et al. ""Hawq-v3: Dyadic neural network quantization."" International Conference on Machine Learning. PMLR, 2021.\n[2] Langroudi, Hamed F., et al. ""Cheetah: Mixed low-precision hardware & software co-design framework for dnns on the edge."" arXiv preprint arXiv:1908.02386 (2019).', 'summary_of_the_review': 'As a reviewer mentioned, the benefits and motivation of the proposed works is not obvious and the experimental result have not shown the advantages of this approach compared to state of the art works in 8-bit integer quantization. Therefore, I recommended this manuscript is marginally rejected. ', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '5: marginally below the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper describes a novel quantization framework that involving only fix-point 8-bit multiplication for DNN execution. The paper first highlights the advantages of the fixed-point numeric format. The paper then conducts some statistical study and derive an empirical formula to relate the fraction length of the fix-point representation with the standard deviation of the value distribution. After that, the paper introduces a novel approach to determine the right format for each layer during the forward propagation of the training. The proposed solution, F8Net, has been evaluation on ImageNet using multiple DNN structures (e.g., MobileNet, ResNet). \n', 'main_review': 'strengths:\n+ The authors consider a practical issue from the perspective of DNN hardware implementation. The problem is well-defined.\n+ The paper is easy to understand. The solution is presented clearly. \n\nWeakness:\n\n- I am not convinced that 8-bit fix-point format is cheaper to implement than the other numeric formats. Quantization has been studied very extensively by the ML community, and numerous numeric formats have been applied to facilitate the implementation of the DNN. (e.g., binary quantized DNN, Logarithm quantization [a1], block floating point [a2], etc). The authors should better motivate this fix-point format by providing some hardware evidence. Without this, it is hard to convince the reviewer about the usefulness of this work.\n- Equation 1 is based on the empirical approximation with Gaussian distribution, not the real DNN trace. Given the importance of this equation, authors should better justify the correctness of this empirical approximation with real DNN trace.\n- This work assumes the ReLU is used for nonlinear operation. How to handle other types of nonlinear operations (e.g., leaky ReLU)? \n- In the paper, the author mentioned that they use previously stored activation fractional length for pre-estimating the fractional length for the following layer. What does the previously stored activation mean? Is it the activation from the last training batches?\n- Observation 1 and 2 is obvious. It is nice to have some real empirical results to demonstrate this,  but I think they should be described very briefly.\n\n[a1] Oh, Sangyun, et al. ""Automated Log-Scale Quantization for Low-Cost Deep Neural Networks."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[a2] Darvish Rouhani, Bita, et al. ""Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point."" Advances in Neural Information Processing Systems 33 (2020).', 'summary_of_the_review': 'Overall, I think this paper lacks some motivation for the fix-point format. Given the fact that quantization has been studied extensively by the previous work, I am not convinced that 8-bit fix-point can achieve a better hardware efficiency than the previous work. Some hardware results may be helpful.\n\n', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '5: marginally below the acceptance threshold', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'The authors propose a new quantization flow to train DNNs using only 8-bit fixed-point multiplications. They show that 8-bit fixed point can represent different exponent ranges based on fractional length, thus choosing the right fractional length is critical. They then empirically derive a formula to calculate the optimal fractional length for a tensor based on its standard deviation. The authors combine PACT (learnable clip threshold) with fixed-point quantization, and propose a two-pass method to handle batch norm. They can achieve a small accuracy improvement (<1%) when training from scratch or fine-tuning on ImageNet using a variety of small models (ResNet-18 and MobileNet variants), compared to other quantization-aware training methods.\n', 'main_review': ""Strengths:\n - The paper is well written and easy to understand. Technical details are clear.\n - The idea seems practical and straightforward to implement in real systems\n\nWeaknesses:\n - The paper claims that the quantized training flow uses only 8-bit fixed-point multiplications. However, calculating the fractional length requires the standard deviation, which needs to be done in float? The two-pass method for batch norm also uses float in the first pass. It's not clear if the technique is suitable on a specialized accelerator that performs only fixed-point multiplication. Instead the technique seems to target quantization-aware training (QAT) on GPUs for producing a quantized model. If this is the case, the results seem weak - the improvement over SOTA is very small, and the models tested are all small scale. The results are not enough to convince me that this is a significant contribution to QAT literature.\n - The novelty of the formulas to compute the fractional length seems weak. It's common in QAT to scale the tensor by dividing by the max value or dividing by some clip value. The scaled tensor then has range [-1, 1] and maps to a fixed-point representation with no integer portion. It's not clear what is gained by using stddev to estimate the fractional length instead. I would like to see some small experiments specifically comparing these ways to handle the dynamic range of a float tensor. In my mind they should achieve similar accuracy results.\n - It's not clear to me what combining PACT with quantization achieves. PACT is already meant to be used with quantization, and it's not clear what we gain by combining the two instead of doing PACT, followed by quantization. Again, some small experiments comparing the two would be useful.\n "", 'summary_of_the_review': ""The authors propose a method to estimate the optimal fractional length of a fixed-point format given a float tensor, as well as other minor improvements to a quantization-aware training flow such as (1) combining PACT and quantization, (2) a two-pass method to handle batch norm. However, the novelty of the proposed methods is low, and it's not clear to me what improvements they offer over existing standards. The end-to-end accuracy improvement of the authors' technique is fairly small. As a result I rate the paper marginally below acceptance. If the authors can provide experiments that show specifically how each of their proposals improve over existing standard, then I would consider raising my score.\n\nEDIT: The authors have addressed most of my concerns and I will raise my score to a 6. I still think that the formulas to compute the fractional length are not novel. The idea of estimating the dynamic range of the tensor and using that to determine the quantization range is well known."", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': 'Not applicable', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization', 'authorids': ['~Qing_Jin1', '~Jian_Ren2', 'rzhuang@snapchat.com', 'shanumante@snapchat.com', '~Zhengang_Li2', '~Zhiyu_Chen3', '~Yanzhi_Wang3', '~Kaiyuan_Yang1', '~Sergey_Tulyakov1'], 'authors': ['Qing Jin', 'Jian Ren', 'Richard Zhuang', 'Sumant Hanumante', 'Zhengang Li', 'Zhiyu Chen', 'Yanzhi Wang', 'Kaiyuan Yang', 'Sergey Tulyakov'], 'keywords': ['Neural Network Quantization', 'Fixed-Point Arithmetic'], 'abstract': 'Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only ﬁxed-point 8-bit multiplication. To derive our method, we ﬁrst discuss the advantages of ﬁxed-point multiplication with different formats of ﬁxed-point numbers and study the statistical behavior of the associated ﬁxed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different ﬁxed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm—parameterized clipping activation (PACT)—and reformulate it using ﬁxed-point arithmetic. Finally, we unify the recently proposed method for quantization ﬁne-tuning and our ﬁxed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or ﬂoating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance.', 'one-sentence_summary': 'We propose a method for neural network quantization with only 8-bit fixed-point multiplication.', 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'jin|f8net_fixedpoint_8bit_only_multiplication_for_network_quantization', 'pdf': '/pdf/aed69dd0c10990a2c4948e6d230de04c5719fb7d.pdf', 'data': '', 'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/f8net-fixed-point-8-bit-only-multiplication/code)', '_bibtex': '@inproceedings{\njin2022fnet,\ntitle={F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization},\nauthor={Qing Jin and Jian Ren and Richard Zhuang and Sumant Hanumante and Zhengang Li and Zhiyu Chen and Yanzhi Wang and Kaiyuan Yang and Sergey Tulyakov},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=_CfpJazzXT2}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Shuming Kong', 'Yanyan Shen', 'Linpeng Huang']",ICLR,Resolving Training Biases via Influence-based Data Relabeling,https://iclr.cc/virtual/2022/oral/6492,2022," The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on \emph{data resampling} have employed influence functions to identify \emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise.","Oral 4: Probablistic Models, Vision",https://openreview.net/pdf?id=EskfH0bwNVn,https://openreview.net/forum?id=EskfH0bwNVn,EskfH0bwNVn,"[{'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': ""All reviewers are very positive about this paper. The reviewer with the lowest score did independent experiments that show that the authors' method works well, and has had an extensive discussion with the authors that justifies a higher score. The paper is potentially very valuable to practitioners, since it shows how to compensate for a training set that is not representative of the test data.\n\nSuggestion from the area chair to the authors: Briefly discuss the relationship between influence scores and propensity scores, which are standard in the literature on causal modeling and on sample selection bias, as in https://jmlr.csail.mit.edu/papers/volume10/bickel09a/bickel09a.pdf for example.""}, {'title': 'Thanks for your further comments ', 'comment': 'Dear reviewer,\n\nWe appreciate the response from the reviewer. It is good to see we are now in sync about the effectiveness of RDIA.\n\nFollowing your constructive suggestion, we would like to describe [Ref1] appropriately in the main body of the paper and include the additional comparison results in the Appendix. Further, we will improve Appendix H to discuss the relation between [Ref1] and our proposed method in a better way. Basically, the DUTI algorithm in [Ref1] learns a relabeling function by bi-level optimization, which has shown to be effective in identifying mislabeled training items and improving learning according to our experiments. \n\nOnce again, thank you very much for your further comments.\n\nBest，\n\nThe authors\n'}, {'title': 'Re: Reply to Reviewer FfyB ', 'comment': 'Thank you for pointing out the bug.\nActually, the threshold 0.5 in Line 58 was the bug, while the threshold 0 in Line 83 and Line 93 were correct.\nI also implemented the suggest thresholding with alpha = 0.001.\nThen, I observed that the scores of RDIA improved (see the end of this post).\n\nAt this point, I confirmed that RDIA does better than the One-Step GD update of [Ref1].\nI will update my score according to this result, and with a strong expectation to the authors for **referring [Ref1] appropriately in the main body of the paper**.\nAs I mentioned in my review, this is not the first study considering relabeling based on the influence function-like technique.\nTo my knowledge, [Ref1] is the first work in this direction (even if [Ref1] does not describe the fact explicitly).\nAs I demonstrated in my code, the simplest version of [Ref1] with only one-step update (and without any human intervention) does good job.\n**I strongly expect the authors to pay certain respect to [Ref1] and do not underestimate their technical contribution**.\nFor example, the current writing in Appendix H seems to be not appropriate.\nEven without human intervention, DUTI [Ref1] can do the same (i.e., relabel the training bias towards better model performance) as the proposed approach (but with slightly inferior performance).\n> (2) The learning task of DUTI is to debug the training instances which may contain the wrong labels and predict the true labels. While the target of our approach is to relabel the training bias towards better model performance. In this way, only our approach could relabel the biased training samples with correct labels towards better model performance.\n\n*Test Accuracy*\n\n|     |   ERM |   One-Step GD |   RDIA |\n|----:|------:|--------------:|-------:|\n| 0   | 0.959 |         0.96  |  0.961 |\n| 0.1 | 0.96  |         0.968 |  0.962 |\n| 0.2 | 0.953 |         0.951 |  0.959 |\n| 0.3 | 0.931 |         0.944 |  0.957 |\n| 0.4 | 0.842 |         0.897 |  0.958 |\n| 0.5 | 0.563 |         0.762 |  0.958 |\n| 0.6 | 0.178 |         0.782 |  0.953 |\n| 0.7 | 0.075 |         0.875 |  0.953 |\n| 0.8 | 0.05  |         0.931 |  0.949 |\n| 0.9 | 0.037 |         0.95  |  0.94  |\n\n*Test Loss*\n\n|     |      ERM |   One-Step GD |     RDIA |\n|----:|---------:|--------------:|---------:|\n| 0   | 0.141025 |      0.161033 | 0.13819  |\n| 0.1 | 0.237151 |      0.24697  | 0.146115 |\n| 0.2 | 0.345921 |      0.347718 | 0.157343 |\n| 0.3 | 0.453018 |      0.451272 | 0.171787 |\n| 0.4 | 0.56437  |      0.557865 | 0.185989 |\n| 0.5 | 0.698697 |      0.63033  | 0.197167 |\n| 0.6 | 0.876071 |      0.590184 | 0.203124 |\n| 0.7 | 1.08732  |      0.492453 | 0.196512 |\n| 0.8 | 1.44151  |      0.362804 | 0.197389 |\n| 0.9 | 2.0108   |      0.265325 | 0.241273 |'}, {'title': 'Reply to Reviewer FfyB', 'comment': 'Thank you very much for your further detailed feedback. We would like to address your concerns below.\n\n \n\nFirst of all, we thank the reviewer again for the patient response and additional experiments. According to the provided code, we found there is a  mistake in **Line 83 and Line 93**:\n\n```\nLine 83&93: zte = (xte.dot(wnew) + bnew > 0).astype(int)\n```\n\nGenerally speaking, we treat the model prediction as the probability of the label belonging to 1. Hence, we typically do not set the label to 1 if the model prediction is only bigger than 0. We believe Line 83 and Line 93 in the provided code need to be consistent with  **Line 58**, i.e., \n\n```\nLine 58 : zte = (xte.dot(wnew) + bnew > 0.5).astype(int)\n```\n\nFor Line **89**, we use a hyperparameter $\\alpha$ to control the amount of the relabeled samples to tolerate the estimation error of influence function which may result in the wrong identification of harmful training samples (see Section 3.2). This is similar to the existing influence-based reweighting approach [3] which uses a hyperparameter to control the sampling ratio. Note that the setting of $\\alpha$ and its effect on model performance are provided in Section 3.2 and Section 5.3, respectively. For simplicity, here we set $\\alpha$ to 0.001 and have the updated Line 89 as follows:\n\n```\nLine 89 (updated): ynew[f.dot(gv) > 0.001] = 1 - ynew[f.dot(gv) > 0.001] # flip label\n```\n\nAfter changing Line 83,89 and 93, we run the code and get the result below:\n\nTest accuracy\n\n| Noise Ratio | ERM   | One-Step GD | RDIA      |\n| ----------- | ----- | ----------- | --------- |\n| 0.0         | 0.950 | 0.947       | **0.950** |\n| 0.1         | 0.929 | 0.935       | 0.947     |\n| 0.2         | 0.872 | 0.873       | 0.944     |\n| 0.3         | 0.728 | 0.745       | 0.939     |\n| 0.4         | 0.497 | 0.458       | 0.943     |\n| 0.5         | 0.354 | 0.375       | 0.941     |\n| 0.6         | 0.241 | 0.448       | 0.939     |\n| 0.7         | 0.141 | 0.652       | 0.935     |\n| 0.8         | 0.078 | 0.848       | 0.931     |\n| 0.9         | 0.053 | 0.908       | 0.922     |\n\nTest loss\n\n| Noise Ratio | ERM      | One-Step GD | RDIA    |\n| ----------- | -------- | ----------- | ------- |\n| 0.0         | 0.141025 | 0.161033    | 0.13819 |\n| 0.1         | 0.237151 | 0.246970    | 0.14611 |\n| 0.2         | 0.345921 | 0.347718    | 0.15734 |\n| 0.3         | 0.453018 | 0.451272    | 0.17178 |\n| 0.4         | 0.564370 | 0.557865    | 0.18598 |\n| 0.5         | 0.698697 | 0.630330    | 0.19716 |\n| 0.6         | 0.876071 | 0.590184    | 0.20312 |\n| 0.7         | 1.087316 | 0.492453    | 0.19651 |\n| 0.8         | 1.441506 | 0.362804    | 0.19738 |\n| 0.9         | 2.010796 | 0.265325    | 0.24127 |\n\nWe can see that RDIA consistently performs better than One-Step GD both in test loss and test accuracy. \n\n \n\nAs for the metrics used in the paper, we follow the influence-based resampling approach [3] and measure the test loss since our objective is to optimize the test loss via influence analysis. Therefore, we compare the test loss between RDIA and other influence-based approach. \n\n\n\nIn what follows, we further provide the accuracy results on four datasets with logistic regression to show that our approach can still effectively improve the accuracy. Since there are some class imbalanced datasets (i.e., diabetes), we report the test loss/test **AUC** below\n\n \n\nClean\n\n| **Dataset** | **MNIST**     | **Real-sim**   | **Diabetes**   | **Cancer**  |\n| ----------- | ------------- | -------------- | -------------- | ----------- |\n| ERM         | 0.0248/0.9921 | 0.2606/0.9883  | 0.5169/0.8679  | 0.0914/1.00 |\n| UIDS         | 0.0238/0.9922 | 0.2607/0.9884  | 0.5068/ 0.8682 | 0.0786/1.00 |\n| RDIA        | 0.0209/0.9926 | 0.2585/ 0.9875 | 0.4920/ 0.8687 | 0.0649/1.00 |\n\nNoise ratio = 0.4\n\n| **Dataset** | **MNIST**     | **Real-sim**   | **Diabetes**   | **Cancer**    |\n| ----------- | ------------- | -------------- | -------------- | ------------- |\n| ERM         | 0.5277/0.9646 | 0.5869/0.9640  | 0.6730/0.7801  | 0.5646/0.9679 |\n| UIDS        | 0.4699/0.9836 | 0.5414/0.9785  | 0.6160/ 0.7974 | 0.3739/1.00   |\n| RDIA        | 0.0434/0.9991 | 0.2714/ 0.9820 | 0.5350/ 0.8170 | 0.0871/1.00   |\n\nNoise ratio = 0.8\n\n| Dataset | MNIST         | Real-sim       | Diabetes       | Cancer        |\n| ------- | ------------- | -------------- | -------------- | ------------- |\n| ERM     | 1.611/0.0023  | 1.1772/0.0112  | 0.8254/0.1448  | 1.3866/0.0003 |\n| UIDS     | 1.148/ 0.0105 | 0.9252/0.0210  | 0.7310/ 0.5953 | 1.1203/0.009  |\n| RDIA    | 0.0406/0.9990 | 0.2672/ 0.9785 | 0.5860/ 0.8078 | 0.0826/1.00   |\n\nBased on the observations, we have found that the test loss results are consistent with test AUC results. When the noise ratio increases, RDIA outperforms the baselines by a large margin in both test loss and test AUC. \n\n \n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.\n\n \n\n\n\n'}, {'title': 'Re: More experiments ', 'comment': ""My code is as follows.\n\n```\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\n\ndef sigmoid(u):\n    return 1.0 / (1.0 + np.exp(-u))\n\ndef logloss(w, b, x, y):\n    p = sigmoid(x.dot(w) + b)\n    return - (y.dot(np.log(p)) + (1 - y).dot(np.log(1 - p))) / y.size\n\ndef logreg(x, y, lam=0.1, lr=0.01, max_itr=100):\n    num, dim = x.shape\n    xo = np.concatenate([x, np.ones((num, 1))], axis=1)\n    w = np.zeros(dim+1)\n    for itr in range(max_itr):\n        p = sigmoid(xo.dot(w))\n        g = (p - y).dot(xo) / num\n        g[:-1] = g[:-1] + lam * w[:-1]\n        w = w - lr * g\n    return w[:-1], w[-1]\n\n# experiment\nlam = 1e-2\nacc, loss = [], []\nnr = np.linspace(0, 0.9, 10)\nfor noise_rate in nr:\n    acc_n, loss_n = [], []\n    for seed in range(10):\n\n        # breast cancer data\n        # train / val / test = 300 / 169 / 100\n        x, y = datasets.load_breast_cancer(return_X_y=True, as_frame=False)\n        x, xte, y, yte = train_test_split(x, y, test_size=269, random_state=seed)\n        xval, xte, yval, yte = train_test_split(xte, yte, test_size=100, random_state=seed)\n\n        # normalize features\n        scaler = StandardScaler().fit(x)\n        x = scaler.transform(x)\n        xval = scaler.transform(xval)\n        xte = scaler.transform(xte)\n\n        # noisy label in training\n        np.random.seed(seed)\n        flip = np.random.rand(y.size) < noise_rate\n        y = np.logical_xor(y, flip).astype(int)\n\n        # fit logreg\n        w, b = logreg(x, y, max_itr=500, lam=lam)\n\n        # test accuracy / loss\n        zte = (xte.dot(w) + b > 0.5).astype(int)\n        acc_te1 = np.mean(yte == zte)\n        loss_te1 = logloss(w, b, xte, yte)\n\n        # influence function\n        num, dim = x.shape\n        xo = np.concatenate([x, np.ones((num, 1))], axis=1)\n        p = sigmoid(x.dot(w) + b)\n        H = xo.T.dot((p * (1 - p))[:, np.newaxis] * xo) / num\n        H = 0.5 * (H + H.T) + np.diag([lam] * dim + [0]) # Hessian\n        g = ((p - y)[:, np.newaxis] * xo) / num # gradient\n        f = - np.linalg.solve(H, g.T).T # influence function\n\n        # label gradient over validation loss\n        xv = np.concatenate([xval, np.ones((xval.shape[0], 1))], axis=1)\n        q = sigmoid(xval.dot(w) + b)\n        gv = (q - yval).dot(xv) / xval.shape[0] # gradient of validation loss\n        gy = f.dot(gv) * (np.log(p) - np.log(1 - p)) # label gradient\n\n        # label correction by one-step gradient descent\n        ynew = y - 1e+3 * gy # step size of GD = 1e+3\n        ynew = np.minimum(1, np.maximum(0, ynew)) # clip label\n        wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)\n\n        # test accuracy / loss after label correction\n        zte = (xte.dot(wnew) + bnew > 0).astype(int)\n        acc_te2 = np.mean(yte == zte)\n        loss_te2 = logloss(wnew, bnew, xte, yte)\n\n        # label correction by RDIA\n        ynew = y.copy()\n        ynew[f.dot(gv) > 0] = 1 - ynew[f.dot(gv) > 0] # flip label\n        wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)\n\n        # test accuracy / loss after label correction\n        zte = (xte.dot(wnew) + bnew > 0).astype(int)\n        acc_te3 = np.mean(yte == zte)\n        loss_te3 = logloss(wnew, bnew, xte, yte)\n\n        acc_n.append((acc_te1, acc_te2, acc_te3))\n        loss_n.append((loss_te1, loss_te2, loss_te3))\n    \n    acc.append(acc_n)\n    loss.append(loss_n)\nacc = np.array(acc)\nloss = np.array(loss)\n\nacc = pd.DataFrame(np.mean(acc, axis=1))\nacc.columns = ['ERM', 'One-Step GD', 'RDIA']\nacc.index = nr\nloss = pd.DataFrame(np.mean(loss, axis=1))\nloss.columns = ['ERM', 'One-Step GD', 'RDIA']\nloss.index = nr\nprint('Test Accuracy')\ndisplay(acc)\nprint('Test Loss')\ndisplay(loss)\n```""}, {'title': 'Re: More experiments', 'comment': 'I would like to thank the authors for the additional experiments.\n\nI also conducted the experiment on breast-cancer by myself (see my next post).\nI confirmed that RDIA works well.\nHowever, I observed that there can be a discrepancy between the loss and accuracy.\nWhen I looked at the results in terms of test accuracy, One-Step GD update could perform well in the low noise regime.\nBased on this observation, I wonder whether the reported improvements in loss imply improvements in accuracy as well.\n\n*Test Accuracy*\n\n|  Noise Ratio   |   ERM |   One-Step GD |   RDIA |\n|----:|------:|--------------:|-------:|\n| 0   | 0.95  |         0.96  |  0.923 |\n| 0.1 | 0.929 |         0.968 |  0.945 |\n| 0.2 | 0.872 |         0.951 |  0.947 |\n| 0.3 | 0.728 |         0.944 |  0.946 |\n| 0.4 | 0.497 |         0.897 |  0.949 |\n| 0.5 | 0.354 |         0.762 |  0.952 |\n| 0.6 | 0.241 |         0.782 |  0.948 |\n| 0.7 | 0.141 |         0.875 |  0.953 |\n| 0.8 | 0.078 |         0.931 |  0.955 |\n| 0.9 | 0.053 |         0.95  |  0.952 |\n\n*Test Loss*\n\n|  Noise Ratio   |      ERM |   One-Step GD |     RDIA |\n|----:|---------:|--------------:|---------:|\n| 0   | 0.141025 |      0.161033 | 0.239982 |\n| 0.1 | 0.237151 |      0.24697  | 0.188336 |\n| 0.2 | 0.345921 |      0.347718 | 0.172467 |\n| 0.3 | 0.453018 |      0.451272 | 0.180556 |\n| 0.4 | 0.56437  |      0.557865 | 0.186278 |\n| 0.5 | 0.698697 |      0.63033  | 0.186627 |\n| 0.6 | 0.876071 |      0.590184 | 0.181963 |\n| 0.7 | 1.08732  |      0.492453 | 0.166516 |\n| 0.8 | 1.44151  |      0.362804 | 0.163885 |\n| 0.9 | 2.0108   |      0.265325 | 0.153742 |'}, {'title': 'Experiments on larger dataset', 'comment': '\n\n| Dataset:   MNIST |  ERM   |   [1]   with b=0.1n    |   [1]   with b=0.5n    | [1]   with b= n |    RDIA    |\n| :--------------: | :----: | :--------------------: | :--------------------: | :-------------: | :--------: |\n|      Clean       | 0.0245 | $\\underline{0.0228  }$ |         0.0239         |     0.0347      | **0.0207** |\n|  20% noisy rate  | 0.2567 | $\\underline{0.1336  }$ |         0.2317         |     0.2543      | **0.0386** |\n|  40% noisy rate  | 0.5296 |         0.4135         | $\\underline{0.3877  }$ |     0.5304      | **0.0413** |\n|  60% noisy rate  | 0.9164 |         0.8860         | $\\underline{0.5985  }$ |     0.6752      | **0.0503** |\n\nIn addition to the two small datasets provided in our earlier response, the results on the larger dataset MNIST with Logistic regression are collected in the table below.\n\n \n\nFrom the results over the three datasets, we obtain the same conclusions as follows. First, the performance of [1] is greatly affected by the budget b which controls the amount of relabeled samples. In practice, it may not be desirable to ask experts to decide an appropriate value of b for every dataset. Note that the bi-level optimization is computation-intensive and hence it is time consuming to treat b as a hyperparameter to be searched. For instance, the influence function only takes 4.9 seconds to identify all the harmful samples in the MNIST while [1] takes more than 2 hours to detect and relabel noisy samples with a predefined budget b.\n\n \n\nSecond, our RDIA approach consistently outperforms [1] on all the three datasets and different noise ratios. This is because the amount of relabeled samples selected by the influence function is close to the real number of wrong labels. This is also the reason why the influence-based resampling approach Dropout [2] which simply drop all the identified harmful samples still consistently outperforms [1].\n\nThird, when the noise ratio increases, the advantage of RDIA is more significant than [1]. The main reason is that [1] aims to find the smallest change in label and restrict the magnitude of label change (Equation (9) in [1]) while our approach relabels all the harmful samples identified by the influence function towards better model performance.\n\nTo sum up, due to different objectives, [1] mainly focuses on identifying the preferred total number b of flagged bugs and hence is inappropriate to resolve all the biases in a training set which is the focus of our paper. In this regard, the chosen comparison methods are the state-of-the-art influence-based reweighting approaches [2,3,4] which upweight/downweight training instances via influence function. \n\n \n\nWe hope we have clarified the difference between our approach and [1]. We sincerely thank the reviewer again for reviewing our paper and giving further feedbacks in the rebuttal round. We are very willing to explain more if there are still any concerns.\n\n \n\n[1] Training Set Debugging Using Trusted Items, AAAI 2018.\n\n[2] Data dropout: Optimizing training data for convolutional neural networks, ICTAI 2018.\n\n[3] Less Is Better: Unweighted Data Subsampling via Influence Function, AAAI2020.\n\n[4] Optimal Subsampling with Influence Functions. NIPS 2018.'}, {'title': 'Thanks for your feedbacks ', 'comment': 'Dear reviewer,\n\nThank you very much for your further feedbacks. For your remaining concerns about the novelty of our approach, we have provided more clarifications as well as the supporting experiments. Please feel free to ask any further questions.\n\nThanks.'}, {'title': 'Reply. Discussion', 'comment': 'I would like to thank the authors for the answers to the comments and for the nice discussion. \nWhat the authors comment make sense about imbalanced datasets, and how influence functions can help at mitigate the effects usually caused by the imbalance. \nI particularly liked the intuitive explanations related to overfitting. \n\nRegarding the score and the novelty, I have decided to increase my score, since I might not have understood the full extent of their approach wrt its novelty and its comparison with the other two approaches that they built on.\n\nGood job. '}, {'title': 'More experiments', 'comment': 'We would much appreciate it if you could kindly comment on our newly added experiments below. For now, we do not have enough time to finish all the experiments. We will try our best to compare [1] and our approach on more datasets for a comprehensive comparison. \n\n \n\nHere we use the code published in the original paper (Matlab) and do the experiment on two relatively small datasets  “Breast-cancer” and “Diabetes”. Specifically, we follow our experimental settings and treat the validation set as trusted items with confidence c=100. We first use [1] to relabel the wrong labels (We use all the original hyperparameters in [1] such as the max iterations.)\n\nWe finely set the budget b which controls the amount of relabeled samples and then retrain the model (logistic regression) with the relabeled training set and report the Logloss in an additional out-of-sample test set which ensures we do not utilize any information of the test data in advance. We use n to represent the amount of training samples.\n\n \n\n| Dataset:  Cancer |  ERM   | [1]  with b=0.1n | [1]  with b=0.5n | [1]  with b= n |  RDIA  |\n| :--------------: | :----: | :--------------: | :--------------: | :------------: | :----: |\n|      Clean       | 0.0914 |      0.0885      |      0.0955      |     0.1557     | 0.0649 |\n| 20%  noisy rate  | 0.1682 |      0.0972      |      0.1393      |     0.1869     | 0.0754 |\n| 40%  noisy rate  | 0.5401 |      0.2574      |      0.1782      |     0.1849     | 0.0871 |\n| 60%  noisy rate  | 0.8839 |      0.3604      |      0.3887      |     0.2705     | 0.0898 |\n\n \n| Dataset: Diabetes |  ERM   | [1]  with b=0.1n | [1]  with b=0.5n | [1]  with b= n |  RDIA  |\n| :---------------: | :----: | :--------------: | :--------------: | :------------: | :----: |\n|       Clean       | 0.5461 |      0.5267      |      0.5701      |     0.5976     | 0.4920 |\n|  20%  noisy rate  | 0.5649 |      0.5402      |      0.5495      |     0.6223     | 0.5131 |\n|  40%  noisy rate  | 0.6355 |      0.5962      |      0.5711      |     0.6325     | 0.5663 |\n|  60%  noisy rate  | 0.7685 |      0.6721      |      0.5985      |     0.6560     | 0.5925 |\n\n \n\n\n\n\nWe have found that b highly affects the model performance. Nevertheless, our approach consistently outperforms [1] since we use influence function to only relabel the harmful samples and update their labels at one shot. Besides, [1] aims to find the smallest change in label and uses $\\gamma$ to control the magnitude of label change (Equation (9) in [1]). This would also restrict the model in handling data with high noise ratios.\n\n \n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.'}, {'title': 'Thank you very much, some clarifications', 'comment': 'Thank you very much for your further detailed feedback! It is a delight to know that we have agreed on some of the novelty of our method, e.g., Lemma, Theorem and the relabeling function. We address your remaining concerns below. \n\n \n\nPlease feel free to ask any further questions if our responses are still insufficient.\n \n\n\n# Target\n\nWhat we want to clarify is that we are not saying [1] could not relabel wrong labels for improving model performance, but that it is not easy to decide the amount of relabeled instances that can lead to better model performance. We agree with the reviewer that when adapting [1] to our problem setting, we could simply allow all labels to be changed and then measure the change in the test loss. However, the model performance in [1] will be severely affected by the predefined budget b which manually controls the amount of relabeled (detected) data. For example, if we only have 10 wrong labels in the training set but set b to be 100, then we may end up with 90 noisy samples after being relabeled in the worst situation. Following your suggestion, we have run the code published in [1] and found that there is a threshold to control the maximum number of iterations, but it is still possible that more than 10 samples are relabeled before the max iteration. The situation will be even worse when the number of wrong labels increases. For example, if we have 300 wrong labels in the training set but set b to be 100, we will leave the remaining 200 wrong labels unchanged. We believe that [1] is an excellent bug detection framework with a novel optimization scheme. However, applying [1] on relabeling all the training samples is not suitable since budget b should be finely set. We have added the comparison experiments in the experiment part to verify our claims.\n\n# Novelty\n> However, the way the influential instances are selected is essentially the same as [Ref1] because the influence function is one specific example of the implicit differentiation used in the bi-level optimization.\n\nWe agree with the reviewer that the influence function is one specific example of the implicit differentiation used in the bi-level optimization, However, we would like to clarify that our contribution is not on how to select the influential instances, but on how to relabel the identified harmful instances which can improve the model performance. As mentioned in the previous rebuttal round, how to propose an explicit relabeling function which has a theoretical guarantee on the model performance is the main technical challenge. Besides, to the best of our knowledge, we are the first one to propose the influence-based relabeling approach which could theoretically achieve the lower test loss than the existing influence-based resampling approach.\n\n \n\n> The labels of instances that are influential for the validation loss are updated to reduce the validation loss, i.e., the labels are *trained* (although the amount of relabeling is determined by the training pair and the current model).\n\n \n\nWe agree with the reviewer that the selection of harmful instances depends on validation loss and influence function in RDIA. However, we would like to illustrate that our relabeling function is not a trainable parameter, and hence it is independent with the validation loss or influence function (As we claimed in the first period of rebuttal, this is the biggest difference between our relabeling function and [1]). An important fact is that our relabeling function can be directly incorporated with other harmful data selection methods. **We have already proposed the RDIA-LS (Appendix F) that uses training loss to identify harmful samples (rather than influence function) and applies our relabeling function to relabel them, achieving good performance on deep models. In this situation, we do not even have a validation set and our relabeling function still works.** \nThe reason is that our relabeling function is not trainable, but only uses the current model’s predictions to relabel harmful samples. This contribution has also been appraised by reviewer 136B and UYUF.'}, {'title': 'Reply (3/3)', 'comment': '## Lemma1 and Theorem1 are non-trivial.\n\nI agree that the proposed specific relabeling function and its validity are nontrivial.\nHowever, the following statement is questionable to me.\n> our proposed relabeling function $\\delta$ is not a trainable parameter\nThe labels of instances that are influential for the validation loss are updated to reduce the validation loss, i.e., the labels are *trained* (although the amount of relabeling is determined by the training pair and the current model).\n\nOverall, I confirm there are a few differences from [Ref1] in the current paper.\nIn particular, degerming the amount of relabeling only by the training pair and the current model seems to be novel and interesting.\nHowever, the way the influential instances are selected is essentially the same as [Ref1] because the influence function is one specific example of the implicit differentiation used in the bi-level optimization.'}, {'title': 'Reply (2/3)', 'comment': '## Experiments\n> Since the experimental settings and metrics are totally different in two works, we could not conduct the comparison experiments. \n\nPlease remind that I am not asking to compare the results based on the metrics (detection of wrong labels) used in [Ref1].\nWhat I expect is to compare [Ref1] in the metrics (loss improvement) used in the current paper because the method of [Ref1] can be used for the same purpose as the current paper under the same problem setup.\nHow much improvements of the loss would be observed by following the suggested relabeling by [Ref1]?\n(under the setting where all the validation set are untrusted items, there are no trusted items, and no assessment by the experts)\n\n## ""our equation (10) is not identical to the above formula""\n\nLet me apologize that the equations are slightly incorrect.\nThe correct one is\n\n$$\n\\frac{dL(Q; \\hat{\\theta}\\_\\delta)}{d\\delta\\_i} = \\nabla\\_\\theta L(Q; \\hat{\\theta}\\_\\delta) \\frac{d\\hat{\\theta}\\_\\delta}{d\\delta\\_i} = -\\frac{1}{N}\\nabla\\_\\theta L(Q; \\hat{\\theta}\\_\\delta)^\\top H\\_{\\hat{\\theta}\\_\\delta}^{-1} \\frac{\\partial \\nabla_\\theta l\\_i(z_{i\\delta_i}, \\hat{\\theta}\\_\\delta)}{\\partial \\delta\\_i} .\n$$\n\nwhich will lead to\n\n$$\nL(Q; \\hat{\\theta}\\_{\\delta}) - L(Q; \\hat{\\theta}\\_0) \\approx \\epsilon \\left. \\frac{dL(Q; \\hat{\\theta}\\_\\delta)}{d\\delta\\_i} \\right|\\_{\\delta\\_i = 0} \n$$\n$$\n= \\frac{\\epsilon}{N} \\nabla\\_\\theta L(Q; \\hat{\\theta}\\_0)^\\top H\\_{\\hat{\\theta}\\_0}^{-1} \\frac{\\partial \\nabla_\\theta l\\_i(z\\_{i\\delta\\_i}, \\theta_0)}{\\partial \\delta\\_i}\n$$\n$$\n\\approx \\frac{1}{N}\\nabla\\_\\theta L(Q; \\hat{\\theta}\\_0)^\\top H\\_{\\hat{\\theta}\\_0}^{-1} \\nabla_\\theta(l\\_i(z\\_{i \\epsilon}, \\theta) - l\\_i(\\theta)) .\n$$'}, {'title': 'Reply (1/3)', 'comment': 'I would like to thank the authors for the detailed feedback.\n\n## Target\nLet me rephrase the feedbacks first.\nThe current paper and [1] are different in a sense that\n\n*1. The Objective of Relabeling*\n* [1] is interested in detecting wrong labels and correct them.\n* The current study focuses on improving validation loss.\n\n*2. Target of Relabeling*\n* [1] limits some of the instances (trusted items) to keep the original labels, and optimize the labels of the remaining instances (untrusted items) only.\n* The current study considers relabeling possibly all the instances.\n\n*3. The Output of Relabeling*\n* [1] outputs labels in the output domain, e.g., class labels.\n* The current study allows outputs outside the domain, e.g., soft labels.\n\n*4. The Usage of Relabeling*\n* The suggested relabelings by [1] are assessed by the experts and only wrong labels are corrected.\n* The current study do not require human assessments and the new label are directly used for the new training.\n\nHere, I would like to point out that the differences above are not significant when evaluating the technical novelty of the paper.\nIndeed, *1. The Objective of Relabeling*, *2. Target of Relabeling*, and *4. The Usage of Relabeling* have nothing to do with the algorithms in the current paper and  [1]. These are all up to the users. \n\nFor example, in *1. The Objective of Relabeling* and *2. Target of Relabeling*, one can treat the validation set as the untrusted item in [1], and set trusted item to null. This will result in the same objective function, i.e., simply improving the validation loss, while allowing all labels to be changed.\nThat is, the problem setup of [1] includes the problem of the current paper as its special case.\n\n*4. The Usage of Relabeling* is also completely independent matters from the algorithms.\nThe users have a choice of not assessing the suggested relabelings, and directly using them for the new trainig.\n\n## Relabeling Function and Optimization\n* [1] uses bi-level optimization.\n* The current paper uses the influence function.\n\n[Ref1] used implicit differentiation for bi-level optimization.\nHere, I would like to remind that implicit differentiation includes influence function as its special case.\nSee e.g.,  [Ref2], for the detail.\n* [Ref2] Optimizing Millions of Hyperparameters by Implicit Differentiation, AISTATS 202\n\nMoreover, I think the statement below seems to be too strong.\n> we propose an explicit relabeling function which has nothing to do with gradient, validation loss or even influence function\n\nThe selection of relabeled instances depend on validation loss and influence function.\nIn this regard, the entire relabeling procedure depends on the gradient (i.e., influence function on the validation set as a special case of implicit differentiation).\n\nI do agree that the way the amount of relabeling is determined is different and would be a novelty of the paper. \n\n## Theoretical Guarantee\n* [1] does not provide any theory.\n* The current paper provide the theory on reduction of validation loss.\n\nI confirm this is true.\nI also agree that proving the reduction of the validation loss for the specific relabeling function under consideration would be a novelty of the paper.\nHowever, I think it would not appropriate to underestimate the contribution of [Ref1] just because they did not make the trivial claim.\nWe will not say explicitly in the paper that ""the loss reduces because we use gradient descent"".\nThis is a trivial fact in the field (as long as the step size is sufficiently small).'}, {'title': 'Response to Reviewer XS8o', 'comment': ""Thank you for the positive review and encouraging feedback! We reply individually to each raised point below\n\n \n\n> I would suggest to either clarify that at the beginning, or simply use validation set instead of test set.\n\n \n\nThank you for pointing out this issue. We have revised the penultimate paragraph of the Introduction to clarify that we actually use influence analysis on the validation set rather than on the test set.\n\n \n\n> Algorithm for RDIA such as done for RDIA-LS (Algorithm 1) in the Appendix E. I believe this will help portray more details for further reproducibility.\n\n \n\nThank you for your suggestion. We have added the algorithm for RDIA in Appendix A and kindly referred to that in the first paragraph of methodology.\n\n \n\n> Regarding reproducibility, a link with the source code would be beneficial for the same reasons as the point above.\n\n \n\nSorry for the confusion. We uploaded the code repository in the Supplementary Material for reproducibility. We promise to include the URL link to our source code in the paper upon acceptance. \n\n \n\n> Imagine we have an imbalanced dataset, and that the algorithm is not able to classify well that portion of the dataset with fewer examples. The problem in this case is that we don't have enough data of some particular group of instances, needing some sort of solution like data augmentation (for example).\n\n \n\nVery inspiring comment and we are happy to discuss the potential benefits of applying our approach on imbalanced dataset. In fact, class imbalance is a typical distribution mismatch problem if the test set has balanced classes ($P_{train}$  is different from  $P_{test}$). According to [1] and [2], influence function is effective to identify the training examples that are responsible for the distribution mismatch. Existing work [3] thus has used influence function to solve the problem of class imbalance. It is important to notice that, if we use influence function to identify harmful samples over imbalanced training set, **most of the identified harmful samples lie in the majority class** which is in line with the analyze in [1] and the experiment results in [3]. In this way, influence-based reweighting approach (e.g. [3]) could improve the model’s predictive performance to the minority class by down-weighting the samples in the majority class. Since we have proved that our approach could achieve lower test loss than the reweighting approach, we think our approach could also solve the class imbalance problem to a certain extent. We provide some intuitive explanations here. Our influence-based relabeling approach acts like adding a few “noises” in the majority class and tells the model to forget the specific pattern of the data corresponding to the majority class, which avoids overfitting to the majority class. This could also mitigate the overfitting of the decision boundary arising from the influential majority samples and thus improve the model generalization ability on the minority class. This idea is like some previous approaches [4],[5] that add noises or penalize the over-confident model predictions to avoid overfitting to the specific pattern of the data and then improve the generalization performance of the model.\n\n \n\n> The reason I don't give a higher score is because I believe that the methodology does not seem to be too novel and the results are marginally better, except for one or two datasets.\n\nPrevious influence-based approaches mainly focus on perturbing the loss term of the harmful samples (i.e. downweight all the harmful samples). **To the best of our knowledge, we are the first to perform fine-grained perturbation on the harmful training sample’s label and evaluate its influence on model’s performance using the influence function.** Furthermore, developing an influence-based relabeling function that theoretically guarantees to improve model’s performance and is applicable for any classification tasks is a non-trivial task. We also empirically show that our relabeling approach could achieve lower test loss than various existing influence-based resampling approaches. **A promising outcome of our approach is that when the noise rates increase (most of training samples are harmful), our approach could outperform other baselines by a large margin (Figure 2).** We believe that our work can inspire new research ideas to explore novel relabeling approaches for solving training bias.\n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.\n\n \n\n Reference\n\n[1] Understanding Black-box Predictions via Influence Functions. ICML 2017 \n\n[2] Less Is Better: Unweighted Data Subsampling via Influence Function. AAAI 2020\n\n[3] Influence-Balanced Loss for Imbalanced Visual Classification. ICCV 2021\n\n[4] DisturbLabel: Regularizing CNN on the Loss Layer. CVPR 2016\n\n[5] Regularizing Neural Networks by Penalizing Confident Output Distributions. ICLR2017""}, {'title': 'Response to Reviewer UYUF', 'comment': 'Thank you for the positive review and encouraging feedback!\n\nIn the appendix, we discussed the limitation of RDIA on deep models due to the erroneous influence estimations. As shown in the following table, we conduct the experiment on CIFAR10 dataset with a CNN (6 convolutional layers followed by 2 fully connected layers) and find the improvement of using RDIA to solve the label noise on deep model is limited. Hence, we extend RDIA to RDIA-LS by using training loss to identify noisy samples. We find that RDIA-LS could achieve better prediction performance than RDIA for combating the noisy labels on deep models.\n\n|        CIFAR10       | ERM   | RDIA  | RDIA-LS |\n| ----------------- | ----- | ----- | ------- |\n| 20% noise | 71.84 | 74.26 | 82.94   |\n| 40% noise | 55.62 | 58.41 | 77.26   |\n\n\n\nFollowing your suggestion, we have discussed the conditions that RDIA-LS could fail and also revised the paper to add this discussion in the last paragraph of Appendix G. We would like to clarify that RDIA-LS relies on the small-loss trick (only holds for deep models) that the samples with larger training loss may contain the corrupted labels. In this way, RDIA-LS is only suitable for deep models against corrupted labels. RDIA-LS would fail in distribution shift since the small-loss trick does not hold any more. However, we believe that if we could find some reliable criterion to select the harmful samples for the specific problem (For example, [1],[2] find that the samples with smaller training loss may hurt the model performance for class imbalance problem), our approach could still relabel these biased samples towards better model performance. Thanks for your valuable comment, we will continue research in this direction in the future.\n\nReference  \n[1] Learning to reweight examples for robust deep learning. ICML 2018  \n[2] Meta-Weight-Net: Learning an Explicit Mapping for Sample Weighting. NIPS 2019\n'}, {'title': 'Response to Reviewer 136B ', 'comment': 'Thank you for your valuable feedback, and for appreciating the motivation of our work and the technical contribution. We address your concerns below.\n\n \n\n>The technique of using influence function for identifying harmful instances is not new and well known and applied in the recent times. Hence I feel the technical novelty is not solid and limited in some aspects. However saying that, applications of existing influence functions are not straightforward and hence that’s one point to be noted.\n\n \n\nWe would like to clarify the novelty of our approach in the following aspects:\n\n- **Labeling perturbation.** Previous influence-based approaches mainly focus on perturbing the loss term of the harmful samples (i.e. downweight all the harmful samples). To the best of our knowledge, we are the first to perform fine-grained perturbation on the harmful training sample’s label and evaluate its influence on model’s performance using the influence function. \n\n- **Theoretical guarantee.** Developing an influence-based relabeling function that theoretically guarantees to improve model’s performance and is applicable for any classification tasks is a non-trivial task. We also empirically show that our relabeling approach could achieve lower test loss than various existing influence-based resampling approaches. We believe that our work can inspire new research ideas to explore novel relabeling approaches for solving training bias.\n\n- **Relabeling on Deep Models.** As pointed out by Reviewer UYUF, the cost of estimating influence functions on non-linear models is a well-known challenge. We extend RDIA to RDIA-LS that combines the small-loss trick with relabeling and works well on deep models. Table 6,7,8,9 show that RDIA-LS is effective and efficient to handle training data with corrupted labels for deep learning models, compared with the baseline which are proposed to specifically solve the noisy labels for deep model. In this way, we believe that our work can inspire new research ideas to derive the extension of other influence-based approaches. \n\n \n\nWe would like to thank the reviewer again for his/her valuable comments, which would improve the quality of our paper. \n\n \n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.'}, {'title': 'Response to Reviewer FfyB (3/3) ', 'comment': '$\\bullet$ **Second, we would like to clarify that Lemma1 and Theorem1 confirming the decrease in the validation loss are non-trivial.**\n\nRecall that the validation loss decreases in [1] because it sets $\\delta_i = 0-\\epsilon\\frac{\\mathrm{d}L (Q;\\hat{\\theta}_\\delta)}{\\mathrm{d}\\delta_i}$. In their approach, as we directly set $\\delta$ to be the gradient of minimizing the validation loss in each training epoch, the decrease in the validation loss is somehow a trivial conclusion.\n\nHowever, like the existing influence-based resampling/reweighting approaches [2,3,5], our proposed relabeling function $\\delta$  is not a trainable parameter which means we could not set $\\delta$ according to the gradient of the validation loss (actually, we do not train the model with the validation set to update any parameters). To this end, how to develop a relabeling function without trainable parameters that has a theoretical guarantee on improving the model’s performance is a non-trivial task. It is even more challenging to develop a relabeling approach that is theoretically better than the existing influence-based resampling approaches, which is what we did in this paper. \n\n\n\n**To sum up, we believe that our approach is quite different from [1] in various aspects. We thank you again for your valuable comments and we are happy to revise the paper and add [1] as related work and discuss the main differences in Appendix H, which we believe would make our paper more comprehensive.** \n\n \n\nWe hope we have addressed your questions; please let us know if there are any additional concerns.\n\n \n\nReference\n\n[1] Training Set Debugging Using Trusted Items, AAAI 2018.\n\n[2] Data dropout: Optimizing training data for convolutional neural networks, ICTAI 2018.\n\n[3] Less Is Better: Unweighted Data Subsampling via Influence Function, AAAI2020.\n\n[4] Understanding Black-box Predictions via Influence Functions. ICML 2017.\n\n[5] Optimal Subsampling with Influence Functions. NIPS 2018.\n\n'}, {'title': 'Response to Reviewer FfyB (2/3)', 'comment': '- **Experiments:** The experiments in [1] report the precision-recall (PR) curve with respect to true bugs (the identified samples actually have the wrong labels) in the training set while our experiments report the predictive performance in the test set. Since the experimental settings and metrics are totally different in two works, we could not conduct the comparison experiments. Specifically,  as **our approach does not focus on finding true labels, it does not make sense for our approach to measure the precision of label correction as [1]**. Besides, when we apply [1] to relabel the training bias, there are two main problems. (1) [1] can only correct corrupted labels but cannot handle the clean but biased training samples. In contrast, our proposed relabeling approach could relabel the biased training samples that may have correct labels towards better model performance. (2) We would like to clarify that indeed there is no criteria in [1] to indicate which data should be relabeled. If we simply use all the predicted labels to replace the original labels, it would add new noises when the predicted labels are incorrect. As shown in Figure 3 of [1], the precision-recall (PR) curve shows that when the flagged bugs (i.e., the label predicted by [1] is different from its original label) contain 50% true noisy data, the precision of label correction is also about 50%. This indicates that, without expert knowledge, it is difficult to relabel noisy data only without changing the original labels of clean data, **making it unsuitable to apply [1] to our problem, e.g., resolving training biases towards better model performance.**\n\n> An important point here is that the gradient of the amount of relabeling considered in [1] is essentially identical to the relabeling criteria proposed in (10) in this paper. Once we interpret the proposed label correction criteria as the gradient descent update, Lemma1 and Theorem1 confirming the decrease of the validation loss are more or less trivial (unless the amount of correction is too large)\n\nFor the convenience of discussion, we briefly copy the derivation given by the reviewer here:\nIf we set $\\delta_i$ as a trainable parameter and optimize it in each training epoch with the single gradient descent step size $\\epsilon$ , i.e., $\\delta_i = 0-\\epsilon\\frac{\\mathrm{d}L(Q;\\hat{\\theta}_\\delta)}{\\mathrm{d}\\delta_i}$. Then we derive that\n$$\nL(Q;\\hat{\\theta}_\\delta)-L(Q;\\hat{\\theta}_0) \\approx \\frac{1}{N} \\nabla_\\theta  L(Q;\\hat{\\theta})^T H_\\hat{\\theta_0}^{-1} \\frac{\\partial l_i(z_i\\delta_i,\\theta_0)}{\\partial \\delta_i}\n$$\n\n$\\bullet$  **First, we would like to clarify that our equation (10) is not identical to the above formula.**\n\n(1) The most important thing is that the our proposed relabeling function $\\delta$ is not a trainable parameter **which means we could not derive** $\\frac{\\partial l_i(z_i\\delta_i,\\theta_0)}{\\partial \\delta_i}$ **since there is no gradient on**  **$\\delta$**  **.** This also verifies our method is fundamentally different from [1] in relabeling function and optimization.\n\n(2) Indeed, we understand that reviewer concerned about the similarity between our method and [1] in measuring the change of validation loss, i.e., $L(Q;\\hat{\\theta}_\\delta)-L(Q;\\hat{\\theta}_0)$. However, our equation is different from the above equation. As we mentioned in section 2, we follow [3,4] to derive the validation loss change as (Equation (10)): \n\n$$\nL(Q;\\hat{\\theta}_\\delta)-L(Q;\\hat{\\theta}_0) \\approx \\frac{1}{N} \\nabla_\\theta  L(Q;\\hat{\\theta}_0)^T H_\\hat{\\theta_0}^{-1} \\nabla_\\theta (l_i(z_i\\delta,\\hat{\\theta}_0)-l_i(\\hat{\\theta}))\n$$\n\n**Here we can clearly see the difference between the two equations, i.e.,** \n$$\n\\nabla_\\theta (l_i(z_{i\\delta},\\hat{\\theta}_0)-l_i(\\hat{\\theta})) \\neq \\frac{\\partial l_i(z_i\\delta_i,\\theta_0)}{\\partial \\delta_i}\n$$\nThe former is the derivation of the model parameters and the latter is the derivation of the relabeling parameters.\n\n(3) Maybe it is our fault, but we do not understand how to get the last step of your derivation, i.e.,   \n\n$$ \n\\frac{1}{N} \\nabla_\\theta L(Q;\\hat{\\theta}_0)^TH_\\hat{\\theta_0}^{-1} \\frac{\\partial l_i(z_i\\delta_i,\\theta_0)}{\\partial \\delta_i}  \\approx \\frac{1}{N}\\nabla_\\theta L(Q;\\hat{\\theta}_0)^TH_\\hat{\\theta_0}^{-1}(l_i(z_i\\delta,\\hat{\\theta}_0)-l_i(\\hat{\\theta}))\n$$\n\nIt seems like  $\\frac{\\partial l_i(z_i\\delta_i,\\theta_0)}{\\partial \\delta_i}  \\approx  l_i(z_{i\\delta},\\hat{\\theta}_0)-l_i(\\hat{\\theta})$ . However, $\\delta$  here is a trainable parameter and it cannot be omitted. Please kindly let us know what we misunderstood.\n\n'}, {'title': 'Response to Reviewer FfyB (1/3) ', 'comment': 'Thank you for your detailed feedback. We would like to clarify in detail the main concern about the similarity between our approach and [1].\n\n >The proposed method is very similar to the one proposed in [1]. Although the ways the amount of relabeling is optimized are different (using gradient descent or by one step update), I believe the close connection between [Ref1] and the proposed method needs to be described appropriately in the paper, e.g., through detailed discussions and experimental comparisons.\n\nIndeed, we respectfully disagree with the reviewer and would like to explain that our method is **quite different** from [1] in the following aspects:   \n\n\n\n- **Target:** The target of [1] is to debug the training instance with the wrong label and predict the true label, while the target of our approach is to relabel the training bias (may have the correct label) towards better model performance. It’s worth noticing that: (1) we do not require the relabeled y belongs to any explicit class, but require the relabeled y to reduce the validation loss; (2) our method would also relabel the biased training samples that have correct labels if such relabeling improves model performance while [1] can only correct the wrong labels. Since the targets of our approach and [1] are different, their solution is unsuitable to address our problem of resolving training biases. Specifically, [1] uses the budget b (predefined by the domain expert) to control the amount of detected wrong labels in the training set and then asks experts to check whether the data in b contains wrong labels and whether the method predicts the correct label. However, when applying [1] to relabel the training bias (even if there are some wrong labels), we have no idea how much data should be relabeled in the training set so that a lower validation loss can be achieved. **This means [1] is only suitable for debugging wrong labels in the training set, but hard to relabel the training bias for improving model performance** (RDIA works since we use the influence function to identify the harmful training samples and then relabel them). \n\n- **Relabeling Function and Optimization:** The most important thing is that the proposed relabeling function and optimization in [1] are totally different from our approach. In [1], a soft label is learned to correct the wrong label. **The relabeling function  $\\delta$  in [1] is trained by a bi-level optimization using the gradient of the validation loss** which means the validation set is used for model training and the relabeling function is determined with the validation loss. However, we would like to clarify that our approach only uses the validation set to calculate the influence function without training on the validation set. More importantly, as shown in Equation (12) of our paper, **we propose an explicit relabeling function which has nothing to do with gradient, validation loss or even influence function.** Our method is close to prior influence-based methods [2,3] where the proposed reweighting/relabeling function is not obtained through optimization and will not be updated.\n\n- **Theoretical Guarantee:** Note that not every relabeling function can reduce validation loss theoretically. One of the main contributions of our paper is to theoretically prove that using our proposed relabeling function could achieve lower validation loss than resampling or reweighting approaches and this **gives a theoretical guarantee on the effectiveness of our approach**. However, we have found that [1] has no theoretical guarantee for its own method, making it even difficult to compare with the existing resampling approaches in theory. We regret that the reviewer has ignored our theoretical contribution.\n\n'}, {'summary_of_the_paper': 'The authors present an approach and framework to mitigate training biases by combining influence functions and data relabeling. \nThe idea behind training biases is that part of the data that is used to train the model does not accurately represent the real data distribution seen in the test set. Thus, having a mismatch between training and test data. This creates a generalization problem for the machine learning model. \n\nOther authors have used different resampling approaches to try and address this problem, relying on the training loss and then relabeling the data; or by using influence functions and changing the weight of the harmful examples so that the effect on the test loss is lower. \nThe current authors combine both approaches, and present a framework that relabels harmful training data based on influence functions (on the test set). \n\nThe results of their experiments show that they are able to reduce the test loss compared to other data resampling approaches.', 'main_review': 'I believe the paper is very well-written and structured. I appreciate that the authors had taken time and consideration into writing an abstract and introduction that clearly motivates the problem in hand, gives enough background into the problem, and that clearly explains the solutions and the experimental results.\n\nThe paper seems to be solidly based in theoretical proofs of their methods, together with experimental results comparing it to some baselines plus the state-of-the-art approach. I liked that the limitations are clearly explained in the appendix.  \nAll in all I think is a good paper. \n\nRegarding weaknesses of the paper that I believe could help at improve the paper if addressed. \n- First, is not until the reader reaches section three that the reader realizes that the authors use influence analysis on the validation set rather than on the test set. I think that the authors should improve the consistency across the paper of using the term ""test set"". Throughout the introduction, abstract, background, and part of the methodology, the validation set is referred as the test set. Which then is clarified to be the validation set, since the test set is only used for a proper test evaluation (without altering the training set). I would suggest to either clarify that at the beginning, or simply use validation set instead of test set. \n- Algorithm for RDIA such as done for RDIA-LS (Algorithm 1) in the Appendix E. I believe this will help portray more details for further reproducibility.\n- Regarding reproducibility, the authors mention that the code is available in the Appendix. Do the authors mean the programming code? If so, I couldn\'t find a link there. A link with the source code would be beneficial for the same reasons as the point above. \n- My last and probably the most relevant concern is the following. Imagine we have an imbalanced dataset, and that the algorithm is not able to classify well that portion of the dataset with fewer examples. The problem in this case is that we don\'t have enough data of some particular group of instances, needing some sort of solution like data augmentation (for example). \nHowever, if we would apply the authors solution (or the other authors solution for that matter) we would be treating these examples as incorrect. In the authors case the positive aspect is that the examples won\'t be removed (as with some of the others solutions). But, how would relabeling those examples help the model learn about this specific category of instances? I am thinking on the lines of a dataset where we have sensitive attributes e.g race, gender. Where some of the people are less represented. From my perspective this solution might have a negative impact in the generalization aspect since the model might not really learn those people that might look different from the rest. Or it could be the case where it helps at mitigating this bias. \nThis comment is more of an opening for a discussion with the authors, rather than ""something to fix"" . It would be great if the authors could just comment what they think on this matter during the discussion period. Since I am sure they have thought about this aspect, and I wouldn\'t want to have missed or misinterpreted some part of the paper. \nJust to clarify, my scoring hasn\'t been influenced by this last comment. ', 'summary_of_the_review': ""I think this is a good paper with merits to be included as part of the conference proceedings. \nI believe that the paper is well-written, the claims are well-supported, and the experiments seem correct. \n\n\nI believe a few things can be improved (as mentioned above)\nThe reason I don't give a higher score is because I believe that the methodology does not seem to be too novel and the results are marginally better, except for one or two datasets. \n\n"", 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'flag_for_ethics_review': ['Yes, Other reasons (please specify below)'], 'details_of_ethics_concerns': 'Not sure if data resampling can have a negative impact and be a fairness concern if we are dealing with imbalanced datasets where the datasets represent people where some minorities are less represented in such datasets.\nI have posted this as part of a possible discussion with the authors, to see what they think. \n\n', 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'summary_of_the_paper': 'The paper presents a relabeling scheme for binary and multiclass\nclassification tasks in which harmful training examples identified by\ninfluence functions are relabeled to improve performance on a hold-out\nset. The authors formally prove that the relabeling scheme determines\na reduction in loss wrt down-weighting or discarding examples, and\nreport experimental comparisons confirming the advantage of the\nproposed strategy.', 'main_review': 'The manuscript is well-written and clear. The proposed solution is\nsimple and clean with a sound theoretical justification.\n\nThe practical utility of the approach is unfortunately limited given\nthe cost of estimating influence functions on non-linear models. This\nis a well-known problem, especially when applying influence function\nto deep architectures. The authors address the problem by proposing to\nreplace influence functions with training loss while retaining their\nrelabeling scheme. I wonder how this simple strategy works in\ncomparison to their RDIA approach (the appendix does not report\ncomparisons in terms of accuracy), and under which conditions it could\nfail (e.g. distribution shift rather than noise). Otherwise one has\nthe (probably false?) impression that all you need is training loss +\nrelabeling. ', 'summary_of_the_review': 'A simple and clean solution for influence-based data relabeling.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper uses influence functions from robust statistics to perform two tasks in order to mitigate label noise and generalize better: (i) Identify harmful instances via IF; (ii) Relabel the harmful instances to have better generalization. \n', 'main_review': 'The paper uses  influence functions from robust statistics to first identify harmful instances and then relabel them based on a novel relabeling function using influence approximation. Influence estimations have been widely used to identify harmful instances or understand the impact of training samples. This paper goes one step further and uses this analysis to relabel the instances in order to achieve better generalization and lower test error. While the technical novelty is limited as the proposed formulations are extensions of existing influence functions and application is interesting. The authors use their relabeling strategy on multiple datasets and models (including deep models).   \u2028\u2028\n\nPros:\n\n(1) The paper does a focused job of using influence function for identifying harmful examples and fixing them by relabeling. While in the recent times there are papers on influence functions to identify harmful instances, this paper does a very good comprehensive and focused study compared to others.\n\n(2) The paper is well-written and easy to follow. Related works is well laid out and well covered.\n\n(3) Experimental section is complete with experiments on deep models and the proposed RDIA-LS.  The authors acknowledge the limitations of RDIA on deep models due to erroneous influence estimates for deep models and provide a workaround for it in the Appendix. This is an improvement from the earlier versions of the paper (from a previous conference). I would definitely like to highlight this section in the main paper as it’s important for all practical purposes. \n\n\nCons:\n\n(1) The technique of using influence function for identifying harmful instances is not new and well known and applied in the recent times. Hence I feel the technical novelty is not solid and limited in some aspects. However saying that, applications of existing influence functions are not straightforward and hence that’s one point to be noted. \n\n\nIn general, the paper is well-organized, a good study on influence based relabeling and has sufficient empirical studies to backup it’s proposed formulation. ', 'summary_of_the_review': 'The paper does an end to end study of using influence functions for relabeling instances to achieve a lower test-loss. The technical novelty is slightly limited as the formulations are extensions of current influence functions, however the paper is supported by strong empirical studies (including limitations about RDIA in deep models).', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'details_of_ethics_concerns': 'N/A', 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.'}, {'summary_of_the_paper': 'This paper proposes a data relabeling technique using influence function.\nThe authors first derived the influence function for data relabeling as follows.\n\n$$\n\\eta_{\\theta \\delta}\\left(z\\_i, z\\_j^{c}\\right) \\left. \\triangleq \\frac{d l\\_j^c \\left(\\hat{\\theta}\\_{\\epsilon\\_i \\delta\\_j}\\right)}{d \\epsilon\\_i}\\right|_{c\\_i=0} =-\\nabla\\_{\\theta} l\\_j^c(\\hat{\\theta}) H\\_{\\hat{\\theta}}^{-1}\\left(\\nabla\\_\\theta l\\_i\\left(z\\_{i \\delta}, \\hat{\\theta}\\right)-\\nabla\\_\\theta l\\_i(\\hat{\\theta})\\right)\n$$\n\nThe authors then derived the specific expression for the cross-entropy loss:\n$$\n\\eta_{\\theta R}\\left(z_{i}, z_{j}^{c}\\right)=-\\nabla_{\\theta} l_{j}(\\hat{\\theta}) H_{\\hat{\\theta}}^{-1} w\\left(z_{i}, \\hat{\\theta}\\right)=-\\nabla_{\\theta} l_{j}(\\hat{\\theta}) H_{\\hat{\\theta}}^{-1}\\left(-\\frac{\\nabla_{\\theta} l_{i}(\\hat{\\theta})}{1-\\varphi\\left(x_{i}, \\hat{\\theta}\\right)}\\right)=\\frac{-\\Phi_{\\theta}\\left(z_{i}, z_{j}^{c}\\right)}{1-\\varphi\\left(x_{i}, \\hat{\\theta}\\right)}\n$$\nThe authors experimentally demonstrated that data relabeling is more effective than the standard sampling/data reweighting-based approach for model improvement.', 'main_review': ""## Strength\nThe authors empirically demonstrated that the label correction is much more effective than the standard sampling/data reweighting-based approach for model improvement.\nThis result is coherent with the one previously reported in [Ref1] where a very similar data relabeling approach was studied.\nThe results on DNNs (on MNIST and CIFAR10) are new where [Ref1] considered only kernel-based models.\n\n* [Ref1] Training Set Debugging Using Trusted Items, AAAI 2018.\n    * *I am not the author of [Ref1].*\n\n## Weakness\nThe proposed method is very similar to the one proposed in [Ref1], while the current paper misses this important prior work.\n[Ref1] formulated the data relabeling problem as the bi-level optimization, and proposed an algorithm to optimize the amount of relabeling using gradient descent.\nAn important point here is that the gradient of the amount of relabeling considered in [Ref1] is essentially identical to the relabeling criteria proposed in (10) in this paper (see below for the detail).\nIn addition, once we interpret the proposed label correction criteria as the gradient descent update, Lemma1 and Theorem1 confirming the decrease of the validation loss are more or less trivial (unless the amount of correction is too large).\nIn summary, although the ways the amount of relabeling is optimized are different (using gradient descent or by one step update), I believe the close connection between [Ref1] and the proposed method needs to be described appropriately in the paper, e.g., through detailed discussions and experimental comparisons.\n\n*Close connection to [Ref1]*\n\nAs the special case of the formulation of [Ref1], we can obtain the following bi-level optimization (here, I removed some terms from the original formulation in [Ref1] for simplicity, and used the notation of the current paper).\n\n$$\n\\min\\_{\\delta \\in \\mathbb{R}^{n}} L(Q; \\hat{\\theta}\\_\\delta) := \\frac{1}{M} \\sum\\_{j=1}^{M} l\\_j^c(\\hat{\\theta}\\_\\delta) , \\\\: \\text { s.t. } \\\\: \\hat{\\theta}\\_\\delta = \\underset{\\theta \\in \\mathbb{R}^{p}}{\\operatorname{argmin}} \\frac{1}{N} \\sum\\_{i=1}^{N} l\\_i(z\\_{i\\delta\\_i}, \\theta) ,\n$$\n\nwhere $\\delta$ is the amount of relabeling.\nThis is the optimization of the amount of relabeling so that the the validation loss to be minimized.\nFollowing [Ref1], we can derive the gradient of the validation loss $L(Q; \\hat{\\theta}_\\delta)$ with respect to $\\delta_i$ as\n\n$$\n\\frac{dL(Q; \\hat{\\theta}\\_\\delta)}{d\\delta\\_i} = \\nabla\\_\\theta L(Q; \\hat{\\theta}\\_\\delta) \\frac{d\\hat{\\theta}\\_\\delta}{d\\delta\\_i} = -\\frac{1}{N}\\nabla\\_\\theta L(Q; \\hat{\\theta}\\_\\delta)^\\top H\\_{\\hat{\\theta}\\_\\delta}^{-1} \\frac{\\partial l\\_i(z_{i\\delta_i}, \\hat{\\theta}\\_\\delta)}{\\partial \\delta\\_i} .\n$$\n\nWhen there is no label correction, $\\delta = 0$ and we have $\\hat{\\theta}_0$.\n\nThen, if we consider relabeling the $i$-th training instance with the gradient step size $\\epsilon$, we have the update\n\n$$\n\\delta\\_i \\leftarrow 0 - \\epsilon \\left. \\frac{dL(Q; \\hat{\\theta}\\_\\delta)}{d\\delta\\_i} \\right|\\_{\\delta\\_i = 0}\n$$\n\nand the decrease of the validation loss induced by this update is \n\n$$\nL(Q; \\hat{\\theta}\\_{\\delta}) - L(Q; \\hat{\\theta}\\_0) \\approx \\epsilon \\left. \\frac{dL(Q; \\hat{\\theta}\\_\\delta)}{d\\delta\\_i} \\right|\\_{\\delta\\_i = 0} \n$$\n$$\n= \\frac{\\epsilon}{N} \\nabla\\_\\theta L(Q; \\hat{\\theta}\\_0)^\\top H\\_{\\hat{\\theta}\\_0}^{-1} \\frac{\\partial l\\_i(z\\_{i\\delta\\_i}, \\theta_0)}{\\partial \\delta\\_i}\n$$\n$$\n\\approx \\frac{1}{N}\\nabla\\_\\theta L(Q; \\hat{\\theta}\\_0)^\\top H\\_{\\hat{\\theta}\\_0}^{-1} (l\\_i(z\\_{i \\epsilon}, \\theta) - l\\_i(\\theta)) .\n$$\nThe last line is nothing but the criteria (10) proposed in this paper.\n\n---\n## After Discussion with Authors\n\nI conducted an experiment on breast cancer dataset by myself (see below).\nThere, I confirmed that RDIA does better than the One-Step GD update of [Ref1].\nI therefore decided to increase my score with a strong expectation to the authors for **referring [Ref1] appropriately in the main body of the paper**.\nAs I mentioned in my original review, this is not the first study considering relabeling based on the influence function-like technique.\nRecalling that influence function is one specific example of implicit gradient, [Ref1] is the first work in this direction to my knowledge (even if [Ref1] does not describe the fact explicitly).\nAs I demonstrated in my code, the simplest version of [Ref1] with only one-step update (and without any human intervention) does good job.\n**I strongly expect the authors to pay certain respect to [Ref1] and do not underestimate their technical contribution**.\nFor example, the current writing in Appendix H seems to be not appropriate.\nEven without human intervention, DUTI [Ref1] can do the same (i.e., relabel the training bias towards better model performance) as the proposed approach (but with slightly inferior performance).\n> (2) The learning task of DUTI is to debug the training instances which may contain the wrong labels and predict the true labels. While the target of our approach is to relabel the training bias towards better model performance. In this way, only our approach could relabel the biased training samples with correct labels towards better model performance.\n\n### The results on breast cancer dataset\n\n*Test Accuracy*\n\n|     |   ERM |   One-Step GD |   RDIA |\n|----:|------:|--------------:|-------:|\n| 0   | 0.959 |         0.96  |  0.961 |\n| 0.1 | 0.96  |         0.968 |  0.962 |\n| 0.2 | 0.953 |         0.951 |  0.959 |\n| 0.3 | 0.931 |         0.944 |  0.957 |\n| 0.4 | 0.842 |         0.897 |  0.958 |\n| 0.5 | 0.563 |         0.762 |  0.958 |\n| 0.6 | 0.178 |         0.782 |  0.953 |\n| 0.7 | 0.075 |         0.875 |  0.953 |\n| 0.8 | 0.05  |         0.931 |  0.949 |\n| 0.9 | 0.037 |         0.95  |  0.94  |\n\n*Test Loss*\n\n|     |      ERM |   One-Step GD |     RDIA |\n|----:|---------:|--------------:|---------:|\n| 0   | 0.141025 |      0.161033 | 0.13819  |\n| 0.1 | 0.237151 |      0.24697  | 0.146115 |\n| 0.2 | 0.345921 |      0.347718 | 0.157343 |\n| 0.3 | 0.453018 |      0.451272 | 0.171787 |\n| 0.4 | 0.56437  |      0.557865 | 0.185989 |\n| 0.5 | 0.698697 |      0.63033  | 0.197167 |\n| 0.6 | 0.876071 |      0.590184 | 0.203124 |\n| 0.7 | 1.08732  |      0.492453 | 0.196512 |\n| 0.8 | 1.44151  |      0.362804 | 0.197389 |\n| 0.9 | 2.0108   |      0.265325 | 0.241273 |\n\n```\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\n\ndef sigmoid(u):\n    return 1.0 / (1.0 + np.exp(-u))\n\ndef logloss(w, b, x, y):\n    p = sigmoid(x.dot(w) + b)\n    return - (y.dot(np.log(p)) + (1 - y).dot(np.log(1 - p))) / y.size\n\ndef logreg(x, y, lam=0.1, lr=0.01, max_itr=100):\n    num, dim = x.shape\n    xo = np.concatenate([x, np.ones((num, 1))], axis=1)\n    w = np.zeros(dim+1)\n    for itr in range(max_itr):\n        p = sigmoid(xo.dot(w))\n        g = (p - y).dot(xo) / num\n        g[:-1] = g[:-1] + lam * w[:-1]\n        w = w - lr * g\n    return w[:-1], w[-1]\n\n# experiment\nlam = 1e-2\nacc, loss = [], []\nnr = np.linspace(0, 0.9, 10)\nfor noise_rate in nr:\n    acc_n, loss_n = [], []\n    for seed in range(10):\n\n        # breast cancer data\n        # train / val / test = 300 / 169 / 100\n        x, y = datasets.load_breast_cancer(return_X_y=True, as_frame=False)\n        x, xte, y, yte = train_test_split(x, y, test_size=269, random_state=seed)\n        xval, xte, yval, yte = train_test_split(xte, yte, test_size=100, random_state=seed)\n\n        # normalize features\n        scaler = StandardScaler().fit(x)\n        x = scaler.transform(x)\n        xval = scaler.transform(xval)\n        xte = scaler.transform(xte)\n\n        # noisy label in training\n        np.random.seed(seed)\n        flip = np.random.rand(y.size) < noise_rate\n        y = np.logical_xor(y, flip).astype(int)\n\n        # fit logreg\n        w, b = logreg(x, y, max_itr=500, lam=lam)\n\n        # test accuracy / loss\n        zte = (xte.dot(w) + b > 0).astype(int)\n        acc_te1 = np.mean(yte == zte)\n        loss_te1 = logloss(w, b, xte, yte)\n\n        # influence function\n        num, dim = x.shape\n        xo = np.concatenate([x, np.ones((num, 1))], axis=1)\n        p = sigmoid(x.dot(w) + b)\n        H = xo.T.dot((p * (1 - p))[:, np.newaxis] * xo) / num\n        H = 0.5 * (H + H.T) + np.diag([lam] * dim + [0]) # Hessian\n        g = ((p - y)[:, np.newaxis] * xo) / num # gradient\n        f = - np.linalg.solve(H, g.T).T # influence function\n\n        # label gradient over validation loss\n        xv = np.concatenate([xval, np.ones((xval.shape[0], 1))], axis=1)\n        q = sigmoid(xval.dot(w) + b)\n        gv = (q - yval).dot(xv) / xval.shape[0] # gradient of validation loss\n        gy = f.dot(gv) * (np.log(p) - np.log(1 - p)) # label gradient\n\n        # label correction by one-step gradient descent\n        ynew = y - 1e+3 * gy # step size of GD = 1e+3\n        ynew = np.minimum(1, np.maximum(0, ynew)) # clip label\n        wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)\n\n        # test accuracy / loss after label correction\n        zte = (xte.dot(wnew) + bnew > 0).astype(int)\n        acc_te2 = np.mean(yte == zte)\n        loss_te2 = logloss(wnew, bnew, xte, yte)\n\n        # label correction by RDIA\n        ynew = y.copy()\n        #ynew[f.dot(gv) > 0] = 1 - ynew[f.dot(gv) > 0] # flip label\n        ynew[f.dot(gv) > 1e-3] = 1 - ynew[f.dot(gv) > 1e-3] # flip label\n        wnew, bnew = logreg(x, ynew, max_itr=500, lam=lam)\n\n        # test accuracy / loss after label correction\n        zte = (xte.dot(wnew) + bnew > 0).astype(int)\n        acc_te3 = np.mean(yte == zte)\n        loss_te3 = logloss(wnew, bnew, xte, yte)\n\n        acc_n.append((acc_te1, acc_te2, acc_te3))\n        loss_n.append((loss_te1, loss_te2, loss_te3))\n    \n    acc.append(acc_n)\n    loss.append(loss_n)\nacc = np.array(acc)\nloss = np.array(loss)\n\nacc = pd.DataFrame(np.mean(acc, axis=1))\nacc.columns = ['ERM', 'One-Step GD', 'RDIA']\nacc.index = nr\nloss = pd.DataFrame(np.mean(loss, axis=1))\nloss.columns = ['ERM', 'One-Step GD', 'RDIA']\nloss.index = nr\nprint('Test Accuracy')\ndisplay(acc)\nprint('Test Loss')\ndisplay(loss)\n```"", 'summary_of_the_review': 'The proposed method is very similar to the one proposed in [Ref1], while the current paper misses this important prior work.\nAlthough the ways the amount of relabeling is optimized are different (using gradient descent or by one step update), I believe the close connection between [Ref1] and the proposed method needs to be described appropriately in the paper, e.g., through detailed discussions and experimental comparisons.\n\n* [Ref1] Training Set Debugging Using Trusted Items, AAAI 2018.\n\nThe strong aspect of this paper over [Ref1] is the experimental results where the results on DNNs (on MNIST and CIFAR10) are reported, while [Ref1] considered only kernel-based models.\n\n---\n## After Discussion with Authors\n\nI conducted an experiment on breast cancer dataset by myself (see below).\nThere, I confirmed that RDIA does better than the One-Step GD update of [Ref1].\nI therefore decided to increase my score with a strong expectation to the authors for **referring [Ref1] appropriately in the main body of the paper**.\nAs I mentioned in my original review, this is not the first study considering relabeling based on the influence function-like technique.\nRecalling that influence function is one specific example of implicit gradient, [Ref1] is the first work in this direction to my knowledge (even if [Ref1] does not describe the fact explicitly).\nAs I demonstrated in my code, the simplest version of [Ref1] with only one-step update (and without any human intervention) does good job.\n**I strongly expect the authors to pay certain respect to [Ref1] and do not underestimate their technical contribution**.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '2: The contributions are only marginally significant or novel.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '6: marginally above the acceptance threshold', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'title': 'Resolving Training Biases via Influence-based Data Relabeling', 'authorids': ['~Shuming_Kong1', '~Yanyan_Shen1', '~Linpeng_Huang1'], 'authors': ['Shuming Kong', 'Yanyan Shen', 'Linpeng Huang'], 'keywords': ['Training bias', 'influence functions', 'data relabeling'], 'abstract': ""The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that estimates the impacts of a training sample on the model’s predictions. Recent studies on \\emph{data resampling} have employed influence functions to identify \\emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise. "", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'kong|resolving_training_biases_via_influencebased_data_relabeling', 'pdf': '/pdf/64c51657be7bb5a9efecafe39344c719ccb4d394.pdf', 'one-sentence_summary': 'We propose an influence-based relabeling framework for solving training bias with a theoretical guarantee', 'supplementary_material': '/attachment/7ef8b8e51f37c9823d3d6cd707735960abd6a147.zip', '_bibtex': '@inproceedings{\nkong2022resolving,\ntitle={Resolving Training Biases via Influence-based Data Relabeling},\nauthor={Shuming Kong and Yanyan Shen and Linpeng Huang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=EskfH0bwNVn}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
"['Divyam Madaan', 'Jaehong Yoon', 'Yuanchun Li', 'Yunxin Liu', 'Sung Ju Hwang']",ICLR,Representational Continuity for Unsupervised Continual Learning,https://iclr.cc/virtual/2022/oral/7120,2022," Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (Lump), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations.",Oral 2: Understanding Deep Learning,https://openreview.net/pdf?id=9Hrka5PA7LW,https://openreview.net/forum?id=9Hrka5PA7LW,9Hrka5PA7LW,"[{'title': 'Response to your concerns', 'comment': 'Hi Jiangwei Zhao,\n\nThank you for your interest in our work. We believe these are misunderstandings and disagree with the concerns raised in the comment. We address them below:\n* **Error in public code:** Can you provide more details about the implementation errors? We have cross-checked our code and believe that this a misunderstanding. You can also open an issue or create a pull request regarding these concerns, and we will happily address them.\n\n* **Error in the paper:** This is a misunderstanding. We mention the learning rate as 0.03 in the Appendix paragraph hyper-parameter \nconfiguration line 4 because the batch size used for all the UCL experiments was 256 (0.03 * batch_size/256=0.03 when batch_size=256). The scaling was implemented to allow the scalability to large batch sizes [2], but we did not experiment with it for these experiments. Compared to our setting, we use the tuned hyper-parameters from DER[1] for the SCL experiments. To this end, we do not tune the learning rate for any UCL methods and adopt them using state-of-the-art DER for SCL [1], as mentioned in our paper (see Appendix A.1 paragraph Hyper-parameter configuration Line 1) to show the robustness and flexibility of these methods. While we agree that the performance can be improved with tuning of the hyper-parameters, but we remark that the focus of our work was not to achieve state-of-the-art CL performance, but instead to evaluate and extend the supervised CL methods to real-world scenarios with unlabelled data.\n\nThank you and Best Regards,    \nAuthors\n\nReferences:  \n[1] Buzzega et al. Dark Experience for General Continual Learning: a Strong, Simple Baseline. NeurIPS 2020.   \n[2] Goyal et al. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.  '}, {'title': 'Paper Decision', 'decision': 'Accept (Oral)', 'comment': 'Exciting work at the intersection of continual learning and representation learning. The reviewers have all commented that the proposed work addresses a number of issues related to catastrophic forgetting, which is very encouraging. The work also shows that the representation learning with the proposed method is more general than the one learned with supervised CL. The reviewers have praised the work as being well-written and with thorough experiments. There was a robust back and forth between the reviewers and the authors during the rebuttal period, in which the authors appear to have addressed most of the concerns. Given the insights, results and potential impact of this work, I think this work definitely should be published at ICLR.'}, {'title': 'Thank you for your response', 'comment': 'Dear Reviewer synL\n\nWe are happy to hear that our response addressed your concerns. Thank you for your valuable feedback and suggestions.\n\nThank you,  \nAuthors'}, {'title': 'Thank you for your response', 'comment': 'Dear Reviewer QCJy,\n\nWe are glad to hear that we addressed your concerns. Thank you for all your insightful comments and suggestions.\n\nThank you,  \nAuthors'}, {'title': 'thanks for your answer.', 'comment': 'Thanks for the detailed answer. My concerns are addressed. After reading the other reviews and the answers, I confirm my rating.'}, {'title': 'Thanks for the rebuttal', 'comment': 'The authors have addressed my concerns. I will keep my score (8).'}, {'title': 'Thx for reply', 'comment': 'Thanks for your detailed response. All my issues have been well addressed, I thus increased the scores.'}, {'title': 'Response to Reviewer u1jR - Part I', 'comment': 'Dear Reviewer u1jR,\n\nThank you for your valuable feedback. We respond to the concerns raised by you below. Please let us know what you think about our response and whether you would like further clarifications.\n\n> **Missed comparison with CURL.**  Though the authors criticised that Continual Unsupervised Representation Learning framework (CURL) to be limited by digit-based gray-scale datasets, no direct comparison with CURL is done by following their evaluation protocol. Adding this result could better reveal the difference between the proposed method and CURL in terms of effectiveness. \nBeside, the evaluation of cluster quality used in CURL seems to be an important evaluation metric in unsupervised continual learning, which has not been used in the paper.\n\nWe remark that the empirical evaluation CURL is limited to digit-based toy datasets (i.e., Split MNIST and Omniglot). The representations learned on these datasets are *not practically useful* for any downstream tasks in practical scenarios. Nevertheless, following the reviewer’s suggestion, we conducted experiments with SimSiam on Split-MNIST for task-incremental setting following CURL and presented the results below. We report the mean and standard deviation across three runs for accuracy and forgetting, while the CURL paper does not include the forgetting measure. Note that simply UCL-Finetune without any continual learning techniques can achieve *comparable performance* to CURL with LUMP, *further improving* the performance. \n\n|       \t\t| Accuracy    \t| Forgetting    \t|\n|--------------\t\t|------------------\t|-------------------\t|\n| CURL    \t\t| $99.10$ $\\pm$ $0.06$ \t| N/A       \t|\n| UCL-FINETUNE \t| $98.16$ $\\pm$ $0.32$ \t| $1.62$ $\\pm$ $0.21$ \t|\n| UCL-LUMP  \t| $99.27$ $\\pm$ $0.17$ \t| $0.01$ $\\pm$ $0.00$ \t|\n\n\nAdditionally, we clarify that we report the accuracy and forgetting following the continual learning evaluation protocol. We utilize Wu et al. 2018 for our KNN evaluation and follow the KNN evaluation from prior representation learning methods, which is *more robust* than the CURL cluster accuracy/quality measure. In particular, the feature level embeddings learned by Wu et al. 2018 are *instance-wise discriminative*, while CURL measures the *class-discriminability* of latent-space. As described in Section A.4 of their paper, it is highly dependent on the dimensionality of the latent space. It works well for MNIST and Omniglot because they have many low-variance black pixels with a relatively small intra-class variance within the center pixels.\n\n--- \n\n> **Degraded Performance of DER and MULTITASK.** In the Table 1, we see that the proposed unsupervised continual learning can improve all baseline methods except DER and MULTITASK. A clear explanation about this performance drop should be added.\n\nWe believe that the performance of DER degraded because DER was not formulated for UCL. In particular, our proposed approximation of utilizing the projected output instead of the logits in the original method might not the best possible choice and its investigation would be an interesting direction for future work. \n\nFurther, MULTITASK shows a degraded performance because it shows the difference for supervised and unsupervised representation learning methods, where unsupervised methods do not outperform the supervised learning methods. In contrast, we show that unsupervised representations are surprisingly more robust to catastrophic forgetting when trained on a sequence of tasks.\n\n---\n\n> **Qualitative Analysis.** Why the visualization of feature maps stops at $\\mathcal{T}\\_{13}$, while the loss landscape visualization continues to $\\mathcal{T}\\_{19}$? \n\nThe reason that we provide the visualization of features at $\\mathcal{T}\\_{13}$ is to *demonstrate knowledge preservation of the task in the middle-time step* after the completion of continual learning. Of course, we can also provide the feature visualization of the last task, but it is not helpful to understand the catastrophic forgetting of feature representation during continual learning.  \n\nIn Figure 5, we visualize the loss landscape of *first task ($\\mathcal{T}_0$)* after the completion of training task $\\mathcal{T}_0$ and $\\mathcal{T}\\_{19}$, respectively, which helps to understand the change of loss landscape before and after occurring the catastrophic forgetting in a continual learner.\n\n---\n\n> In Figure 5, the difference between SCL-DER and LUMP is hard to interpret.\n\nFigure 5, the difference between SCL-DER and LUMP is hard to interpret. We clarify that the scale of loss between SCL-DER and LUMP is different in Figure 5. We can observe that LUMP achieves a *lower value of loss with a smoother loss landscape* than SCL-DER. Further, we have provided other quantitative and qualitative analyses, including average accuracy and forgetting, feature visualization, feature similarity, and few-shot adaptation to demonstrate the effectiveness of LUMP compared to SCL-DER.\n\n---\n[we continue our response below]\n'}, {'title': 'Response to Reviewer QCJy', 'comment': 'Dear Reviewer QCJy, \n\nWe thank the reviewer for the encouraging comments and suggestions. We are glad that you found our paper well-written with rich experiments and believe that it will help design future supervised and unsupervised CL methods.\n\n---\n\n> **K-NN classifier clarification.** Sec 5.1: Knn classifier: which set is used for NN? the replay buffer? validation set?\n\nWe clarify that we use the K-NN evaluation proposed by Wu et al. 2018, where the features for each instance in the training set are stored in a discrete memory bank. The optimal feature-level embeddings are then learned by instance-level discrimination, which maximally scatters the features of the training samples. Following prior works in representation learning, we use the task-level training set without any augmentation in the task-incremental setup for the supervised and unsupervised KNN evaluation. We have further clarified this in Section A.1 of the revision.\n\n---\n\n> The Knn is used both for supervised and unsupervised experiments, right? \n\nThat is correct. We use KNN for evaluation for both the supervised and unsupervised experiments as described in Section 5.1.\n\n---\n\n> Sec 5.3: more explanations about CKA are required for the reader that is not familiar with this measure \n\nThank you for the suggestion. We have updated the description in the revision (Line 4 in Section 5.3, highlighted in blue). \n\n---\n\n> In Fig.5, we can notice that the range of value gets smaller in T19 (from [4.6,5.6] for T0 to [4.4,4.6]). Any idea why?\n\nThank you for pointing out this interesting observation. We believe that this is likely an outcome of the weight-decay during training. In particular, the scale of the network weights is *lower after the last task* compared to the first task. We hope our loss-landscape investigation would encourage future works to evaluate the loss-landscape for CL methods during sequential training to provide further insights to alleviate catastrophic forgetting. \n\n---\n\n> I am not really convinced but the visualization in Fig4. It seems that LUMP has sparser activations. The shapes of the objects are more clearly visible in its feature map. Does it simply mean that it learns lower-level features (similar to edge detector)?\n\nWe first want to emphasize that LUMP features in Figure 4 are not sparser (zero activations are visualized in dark blue). On the contrary, supervised continual learning counterparts and UCL-Finetune obtains more sparse and low-level features. Based on the following observations, we do not believe LUMP learns lower-level features like an edge detector: (a) it *significantly outperforms baselines* in both terms of accuracy and forgetting, which implies the *model preserves high-level feature information to distinguish the characteristics of each class* in the task which low-level features cannot perform, (2) visualized features from LUMP seem to capture the instance-discriminative knowledge while other baselines often obtain blurred and fragmented feature maps.\n\nWe further include the visualization of the low-level features in Figure A.8 in the revision, where we obtain similar conclusions. In particular, UCL seems to capture *more localized low-level features* than SCL, which often captures redundant information. Note that the features in Figure A.8 are different from the features in Figure A.9, which capture high-level features with more information. \n\n---\n\n> **Title update.** I recommend changing the title. ""Rethinking the Representational Continuity"" is much too strong. The conclusions of the paper are great but it does not provoke a real rethinking of the problem.\n\n Thank you for raising your concern. We have updated the title to ""Representational Continuity for Unsupervised Continual Learning"" in the revision. \n\n---\n\nThank you,  \nAuthors\n'}, {'title': 'Response to Reviewer tbme', 'comment': 'Dear Reviewer tbme , \n\nThank you for your review and thoughtful comments. We are glad that you believe that our work is well-motivated, well-written, that tackles one of the most critical issues in CL with comprehensive qualitative and quantitative results. We clarified your concern regarding the novelty of our work and conducted additional experimental results showcasing the utility of UCL for class-incremental CL settings. We hope that you will consider raising your score if you find our response satisfactory.\n\n---\n\n> **Novelty.** Limited novelty as LUMP algorithm is adopted from the supervised mixup technique(Zhang et al, 2018): \n\nWe want to clarify that LUMP is not the main contribution of our work. In particular, our objective was to *analyze the UCL representations*, and in our experimental evaluation, we have shown that even without LUMP, UCL achieves *better performance* than its supervised counterparts. Further, we relax the assumption of the availability of a large amount of unbiased and unlabelled datasets to learn the unsupervised feature representations and propose *representation learning on a sequence of tasks*. To our knowledge, our work is the *first attempt that bridges the gap* between unsupervised representation learning and continual learning and scales UCL to larger datasets including TinyImageNet and Split CIFAR-100.\n\nTo this end, we dissect this improvement by exhaustively comparing the SCL and UCL representations quantitatively on various datasets, OOD evaluation, few-shot adaptation, and qualitatively by feature visualization, feature similarity, and loss-landscape visualization. While mixup has not been utilized for continual learning in prior works, we use LUMP to improve the UCL counterparts, but we do not propose it as our main novelty.\n\n---\n\n> **Class-incremental or task-agnostic CL experiments.** The authors conducted extensive experiments in task-incremental setting. It would be interesting to see how UCL and the proposed method perform in class-incremental and task-agnostic CL settings.\n\nThank you for your suggestion. During the rebuttal, we compared various SCL and UCL representations for class-incremental CL scenarios. We present the results for accuracy in the class-incremental CL setup below. Note that UCL consistently outperforms the SCL counterparts. In particular, UCL-Finetune achieves an absolute improvement of *13.97% and 18.41%* over SCL-Finetune on Split CIFAR-10 and Split CIFAR-100, respectively. Further, UCL-SI improves the absolute performance over SCL-SI by *12.83% and 20.16%* on Split CIFAR-10 and Split CIFAR-100, respectively. We believe these results further demonstrate the flexibility of our method and strengthen our experimental results.\n\n|   \t| Split CIFAR-10 \t| Split CIFAR-100 \t|\n|---------------\t|------------------\t|------------------\t|\n| SCL-Finetune| $53.17 \\pm 1.71$ \t| $13.77 \\pm 0.12$ \t|\n| SCL-SI  \t| $55.15 \\pm 1.20$ \t| $15.77 \\pm 0.61$ \t|\n| SCL-Multitask| $89.41 \\pm 0.15$ \t| $66.07 \\pm 0.18$ \t|\n| UCL-Finetune | $67.04 \\pm 0.53$ \t| $32.18 \\pm 0.53$ \t|\n| UCL-SI  \t| $67.98 \\pm 0.19$ \t| $35.93 \\pm 2.33$ \t|\n| UCL-LUMP | $64.76 \\pm 0.33$ \t| $38.84 \\pm 0.42$ \t|\n| UCL-Multitask | $83.56 \\pm 0.37$ \t| $51.05 \\pm 1.60$ \t|\n\n---\n\n> **Figure 3, an exception in Layer 4 of DER method.** Fig 3: In general, higher layers have lower feature similarity than lower layers, and similarity between UCL models are higher than that of SCLs. However, there is an exception in Layer 4 of DER method -- the similarity of SCL is higher than that of UCL. It is worth some discussion on this exception.\n\nWe conjecture that this is due to the fundamental design of DER. In particular, SCL-DER matches the network logits across a sequence of tasks, which leads to the higher similarity for Layer 4. In contrast, this objective does not apply to UCL, and we match the output of the projected output, which might not be the best possible choice to retain all the properties of the supervised counterpart. \n\nFurther, we want to clarify that it is essential to have higher similarity across lower layers because it highlights that the low-level representations learned across independently trained networks are robust and consistent. We believe that further investigation of this observation would provide more valuable insights for future work.\n\n---\n\nWe thank you again for your time and efforts in reviewing our paper and the constructive comments and suggestions.\n\nThank you,  \nAuthors\n'}, {'title': 'Response to Reviewer u1jR - Part II', 'comment': ""[see the previous comment for the first part of our response]\n\n> **Why Not Directly Use Unsupervised Learned Presentations.** As an important purpose of the unsupervised representation learning is to learn a powerful embedding space that can be quickly fine-tuned for latter down-stream tasks. Why don't we consider a baseline where the feature backbone is initialized with SimSiam or Barlow Twins, and directly fine-tune them on a sequence of tasks. This is probably not considered in the standard continual learning, but the results of this baseline could be informative to the community of both domains.\n\nThank you for your suggestion. However, due to the limited compute budget and time during the rebuttal, we could not train an unsupervised model on ImageNet (powerful embedding space) to evaluate your proposed baseline. While the proposed baseline is orthogonal to both supervised and unsupervised continual learning scenarios, we believe it can be easily integrated with our model and further improve its performance.\n\n---\n\n> The major difference between the SCL (supervised continual learning) and UCL (unsupervised continual learning) seems to be just adding a unsupervised representation loss at the backbone and freezing it in the second stage of predicting head fine-tuning. \n\nFirst, we want to clarify that our work is the *first attempt that bridges the gap between unsupervised representation learning and continual learning*. In particular, it allows the scalability of existing representation learning methods to a sequence of tasks and accommodates the *continuous shifts in data distributions* for representational learning. Further, our proposed framework is flexible and scalable that allows its application to multiple representation learning and continual learning methods, as demonstrated in our experimental evaluation.\n\nSecond, note that UCL is simple, intuitive, easy to implement, and highlights the strengths of learning unsupervised representations. We show that a simple fine-tuning baseline can obtain comparable accuracy and lower forgetting than state-of-the-art supervised continual learning methods. We believe that our method will *open the door to understanding the behavior we demonstrate*, and we feel that it will be valuable for the community.\n\nLastly, we provide comprehensive experiments to back up our claims and dissect the learned representations such as OOD evaluation, few-shot adaptation, features visualization, feature similarity, and loss-landscape visualization. We also demonstrate the utility of mixup for continual learning and propose a simple-yet-effective method coined LUMP that further *improves the performance and alleviates catastrophic forgetting*.\n\n---\n\nThank you again for your time and efforts in reviewing our paper, and we hope that you will consider raising your score if you find our response satisfactory. \n\nThank you,  \nAuthors\n""}, {'title': 'Response to Reviewer synL', 'comment': 'Dear Reviewer synL,\n\nThank you for your review and your thoughtful comments. We appreciate you finding our paper well-motivated, well-written with quantitative and qualitative justification and applicability to real-world use cases. We clarify your concerns regarding computational constraints and comparison with class-incremental baselines below.\n\n> **Runtime and memory constraints for K-NN.** To run K-NN in a lifelong learning setting is challenging due to ever-growing storage requirements for storing samples coming from different stages. During inference, the algorithm also needs to compute distances between the query and many stored data points. Can the authors shed some light about the runtime and memory comparison between the proposed method and supervised counterparts?\n\nWe utilize the K-NN method proposed by Wu et al. 2018, utilized in prior works for representation learning. It is computationally effective for large datasets and architectures. For instance, as mentioned in their paper, it can encode *1.28M ImageNet images in 600 MB* of storage, where exhaustive neighbor search takes 20 ms per image on a TitanX GPU. Similarly, we observed that it takes 110 seconds for the inference on the test set across all the tasks for Split CIFAR-10. The increase in computational cost is *marginal* compared to the supervised counterparts.\n\n---\n\n> **Compare with other recent supervised continual learning approaches.** It would be great to also compare with other recent supervised continual learning approaches as well such as [A1, A2].\n\nThank you for pointing us to these methods; however, we could not directly compare these methods during the rebuttal as the suggested methods focus on the class-incremental learning setup. In contrast, we conducted the experimental evaluation for the task-incremental setup. Nevertheless, we evaluated our method on class-incremental CL setup during the rebuttal, and we show the results below, where UCL achieves *significantly better performance* on both datasets. We report the mean and standard deviation across three independent runs.\n\n|       \t| Split CIFAR-10 \t| Split CIFAR-100 \t|\n|---------------\t|------------------\t|------------------\t|\n| SCL-Finetune| $53.17 \\pm 1.71$ \t| $13.77 \\pm 0.12$ \t|\n| SCL-SI    \t| $55.15 \\pm 1.20$ \t| $15.77 \\pm 0.61$ \t|\n| SCL-Multitask| $89.41 \\pm 0.15$ \t| $66.07 \\pm 0.18$ \t|\n| UCL-Finetune | $67.04 \\pm 0.53$ \t| $32.18 \\pm 0.53$ \t|\n| UCL-SI    \t| $67.98 \\pm 0.19$ \t| $35.93 \\pm 2.33$ \t|\n| UCL-LUMP  | $64.76 \\pm 0.33$ \t| $38.84 \\pm 0.42$ \t|\n| UCL-Multitask | $83.56 \\pm 0.37$ \t| $51.05 \\pm 1.60$ \t|\n\n---\n\nThank you,   \nAuthors'}, {'summary_of_the_paper': 'Unlike most of the continual learning approaches in the literature that perform supervised training at each learning stage, the authors propose to perform unsupervised representation learning on the sequence of incoming data and then classify the samples at each stage using K-NN. Experiments on standard CIFAR10/100 and Tiny-ImageNet shows that the proposed method alleviates catastrophic forgetting and generalizes better in different scenarios.', 'main_review': 'Strengths\n* The submission is well written and easy to follow. The proposed concept is well motivated with various quantitative and qualitative justifications. \n* While many unsupervised/self-supervised training approaches require pre-training on massive unlabeled data, the proposed method here works well with the help from Mixup and does not require additional pre-training set, largely making it more applicable to real-world use cases. \n* I especially enjoy reading Sec 5.3 regarding the analyses on feature similarity between different learning approaches, visualization of feature space, and loss landscape visualization. This section provides additional justification besides the absolute accuracy improvement over the supervised continual learning counterparts.\n\nWeaknesses\n* I think the main limitation of using the proposed pipeline in practice is runtime and memory constraints. To run K-NN in a lifelong learning setting is challenging due to ever-growing storage requirements for storing samples coming from different stages. During inference, the algorithm also needs to compute distances between the query and many stored data points. Can the authors shed some light about the runtime and memory comparison between the proposed method and supervised counterparts?\n* It would be great to also compare with other recent supervised continual learning approaches as well such as [A1, A2]\n\n[A1] Zhao et al. Maintaining discrimination and fairness in class incremental learning. CVPR 2020. \\\n[A2] Liu et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020.\n', 'summary_of_the_review': 'Overall a good quality submission with novel and interesting ideas. It would be great for the authors to address the weaknesses mentioned above.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper studies the problem of representation learning in an unsupervised continual learning(UCL) setting. It shows that the representation learned with UCL is more general than the one learned with supervised CL (SCL), and investigates why UCL is more robust to catastrophic forgetting than SCL by analyzing the similarity of learned features and visualizing loss landscape. The authors also propose to apply mixup technique to UCL setting and present a LUMP algorithm to further improve the performance of CL.This paper studies the problem of representation learning in an unsupervised continual learning(UCL) setting. It shows that the representation learned with UCL is more general than the one learned with supervised CL (SCL), and investigates why UCL is more robust to catastrophic forgetting than SCL by analyzing the similarity of learned features and visualizing loss landscape. The authors also propose to apply mixup technique to UCL setting and present a LUMP algorithm to further improve the performance of CL.', 'main_review': 'Strong Points\n\n* The paper takes one of the most import issues in CL: learning robust representation in unsupervised setting. For me, the problem itself is real and practical.\n\n* The paper provides comprehensive experiments, including both qualitative analysis and quantitative results, to show the effectiveness of UCL and the proposed  LUMP algorithm over SCL methods.\n\n* Overall, the paper is well written. In particular, the Related Work section has a nice flow and puts the proposed method into context. Despite the method having limited novelty, the method has been well motivated by pointing out the limitations in SOTA methods.\n\n* The authors provide code for reproducing the results in the paper.\n\n\nWeak Points\n\n* The proposed LUMP algorithm is adopted from supervised mixup technique(Zhang et al, 2018). So the novelty is limited.\n\n* The authors conducted extensive experiments in task-incremental setting. It would be interesting to see how UCL and the proposed method perform in class-incremental and task-agnostic CL settings.\n\n* Fig 3: In general, higher layers have lower feature similarity than lower layers, and similarity between UCL models are higher than that of SCLs. However, there is an exception in Layer 4 of DER method -- the similarity of SCL is higher than that of UCL. It is worth some discussion on this exception.\n\n', 'summary_of_the_review': 'Overall, I vote for marginally accepting. I like the idea of unsupervised continual learning and handling it by the proposed LUMP method. My major concern is about the limited novelty of the proposed method -- adopted from mixup in supervised learning, and some additional experiments on class-incremental or task-agnostic settings(see weakness above). Hopefully the authors can address my concern in the rebuttal period.\n\n[After rebuttal]\nThe authors addressed most of my concerns, so I would like to raise my score.\n', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '4: The contributions are significant, and do not exist in prior works.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'The paper proposes to tackle the continual learning problem in an unsupervised setting. It shows that recent self-supervised learning methods are efficient tools to learn image representation with lower catastrophic learning problems. Two recent self-supervised methods are evaluated: SimSiam and BarlowTwins. In both cases, the superiority of unsupervised features is demonstrated. \n\nThe widely used mixup method is also adapted to the UCL problem. Straightforwardly, current images are mixed with images of the past tasks sampled from the replay buffer.', 'main_review': 'Strengths:\n- First the paper is well written and easy to read. \n- The experiments are rich and well-chosen to better understand the superiority of unsupervised representations in the context of CL. I especially appreciated the experiments in Fig 2 that investigate the impact of the size of the training dataset.\n- The conclusions are enlightening and will be very helpful to design new supervised or unsupervised CL methods.\n- The code is publicly available and looks clean and  easy to use.\n\nWeaknesses:\n- Some details are unclear: \n   - Sec 5.1: Knn classifier: which set is used for NN? the replay buffer? validation set? \n   - The Knn is used both for supervised and unsupervised experiments, right?\n   - Sec 5.3: more explanations about CKA are required for the reader that is not familiar with this measure\n- I recommend changing the title. ""Rethinking the Representational Continuity"" is much too strong. The conclusions of the paper are great but it does not provoke a real rethinking of the problem.\n- I am not really convinced but the visualization in Fig4. It seems that LUMP has sparser activations. The shapes of the objects are more clearly visible in its feature map. Does it simply mean that it learns lower-level features (similar to edge detector)? Maybe a TSNE visualization would help to see how the features of old tasks are affected when learning a new task.\n- In Fig.5, we can notice that the range of value gets smaller in T19 (from [4.6,5.6] for T0 to [4.4,4.6]). Any idea why?\n\n\n', 'summary_of_the_review': 'The paper shows interesting results that confirm the potential of self-supervised learning methods in the context of continual learning. The technical novelty may look incremental but the experimental conclusions are very interesting for the community. The paper is clear and well written.', 'correctness': '4: All of the claims and statements are well-supported and correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'}, {'summary_of_the_paper': 'This paper attempts to bridge the gap between **unsupervised representation learning** and **continual learning** by extending various supervised continual learning methods to the unsupervised learning framework.\nIt builds upon two recent unsupervised feature learning techniques (*SimSiam* and *BarlowTwins*) and a powerful data augmentation technique *MixUp*.\nImproved performances have been demonstrated in various experimental settings, together with comprehensive feature visualizations.', 'main_review': ""### Strengths\n\n* Rethinking continual learning with unsupervised representation learning is interesting, and empirical results indicate that most supervised continual learning methods can be improved by the proposed approach.\n\n* A bunch of experiments have been conducted to demonstrate the effectiveness of the proposed approach in various settings. And several visualizations have also been included for a better understanding of the learned features.\n\n### Weaknesses\n\n* **[Missed Comparison with CURL]** \\\nThough the authors criticised that *Continual Unsupervised Representation Learning framework (CURL)* to be limited by digit-based gray-scale datasets, no direct comparison with CURL is done by following their evaluation protocol. Adding this result could better reveal the difference between the proposed method and CURL in terms of effectiveness. \\\nBeside, the evaluation of cluster quality used in CURL seems to be an important evaluation metric in unsupervised continual learning, which has not been used in the paper.\n\n* **[Degraded Performance of DER and MULTITASK]** \\ \nIn the Table 1, we see that the proposed unsupervised continual learning can improve all baseline methods except DER and MULTITASK. A clear explanation about this performance drop should be added.\n\n* **[Qualitative Analysis]** \\ \nWhy the visualization of feature maps stops at $\\mathcal{T_{13}}$, while the loss landscape visualization continues to $\\mathcal{T_{19}}$ ? And in Figure 5, the difference between SCL-DER and LUMP is hard to interpret.\n\n* **[Why Not Directly Use Unsupervised Learned Presentations]** \\\nAs an important purpose of the unsupervised representation learning is to learn a powerful embedding space that can be quickly fine-tuned for latter down-stream tasks. Why don't we consider a baseline where the feature backbone is initialized with SimSiam or Barlow Twins, and directly fine-tune them on a sequence of tasks. This is probably not considered in the standard continual learning, but the results of this baseline could be informative to the community of both domains."", 'summary_of_the_review': 'This paper attempts to rethink the standard continual learning in a new point of view by considering unsupervised representation learning methods. This purpose is interesting, but the major difference between the SCL (supervised continual learning) and UCL (unsupervised continual learning) seems to be just adding a unsupervised representation loss at the backbone and freezing it in the second stage of predicting head finetuning.\nMoreover, some parts of the empirical results are not clearly presented or explained, an improved version of experimental results could be helpful for better validating the contributions.', 'correctness': '3: Some of the paper’s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.', 'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.', 'flag_for_ethics_review': ['NO.'], 'recommendation': '8: accept, good paper', 'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'}, {'title': 'Representational Continuity for Unsupervised Continual Learning', 'authorids': ['~Divyam_Madaan1', '~Jaehong_Yoon1', '~Yuanchun_Li1', '~Yunxin_Liu2', '~Sung_Ju_Hwang1'], 'authors': ['Divyam Madaan', 'Jaehong Yoon', 'Yuanchun Li', 'Yunxin Liu', 'Sung Ju Hwang'], 'keywords': ['Continual Learning', 'Representational Learning', 'Deep Learning'], 'abstract': ""Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations. "", 'code_of_ethics': '', 'submission_guidelines': '', 'resubmission': '', 'student_author': '', 'serve_as_reviewer': '', 'paperhash': 'madaan|representational_continuity_for_unsupervised_continual_learning', 'pdf': '/pdf/947f2c6dc3cd63a83d402bf9cbaddf42e362709e.pdf', 'one-sentence_summary': 'We attempt to bridge the gap between continual learning & representation learning and show that unsupervised continual learning achieves better performance and learns perceptual features with a smoother loss landscape than SCL.', '_bibtex': '@inproceedings{\nmadaan2022representational,\ntitle={Representational Continuity for Unsupervised Continual Learning},\nauthor={Divyam Madaan and Jaehong Yoon and Yuanchun Li and Yunxin Liu and Sung Ju Hwang},\nbooktitle={International Conference on Learning Representations},\nyear={2022},\nurl={https://openreview.net/forum?id=9Hrka5PA7LW}\n}', 'venue': 'ICLR 2022 Oral', 'venueid': 'ICLR.cc/2022/Conference'}]"
