Published as a conference paper at ICLR 2021
UNCERTAINTY SETS FOR IMAGE CLASSIFIERS USING
CONFORMAL PREDICTION
Anastasios N. Angelopoulos‚àó, Stephen Bates‚àó, Jitendra Malik, & Michael I. Jordan
Departments of Electrical Engineering and Computer Sciences and Statistics
University of California, Berkeley
{angelopoulos,stephenbates,malik,jordan}@cs.berkeley.edu
ABSTRACT
Convolutional image classiÔ¨Åers can achieve high predictive accuracy, but quanti-
fying their uncertainty remains an unresolved challenge, hindering their deploy-
ment in consequential settings. Existing uncertainty quantiÔ¨Åcation techniques,
such as Platt scaling, attempt to calibrate the network‚Äôs probability estimates, but
they do not have formal guarantees. We present an algorithm that modiÔ¨Åes any
classiÔ¨Åer to output a predictive set containing the true label with a user-speciÔ¨Åed
probability, such as 90%. The algorithm is simple and fast like Platt scaling, but
provides a formal Ô¨Ånite-sample coverage guarantee for every model and dataset.
Our method modiÔ¨Åes an existing conformal prediction algorithm to give more sta-
ble predictive sets by regularizing the small scores of unlikely classes after Platt
scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and
other classiÔ¨Åers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling
baseline.
1
INTRODUCTION
Imagine you are a doctor making a high-stakes medical decision based on diagnostic information
from a computer vision classiÔ¨Åer. What would you want the classiÔ¨Åer to output in order to make
the best decision? This is not a casual hypothetical; such classiÔ¨Åers are already used in medical
settings (e.g., Razzak et al., 2018; Lundervold & Lundervold, 2019; Li et al., 2014). A maximum-
likelihood diagnosis with an accompanying probability may not be the most essential piece of in-
formation. To ensure the health of the patient, you must also rule in or rule out harmful diagnoses.
In other words, even if the most likely diagnosis is a stomach ache, it is equally or more important
to rule out stomach cancer. Therefore, you would want the classiÔ¨Åer to give you‚Äîin addition to
an estimate of the most likely outcome‚Äîactionable uncertainty quantiÔ¨Åcation, such as a set of pre-
dictions that provably covers the true diagnosis with a high probability (e.g., 90%). This is called
a prediction set (see Figure 1). Our paper describes a method for constructing prediction sets from
any pre-trained image classiÔ¨Åer that are formally guaranteed to contain the true class with the de-
sired probability, relatively small, and practical to implement. Our method modiÔ¨Åes a conformal
predictor (Vovk et al., 2005) given in Romano et al. (2020) for the purpose of modern image classi-
Ô¨Åcation in order to make it more stable in the presence of noisy small probability estimates. Just as
importantly, we provide extensive evaluations and code for conformal prediction in computer vision.
Formally, for a discrete response Y ‚ààY = {1, . . . , K} and a feature vector X ‚ààRd, we desire an
uncertainty set function, C(X), mapping a feature vector to a subset of {1, . . . , K} such that
P(Y ‚ààC(X)) ‚â•1 ‚àíŒ±,
(1)
for a pre-speciÔ¨Åed conÔ¨Ådence level Œ± such as 10%. Conformal predictors like our method can mod-
ify any black-box classiÔ¨Åer to output predictive sets that are rigorously guaranteed to satisfy the
desired coverage property shown in Eq. (1). For evaluations, we focus on Imagenet classiÔ¨Åcation
‚àóEqual contribution. Blog: https://people.eecs.berkeley.edu/Àúangelopoulos/blog/
posts/conformal-classification
1

Published as a conference paper at ICLR 2021
Figure 1: Prediction set examples on Imagenet. We show three examples of the class fox squirrel and
the 95% prediction sets generated by RAPS to illustrate how the size of the set changes as a function of the
difÔ¨Åculty of a test-time image.
using convolutional neural networks (CNNs) as the base classiÔ¨Åers, since this is a particularly chal-
lenging testbed. In this setting, X would be the image and Y would be the class label. Note that the
guarantee in Eq. (1) is marginal over X and Y ‚Äîit holds on average, not for a particular image X.
A Ô¨Årst approach toward this goal might be to assemble the set by including classes from highest to
lowest probability (e.g., after Platt scaling and a softmax function; see Platt et al., 1999; Guo et al.,
2017) until their sum just exceeds the threshold 1 ‚àíŒ±. We call this strategy naive and formulate
it precisely in Algorithm 1. There are two problems with naive: Ô¨Årst, the probabilities output by
CNNs are known to be incorrect (Nixon et al., 2019), so the sets from naive do not achieve cov-
erage. Second, image classiÔ¨Åcation models‚Äô tail probabilities are often badly miscalibrated, leading
to large sets that do not faithfully articulate the uncertainty of the model; see Section 2.3. Moreover,
smaller sets that achieve the same coverage level can be generated with other methods.
The coverage problem can be solved by picking a new threshold using holdout samples. For exam-
ple, with Œ± =10%, if choosing sets that contain 93% estimated probability achieves 90% coverage
on the holdout set, we use the 93% cutoff instead. We refer to this algorithm, introduced in Ro-
mano et al. (2020), as Adaptive Prediction Sets (APS). The APS procedure provides coverage but
still produces large sets. To Ô¨Åx this, we introduce a regularization technique that tempers the in-
Ô¨Çuence of these noisy estimates, leading to smaller, more stable sets. We describe our proposed
algorithm, Regularized Adaptive Prediction Sets (RAPS), in Algorithms 2 and 3 (with APS as a
special case). As we will see in Section 2, both APS and RAPS are always guaranteed to satisfy
Eq. (1)‚Äîregardless of model and dataset. Furthermore, we show that RAPS is guaranteed to have
better performance than choosing a Ô¨Åxed-size set. Both methods impose negligible computational
requirements in both training and evaluation, and output useful estimates of the model‚Äôs uncertainty
on a new image given, say, 1000 held-out examples.
In Section 3 we conduct the most extensive evaluation of conformal prediction in deep learning to
date on Imagenet and Imagenet-V2. We Ô¨Ånd that RAPS sets always have smaller average size than
naive and APSsets. For example, using a ResNeXt-101, naive does not achieve coverage, while
APS and RAPS achieve it almost exactly. However, APS sets have an average size of 19, while RAPS
sets have an average size of 2 at Œ± = 10% (Figure 2 and Table 1). We will provide an accompanying
codebase that implements our method as a wrapper for any PyTorch classiÔ¨Åer, along with code to
exactly reproduce all of our experiments.
1.1
RELATED WORK
Reliably estimating predictive uncertainty for neural networks is an unsolved problem. Historically,
the standard approach has been to train a Bayesian neural network to learn a distribution over net-
work weights (Quinonero-Candela et al., 2005; MacKay, 1992; Neal, 2012; Kuleshov et al., 2018;
Gal, 2016). This approach requires computational and algorithmic modiÔ¨Åcations; other approaches
avoid these via ensembles (Lakshminarayanan et al., 2017; Jiang et al., 2018) or approximations of
Bayesian inference (Riquelme et al., 2018; Sensoy et al., 2018). These methods also have major
practical limitations; for example, ensembling requires training many copies of a neural network ad-
versarially. Therefore, the most widely used strategy is ad-hoc traditional calibration of the softmax
scores with Platt scaling (Platt et al., 1999; Guo et al., 2017; Nixon et al., 2019).
This work develops a method for uncertainty quantiÔ¨Åcation based on conformal prediction. Originat-
ing in the online learning literature, conformal prediction is an approach for generating predictive
sets that satisfy the coverage property in Eq. (1) (Vovk et al., 1999; 2005). We use a convenient
data-splitting version known as split conformal prediction that enables conformal prediction meth-
2

Published as a conference paper at ICLR 2021
0.9
0.95
0.99
desired coverage (1- )
0.85
0.90
0.95
1.00
empirical coverage
Naive
APS
RAPS
0.9
0.95
0.99
desired coverage (1- )
0
20
40
60
80
100
average size
Figure 2: Coverage and average set size on Imagenet for prediction sets from three methods. All methods
use a ResNet-152 as the base classiÔ¨Åer, and results are reported for 100 random splits of Imagenet-Val, each of
size 20K. See Section 3.1 for full details.
ods to be deployed for essentially any predictor (Papadopoulos et al., 2002; Lei et al., 2018). While
mechanically very different from traditional calibration as discussed above, we will refer to our
approach as conformal calibration to highlight that the two methodologies have overlapping but
different goals.
Conformal prediction is a general framework, not a speciÔ¨Åc algorithm‚Äîimportant design decisions
must be made to achieve the best performance for each context. To this end, Romano et al. (2020)
and Cauchois et al. (2020) introduce techniques aimed at achieving coverage that is similar across
regions of feature space, whereas Vovk et al. (2003); Hechtlinger et al. (2018) and Guan & Tib-
shirani (2019) introduce techniques aimed at achieving equal coverage for each class. While these
methods have conceptual appeal, thus far there has been limited empirical evaluation of this general
approach for state-of-the-art CNNs. Concretely, the only works that we are aware of that include
some evaluation of conformal methods on ImageNet‚Äîthe gold standard for benchmarking com-
puter vision methods‚Äîare Hechtlinger et al. (2018), Park et al. (2019), Cauchois et al. (2020), and
Messoudi et al. (2020), although in all four cases further experiments are needed to more fully eval-
uate their operating characteristics for practical deployment. At the heart of conformal prediction
is the conformal score - a measure of similarity between labeled examples which is used to com-
pare a new point to among those in a hold out set. Our theoretical contribution can be summarized
as a modiÔ¨Åcation of the conformal score from Romano et al. (2020) to have smaller, more stable
sets. Lastly, there are alternative approaches to returning prediction sets not based on conformal
prediction (Pearce et al., 2018; Zhang et al., 2018). These methods can be used as input to a con-
formal procedure to potentially improve performance, but they do not have Ô¨Ånite-sample coverage
guarantees when used alone.
2
METHODS
In developing uncertainty set methods to improve upon naive, we are guided by three desiderata.
First and most importantly, the coverage desideratum says the sets must provide 1 ‚àíŒ± coverage, as
discussed above. Secondly, the size desideratum says we want sets of small size, since these convey
more detailed information and may be more useful in practice. Lastly, the adaptiveness desidera-
tum says we want the sets to communicate instance-wise uncertainty: they should be smaller for
easy test-time examples than for hard ones; see Figure 1 for an illustration. Coverage and size
are obviously competing objectives, but size and adaptiveness are also often in tension. The size
desideratum seeks small sets, while the adaptiveness desideratum seeks larger sets when the classi-
Algorithm 1 Naive Prediction Sets
Input: Œ±, sorted scores s, associated permutation of classes I, boolean rand
1: procedure NAIVE(Œ±, s, I, rand)
2:
L ‚Üê1
3:
while PL
i=1 si < 1 ‚àíŒ± do
‚ñ∑Stop if 1 ‚àíŒ± probability exceeded
4:
L ‚ÜêL + 1
5:
if rand then
‚ñ∑Break ties randomly (explained in Appendix B)
6:
U ‚ÜêUnif(0, 1)
7:
V ‚Üê(PL
i=1 si ‚àí(1 ‚àíŒ±))/sL
8:
if U ‚â§V then
9:
L ‚ÜêL ‚àí1
10:
return

I1, ..., IL
	
Output: The 1 ‚àíŒ± prediction set,

I1, ..., IL
	
3

Published as a conference paper at ICLR 2021
œÑ^ccal
0
25%
50%
75%
1‚àíŒ±‚Ä≤
100%
œÑ: set size parameter
empirical coverage
(a) Conformal calibration
0.5
0.5
0.77
0.77
0.92
0.92
1.02
1.02
1.09
1.09 1.15
1.15
kreg
œÑ^ccal
0.0
0.2
0.4
c
d
f
b
e
a
class
penalized probability
penalty
probability
(b) A RAPS prediction set
Figure 3: Visualizations of conformal calibration and RAPS sets. In the left panel, the y-axis shows the
empirical coverage on the conformal calibration set, and 1 ‚àíŒ±‚Ä≤ = ‚åà(n + 1)(1 ‚àíŒ±)‚åâ/n. In the right panel, the
printed numbers indicate the cumulative probability plus penalty mass. For the indicated value ÀÜœÑccal, the RAPS
prediction set is {c, d, f, b}.
Ô¨Åer is uncertain. For example, always predicting a set of size Ô¨Åve could achieve coverage, but it is
not adaptive. As noted above, both APSand RAPS achieve correct coverage, and we will show that
RAPS improves upon APS according to the other two desiderata.
We now turn to the speciÔ¨Åcs of our proposed method. We begin in Subsection 2.1 by describing an
abstract data-splitting procedure called conformal calibration that enables the near-automatic con-
struction of valid predictive sets (that is, sets satisfying Eq. (1)). Subsequently, in Subsection 2.2, we
provide a detailed presentation of our procedure, with commentary in Section 2.3. In Subsection 2.4
we discuss the optimality of our procedure, proving that it is at least as good as the procedure that
returns sets of a Ô¨Åxed size, unlike alternative approaches.
2.1
CONFORMAL CALIBRATION
We Ô¨Årst review a general technique for producing valid prediction sets, following the articulation
in Gupta et al. (2019). Consider a procedure that outputs a predictive set for each observation,
and further suppose that this procedure has a tuning parameter œÑ that controls the size of the sets.
(In RAPS, œÑ is the cumulative sum of the sorted, penalized classiÔ¨Åer scores.) We take a small
independent conformal calibration set of data, and then choose the tuning parameter œÑ such that
the predictive sets are large enough to achieve 1 ‚àíŒ± coverage on this set. See Figure 3 for an
illustration. This calibration step yields a choice of œÑ, and the resulting set is formally guaranteed to
have coverage 1 ‚àíŒ± on a future test point from the same distribution; see Theorem 1 below.
Formally, let (Xi, Yi)i=1,...,n be an independent and identically distributed (i.i.d.) set of variables
that was not used for model training. Further, let C(x, u, œÑ) : Rd √ó [0, 1] √ó R ‚Üí2Y be a set-
valued function that takes a feature vector x to a subset of the possible labels. The second argument
u is included to allow for randomized procedures; let U1, . . . , Un be i.i.d. uniform [0, 1] random
variables that will serve as the second argument for each data point. Suppose that the sets are
indexed by œÑ such that they are nested, meaning larger values of œÑ lead to larger sets:
C(x, u, œÑ1) ‚äÜC(x, u, œÑ2)
if
œÑ1 ‚â§œÑ2.
(2)
To Ô¨Ånd a function that will achieve 1 ‚àíŒ± coverage on test data, we select the smallest œÑ that gives
at least 1 ‚àíŒ± coverage on the conformal calibration set, with a slight correction to account for the
Ô¨Ånite sample size:
ÀÜœÑccal = inf

œÑ : |{i : Yi ‚ààC(Xi, Ui, œÑ)}|
n
‚â•‚åà(n + 1)(1 ‚àíŒ±)‚åâ
n

.
(3)
The set function C(x, u, œÑ) with this data-driven choice of œÑ is guaranteed to have correct Ô¨Ånite-
sample coverage on a fresh test observation, as stated formally next.
Theorem 1 (Conformal calibration coverage guarantee).
Suppose (Xi, Yi, Ui)i=1,...,n and
(Xn+1, Yn+1, Un+1) are i.i.d. and let C(x, u, œÑ) be a set-valued function satisfying the nesting prop-
erty in Eq. (2). Suppose further that the sets C(x, u, œÑ) grow to include all labels for large enough
œÑ: for all x ‚ààRd, C(x, u, œÑ) = Y for some œÑ. Then for ÀÜœÑccal deÔ¨Åned as in Eq. (3), we have the
following coverage guarantee:
P

Yn+1 ‚ààC(Xn+1, Un+1, ÀÜœÑccal)

‚â•1 ‚àíŒ±.
4

Published as a conference paper at ICLR 2021
This is the same coverage property as Eq. (1) in the introduction, written in a more explicit manner.
The result is not new‚Äîa special case of this result leveraging sample-splitting Ô¨Årst appears in the
regression setting in Papadopoulos et al. (2002), and the core idea of conformal prediction was
introduced even earlier; see (Vovk et al., 2005).
As a technical remark, the theorem also holds if the observations to satisfy the weaker condition
of exchangeability; see Vovk et al. (2005). In addition, for most families of set-valued functions
C(x, u, œÑ) there is a matching upper bound:
P

Yn+1 ‚ààC(Xn+1, Un+1, ÀÜœÑccal)

‚â§1 ‚àíŒ± +
1
n + 1.
Roughly speaking, this will hold whenever the sets grow smoothly in œÑ. See Lei et al. (2018) for a
formal statement of the required conditions.
2.2
OUR METHOD
Conformal calibration is a powerful general idea, allowing one to achieve the coverage desideratum
for any choice of sets C(x, u, œÑ). Nonetheless, this is not yet a full solution, since the quality of the
resulting prediction sets can vary dramatically depending on the design of C(x, u, œÑ). In particular,
we recall the size and adaptiveness desiderata from Section 1‚Äîwe want our uncertainty sets to be
as small as possible while faithfully articulating the instance-wise uncertainty of each test point. In
this section, we explicitly give our algorithm, which can be viewed as a special case of conformal
calibration with the uncertainty sets C designed to extract information from CNNs.
Our algorithm has three main ingredients. First, for a feature vector x, the base model computes
class probabilities ÀÜœÄx ‚ààRk, and we order the classes from most probable to least probable. Then,
we add a regularization term to promote small predictive sets. Finally, we conformally calibrate the
penalized prediction sets to guarantee coverage on future test points.
Formally, let œÅx(y) = PK
y‚Ä≤=1 ÀÜœÄx(y‚Ä≤)I{ÀÜœÄx(y‚Ä≤)>ÀÜœÄx(y)} be the total probability mass of the set of labels
that are more likely than y. These are all the labels that will be included before y is included. In
addition, let ox(y) = |{y‚Ä≤ ‚ààY : ÀÜœÄx(y‚Ä≤) ‚â•ÀÜœÄx(y)}| be the ranking of y among the label based on the
probabilities ÀÜœÄ. For example, if y is the third most likely label, then ox(y) = 3.1 We take
C‚àó(x, u, œÑ) :=
n
y : œÅx(y) + ÀÜœÄx(y) ¬∑ u + Œª ¬∑ (ox(y) ‚àíkreg)+
|
{z
}
regularization
‚â§œÑ
o
,
(4)
where (z)+ denotes the positive part of z and Œª, kreg ‚â•0 are regularization hyperparameters that
are introduced to encourage small set sizes. See Figure 3 for a visualization of a RAPS predictive
set and Appendix E for a discussion of how to select kreg and Œª.
Since this is the heart of our proposal, we carefully parse each term. First, the œÅx(y) term increases
as y ranges from the most probable to least probable label, so our sets will prefer to include the y that
are predicted to be the most probable. The second term, ÀÜœÄx(y) ¬∑ u, is a randomized term to handle
the fact that the value will jump discretely with the inclusion of each new y. The randomization term
can never impact more than one value of y: there is at most one value of y such that y ‚ààC(x, 0, œÑ)
but y /‚ààC(x, 1, œÑ). These Ô¨Årst two terms can be viewed as the CDF transform after arranging the
classes from most likely to least likely, randomized in the usual way to result in a continuous uniform
random variable (cf. Romano et al., 2020). We discuss randomization further in Appendix B.
Lastly, the regularization promotes small set sizes: for values of y that occur farther down the ordered
list of classes, the term Œª ¬∑ (ox(y) ‚àíkreg)+ makes that value of y require a higher value of œÑ before
it is included in the predictive set. For example, if kreg = 5, then the sixth most likely value of y
has an extra penalty of size Œª, so it will never be included until œÑ exceeds œÅx(y) + ÀÜœÄx(y) ¬∑ u + Œª,
whereas it enters when œÑ exceeds œÅx(y) + ÀÜœÄx(y) ¬∑ u in the nonregularized version. Our method has
the following coverage property:
Proposition 1 (RAPS coverage guarantee). Suppose (Xi, Yi, Ui)i=1,...,n and (Xn+1, Yn+1, Un+1)
are i.i.d. and let C‚àó(x, u, œÑ) be deÔ¨Åned as in Eq. (4). Suppose further that ÀÜœÄx(y) > 0 for all x and
y. Then for ÀÜœÑccal deÔ¨Åned as in Eq. (3), we have the following coverage guarantee:
1 ‚àíŒ± ‚â§P

Yn+1 ‚ààC‚àó(Xn+1, Un+1, ÀÜœÑccal)

‚â§1 ‚àíŒ± +
1
n + 1.
1For ease of notation, we assume distinct probabilities. Else, label-ordering ties should be broken randomly.
5

Published as a conference paper at ICLR 2021
Algorithm 2 RAPS Conformal Calibration
Input: Œ±; s ‚àà[0, 1]n√óK, I ‚àà{1, ..., K}n√óK, and one-hot y ‚àà{0, 1}K corresponding respectively
to the sorted scores, the associated permutation of indexes, and labels for each of n examples in
the calibration set; kreg; Œª; boolean rand
1: procedure RAPSC(Œ±,s,I,y,Œª)
2:
for i ‚àà{1, ¬∑ ¬∑ ¬∑ , n} do
3:
Li ‚Üê{ j : Ii,j = yi }
4:
Ei ‚ÜêŒ£Li
j=0si,j + Œª(Li ‚àíkreg + 1)+
5:
if rand then
6:
U ‚àºUnif(0, 1)
7:
Ei ‚ÜêEi ‚àísi,Li + U ‚àósi,Li
8:
ÀÜœÑccal ‚Üêthe ‚åà(1 ‚àíŒ±)(1 + n)‚åâlargest value in {Ei}n
i=1
9:
return ÀÜœÑccal
Output: The generalized quantile, ÀÜœÑccal
‚ñ∑The value in Eq. (3)
Algorithm 3 RAPS Prediction Sets
Input: Œ±, sorted scores s and the associated permutation of classes I for a test-time example, ÀÜœÑccal
from Algorithm 2, kreg, Œª, boolean rand
1: procedure RAPS(Œ±, s, I, ÀÜœÑccal, kreg, Œª, rand)
2:
L ‚Üê| j ‚ààY : Œ£j
i=0si + Œª(L ‚àíkreg)+ ‚â§ÀÜœÑccal | + 1
3:
V ‚Üê(ÀÜœÑccal ‚àíŒ£L‚àí1
i=0 si ‚àíŒª(L ‚àíkreg)+ + sL‚àí1)/sL‚àí1
4:
if rand & V ‚â§U ‚àºUnif(0, 1) then
5:
L ‚ÜêL ‚àí1
6:
return C =

I1, ...IL
	
‚ñ∑The L most likely classes
Output: The 1 ‚àíŒ± conÔ¨Ådence set, C
‚ñ∑The set in Eq. (4)
Note that the Ô¨Årst inequality is a corollary of Theorem 1, and the second inequality is a special case
of the remark in Section 2.1. The restriction that ÀÜœÄx(y) > 0 is not necessary for the Ô¨Årst inequality.
2.3
WHY REGULARIZE?
In our experiments, the sets from APS are larger than necessary, because APS is sensitive to the
noisy probability estimates far down the list of classes. This noise leads to a permutation problem of
unlikely classes, where ordering of the classes with small probability estimates is determined mostly
by random chance. If 5% of the true classes from the calibration set are deep in the tail due to the
permutation problem, APS will choose large 95% predictive sets; see Figure 2. The inclusion of the
RAPS regularization causes the algorithm to avoid using the unreliable probabilities in the tail; see
Figure 4. We discuss how RAPS improves the adaptiveness of APS in Section 4 and Appendix E.
2.4
OPTIMALITY CONSIDERATIONS
To complement these experimental results, we now formally prove that RAPS with the correct reg-
ularization parameters will always dominate the simple procedure that returns a Ô¨Åxed set size. (Sec-
tion 3.5 shows the parameters are easy to select and RAPS is not sensitive to their values). For a
feature vector x, let ÀÜy(j)(x) be the label with the jth highest predicted probability. We deÔ¨Åne the
top-k predictive sets to be {ÀÜy(1)(x), . . . , ÀÜy(k)(x)}.
Proposition 2 (RAPS dominates top-k sets). Suppose (Xi, Yi, Ui)i=1,...,n and (Xn+1, Yn+1, Un+1)
are i.i.d. draws. Let k‚àóbe the smallest k such that the top-k predictive sets have coverage at least
‚åà(n + 1)(1 ‚àíŒ±)‚åâ/n on the conformal calibration points (Xi, Yi)i=1,...,n. Take C‚àó(x, u, œÑ) as in Eq.
(4) with any kreg ‚â§k‚àóand Œª = 1. Then with ÀÜœÑccal chosen as in Eq. (3), we have
C‚àó(Xn+1, Un+1, ÀÜœÑccal) ‚äÜ{ÀÜy(1)(x), . . . , ÀÜy(k‚àó)(x)}.
In words, the RAPS procedure with heavy regularization will be at least as good as the top-k proce-
dure in the sense that it has smaller or same average set size while maintaining the desired coverage
level. This is not true of either the naive baseline or the APS procedure; Table 2 shows that these
two procedures usually return predictive sets with size much larger than k‚àó.
3
EXPERIMENTS
6

Published as a conference paper at ICLR 2021
Accuracy
Coverage
Size
Model
Top-1
Top-5
Top K
Naive
APS
RAPS
Top K
Naive
APS
RAPS
ResNeXt101
0.793
0.945
0.900
0.889
0.900
0.900
2.42
17.1
19.7
2.00
ResNet152
0.783
0.94
0.900
0.895
0.900
0.900
2.63
9.78
10.4
2.11
ResNet101
0.774
0.936
0.900
0.896
0.900
0.900
2.83
10.3
10.7
2.25
ResNet50
0.761
0.929
0.900
0.896
0.900
0.900
3.14
11.8
12.3
2.57
ResNet18
0.698
0.891
0.900
0.895
0.900
0.900
5.72
15.5
16.2
4.43
DenseNet161
0.771
0.936
0.900
0.894
0.900
0.900
2.84
11.2
12.1
2.29
VGG16
0.716
0.904
0.900
0.895
0.901
0.900
4.75
13.4
14.1
3.54
Inception
0.695
0.887
0.900
0.885
0.900
0.901
6.30
75.4
89.1
5.32
ShufÔ¨ÇeNet
0.694
0.883
0.900
0.891
0.900
0.900
6.46
28.9
31.9
5.05
Table 1: Results on Imagenet-Val. We report coverage and size of the optimal, randomized Ô¨Åxed sets, naive,
APS, and RAPS sets for nine different Imagenet classiÔ¨Åers. The median-of-means for each column is reported
over 100 different trials. See Section 3.1 for full details.
In this section we report on experiments that study the performance of the predictive sets from
naive, APS, and RAPS, evaluating each based on the three desiderata above. We begin with a brief
preview of the experiments. In Experiment 1, we evaluate naive, APS, and RAPS on Imagenet-
Val. Both APS and RAPS provided almost exact coverage, while naive sets had coverage slightly
below the speciÔ¨Åed level. APS has larger sets on average than naive and RAPS. RAPS has a
much smaller average set size than APS and naive. In Experiment 2, we repeat Experiment 1 on
Imagenet-V2, and the conclusions still hold. In Experiment 3, we produce histograms of set sizes
for naive, APS, and RAPS for several different values of Œª, illustrating a simple tradeoff between
set size and adaptiveness. In Experiment 4, we compute histograms of RAPS sets stratiÔ¨Åed by
image difÔ¨Åculty, showing that RAPS sets are smaller for easier images than for difÔ¨Åcult ones. In
Experiment 5, we report the performance of RAPS with many values of the tuning parameters.
In our experiments, we use nine standard, pretrained Imagenet classiÔ¨Åers from the torchvision
repository (Paszke et al., 2019) with standard normalization, resize, and crop parameters. Before
applying naive, APS, or RAPS, we calibrated the classiÔ¨Åers using the standard temperature scal-
ing/Platt scaling procedure as in Guo et al. (2017) on the calibration set. Thereafter, naive, APS,
and RAPS were applied, with RAPS using a data-driven choice of parameters described in Ap-
pendix E. We use the randomized versions of these algorithms‚Äîsee Appendix B for a discussion.
3.1
EXPERIMENT 1: COVERAGE VS SET SIZE ON IMAGENET
In this experiment, we calculated the coverage and mean set size of each procedure for two different
choices of Œ±. Over 100 trials, we randomly sampled two subsets of Imagenet-Val: one conformal
calibration subset of size 20K and one evaluation subset of size 20K. The median-of-means over
trials for both coverage and set size are reported in Table 1. Figure 2 illustrates the performances of
naive, APS, and RAPS; RAPS has much smaller sets than both naive and APS, while achieving
coverage. We also report results from a conformalized Ô¨Åxed-k procedure, which Ô¨Ånds the smallest
Ô¨Åxed set size achieving coverage on the holdout set, k‚àó, then predicts sets of size k‚àó‚àí1 or k‚àóon
new examples in order to achieve exact coverage; see Algorithm 4 in Appendix E.
3.2
EXPERIMENT 2: COVERAGE VS SET SIZE ON IMAGENET-V2
The same procedure as Experiment 1 was repeated on Imagenet-V2, with exactly the same normal-
ization, resize, and crop parameters. The size of the calibration and evaluation sets was 5K, since
Imagenet-V2 is a smaller dataset. The result shows that our method can still provide coverage even
for models trained on different distributions, as long as the conformal calibration set comes from the
new distribution. The variance of the coverage is higher due to having less data.
3.3
EXPERIMENT 3: SET SIZES OF N A I V E, APS, AND RAPS ON IMAGENET
We investigate the effect of regularization in more detail. For three values of Œª, we collected the set
sizes produced by each of naive, APS, and RAPS and report their histograms in Figure 4.
3.4
EXPERIMENT 4: ADAPTIVENESS OF RAPS ON IMAGENET
We now show that RAPS sets are smaller for easy images than hard ones, addressing the adaptiveness
desideratum. Table 4 reports the size-stratiÔ¨Åed coverages of RAPS at the 90% level with kreg = 5
and different choices of Œª. When Œª is small, RAPS allows sets to be large. But when Œª = 1, RAPS
7

Published as a conference paper at ICLR 2021
Accuracy
Coverage
Size
Model
Top-1
Top-5
Top K
Naive
APS
RAPS
Top K
Naive
APS
RAPS
ResNeXt101
0.678
0.874
0.900
0.888
0.899
0.899
7.48
43.0
50.8
6.18
ResNet152
0.67
0.876
0.899
0.896
0.900
0.900
7.18
25.8
27.2
5.69
ResNet101
0.657
0.859
0.901
0.894
0.900
0.898
9.21
28.7
30.7
6.93
ResNet50
0.634
0.847
0.898
0.894
0.899
0.900
10.3
30.3
32.3
7.80
ResNet18
0.572
0.802
0.902
0.895
0.900
0.900
17.5
35.3
37.4
13.3
DenseNet161
0.653
0.862
0.902
0.895
0.901
0.901
8.6
29.9
32.4
6.93
VGG16
0.588
0.817
0.902
0.897
0.900
0.899
15.1
31.9
32.8
11.2
Inception
0.573
0.797
0.900
0.893
0.900
0.899
21.8
145.0
155.0
20.5
ShufÔ¨ÇeNet
0.559
0.781
0.899
0.892
0.900
0.899
26.0
66.2
71.7
22.5
Table 2: Results on Imagenet-V2. We report coverage and size of the optimal, randomized Ô¨Åxed sets, naive,
APS, and RAPS sets for nine different Imagenet classiÔ¨Åers. The median-of-means for each column is reported
over 100 different trials at the 10% level. See Section 3.2 for full details.
0
25
50
75
100
size
10
1
10
2
10
3
frequency
=0.01
0
25
50
75
100
size
10
1
10
2
10
3
=0.1
0
25
50
75
100
size
10
1
10
2
10
3
=1
method
Naive
APS
RAPS
Figure 4: Set sizes produced with ResNet-152. See Section 3.3 for details.
clips sets to be a maximum of size 5. Table 7 (in the Appendix) stratiÔ¨Åes by image difÔ¨Åculty, showing
that RAPS sets are small for easy examples and large for hard ones. Experiments 3 and 4 together
illustrate the tradeoff between adaptiveness and size: as the average set size decreases, the RAPS
procedure truncates sets larger than the smallest Ô¨Åxed set that provides coverage, taming the heavy
tail of the APS procedure. Since RAPS with large Œª undercovers hard examples, it must compensate
by taking larger sets for easy examples to ensure the 1 ‚àíŒ± marginal coverage guarantee. However,
the size only increases slightly since easy images are more common than hard ones, and the total
probability mass can often exceed ÀÜœÑccal by including only one more class. If this behavior is not
desired, we can instead automatically pick Œª to optimize the adaptiveness of RAPS; see Section 4.
3.5
EXPERIMENT 5: CHOICE OF TUNING PARAMETERS
While any value of the tuning parameters Œª and kreg lead to coverage (Proposition 1), some values
will lead to smaller sets. In Experiments 1 and 2, we chose kreg and Œª adaptively from data (see
Appendix E), achieving strong results for all models and choices of the coverage level. Table 3 gives
the performance of RAPS with many choices of kreg and Œª for ResNet-152.
4
ADAPTIVENESS AND CONDITIONAL COVERAGE
In this section, we point to a deÔ¨Ånition of adaptiveness that is more natural for the image classiÔ¨Å-
cation setting than the existing notion of conditional coverage. We show that APS does not satisfy
conditional coverage, and that RAPS with small Œª outperforms it in terms of adaptiveness.
We say that a set-valued predictor C : Rd ‚Üí2Y satisÔ¨Åes exact conditional coverage if P(Y ‚àà
C(X) | X = x) = 1 ‚àíŒ± for each x. Distribution-free guarantees on conditional coverage are im-
possible (Vovk, 2012; Lei & Wasserman, 2014), but many algorithms try to satisfy it approximately
(Romano et al., 2019; 2020; Cauchois et al., 2020). In a similar spirit, Tibshirani et al. (2019) sug-
gest a notion of local conditional coverage, where one asks for coverage in a neighborhood of each
point, weighted according to a chosen kernel. Cauchois et al. (2020) introduce the worst-case slab
metric for measuring violations of the conditional coverage property. We present a different way of
measuring violations of conditional coverage.
Proposition 3. Suppose P(Y ‚ààC(X) | X = x) = 1 ‚àíŒ± for each x ‚ààRd. Then,
P(Y ‚ààC(X) | {|C(X)| ‚ààA}) = 1 ‚àíŒ± for any A ‚äÇ{0, 1, 2, . . . }.
In words, if conditional coverage holds, then coverage holds after stratifying by set size. Based on
this result, In Appendix E, we introduce the size-stratiÔ¨Åed coverage violation criterion, a simple and
pragmatic way of quantifying adaptiveness. Then, we automatically tune Œª on this metric so RAPS
markedly outperforms the adaptiveness of APS (see Table 8).
8

Published as a conference paper at ICLR 2021
kreg|Œª
0
1e-4
1e-3
0.01
0.02
0.05
0.2
0.5
0.7
1.0
1
11.2
10.2
7.0
3.6
2.9
2.3
2.1
2.3
2.2
2.2
2
11.2
10.2
7.1
3.7
3.0
2.4
2.1
2.3
2.2
2.2
5
11.2
10.2
7.2
3.9
3.4
2.9
2.6
2.5
2.5
2.5
10
11.2
10.2
7.4
4.5
4.0
3.6
3.4
3.4
3.4
3.4
50
11.2
10.6
8.7
7.2
7.0
6.9
6.9
6.9
6.9
6.9
Table 3: Set sizes of RAPS with parameters kreg and Œª, a ResNet-152, and coverage level 90%.
Œª = 0
Œª = 0.001
Œª = 0.01
Œª = 0.1
Œª = 1
size
cnt
cvg
cnt
cvg
cnt
cvg
cnt
cvg
cnt
cvg
0 to 1
11627
0.88
11539
0.88
11225
0.89
10476
0.92
10027
0.93
2 to 3
3687
0.91
3702
0.91
3741
0.92
3845
0.93
3922
0.94
4 to 6
1239
0.91
1290
0.91
1706
0.92
4221
0.89
6051
0.83
7 to 10
688
0.93
765
0.93
1314
0.91
1436
0.71
0
11 to 100
2207
0.94
2604
0.93
2014
0.86
22
0.59
0
101 to 1000
552
0.97
100
0.90
0
0
0
Table 4: Coverage conditional on set size. We report average coverage of images stratiÔ¨Åed by the size of the
set output by RAPS using a ResNet-152 for varying Œª. The marginal coverage rate is 90%.
In Table 4, we report on the coverage of APS and RAPS, stratiÔ¨Åed by the size of the prediction set.
Turning our attention to the Œª = 0 column, we see that when APS outputs a set of size 101 ‚àí1000,
APS has coverage 97%, substantially higher than 90% nominal rate. By Proposition 3, we conclude
that APS is not achieving exact conditional coverage, because the scores are far from the oracle
probabilities. The APS procedure still achieves marginal coverage by overcovering hard examples
and undercovering easy ones, an undesirable behavior. Alternatively, RAPS can be used to regularize
the set sizes‚Äîfor Œª = .001 to Œª = .01 the coverage stratiÔ¨Åed by set size is more balanced. In
summary, even purely based on the adaptiveness desideratum, RAPS with light regularization is
preferable to APS. Note that as the size of the training data increases, as long as ÀÜœÄ is consistent,
naive and APS will become more stable, and so we expect less regularization will be needed.
Lastly, we argue that conditional coverage is a poor notion of adaptiveness when the best possible
model (i.e., one Ô¨Åt on inÔ¨Ånite data) has high accuracy. Given such a model, the oracle procedure
from Romano et al. (2020) would return the correct label with probability 1 ‚àíŒ± and the empty set
with probability Œ±. That is, having correct conditional coverage for high-signal problems where Y is
perfectly determined by X requires a perfect classiÔ¨Åer. In our experiments on ImageNet, APS does
not approximate this behavior. Therefore, conditional coverage isn‚Äôt the right goal for prediction
sets with realistic sample sizes. Proposition 3 suggests a relaxation. We could require that we have
the right coverage, no matter the size of the prediction set: P(Y ‚ààC(X) | {|C(x)| ‚ààA}) ‚â•1 ‚àíŒ±
for any A ‚äÇ{0, 1, 2, . . . }; Appendix E.2 develops this idea. We view this as a promising way to
reason about adaptiveness in high-signal problems such as image classiÔ¨Åcation.
5
DISCUSSION
For classiÔ¨Åcation tasks with many possible labels, our method enables a researcher to take any base
classiÔ¨Åer and return predictive sets guaranteed to achieve a pre-speciÔ¨Åed error level, such as 90%,
while retaining small average size. It is simple to deploy, so it is an attractive, automatic way to
quantify the uncertainty of image classiÔ¨Åers‚Äîan essential task in such settings as medical diag-
nostics, self-driving vehicles, and Ô¨Çagging dangerous internet content. Predictive sets in computer
vision (from RAPS and other conformal methods) have many further uses, since they systematically
identify hard test-time examples. Finding such examples is useful in active learning where one only
has resources to label a small number of points. In a different direction, one can improve efÔ¨Åciency
of a classiÔ¨Åer by using a cheap classiÔ¨Åer outputting a prediction set Ô¨Årst, and an expensive one only
when the cheap classiÔ¨Åer outputs a large set (a cascade; see, e.g., Li et al. 2015), and Fisch et al.
(2021) for an implementation of conformal prediction in this setting. One can also use predictive sets
during model development to identify failure cases and outliers and suggest strategies for improving
its performance. Prediction sets are most useful for problems with many classes; returning to our
initial medical motivation, we envision RAPS could be used by a doctor to automatically screen for
a large number of diseases (e.g. via a blood sample) and refer the patient to relevant specialists.
9

Published as a conference paper at ICLR 2021
REFERENCES
Maxime Cauchois, Suyash Gupta, and John Duchi. Knowing what you know: valid and validated
conÔ¨Ådence sets in multiclass and multilabel prediction. 2020. arXiv:2004.10181.
Adam Fisch, Tal Schuster, Tommi S. Jaakkola, and Regina Barzilay. EfÔ¨Åcient conformal predic-
tion via cascaded inference with expanded admission. In International Conference on Learning
Representations, 2021.
Yarin Gal. Uncertainty in Deep Learning. University of Cambridge, 1(3), 2016.
Leying Guan and Rob Tibshirani. Prediction and outlier detection in classiÔ¨Åcation problems. 2019.
arXiv:1905.04396.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural
networks. 2017. arXiv:1706.04599.
Chirag Gupta, Arun K Kuchibhotla, and Aaditya K Ramdas.
Nested conformal prediction and
quantile out-of-bag ensemble methods. arXiv, pp. arXiv‚Äì1910, 2019.
Yotam Hechtlinger, Barnab¬¥as P¬¥oczos, and Larry Wasserman.
Cautious deep learning, 2018.
arXiv:1805.09460.
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classiÔ¨Åer. In
Advances in Neural Information Processing Systems, pp. 5541‚Äì5552, 2018.
Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. 2018. arXiv:1807.00263.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, pp. 6402‚Äì6413, 2017.
Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71‚Äì96, 2014.
Jing Lei, Max G‚ÄôSell, Alessandro Rinaldo, Ryan J. Tibshirani, and Larry Wasserman. Distribution-
free predictive inference for regression. Journal of the American Statistical Association, 113
(523):1094‚Äì1111, 2018. doi: 10.1080/01621459.2017.1307116.
Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua. A convolutional neural
network cascade for face detection. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5325‚Äì5334, 2015.
Qing Li, Weidong Cai, Xiaogang Wang, Yun Zhou, David Dagan Feng, and Mei Chen. Medical
image classiÔ¨Åcation with convolutional neural network. In International Conference on Control
Automation Robotics & Vision, pp. 844‚Äì848. IEEE, 2014.
Alexander SelvikvÀöag Lundervold and Arvid Lundervold. An overview of deep learning in medical
imaging focusing on MRI. Zeitschrift f¬®ur Medizinische Physik, 29(2):102‚Äì127, 2019.
David JC MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of
Technology, 1992.
Soundouss Messoudi, Sylvain Rousseau, and Sebastien Destercke. Deep conformal prediction for
robust models. In Information Processing and Management of Uncertainty in Knowledge-Based
Systems, pp. 528‚Äì540. Springer, 2020.
Radford M Neal. Bayesian Learning for Neural Networks. Springer Science & Business Media,
2012.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-
ing calibration in deep learning. In CVPR Workshops, pp. 38‚Äì41, 2019.
10

Published as a conference paper at ICLR 2021
Harris Papadopoulos, Kostas Proedrou, Vladimir Vovk, and Alex Gammerman. Inductive conÔ¨Å-
dence machines for regression. In Machine Learning: European Conference on Machine Learn-
ing ECML 2002, pp. 345‚Äì356, 2002.
Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. PAC conÔ¨Ådence sets for deep neural
networks via calibrated prediction. 2019. arXiv:2001.00106.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8026‚Äì8037, 2019.
Tim Pearce, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. High-quality prediction inter-
vals for deep learning: A distribution-free, ensembled approach. In International Conference on
Machine Learning, pp. 6473‚Äì6482, 2018.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in Large Margin ClassiÔ¨Åers, 10(3):61‚Äì74, 1999.
Joaquin Quinonero-Candela, Carl Edward Rasmussen, Fabian Sinz, Olivier Bousquet, and Bern-
hard Sch¬®olkopf. Evaluating predictive uncertainty challenge. In Machine Learning Challenges
Workshop, pp. 1‚Äì27. Springer, 2005.
Muhammad Imran Razzak, Saeeda Naz, and Ahmad Zaib. Deep learning for medical image process-
ing: Overview, challenges and the future. In ClassiÔ¨Åcation in BioApps, pp. 323‚Äì350. Springer,
2018.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep Bayesian bandits showdown: An empir-
ical comparison of Bayesian deep networks for Thompson sampling. 2018. arXiv:1802.09127.
Yaniv Romano, Evan Patterson, and Emmanuel Cand`es. Conformalized quantile regression. In
Advances in Neural Information Processing Systems, pp. 3543‚Äì3553. 2019.
Yaniv Romano, Matteo Sesia, and Emmanuel J. Cand`es. ClassiÔ¨Åcation with valid and adaptive
coverage. 2020. arXiv:2006.02544.
Mauricio Sadinle, Jing Lei, and Larry Wasserman. Least ambiguous set-valued classiÔ¨Åers with
bounded error levels. Journal of the American Statistical Association, 114(525):223‚Äì234, 2019.
Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classiÔ¨Åca-
tion uncertainty. In Advances in Neural Information Processing Systems, pp. 3179‚Äì3189, 2018.
Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal predic-
tion under covariate shift. In Advances in Neural Information Processing Systems, pp. 2530‚Äì2540.
2019.
Vladimir Vovk. Conditional validity of inductive conformal predictors. In Proceedings of the Asian
Conference on Machine Learning, volume 25, pp. 475‚Äì490, 2012.
Vladimir Vovk, Alexander Gammerman, and Craig Saunders. Machine-learning applications of
algorithmic randomness. In International Conference on Machine Learning, pp. 444‚Äì453, 1999.
Vladimir Vovk, David Lindsay, Ilia Nouretdinov, and Alex Gammerman. Mondrian conÔ¨Ådence
machine. Technical report, 2003. Technical report.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World.
Springer, 2005.
Chong Zhang, Wenbo Wang, and Xingye Qiao. On reject and reÔ¨Åne options in multicategory clas-
siÔ¨Åcation. Journal of the American Statistical Association, 113(522):730‚Äì745, 2018.
11

Published as a conference paper at ICLR 2021
A
PROOFS
Theorem 1. Let s(x, u, y) = infœÑ{y ‚ààC(x, u, œÑ)}, and let si = s(Xi, Ui, Yi) for i = 1, . . . , n.
Then
{y : s(x, u, y) ‚â§œÑ} = {y : y ‚ààC(x, u, œÑ)}
because C(x, u, œÑ) is a Ô¨Ånite set growing in œÑ by the assumption in Eq. (2). Thus,
{œÑ : |{i : si ‚â§œÑ} | ‚â•‚åà(1 ‚àíŒ±)(n + 1)‚åâ} =

œÑ : |{i : Yi ‚ààC(Xi, Ui, œÑ)}|
n
‚â•‚åà(n + 1)(1 ‚àíŒ±)‚åâ
n

.
Considering the left expression, the inÔ¨Åmum over œÑ of the set on the left hand side is the
‚åà(1
‚àí
Œ±)(n
+
1)‚åâ
smallest
value
of
the
si,
so
this
is
the
value
of
ÀÜœÑccal.
Since
s1, . . . , sn, s(Xn+1, Un+1, Yn+1)
are
exchangeable
random
variables,
|{i
:
s(Xn+1, Un+1, Yn+1) > si}| is stochastically dominated by the discrete uniform distribution
on {0, 1, . . . , n}. We thus have that
P (Yn+1 /‚ààC(Xn+1, Un+1, ÀÜœÑccal)) = P (s(Xn+1, Un+1, Yn+1) > ÀÜœÑccal)
= P (|{i : s(Xn+1, Un+1, Yn+1) > si}| ‚â•‚åà(n + 1)(1 ‚àíŒ±)‚åâ)
= P
|{i : s(Xn+1, Un+1, Yn+1) > si}|
n + 1
‚â•‚åà(n + 1)(1 ‚àíŒ±)‚åâ
n + 1

‚â§Œ±.
Proposition 1. The lower bound follows from Theorem 1. To prove the upper bound, using the
result from Theorem 2.2 of Lei et al. (2018) it sufÔ¨Åces to show that the variables s(Xi, Ui, Yi) =
inf{œÑ : Yi ‚ààC(Xi, Ui, œÑ)} are almost surely distinct. To this end, note that that
s(Xi, Ui, Yi) = œÅXi(Yi) + ÀÜœÄXi(Yi) ¬∑ Ui + Œª(oXi(Yi) ‚àíkreg)+,
and due to the middle term of the sum, these values are distinct almost surely provided ÀÜœÄXi(Yi) >
0.
Proposition 2. We Ô¨Årst show that ÀÜœÑccal ‚â§1 + k‚àó‚àíkreg. Note that since at least ‚åà(1 ‚àíŒ±)(n + 1)‚åâ
of the conformal calibration points are covered by a set of size k‚àó, at least ‚åà(1 ‚àíŒ±)(n + 1)‚åâof the
Ei in Algorithm 2 are less than or equal to 1 + k‚àó‚àíkreg. Thus, by the deÔ¨Ånition of ÀÜœÑccal, we have
that it is less than or equal to 1 + k‚àó‚àíkreg. Then, note that by the deÔ¨Ånition of C‚àóin Eq. (4), we
have that
|C‚àó(Xn+1, Un+1, ÀÜœÑccal)| ‚â§k‚àó.
as long as ÀÜœÑccal ‚â§1+k‚àó‚àíkreg, since for the k‚àó+1 most likely class, the sum in Eq. (4) will exceed
Œª ¬∑ (1 + k‚àó‚àíkreg) = (1 + k‚àó‚àíkreg) ‚â•ÀÜœÑccal, and so the k‚àó+ 1 class will not be in the set.
Proposition 3. Suppose P(Y ‚ààC(X) | X = x) = 1 ‚àíŒ± for each x ‚ààRd. Then,
P(Y ‚ààC(X) | |C(X)| ‚ààA) =
R
x P(Y ‚ààC(x) | X = x})I{|C(x)|‚ààA}dP(x)
P(|C(X)| ‚ààA)
=
R
x(1 ‚àíŒ±)I{|C(x)|‚ààA}dP(x)
P(|C(X)| ‚ààA)
= 1 ‚àíŒ±.
12

Published as a conference paper at ICLR 2021
Accuracy
Coverage
Size
Model
Top-1
Top-5
Top K
Naive
APS
RAPS
Top K
Naive
APS
RAPS
ResNeXt101
0.794
0.945
0.905
0.938
0.950
0.950
5.64
36.4
46.3
4.21
ResNet152
0.783
0.940
0.950
0.943
0.950
0.950
6.36
19.6
22.5
4.40
ResNet101
0.774
0.936
0.950
0.944
0.950
0.950
6.79
20.6
23.2
4.79
ResNet50
0.762
0.929
0.951
0.943
0.950
0.950
8.12
22.9
26.2
5.57
ResNet18
0.698
0.891
0.950
0.943
0.950
0.950
16.0
28.9
33.2
11.7
DenseNet161
0.772
0.936
0.950
0.942
0.950
0.950
6.84
23.4
28.0
5.09
VGG16
0.716
0.904
0.950
0.943
0.950
0.950
12.9
24.6
27.8
8.98
Inception
0.695
0.887
0.950
0.937
0.950
0.950
20.3
142.0
168.0
18.5
ShufÔ¨ÇeNet
0.694
0.883
0.950
0.940
0.950
0.950
19.3
58.7
71.6
16.3
Table 5: Results on Imagenet-Val. We report coverage and size of the optimal, randomized Ô¨Åxed sets, naive,
APS, and RAPS sets for nine different Imagenet classiÔ¨Åers. The median-of-means for each column is reported
over 100 different trials. See Section 3.1 for full details.
Accuracy
Coverage
Size
Model
Top-1
Top-5
Top K
Naive
APS
RAPS
Top K
Naive
APS
RAPS
ResNeXt101
0.678
0.875
0.950
0.937
0.950
0.950
21.7
86.2
107.0
18.5
ResNet152
0.670
0.876
0.951
0.944
0.950
0.950
21.3
51.0
56.6
16.2
ResNet101
0.656
0.86
0.95
0.944
0.950
0.949
25.7
55.8
63.1
19.1
ResNet50
0.634
0.847
0.949
0.944
0.949
0.950
29.5
58.6
65.9
21.5
ResNet18
0.572
0.802
0.950
0.942
0.950
0.949
48.3
65.0
74.0
35.3
DenseNet161
0.653
0.861
0.951
0.941
0.950
0.949
25.9
60.0
72.7
20.4
VGG16
0.588
0.816
0.950
0.943
0.950
0.949
38.5
57.8
63.9
26.4
Inception
0.573
0.797
0.950
0.943
0.949
0.950
73.1
253.0
275.0
70.2
ShufÔ¨ÇeNet
0.560
0.781
0.950
0.941
0.949
0.949
80.0
125.0
140.0
67.4
Table 6: Results on Imagenet-V2. We report coverage and size of the optimal, randomized Ô¨Åxed sets, naive,
APS, and RAPS sets for nine different Imagenet classiÔ¨Åers. The median-of-means for each column is reported
over 100 different trials at the 5% level. See Section 3.2 for full details.
B
RANDOMIZED PREDICTORS
The reader may wonder why we choose to use a randomized procedure. The randomization is
needed to achieve 1 ‚àíŒ± coverage exactly, which we will explain via an example. Note that the
randomization is of little practical importance, since the predictive set output by the randomized
procedure will differ from the that of the non-randomized procedure by at most one element.
Turning to an example, assume for a particular input image we expect a set of size k to have 91%
coverage, and a set of size k ‚àí1 to have 89% coverage. In order to achieve our desired coverage of
90%, we randomly choose size k or k ‚àí1 with equal probability. In general, the probabilities will
not be equal, but rather chosen so the weighted average of the two coverages is exactly 90%. If a
user of our method desires deterministic sets, it is easy to turn off this randomization with a single
Ô¨Çag, resulting in slightly conservative sets.
C
IMAGENET AND IMAGENETV2 RESULTS FOR Œ± = 5%
We repeated Experiments 1 and 2 with Œ± = 5%. See the results in Tables 5 and 6.
D
COVERAGE AND SIZE CONDITIONAL ON IMAGE DIFFICULTY
In order to probe the adaptiveness properties of APS and RAPS we stratiÔ¨Åed coverage and size by
image difÔ¨Åculty (the position of the true label in the list of most likely to least likely classes, based
on the classiÔ¨Åer predictions) in Table 7. With increasing Œª, coverage decreases for more difÔ¨Åcult
images and increases for easier ones. In the most difÔ¨Åcult regime, even though APS can output large
sets, those sets still rarely contain the true class. This suggests regularization is a sensible way to
stabilize the sets. As a Ô¨Ånal word on Table 7, notice that as Œª increases, coverage improves for the
more common medium-difÔ¨Åculty examples, although not for very rare and difÔ¨Åcult ones.
13

Published as a conference paper at ICLR 2021
Œª = 0
Œª = 0.001
Œª = 0.01
Œª = 0.1
Œª = 1
difÔ¨Åculty
count
cvg
sz
cvg
sz
cvg
sz
cvg
sz
cvg
sz
1
15668
0.95
5.2
0.95
3.8
0.96
2.5
0.97
2.0
0.98
2.0
2 to 3
2578
0.78
15.7
0.78
10.5
0.80
6.0
0.84
3.9
0.86
3.6
4 to 6
717
0.68
31.7
0.68
19.7
0.70
9.7
0.71
5.3
0.64
4.4
7 to 10
334
0.63
41.0
0.63
24.9
0.60
11.6
0.22
5.7
0.00
4.5
11 to 100
622
0.55
57.8
0.51
34.1
0.26
14.7
0.00
6.4
0.00
4.6
101 to 1000
81
0.23
96.7
0.00
51.6
0.00
19.1
0.00
7.1
0.00
4.7
Table 7: Coverage and size conditional on difÔ¨Åculty. We report coverage and size of RAPS sets using ResNet-
152 with kreg = 5 and varying Œª (recall that Œª = 0 is the APS procedure). The desired coverage level is 90%.
The ‚ÄòdifÔ¨Åculty‚Äô is the ranking of the true class‚Äôs estimated probability.
Violation at Œ± = 10%
Violation at Œ± = 5%
Model
APS
RAPS
APS
RAPS
ResNeXt101
0.090
0.049
0.048
0.021
ResNet152
0.069
0.038
0.037
0.017
ResNet101
0.073
0.041
0.038
0.017
ResNet50
0.069
0.037
0.037
0.016
ResNet18
0.046
0.025
0.032
0.019
DenseNet161
0.080
0.047
0.040
0.016
VGG16
0.046
0.022
0.030
0.022
Inception
0.085
0.045
0.043
0.023
ShufÔ¨ÇeNet
0.061
0.033
0.035
0.020
Table 8: Adaptiveness results after automatically tuning Œª. We report the median size-stratiÔ¨Åed coverage
violations of APS and RAPS over 10 trials. See Appendix E.2 for experimental details.
E
CHOOSING kreg AND Œª TO OPTIMIZE SET SIZE AND ADAPTIVENESS
This section describes two procedures for picking kreg and Œª that optimize for set size or adaptive-
ness, outperforming APS in both cases.
E.1
OPTIMIZING SET SIZE WITH RAPS
Algorithm 4 Adaptive Fixed-K
Input: Œ±; I ‚àà{1, ..., K}n√óK, and one-hot y ‚àà{0, 1}K corresponding respectively to the classes
from highest to lowest estimated probability mass, and labels for each of n examples in the
dataset
1: procedure GET-KSTAR(Œ±,I,y)
2:
for i ‚àà{1, ¬∑ ¬∑ ¬∑ , n} do
3:
Li ‚Üê{ j : Ii,j = yi }
4:
ÀÜk‚àó‚Üêthe ‚åà(1 ‚àíŒ±)(1 + n)‚åâlargest value in {Li}n
i=1
5:
return ÀÜk‚àó
Output: The estimate of the smallest Ô¨Åxed size set that achieves coverage, ÀÜk‚àó
To produce Tables 1, 5, 2, and 6, we chose kreg and Œª adaptively. This required an extra data splitting
step, where a small amount of tuning data

xi, yi
	m
i=1 were used to estimate k‚àó, and then kreg is set
to k‚àó. Taking m ‚âà1000 was sufÔ¨Åcient, since the algorithm is fairly insensitive to kreg (see Table 3).
Then, ÀÜk‚àówas calculated with Algorithm 4. We produced the Imagenet V2 tables with m = 1000
and the Imagenet tables with m = 10000.
After choosing ÀÜk‚àó, we chose Œª to have small set size. We used the same tuning data to pick ÀÜk‚àóand Œª
for simplicity (this does not invalidate our coverage guarantee since conformal calibration still uses
fresh data). A coarse grid search on Œª sufÔ¨Åced, since small parameter variations have little impact
on RAPS. For example, we chose the Œª ‚àà{0.001, 0.01, 0.1, 0.2, 0.5} that achieved the smallest size
on the m holdout samples in order to produce Tables 1, 5, 2, and 6. We include a subroutine that
automatically chooses ÀÜk‚àóand Œª to optimize size in our GitHub codebase.
14

Published as a conference paper at ICLR 2021
E.2
OPTIMIZING ADAPTIVENESS WITH RAPS
In this appendix, we show empirically that RAPS with an automatically chosen set of kreg and Œª
improves the adaptiveness of APS. Recall our discussion in Section 4 and Proposition 3, wherein we
propose size-stratiÔ¨Åed coverage as a useful deÔ¨Ånition of adaptiveness in image classiÔ¨Åcation. After
picking kreg as in Appendix E, we can choose Œª using the same tuning data to optimize this notion
of adaptiveness.
We now describe a particular manifestation of our adaptiveness criterion that we will use to optimize
Œª. Consider disjoint set-size strata {Si}i=s
i=1, where Sj=s
j=1 Si = {1, . . . , |Y|}. Then deÔ¨Åne the indexes
of examples stratiÔ¨Åed by the prediction set size of each example from algorithm C as Jj =

i :
|C(Xi, Yi, Ui)| ‚ààSj
	
. Then we can deÔ¨Åne the size-stratiÔ¨Åed coverage violation of an algorithm C
on strata {S}i=s
i=1 as
SSCV
 C, {S}j=s
j=1

= sup
j

|{i : Yi ‚ààC(Xi, Yi, Ui), i ‚ààJj}|
|Jj|
‚àí(1 ‚àíŒ±)
.
(5)
In words, Eq. (5) is the worst-case deviation of C from exact coverage when it outputs sets of a
certain size. Computing the size-stratiÔ¨Åed coverage violation thus only requires post-stratifying the
results of C on a set of labeled examples. If conditional coverage held, the worst stratum coverage
violation would be 0 by Proposition 3.
To maximize adaptiveness, we‚Äôd like to choose Œª to minimize the size-stratiÔ¨Åed coverage violation
of RAPS. Write CŒª to mean the RAPS procedure for a Ô¨Åxed choice of kreg and Œª. Then we would
like to pick
Œª = arg min
Œª‚Ä≤
SSCV(CŒª‚Ä≤, {S}j=s
j=1).
(6)
In our experiments, we choose a relatively coarse partitioning of the possible set sizes: 0-1, 2-3, 4-
10, 11-100, and 101-1000. Then, we chose the Œª ‚àà{0.00001, 0.0001, 0.0008, 0.001, 0.0015, 0.002}
which minimized the size-stratiÔ¨Åed coverage violation on the tuning set. The results in Table 8
show RAPS always outperforms the adaptiveness of APS on the test set, even with this coarse,
automated choice of parameters. The table reports the median size-stratiÔ¨Åed coverage violation over
10 independent trials of APS and RAPS with automated parameter tuning.
F
COMPARISON WITH LEAST AMBIGUOUS SET-VALUED CLASSIFIERS
In this section, we compare RAPS to the Least Ambiguous Set-valued ClassiÔ¨Åer (LAC) method
introduced in Sadinle et al. (2019), an alternative conformal procedure that is designed to have small
sets. The LAC method provable gives the smallest possible average set size in the case where the
input probabilities are correct, with the idea that these sets should be small even when the estimated
probabilities are only approximately correct. In the notation of this paper, the LAC method considers
nested sets of the following form:
CLAC(x, œÑ) := {y : ÀÜœÄx(y) ‚â•1 ‚àíœÑ},
which can be calibrated using as before in using ÀÜœÑccal from Eq. (3).
We Ô¨Årst compare naive, APS, RAPS, and LAC in terms of power and coverage in Table 9. In this
experiment, we tuned RAPS to have small set size as described in Appendix E.1. We see that LAC
also achieves correct coverage, as expected since it is a conformal method and satisÔ¨Åes the guarantee
from Theorem 1. We further see that it has systematically smaller sets that RAPS, although the
difference is slight compared to the gap between APS and RAPS or APS and LAC.
We next compare RAPS to LAC in terms of adaptiveness, tuning RAPS as in Section E.2. First, in
Table 10, we report on the coverage of LAC for images of different difÔ¨Åculties, and see that LAC
has dramatically worse coverage for hard images than for easy ones. Comparing this to RAPS in
Table 7, we see that RAPS also has worse coverage for more difÔ¨Åcult images, although the gap is
much smaller for RAPS. Next, in Table 11, we report on the SSCV metric for of adaptiveness (and
conditional coverage) for APS, RAPS, and LAC. We Ô¨Ånd that APS and RAPS have much better
adaptiveness than LAC, with RAPS being the overall winner. The results of all of these comparisons
are expected: LAC is not targetting adpativeness and instead trying to achieve the smallest possible
set size. It succeeds at its goal, sacriÔ¨Åcing adaptiveness to do so.
15

Accuracy
Coverage
Size
Model
Top-1
Top-5
Top K
Naive
APS
RAPS
LAC
Top K
Naive
APS
RAPS
LAC
ResNeXt101
0.793
0.945
0.900
0.889
0.900
0.900
0.900
2.42
17.2
19.9
2.01
1.65
ResNet152
0.783
0.941
0.900
0.894
0.900
0.900
0.900
2.64
9.68
10.4
2.09
1.76
ResNet101
0.774
0.936
0.900
0.895
0.900
0.900
0.900
2.83
10.0
10.8
2.25
1.87
ResNet50
0.761
0.929
0.899
0.896
0.900
0.900
0.900
3.13
11.7
12.3
2.55
2.05
ResNet18
0.698
0.891
0.900
0.895
0.900
0.900
0.900
5.74
15.3
16.1
4.38
3.64
DenseNet161
0.771
0.936
0.900
0.894
0.900
0.900
0.900
2.84
11.2
12.0
2.29
1.90
VGG16
0.716
0.904
0.900
0.896
0.901
0.900
0.900
4.75
13.4
14.1
3.54
3.00
Inception
0.695
0.886
0.899
0.884
0.900
0.899
0.900
6.27
74.8
88.8
5.24
4.06
ShufÔ¨ÇeNet
0.694
0.883
0.900
0.892
0.900
0.899
0.900
6.45
28.8
32.1
5.01
4.14
Table 9: Results on Imagenet-Val. We report coverage and size of the optimal, randomized Ô¨Åxed sets, naive, APS, RAPS , and the LAC sets for nine different Imagenet classiÔ¨Åers.
The median-of-means for each column is reported over 100 different trials at the 10% level. See Section 3.1 for full details.

Published as a conference paper at ICLR 2021
difÔ¨Åculty
count
cvg
sz
1
15668
1.00
1.5
2 to 3
2578
0.81
2.6
4 to 6
717
0.23
3.0
7 to 10
334
0.00
2.9
11 to 100
622
0.00
2.7
101 to 1000
81
0.00
2.4
Table 10: Coverage and size conditional on difÔ¨Åculty. We report coverage and size of the LAC sets for
ResNet-152.
Violation at Œ± = 10%
Violation at Œ± = 5%
Model
APS
RAPS
LAC
APS
RAPS
LAC
ResNeXt101
0.086
0.047
0.217
0.047
0.022
0.127
ResNet152
0.067
0.039
0.156
0.04
0.021
0.141
ResNet101
0.075
0.060
0.152
0.039
0.015
0.120
ResNet50
0.071
0.042
0.131
0.037
0.014
0.109
ResNet18
0.050
0.024
0.196
0.031
0.021
0.059
DenseNet161
0.076
0.055
0.140
0.039
0.016
0.110
VGG16
0.051
0.023
0.165
0.029
0.019
0.070
Inception
0.086
0.042
0.181
0.043
0.023
0.135
ShufÔ¨ÇeNet
0.058
0.030
0.192
0.033
0.019
0.045
Table 11: Adaptiveness results after automatically tuning Œª. We report the median SSCV of APS RAPS
and LAC over 10 trials. See Appendix E.2 for experimental details.
17

