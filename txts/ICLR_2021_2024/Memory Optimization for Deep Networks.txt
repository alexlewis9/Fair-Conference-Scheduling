Published as a conference paper at ICLR 2021
MEMORY OPTIMIZATION FOR DEEP NETWORKS
Aashaka Shah1, Chao-Yuan Wu1, Jayashree Mohan1, Vijay Chidambaram1,2, Philipp Kr¨ahenb¨uhl1
1University of Texas at Austin
2VMware Research
{aashaka,cywu,jaya,vijay,philkr}@cs.utexas.edu
ABSTRACT
Deep learning is slowly, but steadily, hitting a memory bottleneck. While the ten-
sor computation in top-of-the-line GPUs increased by 32× over the last ﬁve years,
the total available memory only grew by 2.5×. This prevents researchers from
exploring larger architectures, as training large networks requires more memory
for storing intermediate outputs. In this paper, we present MONET, an automatic
framework that minimizes both the memory footprint and computational overhead
of deep networks. MONET jointly optimizes the checkpointing schedule and the
implementation of various operators. MONET is able to outperform all prior hand-
tuned operations as well as automated checkpointing. MONET reduces the overall
memory requirement by 3× for various PyTorch models, with a 9-16% overhead
in computation. For the same computation cost, MONET requires 1.2-1.8× less
memory than current state-of-the-art automated checkpointing frameworks. Our
code is available at https://github.com/utsaslab/MONeT.
1
INTRODUCTION
Deep networks are widely used in domains ranging from image classiﬁcation (Krizhevsky et al.,
2012; Simonyan & Zisserman, 2015; He et al., 2016) to video recognition (Wu et al., 2019; Fe-
ichtenhofer et al., 2019) or natural language processing (Devlin et al., 2019; Yang et al., 2019).
However, training deep networks is resource-intensive. In particular, the amount of GPU memory
bottlenecks training many deep networks (Dong et al., 2016; Kim et al., 2016; Chen et al., 2018;
Child et al., 2019). This bottleneck requires either modifying the network architecture or scaling
training to multiple nodes, incurring signiﬁcant overheads.
We presents MONET, an automatic framework to minimize memory footprint for deep networks.
MONET jointly optimizes global compute-graph-level techniques (such as checkpointing) and lo-
cal techniques (such as memory-efﬁcient implementations of individual operator). At the heart of
MONET is a theoretical analysis that enables joint optimization and provides tight bounds on mem-
ory consumption. We analyze the memory consumption and computational cost of a general for-
ward and backward pass under changing local operator implementations and a global checkpointing
schedule. Speciﬁcally, we are able to tightly bound the peak memory consumption for network for-
ward, backward, and recomputation stages. MONET uses these constraints to optimize for the most
efﬁcient forward and backward implementation both locally and globally under a ﬁxed memory bud-
get. We linearize all memory bounds, and express both implementation selection and checkpointing
as a 0-1 integer program, which we solve using standard solvers.
We conduct extensive experiments, demonstrating that MONET signiﬁcantly outperforms ex-
isting automatic frameworks that use local or global techniques.
On multiple architectures
(ResNet (He et al., 2016), VGG (Simonyan & Zisserman, 2015), UNet (Ronneberger et al., 2015),
GoogleNet (Szegedy et al., 2015), MobileNet-V2 (Sandler et al., 2018)), memory budgets (5-10
GB), and network conﬁgurations (multiple resolutions), MONET consistently achieves lower mem-
ory footprints at equivalent or lower computational overhead. MONET reduces the overall memory
requirement by 3× for various models, with a 9-16% overhead in computation. For the same com-
putation cost, MONET requires 1.2-1.8× less memory than the current state-of-the-art automated
checkpointing framework. The results achieved by MONET demonstrate the power of jointly opti-
mizing global checkpointing schedules and local operator implementations.
1

Published as a conference paper at ICLR 2021
Figure 1: Memory Optimized Network Training (MONeT), an automatic framework that mini-
mizes the memory footprint of deep networks by jointly optimizing global and local techniques.
2
RELATED WORK
There are two broad families of approaches to reduce the memory footprint of a deep network during
training: operator-level implementation changes, and global, graph-level optimizations. The novel
aspect of MONET is that it is able to combine both approaches and ﬁnd the optimal mix of local and
global techniques for a given network.
Operator-Speciﬁc Optimizations. Researchers have found creative ways to implement individ-
ual operators or groups of operators in a more memory-efﬁcient manner. Standard deep learning
frameworks (Jia et al., 2014; Collobert et al., 2011; Paszke et al., 2019; Abadi et al., 2016) provide
different implementations of certain operators that trade computation for intermediate memory use.
These implementation are chosen according to local search heuristics, and are not globally optimal.
Gist (Jain et al., 2018) proposes several hand-crafted optimizations such as storing only ReLU signs.
RevNets (Gomez et al., 2017) redesigns a ResNet (He et al., 2016) architecture making each network
block reversible, thereby eliminating the need to store intermediate activations for backpropagation.
Memory-efﬁcient DenseNets (Pleiss et al., 2017) reduce memory utilized for feature maps by re-
computing all intermediate feature maps during the backward pass with a small compute overhead.
In-place activated batchnorm (Bul`o et al., 2018) or ReLU layers use output activations to compute
their gradients, thus reusing a single memory buffer for the gradient computation in consecutive lay-
ers. Mixed precision training (Micikevicius et al., 2018) uses half precision (FP16) instead of single
precision (FP32) for all tensors and arithmetic during training, reducing the memory by nearly half.
While training at precision lower than FP16 results in loss of training quality (Banner et al., 2018),
prior work like backpropagation with approximate activations (Chakrabarti & Moseley, 2019) care-
fully quantize certain intermediate outputs (activations) to 4 bits, resulting in signiﬁcant memory
savings. Although these hand-crafted techniques independently result in memory savings, there is
no one-size-ﬁts-all recipe, and different implementations perform best on different architectures.
In contrast, MONET automatically ﬁnds the best implementation for each forward and backward
operator given a memory budget.
Checkpointing. Chen et al. (2016) proposed dividing a network into different segments, dropping
all intermediate outputs within each segment, and recomputing them later. Chen et al. use √n equal
segments, trading memory savings for the cost of an extra forward pass. Checkmate (Jain et al.,
2019) solves the problem in a more general setting, using an mixed-integer linear program solver
to decide which layers to recompute for a given network. Like Checkmate, our work optimizes
a checkpointing schedule, but on a different computation graph. Our computation graph allows
for the optimization of an entire execution plan jointly ﬁnding a checkpointing schedule and the
best implementation of each forward and backward operator. In Checkmate, changes in operator
implementation induce a different computation graph, and could thus not directly be optimized.
Appendix F highlights some of the difﬁculties of adding operator optimizations into Checkmate.
In summary, while much work has been done on local optimizations (operator implementations)
and global compute-graph-level techniques (automated checkpointing), MONET is the ﬁrst system
to jointly optimize a given architecture using both local and global techniques.
2

Published as a conference paper at ICLR 2021
Algorithm 1: Forward Pass
Input : Inputs, θ, a schedule (s, r).
Output: Output tensor
1 SN = {}
/* Saved tensors for backward */
2 L = {inputs, θ} /* Local tensors for forward */
3 for i = 1 . . . N do
4
xi = forwardi(L)
5
Add xi to L
6
Remove all tensors from L that are not used later
7
if sN
i then
8
Add xi to SN
9 return L
Algorithm 2: Backward Pass
Input : Loss gradients, inputs, θ, SN, (s, r).
Output: Output tensor
1
ˆL = {loss gradients} /* Local backward tensors */
2 for k = N . . . 1 do
3
L = Sk
/* Local forward tensors */
4
Sk−1 = {}
/* Saved tensors */
5
for i = 1 . . . N do
6
if rk
i then
7
xi = forwardi(L)
8
Add xi to L
9
Remove all tensors from L not used later
10
if sk−1
i
then
11
Add xi to Sk−1
/* use xi ∈L */
12
yk = backwardk(ˆL, L)
13
Add yk to ˆL
14
Remove tensors from ˆL that are not used later
Figure 2: Schematic overview of the forward and backward passes. The algorithms include ag-
gressive memory savings by greedily freeing unused tensors, and allow for a general checkpointing
schedule (s, r) to be executed.
3
PRELIMINARIES
Let the forward pass of a CNN with parameters Θ be expressed as a directed-acyclic graph (DAG),
where each node i ∈{1, . . . , N} corresponds to an operator forwardi, and edges (i, j) ∈E specify
the data-ﬂow dependencies, i.e., the output of operator i is used as input in operator j. Without loss
of generality, computational dependency (i, j) ∈E implies i < j. Let Nj = {i : (i, j) ∈E} be the
set of all incoming edges of an operation j.
We will ﬁrst discuss the forward pass through a network and the basic form of a backward pass using
checkpointing. The backward pass reverses all computational dependency expressed in our DAG,
and induces certain dependencies on forward activations. We call these checkpoint dependencies
Dk. They are either saved or recomputed depending on a schedule (s, r). Checkpointing creates a
trade-off between computation and memory consumption. To highlight this trade-off, we formally
compute the amount of memory consumed in both forward and backward passes, which allows us
to optimize for the ideal execution plan in Sec. 4. We provide a reference to the notations introduced
in this section and the next along with their explanations in Appendix A.
The Forward Pass. Alg. 1 shows a general overview of the forward pass in a deep network, as
implemented in standard deep learning frameworks (Jia et al., 2014; Collobert et al., 2011; Paszke
et al., 2019; Abadi et al., 2016). The algorithm proceeds in increasing order of index i. Each operator
forwardi(·) depends on a set of tensors L stored in local memory. These tensors include model pa-
rameters Θ, computational dependencies Ni, and tensors stored for later forward operators, i.e. skip
or residual activations (He et al., 2016). At each iteration, we add any output tensors of forwardi to
the local memory L. Early deep learning frameworks (Jia et al., 2014; Collobert et al., 2011) strictly
grew the set of local tensors L leading to an unnecessarily high memory consumption. Modern
graph-based frameworks (Paszke et al., 2019; Abadi et al., 2016) reduce the memory footprint by
aggressively pruning local memory L and freeing any tensor that is no longer used in later compu-
tations. Some output activations xi are used in the backward pass, and have to be saved for later.
We use a checkpointing schedule sN to determine which. Formally, sN
i ∈{0, 1} indicates whether
the activation of node i is stored during the forward pass. An activation which is not stored will be
recomputed if it is needed during the backward pass.
Analyzing peak memory consumption of the forward pass. Only the forwardi operator (Alg.
1 L. 4) allocates memory. All other operators perform mere bookkeeping on existing tensor. It is
thus sufﬁcient to study the peak memory consumption mN
i in forwardi for each node i. Let Li, SN
i
be the set of local tensors L and saved tensors S while calling forwardi respectively. Li includes
all parameters and computational dependencies for this and later forward passes Li = Θ ∪{xj :
j ∈Nt for any t ≥i and j < i}. Li is constant and computed ahead of time. The schedule sN
determines the set of saved tensors SN
i
= {xj : sN
j
= 1 for j < i}. In addition, each forward
3

Published as a conference paper at ICLR 2021
operator uses a certain amount of workspace memory ci to store intermediate results. The total
memory consumption of a forward operator is thus
mi = ci + |xi| + |SN
i ∪Li| = ci + |xi| +
X
xj∈Li
|xj| +
X
j<i:xj /∈Li
|xj|sN
j ,
(1)
where | · | refers to the memory consumed by a tensor or set of tensors. Most of the memory
consumption is constant and does not depend on the schedule.
The Backward Pass. The backward pass proceeds in a reverse order, as summarized in Alg. 2.
backwardk(·) of each node k depends on a set of gradient tensors ˆL and forward tensors {xi : i ∈
Dk}. Any gradients required by the current and later backward passes are stored in local memory
ˆL. Dependencies Dk may either be stored in Sk or need to be recomputed from checkpoints in Sk.
Recomputation involves forward computation of one or more nodes, which increases computational
overhead, and allows for a new set of tensors Sk−1 to be saved. After recomputation, all dependen-
cies Dk are kept in memory. The backward operation produces a gradient for each input tensor of
the original forward operation, which is added to ˆL if required for a later backward computation.
We aggressively remove tensors in ˆL that are not required.
Analyzing the peak memory consumption of the backward pass. Peak memory consumption
ˆmk again only depends on the forwardi (Alg. 2 L. 7) and backwardk (Alg. 2 L. 12) operations. For
the backwardk operation, let ˆck be the workspace memory, ˆLk be the set of gradient tensors stored,
Dk = {xi : i ∈Dk} be the forward tensors used, and Sk−1 be the set of newly saved tensors. Here
ˆLk and Dk can be pre-computed. The total memory consumption for the backwardk call is
ˆmk = ˆck + |yk| + |Sk−1 ∪ˆLk ∪Dk| = ˆck + |yk| +
X
yl∈ˆLk
|yl| +
X
xi∈Dk
|xi| +
X
xi /∈Dk
sk−1
i
|xi|. (2)
Here again, only the last term depends on the checkpointing schedule, while the rest is a constant.
Analyzing the peak memory consumption of the recomputation. Finally, the peak memory ˜mk
i
for the forwardi call (Alg. 2 L. 7) depends on the set of local tensors L, checkpoint dependencies
D, saved tensors S, and gradient tensors ˆL, named Lk
i , Dk, Sk−1
i
, ˆLk respectively. Following the
forward pass:
˜mk
i = ci + |xi| + |ˆLk| + |Sk−1
i
∪Lk
i ∪Dk|
= ci + |xi| + |ˆLk| +
X
j<i:xj /∈Lk
i ∪Dk
sk−1
j
|xj| +
X
j<i:xj∈Lk
i ∪Dk
|xj| +
X
j>i
sk
j |xj|.
(3)
Unlike the forward pass, Lk
i is no longer constant, but depends on past saved tensors and future
recomputations in (s, r): Lk
i = Θ ∪{xj : j ∈Nt for any t ≥i with rk
t = 1 and j < i}.
In the next section, we show how to take this formalization of the forward and backward pass, and
ﬁnd an optimal execution plan including checkpointing schedule (s, r), forwardi implementations,
and backwardk implementations, under a ﬁxed memory budget.
4
METHOD
Our goal is to ﬁnd a global checkpointing schedule (s, r) and local forwardi/backwardk implemen-
tations that jointly minimize the computation cost τ within a memory budget M. We show how to
express this optimization in a 0-1 integer program and efﬁciently solve it. To this end, we linearize
any peak memory consumption constraints, ensure that the checkpointing schedule is valid, and
solve to minimize a computation cost objective. We keep track of the three contributors to memory
and computational cost - forward pass, backward pass, and recomputation of forward operators.
Memory Constraints. Consider the case of basic checkpointing using only a single implementation
for forwardi and backwardk. The memory consumption of the forward 1 and backward 2 pass are
linear in s, and thus efﬁciently expressed in an integer program. However, recomputation depends
both on sk−1 and rk in a non-linear manner through the local memory Lk
i . This joint dependence on
optimization variables gives rise to quadratic constraints, which cannot directly be incorporated into
4

Published as a conference paper at ICLR 2021
an integer program. For simplicity in this derivation, we bound the set of local tensors from above,
assuming every future tensor is recomputed. We give more information about this in Appendix B.
The upper bound ¯Lk
i is constant, yielding a linear upper bound ¯mk
i of the recomputation memory
˜mk
i analogous to Eq. 3. The set of memory constraints is thus
mi ≤M
∀i
and
ˆmk ≤M
∀k
and
¯mk
i ≤M
∀k,i
(4)
To enable operator optimization, we use a bit-vector δ to indicate the selection of an operator imple-
mentation. We add δ to the constraints which allows us to jointly optimize checkpointing (s, r) and
operator implementations δ.
Forward Operator Optimization. Let each forward operator forwardi have multiple different
implementations Ii = {a, b, c, . . .}. For examples, convolution may be implemented using matrix
multiplication, the Winograd algorithm (Winograd, 1980), a Fourier transform, etc. (Chetlur et al.,
2014). All implementations follow the same DAG structure, and thus use the same dependencies Ni.
However, each implementation trades workspace memory {ca
i , cb
i, . . .} for computational efﬁciency
{τ a
i , τ b
i , . . .} in a different manner. Our experiments show that this trade-off is often complex.
Our goal is to represent the peak memory when using multiple forwardi implementations in the
forward pass and recomputation. Let δi,a ∈{0, 1} indicate that implementation a ∈Ii is used
for forwardi in the forward pass. Each forward operator should use exactly one implementation
P
l δi,l = 1. The choice of implementation determines the operator’s computational cost P
l τ l
iδi,l
and workspace memory ci = P
l cl
iδi,l.
Analogously, each recomputation of forwardi during
backwardk chooses between implementations δk
i,a ∈{0, 1} when needed P
l δk
i,l = rk
i , with equiv-
alent cost estimates P
l τ l
iδk
i,l and workspace memory use ck
i = P
l cl
iδk
i,l. In this formulation, all
additional memory requirements remain linear and are directly integrated into the linear memory
constraints or their linear relaxations (equation 4).
Backward Operator Optimization. Let each backward operator backwardk have a set of differ-
ent implementations ˆIk = {a, b, c, . . .}. Each implementation again trades workspace memory
{ˆca
k, ˆcb
k, . . .} for computational cost {ˆτ a
k , ˆτ b
k, . . .}. While gradient tensors follow the ﬁxed DAG
structure, different implementations may depend on different forward activations {Da
k, Db
k, . . .}. For
example, in-place activated operators (Bul`o et al., 2018) depend on their output activation, while reg-
ular operators use the input activation. This change in the dependency structure makes optimizing
for backward-operator implementations challenging.
We again aim to represent memory in terms of implementations for each backwardk operator. Let
ˆδk,a ∈{0, 1} indicate that implementation a ∈ˆIk is used at node k in the backward pass. Each
backward operator should use exactly one implementation P
l ˆδk,l = 1, with a computational cost
P
l ˆτ l
kˆδk,l and workspace memory ˆck = P
l ˆcl
kˆδk,l. The workspace memory adds a linear constraint
to the memory consumption ˆmk equation 2.
The biggest changes to the optimization problem, comes from the changing dependency structure.
Dk is no longer constant. Instead, the implementation of a backward operator changes the set of
computational dependencies Dk obtained from Dl
k. To deal with this changing dependency struc-
ture, we use the indicator vector ˆδk to select memory contribution of dependencies from the chosen
implementation. This changes the backward memory consumption to
ˆmk =
X
l
ˆcl
kˆδk,l
|
{z
}
ˆck
+|yk| + |ˆLk| +
X
l
ˆδk,l.|Dl
k ∪Sk−1|,
(5)
and the corresponding peak recomputation memory ¯mk
i to
¯mk
i = ci + |xi| + |ˆLk| +
X
l
ˆδk,l.|Sk−1
i
∪¯Lk
i ∪Dl
k|.
(6)
Note, the last term of equation 5 and equation 6 are quadratic in the original optimization variables
sk−1
i
, which determines Sk−1, and ˆδk,l. However, for binary variables, it can be linearized using an
auxiliary variable (see Appendix C.4). We show the full equation expansion in Appendix C.1.
5

Published as a conference paper at ICLR 2021
Checkpointing Constraints. The computational dependencies of forward and backward operators
impose strict constraints on the checkpointing schedule. Any schedule violating these constraints
cannot be executed. Recomputation rk
i requires saved sk−1
j
or recomputed rk
j dependencies j ∈Ni,
and only previously stored or recomputed tensors can be saved:
rk
i ≤sk−1
j
+ rk
j
∀i,k,j∈Ni
and
sk−2
i
≤sk−1
i
+ rk
i
∀i,k.
(7)
Furthermore, all forward tensors Dl
k required by backwardk need to be stored or computed
sk−1
i
+ rk
i ≥ˆδk,l
∀k,l,i∈Dl
k.
(8)
Objective. Our goal is to minimize the amount of computation required for the forward and back-
ward pass. This is represented as the sum of computational costs of all operators:
X
i
X
l
τ l
iδi,l
|
{z
}
forward pass
+
X
k
X
l
ˆδk,lˆτ l
k
|
{z
}
backward pass
+
X
k
X
l
τ l
iδk
i,l
|
{z
}
recomputation
.
(9)
Objective equation 9 with constraints equation 4, equation 7, equation 8, and deﬁnitions equation 1,
equation 5, equation 6 form our ﬁnal optimization objective. It jointly solves for the optimal imple-
mentation of each forward and backward operator, as well as an efﬁcient checkpointing schedule.
5
EXPERIMENTS
Implementation Details. We develop MONET in PyTorch v1.5.1 and solve the joint optimization
problem using the Gurobi (2014) solver. Appendix D provides more implementation details and a
full list of optimized operators.
The UNet experiments use 608×416 inputs following prior work (Jain et al., 2019). All other exper-
iments use 224×224 inputs following conventions (Krizhevsky et al., 2012; Simonyan & Zisserman,
2015; He et al., 2016). Batch size for the experiments is ﬁxed to be the maximum at which the model
can be trained using baseline PyTorch on a 16 GB GPU. Since Checkmate’s (Jain et al., 2019) ex-
ecution engine is built for TensorFlow, and an ofﬁcial Gist (Jain et al., 2018) implementation is not
available, we reimplement them in PyTorch for our comparisons. Our Checkmate implementation
is competitive, it uses the original Checkmate solver and has the same network structure as MONET.
Checkmate does not optimize for operator implementations like convolutions, so we show its run-
time using the default convolution algorithm (Checkmate-D). For a stronger comparison, we also
show the runtime of a Checkmate schedule that is post-optimized to greedily run the fastest convo-
lution algorithm (Checkmate-O). Wherever not explicitly speciﬁed, we compare with Checkmate-O.
All checkpointing schedules are run using the same software implementations and costs are proﬁled
on the same hardware (NVIDIA P100 GPUs). In order to compare against operator-speciﬁc opti-
mizations, we reimplement all Gist techniques in PyTorch and run them on our execution engine.
See Appendix E for more details about our baseline implementations.
Detailed Comparison to Baselines. (a) Checkpointing: Table 1 compares the memory savings
obtained by MONET and Checkmate for ﬁve different models when computational overhead over
PyTorch is ﬁxed to be 10%. MONET schedules use 2-3× less memory than PyTorch. For the same
computational overhead, MONET uses 1.2-1.8× less memory than Checkmate.
Fig. 3 shows more detailed runtime-memory trade-offs of MONET to PyTorch and Checkmate for
different models. We plot the average iteration time of training as % overhead over PyTorch for
ResNet-50
GoogleNet
UNet
VGG-16
MobileNet-V2
PyTorch
15.1
14.9
14.3
14.1
14.5
Checkmate (Jain et al., 2019)
8.2
10.5
9.1
9.9
5.8
MONeT
5.7
6.9
5.2
5.5
4.8
Table 1: Memory usage comparison (in GB) for a ﬁxed compute overhead. At 10% compute
overhead over PyTorch, MONeT uses 2-3× less memory than PyTorch. At the same overhead,
MONeT can train models using 1.2-1.8× less memory than Checkmate.
6

Published as a conference paper at ICLR 2021
0.4
0.6
0.8
1
0
10
20
30
40
Memory ratio
Overhead (%)
(a) ResNet-50 (184)
0.4
0.6
0.8
1
0
10
20
30
Memory ratio
(b) GoogleNet (320)
0.4
0.6
0.8
1
0
20
40
60
80
Memory ratio
(c) UNet (11)
0.4
0.6
0.8
1
0
20
40
60
80
Memory ratio
(d) VGG-16 (176)
0.4
0.6
0.8
1
0
10
20
30
Memory ratio
PyTorch
Checkmate-D
Checkmate-O
MONeT
(e) Mobile-V2 (272)
Figure 3: Comparing MONeT with PyTorch and Checkmate. MONeT reduces memory by 3×
compared to PyTorch, with 9-16% compute overhead. It achieves a better memory-compute trade-
off than default Checkmate-D and conv-optimized Checkmate-O.
5 GB
6 GB
7 GB
8 GB
9 GB
10 GB
ResNet-50
Checkmate
-
8.96
12.01
10.78
4.54
2.98
MONeT-NoOp
1.18
0.46
0.14
0.09
0.06
0.07
MONeT
7.24
3.84
0.73
0.70
0.31
0.11
GoogleNet
Checkmate
-
12.72
4.56
4.32
3.92
0.86
MONeT-NoOp
0.10
0.11
0.07
0.07
0.07
0.07
MONeT
3.53
0.47
0.54
0.31
0.25
0.24
VGG-16
Checkmate
-
-
-
0.002
0.002
0.001
MONeT-NoOp
-
-
-
0.001
0.000
0.000
MONeT
-
0.003
0.003
0.003
0.003
0.003
Table 2: Solver time (in hours) to reach 5% close to optimal solution. MONeT-NoOp reaches a
5% close-to-optimal solution 1.6×-117× faster than Checkmate. MONeT gets close to 5% of the
optimal solution only in a few hours, and up-to 16× faster than Checkmate for larger models.
MONET and Checkmate schedules. The memory budgets range from 5 GB to 10 GB, or equiva-
lently, 0.33× to 0.70× PyTorch memory consumption. Batch size for these models is mentioned in
paranthesis. For all models, MONET reduces memory usage by 3× (0.33 memory ratio) as com-
pared to baseline PyTorch with 9 −16% compute overhead. For the same memory budget, MONET
schedules are up-to 34% faster than Checkmate schedules. Note that we measure the empirical per-
formance of the schedules running on GPUs instead of just providing a simulation of runtime and
memory using the solver values; this is important since Checkmate does not consider workspace
cost and overestimates its savings.
For networks with individual memory-intensive layers, like VGG-16, operator optimization be-
comes even more important for reducing memory; Checkmate can reduce memory for VGG-16
only up to 7 GB, whereas MONET with its optimizations is able to run VGG-16 with 5.5 GB mem-
ory. The small runtime improvement of MONET schedules over PyTorch for VGG-16 and UNet at
higher memory budgets is mainly because of choosing faster convolution algorithms. MobileNet-
V2 uses depthwise convolutions, and hence does not signiﬁcantly beneﬁt from joint convolution-
optimization. As a result, the performance of MONET and Checkmate is closer for MobileNet-V2.
We provide additional results for MONET on a memory-intensive model, 3D-UNet (C¸ ic¸ek et al.,
2016), in Appendix J, for which we observe a consistent memory reduction to 0.54× of PyTorch
memory with an overhead of 8.86%.
For our evaluation, we cap the solver time to 24 hours for both MONET and Checkmate, and run
the schedule thus obtained on our execution framework. At tighter memory budgets for non-linear
models like ResNet-50 and GoogleNet, Checkmate is unable to ﬁnd a feasible solution within a
couple of hours. In contrast to Checkmate, MONET ﬁnds the execution plans efﬁciently. For all the
models and memory limits that we evaluate, MONET reaches a 5% close-to-optimal solution within
7

Published as a conference paper at ICLR 2021
VGG-16 (176)
ResNet50 (184)
GoogleNet (320)
MobileNetV2 (256)
UNet (11)
mem overhead
mem overhead
mem
overhead
mem
overhead
mem overhead
Gist
0.76
44.34
0.58
105.69
0.52
35.94
0.69
153.98
0.73
38.26
MONeT
0.39
9.11
0.33
11.94
0.33
15.77
0.34
8.80
0.35
11.51
Table 3: Memory ratio and overhead (%) over PyTorch for Gist and MONeT. MONET obtains
1.4×-2.1× higher memory savings over Gist across models. Number in parenthesis after model
name shows the batch size.
none
conv
out
int
all
6
8
10
9.21
6.996.37
9.3
5.53
Overhead (%)
(a) ResNet-50
none
conv
out
int
all
8
10
12
11.78
10.67
8.78
11.29
8.45
(b) GoogleNet
none
conv
out
int
all
−3
17
37
39.48
-0.70
32.58
22.67
-2.18
(c) VGG-16
Figure 4: Ablation results for memory ratio 0.53. Lowest compute overhead across models is seen
only when all optimizations are jointly optimized.
a few hours or sometimes even minutes. Table 2 shows the time it takes for the solver to reach 5%
close to the optimal solution, for Checkmate, MONET-NOOP (MONET with checkpointing enabled
but operator-optimization disabled), and MONET. MONET-NOOP converges to a close-to-optimal
solution 1.6×-117.4× faster than Checkmate. For larger models, MONET’s solver converges to a
close-to-optimal solution up to 27× faster than Checkmate. Note that running a solver is a one-time
cost for a model - once a MONET schedule has been solved for, it can be used by everyone to train
the model for different purposes with different batch sizes. The cost (typically seconds to hours)
is tiny compared to the efforts and costs to develop a model for distribution in most cases. See
Appendix H for more discussion regarding solver times, problem statistics, and full Table 2 data.
(b) Operator optimizations: Table 3 shows the comparison of MONET with Gist. While MONET
can determine a range of memory-runtime tradeoffs, purely operator-optimization-based schemes
like Gist only provide a single memory-runtime data point. For MONET, we show the memory-
runtime data point with the most memory saving. MONET uses 1.4×-2.1× less memory than Gist for
multiple architectures while maintaining full-precision. Overall, Gist provides impressive memory
savings, but incurs a high computation cost to achieve the savings.
While we get similar memory saving results for reimplemented-Gist as Jain et al. (2018) for VGG-
16, our compute overhead results are higher. This could be because of evaluations on different
frameworks (PyTorch v/s CNTK) and different GPU models (Nvidia P100 v/s Nvidia Maxwell
GTX Titan X). Gist uses dense to sparse conversion using cusparseSdense2csr in one of its
techniques. For the ﬁrst ReLU-Conv layer in VGG-16 (shape (2207744,256)), this function
takes 144ms, which itself is 10% of the VGG-16 execution time. We see similar results for other
networks. To ensure a fair comparison, we focus on the maximum memory savings obtained by
MONET with Gist, while reporting the compute overhead for completeness.
Ablation Experiments. Fig. 4 shows additional ablation experiments. We show the % compute
overhead over PyTorch on ResNet-50, GoogleNet, and VGG-16 for different types of MONET
checkpointing schedules with a memory budget of 8 GB - with no operator optimizations en-
abled, with only one type of operator optimization enabled (conv-optimized, output-activated op-
timized, intermediate-activated optimized), and with all optimizations enabled. Schedules which
do not jointly optimize convolution algorithms are run with greedily post-optimized convolution
algorithm. Plots for other models look similar to that of ResNet-50 and GoogleNet. The only dif-
ference between ’none’ and ’conv’ is that convolution algorithms are jointly optimized in the latter.
However, this fact leads to signiﬁcant improvement in compute time for all cases. In fact, convolu-
tion algorithms have complex workspace memory - compute characteristics, reserving slightly more
memory for convolution workspace while checkpointing can allow for a much faster convolution
(see Appendix I). This makes it important to jointly optimize conv algorithms with checkpointing.
Similarly, output-activated optimization also provides signiﬁcant beneﬁts over vanilla checkpoint-
ing, since it effectively reduces the number of recomputations required. For memory-intensive net-
8

Published as a conference paper at ICLR 2021
0.5
1
1.5
·104
PyTorch (14.7 GB)
MONeT (8.0 GB)
Mem. (MB)
PyTorch (860 ms)
MONeT-NoOp (939 ms)
MONeT (908 ms)
−10
−5
0
5
·102
Mem. Diff (MB)
50
100
150
200
250
300
0
2
4
Network Layer Index
Overhead
(%)
Figure 5: Detailed case study on ResNet-50. Top : memory usage along execution (forward and
backward). Middle: memory saving of MONeT over PyTorch for each layer. Bottom: compute
overhead of MONeT over PyTorch. MONeT saves memory in early layers to reduce peak memory.
Most compute overhead happens at recomputation during backward (right-hand-side of the ﬁgure).
works, intermediate-activated optimization becomes more important. Jointly optimizing all strate-
gies together gives the least computational overhead. See Appendix G for detailed ablation plots.
Detailed Case Study. The top graph of Fig. 5 shows memory usage while executing PyTorch,
MONET without operator optimization, and MONET for ResNet-50 at batch size 184. As the training
progresses along network layers represented on X-axis, PyTorch and both MONET schedules store
forward-pass outputs, leading to an increasing memory footprint. MONET reaches peak memory
of 8 GB, whereas PyTorch requires 14.7 GB. Stored forward outputs are freed up one after other
as backward pass proceeds, leading to reduced usage of memory. According to the checkpointing
schedule, MONET saves only a subset of the outputs stored by PyTorch, resulting in the memory
saving shown in the middle graph for layer outputs that are not stored. The bottom graph shows the
per-layer compute overhead of recomputation of MONET over PyTorch. For MONET, later layers
which are backward operators result in a recomputation of the forward, and have higher overhead.
6
CONCLUSION
We present MONET, a system to automatically reduce memory requirements for training deep net-
works. MONET jointly optimizes local (operator-level) and global (graph-level) optimizations to
yield a compute- and memory-efﬁcient checkpointing schedule. MONET reduces memory usage
by 3× over PyTorch, with a 9 −16% compute overhead. It uses 1.2-1.8× less memory than the
state-of-the-art automated checkpointing framework for the same computational cost. Our experi-
mental results show that MONET leads to better memory-computation trade-offs compared to the
state-of-the-art.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their feedback. Aashaka Shah and Vijay
Chidambaram were partially supported by donations from VMware and Google. Chao-Yuan Wu
was partially supported by a Facebook Fellowship. Jayashree Mohan was supported by a Microsoft
Research Fellowship. The results presented in this paper were obtained using the Chameleon testbed
supported by the National Science Foundation
9

Published as a conference paper at ICLR 2021
REFERENCES
Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Va-
sudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: A system for
large-scale machine learning. In OSDI, 2016.
Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for
convex optimization problems. Journal of Control and Decision, 2018.
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of
neural networks. In NeurIPS, 2018.
Samuel Rota Bul`o, Lorenzo Porzi, and Peter Kontschieder.
In-place activated batchnorm for
memory-optimized training of dnns. In CVPR, 2018.
Ayan Chakrabarti and Benjamin Moseley. Backprop with approximate activations for memory-
efﬁcient network training. In NeurIPS, 2019.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected CRFs. TPAMI, 2018.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. CoRR, 2016.
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catan-
zaro, and Evan Shelhamer.
cudnn: Efﬁcient primitives for deep learning.
arXiv preprint
arXiv:1410.0759, 2014.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, 2019.
¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D
U-Net: learning dense volumetric segmentation from sparse annotation. In MICCAI, 2016.
Ronan Collobert, Koray Kavukcuoglu, and Cl´ement Farabet. Torch7: A matlab-like environment
for machine learning. In BigLearn, NeurIPS workshop, 2011.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. NAACL, 2019.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 2016.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep
convolutional networks. TPAMI, 2016.
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. SlowFast networks for video
recognition. In ICCV, 2019.
Aidan N. Gomez, Mengye Ren, Raquel Urtasun, and Roger B. Grosse. The reversible residual
network: Backpropagation without storing activations. In NeurIPS, 2017.
Gurobi. Gurobi optimizer reference manual, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Animesh Jain, Amar Phanishayee, Jason Mars, Lingjia Tang, and Gennady Pekhimenko.
Gist:
Efﬁcient data encoding for deep neural network training. In ISCA, 2018.
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt Keutzer, Ion Stoica,
and Joseph E. Gonzalez. Checkmate: Breaking the memory wall with optimal tensor rematerial-
ization. CoRR, 2019.
10

Published as a conference paper at ICLR 2021
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature em-
bedding. In ACM MM, 2014.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep
convolutional networks. In CVPR, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep convo-
lutional neural networks. In NeurIPS, 2012.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garc´ıa,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In ICLR, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance
deep learning library. In NeurIPS, 2019.
Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, and Kilian Q. Wein-
berger. Memory-efﬁcient implementation of densenets. CoRR, 2017.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation. In MICCAI, 2015.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bileNetV2: Inverted residuals and linear bottlenecks. In CVPR, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
CVPR, 2015.
Shmuel Winograd. Arithmetic complexity of computations. SIAM, 1980.
Adrian Wolny.
PyTorch 3D-UNet.
https://github.com/wolny/pytorch-3dunet,
2019.
Adrian Wolny, Lorenzo Cerrone, Athul Vijayan, Rachele Tofanelli, Amaya Vilches Barro, Marion
Louveaux, Christian Wenzl, Susanne Steigleder, Constantin Pape, Alberto Bailoni, Salva Duran-
Nebreda, George Bassel, Jan U. Lohmann, Fred A. Hamprecht, Kay Schneitz, Alexis Maizel,
and Anna Kreshuk. Accurate and versatile 3d segmentation of plant tissues at cellular resolution.
bioRxiv, 2020.
Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross
Girshick. Long-term feature banks for detailed video understanding. In CVPR, 2019.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.
11

Published as a conference paper at ICLR 2021
A
NOTATIONS
Table 4 gives a list of notations used in the paper along with explanations.
Notation
Meaning
Tensors
xi
Tensor which is the output of forwardi in forward pass and during recomputation.
yk
Gradient tensor which is the output of backwardi in the backward pass.
Sets
SN
i
Set of stored tensors after forwardi in forward pass. (N = num backward operators)
Li
Set of all parameters and forward tensors created till forward node i, required as
computational dependencies for forwardi and later forward passes.
Dk
Set of forward pass tensors required as computational dependencies for backwardk.
Sk−1
Set of stored forward pass tensors right before calling backwardk.
ˆ
Lk
Set of gradient tensors created before backward node k, and required as computa-
tional dependencies for backwardk and later backward passes.
Sk−1
i
Set of stored tensors available to recomputation of forwardi before computing
backwardk.
Lk
i
Set of all parameters and forward tensors created till forward node i, required as
computational dependencies for forwardi and later forward recomputations to be
done before backwardk.
Ii
Set of implementations for operator forwardi.
ˆIk
Set of implementations for operator backwardk.
Solver variables
sN
i
Indicate if output of forwardi is stored in memory in the forward pass.
sk−1
i
Indicate if output of forwardi is stored in memory when computing backwardk.
rk
i
Indicate if forwardi is recomputed before computing backwardk.
δi,l
Indicate if forwardi uses implementation l ∈Ii in the forward pass.
δk
i,l
Indicate if forwardi uses implementation l
∈
Ii when recomputed before
backwardk.
ˆδk,l
Indicate if backwardk uses implementation l ∈ˆIk.
Memory formulations
mi
Peak memory of forwardi in forward pass.
¯mk
i
Peak memory of forwardi when it is recomputed before backwardk.
ˆmk
Peak memory of backwardk.
Operator costs
cl
i
Workspace memory of operator forwardi executed using implementation l ∈Ii.
ˆcl
k
Workspace memory of operator backwardk executed using implementation l ∈ˆIk.
τ l
i
Compute cost of operator forwardi executed using implementation l ∈Ii.
ˆτ l
k
Compute cost of operator backwardk executed using implementation l ∈ˆIk.
Table 4: Notations used in paper with explanations. Notations with only i in subscript/superscript
generally relate to the forward pass, with only k relate to the backward pass, and with both i and k
relate to the recomputation phase.
B
BOUNDS ON LOCAL MEMORY
In Section 3, we mentioned that local memory Lk
i is dependent on solver variable rk
t .
Lk
i = Θ ∪{xj : j ∈Nt for any t ≥i with rk
t = 1 and j < i}.
In order to remove this dependence, we can get an upper bound ¯Lk
i on Lk
i by assuming that all future
tensors after i will always be recomputed, that is, rk
t = 1, ∀t > i, and
Lk
i ⊆¯Lk
i = Θ ∪{xj : j ∈Nt for any t ≥i and j < i}.
Our experiments also use this upper bound. It is possible to tighten the upper bound by noting that
rk
t may be 1 only in the case when t ≤k. That is, forward node t will not be recomputed before
computing backward of node k if node t lies after node k. Thus, a tighter bound to Lk
i follows
Lk
i ⊆˙Lk
i = Θ ∪{xj : j ∈Nt for any t ≥i and t ≤k and j < i} ⊆¯Lk
i .
12

Published as a conference paper at ICLR 2021
C
DETAILED CONSTRAINTS
C.1
EXPANDED BACKWARD PASS MEMORY CONSTRAINTS
Sec. 4 formulates backward peak memory ˆmk and recomputation peak memory ¯mk
i as sum of mem-
ory of a set of tensors. We expand the memory formulation and represent it in the terms of optimiza-
tion varaible here:
ˆmk =
X
l
ˆcl
kˆδk,l + |yk| + |ˆLk| +
X
l
ˆδk,l.|Dl
k ∪Sk−1|
=
X
l
ˆcl
kˆδk,l + |yk| +
X
yl∈ˆLk
|yl| +
X
l
X
xi∈Dl
k
ˆδk,l|xi| +
X
l
X
xi /∈Dl
k
ˆδk,lsk−1
i
|
{z
}
σk,l,s
|xi|,
(10)
¯mk
i = ci + |xi| + |ˆLk| +
X
l
ˆδk,l.|Sk−1
i
∪¯Lk
i ∪Dl
k|
= ci + |xi| + |ˆLk| +
X
l
X
j<i:
xj /∈¯Lk
i ∪Dl
k
ˆδk,lsk−1
j
|xj| +
X
l
X
j<i:
xj∈¯Lk
i ∪Dl
k
ˆδk,l|xj| +
X
j>i
sk
j |xj|.
(11)
C.2
COMPLETE MEMORY CONSTRAINTS
In this section, we present the complete memory constraints which we use for MONET optimization.
These constraints include the recomputation variable rk
i , which was excluded from the main text to
make understanding simpler. As discussed in Sec. 3, the peak memory of a forwardi recomputa-
tion before computing backwardk is denoted by ˜mk
i . This represents the recomputation memory
(renamed to mk
Ri) when forwardi is actually recomputed, that is, rk
i = 1. When this is not true, the
peak memory ( ˜mk
Si) only depends on stored checkpoints Sk−1
i
, checkpoint dependencies for Dk,
and gradient tensors ˆLk. Thus,
˜mk
Ri = ci + |xi| + |ˆLk| + |Sk−1
i
∪Lk
i ∪Dk|
= rk
i ci + rk
i |xi| + |ˆLk| +
X
j<i:xj /∈Lk
i ∪Dk
sk−1
j
|xj| +
X
j<i:xj∈Lk
i
rk
i |xj| +
X
j<i:xj∈Dk−Lk
i
|xj| +
X
j>i
sk
j |xj|.
(12)
˜mk
Si = |ˆLk| + |Sk−1
i
∪Dk|
= |ˆLk| +
X
j≤i:xj /∈Dk
sk−1
j
|xj| +
X
j≤i:xj∈Dk
|xj| +
X
j>i
sk
j |xj|.
(13)
Local memory Lk can be bounded by ¯Lk, which gives us ¯mk
Ri. To add forward operator optimiza-
tions to ¯mk
Ri, we recall the trade-off between workspace memory and compute time. We replace the
workspace memory contributor rk
i ci in equation 12 with P
l δk
i,lcl
i.
The complete memory constraints are:
mi ≤M
∀i
and
ˆmk ≤M
∀k
and
¯mk
Ri ≤M
∀k,i
and
˜mk
Si ≤M
∀k,i
(14)
C.3
IN-PLACE CONSTRAINTS
We show how to represent the decision of computing an operator using an in-place or out-of-place
implementation. If an operator like ReLU uses an in-place implementation, its input tensor is over-
written with its output. In this case, its input tensor cannot be stored or used as input to a computation
in this stage. This needs to be reﬂected in our constraints. We introduce two new binary variables
to model in-place computations: qk
i represents if forwardi is recomputed in-place when computing
backwardk. pk
i represents that the output of forwardi has been computed and will not be overwrit-
ten by any other forward node recomputations in this stage. If qk
i is true, then pk
j will be false else
pk
j will be the same as rk
j , where j ∈Ni. Further, sk−1
j
will also be false if qk
i is true. This can be
written in the form of boolean constraints as follows:
pk
j ≥rk
j −2qk
i
and
pk
j ≤2 −2qk
i
and
sk−1
k
≤2 −2qk
i .
(15)
13

Published as a conference paper at ICLR 2021
The checkpointing constraint 7 changes, with pk
j replacing rk
j on the RHS. Further, qk
i (or pk
j ) can
only be true if forwardi (or forwardj) is actually recomputed prior to computing backward node k.
Thus,
pk
j ≤rk
j
and
qk
i ≤rk
i .
(16)
C.4
CONSTRAINT LINEARIZATION
The memory constraints we introduce in Section 4 contain quadratic terms in the form of xi · xj,
with xi, xj ∈{0, 1}. The quadratic terms cannot directly be incorporated into an integer program.
However, we can linearize these terms by replacing each quadratic term xi · xj by an auxiliary
variable αi,j ∈{0, 1} and introducing additional linear constraints αi,j ≥xi + xj −1, αi,j ≤xi,
and αi,j ≤xj. After this substitution for all quadratic terms, all constraints in MONET are linear.
D
IMPLEMENTATION
We develop MONET in the PyTorch (v1.5.1) framework. We use PyTorch’s default Autograd pack-
age for backward implementation of elementary functions when the autograd implementation is
stateless. In all other cases, we implement custom forward and backward functions leveraging Py-
Torch ATen library functions to ﬂexibly support multiple operators and execution schedules. Each
backward operator implementation is annotated with its computational dependencies, which is gen-
erally the input or the output of its corresponding forward operator. Certain backward operators
implementations may have dependencies on intermediate activations generated in the forward pass.
For example, an intermediate-activated ReLU backward uses an encoded bitmask representing the
sign of forward operator’s input. We annotate this as an intermediate storage node and add it to our
optimization problem, with a strict recomputation dependency of the interemediate storage node on
its creator node. Our operator optimizations select from different backward operator implementa-
tions, convolution algorithms, in-place operators etc. We split the convolution backward operator
into two - a parameter-gradient operator followed by an input-gradient operator. Since the input-
gradient operator does not have any computational dependency on the forward pass, we can agres-
sively free the forward input tensor right after the parameter-gradient is computed. We also reuse
BatchNorm statistics in case of their recomputation. For our experiments, we limit ourselves to full
precision training as quantization or lower precision computations introduce additional noise into
SGD and change its convergence properties. We solve the joint optimization problem using the
CVXPY (Diamond & Boyd, 2016; Agrawal et al., 2018) solver with Gurobi (2014) backend.
MONET workﬂow. We obtain the forward pass dependencies in MONET by JIT tracing a model to
obtain its graph. We proﬁle each layer for workspace memory and compute cost, and obtain memory
usage of the tensors from their shape and type. Note that the workspace memory for many convolu-
tion operators in VGG-16 is greater than 2GB, making it an important factor to model. Unlike prior
approaches like Checkmate, we account for this workspace memory in our optimization problem,
bringing the memory model very close to actual memory allocation. We phrase a boolean inte-
ger programming problem using the generated graph and the proﬁled compute cost and workspace
memory and solve it using the CVXPY (Diamond & Boyd, 2016; Agrawal et al., 2018) modeling
language and GUROBI (Gurobi, 2014) solver. The solution is used to generate a schedule that can
be run by the MONET scheduler.
Operator optimizations. We divide operator optimizations according to the different type of imple-
mentations they select from. (1) Output-activated: Backward calculation of operators like ReLU and
BatchNorm can have computational dependency either on on their forward node’s inputs or outputs.
(2) Intermediate-activated: Backward of ReLU has computational dependency on a 1-bit encoding
of the sign of its forward node’s input. Backward of MaxPool is calculated using an intermediate 8-
bit output-shaped tensor which contains the kernel-index of the maximum element. (3) Convolution
algorithms: We choose from 8 forward and 6 backward cuDNN convolution algorithms. (4) Inplace
operations: The solver can choose to do inplace computation for operators like ReLU forward. We
discuss constraints for in-place operator selection in C.3. All MONET experiments enable in-place
operation selection.
14

Published as a conference paper at ICLR 2021
E
BASELINE IMPLEMENTATIONS
Checkmate implementation. We reimplement Checkmate (Jain et al., 2019) in PyTorch for our
comparisons. We use the Checkmate solver as-is to obtain Checkmate schedules. Since Checkmate
does not provide an execution engine for PyTorch, we run the generated Checkmate schedules on
our own execution framework. Our inference engine uses the same operator implementations for
Checkmate and MONeT. We have released our Checkmate implementation with the MONET code.
Gist implementation. Gist (Jain et al., 2018) is an operator-based memory-efﬁcient scheme for
training DNNs. It encodes stashed forward tensors into smaller tensors which require less memory.
Jain et al. (2018) evaluate Gist using CNTK on an Nvidia Maxwell GTX Titan X GPU. Since we
implement MONET in PyTorch and have access to an Nvidia P100 GPU, a direct comparison with
the numbers in the Gist paper is not possible. As an ofﬁcial Gist implementation is not available, we
reimplement it on PyTorch and evaluate its execution using MONET ’s execution framework.
We implement all Gist optimizations — Binarize (intermediate encodings for ReLU-Pool layers),
Sparse Storage Dense Compute (compress and store sparse convolution inputs in ReLU-Conv layers
as sparse storage), Delayed Precision Reduction (storing stashed non-compressed tensors in FP-16,
but computing in FP-32), and Inplace (performing ReLU operators in-place wherever possible) over
MONET’s execution framework. In Gist, the Sparse Storage Dense Compute (SSDC) technique
creates a sparse storage tensor in the Compressed Sparse Row (CSR) representation using the Nvidia
cuSPARSE library. The dense storage is reshaped into a 256-sized column tensor before storing it
in a sparse format, allowing the column index of CSR representation to be saved in 8 bits instead
of using 32 bits (termed Narrow Value Optimization in the paper). We also implement SSDC using
Nvidia’s cuSPARSE library (function cusparseSdense2csr) with CUDA Toolkit version 10.1
using PyTorch’s C++ extensions.
In their paper, Jain et al. (2018) use the most memory-efﬁcient convolution algorithms in Gist and
compare its memory saving against a baseline which also chooses the most memory-efﬁcient con-
volution algorithm. Using memory-efﬁcient convolution algorithms, our Gist reimplementation can
train VGG-16 with 0.55× of the PyTorch-required memory (1.81× memory footprint), which is
close to the data presented by Jain et al. (2018). However, it is 59% slower than when convolution
selection is enabled, in which case it can train using 0.76× of the PyTorch-required memory. Since
implementing Gist using memory-efﬁcient convolutions is not optimal in terms of compute time, we
implement Gist to use PyTorch’s convolution selection algorithm. For all models other than VGG-
16 and UNet, we see similar memory savings for Gist with memory-efﬁcient convolutions and with
convolution-selection enabled. We have released our Gist implementation with the MONET code.
F
ON OPERATOR SELECTION FOR CHECKMATE
In this section, we brieﬂy explain the difﬁculties of including operator selection directly into check-
mate (Jain et al., 2019). We will refer directly to notation and equations in the checkmate paper
(arxiv v3; 14 May 2020). The most direct way to incorporate operator selection into checkmate
is to introduce an auxiliary variable Rv
t,i ∈{0, 1} that refers to re-computing layer i at time t us-
ing implementation v. Most constraints in equation 1 could stay the same, given Rt,i = P
v Rv
t,i,
and loss (1a) P
t
P
i
P
v Rv
t,iCv
i . Some of our operators produce a different kind of checkpoint
(e.g. binary activated ReLUs), which could be handled in check-mate by splitting Sv
t,i. The main
issues in checkmate arise in the memory modeling and its relaxations (eq 4,5,7). The memory
consumed by a speciﬁc checkpoint may depend on the operator implementation: DEPS[k] and
USERS[i] both depend on the operator implementation (output activated, input activated, ...). In
short, the checkmate computation graph is dynamic and depends on operator implementations. The
most direct way to address this is to mem freedt(vk) = P
v Rv
t,imem freedt(vk) in a implementa-
tion dependent way mem freedv
t (vk), and select the right version dependent on the operator used.
Likewise, we need to extend FREEv
i,t,k to account for different operator implementations in Rv
t,k.
Likewise the product in equation (5) will now go over all implementations Rv
i,j using different
USERS sets. This leads to a linear blowup in the number of constraints, and number of auxil-
iary variables, leading to an at least quadratic expansion on computational costs. Furthermore,
mem freedt(vk) = P
v Rv
t,imem freedt(vk) is a quadratic constrain that further needs to be re-
solved using additional auxiliary variables. Given that Checkmate already pushes the limits of
15

Published as a conference paper at ICLR 2021
0.4
0.6
0.8
5
10
15
Memory ratio
Compute overhead (%)
(a) ResNet-50 (184)
0.4
0.6
0.8
10
15
Memory ratio
(b) GoogleNet (320)
0.4
0.6
0.8
0
10
20
Memory ratio
(c) UNet (11)
0.4
0.6
0.8
1
0
20
40
Memory ratio
(d) VGG-16 (176)
0.4
0.6
0.8
5
10
Memory ratio
NoOp
Int
Out
Conv
All
(e) Mobile-V2 (272)
Figure 6: Ablation results on ResNet-50, GoogleNet, UNet, VGG-16, MobileNet-V2.
current solvers, it is unlikely able to handle this explosion in constraints and variables, without
signiﬁcant modiﬁcations. MONET in the other hand represents the compute-graph more compactly
and efﬁciently integrates different operator implementations.
G
DETAILED ABLATIONS
Fig. 6 shows a detailed plot of our ablation experiments comparing the compute overhead of vari-
ants of MONET across a range of memory limits. Y-axis shows the compute overhead over PyTorch
and X-axis shows the memory ratio to a PyTorch model. All variants which are not conv-optimized
are greedily post-optimized to use the fastest convolution. We see that MONET with no opera-
tor optimization (NoOp) is generally slower than the other variants for all models and memory
limits. Convolution and output-activated optimizations are both important in reducing compute
overhead. MobileNet-V2 uses depthwise separable convolutions, and hence does not signiﬁcantly
beneﬁt from convolution-optimization. Further, MobileNet-V2 has hardtanh operators instead of
ReLU operators, for which we have not implemented intermediate-activated backward optimization.
Interemediate-activated optimizations provide memory savings in memory-intensive models, allow-
ing models like VGG-16 to reach memory savings which are not attainable by other optimizations.
All optimizations together result in the least compute overhead for any model or memory limit.
H
SOLVER TIME AND ILP STATISTICS
Solver runtime. MONET’s solver runtimes vary for different models and different memory limits.
We evaluate schedules obtained using solver times set to a maximum of 24 hours. For moderate
memory limits, both MONeT and Checkmate achieve an optimal solution before 24 hours. For
tighter memory limits, the solution obtained by MONeT and Checkmate may not be most optimal.
For multiple models and memory limits, Table 2 in Sec. 5 shows the time it takes for the solver to
reach 5% close to the optimal solution for Checkmate, MONET-NOOP (MONET with checkpointing
enabled but operator-optimization disabled), and MONET. We add the data for MobileNet-V2 and
UNet in Table 5 which also follow a similar pattern. We also provide Table 6 which shows the time
taken by the solver to reach 2% close to the optimal solution. We note that it has similar behavior as
the time taken the solver to reach 5% close to the optimal solution. MONET-NOOP converges to 2%
close-to-optimal solution 1.3×-139× faster than Checkmate. For larger models, MONeT’s solver
converges to a 2% close-to-optimal solution up to 16× faster than Checkmate. At tighter memory
limits for MobileNet-V2, the Checkmate solver reaches 2% close-to-optimal solution faster than
MONET, but is still much slower than MONET-NOOP.
ILP statistics. For different models, Table 7 shows the solver statistics after presolving for the
problem formulated by Checkmate, MONET-NOOP and MONET for a 10 GB memory limit. It
shows the number of forward operators in the model and the number of constraints and variables
for each solver. MONET-NOOP, which is MONET with only checkpointing enabled and without
using operator optimization, has on average 50% fewer constraints and 67% fewer variables than
Checkmate. Jointly-optimized MONET has a slightly larger number of constraints, and on average
16

Published as a conference paper at ICLR 2021
5 GB
6 GB
7 GB
8 GB
9 GB
10 GB
ResNet-50
Checkmate
-
8.96
12.01
10.78
4.54
2.98
MONeT-NoOp
1.18
0.46
0.14
0.09
0.06
0.07
MONeT
7.24
3.84
0.73
0.70
0.31
0.11
GoogleNet
Checkmate
-
12.72
4.56
4.32
3.92
0.86
MONeT-NoOp
0.10
0.11
0.07
0.07
0.07
0.07
MONeT
3.53
0.47
0.54
0.31
0.25
0.24
MobileNet-V2
Checkmate
2.16
2.88
1.16
0.29
0.34
0.14
MONeT-NoOp
0.11
0.04
0.02
0.02
0.04
0.08
MONeT
0.37
0.28
0.52
0.05
0.06
0.03
UNet
Checkmate
0.149
0.031
0.022
0.020
0.010
0.009
MONeT-NoOp
0.048
0.002
0.002
0.002
0.002
0.002
MONeT
0.363
0.064
0.028
0.027
0.024
0.006
VGG-16
Checkmate
-
-
-
0.002
0.002
0.001
MONeT-NoOp
-
-
-
0.001
0.000
0.000
MONeT
-
0.003
0.003
0.003
0.003
0.003
Table 5: Solver time (in hours) to reach 5% close to optimal solution. MONeT-NoOp reaches a
5% close-to-optimal solution 1.6×-117× faster than Checkmate. MONeT gets close to 5% of the
optimal solution only in a few hours, and up-to 16× faster than Checkmate for larger models.
5 GB
6 GB
7 GB
8 GB
9 GB
10 GB
ResNet-50
Checkmate
-
16.44
13.43
11.91
5.74
3.81
MONeT-NoOp
-
2.06
1.28
0.16
0.08
0.07
MONeT
-
-
12.64
3.00
3.60
0.62
GoogleNet
Checkmate
-
15.08
4.93
5.04
3.92
0.90
MONeT-NoOp
0.10
0.11
0.07
0.07
0.07
0.07
MONeT
-
5.47
5.34
0.31
0.25
0.24
MobileNet-V2
Checkmate
2.16
2.88
1.16
0.29
0.34
0.14
MONeT-NoOp
0.43
0.37
0.02
0.02
0.10
0.09
MONeT
9.49
5.33
1.53
0.14
0.18
0.05
UNet
Checkmate
0.243
0.031
0.027
0.021
0.011
0.009
MONeT-NoOp
0.181
0.003
0.003
0.002
0.002
0.002
MONeT
5.001
0.204
0.164
0.069
0.083
0.027
VGG-16
Checkmate
-
-
-
0.003
0.002
0.001
MONeT-NoOp
-
-
-
0.001
0.000
0.000
MONeT
-
0.004
0.006
0.004
0.003
0.003
Table 6: Solver time (in hours) to reach 2% close to optimal solution. MONeT-NoOp reaches
a 2% close-to-optimal solution 1.3×-139× faster than Checkmate. MONeT reaches a 2% close-to-
optimal solution within few hours in most cases, and up to 27× faster than Checkmate for larger
models.
40% fewer variables than Checkmate. MONET’s formulation is more efﬁcient, and might be the
reason that it reaches a good solution faster than Checkmate.
17

Published as a conference paper at ICLR 2021
Checkmate
MONeT-NoOp
MONeT
Fwd ops
Constraints
Variables
Constraints
Variables
Constraints
Variables
GoogleNet
215
719,327
519,252
221,630
104,673
781,747
362,640
ResNet-50
175
473,592
344,659
344,652
167,238
487,842
229,431
Mobilen.V2
153
337,316
247,033
153,828
74,579
241,478
115,047
UNet
67
65,715
48,744
32,273
15,982
73,624
36,548
VGG-16
40
25,334
18,968
12,772
6,306
21,918
11,234
Table 7: ILP statistics for Checkmate, MONeT-NoOp, and MONeT. MONeT-NoOp has on av-
erage 50% fewer constraints and 67% fewer variables than Checkmate. MONeT has slightly larger
number of constraints, on average 40% fewer variables than Checkmate.
I
CONVOLUTION ALGORITHMS
Fig. 7 shows the complex workspace memory-compute trade-off for different convolution algo-
rithms. The memory used is not always inversely proportional to the compute requirement. Jointly
optimizing convolution algorithms enables MONET to make the best decisions for which convolu-
tion algorithm to select.
J
APPLICABILITY TO MEMORY-INTENSIVE MODELS
To further show MONET’s applicability to memory-intensive models, we evaluate it on 3D-
UNet (C¸ ic¸ek et al., 2016), a fully-convolutional model for volumetric images.
Fig. 8 presents
the runtime-memory trade-off for MONET on 3D-UNet.
We used a commonly used 3D-
UNet implementation (Wolny, 2019; Wolny et al., 2020) with training conﬁguration similar to
3DUnet confocal boundary provided in the repository and a batch size of 22, which just
ﬁts on a 16 GB P100 GPU. MONET reduces memory usage to 0.54× of PyTorch, while incurring
8.86% overhead in compute time. At a memory ratio of 0.81, MONET incurs almost no compu-
tational overhead, because it makes use of operator optimizations and is able to bring down the
recomputation cost to zero.
0
1
2
0
0.2
0.4
Workspace memory (GB)
Time (ms)
Figure 7: Memory vs. compute for 7 convo-
lution algorithms with 256×64×56×56 in-
put, 3×3 kernel, 64 output channels.
0.4
0.6
0.8
1
0
2
4
6
8
10
Memory ratio
Overhead (%)
Figure
8:
Runtime-memory
trade-off
curve for 3D-UNet using MONET. The
green point denotes the PyTorch baseline.
18

