Published as a conference paper at ICLR 2024
LATENT TRAJECTORY LEARNING FOR LIMITED TIMES-
TAMPS UNDER DISTRIBUTION SHIFT OVER TIME
Qiuhao Zeng1
Changjian Shui2
Long-Kai Huang
Peng Liu3
Xi Chen4
Charles X. Ling1
Boyu Wang1∗
1University of Western Ontario
2Vector Institute
3University of Toronto
4Noah’s Ark Lab
ABSTRACT
Distribution shifts over time are common in real-world machine-learning appli-
cations. This scenario is formulated as Evolving Domain Generalization (EDG),
where models aim to generalize well to unseen target domains in a time-varying
system by learning and leveraging the underlying evolving pattern of the distribu-
tion shifts across domains. However, existing methods encounter challenges due
to the limited number of timestamps (every domain corresponds to a timestamp)
in EDG datasets, leading to difficulties in capturing evolving dynamics and risk-
ing overfitting to the sparse timestamps, which hampers their generalization and
adaptability to new tasks. To address this limitation, we propose a novel approach
SDE-EDG that collects the Infinitely Fined-Grid Evolving Trajectory (IFGET) of
the data distribution with continuous-interpolated samples to bridge temporal gaps
(intervals between two successive timestamps). Furthermore, by leveraging the
inherent capacity of Stochastic Differential Equations (SDEs) to capture continuous
trajectories, we propose their use to align SDE-modeled trajectories with IFGET
across domains, thus enabling the capture of evolving distribution trends. We
evaluate our approach on several benchmark datasets and demonstrate that it can
achieve superior performance compared to existing state-of-the-art methods.
1
INTRODUCTION
Domain Generalization (DG) is a fundamental problem in machine learning. It aims to learn a
model that can perform well on unseen data based on the knowledge learned from multiple related
domains (Arjovsky et al., 2019; Li et al., 2018b; Sagawa et al.; 2019; Sun & Saenko, 2016; Chen
et al., 2024). However, traditional DG techniques assume that the distribution in different domains
remains stationary over time, which is often impractical in many real-world scenarios. In practice,
the distribution of data may shift over time due to factors such as changes in the environment or
the underlying system. For example, Age-related changes occur in all ocular tissues, including
age-related structural changes in the optic nerve (Grossniklaus et al., 2013). Age-related ocular
disease is the most prevalent condition associated with vision impairment and blindness in older
adults worldwide (Flaxman et al., 2017). However, the data collected from individuals aged beyond
80 is lacking due to a very small sample size, privacy, and other factors. It’s necessary to build a
prediction model based on the age-related pattern learned from the data collected from younger cases.
To adapt to changing environments over time, recent research in the community has focused on the
scenario of Evolving Domain Generalization (EDG) (Bai et al., 2023; Nasery et al., 2021b; Qin
et al., 2022; Zeng et al., 2023; Yao et al., 2022), aimed to tackle such problems. Specifically, the
goal of EDG is to learn and leverage the evolving patterns captured from source domains to achieve
generalization capability on the unseen future target domains in a time-varying environment.
However, one fundamental obstacle in existing EDG studies (Bai et al., 2023; Qin et al., 2022; Zeng
et al., 2023) is that they suffer from a limited number of timestamps, resulting in overfitting to the
sparse timestamp data. Consequently, they cannot properly capture the underlying evolving pattern
and extrapolate into the future. In fact, recent research (Mariet & Kuznetsov, 2019) has revealed that
∗Corresponding authors: Boyu Wang, Charles X. Ling.
1

Published as a conference paper at ICLR 2024
the sample complexity of time series forecasting tasks scales as O(
p
1/M), where M is the number
of timestamps of the training time-series data.
In this paper, we tackle this problem by constructing a continuously evolving trajectory. Specifically,
we create the Infinitely Fined-Grid Evolving Trajectory (IFGET) in the latent representation space,
with two steps: i) firstly, we develop sample-to-sample correspondence to collect the evolving
trajectory of each individual sample; ii) Next, we generate continuous-interpolated samples by
leveraging such correspondence, aimed to bridge the temporal gaps between timestamp intervals and
avoid overfitting to sparse timestamps. It is denoted as IFGET, since it is a continuous trajectory and
thereby can be subdivided into infinitely fine temporal grids. Nevertheless, dealing with continuous
trajectories poses another challenge in EDG. Most existing EDG algorithms are designed for discrete
timestamps and are not able to handle continuous timestamps, since they employ transition functions
to predict data at the next timestamp based on current observation, inherently representing time
discretely (Bai et al., 2023; Qin et al., 2022; Zeng et al., 2023; 2024).
To address this issue, we propose to model the temporal dynamics of latent representations by
employing Stochastic Differential Equations (SDEs) (Kong et al., 2020; Li et al., 2020; Xu et al.,
2022; Kidger et al., 2021) to fit the IFGET, which provides a natural approach for characterizing
continuous-time stochastic processes. Specifically, we propose a path alignment regularizer, which
aligns the latent trajectories characterized by SDEs with the paths generated by IFGET, by maximizing
the likelihood of the SDE trajectories based on the observations of IFGET.
To summarize, our proposed algorithm, termed as SDE-EDG, has the following desirable properties1:
Capturing Evolving Patterns via Infinitely Fined-grid Evolving Trajectory (IFGET) To overcome
the limitations of the small number of timestamps in current EDG data, we propose to learn the
evolving dynamics by constructing the IFGET. To construct the evolving trajectory, we identify the
sample-to-sample correspondence between successive domains and employ an interpolation function
to generate a continuous-interpolated sample. IFGET alleviates overfitting to the limited timestamps
and improves the generalization to distribution shifts over time.
Modelling Trajectories of Latent Representations with Stochastic Differential Equations (SDEs)
Leveraging the inherent capacity of SDE to model continuous temporal trajectory, our model aligns
the depicted latent trajectories of SDE-EDG with IFGET during the training process. We show that
SDE-EDG is capable of quantifying evolving stochastic processes, and theoretically demonstrate that
SDE-EDG results in a lower generalization bound for downstream tasks.
2
RELATED WORKS
Evolving Domain Adaptation / Generalization Evolving Domain Adaptation (EDA) (Hoffman
et al., 2014; Kumagai & Iwata, 2017; Mancini et al., 2019; Liu et al., 2020a; Wang et al., 2020;
Kumar et al., 2020; Wang et al., 2022) is a related field that focuses on scenarios where a single
labeled domain is available alongside multiple unlabeled intermediate domains. The objective of
EDA is to achieve generalization on unseen target domains. Recently, EDG has received considerable
attention from researchers. Approaches to solving the EDG problem can be broadly categorized into
two groups. The first group parameterizes the learning model with time-sensitive models (Qin et al.,
2022; Nasery et al., 2021a; Bai et al., 2023). (Qin et al., 2022) proposes to tackle the challenges of
covariate shift and concept shift, with the probabilistic model incorporating variational inference.
The second group (Zeng et al., 2023; 2024) maps the source data into future data leveraging evolving
patterns. However, these methods still suffer from limited timestamps, which hinder the capture of
temporal trends. In contrast, we propose to construct IFGET, where the temporal gaps are filled with
interpolations.
Ordinary Differential Equations (ODE)/ Stochastic Differential Equations (SDE) In recent
years, Neural-ODEs (Chen et al., 2018b; Sun et al., 2020) have emerged as a powerful tool for
continuous-time representation of neural networks. SDEs (Øksendal & Øksendal, 2003; Kong et al.,
2020; Liu et al., 2020b) have incorporated stochastic terms into ODE solvers, injecting the model
with slight random noise to improve generalization ability and noise robustness. To learn SDE neural
networks’ parameters, (Ryder et al., 2018; Xu et al., 2022; Li et al., 2020) use variational inference
1Our code is available at https://github.com/HardworkingPearl/SDE-EDG-official.
2

Published as a conference paper at ICLR 2024
technology and overcome the overfitting problem. SDE has shown excellent performance in machine
learning applications, such as generative adversarial model (GAN) (Kidger et al., 2021; Park et al.,
2021a), score-based diffusion model (Song et al., 2020). In this work, we propose a novel approach
to modeling Evolving Domain Generalization (EDG) as a dynamical system by utilizing SDEs to
effectively represent the continuously evolving trajectory of the data representations, and we apply
maximum likelihood to efficiently fit the latent evolving trajectories utilizing SDEs.
3
PRELIMINARIES
Ordinary / Stochastic Differential Equations
Neural ordinary differential equations (Neural
ODEs) (Chen et al., 2018a) approximate the evolving dynamics with the ordinary differential equation
and are defined as
zt = z0 +
Z t
0
f(zs, s)ds,
(1)
where the hidden state zt ∈Rd evolves with certain dynamics characterized by a neural network
f : Rd →Rd, z0 is the initial state, and s represents time in integrals. An SDE can be regarded as an
ODE injected with noise over time:
zt = z0 +
Z t
0
f(zs, s)ds +
Z t
0
g(zs, s)dBs,
(2)
where zt is a latent state that evolves over time, f : Rd × R →Rd is the drift function to capture
the evolving dynamics, g : Rd × R →Rd×ω is the diffusion funtion to reflect the uncertainties, and
Bs is an d-dimensional Brownian motion (Wiener Process). SDE has shown superior performance
in modeling the dynamical system (Park et al., 2021a;b; Øksendal & Øksendal, 2003). Under the
EDG settings, the drift function of SDE describes the trends of the distribution shift over time, and
the diffusion function models the samples’ individual stochastics in their representation space.
Evolving Domain Generalization
Let D(x, y, t) be the probability distribution that characterizes
temporal dynamics of an instance x ∈X and its label y ∈Y, in which there exist underlying
evolving patterns over t. In Evolving Domain Generalization (EDG), we are given M source domains
{Sm}M
m=1, where Sm = {(xi|m, yi|m)}N
i=1 is the data of m-th domain sampled from D(x, y|tm) at
the timestamp tm ∈[0, T], and N is the sample size of the m-th domain. Note that most existing
works (Bai et al., 2023; Nasery et al., 2021b; Qin et al., 2022; Zeng et al., 2023) assume a constant
interval ∆t between two consecutive domains. In contrast, our approach exhibits flexibility in tackling
the EDG problem even with irregular time intervals. In the proposed method, we will learn the
evolving dynamics in a latent space Z, given the practical advantages (Kirchmeyer et al., 2022),
e.g., improved discrimination, dimension reduction, and computation resources savings compared
to operations in the original input space X. Specifically, for every instance x, we encode it with a
feature extractor ϕ : X →Z, and we obtain the embedded feature z = ϕ(x) ∈Z. We focus on the
dynamics of D(z, y, t) throughout this work.
The goal of EDG is to learn a robust and generalizable model from source domains by capturing
and leveraging the evolving pattern learned from source domains so that it can perform well on
the unseen target domains in the path space (Boué & Dupuis, 1998) at L timestamps tM+l ∈
{tM+1, . . . , tM+L} ∈(T, T + T ∗] (T ∗= tM+L −T):
min
θ
Rν(hθ) = min
θ
E(z,y)∼D(zM+1:M+L,yM+1:M+L)[hθ(z) ̸= y],
(3)
where ν is the distribution of the stochastic path (Boué & Dupuis, 1998) of D along timestamps T
to T + T ∗, zM+1:M+L and yM+1:M+L are short for z and y at the timestamps {tM+1, . . . , tM+L},
Rν is the risk of a learning model hθ parameterized by parameters θ.
4
METHODS
With the prior knowledge of SDEs and EDG, we will now formally present our SDE-EDG approach:
To build IFGET (section 4.1), we search sample-to-sample correspondence, which aids in generation
of continuous interpolations; Neural SDEs models the trajectories of latent representations (Section
4.2); we construct IFGET and employ it as a regularization mechanism to promote the learning of
evolving representations while avoiding the acquisition of invariant representations (Section 4.3).
3

Published as a conference paper at ICLR 2024
4.1
CONSTRUCT INFINITELY FINED-GRID EVOLVING TRAJECTORY
𝒁𝒎
𝑿𝒎
𝒀𝒎
𝒁𝒎+𝟏
෡𝒁𝒎+𝝀
𝝀
෡𝒁𝒎+𝝀= Interp(𝒁𝒎, 𝒁𝒎+𝟏, 𝝀)
𝒁𝒎−𝟏
Figure 1: The graphical model
depicts the data generation pro-
cess of SDE-EDG, where we hide
the label superscript k = Ym and
the sample index i. Xm is the in-
put sample (for example, images)
at tm, Ym is the label at tm, and
Zm is the latent representation at
tm. The continuous-interpolation
ˆZm+λ is interpolated by the sam-
ple pair from IFGET.
In EDG, datasets have a considerably small size of do-
mains/timestamps (e.g., at most 30 domains) (Yao et al., 2022).
In contrast, models are trained on the historical data for time-series
forecasting tasks spanning over at least hundreds of timestamps to
predict future states (Addison Howard inversion, 2020; Yu et al.,
2016).
In light of this obstacle, we generate intermediate domains by ap-
plying interpolations between two consecutive domains. To ensure
such interpolations reflect the evolving pattern of the underlying
trajectory over domains, one should have the complete trajectory of
each individual sample across domains. For example, in weather
forecasting, one must have the historical meteorological data of each
individual observation station to characterize the climate change
trends. Unfortunately, such trajectories usually do not exist in EDG
as there is no sample-to-sample correspondence across domains (e.g.,
we may not have images of the same person at different age stages),
thereby preventing the model from tracking the complete trajectories
and extracting the evolving trends. To address this issue, we propose
to identify sample correspondence between timestamps, which is
critical to better alignment of the data structure across domains (Lu et al., 2023; Chen et al., 2022;
Blitzer et al., 2006; Das & Lee, 2018).
Specifically, for each class k, we take the closest sample at tm+1 to the datapoint zk
m at tm, as its
subsequent state at time tm+1 to build sample-to-sample correspondence:
˜zk
i|m+1 =
argmin
zk
j|m+1∈Sk
m+1
Dist(zk
i|m, zk
j|m+1),
(4)
where Dist : Z × Z →[0, +∞) is a distance metric defined over the embedding space, Sk
m+1 be the
set of NB data points sampled from Dm+1 (short for D(z, y|tm+1)) with class y = k ∈{1, ..., K}
in a training iteration. The rationale here lies in the decomposition of latent variables into class-
dependent and domain-dependent evolving components (Qin et al., 2022), resulting in samples
from the same class in the m-th and (m+1)-th domains exhibiting smaller distances due to such
sample pair’s shared class-dependent similarities, while the evolving difference maintains consistent
magnitude. Utilizing sample-to-sample correspondence, we gather discrete samples within IFGET.
To render it continuous, we generate continuously-interpolated samples bridging the temporal gaps.
With the sample correspondence, we leverage the interpolation function to generate continuous-
interpolated samples, such that the interpolation is generated along the approximated individual
trajectory of a data point as shown in Figure 1
ˆzi|m+λ = Interp(zk
i|m, ˜zk
i|m+1, λ) = (1 −λ)zk
i|m + λ˜zk
i|m+1, ∀zk
i|m ∈Sk
m
(5)
where the interpolation rate λ ∈(0, 1) is sampled from a Beta distribution B(β1, β2), β1 and β2 are
the parameters of the Beta distribution, and Sk
m consists of instances sampled from k-th class of m-th
domain. Here we apply a linear interpolation (Yan et al., 2020; Zhang et al., 2018) as the interpolation
function. The continuous-interpolated samples bridge temporal gaps in the discrete evolving trajectory,
converting it into an infinitely fine-grained trajectory due to λ ∈(0, 1). Specifically, as λ can be any
value between (0, 1), it enables us to approximate time moments between the m-th and (m + 1)-th
timestamps. We theoretically show that the sample complexity of EDG reduces with a smaller
temporal interval in Theorem D.3, which leads to a reduction in error. We take interpolations as
approximations of samples between time intervals, leading to a smaller time interval and thus a
smaller sample complexity. Above all, we construct the Infinitely Fined-grid Evolving Trajectory
{zk
i|m, ˆzk
i|m+λ, ˜zk
i|m+1}M−1
m=1 by leveraging the sample-to-sample correspondence, and collecting the
interpolations.
4.2
MODELING EDG WITH STOCHASTIC DIFFERENTIAL EQUATIONS
The continuous trajectory in Section 4.1 significantly enhances the capability to capture evolving
patterns, but existing EDG methods can not handle the continuous timestamp data. Hence, we
4

Published as a conference paper at ICLR 2024
propose to model the data of EDG in the representation space with neural SDEs, since Neural SDEs
naturally model continuous temporal trajectories. In contrast, traditional temporal models such as
LSTM (Hochreiter & Schmidhuber, 1997) and Markov models (Bishop & Nasrabadi, 2006) are only
able to model discrete timestamps.
Here, SDE-EDG learns the temporal dynamics governing the semantic conditional distributions
D(z|y, t) over time. Specifically, SDE-EDG models the temporal trajectory of the data point from
the domain at tm to the arbitrary future timestamp tm′ : tm′ > tm of each class k ∈{1, . . . , K}:
ˆzk
m′ = zk
m +
Z tm′
tm
fk(ˆzk
s , s)ds +
Z tm′
tm
gk(ˆzk
s , s)dBs,
(6)
where the latent variable ˆzk
m′ is transformed from m-th domains latent variable zk
m, and fk is the
drift function of the k-th class to capture the evolving patterns, and gk is the diffusion function of
the k-th class to characterize the stochastics of the latent representations. Note that z is the latent
variable (representation) induced by z = ϕ(x), but ˆz is the synthetic feature generated by Eq. (6).
Hence, SDE-EDG can generate the latent continuous trajectory by gradually transforming the sample
representation from the current timestamp m to any desired future timestamp m′. Thereby, our latent
trajectories of SDE-EDG could effectively align with the collected continuous trajectories IFGET,
which prevents overfitting to sparse timestamps.
We design two objective functions to learn the drift functions f = {fk}K
k=1 and diffusion functions
g = {gk}K
k=1 characterized by neural networks: one is aimed to impose Path Alignment Loss in
Eq. (7), and another one is downstream classification loss in Eq. (10). By jointly optimizing {ϕ, f, g}
w.r.t these two losses, our approach achieves improved performance on EDG.
4.3
ALIGN SDE-EDG WITH IFGET VIA MAXIMUM LIKELIHOOD
Neural SDEs are designed to capture the dynamics and evolution of data over time, particu-
larly in continuous spaces. To fit the SDE-EDG into the evolving stochastic path given obser-
vations, we propose the path alignment regularizer by maximizing its likelihood of the IFGET
{zk
i|m, ˆzk
i|m+λ, ˜zk
i|m+1}M−1
m=1 :
Jmle =
M
X
m=1
K
X
k=1
NB
X
i=1
−
1
MKNB

log D
 z = ˜zk
i|m+1
z = zk
i|m
 + log D
 z = ˆzk
i|m+λ
z = zk
i|m

,
(7)
Figure 2: The left and right images depict representations acquired for
the Circle dataset through the SDE-EDG and IRM by ϕ. Distinct classes
are distinguished by different shapes (triangles and circles), while various
domains are denoted by different colors as indicated by the rainbow bar.
SDE-EDG successfully learns representations with a discernible decision
boundary, whereas IRM collapses towards a single direction, failing to
depict a clear decision boundary.
Filling
the
gap
between
domains
with
continuous-
interpolated
samples
results
in a continuous and smooth
evolving trajectory over time.
Taking Jmle as a regularizer
brings two advantages to EDG
model training: 1) Empirically,
the training process of neural
SDEs converges faster with
Jmle,
as shown in Figure
4a.
2) Jmle regularizes the
latent space to capture evolving
patterns.
This contributes to
learning the evolving patterns in
the EDG problem and improves
the generalization capability
to target domains as shown in Figure 2. On the other hand, in the absence of Jmle, the model
learns invariant representations across domains, leading to the occurrence of the Neural Collapse
phenomenon (Han et al., 2022), where the latent representations of the same class across the domain
collapse to a single point. Consequently, no evolving patterns manifest in the latent representation
space as shown in Figure 3.
5

Published as a conference paper at ICLR 2024
(a) Ground truth
(b) ERM
(c) SDE-EDG
(d) Paths w/ Jmle
(e) Paths w/o Jmle
Figure 3: (a) Ground truth of the Sine dataset, and positive and negative labels are red and blue dots separately.
(b-c) show prediction results to the Sine made by ERM, and SDE-EDG respectively, positive and negative
predictions are red and blue dots separately. (d) Visualized learned evolving paths (synthetic latent variables)
with the Path Alignment loss Jmle by SDE-EDG learned from the Sine dataset. (e) Synthetic latent variables
without the Path Alignment loss Jmle by SDE-EDG learned from the Sine dataset. With Jmle, the latent
evolving dynamics can be correctly characterized, and SDE-EDG can capture the evolving patterns.
4.4
SDE-EDG FOR THE PREDICTION LOSS
In this section, we formulate our approach for handling downstream classification tasks. With the
Bayes rule, the predictive distribution is
D(y = k|z, t = tm) =
D(z|y = k, t = tm) × D(y = k|t = tm)
PK
k′=1 D(z|y = k′, t = tm) × D(y = k′|t = tm)
,
(8)
where we model D(z|y = k, t = tm) with non-parametric model, and D(y|t = tm) as a neural
net with input as timestamp t, function denoted as r(t). In each iteration, we first compute label
distribution with respect to time D(y|t = tm) = [D(y = 1|t = tm), . . . , D(y = K|t = tm)].
Specifically, D(y = k|t = tm) =
|Sk
m|
PK
k′=1 |Sk′
m|, where Sk
m consists of instances sampled from k-th class
of m-th domain, | · | denotes the size of the set. r is optimized by minimizing ||
|Sk
m|
PK
k′=1 |Sk′
m| −r(tm)||.
The conditional distribution D(z|y, t) modeled by SDEs lacks an analytic expression, and here
we approximate it with a non-parametric model. Given that distributions characterized by SDEs
may exhibit either uni-modal or multi-modal patterns, it’s also advantageous to model multi-modal
representations with Neural SDEs (Min et al., 2023). In this context, we present the multi-modal
classification loss here but leave the uni-modal loss in the appendix A due to space limitation. To
preserve the multi-modal pattern of the latent variables, we employ the non-parametric distribution
density method Parzen Window (Parzen, 1962)
D(z|y = k, t = tm) =
P
ˆzi∈ˆSkm −exp(−Dist(z, ˆzi))
|ˆSkm|
(9)
where ˆSk
m includes instances sample from learned SDE-EDG belong to k-th class of m-th domain. By
incorporating the estimations of label distribution (D(y|t)) and conditional distribution (D(z|y, t)),
our predictions encompass the temporal evolution of both D(z|y, t) and D(y|t).
Model optimization proceeds by minimizing the negative log probability:
Jcls =
M
X
m=1
K
X
k=1
NB
X
i=1
−
1
MKNB
log D(y = k|z = zi, t = tm)
(10)
The ultimate objective function is J = Jcls + αJmle The Maximum Likelihood Loss Jmle is a path
alignment regularizer that aims to fit the stochastic evolving paths. The hyper-parameter α > 0 is the
weighting of Jmle to adjust its contribution to the overall loss.
5
EXPERIMENTS
To evaluate the effectiveness of SDE-EDG, we verify our method on various datasets, Rotated
Gaussian, Sine, and Circle datasets, Rotated MNIST, Portraits, Caltran, Power Supply, and Ocular
6

Published as a conference paper at ICLR 2024
Algorithm 1 SDE-EDG (an iteration during the training phase)
1: Input: {S1, S2, ..., SM}: M data sets from consecutive domains. NB: the number of instances
sampled for each class in an iteration. RANDOMSAMPLE(S, N): a set of N instances sampled
uniformly from the set S without replacement. J ←0
2: for m in {1, ..., M −1} do
3:
for k in {1, ..., K} do
4:
Sk
m ←RANDOMSAMPLE(Sk
m, NB)
5:
ˆzk
m+1 = zk
m +
R tm+1
tm
fk(ˆzk
s , s)ds +
R tm+1
tm
gk(ˆzk
s , s)dBs is computed for zk
m ∈Sk
6:
Sk
m+1 ←RANDOMSAMPLE(Sk
m+1, NB)
7:
Use Eq. (4) to search subsequent state ˜zk
m+1.
8:
Use Eq. (5) to generate continuous-interpolated samples ˆzm+λ.
9:
J ←J + α · Jmle, where Jmle is calculated w.r.t Eq. (7).
10:
for (zi, yi) in Sk
m+1 do
11:
J ←J +
1
MKNB log D(y = yi|z = zi, t = tm)
12: Optimize the loss w.r.t. J
Disease. The objective of this section aims to answer the following key questions (1) What are the
evolving trajectories that SDE-EDG learns, and what is the nature of its learning process? (Shown in
Figure 3) (2) How does SDE-EDG compare to other methods in improving EDG performance? (such
as Table 1, Figure 5, and Figure 4 (a)-(b)) (3) What is the influence of the Maximum Likelihood Loss
on the performance of SDE-EDG? (In Figure 3 (d-e) and Figure 4 (c)-(d))
5.1
EXPERIMENTAL SETUP
Dataset: Rotated Gaussian (Zeng et al., 2023) consists of 30 domains where each domain has 500
instances generated by the same Gaussian distribution, but the decision boundary rotates from 0◦to
338◦with an interval of 12◦. We split the domains into source domains (1-22 domains), intermediate
domains (22-25 domains), and target domains (26-30 domains). The intermediate domains are utilized
as the validation set. Circle (Pesaranghader & Viktor, 2016) contains evolving 30 domains where the
instance are sampled from 30 2D Gaussian distributions. The label is assigned using a half-circle
curve as the decision boundary (15 source domains, 5 validation domains, and 10 target domains).
Sine (Pesaranghader & Viktor, 2016) The label is assigned using a sine curve as the decision boundary.
We rearrange this dataset by extending it to 24 evolving domains. (12 source domains, 4 validation
domains, and 8 target domains) Rotated MNIST (RMNIST) (RMNIST) (Ghifary et al., 2015)
is composed of MNIST digits of various rotations. We follow (Qin et al., 2022) and extend it to
19 evolving domains via applying the rotations with degree of {0◦, 15◦, 30◦, . . . , 180◦} in order
(10 source domains, 3 validation domains, and 6 target domains). Portraits (Ginosar et al., 2015)
(Yearbook (Yao et al., 2022)) is a real-world dataset that comprises photos of American high school
seniors collected over 108 years (1905-2013) for gender classifications. The dataset is divided into 34
domains (19 source domains, 5 validation domains, and 10 target domains). Caltran (Hoffman et al.,
2014) consists of real-world images captured by a fixed traffic camera deployed in an intersection over
time. We divide it into 34 domains by time. The task of Caltran is to classify scenes to identify the
presence of one or more vehicles in or approaching the intersection (19 source domains, 5 validation
domains, and 10 target domains). PowerSupply (Dau et al., 2019) is created for the purpose of
predicting the current power supply based on hourly records from an Italian electricity company. It
includes 30 domains based on days and each data point is labeled as either morning or afternoon
(15 source domains, 5 validation domains, and 10 target domains). Ocular Disease (Kaggle, 2020)
Ocular Disease Intelligent Recognition (ODIR) is set with three classes: Normal, Diabetes and other
diseases. Following the EDG setup, we sort the photographs in ascending order of the age of the
patients (27 source domains, 2 validation domains, and 4 target domains).
Baselines We compare with following baselines: (1) ERM (Vapnik, 1999); (2) Mixup (Yan et al.,
2020); (3) MMD (Li et al., 2018b); (4) MLDG (Li et al., 2018a); (5) IRM (Arjovsky et al., 2019); (6)
RSC (Huang et al., 2020); (7) MTL (Blanchard et al., 2021); (8) Fish (Shi et al., 2021); (9) CORAL
(Sun & Saenko, 2016); (10) AndMask (Parascandolo et al., 2020); (11) DIVA (Ilse et al., 2020); (12)
LSSAE (Qin et al., 2022); (13) GI (Nasery et al., 2021a) (14) DDA (Zeng et al., 2023) (15) DRAIN
7

Published as a conference paper at ICLR 2024
Table 1: The comparison of the classification accuracy (%) between SDE-EDG and other baseline methods
across the synthetic and real-world datasets. The reported results are the average accuracy of the multiple target
domains. ("RG" for RotatedGaussian, "Cir" for Circle, "RM" for Rotated MNIST, "Por" for Portraits, "Cal" for
Caltran, "PS" for PowerSupply, "OD" for OcularDisease. GI fails to complete OD due to high time complexity.)
ALGORITHM
RG
CIR
SINE
RM
POR
CAL
PS
OD
AVG
DG METHODS
ERM
59.0
49.9
63.0
43.6
87.8
66.3
71.0
57.9
62.3
MIXUP
55.4
48.4
62.9
44.9
87.8
66.0
70.8
59.7
62.0
MMD
56.0
50.7
55.8
44.8
87.3
57.1
70.9
57.6
60.0
MLDG
59.9
50.8
63.2
43.1
88.5
66.2
70.8
43.9
60.8
IRM
47.5
51.3
63.2
39.0
85.4
64.1
70.8
46.2
58.4
RSC
32.8
48.0
61.5
41.7
87.3
67.0
70.9
54.5
58.0
MTL
59.0
51.2
62.9
41.7
89.0
68.2
70.7
59.7
62.8
FISH
41.6
48.8
62.3
44.2
88.8
68.6
70.8
48.2
59.2
CORAL
53.0
53.9
51.6
44.5
87.4
65.7
71.0
60.1
60.9
ANDMASK
76.3
47.9
69.3
42.8
70.3
56.9
70.7
51.2
60.7
DIVA
56.6
67.9
52.9
42.7
88.2
69.2
70.8
53.1
62.7
EDG METHODS
LSSAE
48.7
73.8
71.4
46.4
89.1
70.6
71.1
52.3
65.4
GI
50.8
54.4
65.2
44.6
88.1
70.7
71.4
-
-
DDA
66.8
51.2
66.6
45.1
87.9
66.1
70.9
55.8
63.8
DRAIN
61.0
50.7
71.3
43.8
89.4
69.0
71.0
58.7
64.4
SDE-EDG
97.7
81.5
72.2
52.6
89.6
71.3
75.7
62.6
75.4
(Bai et al., 2023). All experimental implementations are conducted using the PyTorch packages
and are based on the DomainBed (Gulrajani & Lopez-Paz, 2020). To ensure a fair comparison, the
neural network architecture (shown in Appendix C.2) of the encoding and classification parts are
kept constant across all baselines used in different benchmarks. Five independent experiments with
different random seeds are repeated to reduce the variances.
5.2
EXPERIMENTAL RESULTS
SDE-EDG Aligns with the Evolving Trajectories
To find out what SDE-EDG learns, we visualize
the temporal trajectories sampled by SDE-EDG in Fig 3d. It should be noted that SDE-EDG learns
the evolving dynamics in the latent space Z, which is not directly interpretable. To address this issue,
we use an Identity function as the encoding function, which enables us to learn the dynamics directly
in raw data space X. Learning in original data space is more challenging but provides us with a more
intuitive understanding of the learned dynamics.
Fig 3 shows that the source domains data are between the range [−π
2 , 0], which means we only train
machine learning models with the half sine. The trajectories of the same class are compact within this
range, and the boundaries between them have a large margin to ensure good performance. However,
the trajectories become looser and the margins become smaller as we move into unobserved domains
with timings in the range (0, π
2 ]. With a much longer time gap from the source domains, SDE-EDG
will eventually fail due to a larger discrepancy between learned paths and the ground truth.
Quantitative Results
The experimental results of SDE-EDG and other baselines are presented in
Table 1, which shows the accuracy the average accuracy for all the target domains (complete results
for each domain shown in Table 3-10 for space limitations). SDE-EDG surpasses other baselines’
average accuracy on all datasets. The results indicate a significant improvement over the compared
traditional DG methods, which is a reasonable finding since traditional DG methods do not address
evolving patterns in EDG. Furthermore, SDE-EDG outperforms LSSAE, DDA, and DRAIN (most
recently EDG method) by 10.0%, 11.6%, and 11.0% on overall average accuracy, demonstrating the
superior ability of our method to capture evolving patterns. The significant accuracy improvements
observed in Table 1 indicate consistent enhancements in the performance of EDG tasks. In addition,
EDG methods perform consistently better than DG methods, showing the importance of modeling
evolving patterns to improve prediction performance in EDG tasks. In particular, the OcularDisease
dataset is a challenging medical image classification task, significantly more complex than standard
image classification. SDE-EDG demonstrates its ability to capture evolving patterns in demanding
real-world scenarios.
8

Published as a conference paper at ICLR 2024
(a) Training Acc
(b) RMNIST
(c) Circle
(d) Alation on RM
(e) Alation on PS
Figure 4: (a) Training accuracy convergence trajectories on Portraits (b)-(c) Accuracy with the domain index of
RMNIST and Circle Datasets, respectively. (d)-(e) Effects of weighting α on the Maximum Likelihood Loss on
RMNIST (RM) and PowerSupply (PS).
Table 2: Rotated MNIST with different temporal gaps ∆t (t here represents domain index).
∆t
t
130◦
140◦
150◦
160◦
170◦
180◦
AVG
∆t/2
75.6 ± 0.8
61.8 ± 0.8
49.9 ± 0.8
50.0 ± 0.9
45.1 ± 0.7
44.1 ± 0.9
54.4
∆t
75.1 ± 0.8
61.3 ± 0.9
49.8 ± 0.8
49.8 ± 0.8
39.7 ± 0.7
39.7 ± 0.9
52.6
∆t × 2
58.6 ± 0.8
49.1 ± 0.7
45.6 ± 0.7
42.4 ± 0.8
36.9 ± 0.8
36.1 ± 0.8
44.8
Figure 4b and figure 4c plot the accuracy trajectory of the baselines (ERM, MLDG, GI) and SDE-EDG
across domains for RMNIST and Circle datasets, which show the superiority of SDE-EDG on the
other 3 baselines with large margin improvements. SDE-EDG could keep large improvements initially,
but eventually, as the distance between the target and source domains increases, all methods will
achieve similar performance. Therefore, we conclude, that in EDG, models can achieve generalization
in the relatively near future.
Ablations: the Impact of Maximum Likelihood Loss on EDG Classifications
We conducted
an ablation study on the RMNIST and PowerSupply datasets to evaluate the effectiveness of the
proposed Maximum Likelihood Loss Jmle, which aims to train SDE-EDG to fit IFGET. Figure 4d
and Figure 4e show that SDE-EDG achieves the best performance with α = 1 on the RMNIST
dataset and α = 10 on the PowerSupply dataset. The above empirical results suggest that Jmle
improves the performance of EDG by aligning the underlying evolving paths (optimizing the Path
Alignment loss Jpa) and quantifying stochastic uncertainties (minimize Stochastic Uncertainty loss
Jsu), details proof in Appendix D.1. Specifically, when we only apply classification loss with α = 0,
the performance is the worst. On the other hand, with a much larger α = 200, SDE-EDG focuses
on aligning the evolving paths and giving lower importance to classification tasks. Thus, aligning
stochastic evolving processes improves performance in EDG.
Ablations: ∆t influence on EDG performance
In Table 2, we set interval ∆t to 5◦, and 20◦
between source domains with SDE-EDG, where the 10◦interval is in the original setting. With smaller
∆t (∆t × 2 →∆t →∆t/2), the accuracy experiences a consistent improvement, a finding that
aligns with our motivation: a smaller temporal gap between domains reduces the generalization error.
Therefore, interpolations between temporal gaps as an approximation to the sample at an arbitrary
timestamp would lead to reduced ∆t and overcoming overfitting to available limited timestamps.
6
CONCLUSION
This work presents a new approach SDE-EDG for modeling Evolving Domain Generalization (EDG).
Our approach involves constructing IFGET by identifying sample-to-sample correspondence and
generating continuous-interpolated samples via linear interpolations. Subsequently, we employ
Stochastic Differential Equations (SDE) and train it in alignment with IFGET. Our contribution lies
in revealing the importance of capturing the evolving patterns through the collected individual’s
temporal trajectories, and of interpolating between time intervals to mitigate the issue of the limited
number of source timestamps, which effectively prevents SDE-EDG from overfitting to the limited
timestamps. We also provide a theoretical analysis demonstrating that our method can reduce the
generalization risk.
9

Published as a conference paper at ICLR 2024
ACKNOWLEDGEMENTS
We appreciate constructive feedback from anonymous reviewers and meta-reviewers. This work is
supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), Discovery
Grants program.
REFERENCES
Spyros Makridakis vangelis Addison Howard inversion. M5 forecasting - accuracy, 2020. URL
https://kaggle.com/competitions/m5-forecasting-accuracy.
Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Guangji Bai, Chen Ling, and Liang Zhao. Temporal domain generalization with drift-aware dynamic
neural networks. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=sWOsRj4nT1n.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, volume 4.
Springer, 2006.
Gilles Blanchard, Aniket Anand Deshmukh, Ürun Dogan, Gyemin Lee, and Clayton Scott. Domain
generalization by marginal transfer learning. The Journal of Machine Learning Research, 22(1):
46–100, 2021.
John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 conference on empirical methods in natural language
processing, pp. 120–128, 2006.
Michelle Boué and Paul Dupuis. A variational representation for certain functionals of brownian
motion. The Annals of Probability, 26(4):1641–1659, 1998.
Liang Chen, Yihang Lou, Jianzhong He, Tao Bai, and Minghua Deng. Geometric anchor correspon-
dence mining with uncertainty modeling for universal domain adaptation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16134–16143, 2022.
Qi Chen, Changjian Shui, Ligong Han, and Mario Marchand. On the stability-plasticity dilemma
in continual meta-learning: Theory and algorithm. Advances in Neural Information Processing
Systems, 36, 2024.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Advances in neural information processing systems, 31, 2018a.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. Advances in neural information processing systems, 31, 2018b.
Debasmit Das and CS George Lee. Sample-to-sample correspondence for unsupervised domain
adaptation. Engineering Applications of Artificial Intelligence, 73:80–91, 2018.
Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh
Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive.
IEEE/CAA Journal of Automatica Sinica, 6(6):1293–1305, 2019.
Li Deng. The mnist database of handwritten digit images for machine learning research [best of the
web]. IEEE signal processing magazine, 29(6):141–142, 2012.
Seth R Flaxman, Rupert RA Bourne, Serge Resnikoff, Peter Ackland, Tasanee Braithwaite, Maria V
Cicinelli, Aditi Das, Jost B Jonas, Jill Keeffe, John H Kempen, et al. Global causes of blindness
and distance vision impairment 1990–2020: a systematic review and meta-analysis. The Lancet
Global Health, 5(12):e1221–e1234, 2017.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization
for object recognition with multi-task autoencoders. In Proceedings of the IEEE international
conference on computer vision, pp. 2551–2559, 2015.
10

Published as a conference paper at ICLR 2024
Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, and Alexei A Efros. A century of portraits:
A visual historical record of american high school yearbooks.
In Proceedings of the IEEE
International Conference on Computer Vision Workshops, pp. 1–7, 2015.
Hans E Grossniklaus, John M Nickerson, Henry F Edelhauser, Louise AMK Bergman, and Lennart
Berglin. Anatomic alterations in aging and age-related diseases of the eye. Investigative ophthal-
mology & visual science, 54(14):ORSF23–ORSF27, 2013.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
X.Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under MSE loss: Proximity to and
dynamics on the central path. In International Conference on Learning Representations, 2022.
URL https://openreview.net/forum?id=w1UbdvWH_R3.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.
Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolving
visual domains. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 867–874, 2014.
Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain
generalization. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August
23–28, 2020, Proceedings, Part II 16, pp. 124–140. Springer, 2020.
Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant
variational autoencoders. In Medical Imaging with Deep Learning, pp. 322–348. PMLR, 2020.
Kaggle.
Kaggle: ocular disease recognition.
https://www.kaggle.com/andrewmvd/
ocular-disease-recognition-odir5k, 2020. Accessed: 2022-06-15.
Patrick Kidger, James Foster, Xuechen Li, and Terry J Lyons. Neural sdes as infinite-dimensional
gans. In International Conference on Machine Learning, pp. 5453–5463. PMLR, 2021.
Matthieu Kirchmeyer, Alain Rakotomamonjy, Emmanuel de Bezenac, and patrick gallinari. Mapping
conditional distributions for domain adaptation under generalized target shift. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?
id=sPfB2PI87BZ.
Lingkai Kong, Jimeng Sun, and Chao Zhang. Sde-net: Equipping deep neural networks with
uncertainty estimates. In International Conference on Machine Learning, pp. 5405–5415. PMLR,
2020.
Atsutoshi Kumagai and Tomoharu Iwata. Learning non-linear dynamics of decision boundaries
for maintaining classification performance. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 31, 2017.
Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain
adaptation. In International Conference on Machine Learning, pp. 5468–5479. PMLR, 2020.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning
for domain generalization. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018a.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial
feature learning. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 5400–5409, 2018b.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. In International Conference on Artificial Intelligence and
Statistics, pp. 3870–3882. PMLR, 2020.
11

Published as a conference paper at ICLR 2024
Hong Liu, Mingsheng Long, Jianmin Wang, and Yu Wang. Learning to adapt to evolving domains.
Advances in neural information processing systems, 33:22338–22348, 2020a.
Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. How does noise help
robustness? explanation and exploration under the neural sde framework. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 282–290, 2020b.
Xuanchen Lu, Xiaolong Wang, and Judith E Fan. Learning dense correspondences between photos
and sketches. 2023.
Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Adagraph: Unifying
predictive and continuous domain adaptation through graphs. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 6568–6577, 2019.
Zelda Mariet and Vitaly Kuznetsov. Foundations of sequence-to-sequence modeling for time series.
In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of the Twenty-Second Interna-
tional Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine
Learning Research, pp. 408–417. PMLR, 16–18 Apr 2019.
Ming Min, Ruimeng Hu, and Tomoyuki Ichiba. Directed chain generative adversarial networks.
arXiv preprint arXiv:2304.13131, 2023.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.
Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant
feature representation. In International conference on machine learning, pp. 10–18. PMLR, 2013.
Anshul Nasery, Soumyadeep Thakur, Vihari Piratla, Abir De, and Sunita Sarawagi. Training for
the future: A simple gradient interpolation loss to generalize along time. Advances in Neural
Information Processing Systems, 34:19198–19209, 2021a.
Anshul Nasery, Soumyadeep Thakur, Vihari Piratla, Abir De, and Sunita Sarawagi. Training for
the future: A simple gradient interpolation loss to generalize along time. Advances in Neural
Information Processing Systems, 34:19198–19209, 2021b.
A Tuan Nguyen, Toan Tran, Yarin Gal, and Atilim Gunes Baydin. Domain invariant representation
learning with domain density transformations. Advances in Neural Information Processing Systems,
34:5264–5275, 2021.
Bernt Øksendal and Bernt Øksendal. Stochastic differential equations. Springer, 2003.
Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard
Schölkopf. Learning explanations that are hard to vary. arXiv preprint arXiv:2009.00329, 2020.
Sung Woo Park, Dong Wook Shu, and Junseok Kwon. Generative adversarial networks for markovian
temporal dynamics: Stochastic continuous data generation. In International Conference on
Machine Learning, pp. 8413–8421. PMLR, 2021a.
Sunghyun Park, Kangyeol Kim, Junsoo Lee, Jaegul Choo, Joonseok Lee, Sookyung Kim, and Edward
Choi. Vid-ode: Continuous-time video generation with neural ordinary differential equation. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 2412–2422, 2021b.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathemat-
ical statistics, 33(3):1065–1076, 1962.
Ali Pesaranghader and Herna L Viktor. Fast hoeffding drift detection method for evolving data
streams. In Machine Learning and Knowledge Discovery in Databases: European Conference,
ECML PKDD 2016, Riva del Garda, Italy, September 19-23, 2016, Proceedings, Part II 16, pp.
96–111. Springer, 2016.
Tiexin Qin, Shiqi Wang, and Haoliang Li. Generalizing to evolving domains with latent structure-
aware sequential autoencoder. In International Conference on Machine Learning, pp. 18062–18082.
PMLR, 2022.
12

Published as a conference paper at ICLR 2024
Tom Ryder, Andrew Golightly, A Stephen McGough, and Dennis Prangle. Black-box variational
inference for stochastic differential equations. In International Conference on Machine Learning,
pp. 4423–4432. PMLR, 2018.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
arXiv preprint arXiv:1911.08731, 2019.
Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel
Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.
Changjian Shui, Qi Chen, Jun Wen, Fan Zhou, Christian Gagné, and Boyu Wang. A novel domain
adaptation theory with jensen–shannon divergence. Knowledge-Based Systems, 257:109808, 2022.
ISSN 0950-7051.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456, 2020.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
Computer Vision–ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16,
2016, Proceedings, Part III 14, pp. 443–450. Springer, 2016.
Yifan Sun, Linan Zhang, and Hayden Schaeffer. Neupde: Neural network based ordinary and partial
differential equations for modeling time-dependent data. In Mathematical and Scientific Machine
Learning, pp. 352–372. PMLR, 2020.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks,
10(5):988–999, 1999.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Hao Wang, Hao He, and Dina Katabi. Continuously indexed domain adaptation. arXiv preprint
arXiv:2007.01807, 2020.
Haoxiang Wang, Bo Li, and Han Zhao. Understanding gradual domain adaptation: Improved analysis,
optimal path and beyond. In International Conference on Machine Learning, pp. 22784–22801.
PMLR, 2022.
Winnie Xu, Ricky TQ Chen, Xuechen Li, and David Duvenaud. Infinitely deep bayesian neural net-
works with stochastic differential equations. In International Conference on Artificial Intelligence
and Statistics, pp. 721–738. PMLR, 2022.
Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain
adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020.
Huaxiu Yao, Caroline Choi, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark
of in-the-wild distribution shift over time. In ICML 2022 Shift Happens Workshop, 2022. URL
https://openreview.net/forum?id=BUQD1tJ2UwK.
Hsiang-Fu Yu, Nikhil Rao, and Inderjit S Dhillon. Temporal regularized matrix factorization for
high-dimensional time series prediction. Advances in neural information processing systems, 29,
2016.
Qiuhao Zeng, Wei Wang, Fan Zhou, Charles Ling, and Boyu Wang. Foresee what you will learn: Data
augmentation for domain generalization in non-stationary environment. Proceedings of the AAAI
Conference on Artificial Intelligence, 37(9):11147–11155, Jun. 2023. doi: 10.1609/aaai.v37i9.
26320. URL https://ojs.aaai.org/index.php/AAAI/article/view/26320.
13

Published as a conference paper at ICLR 2024
Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian Shui, Christian Gagne,
Shichun Yang, Boyu Wang, and Charles X Ling. Generalizing across temporal domains with
koopman operators. arXiv preprint arXiv:2402.07834, 2024.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=r1Ddp1-Rb.
14

Published as a conference paper at ICLR 2024
A
UNI-MODAL CLASSIFICATION LOSS
We calculate the Evolving Centroid of domain at tm as the mean vector of the instances belonging
to ˆSk
m, ˆck
m =
1
NB
P
(ˆzk
i|m,ˆyk
i|m)∈ˆSkm ˆzk
i|m, where ˆSk
m be the set of NB data points sampled from ˆDm
(short for ˆD(z, y|tm)) in a training iteration by SDE-EDG, i|m stands for the i-th instance at time
tm, and NB is the number of instances sampled in a training iteration belonging to k-th class of the
domain at timestamp tm.
At each timestamp, the Evolving Centroids exhibit holistic evolving patterns reflecting the dynamics
of class-conditional distribution. This differs from the conventional Domain Adaptation (DA) or
Domain Generalization (DG) methods, which aim to learn the invariant features by maintaining a
global centroid for each class(Arjovsky et al., 2019; Li et al., 2018b; Muandet et al., 2013; Nguyen
et al., 2021). Instead, we do not aim to align the class centroid across all domains but allow the
Evolving Centroids to evolve over time, which offers a more accurate representation of the data
distribution’s temporal changes. The predictive distribution for Dm is
D(y = k|z, t = tm) =
exp(−Dist(z, ˆck
m))
PK
k′=1 exp(−Dist(z, ˆck′
m))
,
(11)
During the training stage, at each step, we sample NB samples from each class k in ˆSm, which is
used to compute Evolving Centroid ˆck
m to make predictions for data in Sm.
In the inference stage, we search the closest Evolving Centroids sampled by SDE-EDG at a given
timestamp, and assign the class of the closest Evolving Centroid as the classification prediction.
B
FULL EXPERIMENTAL RESULTS
In this section, we present complete experimental results to validate the efficacy of our proposed
evolving domain generalization task. Our findings demonstrate that, across a wide range of scenarios,
our approach consistently outperforms existing domain generalization baselines, thus establishing a
new state-of-the-art performance on existing benchmarks. These results affirm the effectiveness of our
method in addressing the challenges posed by evolving domains. By achieving superior performance,
our approach shows its applicability to real-world problems.
Table 3: Rotated Gaussian. We show the results on each target domain by domain index.
ALGORITHM
26
27
28
29
30
AVG
ERM
58.8 ± 1.3
58.0 ± 1.5
57.8 ± 1.3
62.0 ± 1.1
58.6 ± 1.1
59.0
MIXUP
56.2 ± 1.5
63.4 ± 3.0
56.8 ± 1.4
49.4 ± 1.5
41.4 ± 2.0
55.4
MMD
53.8 ± 0.9
53.0 ± 1.0
52.8 ± 1.0
57.0 ± 1.6
63.6 ± 1.9
56.0
MLDG
64.0 ± 1.6
58.8 ± 0.9
57.2 ± 1.0
57.2 ± 1.1
62.2 ± 1.3
59.9
IRM
56.8 ± 1.9
55.8 ± 3.1
51.8 ± 2.3
41.6 ± 1.6
31.4 ± 2.1
47.5
RSC
31.6 ± 1.9
33.8 ± 2.2
31.8 ± 1.6
31.8 ± 2.0
35.2 ± 2.0
32.8
MTL
54.2 ± 2.2
54.6 ± 1.3
58.8 ± 1.4
59.8 ± 1.6
67.6 ± 2.2
59.0
FISH
52.0 ± 2.4
49.6 ± 3.4
44.6 ± 2.0
35.6 ± 1.4
26.2 ± 1.7
41.6
CORAL
54.8 ± 1.6
54.0 ± 0.6
53.8 ± 1.0
52.0 ± 0.8
50.6 ± 1.6
53.0
ANDMASK
81.8 ± 1.2
79.4 ± 1.6
75.4 ± 1.6
75.4 ± 1.3
69.4 ± 1.7
76.3
DIVA
59.0 ± 1.5
55.8 ± 0.9
53.6 ± 0.7
59.2 ± 1.3
55.6 ± 1.5
56.6
LSSAE
50.6 ± 0.9
50.8 ± 2.3
43.4 ± 1.4
48.4 ± 2.4
50.4 ± 2.1
48.7
GI
51.6 ± 2.9
53.2 ± 2.2
47.4 ± 1.8
49.6 ± 2.7
52.4 ± 2.0
50.8
DDA
77.4 ± 1.3
75.0 ± 1.5
68.2 ± 1.1
60.8 ± 1.0
52.8 ± 1.5
66.8
DRAIN
73.2 ± 2.9
70.0 ± 1.7
63.8 ± 2.4
53.2 ± 2.2
45.0 ± 1.2
61.0
SDE-EDG (OURS)
98.4 ± 0.6
97.0 ± 0.6
97.4 ± 0.8
97.8 ± 0.6
97.8 ± 0.7
97.7
15

Published as a conference paper at ICLR 2024
Table 4: Circle. We show the results on each target domain by domain index.
ALGORITHM
21
22
23
24
25
26
27
28
29
30
AVG
ERM
53.9 ± 3.5
55.8 ± 4.8
53.9 ± 5.2
44.7 ± 6.3
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
49.9
MIXUP
48.6 ± 3.8
51.7 ± 4.0
49.4 ± 4.5
43.6 ± 5.8
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
48.4
MMD
50.0 ± 3.9
53.6 ± 4.4
55.0 ± 4.3
51.9 ± 6.0
60.8 ± 3.9
49.7 ± 7.2
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
50.7
MLDG
57.8 ± 3.6
57.7 ± 5.0
55.3 ± 4.9
46.4 ± 6.8
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
50.8
IRM
57.8 ± 3.9
59.4 ± 5.4
56.9 ± 4.9
48.1 ± 7.4
57.5 ± 4.3
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
51.3
RSC
45.3 ± 3.6
51.4 ± 3.8
49.4 ± 4.5
43.6 ± 5.8
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
48.0
MTL
61.4 ± 2.2
57.2 ± 6.4
53.3 ± 5.1
48.3 ± 6.2
56.9 ± 4.8
49.2 ± 3.7
43.3 ± 5.0
45.8 ± 2.8
54.2 ± 5.7
42.2 ± 4.9
51.2
FISH
51.7 ± 3.7
53.1 ± 3.7
49.4 ± 4.5
43.6 ± 5.8
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
48.8
CORAL
65.3 ± 3.2
63.9 ± 4.4
60.0 ± 4.8
56.4 ± 6.0
60.2 ± 4.3
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
47.8 ± 5.7
53.9
ANDMASK
42.8 ± 3.4
50.6 ± 4.1
49.4 ± 4.5
43.6 ± 5.8
56.9 ± 4.4
47.8 ± 5.8
41.9 ± 7.5
41.7 ± 5.9
54.2 ± 3.0
49.7 ± 5.4
47.9
DIVA
81.3 ± 3.5
76.3 ± 4.2
74.7 ± 4.6
56.7 ± 5.1
67.0 ± 6.1
62.3 ± 5.1
62.0 ± 5.6
66.3 ± 4.1
70.3 ± 5.6
62.0 ± 4.2
67.9
LSSAE
95.8 ± 1.9
95.6 ± 2.1
93.5 ± 2.9
96.3 ± 1.8
83.8 ± 5.2
74.3 ± 3.6
51.9 ± 5.6
52.3 ± 8.1
46.5 ± 9.2
48.4 ± 5.3
73.8
GI
72.0 ± 4.9
62.0 ± 4.9
61.0 ± 4.9
58.0 ± 2.8
56.0 ± 4.9
49.0 ± 0.7
42.0 ± 1.4
45.0 ± 2.1
55.0 ± 3.5
44.0 ± 0.0
54.4
DDA
71.0 ± 0.7
56.0 ± 0.0
51.0 ± 4.9
44.0 ± 2.8
55.0 ± 0.7
49.0 ± 2.1
42.0 ± 7.1
45.0 ± 0.7
55.0 ± 3.5
44.0 ± 4.2
51.2
DRAIN
48.0 ± 2.1
52.0 ± 0.7
54.0 ± 3.5
47.0 ± 1.4
58.0 ± 3.5
52.0 ± 3.5
45.0 ± 4.2
48.0 ± 4.9
58.0 ± 4.9
47.0 ± 2.8
50.7
SDE-EDG (OURS)
95.0 ± 0.7
99.0 ± 1.4
98.0 ± 1.4
94.0± 1.4
95.0 ± 1.4
93.0 ± 7.8
74.0 ± 5.7
66.0 ± 0.7
45.0 ± 6.4
56.0 ± 4.5
81.5
Table 5: Sine. We show the results on each target domain by domain index.
ALGORITHM
17
18
19
20
21
22
23
24
AVG
ERM
71.4 ± 6.1
91.0 ± 1.5
81.6 ± 2.4
53.4 ± 2.9
51.1 ± 6.7
54.3 ± 4.7
49.5 ± 4.8
51.7 ± 5.0
63.0
MIXUP
63.1 ± 5.9
93.5 ± 1.7
80.6 ± 3.8
52.8 ± 2.9
60.3 ± 7.2
54.2 ± 2.7
49.5 ± 4.4
49.3 ± 8.0
62.9
MMD
57.0 ± 4.2
57.1 ± 4.1
47.6 ± 5.4
50.0 ± 1.8
55.1 ± 6.7
54.4 ± 4.7
49.5 ± 4.8
51.7 ± 5.0
55.8
MLDG
69.2 ± 4.2
67.7 ± 4.1
52.1 ± 5.4
50.7 ± 1.8
51.1 ± 6.7
54.3 ± 4.7
49.5 ± 4.8
51.7 ± 5.0
63.2
IRM
66.9 ± 6.2
81.1 ± 3.2
88.5 ± 3.0
56.6 ± 6.0
57.2 ± 5.8
53.7 ± 5.1
49.5 ± 2.2
51.7 ± 5.4
63.2
RSC
61.3 ± 6.6
83.5 ± 1.9
84.5 ± 2.6
52.8 ± 2.8
55.1 ± 6.7
54.4 ± 4.7
49.5 ± 4.8
51.7 ± 5.0
61.5
MTL
70.6 ± 6.6
91.6 ± 1.2
79.9 ± 3.4
51.0 ± 4.7
60.3 ± 7.6
53.6 ± 5.2
49.5 ± 5.3
46.9 ± 5.9
62.9
FISH
66.1 ± 6.9
82.0 ± 2.7
87.5 ± 2.4
55.2 ± 3.0
51.1 ± 6.7
54.3 ± 4.7
49.5 ± 4.8
51.7 ± 5.0
62.3
CORAL
60.0 ± 5.3
57.1 ± 4.2
48.6 ± 6.4
50.7 ± 1.8
49.7 ± 6.2
48.6 ± 4.6
46.3 ± 5.0
51.7 ± 5.0
51.6
ANDMASK
44.2 ± 5.1
42.9 ± 4.2
54.2 ± 7.0
71.9 ± 1.9
86.4 ± 3.2
90.4 ± 2.9
88.1 ± 3.4
76.4 ± 3.7
69.3
DIVA
79.0 ± 6.6
60.8 ± 1.9
47.6 ± 2.6
50.0 ± 2.8
55.1 ± 6.7
51.9 ± 4.7
38.6 ± 4.8
40.4 ± 5.0
52.9
LSSAE
93.0 ± 1.7
86.9 ± 0.7
69.2 ± 1.5
63.8 ± 3.8
68.8 ± 2.5
76.8 ± 4.8
63.9 ± 1.3
49.0 ± 3.1
71.4
GI
77.0 ± 0.7
84.2 ± 4.4
89.1 ± 0.6
60.9 ± 3.6
55.1 ± 0.8
53.5 ± 6.0
49.8 ± 4.1
52.0 ± 2.8
65.2
DDA
43.0 ± 0.7
47.2 ± 0.6
74.2 ± 1.6
89.6 ± 1.1
75.5 ± 1.1
70.0 ± 7.1
59.1 ± 2.2
74.0 ± 1.4
66.6
DRAIN
43.0 ± 4.9
44.6 ± 4.7
69.3 ± 0.9
89.6 ± 1.1
82.8 ± 4.8
78.5 ± 2.5
70.2 ± 4.1
92.0 ± 5.7
71.3
SDE-EDG (OURS)
99.0 ± 0.7
96.9 ± 0.6
90.0 ± 4.2
88.8 ± 0.6
64.9 ± 2.2
52.4 ± 0.3
43.0 ± 4.9
42.3 ± 4.0
72.2
Table 6: RMNIST. We show the results on each target domain by domain index.
ALGORITHM
130◦
140◦
150◦
160◦
170◦
180◦
AVG
ERM
56.8 ± 0.9
44.2 ± 0.8
37.8 ± 0.6
38.3 ± 0.8
40.9 ± 0.8
43.6 ± 0.8
43.6
MIXUP
61.3 ± 0.7
47.4 ± 0.8
39.1 ± 0.7
38.3 ± 0.7
40.5 ± 0.8
42.8 ± 0.9
44.9
MMD
59.2 ± 0.9
46.0 ± 0.8
39.0 ± 0.7
39.3 ± 0.8
41.6 ± 0.7
43.7 ± 0.8
44.8
MLDG
57.4 ± 0.7
44.5 ± 0.9
37.5 ± 0.8
37.5 ± 0.8
39.9 ± 0.8
42.0 ± 0.9
43.1
IRM
47.7 ± 0.9
38.5 ± 0.7
34.1 ± 0.7
35.7 ± 0.8
37.8 ± 0.8
40.3 ± 0.8
39.0
RSC
54.1 ± 0.9
41.9 ± 0.8
35.8 ± 0.7
37.0 ± 0.8
39.8 ± 0.8
41.6 ± 0.8
41.7
MTL
54.8 ± 0.9
43.1 ± 0.8
36.4 ± 0.8
36.1 ± 0.8
39.1 ± 0.9
40.9 ± 0.8
41.7
FISH
60.8 ± 0.8
47.8 ± 0.8
39.2 ± 0.8
37.6 ± 0.7
39.0 ± 0.8
40.7 ± 0.7
44.2
CORAL
58.8 ± 0.9
46.2 ± 0.8
38.9 ± 0.7
38.5 ± 0.8
41.3 ± 0.8
43.5 ± 0.8
44.5
ANDMASK
53.5 ± 0.9
42.9 ± 0.8
37.8 ± 0.7
38.6 ± 0.8
40.8 ± 0.8
43.2 ± 0.8
42.8
DIVA
58.3 ± 0.8
45.0 ± 0.8
37.6 ± 0.8
36.9 ± 0.7
38.1 ± 0.8
40.1 ± 0.8
42.7
LSSAE
64.1 ± 0.8
51.6 ± 0.8
43.4 ± 0.8
38.6 ± 0.7
40.3 ± 0.8
40.4 ± 0.8
46.4
GI
61.6 ± 0.9
46.4 ± 0.9
39.2 ± 0.8
40.0 ± 0.8
40.1 ± 0.8
40.1 ± 0.7
44.6
DDA
60.7 ± 0.8
50.0 ± 0.8
42.6 ± 0.8
39.6 ± 0.8
38.0 ± 0.8
39.7 ± 0.8
45.1
DRAIN
59.5 ± 0.8
45.4 ± 0.8
40.2 ± 0.7
37.2 ± 0.7
39.6 ± 0.8
41.0 ± 0.7
43.8
SDE-EDG (OURS)
75.1 ± 0.8
61.3 ± 0.9
49.8 ± 0.8
49.8 ± 0.8
39.7 ± 0.7
39.7 ± 0.9
52.6
16

Published as a conference paper at ICLR 2024
Table 7: Portraits. We show the results on each target domain by domain index.
ALGORITHM
25
26
27
28
29
30
31
32
33
34
AVG
ERM
75.5 ± 0.9
83.8 ± 0.9
88.5 ± 0.8
93.3 ± 0.7
93.4 ± 0.6
92.1 ± 0.7
90.6 ± 0.8
84.3 ± 0.9
88.5 ± 0.9
87.9 ± 1.4
87.8
MIXUP
75.5 ± 0.9
83.8 ± 0.9
88.5 ± 0.8
93.3 ± 0.7
93.4 ± 0.6
92.1 ± 0.7
90.6 ± 0.8
84.3 ± 0.9
88.5 ± 0.9
87.9 ± 1.4
87.8
MMD
74.0 ± 1.0
83.8 ± 0.8
87.2 ± 0.8
93.0 ± 0.7
93.0 ± 0.6
91.9 ± 0.7
90.9 ± 0.7
84.7 ± 1.4
88.3 ± 0.9
85.8 ± 1.8
87.3
MLDG
76.4 ± 0.8
85.5 ± 0.9
90.1 ± 0.7
94.3 ± 0.6
93.5 ± 0.6
92.0 ± 0.7
90.8 ± 0.8
85.6 ± 1.1
89.3 ± 0.8
87.6 ± 1.6
88.5
IRM
74.2 ± 0.9
83.5 ± 0.9
88.5 ± 0.8
91.0 ± 0.8
90.4 ± 0.7
87.3 ± 0.8
87.0 ± 0.9
80.4 ± 1.5
86.7 ± 0.9
85.1 ± 1.8
85.4
RSC
75.2 ± 0.9
84.7 ± 0.8
87.9 ± 0.7
93.3 ± 0.7
92.5 ± 0.7
91.0 ± 0.7
90.0 ± 0.7
84.6 ± 1.2
88.2 ± 0.8
85.8 ± 1.9
87.3
MTL
78.2 ± 0.9
86.5 ± 0.8
90.9 ± 0.8
94.2 ± 0.7
93.8 ± 0.6
92.0 ± 0.7
91.2 ± 0.7
86.0 ± 1.2
89.3 ± 0.8
87.4 ± 1.4
89.0
FISH
78.6 ± 0.9
86.9 ± 0.8
89.5 ± 0.8
93.5 ± 0.7
93.3 ± 0.6
92.1 ± 0.6
91.1 ± 0.7
86.2 ± 1.3
88.7 ± 0.9
87.7 ± 1.6
88.8
CORAL
74.6 ± 0.9
84.6 ± 0.8
87.9 ± 0.8
93.3 ± 0.6
92.7 ± 0.7
91.5 ± 0.7
90.7 ± 0.7
84.6 ± 1.5
88.1 ± 0.9
85.9 ± 1.9
87.4
ANDMASK
62.0 ± 1.1
70.8 ± 1.1
67.0 ± 1.2
70.2 ± 1.1
75.2 ± 1.1
74.1 ± 1.0
72.7 ± 1.1
64.7 ± 1.6
77.3 ± 1.1
74.9 ± 2.1
70.9
DIVA
76.2 ± 1.0
86.6 ± 0.8
88.8 ± 0.8
93.5 ± 0.7
93.1 ± 0.6
91.6 ± 0.6
91.1 ± 0.7
84.7 ± 1.3
89.1 ± 0.8
87.0 ± 1.5
88.2
LSSAE
77.7 ± 0.9
87.1 ± 0.8
90.8 ± 0.7
94.3 ± 0.6
94.3 ± 0.6
92.2 ± 0.6
91.2 ± 0.7
86.7 ± 1.1
89.6 ± 0.8
86.9 ± 1.4
89.1
GI
77.8 ± 1.2
86.6 ± 1.3
90.8 ± 1.1
95.3 ± 1.3
93.1 ± 1.2
89.3 ± 1.1
88.9 ± 1.2
84.1 ± 1.8
87.7 ± 1.0
87.5 ± 2.0
88.1
DDA
76.0 ± 1.0
85.6 ± 0.8
88.6 ± 0.8
93.6 ± 0.6
92.9 ± 0.7
92.9 ± 0.6
90.3 ± 0.8
84.3 ± 1.2
88.7 ± 0.8
85.9 ± 1.2
87.9
DRAIN
77.7 ± 0.8
86.2 ± 0.8
90.6 ± 0.6
94.8 ± 0.5
94.4 ± 0.6
92.8 ± 0.7
92.2 ± 0.6
87.2 ± 1.2
89.9 ± 0.8
87.9 ± 1.1
89.4
SDE-EDG (OURS)
78.6 ± 0.8
86.6 ± 0.9
90.1 ± 0.8
94.8 ± 0.6
94.5 ± 0.6
93.3 ± 0.7
92.1 ± 0.7
87.9 ± 1.3
89.6 ± 0.9
89.0 ± 1.1
89.6
Table 8: Caltran. We show the results on each target domain by domain index.
ALGORITHM
25
26
27
28
29
30
31
32
33
34
AVG
ERM
29.9 ± 3.5
88.4 ± 2.1
61.1 ± 3.5
56.3 ± 3.2
90.0 ± 1.6
60.1 ± 2.5
55.5 ± 3.5
88.8 ± 2.4
57.1 ± 3.5
50.5 ± 5.2
66.3
MIXUP
53.6 ± 3.9
89.0 ± 2.0
61.8 ± 2.4
55.7 ± 2.9
88.2 ± 2.1
58.6 ± 3.0
52.3 ± 3.7
88.6 ± 2.7
57.1 ± 3.0
55.1 ± 4.3
66.0
MMD
30.2 ± 2.1
92.7 ± 1.7
56.4 ± 3.7
39.1 ± 3.2
93.6 ± 1.7
52.1 ± 3.2
42.8 ± 3.0
92.1 ± 2.2
42.1 ± 3.8
29.4 ± 3.8
57.1
MLDG
54.8 ± 4.1
88.6 ± 2.6
62.2 ± 3.6
55.1 ± 4.1
88.3 ± 1.7
60.9 ± 4.3
51.7 ± 2.6
89.0 ± 1.9
56.5 ± 3.4
55.3 ± 4.8
66.2
IRM
46.4 ± 3.7
90.8 ± 1.7
60.8 ± 3.4
52.9 ± 3.1
91.8 ± 1.7
56.6 ± 3.1
52.1 ± 2.9
90.9 ± 2.6
55.6 ± 3.9
43.1 ± 5.5
64.1
RSC
57.2 ± 3.0
88.4 ± 2.6
62.6 ± 3.0
56.5 ± 3.7
88.0 ± 2.4
59.4 ± 3.0
51.9 ± 2.9
90.0 ± 2.0
59.4 ± 2.9
56.0 ± 3.1
67.0
MTL
64.2 ± 3.0
87.2 ± 2.5
64.9 ± 3.9
60.0 ± 4.8
84.5 ± 2.2
60.6 ± 3.5
52.6 ± 3.7
83.9 ± 2.9
58.2 ± 4.1
65.7 ± 5.6
68.2
FISH
61.1 ± 3.5
88.2 ± 1.5
64.7 ± 4.0
57.9 ± 3.1
88.3 ± 2.2
59.9 ± 3.0
57.5 ± 2.7
87.4 ± 2.8
57.7 ± 3.7
63.0 ± 6.1
68.6
CORAL
50.4 ± 3.0
90.8 ± 2.0
61.2 ± 3.8
55.0 ± 2.5
92.0 ± 1.7
56.8 ± 3.8
52.0 ± 3.8
90.9 ± 1.6
56.8 ± 2.4
50.9 ± 5.6
65.7
ANDMASK
30.0 ± 2.2
92.7 ± 1.7
56.2 ± 3.8
39.1 ± 3.2
93.6 ± 1.7
51.6 ± 3.2
42.6 ± 2.9
92.1 ± 2.2
41.2 ± 3.7
29.9 ± 3.6
56.9
DIVA
60.6 ± 2.9
90.1 ± 1.7
67.5 ± 3.1
58.9 ± 3.5
88.4 ± 2.8
58.7 ± 3.3
53.8 ± 3.6
89.8 ± 1.7
61.8 ± 4.8
62.0 ± 3.4
69.2
LSSAE
63.4 ± 3.4
92.1 ± 2.0
62.6 ± 4.7
58.8 ± 4.4
92.9 ± 1.6
62.0 ± 3.9
54.3 ± 3.0
92.1 ± 2.2
60.5 ± 3.8
67.4 ± 3.6
70.6
GI
68.8 ± 3.1
86.6 ± 1.9
65.5 ± 3.2
60.6 ± 4.3
88.8 ± 2.4
58.5 ± 3.6
53.1 ± 2.8
88.7 ± 2.1
63.7 ± 2.9
73.0 ± 5.1
70.7
DDA
31.0 ± 3.4
92.6 ± 1.8
56.8 ± 0.9
59.0 ± 2.8
94.0 ± 2.2
61.7 ± 2.3
52.9 ± 2.3
92.9 ± 2.2
57.8 ± 5.3
62.9 ± 5.8
66.1
DRAIN
66.4 ± 3.3
83.8 ± 1.0
65.7 ± 2.2
62.8 ± 3.2
77.9 ± 3.3
62.3 ± 3.7
55.7 ± 3.7
78.9 ± 3.1
60.6 ± 3.8
75.7 ± 5.6
69.0
SDE-EDG (OURS)
70.5 ± 2.9
88.8 ± 4.9
66.1 ± 2.8
55.1 ± 2.6
85.1 ± 3.6
59.5 ± 4.4
58.6 ± 3.3
88.2 ± 3.4
68.9 ± 3.5
72.2 ± 5.7
71.3
Table 9: PowerSupply. We show the results on each target domain by domain index.
ALGORITHM
21
22
23
24
25
26
27
28
29
30
AVG
ERM
69.8 ± 1.4
70.0 ± 1.4
69.2 ± 1.3
64.4 ± 1.5
85.8 ± 1.0
76.0 ± 1.3
70.1 ± 1.5
69.8 ± 1.5
69.0 ± 1.3
65.5 ± 1.5
71.0
MIXUP
69.6 ± 1.4
69.5 ± 1.5
68.3 ± 1.5
64.3 ± 1.5
87.1 ± 1.0
76.6 ± 1.3
70.1 ± 1.4
69.2 ± 1.3
68.1 ± 1.5
65.0 ± 1.6
70.8
MMD
70.0 ± 1.3
69.7 ± 1.4
68.7 ± 1.4
64.8 ± 1.5
85.6 ± 1.0
76.1 ± 1.3
70.0 ± 1.5
69.5 ± 1.4
68.7 ± 1.3
65.6 ± 1.5
70.9
MLDG
69.7 ± 1.4
69.7 ± 1.5
68.6 ± 1.5
64.6 ± 1.5
86.4 ± 1.1
76.3 ± 1.4
70.1 ± 1.4
69.4 ± 1.3
68.4 ± 1.5
65.6 ± 1.5
70.8
IRM
69.8 ± 1.4
69.5 ± 1.4
68.3 ± 1.4
64.1 ± 1.4
87.2 ± 0.9
76.5 ± 1.3
70.0 ± 1.5
69.1 ± 1.5
68.2 ± 1.3
65.0 ± 1.4
70.8
RSC
69.9 ± 1.4
69.6 ± 1.4
68.6 ± 1.4
64.4 ± 1.5
86.6 ± 1.0
76.3 ± 1.3
70.0 ± 1.5
69.4 ± 1.4
68.4 ± 1.3
65.4 ± 1.5
70.9
MTL
69.6 ± 1.4
69.4 ± 1.5
68.2 ± 1.6
64.2 ± 1.5
87.4 ± 1.2
76.6 ± 1.3
69.9 ± 1.5
69.1 ± 1.5
68.2 ± 1.5
64.6 ± 1.4
70.7
FISH
69.7 ± 1.4
69.4 ± 1.4
68.2 ± 1.4
64.2 ± 1.4
87.3 ± 1.0
76.6 ± 1.3
69.9 ± 1.5
69.2 ± 1.5
68.2 ± 1.3
65.2 ± 1.5
70.8
CORAL
69.9 ± 1.4
69.7 ± 1.4
68.9 ± 1.4
64.6 ± 1.4
86.1 ± 1.0
76.3 ± 1.3
70.0 ± 1.5
69.5 ± 1.5
68.8 ± 1.3
65.7 ± 1.5
71.0
ANDMASK
69.9 ± 1.4
69.4 ± 1.4
68.2 ± 1.3
64.0 ± 1.4
87.4 ± 0.9
76.7 ± 1.3
70.0 ± 1.5
69.1 ± 1.5
68.0 ± 1.3
64.7 ± 1.5
70.7
DIVA
69.7 ± 1.4
69.5 ± 1.3
68.2 ± 1.4
63.9 ± 1.5
87.5 ± 1.0
76.5 ± 1.3
69.9 ± 1.5
69.1 ± 1.5
68.1 ± 1.3
64.7 ± 1.5
70.7
LSSAE
70.0 ± 1.4
69.8 ± 1.4
69.0 ± 1.5
65.4 ± 1.4
85.1 ± 1.1
76.0 ± 1.4
70.1 ± 1.7
69.9 ± 1.3
69.0 ± 1.6
66.3 ± 1.4
71.1
GI
70.2 ± 1.4
71.0 ± 1.4
70.5 ± 1.5
69.6 ± 1.5
80.7 ± 1.1
68.4 ± 1.3
72.9 ± 1.5
72.0 ± 1.3
71.8 ± 1.3
66.5 ± 1.5
71.4
DDA
69.8 ± 1.6
72.4 ± 1.5
70.5 ± 1.5
63.8 ± 1.5
83.7 ± 1.2
73.1 ± 1.2
70.1 ± 1.3
71.4 ± 1.5
70.5 ± 1.7
63.4 ± 1.2
70.9
DRAIN
70.1 ± 1.3
70.0 ± 1.0
69.3 ± 1.1
65.5 ± 1.5
83.6 ± 1.0
75.8 ± 1.7
70.3 ± 1.3
69.8 ± 1.5
68.9 ± 1.9
66.4 ± 1.2
71.0
SDE-EDG (OURS)
67.7 ± 1.2
74.2 ± 1.5
79.3 ± 1.0
75.2 ± 1.3
87.9 ± 1.0
78.0 ± 1.2
67.2 ± 1.6
72.0 ± 1.5
79.8 ± 1.2
75.3 ± 1.0
75.7
C
EXPERIMENTAL SETUP
C.1
DATASETS
Dataset: Rotated Gaussian (Zeng et al., 2023) consists of 30 domains generated by the same
Gaussian distribution, but the decision boundary rotates from 0◦to 338◦with an interval of 12◦. We
split the domains into source domains (1-22 domains), intermediate domains (22-25 domains), and
target domains (26-30 domains). The intermediate domains are utilized as the validation set.
Dataset: Circle (Pesaranghader & Viktor, 2016) contains evolving 30 domains where the instance
are sampled from 30 2D Gaussian distributions. The label is assigned using a half-circle curve as the
decision boundary. (15 source domains, 5 validation domains, and 10 target domains)
17

Published as a conference paper at ICLR 2024
Table 10: OcularDisease. We show the results on each target domain by domain index.
ALGORITHM
30
31
32
33
AVG
ERM
53.4 ± 5.8
58.1 ± 4.8
60.8 ± 5.6
59.2 ± 7.4
57.9
MIXUP
59.1 ± 4.8
67.8 ± 2.9
46.7 ± 4.6
65.4 ± 5.8
59.7
MMD
58.0 ± 5.2
56.1 ± 7.0
66.7 ± 7.2
49.6 ± 6.5
57.6
MLDG
46.6 ± 6.0
57.8 ± 6.6
30.0 ± 5.7
41.2 ± 7.4
43.9
IRM
39.8 ± 7.2
41.4 ± 4.3
53.3 ± 6.8
50.4 ± 4.0
46.2
RSC
43.2 ± 6.7
59.4 ± 4.7
49.2 ± 6.5
66.2 ± 7.7
54.5
MTL
58.0 ± 6.5
61.9 ± 6.3
56.7 ± 14.1
62.1 ± 9.3
59.7
FISH
50.0 ± 6.2
43.6 ± 6.6
39.2 ± 4.3
60.0 ± 8.0
48.2
CORAL
52.3 ± 7.0
68.6 ± 4.6
54.2 ± 5.1
65.4 ± 6.5
60.1
ANDMASK
42.0 ± 7.1
65.6 ± 6.9
50.0 ± 7.1
47.1 ± 6.7
51.2
DIVA
51.1 ± 7.1
53.1 ± 4.2
51.7 ± 12.2
56.7 ± 4.6
53.1
LSSAE
55.0 ± 4.8
45.3 ± 3.8
57.5 ± 8.2
51.2 ± 5.9
52.3
DDA
53.4 ± 7.9
63.6 ± 3.1
44.2 ± 6.0
62.1 ± 5.1
55.8
DRAIN
58.0 ± 6.9
62.8 ± 6.4
53.3 ± 6.8
60.8 ± 9.8
58.7
SDE-EDG (OURS)
54.5 ± 6.7
66.4 ± 5.9
64.2 ± 8.8
65.4 ± 8.7
62.6
Table 11: Neural network architectures for different datasets. (MLP is short for Multiple-layer Perceptrons)
DATASET
FEATURE EXTRACTOR
CLASSIFIER
f & g
RGAUSSIAN
[2, 32, 32]-MLP
A LINEAR LAYER
[32, 32]-MLP
CIRCLE
[2, 32, 32]-MLP
A LINEAR LAYER
[32, 32]-MLP
SINE
[2, 32, 32]-MLP
A LINEAR LAYER
[32, 32]-MLP
ROTATING MNIST
MNIST CONVNET
A LINEAR LAYER
[128, 512, 512]-MLP
PORTRAIT
RESNET-18
A LINEAR LAYER
[512, 512]-MLP
CALTRAN
RESNET-18
A LINEAR LAYER
[512, 512]-MLP
POWERSUPPLY
[2, 256, 256]-MLP
A LINEAR LAYER
[256, 256]-MLP
OCULARDISEASE
RESNET-18
A LINEAR LAYER
[512, 512]-MLP
Dataset: Sine In Sine (Pesaranghader & Viktor, 2016) each data owns two attributes (x1, x2). The
label is assigned using a sine curve as the decision boundary. We rearrange this dataset by extending
it to 24 evolving domains. Each domain covers
1
24 the period of the sinusoid. (12 source domains, 4
validation domains, and 8 target domains)
Dataset: Rotated MNIST (RMNIST) (Ghifary et al., 2015) is an adaptation of the popular MNIST
digit dataset (Deng, 2012), composed of MNIST digits of various rotations. The task is to classify
a digit from 0 to 9 given an image of the digit. We follow (Qin et al., 2022) and extend it to 19
evolving domains via applying the rotations with degree of {0◦, 15◦, 30◦, . . . , 180◦} in order. (10
source domains, 3 validation domains, and 6 target domains).
Dataset: Portraits (Ginosar et al., 2015) is a real-world dataset that comprises photographs of
American high school seniors collected over a period of 108 years (1905-2013) across 26 states. The
objective is to accurately classify the gender for each photograph. The dataset is divided into 34
domains based on a fixed interval over time. (19 source domains, 5 validation domains, and 10 target
domains)
Dataset: Caltran (Hoffman et al., 2014) consists of real-world images captured by a fixed traffic
camera deployed in an intersection over time. Frames were updated at 3-minute intervals each with a
resolution 320×320. We divide it into 34 domains by time. The task of Caltran is to classify scenes to
identify the presence of one or more vehicles in or approaching the intersection. The challenge mainly
raise from the continually evolving domain shift as changes include time, illumination, weather, etc.
(19 source domains, 5 validation domains, and 10 target domains)
Dataset: PowerSupply (Dau et al., 2019) is a dataset designed for the task of time-section prediction
of current power supply based on hourly records obtained from an Italian electricity company. The
dataset consists of 30 domains formed according to days. Each data point is assigned a binary class
label indicating whether the current power supply belongs to the morning or the afternoon. Domain
18

Published as a conference paper at ICLR 2024
Table 12: Hyper-parameters and selected values
DATASET
PARAMETERS
VALUE
RGAUSSIAN
α
10
LEARNING RATE
1e-3
BATCH SIZE
64
CIRCLE
α
10
LEARNING RATE
1e-3
BATCH SIZE
64
SINE
α
10
LEARNING RATE
1e-3
BATCH SIZE
64
RMNIST
α
1
LEARNING RATE
1e-3
BATCH SIZE
48
PORTRAITS
α
1
LEARNING RATE
1e-4
BATCH SIZE
24
CALTRAN
α
1
LEARNING RATE
5e-5
BATCH SIZE
24
POWERSUPPLY
α
10
LEARNING RATE
1e-3
BATCH SIZE
64
OCULARDISEASE
α
0.5
LEARNING RATE
5e-5
BATCH SIZE
24
shifts may arise due to variations in season, weather, price, or the differences between working days
and weekends. (15 source domains, 5 validation domains, and 10 target domains)
Dataset: OcularDisease (Kaggle, 2020) (from the Kaggle Competition (Kaggle 2020)) Ocular
Disease Intelligent Recognition (ODIR) is a structured ophthalmic database of 5,000 patients with
age, color fundus photographs from left and right eyes and doctors’ diagnostic keywords from doctors.
We set three classes: Normal, Diabetes and other diseases. To generate non-stationary environments,
we sort the photographs in ascending order of the age of the patients. (27 source domains, 2 validation
domains, and 4 target domains)
C.2
EXPERIMENT SETTING AND IMPLEMENTATION DETAILS
Neural network architectures used for different datasets in Table 11. ResNet18, and MNIST ConvNet
are from domainbed codes (Gulrajani & Lopez-Paz, 2020). Specifically, Table 11 demonstrates the
classifier architecture for baselines equipped with learned classifier. SDE-EDG does not equip with a
classifier network, since the prediction is based on the closest distance to the Evolving Centroids.
We list the values of the hyper-parameters of SDE-EDG for different datasets in Table 12. We use
torchsde (Kidger et al., 2021) package for SDE implementations and the step size is 0.05.
For SDE-EDG, to address the issue of lacking access to data at t0, we adopt a sampling strategy that
zk
0 is sampled from N(µk
0, σ0I) using the Reparameterization trick, where µk
0 is a learned parameter
and σ0 is a predefined constant.
D
THEORETICAL ANALYSIS
In this section, we show Jmle can be decomposed into sums of Path Alignment Loss for stochas-
tic path fittings, and Stochastic Uncertainty Loss for minimizing diffusion terms at observations.
Furthermore, we prove that SDE-EDG learning methodology leads to a lower generalization bound.
Lemma D.1. The optimization loss Jmle defined in Eq. (7), is of the upper bound of the Path
Alignment loss Jpa and the Stochastic Uncertainty loss Jsu, which implies (the proof in Appendix E.5)
19

Published as a conference paper at ICLR 2024
Jmle ≥
M
X
m=1
K
X
k=1
NB
X
i=1
1
2MKNB
(zm+1 −ˆzm+1)2
g(zm)2∆tm
|
{z
}
Jpa
+
M
X
m=1
K
X
k=1
NB
X
i=1
1
2MKNB log(2πg(zm)2∆tm)
|
{z
}
Jsu
where we hide the subscript, sample index i, for zm|i as zm for simplicity.
Lemma D.1 shows (1) The term Jpa is included in Eq. (13). Thus, minimizing Jpa can lead to a
smaller KL(ν||ˆν) and eventually help to minimize the expected generalization error in Eq. (12). (2)
The Stochastic Uncertainty loss Jsu serves as a loss function to optimize the diffusion terms g. As
domains at timestamps {t1, . . . , tm, . . . , tM} are observed, the stochastic uncertainty depicted by
diffusion term at these timestamps should be small (Kong et al., 2020).
Remark D.2. (adapted from (Shui et al., 2022)) Let ν be the stochastic path of unseen target evolving
domains sampled sequentially from an evolving environment E between timestamps from T to
T + T ∗, and ˆν be the learned stochastic path by our SDE-EDG. Then, we have
Rν(h) ≤Rˆν(h) + G
√
2
p
KL(ν||ˆν)
(12)
The proof is in E.2. To achieve a low risk on ν, Remark D.2 suggests learning SDE-EDG to minimize
the KL divergence between ˆν and ν, while Rˆν(h) can be approximated by the empirical risk.
In this analysis, we assume that D(t) follows U(0, T +T ∗) and D(y) is invariant through time In this
theorem, we analyze the simple case for constant time intervals ∆t between consecutive domains.
Theorem D.3. For any ϵ > 0, with probability at least 1 −ϵ, the KL-divergence on path space of ν
and ˆν can be upper bounded by:
KL(ν||ˆν) ≤L
2

Jpa + 2µ ˆRH + 3M
r
∆t log 2/ϵ
2TN

(13)
where ˆRH is the Rademacher complexity of the hypothesis set H. M > 0 is the upper bound of ℓ=
(zm+1−ˆzm+1)2
g(zm)2∆tm
and ℓis µ-Lipschitz for some µ > 0 for any fixed zm and zm+1 (ˆzm+1 is mapped from zm).
The proof is in Appendix E.6. Theorem D.3 suggests: (1) L corresponds to the extrapolation steps
on future time. The bound is minimized when L →0. LHS gets larger with a larger L, which
indicates the EDG generalizes worse with more future steps. (2) The third term on the RHS gets
smaller with a smaller ∆t. It implies we should collect sufficient and diverse source domains to
ensure that temporal dynamics can be learned in the learning process. It also indicates that generating
continuous-interpolated samples will help the learning process. If we collect only one domain even
with an infinite number of samples, obviously EDG can not be achieved; Equally, we can collect a
longer time horizon in the source domains, which means a larger T will contribute to better learning
evolving patterns and thus a smaller generalization error. A bigger sample size N in each source
domain also contributes to a smaller generalization error.
E
PROOF OF THEORIES
We first prove an intermediate lemma:
Lemma E.1. (Shui et al., 2022) Let v ∈V = X × Y be the real-valued integrable random variable,
let P and Q be two distributions on a common space V such that Q is absolutely continuous w.r.t. P.
If for any function f and λ ∈R such that EP [eλ(f(v)−EP (f(v))] < ∞, then we have:
λ(EQf(v) −EP f(v)) ≤DKL(Q∥P) + log EP [eλ(f(v)−EP (f(v))],
where DKL(Q∥P) is the Kullback–Leibler divergence between distribution Q and P, and the equality
arrives when f(v) = EP f(v) + 1
λ log( dQ
dP ).
Proof. We let g be any function such that EP [eg(v)] < ∞, then we define a random variable
Vg(v) =
eg(v)
EP [eg(v)], then we can verify that EP (Vg) = 1. We assume another distribution Q such that
20

Published as a conference paper at ICLR 2024
Q (with distribution density q(v)) is absolutely continuous w.r.t. P (with distribution density p(v)),
then we have:
EQ[log Vg] = EQ[log q(v)
p(v) + log(Vg
p(v)
q(v))] = DKL(Q∥P) + EQ[log(Vg
p(v)
q(v))]
≤DKL(Q∥P) + log EQ[p(v)
q(v)Vg] = DKL(Q∥P) + log EP [Vg]
Since EP [Vg]
=
1 and according to the definition we have EQ[log Vg]
=
EQ[g(v)] −
EQ log EP [eg(v)] = EQ[g(v)] −log EP [eg(v)] (since EP [eg(v)] is a constant w.r.t. Q) and we
therefore have:
EQ[g(v)] ≤log EP [eg(v)] + DKL(Q∥P)
(14)
Since this inequality holds for any function g with finite moment generation function, then we let
g(v) = λ(f(v) −EP f(v)) such that EP [ef(v)−EP f(v)] < ∞. Therefore we have ∀λ and f we have:
EQλ(f(v) −EP f(v)) ≤DKL(Q∥P) + log EP [eλ(f(v)−EP f(v)]
Since we have EQλ(f(v) −EP f(v)) = λEQ(f(v) −EP f(v))) = λ(EQf(v) −EP f(v)), therefore
we have:
λ(EQf(v) −EP f(v)) ≤DKL(Q∥P) + log EP [eλ(EQf(v)−EP f(v))]
As for the attainment in the equality of Eq. (14), we can simply set g(v) = log( q(v)
p(v)), then we can
compute EP [eg(v)] = 1 and the equality arrives. Therefore in Lemma 1, the equality reaches when
λ(f(v) −EP f(v)) = log( dQ
dP ).
In the classification problem, we define the observation pair v = (x, y). We also define the loss
function ℓ(v) = L ◦h(v) with deterministic hypothesis h and prediction loss function L. Then for
abuse of notation, we simply denote the loss function ℓ(v) in this part.
Then we introduce the following bound between SDE-EDG’s stochastic path ˆν and real evolving path
ν.
Remark E.2. (Restatement of Remark D.2) Let ˆν be the learned stochastic path by SDE-EDG, and
suppose the loss function ℓis bounded within an interval G : G = max(ℓ) −min(ℓ). Then, for any
h ∈H, its target risk Rν(h) can be upper bounded by:
Rν(h) ≤Rˆν(h) + G
√
2
p
KL(ν||ˆν).
where we use KL(·||·) to denote the KL divergence for simplification in the remaining paragraphs.
Proof. According to Lemma E.1, ∀λ > 0 we have:
EQf(v) −EP f(v) ≤1
λ(log EP e[λ(f(v)−EP f(v))] + DKL(Q∥P))
(15)
And ∀λ < 0 we have:
EQf(v) −EP f(v) ≥1
λ(log EP e[λ(f(v)−EP f(v))] + DKL(Q∥P))
(16)
Let f = ℓ. Since the random variable ℓis bounded through G = max(ℓ) −min(ℓ), then according
to (Wainwright, 2019) (Chapter 2.1.2), ℓ−EP ℓis sub-Gaussian with parameter at most σ = G
2 , then
we can apply Sub-Gaussian property to bound the log moment generation function:
log EP e[λ(ℓ(v)−EP ℓ(v))] ≤log e
λ2σ2
2
≤λ2G2
8
.
In Eq. (15), we let Q = D′ and P = D, then ∀λ > 0 we have:
ED′ ℓ(v) −ED ℓ(v) ≤1
λDKL(D′∥D) + G2λ
8
(17)
21

Published as a conference paper at ICLR 2024
Since the inequality holds for ∀λ, then by taking λ = 2
√
2
G
p
DKL(D′∥D) we finally have:
ED′ ℓ(v) ≤ED ℓ(v) + G
√
2
p
DKL(D′∥D)
(18)
Let D′ = ν and D = ˆν, we complete our proof.
Lemma E.3. Define Rpa = E ||zt+∆t−ˆzt+∆t||2
2gk(zt)2∆t
as the expected of the model parameterized by θ with
the sample{z, y, t}, which is upper bounded by M > 0 and is µ-Lipschitz for some µ > 0 for any
fixed zk
t and ˆzk
t . Then, for any ϵ > 0, with probability at least 1 −ϵ, the following inequalities holds
Rpa ≤Jpa + 2µ ˆRH + 3M
r
∆t log 2/ϵ
2TN
(19)
where ˆRH is the Rademacher complexity of the hypothesis set H.
Lemma E.3 is directly adapted from Theorem 11.3 in (Mohri et al., 2018).
Assumption E.4. (Realizable) Assume there exists a hypothesis h∗∈H that satisfies
zk
m+1 = zk
m +
Z tm+1
tm
ζk(zk
s , s)ds +
Z tm+1
tm
gk(zk
s , s)dBs,
(20)
where ζk is the drift function of h∗and gk is the diffusion function of h∗. Given the same diffusion
function shared in Eq. (6) and Eq. (20), the KL between them on path space can be approximated (Xu
et al., 2022; Li et al., 2020).
Lemma E.5. (Restatement of Lemma D.1) The optimization loss Jmle defined in Eq. (7), is of the
upper bound of the Path Alignment loss Jpa and the Stochasti Uncertainty loss Jsu, which implies
Jmle ≥
M
X
m=1
K
X
k=1
NB
X
i=1
1
2MKNB
(zk
m+1 −ˆzk
m+1)2
gk(zm)2∆tm
|
{z
}
Jpa
+
M
X
m=1
K
X
k=1
NB
X
i=1
1
2MKNB log(2πgk(zk
m)2∆tm)
|
{z
}
Jsc
Proof. We did not write the superscript k in the proof for simplicity. Let zm, ˆzm+1 generated by the
Euler discretization:
ˆzm+1 = zm + f(zm)∆tm + g(zm)(Bm+1 −Bm) = zm + f(zm)∆tm + g(zm)∆t1/2
m ϵm+1
(21)
where {Bm}tm≥0 is the Brownian motion, and ϵm is sampled from N(0, I). This implies that conditional on
the previous state, the current state is normally distributed: ˆD(zm+1|zm) = N(zm +f(zm)∆tm, g(zm)2∆tm).
Thus, the log-densities can be evaluated as
MKNB · Jmle =
M
X
m=1
K
X
k=1
NB
X
i=1
−log ˆD
 z = ˜zk
m+1|i
z = zk
m|i

=
M
X
m=1
K
X
k=1
NB
X
i=1
−E log ˆD(zm+1|zm)
=
M
X
m=1
K
X
k=1
NB
X
i=1
1
2 log(2πg(zm)2∆tm) + 1
2
(zm+1 −(zm + f(zm)∆tm))2
g(zm)2∆tm

=
M
X
m=1
K
X
k=1
NB
X
i=1
1
2 log(2πg(zm)2∆tm) + 1
2
(zm+1 −ˆzm+1 + g(zm)∆t1/2
m ϵm+1)2
g(zm)2∆tm

=
M
X
m=1
K
X
k=1
NB
X
i=1
1
2 log(2πg(zm)2∆tm) + 1
2
(zm+1 −ˆzm+1)2
g(zm)2∆tm
+ 1
2ϵ2
m+1
+ (zm+1 −ˆzm+1)ϵm+1
g(zm)∆t1/2
m

22

Published as a conference paper at ICLR 2024
where the second equality is because ˜zk
m+1|i is the feature of a real sample and we hide the subscript index i for
simplicity.
M
X
m=1
K
X
k=1
NB
X
i=1
1
2ϵ2
m+1 ≈1
2Eϵ2
m+1 = 1/2
where we applies that ϵm follows normal distribution.
M
X
m=1
K
X
k=1
NB
X
i=1
(zm+1 −ˆzm+1)ϵm+1
g(zm)∆t1/2
m
≈E(f(zm) −ζ(zm))∆t1/2
m ϵm+1
g(zm)
= 0
(22)
where we apply the above equation is martingale and its expectation is zero (Xu et al., 2022; Li et al., 2020).
Then, we conclude our proof.
Theorem E.6. The KL-divergence on path space of ν and ˆν is upper bounded by:
KL(ν||ˆν) ≤L
2

Jpa + 2µ ˆRH + 3M
r
∆t log 2/ϵ
2TN

(23)
Proof.
KL(ν||ˆν) = Eν

log
L
Y
l=1
D(zl+1, yl+1|zl) −log
L
Y
l=1
ˆD(zl+1, yl+1|zl)

= Eν

log
L
Y
l=1
D(zl+1|zl, yl+1)
ˆD(zl+1|zl, yl+1)
+ log
L
Y
l=1
D(yl+1)
ˆD(yl+1)

= Eν log
L
Y
l=1
D(zl+1|zl, yl+1)
ˆD(zl+1|zl, yl+1)
where in the first line, we replace the KL-divergence corresponding to the diagram of data generation process
and D(t) following U(0, T + T ∗) cancels out the KL-divergence of t between D and ˆD; in the second equation,
we use the independence of yM+l+1 to zM+l by d-separation (Bishop & Nasrabadi, 2006) from figure 1.
Let zM+1, zM+2, . . . , zM+L generated by the Euler discretization:
zM+l+1 = zM+l + ζ(zM+l)∆tl + g(zM+l)(BM+l+1 −BM+l)
= zM+l + ζ(zM+l)∆tl + g(zM+l)∆t1/2
l
ϵM+l+1
(24)
where {Bt}t≥0 is the Brownian motion, and ϵt is sampled from N(0, I). This implies that conditional on the pre-
vious state, the current state is normally distributed: zM+l+1|zM+l ∼N(zM+l + ζ(zT +l)∆tl, g(zM+l)2∆tl).
Thus, the log-densities can be evaluated as
log D(zM+l+1|zM+l) = −1
2 log(2πg(zM+l)2∆tl) −1
2
(zM+l −(zM+l + ζ(zM+l)∆tl))2
g(zM+l)2∆tl
(25)
where i = 0, 1, · · · , L. On the other hand, if at any time step, the state in the next domain was generated by
observing Dt, we would have the following log-densities:
log ˆD(zM+l+1|zM+l) = −1
2 log(2πg(zM+l)2∆tl) −1
2
(zM+l+1 −(zM+l + f(zM+l)∆tl))2
g(zM+l)2∆tl
(26)
Now, we replace zM+l+1 in Eq. (24) into Eq. (25) and Eq. (26),
log D(zM+l+1|zM+l) = −1
2 log(2πg(zM+l)2∆tl) −1
2ϵ2
M+l+1
log ˆD(zM+l+1|zM+l) = −1
2 log(2πg(zM+l)2∆tl) −1
2
(ζ(zM+l) −f(zM+l))2
g(zM+l)2
∆tl
+ 2(ζ(zM+l) −f(zM+l))ϵM+l+1
g(zM+l)
∆t1/2
l
+ ϵ2
M+l+1

23

Published as a conference paper at ICLR 2024
The KL divergence on the path space could then be regarded as a sum of infinitely many KL-divergences between
Gaussians (applying ∆tl = ∆t):
KL(ν||ˆν)
=Eν lim
∆t→0
T ∗/∆t
X
l=0
EzM+l

KL(D(zM+l+1|zM+l, y = k)|| ˆD(zM+l+1|zM+l, y = k))

=Eν lim
∆t→0
T ∗/∆t
X
l=0
Ezk
M+lEzk
M+l+1 log D(zM+l+1|zM+l, y = k)
ˆD(zM+l+1|zM+l, y = k)
=Eν lim
∆t→0
T ∗/∆t
X
l=0
Ezk
M+lEϵM+l+1
(ζk(zk
M+l) −fk(zk
M+l))2
2gk(zM+l)2
∆tl + (ζk(zk
M+l) −fk(zk
M+l))
gk(zM+l)
∆t1/2
l
ϵM+l+1
=Eν lim
∆t→0
T ∗/∆t
X
l=0
∆t
2 Ezk
t (uk
t )2 +
Z T +T ∗
T
uk
t dBt
=Eν lim
∆t→0
T ∗/∆t
X
l=0
∆t
2 Ezk
t (uk
t )2
where uk
t = (ζk(zk
t , t) −fk(zk
t , t))/gk(zt), and t ∈{t1, . . . , tM+L}. In the last equality, we used the fact that
the Itˆo integral
R T +T ∗
T
uk
t dBt is a martingale.
Eν lim
∆t→0
T ∗/∆t
X
l=0
∆t
2 Ezk
t (ζk(zk
t ) −fk(zk
t )
gk(zt)
)2 =T ∗
2 ED(ζk(zk
t ) −fk(zk
t )
gk(zt)
)2
(27)
With fixed interval tm+1 −tm = tM+l+1 −tM+l = ∆t, we have
zk
t+∆t = zk
t + ζk(zk
t , t)∆t + gk(zk
t , tt)(Bt+∆t −Bt)
ˆzk
t+∆t = zk
t + fk(zk
t , t)∆t + gk(zk
t , t)(Bt+∆t −Bt)
Hence we have
KL(ν||ˆν)
≤T ∗E
1
2gk(zt)2∆t2
[zk
t + ζk(zk
t )∆t + gk(zk
t , t)(Bt+∆t −Bt)] −[zk
t + fk(zt)∆t + gk(zk
t , t)(Bt+∆t −Bt)]
2
=L · E||zt+∆t −ˆzt+∆t||2
2gk(zt)2∆t
Hence, we conclude the proof.
F
ADDITIONAL EXPERIMENT RESULTS
Evolving Domain Generalization with Continuous Index
We evaluate the performance of our
method on the continuous index cases by modifying the rotation degrees of the target domains in the
RMNIST dataset as {129.1◦, 137.6◦, 144.3◦, 149.2◦, 155.5◦}. We conduct experiments and compare
our method with GI, which can also work with continuous index. Compared to GI incremental
improvements (0.7%) on RMNIST, SDE-EDG could achieve a significant improvement with 12.9%
higher accuracy on ERM. This shows the superiority of our proposed approach not only in handling
discrete index EDG tasks, but also the continuous cases.
Rotated Gaussian Results The prediction results of the last 4 target domains (27-30) are visualized
in Figure 5. Rotated Gaussian has a fixed margin p(x) but an evolving p(y|x). ERM has been biased
by the source domains, and it fails for all target domains by making the opposite labelings. Instead,
SDE-EDG could give the correct predictions capturing the temporal patterns (rotating decision
boundary by 12◦with time).
24

Published as a conference paper at ICLR 2024
Table 13: Rotated MNIST (RMNIST) with the Random Continuous Index
ALGORITHM
129.1◦
137.6◦
144.3◦
149.2◦
155.5◦
AVG
ERM
57.8 ± 0.9
45.0 ± 0.9
40.6 ± 0.8
40.0 ± 0.9
38.3 ± 0.8
44.3
GI
59.0 ± 0.8
45.7 ± 0.7
41.8 ± 0.8
39.6 ± 0.8
38.9 ± 0.7
45.0
SDE-EDG (OURS)
76.2 ± 0.7
62.9 ± 0.8
54.3 ± 0.8
49.6 ± 0.8
42.8 ± 0.8
57.2
|
{z
}
(a) Ground truth of domain 27-30
|
{z
}
(b) Predictions by ERM
|
{z
}
(c) Predictions by LSSAE
|
{z
}
(b) Predictions by SDE-EDG
Figure 5: (a) The ground truth of the Rotated Gaussian between domains 27-30, and positive and
negative labels are red and blue dots separately. (b), (c), (d) show prediction results made by ERM,
LSSAE, and SDE-EDG respectively. The dashed line is the ground truth decision boundary of the
Rotated Gaussian, which rotates 12◦between two consecutive domains. Only SDE-EDG could
effectively capture the exact evolving patterns, achieving the best performance on Rotated Gaussian.
G
ADDITIONAL ABLATIONS
G.1
IMPACT OF DIFFERENT TIME INTERVALS ON BASELINES
The experiments applying various intervals for the baselines show:
• Smaller time intervals generally enhance the performance of all methods.
• With different intervals, our method, SDE-EDG, consistently outperforms the other base-
lines.
Table 14: RMNIST with time interval as ∆t/2
INTERVAL= ∆t/2
130◦
140◦
150◦
160◦
170◦
180◦
AVG
ERM
60.7 ± 0.9
46.6 ± 0.8
39.4 ± 0.8
38.3 ± 0.8
39.8 ± 0.6
41.0 ± 0.8
44.3
LSSAE
65.5 ± 0.8
52.6 ± 0.8
45.2 ± 0.8
39.0 ± 0.8
40.1 ± 0.7
41.1 ± 0.9
47.3
SDE-EDG (OURS)
75.6 ± 0.8
61.8 ± 0.8
49.9 ± 0.8
50.0 ± 0.9
45.1 ± 0.7
44.1 ± 0.9
54.4
Table 15: RMNIST with time interval as 2∆t
INTERVAL= 2∆t
130◦
140◦
150◦
160◦
170◦
180◦
AVG
ERM
47.2 ± 0.8
37.9 ± 0.8
32.9 ± 0.8
34.9 ± 0.7
38.1 ± 1.0
43.8 ± 0.7
39.1
LSSAE
49.3 ± 0.9
38.8 ± 0.8
34.9 ± 0.8
37.8 ± 0.9
40.6 ± 0.8
40.7 ± 0.8
40.4
SDE-EDG (OURS)
58.6 ± 0.8
49.1 ± 0.7
45.6 ± 0.7
42.4 ± 0.8
36.9 ± 0.8
36.1 ± 0.8
44.8
G.2
ABLATION OF α ∈{0.8, 1, 3, 5, 7, 9}
In the ablation study, the hyperparameters were varied within a wide range {0, 0.1, 1, 10, 100, 200},
causing notable fluctuations in the results. On the other hand, within the range of α ∈[0.8, 10),
the model’s performance remains relatively steady (in Figure 4 (d,e)). The subsequent ablation
experiments further show the stable performance by setting α = {0.8, 1, 3, 5, 7, 9}:
In practice, we select hyperparameter α based on the validation set. Without employing cross-
validation, we still can set α = 1 yields consistent results shown in Figure 4 (d,e).
25

Published as a conference paper at ICLR 2024
Table 16: RMNIST with α ∈[0.8, 10]
α
130◦
140◦
150◦
160◦
170◦
180◦
AVG
0.8
72.7 ± 0.7
59.8 ± 0.7
49.6 ± 0.8
49.2 ± 0.7
37.9 ± 0.8
39.5 ± 0.6
51.5
1
75.1 ± 0.8
61.3 ± 0.9
49.8 ± 0.8
49.8 ± 0.9
39.7 ± 0.7
39.7 ± 0.9
52.6
3
73.7 ± 0.8
58.8 ± 0.8
48.6 ± 0.7
45.9 ± 0.7
40.7 ± 0.8
39.3 ± 0.5
51.2
5
73.6 ± 0.7
58.1 ± 0.8
48.3 ± 0.9
42.7 ± 0.8
39.0 ± 0.9
39.7 ± 0.7
50.2
7
76.7 ± 0.7
63.1 ± 0.9
50.5 ± 0.8
41.8 ± 0.9
39.0 ± 0.8
39.1 ± 0.7
51.7
9
73.7 ± 0.8
59.8 ± 0.7
47.9 ± 0.8
44.0 ± 0.7
39.4 ± 0.7
38.5 ± 0.6
50.6
G.3
ABLATION ON UNI- OR MULTI-MODAL CLASSIFICATION LOSS
We conduct the experiments with uni-modal and multi-modal classification loss respectively. The
results reveal that the performance of the uni-modal approach closely resembled that of the multi-
modal. The choice between the modalities often hinges upon the inherent characteristics of the
datasets themselves. In real-world applications, cross-validation can be applied to determine the most
suitable method for a specific dataset.
Table 17: Ablation on uni- and multi-modal loss
DATASETS
RMNIST
PORTRAIT
POWERSUPPLY
UNI
52.4
89.6
75.7
MULTI
52.6
88.9
75.1
26

