Published as a conference paper at ICLR 2021
WHAT ARE THE STATISTICAL LIMITS OF OFFLINE RL
WITH LINEAR FUNCTION APPROXIMATION?
Ruosong Wang
Carnegie Mellon University
ruosongw@andrew.cmu.edu
Dean P. Foster
University of Pennsylvania and Amazon
dean@foster.net
Sham M. Kakade
University of Washington, Seattle and Microsoft Research
sham@cs.washington.edu
ABSTRACT
Ofﬂine reinforcement learning seeks to utilize ofﬂine (observational) data to guide
the learning of (causal) sequential decision making strategies. The hope is that
ofﬂine reinforcement learning coupled with function approximation methods (to
deal with the curse of dimensionality) can provide a means to help alleviate the
excessive sample complexity burden in modern sequential decision making prob-
lems. However, the extent to which this broader approach can be effective is not
well understood, where the literature largely consists of sufﬁcient conditions.
This work focuses on the basic question of what are necessary representational and
distributional conditions that permit provable sample-efﬁcient ofﬂine reinforce-
ment learning. Perhaps surprisingly, our main result shows that even if: i) we have
realizability in that the true value function of every policy is linear in a given set
of features and 2) our off-policy data has good coverage over all features (under a
strong spectral condition), any algorithm still (information-theoretically) requires
a number of ofﬂine samples that is exponential in the problem horizon to non-
trivially estimate the value of any given policy. Our results highlight that sample-
efﬁcient ofﬂine policy evaluation is not possible unless signiﬁcantly stronger con-
ditions hold; such conditions include either having low distribution shift (where
the ofﬂine data distribution is close to the distribution of the policy to be evaluated)
or signiﬁcantly stronger representational conditions (beyond realizability).
1
INTRODUCTION
Ofﬂine methods (also known as off-policy methods or batch methods) are a promising methodol-
ogy to alleviate the sample complexity burden in challenging reinforcement learning (RL) settings,
particularly those where sample efﬁciency is paramount (Mandel et al., 2014; Gottesman et al.,
2018; Wang et al., 2018; Yu et al., 2019). Off-policy methods are often applied together with func-
tion approximation schemes; such methods take sample transition data and reward values as inputs,
and approximate the value of a target policy or the value function of the optimal policy. Indeed,
many practical deep RL algorithms ﬁnd their prototypes in the literature of ofﬂine RL. For exam-
ple, when running on off-policy data (sometimes termed as “experience replay”), deep Q-networks
(DQN) (Mnih et al., 2015) can be viewed as an analog of Fitted Q-Iteration (Gordon, 1999) with
neural networks being the function approximators. More recently, there are an increasing number
of both model-free (Laroche et al., 2019; Fujimoto et al., 2019; Jaques et al., 2020; Kumar et al.,
2019; Agarwal et al., 2020) and model-based (Ross & Bagnell, 2012; Kidambi et al., 2020) ofﬂine
RL methods, with steady improvements in performance (Fujimoto et al., 2019; Kumar et al., 2019;
Wu et al., 2020; Kidambi et al., 2020).
However, despite the importance of these methods, the extent to which data reuse is possible, espe-
cially when off-policy methods are combined with function approximation, is not well understood.
For example, deep Q-network requires millions of samples to solve certain Atari games (Mnih et al.,
2015). Also important is that in some safety-critical settings, we seek guarantees when ofﬂine-
1

Published as a conference paper at ICLR 2021
trained policies can be effective (Thomas, 2014; Thomas et al., 2019). A basic question here is
that if there are fundamental statistical limits on such methods, where sample-efﬁcient ofﬂine RL is
simply not possible without further restrictions on the problem.
In the context of supervised learning, it is well-known that empirical risk minimization is sample-
efﬁcient if the hypothesis class has bounded complexity. For example, suppose the agent is given
a d-dimensional feature extractor, and the ground truth labeling function is a (realizable) linear
function with respect to the feature mapping. Here, it is well-known that a polynomial number
of samples in d sufﬁce for a given target accuracy. Furthermore, in this realizable case, provided
the training data has a good feature coverage, then we will have good accuracy against any test
distribution.1
In the more challenging ofﬂine RL setting, it is unclear if sample-efﬁcient methods are possible,
even under analogous assumptions. This is our motivation to consider the following question:
What are the statistical limits for ofﬂine RL with linear function approximation?
Here, one may hope that value estimation for a given policy is possible in the ofﬂine RL setting
under the analogous set of assumptions that enable sample-efﬁcient supervised learning, i.e., 1)
(realizability) the features can perfectly represent the value functions and 2) (good coverage) the
feature covariance matrix of our off-policy data has lower bounded eigenvalues.
The extant body of provable methods on ofﬂine RL either make representational assumptions that are
far stronger than realizability or assume distribution shift conditions that are far stronger than having
coverage with regards to the spectrum of the feature covariance matrix of the data distribution. For
example, Szepesv´ari & Munos (2005) analyze ofﬂine RL methods by assuming a representational
condition where the features satisfy (approximate) closedness under Bellman updates, which is a far
stronger representation condition than realizability. Recently, Xie & Jiang (2020a) propose a ofﬂine
RL algorithm that only requires realizability as the representation condition. However, the algorithm
in (Xie & Jiang, 2020a) requires a more stringent data distribution condition. Whether it is possible
to design a sample-efﬁcient ofﬂine RL method under the realizability assumption and a reasonable
data coverage assumption — an open problem in (Chen & Jiang, 2019) — is the focus of this work.
Our Contributions.
Perhaps surprisingly, our main result shows that, under only the above two
assumptions, it is information-theoretically not possible to design a sample-efﬁcient algorithm to
non-trivially estimate the value of a given policy. The following theorem is an informal version of
the result in Section 4.
Theorem 1.1 (Informal). In the ofﬂine RL setting, suppose the data distributions have (polynomi-
ally) lower bounded eigenvalues, and the Q-functions of every policy are linear with respect to a
given feature mapping. Any algorithm requires an exponential number of samples in the horizon
H to output a non-trivially accurate estimate of the value of any given policy π, with constant
probability.
This hardness result states that even if the Q-functions of all polices are linear with respect to the
given feature mapping, we still require an exponential number of samples to evaluate any given
policy. Note that this representation condition is signiﬁcantly stronger than assuming realizability
with regards to only a single target policy; it assumes realizability for all policies. Regardless, even
under this stronger representation condition, it is hard to evaluate any policy, as speciﬁed in our
hardness result.
This result also formalizes a key issue in ofﬂine reinforcement learning with function approxima-
tion: geometric error ampliﬁcation. To better illustrate the issue, in Section 5, we analyze the clas-
sical Least-Squares Policy Evaluation (LSPE) algorithm under the realizability assumption, which
demonstrates how the error propagates as the algorithm proceeds. Here, our analysis shows that,
if we only rely on the realizability assumption, then a far more stringent condition is required for
sample-efﬁcient ofﬂine policy evaluation: the off-policy data distribution must be quite close to the
distribution induced by the policy to be evaluated.
1Speciﬁcally, if the features have a uniformly bounded norm and if the minimum eigenvalue of the feature
covariance matrix of our data is bounded away from 0, say by 1/poly(d), then we have good accuracy on any
test distribution. See Assumption 2 and the comments thereafter.
2

Published as a conference paper at ICLR 2021
Our results highlight that sample-efﬁcient ofﬂine RL is simply not possible unless either the distri-
bution shift condition is sufﬁciently mild or we have stronger representation conditions that go well
beyond realizability. See Section 5 for more details.
Furthermore, our hardness result implies an exponential separation on the sample complexity be-
tween ofﬂine RL and supervised learning, since supervised learning (which is equivalent to ofﬂine
RL with H = 1) is possible with polynomial number of samples under the same set of assumptions.
A few additional points are worth emphasizing with regards to our lower bound construction:
• Our results imply that Least-Squares Policy Evaluation (LSPE, i.e., using Bellman backups with
linear regression) will fail. Interestingly, while LSPE will provide an unbiased estimator, our
results imply that it will have exponential variance in the problem horizon.
• Our construction is simple and does not rely on having a large state or action space: the size of
the state space is only O(d · H) where d is the feature dimension and H is the planning horizon,
and the size of the action space is only is 2. This stands in contrast to other RL lower bounds,
which typically require state spaces that are exponential in the problem horizon (e.g. see (Du
et al., 2020)).
• We provide two hard instances, one with a sparse reward (and stochastic transitions) and another
with deterministic dynamics (and stochastic rewards). These two hard instances jointly imply that
both the estimation error on reward values and the estimation error on the transition probabilities
could be geometrically ampliﬁed in ofﬂine RL.
• Of possibly broader interest is that our hard instances are, to our knowledge, the ﬁrst concrete
examples showing that geometric error ampliﬁcation is real in RL problems (even with realizabil-
ity). While this is a known concern in the analysis of RL algorithms, there have been no concrete
examples exhibiting such behavior under only a realizability assumption.
2
RELATED WORK
We now survey prior work on ofﬂine RL, largely focusing on theoretical results. We also discuss
results on the error ampliﬁcation issue in RL. Concurrent to this work, Xie & Jiang (2020a) propose
a ofﬂine RL algorithm under the realizability assumption, which requires stronger distribution shift
conditions. We will discuss this work shortly.
Existing Algorithms and Analysis.
Ofﬂine RL with value function approximation is closely re-
lated to Approximate Dynamic Programming (Bertsekas & Tsitsiklis, 1995). Existing work (Munos,
2003; Szepesv´ari & Munos, 2005; Antos et al., 2008; Munos & Szepesv´ari, 2008; Tosatto et al.,
2017; Xie & Jiang, 2020b; Duan & Wang, 2020) that analyze the sample complexity of approximate
dynamic programming-based approaches usually make the following two categories of assumptions:
(i) representation conditions that assume the function class approximates the value functions well
and (ii) distribution shift conditions that assume the given data distribution has sufﬁcient coverage
over the state-action space. As mentioned in the introduction, the desired representation condition
would be realizability, which only assumes the value function of the policy to be evaluated lies in
the function class (for the case of ofﬂine policy evaluation) or the optimal value function lies in
the function class (for the case of ﬁnding near-optimal policies), and existing works usually make
stronger assumptions. For example, Szepesv´ari & Munos (2005); Duan & Wang (2020) assume (ap-
proximate) closedness under Bellman updates, which is much stronger than realizability. Whether
it is possible to design a sample-efﬁcient ofﬂine RL method under the realizability assumption and
reasonable data coverage assumption, is left as an open problem in (Chen & Jiang, 2019).
To measure the coverage over the state-action space of the given data distribution, existing works
assume the concentrability coefﬁcient (introduced by Munos (2003)) to be bounded. The concen-
trability coefﬁcient, informally speaking, is the largest possible ratio between the probability for a
state-action pair (s, a) to be visited by a policy, and the probability that (s, a) appears on the data
distribution. Since we work with linear function approximation in this work, we measure the distri-
bution shift in terms of the spectrum of the feature covariance matrices (see Assumption 2), which
is a well-known sufﬁcient condition in the context of supervised learning and is much more natural
for the case of linear function approximation.
3

Published as a conference paper at ICLR 2021
Concurrent to this work, Xie & Jiang (2020a) propose an algorithm that works under the realizabil-
ity assumption instead of other stronger representation conditions used in prior work. However, the
algorithm in (Xie & Jiang, 2020a) requires a much stronger data distribution condition which as-
sumes a stringent version of concentrability coefﬁcient introduced by (Munos, 2003) to be bounded.
In contrast, in this work we measure the distribution shift in terms of the spectrum of the feature
covariance matrix of the data distribution, which is more natural than the concentrability coefﬁcient
for the case of linear function approximation.
Recently, there has been great interest in approaching ofﬂine policy evaluation (Precup, 2000) via
importance sampling. For recent work on this topic, see (Dud´ık et al., 2011; Mandel et al., 2014;
Thomas et al., 2015; Li et al., 2015; Jiang & Li, 2016; Thomas & Brunskill, 2016; Guo et al.,
2017; Wang et al., 2017; Liu et al., 2018; Farajtabar et al., 2018; Xie et al., 2019; Kallus & Ue-
hara, 2019; Liu et al., 2019; Uehara & Jiang, 2019; Kallus & Uehara, 2020; Jiang & Huang, 2020;
Feng et al., 2020). Ofﬂine policy evaluation with importance sampling incurs exponential variance
in the planning horizon when the behavior policy is signiﬁcantly different from the policy to be
evaluated. Bypassing such exponential dependency requires non-trivial function approximation as-
sumptions (Jiang & Huang, 2020; Feng et al., 2020; Liu et al., 2018). Finally, Kidambi et al. (2020)
provide a model-based ofﬂine RL algorithm, with a theoretical analysis based on hitting times.
Hardness Results.
Algorithm-speciﬁc hardness results have been known for a long time in the
literature of Approximate Dynamic Programming. See Chapter 4 in (Van Roy, 1994) and also (Gor-
don, 1995; Tsitsiklis & Van Roy, 1996). These works demonstrate that certain approximate dynamic
programming-based methods will diverge on hard cases. However, such hardness results only hold
for a restricted class of algorithms, and to demonstrate the fundamental difﬁculty of ofﬂine RL, it is
more desirable to obtain information-theoretic lower bounds as initiated by Chen & Jiang (2019).
Existing (information-theoretic) exponential lower bounds (Krishnamurthy et al., 2016; Sun et al.,
2017; Chen & Jiang, 2019) usually construct unstructured MDPs with an exponentially large state
space. Du et al. (2020) prove an exponential lower bound under the assumption that the optimal Q-
function is approximately linear. The condition that the optimal Q-function is only approximately
linear is crucial for the hardness result in Du et al. (2020). The techniques in (Du et al., 2020) are
later generalized to other settings (Kumar et al., 2020; Wang et al., 2020; Mou et al., 2020).
Error Ampliﬁcation In RL.
Error ampliﬁcation induced by distribution shift and long planning
horizon is a known issue in the theoretical analysis of RL algorithms. See (Gordon, 1995; 1996;
Munos & Moore, 1999; Ormoneit & Sen, 2002; Kakade, 2003; Zanette et al., 2019) for papers on
this topic and additional assumptions that mitigate this issue. Error ampliﬁcation in ofﬂine RL is
also observed in empirical works (see e.g. (Fujimoto et al., 2019)). In this work, we provide the ﬁrst
information-theoretic lower bound showing that geometric error ampliﬁcation is real in ofﬂine RL.
3
THE OFFLINE POLICY EVALUATION PROBLEM
Throughout this paper, for a given integer H, we use [H] to denote the set {1, 2, . . . , H}.
Episodic Reinforcement Learning.
Let M = (S, A, P, R, H) be a Markov Decision Process
(MDP) where S is the state space, A is the action space, P : S × A →∆(S) is the transition
operator which takes a state-action pair and returns a distribution over states, R : S × A →∆(R)
is the reward distribution, H ∈Z+ is the planning horizon. For simplicity, we assume a ﬁxed initial
state s1 ∈S. A (stochastic) policy π : S →∆(A) chooses an action a randomly based on the
current state s. The policy π induces a (random) trajectory s1, a1, r1, s2, a2, r2, . . . , sH, aH, rH,
where a1 ∼π1(s1), r1 ∼R(s1, a1), s2 ∼P(s1, a1), a2 ∼π2(s2), etc. To streamline our analysis,
for each h ∈[H], we use Sh ⊆S to denote the set of states at level h, and we assume Sh do not
intersect with each other. We assume, almost surely, that rh ∈[−1, 1] for all h ∈[H].
Value Functions.
Given a policy π, h ∈[H] and (s, a) ∈Sh × A, deﬁne Qπ
h(s, a) =
E
hPH
h′=h rh′ | sh = s, ah = a, π
i
and V π
h (s) = E
hPH
h′=h rh′ | sh = s, π
i
. For a policy π, we
deﬁne V π = V π
1 (s1) to be the value of π from the ﬁxed initial state s1.
4

Published as a conference paper at ICLR 2021
Linear Function Approximation.
When applying linear function approximation schemes, it is
commonly assumed that the agent is given a feature extractor φ : S × A →Rd which can either be
hand-crafted or a pre-trained neural network that transforms a state-action pair to a d-dimensional
embedding, and the Q-functions can be predicted by linear functions of the features. In this paper,
we are interested in the following realizability assumption.
Assumption 1 (Realizable Linear Function Approximation). For every policy π : S →∆(A), there
exists θπ
1 , . . . θπ
H ∈Rd such that for all (s, a) ∈S × A and h ∈[H], Qπ
h(s, a) = (θπ
h)⊤φ(s, a).
Note that our assumption is much stronger than assuming realizability with regards to a single policy
π (say the policy that we wish to evaluate); our assumption imposes realizability for all policies.
Ofﬂine Reinforcement Learning.
This paper is concerned with the ofﬂine RL setting. In this
setting, the agent does not have direct access to the MDP and instead is given access to data dis-
tributions {µh}H
h=1 where for each h ∈[H], µh ∈∆(Sh × A). The inputs of the agent are H
datasets {Dh}H
h=1, and for each h ∈[H], Dh consists i.i.d. samples of the form (s, a, r, s′) ∈
Sh × A × R × Sh+1 tuples, where (s, a) ∼µh, r ∼r(s, a), s′ ∼P(s, a).
In this paper, we focus on the ofﬂine policy evaluation problem with linear function approximation:
given a policy π : S →∆(A) and a feature extractor φ : S × A →Rd, the goal is to output an
accurate estimate of the value of π (i.e., V π) approximately, using the collected datasets {Dh}H
h=1,
with as few samples as possible.
Notation.
For a vector x ∈Rd, we use ∥x∥2 to denote its ℓ2 norm. For a positive semideﬁnite
matrix A, we use ∥A∥2 to denote its operator norm, and σmin(A) to denote its smallest eigenvalue.
For two positive semideﬁnite matrices A and B, we write A ⪰B to denote the L¨owner partial
ordering of matrices, i.e, A ⪰B if and only if A −B is positive semideﬁnite. For a policy π : S →
∆(A), we use µπ
h to denote the marginal distribution of sh under π, i.e., µπ
h(s) = Pr[sh = s | π].
For a vector x ∈Rd and a positive semideﬁnite matrix A ∈Rd×d, we use ∥x∥A to denote
√
x⊤Ax.
4
THE LOWER BOUND: REALIZABILITY AND COVERAGE ARE INSUFFICIENT
We now present our main hardness result for ofﬂine policy evaluation with linear function approxi-
mation. It should be evident that without feature coverage in our dataset, realizability alone is clearly
not sufﬁcient for sample-efﬁcient estimation. Here, we will make the strongest possible assumption,
with regards to the conditioning of the feature covariance matrix.
Assumption 2 (Feature Coverage). For all (s, a) ∈S × A, assume our feature map is bounded
such that ∥φ(s, a)∥2 ≤1. Furthermore, suppose for each h ∈[H], the data distributions µh satisfy
the following minimum eigenvalue condition: σmin
 E(s,a)∼µh[φ(s, a)φ(s, a)⊤]

= 1/d.2
Clearly, for the case where H = 1, the realizability assumption (Assumption 1), and feature cov-
erage assumption (Assumption 2) imply that the ordinary least squares estimator will accurately
estimate θ1.3 Our main result now shows that these assumptions are not sufﬁcient for ofﬂine policy
evaluation for long horizon problems.
Theorem 4.1. Suppose Assumption 2 holds. Fix an algorithm that takes as input both a policy and
a feature mapping. There exists a (deterministic) MDP satisfying Assumption 1, such that for any
policy π : S →∆(A), the algorithm requires Ω((d/2)H) samples to output the value of π up to
constant additive approximation error with probability at least 0.9.
Although we focus on ofﬂine policy evaluation in this work, our hardness result also holds for
ﬁnding near-optimal policies under Assumption 1 in the ofﬂine RL setting. Below we give a simple
reduction. At the initial state, if the agent chooses action a1, then the agent receives a ﬁxed reward
value (say 0.5) and terminates. If the agent chooses action a2, then the agent transits to our hard
2Note that 1/d is the largest possible minimum eigenvalue due to that, for any data distribution eµh,
σmin(E(s,a)∼eµh[φ(s, a)φ(s, a)⊤]) ≤1/d since ∥φ(s, a)∥2 ≤1 for all (s, a) ∈S × A.
3For H = 1, the ordinary least squares estimator will satisfy that ∥θ1 −bθOLS∥2
2 ≤O(d/n) with high
probability. See e.g. (Hsu et al., 2012b).
5

Published as a conference paper at ICLR 2021
instance. In order to ﬁnd a policy with suboptimality at most 0.5, the agent must evaluate the value
of the optimal policy in our hard instance up to an error of 0.5, and hence the hardness result holds.
Remark 1 (The sparse reward case). As stated, the theorem uses a deterministic MDP (with stochas-
tic rewards). See Appendix C for another hard case where the transition is stochastic and the reward
is deterministic and sparse (only occurring at two states at h = H).
Remark 2 (Least-Squares Policy Evaluation (LSPE) has exponential variance). For ofﬂine policy
evaluation with linear function approximation, the most na¨ıve algorithm here would be LSPE, i.e.,
using ordinary least squares (OLS) to estimate θπ, starting at level h = H and then proceeding
backwards to level h = 1, using the plug-in estimator from the previous level. Here, LSPE will pro-
vide an unbiased estimate (provided the feature covariance matrices are full rank, which will occur
with high probability). As a direct corollary, the above theorem implies that LSPE has exponential
variance in H. See Section 5 for a more detailed discussion on LSPE. More generally, our theorem
implies that there is no estimator that can avoid such exponential dependence in the ofﬂine setting.
Remark 3 (Least-Squares Value Iteration (LSVI) versus Least-Squares Policy Iteration (LSPI)). In
the ofﬂine setting, under Assumptions 1 and 2, in order to ﬁnd a near-optimal policy, the most na¨ıve
algorithm would be LSVI, i.e., using ordinary least squares (OLS) to estimate θ∗, starting at level
h = H and then proceeding backwards to level h = 1, using the plug-in estimator from the previous
level and the bellman operator. The above theorem implies that LSVI will require an exponential
number of samples to ﬁnd a near-optimal policy. On the other hand, if the regression targets are
collected by using rollouts (i.e. on-policy sampling) as in LSPI (Lagoudakis & Parr, 2003), then a
polynomial number of samples sufﬁce. See Section D in (Du et al., 2020) for an analysis. Thus,
Theorem 4.1 implies an exponential separation on the sample complexity between LSVI and LSPI.
Of course, LSPI requires adaptive data samples and thus does not work in the ofﬂine setting.
One may wonder if Theorem 4.1 still holds when the data distributions {µh}H
h=1 are induced by
a policy. In Appendix C, we prove another exponential sample complexity lower bound under the
additional assumption that the data distributions are induced by a ﬁxed policy π. However, under
such an assumption, it is impossible to prove a hardness result as strong as Theorem 4.1 (which
shows that evaluating any policy is hard), since one can at least evaluate the policy π that induces
the data distributions. Nevertheless, we are able to prove the hardness of ofﬂine policy evaluation,
under a weaker version of Assumption 1. See Appendix C for more details.
In the remaining part of this section, we give the hard instance construction and the proof of Theo-
rem 4.1. We use d the denote the feature dimension, and we assume d is even for simplicity. We use
ˆd to denote d/2 for convenience. We also provide an illustration of the construction in Figure 1.
State Space, Action Space and Transition Operator.
The action space A = {a1, a2}. For each
h ∈[H], Sh contains ˆd + 1 states s1
h, s2
h, . . . , s ˆd
h and s
ˆd+1
h
. For each h ∈[H −1], for each
c ∈{1, 2, . . . , ˆd + 1}, we have P(sc
h, a1) = s
ˆd+1
h+1 and P(sc
h, a1) = sc
h+1.
Reward Distributions.
Let 0 ≤r0 ≤ˆd−H/2 be a parameter to be determined. For each (h, c) ∈
[H −1] × [ ˆd] and a ∈A, we set R(sc
h, a) = 0 and R(s
ˆd+1
h
, a) = r0 · ( ˆd1/2 −1) · ˆd(H−h)/2. For
the last level, for each c ∈[ ˆd] and a ∈A, we set R(sc
H, a) =
1
with probability (1 + r0)/2
−1
with probability (1 −r0)/2
so that E[R(sc
H, a)] = r0. Moreover, for all actions a ∈A, R(s
ˆd+1
H
, a) = r0 · ˆd1/2.
Feature Mapping.
Let e1, e2, . . . , ed be a set of orthonormal vectors in Rd. Here, one possible
choice is to set e1, e2, . . . , ed to be the standard basis vectors. For each (h, c) ∈[H] × [ ˆd], we set
φ(sc
h, a1) = ec, φ(sc
h, a2) = ec+ ˆd, and φ(s
ˆd+1
h
, a) = P
c∈ˆd ec/ ˆd1/2 for all a ∈A.
Verifying Assumption 1.
The following lemma shows that Assumption 1 holds for our construc-
tion. The formal proof can be found in Appendix A.
Lemma 4.2. For every policy π : S →∆(A), for each h ∈[H], for all (s, a) ∈Sh × A, we have
Qπ
h(s, a) = (θπ
h)⊤φ(s, a) for some θπ
h ∈Rd.
6

Published as a conference paper at ICLR 2021
…
…
…
…
𝑄𝑠, 𝑎! = 𝑟" '𝑑($%!)/(
𝑅𝑠, 𝑎= 0
𝑄𝑠, 𝑎! = 𝑟" '𝑑($%()/(
𝑅𝑠, 𝑎= 0
𝑄𝑠, 𝑎! = 𝑟" '𝑑!/(
𝑅𝑠, 𝑎= 0
𝑄𝑠, 𝑎= 𝑟"
𝔼[𝑅𝑠, 𝑎] = 𝑟"
𝑄𝑠!
)*+!, 𝑎= 𝑟" '𝑑$/(
𝑅𝑠!
)*+!, 𝑎= 𝑟"( '𝑑$/( −'𝑑($%!)/()
𝑄𝑠(
)*+!, 𝑎= 𝑟" '𝑑($%!)/(
𝑅𝑠(
)*+!, 𝑎= 𝑟"( '𝑑($%!)/( −'𝑑($%()/()
𝑄𝑠$%!
)*+!, 𝑎= 𝑟" '𝑑
𝑅𝑠$%!
)*+!, 𝑎= 𝑟"( '𝑑−'𝑑!/()
𝑄𝑠$
)*+!, 𝑎= 𝑟" '𝑑!/(
𝑅𝑠$
)*+!, 𝑎= 𝑟" '𝑑!/(
𝑎!
𝑎(
𝜙𝑠,
-, 𝑎! = 𝑒-
𝜙𝑠,
-, 𝑎( = 𝑒-+ )*
𝜙𝑠,
)*+!, 𝑎= (𝑒! + 𝑒( + ⋯+ 𝑒)*)/ '𝑑!/(
𝑠!
!
𝑠!
(
𝑠!
.
𝑠!
)*
𝑠!
)*+!
𝑠(
!
𝑠(
(
𝑠(
.
𝑠(
)*
𝑠(
)*+!
𝑠$%!
!
𝑠$%!
)*+!
𝑠$%!
)*
𝑠$%!
(
𝑠$%!
.
𝑠$
!
𝑠$
(
𝑠$
.
𝑠$
)*
𝑠$
)*+!
⋯
⋯
⋯
⋯
⋯
…
𝑄𝑠,
)*+!, 𝑎= 𝑟" '𝑑($%,+!)/(
𝑅𝑠,
)*+!, 𝑎= 𝑟"( '𝑑($%,+!)/( −'𝑑($%,)/()
𝑄𝑠, 𝑎! = 𝑟" '𝑑($%,)/(
𝑅𝑠, 𝑎= 0
Figure 1: An illustration of the hard instance. Recall that ˆd = d/2. States on the top are those in
the ﬁrst level (h = 1), while states at the bottom are those in the last level (h = H). Solid line
(with arrow) corresponds to transitions associated with action a1, while dotted line (with arrow)
corresponds to transitions associated with action a2. For each level h ∈[H], reward values and
Q-values associated with s1
h, s2
h, . . . , s ˆd
h are marked on the left, while reward values and Q-values
associated with s
ˆd+1
h
are mark on the right. Rewards and transitions are all deterministic, except
for the reward distributions associated with s1
H, s2
H, . . . , s ˆd
H. We mark the expectation of the reward
value when it is stochastic. For each level h ∈[H], for the data distribution µh, the state is chosen
uniformly at random from those states in the dashed rectangle, i.e., {s1
h, s2
h, . . . , s ˆd
h}, while the action
is chosen uniformly at random from {a1, a2}. Suppose the initial state is s
ˆd+1
1
. When r0 = 0, the
value of the policy is 0. When r0 = ˆd−H/2, the value of the policy is r0 · ˆdH/2 = 1.
The Data Distributions.
For each level h ∈[H], the data distribution µh is a uniform distribution
over {(s1
h, a1), (s1
h, a2), (s2
h, a1), (s2
h, a2), . . . , (s ˆd
h, a1), (s ˆd
h, a2)}. Notice that (s
ˆd+1
h
, a) is not in the
support of µh for all a ∈A. It can be seen that, E(s,a)∼µh

φ(s, a)φ(s, a)⊤
= 1
d
Pd
c=1 ece⊤
c = 1
dI.
The Lower Bound.
We show that it is information-theoretically hard for any algorithm to distin-
guish the case r0 = 0 and r0 = ˆd−H/2. We ﬁx the initial state to be s
ˆd+1
1
, and consider any policy π.
When r0 = 0, all reward values will be zero, and thus the value of π is zero. On the other hand, when
r0 = ˆd−H/2, the value of π would be r0 · ˆdH/2 = 1. Thus, if the algorithm approximates the value
of the policy up to an error of 1/2, then it must distinguish the case that r0 = 0 and r0 = ˆd−H/2.
We ﬁrst notice that for the case r0 = 0 and r0 = ˆd−H/2, the data distributions {µh}H
h=1, the fea-
ture mapping φ : S × A →Rd, the policy π to be evaluated and the transition operator P are the
same. Thus, in order to distinguish the case r0 = 0 and r0 = ˆd−H/2, the only way is to query the
reward distribution by using sampling taken from the data distributions. For all state-action pairs
(s, a) in the support of the data distributions of the ﬁrst H −1 levels, the reward distributions will
be identical. This is because for all s ∈Sh \ {s
ˆd+1
h
} and a ∈A, we have R(s, a) = 0. For
the case r0 = 0 and r0 = ˆd−H/2, for all state-action pairs (s, a) in the support of the data dis-
tribution of the last level, R(s, a) =
1
with probability (1 + r0)/2
−1
with probability (1 −r0)/2 . Therefore, to distinguish
7

Published as a conference paper at ICLR 2021
Algorithm 1 Least-Squares Policy Evaluation
1: Input: policy π to be evaluated, number of samples N, regularization parameter λ > 0
2: Let QH+1(·, ·) = 0 and VH+1(·) = 0
3: for h = H, H −1, . . . , 1 do
4:
Take samples (si
h, ai
h) ∼µh, ri
h ∼r(si
h, ai
h) and si
h ∼P(si
h, ai
h) for each i ∈[N]
5:
Let ˆΛh = P
i∈[N] φ(si
h, ai
h)φ(si
h, ai
h)⊤+ λI
6:
Let ˆθh = ˆΛ−1
h
PN
i=1 φ(si
h, ai
h) · (ri
h + ˆVh+1(si
h))

7:
Let ˆQh(·, ·) = φ(·, ·)⊤ˆθh and ˆVh(·) = ˆQ(·, π(·))
the case that r0 = 0 and r0 = ˆd−H/2, the agent needs to distinguish two reward distributions
r1 =
1
with probability 1/2
−1
with probability 1/2 and r2 =
(
1
with probability (1 + ˆd−H/2)/2
−1
with probability (1 −ˆd−H/2)/2 . Now we in-
voke Lemma B.1 in Section B by setting ε = ˆd−H/2/2 and δ = 0.9. By Lemma B.1, in order to
distinguish r1 and r2 with probability at least 0.9, any algorithm requires Ω( ˆdH) samples.
Remark 4. The key in our construction is the state s
ˆd+1
h
in each level, whose feature vector is
deﬁned to be P
c∈ˆd ec/ ˆd1/2. In each level, s
ˆd+1
h
ampliﬁes the Q-value by a ˆd1/2 factor, due to the
linearity of the Q-function. After all the H levels, the value will be ampliﬁed by a ˆdH/2 factor. Since
s
ˆd+1
h
is not in the support of the data distribution, the only way to estimate the value of the policy is
to estimate the expected reward value in the last level. Our construction forces the estimation error
of the last level to be ampliﬁed exponentially and thus implies an exponential lower bound.
5
UPPER BOUNDS: LOW DISTRIBUTION SHIFT OR POLICY COMPLETENESS
ARE SUFFICIENT
In order to illustrate the error ampliﬁcation issue and discuss conditions that permit sample-efﬁcient
ofﬂine RL, in this section, we analyze Least-Squares Policy Evaluation when applied to the ofﬂine
policy evaluation problem under the realizability assumption. The algorithm is presented in Algo-
rithm 1. For simplicity here we assume the policy π to be evaluated is deterministic.
Notation.
For each h ∈[H], deﬁne Λh = E(s,a)∼µh

φ(s, a)φ(s, a)⊤
to be the feature covariance
matrix at level h. For each h ∈[H−1], deﬁne Λh+1 = E(s,a)∼µh,s∼P (·|s,a)

φ(s, π(s))φ(s, π(s))⊤
to be the feature covariance matrix of the one-step lookahead distribution at level h. Moreover, deﬁne
Λ1 = φ(s1, π(s1))φ(s1, π(s1))⊤. We deﬁne Φh to be a N × d matrix, whose i-th row is φ(si
h, ai
h),
and deﬁne Φh+1 to be another N × d matrix whose i-th row is φ(si
h, π(si
h)). For each h ∈[H] and
i ∈[N], deﬁne ξi
h = ri
h + V (si
h) −Q(si
h, ai
h). We use ξh to denote a vector whose i-th entry is ξi
h.
Now we present a general lemma that characterizes the estimation error of Algorithm 1 by an equal-
ity. The proof can be found in Appendix D. Later, we apply this general lemma to special cases.
Lemma 5.1. Suppose λ > 0 in Algorithm 1, and for the given policy π, there exists θ1, θ2, . . . , θd ∈
Rd such that for each h ∈[H], Qπ
h(s, a) = φ(s, a)⊤θh for all (s, a) ∈Sh × A. Then we have
(Qπ(s1, π(s1)) −ˆQ(s1, π(s1)))2 =

H
X
h=1
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 · · · (ˆΛ−1
h Φ⊤
h ξh −λˆΛ−1
h θh)

2
Λ1
. (1)
Now we consider two special cases where the estimation error in Equation (1) can be upper bounded.
Low Distribution Shift.
The ﬁrst special we focus on is the case where the distribution shift
between the data distributions and the distribution induced by the policy to be evaluated is low. To
measure the distribution shift formally, our main assumption is as follows.
Assumption 3. We assume that for each h ∈[H], there exists Ch ≥1 such that Λh ⪯ChΛh.
8

Published as a conference paper at ICLR 2021
Remark 5. For each h ∈[H], if σmin(Λh) ⪰
1
Ch I for some Ch ≥1, we have Λh ⪯I ⪯ChΛh.
Therefore, Assumption 3 can be replaced with the assumption that ChΛh ⪰I. However, we stick
to the original version of Assumption 3 as it gives a tighter characterization of the distribution shift.
Now we state the theoretical guarantee of Algorithm 1. The proof can be found in Appendix D.
Theorem 5.2. Suppose for the given policy π, there exists θ1, θ2, . . . , θd ∈Rd such that for
each h ∈[H], Qπ
h(s, a) = φ(s, a)⊤θh for all (s, a) ∈Sh × A and ∥θh∥2 ≤H
√
d.4
Let
λ = CH
p
d log(dH/δ)N for some C > 0. With probability at least 1 −δ, for some c > 0,
(Qπ
1(s1, π(s1)) −ˆQ1(s1, π(s1)))2 ≤c · QH
h=1 Ch · dH5 ·
p
d log(dH/δ)/N.
Remark 6. The factor QH
h=1 Ch in Theorem 5.2 implies that the estimation error will be ampliﬁed
geometrically. Now we discuss how the error is ampliﬁed when running Algorithm 1 on the instance
in Section 4 to better illustrate the issue. If we run Algorithm 1 on the hard instance in Section 4,
when h = H, the estimation error on V (sc
H) would be roughly N −1/2 for each c ∈[ ˆd]. When using
the linear predictor at level H to predict the value of s∗
H, the error will be ampliﬁed by ˆd1/2. When
h = H −1, the dataset contains only sc
H−1 for c ∈[ ˆd], and the estimation error on the value of sc
H−1
will be the same as that of s∗
H, which is roughly ( ˆd/N)1/2. Again, the estimation error on the value
of s∗
H−1 will be ( ˆd2/N)1/2 when using the linear predictor at level H −1. The error will eventually
be ampliﬁed by a factor of ˆdH/2, which corresponds to the factor QH
h=1 Ch in Theorem 5.2.
Policy Completeness.
In ofﬂine RL, another representation condition is closedness under Bellman
update (Szepesv´ari & Munos, 2005; Duan & Wang, 2020), which is stronger than realizability. In
the context of ofﬂine policy evaluation, we have the following policy completeness assumption.
Assumption 4. For the given policy π, for any h > 1 and θh ∈Rd, there exists θ′ ∈Rd such that
for any (s, a) ∈Sh−1 × A, E[R(s, a)] + P
s′∈Sh P(s′ | s, a)φ(s′, π(s′))⊤θh = φ(s, a)⊤θ′.
Under Assumption 4 and the assumption that σmin(Λh) ≥λ0 for all h ∈[H] for some λ0 > 0,
Duan & Wang (2020) have shown that for Algorithm 1, by taking N = poly(H, d, 1/ε, 1/λ0), we
have (Qπ
1(s1, π(s1)) −ˆQ1(s1, π(s1)))2 ≤ε. We refer interested readers to (Duan & Wang, 2020).
We remark that the above analysis again implies that geometric error ampliﬁcation is a real issue in
ofﬂine RL, and sample-efﬁcient ofﬂine RL is impossible unless the distribution shift is sufﬁciently
low, i.e., QH
h=1 Ch is bounded, or strong representation condition (e.g. policy completeness) holds.
6
CONCLUSION
While the extant body of provable results in the literature largely focus on sufﬁcient conditions for
sample-efﬁcient ofﬂine RL, this work focuses on obtaining a better understanding of the necessary
conditions, where we seek to understand to what extent mild assumptions can imply sample-efﬁcient
ofﬂine RL. This work shows that for off-policy evaluation, even if we are given a representation that
can perfectly represent the value function of the given policy and the data distribution has good
coverage over the features, any provable algorithm still requires an exponential number of samples
to non-trivially approximate the value of the given policy. These results highlight that provable
sample-efﬁcient ofﬂine RL is simply not possible unless either the distribution shift condition is
sufﬁciently mild or we have stronger representation conditions that go well beyond realizability.
ACKNOWLEDGMENTS
The authors would like to thank Akshay Krishnamurthy, Alekh Agarwal, Wen Sun, and Nan Jiang
for numerous helpful discussion.
Sham M. Kakade gratefully acknowledges funding from the
ONR award N00014-18-1-2247, and NSF Awards CCF-1703574 and CCF-1740551.
Ruosong
Wang was supported in part by the NSF IIS1763562, US Army W911NF1920104, and ONR Grant
N000141812861. Research performed while Ruosong Wang was an intern at Microsoft Research.
4Without loss of generality, we can work in a coordinate system such that ∥θh∥2 ≤H
√
d and ∥φ(s, a)∥2 ≤
1 for all (s, a) ∈S × A. This follows due to John’s theorem (e.g. see (Ball, 1997; Bubeck et al., 2012)).
9

Published as a conference paper at ICLR 2021
REFERENCES
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofﬂine
reinforcement learning. In International Conference on Machine Learning, 2020.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Andr´as Antos, Csaba Szepesv´ari, and R´emi Munos. Learning near-optimal policies with bellman-
residual minimization based ﬁtted policy iteration and a single sample path. Machine Learning,
71(1):89–129, 2008.
Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:1–58,
1997.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In Pro-
ceedings of 1995 34th IEEE Conference on Decision and Control, volume 1, pp. 560–564. IEEE,
1995.
S´ebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade.
Towards minimax policies for
online linear optimization with bandit feedback. In COLT 2012 - The 25th Annual Conference on
Learning Theory, June 25-27, 2012, Edinburgh, Scotland, volume 23 of JMLR Proceedings, pp.
41.1–41.14, 2012.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.
In International Conference on Machine Learning, pp. 1042–1051, 2019.
Herman Chernoff. Sequential analysis and optimal design. SIAM, 1972.
Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation suf-
ﬁcient for sample efﬁcient reinforcement learning?
In International Conference on Learning
Representations, 2020.
Yaqi Duan and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approx-
imation. arXiv preprint arXiv:2002.09516, 2020.
Miroslav Dud´ık, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, pp. 1097–1104, 2011.
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning, pp. 1447–1456, 2018.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. arXiv preprint arXiv:2008.06668, 2020.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062, 2019.
Geoffrey J Gordon. Stable function approximation in dynamic programming. In Machine Learning
Proceedings 1995, pp. 261–268. Elsevier, 1995.
Geoffrey J Gordon. Stable ﬁtted reinforcement learning. In Advances in neural information pro-
cessing systems, pp. 1052–1058, 1996.
Geoffrey J Gordon.
Approximate solutions to markov decision processes.
Technical report,
CARNEGIE-MELLON UNIV PITTSBURGH PA SCHOOL OF COMPUTER SCIENCE, 1999.
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan,
Linying Zhang, Yi Ding, David Wihl, Xuefeng Peng, Jiayu Yao, Isaac Lage, Christopher Mosch,
Li wei H. Lehman, Matthieu Komorowski, Matthieu Komorowski, Aldo Faisal, Leo Anthony
Celi, David Sontag, and Finale Doshi-Velez. Evaluating reinforcement learning algorithms in
observational health settings, 2018.
10

Published as a conference paper at ICLR 2021
Zhaohan Guo, Philip S Thomas, and Emma Brunskill. Using options and covariance testing for long
horizon off-policy policy evaluation. In Advances in Neural Information Processing Systems, pp.
2492–2501, 2017.
Daniel Hsu, Sham Kakade, Tong Zhang, et al. A tail inequality for quadratic forms of subgaussian
random vectors. Electronic Communications in Probability, 17, 2012a.
Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
Conference on learning theory, pp. 9–1, 2012b.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning
of human preferences in dialog, 2020.
URL https://openreview.net/forum?id=
rJl5rRVFvH.
Nan Jiang and Jiawei Huang. Minimax conﬁdence interval for off-policy evaluation and policy
optimization. arXiv preprint arXiv:2002.02081, 2020.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In
International Conference on Machine Learning, pp. 652–661. PMLR, 2016.
Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,
University of London London, England, 2003.
Nathan Kallus and Masatoshi Uehara. Efﬁciently breaking the curse of horizon in off-policy evalu-
ation with double reinforcement learning. arXiv preprint arXiv:1909.05850, 2019.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efﬁcient off-policy evalu-
ation in markov decision processes. Journal of Machine Learning Research, 21(167):1–63, 2020.
Emilie Kaufmann, Olivier Capp´e, and Aur´elien Garivier. On the complexity of best-arm identiﬁ-
cation in multi-armed bandit models. The Journal of Machine Learning Research, 17(1):1–42,
2016.
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel : Model-
based ofﬂine reinforcement learning, 2020.
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Pac reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems, pp. 1840–1848, 2016.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. neural information processing systems, pp. 11761–
11771, 2019.
Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement
learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107–1149, 2003.
Romain Laroche, Paul Trichelair, and R´emi Tachet des Combes. Safe policy improvement with
baseline bootstrapping. In Proceedings of the 36th International Conference on Machine Learning
(ICML), 2019.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In
Artiﬁcial Intelligence and Statistics, pp. 608–616, 2015.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Inﬁnite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356–
5366, 2018.
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy
evaluation via conditional importance sampling. arXiv preprint arXiv:1910.06508, 2019.
11

Published as a conference paper at ICLR 2021
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Ofﬂine policy
evaluation across representations with applications to educational games. In AAMAS, pp. 1077–
1084, 2014.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit
problem. Journal of Machine Learning Research, 5(Jun):623–648, 2004.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.
Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning with
policy space generalization. arXiv preprint arXiv:2008.07353, 2020.
R´emi Munos. Error bounds for approximate policy iteration. In ICML, volume 3, pp. 560–567,
2003.
Remi Munos and Andrew W Moore. Barycentric interpolators for continuous space and time re-
inforcement learning. In Advances in neural information processing systems, pp. 1024–1030,
1999.
R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(May):815–857, 2008.
Dirk Ormoneit and ´Saunak Sen. Kernel-based reinforcement learning. Machine learning, 49(2-3):
161–178, 2002.
Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department
Faculty Publication Series, pp. 80, 2000.
St´ephane Ross and Drew Bagnell. Agnostic system identiﬁcation for model-based reinforcement
learning. In Proceedings of the 29th International Conference on Machine Learning, ICML 2012,
Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012.
Wen Sun, Arun Venkatraman, Geoffrey J Gordon, Byron Boots, and J Andrew Bagnell. Deeply ag-
grevated: Differentiable imitation learning for sequential prediction. In International Conference
on Machine Learning, pp. 3309–3318, 2017.
Csaba Szepesv´ari and R´emi Munos. Finite time bounds for sampling based ﬁtted value iteration. In
Proceedings of the 22nd international conference on Machine learning, pp. 880–887, 2005.
Philip Thomas and Emma Brunskill. Data-efﬁcient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139–2148, 2016.
Philip S. Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts, Amherst,
2014.
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-conﬁdence off-
policy evaluation. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence, 2015.
Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Stephen Giguere, Yuriy Brun, and
Emma Brunskill. Preventing undesirable behavior of intelligent machines. Science, 366(6468):
999–1004, 2019. ISSN 0036-8075. doi: 10.1126/science.aag3311.
Samuele Tosatto, Matteo Pirotta, Carlo d’Eramo, and Marcello Restelli. Boosted ﬁtted q-iteration.
In International Conference on Machine Learning, pp. 3434–3443. PMLR, 2017.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and Trends R⃝in
Machine Learning, 8(1-2):1–230, 2015.
J Tsitsiklis and B Van Roy. An analysis of temporal-difference learning with function approximation
(technical report lids-p-2322). Laboratory for Information and Decision Systems, 1996.
Masatoshi Uehara and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.
arXiv preprint arXiv:1910.12809, 2019.
12

Published as a conference paper at ICLR 2021
Benjamin Van Roy. Feature-based methods for large scale dynamic programming. PhD thesis,
Massachusetts Institute of Technology, 1994.
L. Wang, Wei Zhang, Xiaofeng He, and H. Zha. Supervised reinforcement learning with recurrent
neural network for dynamic treatment recommendation. Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, 2018.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2006.11274, 2020.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudık. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning, pp. 3589–3597. PMLR,
2017.
Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement learning,
2020. URL https://openreview.net/forum?id=BJg9hTNKPH.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. arXiv
preprint arXiv:2008.04990, 2020a.
Tengyang Xie and Nan Jiang. Q⋆approximation schemes for batch reinforcement learning: A
theoretical comparison. arXiv preprint arXiv:2003.03924, 2020b.
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang. Towards optimal off-policy evaluation for rein-
forcement learning with marginalized importance sampling. In Advances in Neural Information
Processing Systems, pp. 9668–9678, 2019.
C. Yu, G. Ren, and J. Liu. Deep inverse reinforcement learning for sepsis treatment. In 2019 IEEE
International Conference on Healthcare Informatics (ICHI), pp. 1–3, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. Limiting ex-
trapolation in linear approximate value iteration. In Advances in Neural Information Processing
Systems, pp. 5616–5625, 2019.
13

Published as a conference paper at ICLR 2021
A
PROOF OF LEMMA 4.2
Proof. We ﬁrst verify Qπ is linear for the ﬁrst H −1 levels. For each (h, c) ∈[H −1] × [ ˆd], we
have
Qπ
h(sc
h, a1) =R(sc
h, a1) + R(s
ˆd+1
h+1, a1) + R(s
ˆd+1
h+2, a1) + . . . + R(s
ˆd+1
H
, a1) = r0 · ˆd(H−h)/2.
Moreover, for all a ∈A,
Qπ
h(s
ˆd+1
h
, a) =R(s
ˆd+1
h
, a) + R(s
ˆd+1
h+1, a1) + R(s
ˆd+1
h+2, a1) + . . . + R(s
ˆd+1
H
, a1) = r0 · ˆd(H−h+1)/2.
Therefore, if we deﬁne
θπ
h =
ˆd
X
c=1
r0 · ˆd(H−h)/2 · ec +
ˆd
X
c=1
Qπ
h(sc
h, a2) · ec+ ˆd,
then Qπ
h(s, a) = (θπ
h)⊤φ(s, a) for all (s, a) ∈Sh × A.
Now we verify that the Q-function is linear for the last level. Clearly, for all c ∈[ ˆd] and a ∈A,
Qπ
H(sc
H, a) = r0 and Qπ
H(s
ˆd+1
H
, a) = r0 ·
p
ˆd. Thus by deﬁning θπ
H = Pd
c=1 r0 · ec, we have
Qπ
H(s, a) = (θπ
H)⊤φ(s, a) for all (s, a) ∈SH × A.
B
A TECHNICAL LEMMA
We need the following lemma in the proof of our hardness results.
Lemma B.1. Let α be a random variable uniformly distributed on {α+, α−}, where α−= 1/2 and
α+ = 1/2 + ε with 0 < ε < 1. Suppose that ξ1, ξ2, . . . , ξm are i.i.d. {+1, −1}-valued random
variables with Pr[ξi = +1] = α for all i ∈[m]. Let f be a function from {+1, −1}m to {α+, α−}.
Suppose m ≤C/ε2 log(1/δ) for some ﬁxed constant C. Then
Pr[f(ξ1, ξ2, . . . , ξm) ̸= α] > δ.
To our best knowledge, Lemma B.1 was ﬁrst proved in (Chernoff, 1972) and has enormous ap-
plications in statistical learning theory (see, e.g., Chapter 5 in (Anthony & Bartlett, 2009)) and
bandits (Mannor & Tsitsiklis, 2004).
To prove Lemma B.1, one can ﬁrst prove that the maximum likelihood estimator (MLE) is optimal
and then show that MLE requires Ω(1/ε2 log(1/δ)) samples to correctly output α with probability
1 −δ by anti-concentration. Lemma B.1 can also be proved by using information theory. See, e.g.,
(Kaufmann et al., 2016) for such a proof.
C
ANOTHER HARD INSTANCE
In this section, we present another hard case under a weaker version of Assumption 1. Here the
transition operator is stochastic and the reward is deterministic and sparse, meaning that the reward
value is non-zero only for the last level. Moreover, the data distributions {µh}H
h=1 are induced by a
ﬁxed policy πdata. We also illustrate the construction in Figure 2. Throughout this section, we use
d the denote the feature dimension, and we assume d is an even integer for simplicity. We use ˆd to
denote d/2 −1.
In this section, we adopt the following realizability assumption, which is a weaker version of As-
sumption 1.
Assumption 5 (Realizability). For the policy π : S →∆(A) to be evaluated, there exists
θ1, . . . θH ∈Rd such that for all (s, a) ∈S × A and h ∈[H],
Qπ
h(s, a) = θ⊤
h φ(s, a).
14

Published as a conference paper at ICLR 2021
…
…
…
𝑄𝑠!
", 𝑎= 𝑟# '𝑑(%&')/!
𝑄𝑠%&*
"
, 𝑎= 𝑟#
𝑃𝑠%
+ 𝑠%&*
"
, 𝑎) = (1 + 𝑟#)/2
𝑃𝑠%
& 𝑠%&*
"
, 𝑎) = (1 −𝑟#)/2
𝑄𝑠%
" , 𝑎= 0
𝑅𝑠%
+, 𝑎= 1
𝑅𝑠%
&, 𝑎= −1
𝑃𝑠'
+ 𝑠!
,-+*, 𝑎-) = (1 + 𝑟# '𝑑(%&!)/!)/2
𝑃𝑠'
& 𝑠!
,-+*, 𝑎-) = (1 −𝑟# '𝑑(%&!)/!)/2
𝑄𝑠!
,-+*, 𝑎- = 𝑟# '𝑑(%&!)/!
𝑃𝑠%
+ 𝑠%&*
,-+*, 𝑎-) = (1 + 𝑟# '𝑑*/!)/2
𝑃𝑠%
& 𝑠%&*
,-+*, 𝑎-) = (1 −𝑟# '𝑑*/!)/2
𝑄𝑠%&*
,-+*, 𝑎- = 𝑟# '𝑑*/!
𝑠%&*
,-+*
𝑠%&*
&
𝑠%&*
+
𝑠%&*
,-
𝑠%&*
!
𝑠%&*
*
⋯
⋯
⋯
⋯
⋯
…
𝑠%
*
𝑠%
!
𝑠%
,-
𝑠%
+
𝑠%
&
𝑠%
,-+*
𝑠!
!
𝑠!
*
𝑠!
,-+*
𝑠*
𝑠!
&
𝑠!
+
𝑠!
,-
⋯
𝑄𝑠*, 𝑎- = 𝑟# '𝑑(%&!)/!
𝑄𝑠.
", 𝑎= 𝑟# '𝑑(%&.&*)/!
𝑃𝑠.+*
+
𝑠.
,-+*, 𝑎-) = (1 + 𝑟# '𝑑(%&.)/!)/2
𝑃𝑠.+*
&
𝑠.
,-+*, 𝑎-) = (1 −𝑟# '𝑑(%&.)/!)/2
𝑄𝑠.
,-+*, 𝑎-
= 𝑟# '𝑑(%&.)/!
𝑎*, 𝑎!, … , 𝑎,-
𝑎,-+*, 𝑎,-+!, … , 𝑎-
Figure 2: An illustration of the hard instance. Recall that ˆd = d/2 −1. States on the top are those
in the ﬁrst level (h = 1), while states at the bottom are those in the last level (h = H). Dotted
line (with arrow) corresponds to transitions associated with actions a1, a2, . . . , a ˆd, while solid line
(with arrow) corresponds to transitions associated with actions a ˆd+1, a ˆd+2, . . . , ad. We omit the
transition associated with a1, a2, . . . , a ˆd in the ﬁgure if all actions give the same transition. For
each level h ∈[H], Q-values associated with s1
h, s2
h, . . . , s ˆd
h, s+
h , s−
h are marked on the left, while
transition distributions and Q-values associated with s
ˆd+1
h
are marked on the right. Rewards are all
deterministic, and the only two states (s+
H and s−
H) with non-zero reward values are marked in black
and grey. Consider the ﬁxed policy that returns ad for all input states. When r0 = 0, the value of the
policy is 0. When r0 = ˆd−(H−2)/2, the value of the policy is = r0 ˆd(H−2)/2 = 1.
State Space, Action Space and Transition Operator.
In this hard case, the action space A =
{a1, a2, . . . , ad} contains d elements. S1 contains a single state s1. For each h ≥2, Sh contains
ˆd + 3 states s1
h, s2
h, . . . , s ˆd
h, s
ˆd+1
h
, s+
h and s−
h .
Let 0 ≤r0 ≤ˆd−(H−2)/2 be a parameter to be determined. We ﬁrst deﬁne the transition operator for
the ﬁrst level. We have
P(s1, a) =









sc
2
a = ac, c ∈[ ˆd]
s+
2
a = a ˆd+1
s−
2
a = a ˆd+2
s
ˆd+1
2
a ∈

a ˆd+3, a ˆd+4, . . . , ad
	
.
Now we deﬁne the transition operator when h ∈{2, 3, . . . , H −2}. For each h ∈{2, 3, . . . , H −2},
a ∈A and c ∈[ ˆd], we have P(sc
h, a) = s
ˆd+1
h+1, P(s+
h , a) = s+
h+1 and P(s−
h , a) = s−
h+1. For each
h ∈{2, 3, . . . , H −2} and c ∈[ ˆd], we have P(s
ˆd+1
h
, ac) = sc
h+1. For all a ∈

a ˆd+1, a ˆd+2, . . . , ad
	
,
15

Published as a conference paper at ICLR 2021
we have
P

s
ˆd+1
h
, a

=
(
s+
h+1
with probability (1 + r0 · ˆd(H−h)/2)/2
s−
h+1
with probability (1 −r0 · ˆd(H−h)/2)/2 .
Now we deﬁne the transition operator for the second last level. For all c ∈[ ˆd] and a ∈A, we have
P(sc
H−1, a) =
s+
H
with probability (1 + r0)/2
s−
H
with probability (1 −r0)/2 .
For all a ∈A, we have P(s+
H−1, a) = s+
H and P(s−
H−1, a) = s−
H. For each c ∈[ ˆd], we have
P(s
ˆd+1
H−1, ac) = sc
H. For all a ∈

a ˆd+1, a ˆd+2, . . . , ad
	
, we have
P(s
ˆd+1
H−1, a) =



s+
H
with probability

1 + r0 ·
p
ˆd

/2
s−
H
with probability

1 −r0 ·
p
ˆd

/2
.
Reward Values.
In this hard case, all reward values are deterministic, and reward values can be
non-zero only for the last level. Formally, we have
R(s, a) =



1
s = s+
H
−1
s = s−
H
0
otherwise
.
Feature Mapping.
As in the in hard instance in Section 4, let e1, e2, . . . , ed be a set of of orthonor-
mal vectors in Rd. For the initial state, for each c ∈[d], we have φ(s1, ac) = ec.
Now we deﬁne the feature mapping when h ∈{2, 3, . . . , H}. For each h ∈{2, 3, . . . , H}, a ∈A
and c ∈[ ˆd], φ(sc
h, a) = ec, φ(s+
h , a) = e ˆd+1 and φ(s−
h , a) = e ˆd+2. Moreover, for all actions a ∈A,
φ(s
ˆd+1
h
, a) =
(
e ˆd+2+c
a = ac, c ∈[ ˆd]
1
ˆd1/2
 e1 + e2 + . . . + e ˆd

a ∈

a ˆd+1, a ˆd+2, . . . , ad
	 .
Clearly, for all (s, a) ∈S × A, ∥φ(s, a)∥2 ≤1.
Verifying Assumption 5.
Now we consider the deterministic policy π : S →A, which is deﬁned
to be π(s) = ad for all s ∈S. We show that Assumption 5 holds.
When h = 1, deﬁne
θ1 =
ˆd
X
c=1
r0 · ˆd(H−3)/2 · ec + e ˆd+1 −e ˆd+2 +
ˆd
X
c=1
r0 · ˆd(H−2)/2 · e ˆd+2+c.
For each h ∈{2, 3, . . . , H −2}, deﬁne
θh =
ˆd
X
c=1
r0 · ˆd(H−h−1)/2 · ec + e ˆd+1 −e ˆd+2 +
ˆd
X
c=1
r0 · ˆd(H−h−2)/2 · e ˆd+2+c.
For the second last level h = H −1, deﬁne
θH−1 =
ˆd
X
c=1
r0 · ec + e ˆd+1 −e ˆd+2.
Finally, for the last level h = H, deﬁne
θH = e ˆd+1 −e ˆd+2.
It can be veriﬁed that for each h ∈[H], Qπ
h(s, a) = θ⊤
h φ(s, a) for all (s, a) ∈Sh × A.
16

Published as a conference paper at ICLR 2021
The Data Distributions.
For the ﬁrst level, the data distribution µ1 is deﬁned to be the uniform
distribution over {(s1, ac) | c ∈[d]}. For each h ≥2, the data distribution µh is a uniform
distribution over
{(s1
h, a1), (s2
h, a1), . . . , (s
ˆd
h, a1), (s+
h , a1), (s−
h , a1), (s
ˆd+1
h
, a1), (s
ˆd+1
h
, a2), . . . , (s
ˆd+1
h
, a ˆd)}.
Notice that again (s
ˆd+1
h
, a) is not in the support of µh for all actions a ∈

a ˆd+1, a ˆd+2, . . . , ad
	
. It
can be seen that for all h ∈[H],
E(s,a)∼µh[φ(s, a)φ(s, a)⊤] = 1
d
d
X
c=1
ece⊤
c = 1
dI.
Moreover, by deﬁning
πdata(s) =













Uniform(A)
s = s1
a1
s ∈{sc
h | h ∈{2, 3, . . . , H}, c ∈[ ˆd]}
a1
s ∈{s+
h | h ∈{2, 3, . . . , H}}
a1
s ∈{s−
h | h ∈{2, 3, . . . , H}}
Uniform({a1, a2, . . . , a ˆd})
s ∈{s
ˆd+1
h
| h ∈{2, 3, . . . , H}}
,
we have µh = µπdata
h
for all h ∈[H].
The Lower Bound.
Now we show that it is information-theoretically hard for any algorithm to
distinguish the case r0 = 0 and r0 = ˆd−(H−2)/2 in the ofﬂine setting by taking samples from the
data distributions {µh}H
h=1. Here we consider the above policy π deﬁned above which returns action
ad for all input states. Notice that when r0 = 0, the value of the policy would be zero. On the other
hand, when r0 = ˆd−(H−2)/2, the value of the policy would be r0 · ˆd(H−2)/2 = 1. Therefore, if the
algorithm approximates the value of the policy up to an approximation error of 1/2, then it must
distinguish the case that r0 = 0 and r0 = ˆd−(H−2)/2.
We ﬁrst notice that for the case r0 = 0 and r0 = ˆd−(H−2)/2, the data distributions {µh}H
h=1, the
feature mapping φ : S × A →Rd, the policy π to be evaluated and the reward distributions R are
the same. Thus, in order to distinguish the case r0 = 0 and r0 = ˆd−(H−2)/2, the only way is to
query the transition operator P by using sampling taken from the data distributions.
Now, for all state-action pairs (s, a) in the support of the data distributions of the ﬁrst H −2 lev-
els (namely µ1, µ2, . . . , µH−2), the transition operator will be identical. This is because chang-
ing r0 only changes the transition distributions of (s
ˆd+1
h
, a ˆd+1), (s
ˆd+1
h
, a ˆd+2), . . . , (s
ˆd+1
h
, ad), and
such state-actions are not in the support of µh for all h ∈[H −2].
Moreover, for any
(s, a) ∈{s+
H−1, s−
H−1, s
ˆd+1
H−1} × A in the support of µH−1, P(s, a) will also be identical no mat-
ter r0 = 0 or r0 = ˆd−(H−2)/2. For those state-action pairs (s, a) in the support of µH−1 with
s /∈{s+
H−1, s−
H−1, s
ˆd+1
H−1}, we have
P(s, a) =
s+
H
with probability (1 + r0)/2
s−
H
with probability (1 −r0)/2 .
Again, this is because (s
ˆd+1
H−1, a) is not in the support of µH−1 for all a ∈

a ˆd+1, a ˆd+2, . . . , ad
	
.
Therefore, in order to distinguish the case r0 = 0 and r0 = ˆd−(H−2)/2, the agent needs distinguish
two transition distributions
p1 =
s+
H
with probability 1/2
s−
H
with probability 1/2
and
p2 =
(
s+
H
with probability (1 + ˆd−(H−2)/2)/2
s−
H
with probability (1 −ˆd−(H−2)/2)/2 .
Again, by Lemma B.1, in order to distinguish p1 and p2 with probability at least 0.9, one needs
Ω( ˆdH−2) samples. Formally, we have the following theorem.
17

Published as a conference paper at ICLR 2021
Theorem C.1. Suppose Assumption 2 holds, and rewards are deterministic and could be none-
zero only for state-action pairs in the last level. Fix an algorithm that takes as input both a policy
and a feature mapping. There exists an MDP satisfying Assumption 5, such that for a ﬁxed policy
π : S →A, the algorithm requires Ω((d/2−1)H/2) samples to output the value of π up to constant
additive approximation error with probability at least 0.9.
D
ANALYSIS OF ALGORITHM 1
D.1
PROOF OF LEMMA 5.1
Clearly,
ˆθh = ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + ˆVh+1(si
h))
!
= ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + ˆQh+1(si
h, π(si
h)))
!
= ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + φ(si
h, π(si
h))⊤ˆθh+1)
!
= ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + φ(si
h, π(si
h))⊤θh+1) +
N
X
i=1
φ(si
h, ai
h) · φ(si
h, π(si
h))⊤(ˆθh+1 −θh+1)
!
= ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + φ(si
h, π(si
h))⊤θh+1)
!
+ ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · φ(si
h, π(si
h))⊤(ˆθh+1 −θh+1)
!
.
For the ﬁrst term, we have
ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + φ(si
h, π(si
h))⊤θh+1)
!
=ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + Qπ(si
h, π(si
h)))
!
=ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (ri
h + V π(si
h))
!
=ˆΛ−1
h
 N
X
i=1
φ(si
h, ai
h) · (Qπ(si
h, ai
h) + ξi
h)
!
=ˆΛ−1
h
N
X
i=1
φ(si
h, ai
h) · ξi
h + ˆΛ−1
h
N
X
i=1
φ(si
h, ai
h) · φ(si
h, ai
h)⊤θh
=ˆΛ−1
h
N
X
i=1
φ(si
h, ai
h) · ξi
h + ˆΛ−1
h (Φ⊤
h Φh)θh
=ˆΛ−1
h Φhξh + θh −λˆΛ−1
h θh.
Therefore,
ˆθ1 −θ1 = (ˆΛ−1
1 Φ1ξ1 −λˆΛ−1
1 θ1) + ˆΛ−1
1 Φ⊤
1 Φ2(θ2 −ˆθ2)
= (ˆΛ−1
1 Φ1ξ1 −λˆΛ−1
1 θ1) + ˆΛ−1
1 Φ⊤
1 Φ2(ˆΛ−1
2 Φ⊤
2 ξ2 −λˆΛ−1
2 θ2)
+ ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3(θ3 −ˆθ3)
= . . .
=
H
X
h=1
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · (ˆΛ−1
h Φ⊤
h ξh −λˆΛ−1
h θh).
18

Published as a conference paper at ICLR 2021
Also note that
(Qπ(s1, π(s1)) −ˆQ(s1, π(s1)))2 = ∥θ1 −ˆθ1∥2
Λ1.
D.2
PROOF OF THEOREM 5.2
By matrix concentration inequality (Tropp, 2015), we have the following lemma.
Lemma D.1. For each h ∈[H], with probability 1 −δ/(4H), for some universal constant C, we
have

1
N Φ⊤
h Φh −Λh

2
≤C
p
d log(dH/δ)/N.
and

1
N Φh+1Φh+1 −Λh+1

2
≤C
p
d log(dH/δ)/N.
Therefore, since λ = CH
p
d log(dH/δ)N, with probability 1 −δ/(4H), we have
ˆΛh = Φ⊤
h Φh + λI ⪰NΛh.
Note that
(Qπ(s1, π(s1)) −ˆQ(s1, π(s1)))2
≤H ·
 H
X
h=1
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · (ˆΛ−1
h Φ⊤
h ξh −λˆΛ−1
h θh)

2
Λ1
!
≤2H ·
 H
X
h=1
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · ˆΛ−1
h Φ⊤
h ξh

2
Λ1 +
H
X
h=1
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · λˆΛ−1
h θh

2
Λ1
!
.
For each h ∈[H],
∥ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · ˆΛ−1
h Φ⊤
h ξh∥2
Λ1
≤∥Φ1ˆΛ−1
1 Λ1ˆΛ−1
1 Φ⊤
1 ∥2 · ∥Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · ˆΛ−1
h Φ⊤
h ξh∥2
2
≤∥ˆΛ−1/2
1
Λ1ˆΛ−1/2
1
∥2 · ∥Φ1ˆΛ−1
1 Φ⊤
1 ∥2 · ∥Φ2ˆΛ−1
2 Φ⊤
2 Φ2 · · · ˆΛ−1
h Φ⊤
h ξh∥2
2
≤∥ˆΛ−1/2
1
Λ1ˆΛ−1/2
1
∥2 ·
h−1
Y
h′=1

∥Φh′ ˆΛ−1
h′ Φ⊤
h′∥2 · ∥ˆΛ−1/2
h′+1(Φ
⊤
h′+1Φh′+1)ˆΛ−1/2
h′+1∥2

· ∥ξh∥2
Φh ˆΛ−1
h Φ⊤
h .
Similarly,
∥ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · λˆΛ−1
h θh∥2
Λ1
≤∥ˆΛ−1/2
1
Λ1ˆΛ−1/2
1
∥2 ·
h−1
Y
h′=1

∥Φh′ ˆΛ−1
h′ Φ⊤
h′∥2 · ∥ˆΛ−1/2
h′+1(Φ
⊤
h′+1Φh′+1)ˆΛ−1/2
h+1 ∥2

· λ2 · ∥θh∥2
ˆΛ−1
h
≤∥ˆΛ−1/2
1
Λ1ˆΛ−1/2
1
∥2 ·
h−1
Y
h′=1

∥Φh′ ˆΛ−1
h′ Φ⊤
h′∥2 · ∥ˆΛ−1/2
h′+1(Φ
⊤
h′+1Φh′+1)ˆΛ−1/2
h′+1∥2

· λ · H2d.
For all h ∈[H], we have
∥ΦhˆΛ−1
h Φ⊤
h ∥2 ≤1
and
∥ˆΛ−1/2
h
(Φ
⊤
h Φh)ˆΛ−1/2
h
∥2 ≤∥N ˆΛ−1/2
h
ΛhˆΛ−1/2
h
∥2 + ∥ˆΛ−1/2
h
(Φ
⊤
h Φh −NΛh)ˆΛ−1/2
h
∥2.
Conditioned on the event in Lemma D.1,
ˆΛh ⪰NΛh ⪰N
Ch
Λh,
19

Published as a conference paper at ICLR 2021
which implies ∥N ˆΛ−1/2
h
ΛhˆΛ−1/2
h
∥≤Ch. Moreover, conditioned on the event in Lemma D.1,
∥ˆΛ−1/2
h
(Φ
⊤
h Φh −NΛh)ˆΛ−1/2
h
∥2 ≤C
p
d log(dH/δ)N/λ.
Thus,
∥ˆΛ−1/2
1
Λ1ˆΛ−1/2
1
∥2 ≤C1/N.
and
∥ˆΛ−1/2
h
(Φ
⊤
h Φh)ˆΛ−1/2
h
∥2 ≤Ch + C
p
d log(dH/δ)N/λ.
Finally, by Theorem 1.2 in (Hsu et al., 2012a), with probability 1 −δ/(4H), for some constant C′,
we have
∥ξh∥2
Φh ˆΛ−1
h Φ⊤
h ≤C′H2d log(H/δ).
Therefore,
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · ˆΛ−1
h Φ⊤
h ξh

2
Λ1 +
ˆΛ−1
1 Φ⊤
1 Φ2ˆΛ−1
2 Φ⊤
2 Φ3 · · · λˆΛ−1
h θh

2
Λ1
≤C1
N (C2 + C
p
d log(d/δ)N/λ) × · · · × (Ch + C
p
d log(d/δ)N/λ) × (C′H2d log(H/δ) + λH2d)
≤C1
N (C2 + 1/H) × · · · × (Ch + 1/H) × (C′H2d log(H/δ) + λH2d)
≤e
N C1 × C2 × · · · × Ch × (C′H2d log(H/δ) + CdH3p
d log(dH/δ)N).
Let c > 0 be a large enough constant. We now have
Es1[(Qπ
1(s1, π(s1)) −ˆQ1(s1, π(s1)))2] ≤c ·
 H
Y
h=1
Ch
!
· dH5 ·
r
d log(dH/δ)
N
.
20

