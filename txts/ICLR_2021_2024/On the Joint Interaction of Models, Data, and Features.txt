Published as a conference paper at ICLR 2024
ON THE JOINT INTERACTION OF MODELS, DATA, AND
FEATURES
Yiding Jiang
Carnegie Mellon University
yidingji@cs.cmu.edu
Christina Baek
Carnegie Mellon University
kbaek@cs.cmu.edu
J. Zico Kolter
Carnegie Mellon University
Bosch Center for AI
zkolter@cs.cmu.edu
ABSTRACT
Learning features from data is one of the defining characteristics of deep learning,
but the theoretical understanding of the role features play in deep learning is still in
early development. To address this gap, we introduce a new tool, the interaction
tensor, for empirically analyzing the interaction between data and model through
features. With the interaction tensor, we make several key observations about
how features are distributed in data and how models with different random seeds
learn different features. Based on these observations, we propose a conceptual
framework for feature learning. Under this framework, the expected accuracy for a
single hypothesis and agreement for a pair of hypotheses can both be derived in
closed form. We demonstrate that the proposed framework can explain empirically
observed phenomena, including the recently discovered Generalization Disagree-
ment Equality (GDE) that allows for estimating the generalization error with only
unlabeled data. Further, our theory also provides explicit construction of natural
data distributions that break the GDE. Thus, we believe this work provides valuable
new insight into our understanding of feature learning.
1
INTRODUCTION
It is commonly said that deep learning performs feature learning, whereby the models extract useful
patterns from the data and use the patterns to make predictions. Most successful applications of deep
learning today involve first training the models on a large amount of data and then fine-tuning the
pre-trained model on downstream tasks (Chen et al., 2020; Brown et al., 2020; Radford et al., 2021).
Their success suggests the models are learning useful and transferable knowledge from the data that
allows them to solve similar tasks more efficiently. Experimentally, many different works (Nguyen
et al., 2016; Zeiler and Fergus, 2014; Bau et al., 2017; Olah et al., 2017; 2018; Li et al., 2015)
have studied various aspects of the features learned by deep neural networks. These works help
the community gain a better intuitive understanding of the mechanisms underpinning deep learning
as well as improve the interpretability of deep models. However, to the best of our knowledge,
the theoretical understanding of the role features play in deep learning is still under-explored. For
example, the popular neural tangent kernel (NTK) (Jacot et al., 2018) framework analyzes deep
learning as kernel regression with features defined by the model’s gradient at initialization, which is
independent of the training data.
While it may be intuitive to think of defining features as quantifying the “unit” of information in
data that models use to make predictions, the community has yet to reach a consensus on the exact
definition of features in deep learning beyond toy models. Nonetheless, it is undeniable that the models
have learned something from the data. In fact, the same models trained with different random seeds
would learn different information that leads to different predictions (Lakshminarayanan et al., 2017).
This phenomenon has important downstream consequences for ensembling randomly initialized
networks including better generalization (Allen-Zhu and Li, 2020), calibration (Lakshminarayanan
et al., 2017), and the Generalization Disagreement Equality (GDE) (Jiang et al., 2022) where the
expected test accuracy is equal to the expected agreement in deep ensembles.
We postulate that a good definition of features should be fine-grained enough to differentiate the
difference in the knowledge of different models. To this end, we attempt to define such a construct
that allows us to analyze the similarities and differences of information learned by different models,
while also remaining amenable to quantitative and theoretical analysis. Through this definition, we
1

Published as a conference paper at ICLR 2024
Least features
Most features
Airplane
Car
Cat
Dog
Horse
Figure 1: Visualization of images with the least features (left) and the most features (right) for classes
of CIFAR 10 under our feature definition (defined in Section 3.1). Each row corresponds to one class
of CIFAR 10 (zoom in for better viewing quality). We can see that the images with the least features
are semantically similar to each other whereas the images with the most features are much more
diverse and contain unusual instances of the class or objects in rare viewpoints. More examples for
all classes can be found in Figure 12 of the appendix.
can gain insights into the behavior of models and the underlying mechanisms of feature learning.
Notably, we show that GDE arises immediately as a consequence of how neural networks learn
appropriately defined features. This phenomenon was previously explained by assuming calibration
of the underlying ensemble, which is often a strong assumption to make.
We begin with an empirical investigation of feature learning, using a natural definition of features on
real data (Figure 1) that allows us to easily compare information about the data distribution learned
by different models and a construction we propose called the interaction tensor. We define features
to be the projection onto the principal components of different models’ last-layer activation. The
interaction tensor then jointly models the features learned by multiple models and across multiple
data points. Inspecting this tensor constructed on collections of models, we find that the model of data
presented in Allen-Zhu and Li (2020) is not conceptually reflective of the actual observed phenomena,
suggesting that an alternative model is needed.
Based on these observations, we propose an alternative (still simplified) model of feature learning,
which better captures the above phenomenon. Specifically, we posit a framework where features
come in two types: dominant (more frequent) and rare (less frequent), which captures the observed
heavy-tailed nature of features. We also assume that data points either contain a small number
of dominant features or a large number of rare features and that models learn features according
to their frequency in the data set; this captures the observed phenomenon where data points with
fewer features receive higher-confidence predictions. Under this model, we can analytically derive
expressions for the accuracy and agreement of resulting classifiers. Despite the simplification, we
show that our framework naturally captures the phenomenon of GDE without assuming calibration.
Instead, we show that GDE arises immediately as a consequence of the distributional properties of
features in natural data. Finally, we demonstrate that the framework can make accurate predictions
about the effects of merging classes and changing data distribution on GDE and calibration, leading
to the construction of natural data distributions that break GDE.
Thus, we believe overall that our framework of feature learning shows promise as an additional
useful and valuable conceptual tool in understanding how deep learning works. Note that we do
not attempt to derive how our model can arise mechanistically via optimization, but we believe that
this can be considered a strength of our approach. Similar to natural sciences and econometrics, the
empirical phenomena of deep learning can be phenomenologically understood “on their own terms”
at many different layers of abstraction, and our model provides one such formalism that comports
with observed behavior at the level of observed feature learning phenomena.
2
RELATED WORKS
Feature learning.
Representation learning (Bengio et al., 2013) is the practice of discovering
useful features from raw data directly instead of using hand-crafted features. Deep learning is the
de facto approach for learning features from a large amount of data (Chen et al., 2020; Brown et al.,
2

Published as a conference paper at ICLR 2024
2020; Radford et al., 2021), but a formal understanding of the process is still lacking. Recent works
have started to incorporate feature learning into theoretical analysis (Li et al., 2019; Allen-Zhu and
Li, 2020; Yang and Hu, 2021; Karp et al., 2021; Wen and Li, 2021; Allen-Zhu and Li, 2022; Ba
et al., 2022). This paper is most immediately related to Allen-Zhu and Li (2020) which proposes the
multi-view data structure where there exist two types of data: multi-view data which contain all the
features of a class and single-view data which contain only one feature. In this work, we investigate
whether this structure of features holds in practice by treating features as first-class citizens in both
empirical investigation and theoretical analysis. Our experimental results reveal a more nuanced
perspective on the structure of data and “features”. Based on these observations, we propose an
alternative conceptual model that better reflects how features, data, and models behave in reality.
We analyze the generalization property of the model and also its agreement property (Nakkiran and
Bansal, 2020). We also show that this feature learning model provides an alternative condition under
which the curious GDE phenomena observed in Jiang et al. (2022) can arise.
Ensemble and Generalization Disagreement Equality.
Deep ensembles (Lakshminarayanan
et al., 2017), trained with different initializations on the same dataset, have demonstrated superior
performance over classical methods such as bagging and Bayesian approaches (Breiman, 1996;
Welling and Teh, 2011; Gal and Ghahramani, 2016). One of the notable attributes of deep ensembles
is their well-calibrated nature — making predictions with appropriate confidence levels (Murphy
and Epstein, 1967). Jiang et al. (2022) explored the implications of this calibration by showing
that test accuracy can be estimated using the agreement of two independent models (Nakkiran and
Bansal, 2020), a phenomenon that the authors called the Generalization Disagreement Equality
(GDE). However, calibration is a strong assumption – why deep ensembles are calibrated is largely
an open problem (Fort et al., 2019). To address this gap, this work shows that the phenomenon of
GDE can arise within a feature learning context, without making explicit calibration assumptions.
Understanding representations.
One line of work tries to understand deep learning with a more
empirical approach. Many try to understand the features by visualizing what aspects of the input
data they correspond to (Nguyen et al., 2016; Zeiler and Fergus, 2014; Bau et al., 2017; Olah et al.,
2017; 2018). Another line of work attempts to compare representations of different models (Li et al.,
2015; Raghu et al., 2017; Morcos et al., 2018; Kornblith et al., 2019). We demonstrate that simple
PCA can reduce the redundancies in high-dimensional representation and find a parsimonious set of
features that the model relies on to make predictions. Our feature clustering algorithm may be seen
as a generalization of pair-wise matching proposed by Li et al. (2015).
3
CONNECTING MODELS AND DATA WITH FEATURES
In this section, we describe the procedure for constructing the interaction tensor Ω∈{0, 1}M×N×T .
The first axis corresponds to M models, the second axis corresponds to N data, and the last axis
corresponds to T features. If the nth data point contains the tth feature and the mth model has learned
the T th feature, then Ωmnt would be 1. This tensor captures how features are distributed (see Figure 6
for an illustration). First, we identify features within each model of ensembles. Then, we cluster the
features and construct the interaction tensor with the identified feature clusters.
Notations.
Let x denote a point in X, the input space, and y ∈[C] denote the label, where [C]
is the set of labels, [1, 2, . . . , C]. Let D be the data distribution over X × [C]. We use (x, y) to
denote samples from the random variable following D. Let f : X →[C] be a parameterized
function, f(x) = (ψ ◦φ)(x). φ : X →Rd maps the input x to a d-dimensional representation, and
ψ : Rd →[C] maps the representation to a class. For a collection of models {f1, . . . , fM}, we use
confidence to denote their average one-hot predictions for the ground truth label.
3.1
PRINCIPAL COMPONENTS OF ACTIVATION AS FEATURES
Many works (Li et al., 2015; Olah et al., 2017) study the representations of deep neural networks by
treating individual neurons as features or the most elementary unit of the representation, but these rep-
resentations are high-dimensional vectors, which contain redundant information. Furthermore, a sin-
gle feature may be distributed across multiple neurons. Instead, ideal features should be parsimonious
and can capture the dependencies between different coordinates of the representations. These criteria
can be fulfilled by dimensionality reduction methods. We choose principal component analysis (PCA)
3

Published as a conference paper at ICLR 2024
which captures the linear dependencies between different coordinates of the representations and can
be efficiently computed with stochastic algorithms (similar to H¨ark¨onen et al. (2020)). While this pro-
cedure can be applied to any layer, we use the last layer representation to avoid non-linear interaction
through superposition (Elhage et al., 2022). Concretely, given a neural network and a set of data points
X =

x(1), x(2), . . . , x(N)⊤, we use Φ ∈RN×d be the matrix that contains all of φ
 x(i)
as its
rows. The singular value decomposition (SVD) yields Φ = UΣV⊤where the columns of V ∈Rd×d
contain the principal components of Φ. We use the top K principal components V:K and project the
representations to RK, Φproj ≜ΦV:K =

V⊤
:Kφ
 x(1)
, V⊤
:Kφ
 x(2)
, . . . , V⊤
:Kφ
 x(N)⊤. For
notation simplicity, we will use υ(x) to denote V⊤
:Kφ (x) and υm,k(x) to denote the kth entry of of
the mth model’s υ(x). Intuitively, we can interpret the principal components as a feature, or more
concretely, orthogonal subspaces that the model uses to classify any given data points in X.
3.2
CONSTRUCTING THE INTERACTION TENSOR
Clustering features of different models.
Given M models, {f1, f2, . . . , fM}, we can compute the
projected representation for each network of the M models,
n
Φproj
1 , Φproj
2 , . . . , Φproj
M
o
. For a single
model fm and its kth feature, we can compute its mean and variance:
µm,k ≜E(x,y)∼D [υm,k(x)] , σ2
m,k ≜E(x,y)∼D
h
(υm,k(x) −µm,k)2i
.
(1)
For models (fi, fj) and their respective ath and bth features, we can define their correlation to be:
ρ(i,j),(a,b) ≜E(x,y)∼D [(υi,a(x) −µi,a) (υj,b(x) −µj,b)] (σi,a σj,b)−1.
(2)
This can be seen as performing the procedure of Li et al. (2015) with PCA projected representations.
We use Ki,j ∈[−1, 1]K×K to denote the collection of all pair-wise correlation values between the
features of fi and fj, and Λ ∈[−1, 1]M×M×K×K to denote the collection of all the correlation
matrices between every pair of models. A centralized list of notations is provided in Appendix A.
With Λ, we can identify unique feature clusters in the MK features learned by all models. To
account for the arbitrary direction of correlation in Λ, we take the absolute value and use a threshold,
γcorr ∈(0, 1), to determine whether two features should be considered as the same feature. We use
a greedy clustering algorithm (Algorithm 1) to match the features with one another, as k-partite
matching is known to be NP-complete for k > 2 (Garey and Johnson, 1979). After running the
clustering algorithm, each feature is assigned to one of T clusters (where T ≤MK), and we treat
every feature in a single cluster as the same feature. The greedy algorithm is efficient and does
not generate a fixed number of clusters, which is desirable in cases where some features have low
correlations with other features and should be isolated as a unique cluster. More sophisticated
algorithms, such as graph cut, could be used, but we find the greedy algorithm sufficient for our
purposes. See Algorithm 1 and Appendix E.2 for details on the algorithm and hyperparameters.
Matching features to data points
Once the features of all models are clustered, we can identify
which features are present in each data point of X. First, we normalize each individual υm,k by its
ℓ∞-norm, which ensures that all features are between 0 and 1. We will denote the row-normalized
Φproj
m as bΦproj
m . We then pick another threshold, γdata ∈(0, 1), that decides whether a feature is present
in a data point. Concretely, if the kth entry of the nth row in bΦproj
m is larger than γdata, we assign to the
nth data point in X the feature cluster containing the mth model’s kth feature. In other words, if the
mth model’s kth feature belongs to the tth cluster, we say the nth data point contains the tth feature. In
Figure 1, we visualize the data points with the most and least number of features.
Aggregating Information.
After thresholding, we have enough information to construct the inter-
action tensor. Each entry indicates whether the tth feature is present in both the mth model and nth
data point. In the next section, we will inspect various aspects of the interaction tensor Ωand other
experimental artifacts to understand how the models learn features from the data.
4
EXPERIMENTS AND OBSERVATIONS
Experimental setup.
We use a collection of M = 20 ResNet18 (He et al., 2016a) trained on
the CIFAR-10 dataset (Krizhevsky et al., 2009) following the experimental set up of Jiang et al.
4

Published as a conference paper at ICLR 2024
0
1
2
3
4
5
6
7
8
feature (1e2)
1e2
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
3.8
occurrences (base 10)
feature frequency over training
Init
ep 1
ep 5
ep 9
ep 13
ep 17
ep 21
ep 25
ep 29
(a)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
0
1
2
3
4
5
6
7
occurrences (1e3)
1e3
feature frequency (45k)
(b)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
4.0
3.5
3.0
2.5
2.0
log prob (base 10)
log feature density by confidence (45k)
high confidence
low confidence
(c)
Figure 2: (a) Feature frequency over the course of training. The red curve represents the feature
frequency at the initialization. (b) Features vs how often they appear in the dataset. The features are
sorted by frequency and the distribution appears to be long-tailed. (c) Feature frequency by different
confidence levels. Low-confidence data tend to have more low-frequency features.
0.0
0.2
0.4
0.6
0.8
1.0
confidence
0
25
50
75
100
125
150
number of features
conf vs. feature count (45k)
(a)
0
1
2
3
4
5
6
7
# of data with a feature (1e3)
1e3
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
# of models with a feature
n data vs. n models w/ feat (45k)
(b)
4
6
8
10
12
14
shared features
0.36
0.37
0.38
0.39
0.40
0.41
0.42
0.43
shared error
shared feature vs. shared error (45k)
(c)
Figure 3: (a) Density estimation of confidence vs number of features over data. High-confidence data
tend to have fewer features. (b) Scatter plot of the number of data with a feature vs. the number of
models with that feature. The strong positive correlation suggests that the more data has a certain
feature, the more likely the model will learn that feature. (c) Number of shared features vs shared
error. The lower bound of shared error monotonically increases with the number of shared features.
(2022). The models are trained on random 45000 subsets of the whole training set (45k). In
addition, we repeat the experiments on CIFAR-10 using random 10000 subsets (10k) and the SVHN
dataset (Netzer et al., 2011) using random 45000 subsets of the whole training set (SVHN). We
compute Λ and Ωon the test set using the output of the penultimate layer (i.e., ψ is the final linear
layer). The experimental details can be found in Appendix E. We primarily show 45k here and leave
the 10k and SVHN results to Appendix F, both of which are similar to the 45k results qualitatively.
Observation 1 : Feature frequency is long-tailed.
(O.1) We use the interaction tensor Ωto
compute the frequency of each of the T features in the dataset. First, we sum Ωover the model
axis and then clip the values to 1 (since we are only interested in the relation between data and
features here). Then, we sum over the data axis to obtain the number of data points that contain
each individual feature. The features are sorted by their frequencies, which reveals a long-tailed
distribution (Figure 2b). Furthermore, this distribution is a consequence of learning. In Figure 2a,
we compare the feature frequency computed from 20 models over the first 30 epochs of training
(i.e., 600 checkpoints) with the feature distribution for untrained models. We observe that untrained
models have much higher frequencies for tail features (low frequency). After even a single epoch, the
frequency of tail features decreases significantly and then stays relatively stable over training. At the
head of the distribution, the models first learn a large number of features and then prune the features
as training continues, eventually converging to a fixed distribution with a small number of features.
Observation 2: The ensemble tends to be more confident on data points with fewer features,
and data points with lower confidence tend to have more features with low density.
(O.2)
Another question that we want to understand is how the features interact with the confidence of the
ensemble. In Figure 3a, we show the joint density plot of the ensemble’s confidence for a data point
and the number of features the data point has for 45k. We can see that low-confidence data tend to
have more features, whereas high-confidence data points tend to have fewer features. This finding
5

Published as a conference paper at ICLR 2024
contradicts the model of Allen-Zhu and Li (2020) in which if a data point is multi-view (i.e., contains
more features), all members of the ensembles will classify it correctly (i.e., high confidence). A
plausible explanation for this observation is that there is a small sub-population of features that are
learned by a large number of models. Based on O.1, we postulate that these features are learned by
more models because they appear with higher probability. Furthermore, we plot the log density of
features1 in high-confidence data and all other data points (low-confidence) in Figure 2c. We see that
the low-confidence data tend to have more features with low density in Figure 2b. One explanation is
that the features in the tail are responsible for different models making different predictions.
Observation 3: Number of models with a certain feature is positively correlated with the
feature’s frequency.
(O.3) The interaction tensor also reveals how the number of data points
containing a certain feature relates to the number of models that have learned that feature. This
relationship can shed light on how models learn features of different frequencies in practice. In
Figure 3b, we observe that the number of models with any given feature has a strong positive
correlation with the frequency of that feature appearing in the data (linear / super-linear). This implies
that the more a feature appears in the data, the more likely a model will pick it up. We hypothesize
that the feature learning procedure can be phenomenologically approximated by a sampling process
where the probability of learning a feature is related to how often that feature appears in the data.
Observation 4: Models with similar features make similar mistakes.
(O.4) Another natural
hypothesis is that models with similar features should make similar mistakes. For every pair of
models, we compare how many features they share to how often they make the same mistake relative
to the average number of mistakes both models make (Figure 3c). We can see that the lower bound of
shared error is almost monotonically increasing with the value of shared features. On the other hand,
when models share a smaller number of features, the shared errors have a much larger variance, which
indicates their errors are less correlated and therefore more random. This effect is more amplified for
different architectures. We show this result on more than 20 diverse architectures in Appendix F.6.
Finally, we provide more analysis on the effect of using PCA for clustering in Appendix F.2 and
different hyperparameters for the interaction tensor in Appendix F.10. We also explore the properties
of features found under our definition in Figure 1 and Appendix F.3. In Appendix F.11, we investigate
the feature distributions at different depths and discuss the interpretation of these features.
5
A COMBINATORIAL FRAMEWORK OF FEATURE LEARNING
In this section, we present a new framework of feature learning for a binary classification based on
the insights from the experiments. We saw in O.1 that the distribution of features is long-tailed. This
means a relatively small number of unique features constitute a large proportion of all the features in
the data. To facilitate analysis, we will assume there are two types of features, dominant features and
rare features, where the dominant features appear with much higher probability than rare features.
Further, we observed that data with high confidence tend to have much fewer features than the ones
with high confidence (O.2). To model this behavior, we will assume that there are two types of data
points: dominant data and rare data. The former contains a small number of dominant features and
the latter contains a larger number of rare features. This is another simplification based on O.2, which
shows that high-confidence data tend to have fewer high-frequency features.
Definitions and additional notations.
Before describing the full model, we first define the pa-
rameters of the model as well as some additional notations: pd: the proportion of all data that are
dominant, pr: the proportion of all data that are rare (this parameter is equal to 1 −pd), c: the total
model capacity which represents how many features a single model can learn, td: the total number of
dominant features available in the data for one class, tr: the total number of rare features available in
the data for one class, nd: the total number of dominant features a single dominant data point has
(nd ≤td), and nr: the total number of rare features a single rare data point has (nr ≤tr). We will
use Ψ(·) to denote the set of all features a model or a feature has (an expanded list in Appendix A).
Data generating process.
We can see the data generating process as the following sampling
procedure. First, we decide which class the data point belongs to. We consider a class-balanced
1The features are plotted in the same order as Figure 2c (hence the jaggedness), and the density is obtained
by normalizing with the total number of features in both high confidence and low confidence groups of data.
6

Published as a conference paper at ICLR 2024
binary classification problem so each class occurs with an equal probability of 1
2. Then, we decide
whether the data point is dominant or rare. This is equivalent to sampling from a Bernoulli distri-
bution, Ber(pd). If the data point is dominant, we sample nd dominant features uniformly without
replacement. Vice versa, if the data point is rare, we sample nr dominant features uniformly without
replacement. It is easy to verify that the proportion of dominant data points and features is pd and the
proportion of rare data points and features is pr.
How the models learn.
We saw in O.3 that the frequency of features occurring in different models
is positively correlated with the frequency at which the features occur in the data. We can model the
learning process as another sampling-without-replacement process where the probability that a model
learns a feature is proportional to the frequency at which the feature occurs in the data. Under this
assumption, in expectation, cd = 1
2pdc of the features in a single model would be dominant features
for a single class, and cr = 1
2prc of the features for a single class would be rare. We can further
simplify this process by assuming that the model will always sample cd dominant features for each
class and cr rare features for each class2.
How the models make predictions.
For a data point x and a model f, we assume that the
model will correctly classify x if the overlap between the features of x and the features of f is not
empty (similar to assumptions in Allen-Zhu and Li (2020)). Otherwise, the model will perform
a random guess. The expected error that a single model f makes on a single datum pair (x, y) is
thus err(f, x, y) = 1
21 {Ψ(f) ∩Ψ(x) = ∅}. Further, given a pair of models (f, g) and a single
datum pair (x, y), there are three distinct behaviors for how they will make predictions. (1) The
two models will always agree with each other if both of them share feature with x, since both will
classify x correctly (i.e., if |Ψ(f) ∩Ψ(x)| > 0 and |Ψ(g) ∩Ψ(x)| > 0). (2) If the models both do not
share any features with x, then by the previous assumptions, the models will make random guesses
(see Appendix D.2 for why this is justified); however, if the models share features with each other,
their random guesses will not be independent of each other (O.4). We hypothesize that how two
models agree with each other is a function of k, the number of features they share, and c, the model
capacity. We capture this intuition with an agreement function, ζ : N × N →[0, 1], which returns
the probability that two models will agree based on how many features they share relative to the full
model capacity. This function is crucial for understanding how models make mistakes. (3) Finally, if
the models do not share any features with each other or with x, both models will perform independent
random guesses, in which case they will agree 50% of the time.
It is natural to ask how reasonable the simplifications are. In Appendix D, we discuss these simpli-
fications (e.g., random guess and number of classes) in detail and provide a comparison between
this framework and Allen-Zhu and Li (2020). We encourage interested readers to read this section.
Still, we will see that this relatively simplified model readily offers interesting insights into observed
phenomena and can make surprisingly accurate predictions about the results of experiments a priori.
5.1
ANALYTICAL FORMS OF ACCURACY AND AGREEMENT
Using this model, the closed-form form of expected accuracy, Acc, and expected agreement rates,
Agr, can be derived through combinatorics. All propositions are proven in Appendix C.
Proposition 5.1. The expected accuracy over the model distribution and data distribution is:
Acc = pd


1 −
1
2
td−cd
nd

 td
nd



+ pr

1 −
1
2
tr−cr
nr

 tr
nr


.
(3)
Proposition 5.2. Let
 n
r

= 0 when n < 0, r < 0 or n < r, and let:
q1 = pd


1 −
td−cd
nd

 td
nd




2
+ pr

1 −
tr−cr
nr

 tr
nr



2
,
q2(k) = pd
td−nd
cd
2
td
cd
2



X
a+b=k
cd
a
td−nd−cd
cd−a

td−nd
cd

cr
b
tr−cr
cr−b

tr
cr



+ pr
tr−nr
cr
2
tr
cr
2



X
a+b=k
cd
a
td−cd
cd−a

td
cd

cr
b
tr−nr−cr
cr−b

tr−nr
cr



,
then the expected agreement between an i.i.d pair (f, g) drawn for the model distribution is:
Agr =
1
2
+
1
2
q1 +
c
X
k=1

ζ(k, c) −
1
2

q2(k).
(4)
2Both cr and cd are rounded to the nearest integer such that the total number of features in a model is still c.
7

Published as a conference paper at ICLR 2024
0.2
0.4
0.6
0.8
1.0
pd
0.5
0.6
0.7
0.8
0.9
1.0
ablating pd
accuracy
agreement
50
100
150
200
250
td
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
ablating td
100
200
300
400
500
600
tr
0.89
0.90
0.91
0.92
0.93
0.94
ablating tr
100
200
300
400
500
tr
0.65
0.70
0.75
0.80
0.85
0.90
0.95
ablating tr
td
5
10 15 20 25 30 35 40 45
c
0.65
0.70
0.75
0.80
0.85
0.90
0.95
ablating c
2
4
6
8
10
12
nd
0.65
0.70
0.75
0.80
0.85
0.90
0.95
ablating nd
0
10
20
30
40
50
nr
0.88
0.90
0.92
0.94
0.96
ablating nr
0
10
20
30
40
50
nr
0.5
0.6
0.7
0.8
0.9
1.0
ablating nr
nd
Figure 4: Numerical simulations using the analytical forms of accuracy and agreement. Each plot in
the left three columns ablates a single parameter in the framework. The right column shows the effect
of coupling two parameters, namely (tr, td) and (nr, nd). Changing tr and nr alone deviates from
GDE, but if they are coupled with dominant features, we can recover GDE (approximately).
Further, we discuss irreducible error and other properties of this framework in Appendix B and
discuss a potential connection between feature learning and data scaling (Hestness et al., 2017).
5.2
NUMERICAL SIMULATION
We now study the properties of the analytical forms of the expected agreement and the expected
accuracy. Instead of bounding their difference, we will use numerical simulation to characterize
their properties and differences. The model has 6 free parameters, namely, pd, c, td, tr, nd, nr. We
first pick a set of initial values and then vary each value to study the behavior of the model. Unless
specified otherwise, the initial values used for all simulations are pd = 0.7, c = 20, td = 20, tr =
180, nd = 5, nr = 10. Further, we pick ζ(k, c) = 0.9 1{k > 0}. This reflects the intuition that if
two models share any features, then with high probability they would agree with each other. This
is reasonable if we assume all the models in the same hypothesis distribution are naturally more
inclined to agree with each other (O.4 and Figure 3c show that for a 10-class classification the shared
error is at least 35%). We study the properties and effects of ζ in Appendix F.7.
In the left 3 columns of Figure 4, we vary each parameter of the framework over a wide range of
values. We observe that for both pd and c, the agreement closely tracks each other for a large portion
of the parameter values. This suggests that the difference between generalization error and agreement
is robust to how much of the data has dominant features and the size of the model. Further, we
observe both accuracy and agreement saturate as the model capacity, c, increases. This is equivalent
to increasing the model capacity with an infinite amount of training data. This is consistent with
prior works on model scaling (Tan and Le, 2019) which suggests that model size may be related
to how many features the model can learn. On the other hand, the behaviors of agreement and
accuracy appear to be more sensitive to the other parameters that describe the relationship between
total numbers of existing features and how often these features appear in a single data point, in
particular tr and nr, quantities that govern the distribution of rare data. Intuitively, the rare features
and data represent the part of data distribution that appears in the tail of the data distribution and
require memorization to learn (Feldman, 2020).
The observations about nr and tr suggest that GDE requires some distributional assumptions on
the features and data in our framework (also in Jiang et al. (2022)). One possible hypothesis is
that the relationship between tr and td and the relationship between nr and nd follow the Pareto
principle (Pareto, 1964) (given the long-tailed behavior in O.1). To verify the effect of this hypothesis,
we vary nr and tr while keeping nd and nr proportional to them, that is, nd = ⌊αnr⌋and td = ⌊αtr⌋.
We choose α = 0.2 and show the results in the right column of Figure 4. Notice that if the ratio
between these quantities is constant, agreement once again tracks the accuracy closely, indicating
that the relationship between dominant and rare data is central to the origin of GDE in this model.
8

Published as a conference paper at ICLR 2024
Table 1: Average accuracy and agreement on datasets with different interventions. eK represents
different number of superclasses and eK = 10 is the original CIFAR10. Gi presents the data in the
(i −1) × 20th to i × 20th percentile of blue intensity. The difference between accuracy and agreement
is approximately the same for different eK’s (thus GDE holds) but not for different Gi’s.
e
K = 2
e
K = 3
e
K = 5
e
K = 10
G1
G2
G3
G4
G5
Accuracy
0.80
0.84
0.78
0.81
0.62
0.60
0.59
0.66
0.70
Agreement
0.85
0.88
0.83
0.85
0.70
0.67
0.69
0.71
0.75
Difference
0.05
0.04
0.04
0.05
0.08
0.07
0.10
0.05
0.05
6
FROM DESCRIPTION TO PREDICTION
The proposed theoretical model makes a series of simplifications. We now demonstrate its predic-
tive power of what actually happens in deep learning under specific interventions – the following
experiments on GDE are conducted after we derived the theoretical framework and to the best of our
knowledge have never been done in prior works. In other words, our model has not been specifically
adjusted to account for the results of these experiments, but rather the experiments are designed based
on the framework. Results for both experiments are shown in Table 1 (with uncertainty in Table 2)
and the experimental details are in Appendix F.8 and F.9.
The first experiment considers merging classes. We observed in Section 5.2 that for GDE to hold
approximately, the features distribution needs specific properties, namely, tr ∝td and nr ∝nd. If
features do not interfere with each other significantly, our framework predicts that merging classes
into superclasses should not change the ratios and thus would not break the GDE. We merge the
classes of CIFAR 10 into different superclasses and run the same learning algorithms as Section 4 on
the new data (6 random seeds). The accuracy-agreement difference does not change significantly
across different partitions as predicted, even though the accuracy and agreement are different.
This result suggests that breaking the GDE requires intervening on the covariate distribution in order
to change tr
td . Thus, our second set of experiments considers re-partitioning data. In particular,
we sort CIFAR 10 images by the proportion of blue in their total color intensity and partition them
into 5 equally sized groups with increasing blue intensity. We observed that the accuracy-agreement
differences of different data partitions are drastically different, corroborating the prediction made
by the theoretical framework. Furthermore, we see that group 0 has the largest difference between
accuracy and agreement which according to our theoretical framework suggests that the total number
of rare features is larger. Through visual inspection (Figure 17), we can see that in group 1, the
examples seem more visually complex and diverse, which could lead to a larger number of rare
features (i.e., larger tr). It is worth noting that, unlike the setting of Kirsch and Gal (2022), each
group is still i.i.d. Therefore, the violation of GDE immediately implies that the ensemble is not
calibrated on the data partition (Theorem 4.2 of Jiang et al. (2022)). We believe this is the first
direct, non-adversarial construction of natural datasets where a deep ensemble is not well-calibrated
in-distribution from a dataset on which the deep ensemble is usually well-calibrated.
7
CONCLUSION
We investigate distributions of features in data and how neural networks perform feature learning.
Based on the empirical observations, we propose a new framework for understanding feature learning.
We show that the proposed framework is more reflective of reality and can explain other phenomena in
deep learning, notably GDE, without making any assumption about calibration. We believe this work
provides new insight into our understanding of feature learning and data distribution in deep learning.
The proposed framework could be useful for studying other phenomena related to agreement and
ensembles such as calibration (Jiang et al., 2022), phenomena related to distribution shift such as
accuracy-on-the-line (Miller et al., 2021) and agreement-on-the-line (Baek et al., 2022), and transfer
learning. We discuss our framework’s limitations and future directions in Appendix D.3. The new
empirical tools we introduced can be valuable for other empirical investigations beyond the scope of
this work. While recognizing that our framework, like all models, has its simplifications, we hope the
intriguing results will catalyze more future works to study feature learning in this direction.
9

Published as a conference paper at ICLR 2024
ACKNOWLEDGEMENT
We would like to thank Vaishnavh Nagarajan, Samuel Sokota, Elan Rosenfeld, Saurabh Garg, Jeremy
Cohen, and Zixin Wen for the helpful discussion. We also thank Victor Akinwande, Zhili Feng, and
Josh Williams for their feedback on an early draft of this work. Yiding and Christina were partially
supported by funding from the Bosch Center for Artificial Intelligence. Yiding is also supported by
the Google PhD fellowship.
REFERENCES
Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation
in deep learning. arXiv preprint arXiv:2012.09816, 2020.
Z. Allen-Zhu and Y. Li. Feature purification: How adversarial training performs robust deep learning.
In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages
977–988. IEEE, 2022.
J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics of fea-
ture learning: How one gradient step improves the representation. arXiv preprint arXiv:2205.01445,
2022.
C. Baek, Y. Jiang, A. Raghunathan, and J. Z. Kolter. Agreement-on-the-line: Predicting the perfor-
mance of neural networks under distribution shift. Advances in Neural Information Processing
Systems, 35:19274–19289, 2022.
D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba. Network dissection: Quantifying inter-
pretability of deep visual representations. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 6541–6549, 2017.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013.
L. Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
processing systems, 33:1877–1901, 2020.
N. Carlini, U. Erlingsson, and N. Papernot. Prototypical examples in deep learning: Metrics,
characteristics, and utility. 2019.
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of
visual representations. In International conference on machine learning, pages 1597–1607. PMLR,
2020.
A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient
descent. In Conference on Learning Theory, pages 5413–5452. PMLR, 2022.
Y. Dandi, F. Krzakala, B. Loureiro, L. Pesce, and L. Stephan. How two-layer neural networks
learn, one (giant) step at a time. In NeurIPS 2023 Workshop on Mathematics of Modern Machine
Learning, 2023.
N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby,
D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and
C. Olah. Toy models of superposition. Transformer Circuits Thread, 2022. https://transformer-
circuits.pub/2022/toy model/index.html.
V. Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of
the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954–959, 2020.
S. Fort, H. Hu, and B. Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv
preprint arXiv:1912.02757, 2019.
Y. Gal and Z. Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in
deep learning. In international conference on machine learning, pages 1050–1059. PMLR, 2016.
10

Published as a conference paper at ICLR 2024
M. R. Garey and D. S. Johnson. Computers and intractability, volume 174. freeman San Francisco,
1979.
E. H¨ark¨onen, A. Hertzmann, J. Lehtinen, and S. Paris. Ganspace: Discovering interpretable gan
controls. Advances in Neural Information Processing Systems, 33:9841–9850, 2020.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.
K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks, 2016b. URL
http://arxiv.org/abs/1603.05027. cite arxiv:1603.05027Comment: ECCV 2016
camera-ready.
J. Hestness, S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. Patwary, M. Ali, Y. Yang,
and Y. Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409,
2017.
A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and
H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.
ArXiv, abs/1704.04861, 2017.
J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-excitation networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 42:2011–2023, 2020.
G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks. 2017 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2017.
F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K. Keutzer. Squeezenet:
Alexnet-level accuracy with 50x fewer parameters and ¡1mb model size. ArXiv, abs/1602.07360,
2016.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. Advances in neural information processing systems, 31, 2018.
Y. Jiang, V. Nagarajan, C. Baek, and J. Z. Kolter. Assessing generalization of SGD via disagreement.
In International Conference on Learning Representations, 2022. URL https://openreview.
net/forum?id=WvOGCEAQhxl.
Z. Jiang, C. Zhang, K. Talwar, and M. C. Mozer. Characterizing structural regularities of labeled data
in overparameterized models. arXiv preprint arXiv:2002.03206, 2020.
S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in neural
networks beyond kernels. Advances in Neural Information Processing Systems, 34, 2021.
A. Kirsch and Y. Gal. A note on” assessing generalization of sgd via disagreement”. arXiv preprint
arXiv:2202.01851, 2022.
S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations
revisited. In International Conference on Machine Learning, pages 3519–3529. PMLR, 2019.
A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. In Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, 2017.
Y. Li, J. Yosinski, J. Clune, H. Lipson, J. E. Hopcroft, et al. Convergent learning: Do different neural
networks learn the same representations? In FE@ NIPS, pages 196–212, 2015.
Y. Li, C. Wei, and T. Ma. Towards explaining the regularization effect of initial large learning rate in
training neural networks. Advances in Neural Information Processing Systems, 32, 2019.
C. Liu, B. Zoph, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. L. Yuille, J. Huang, and K. P. Murphy.
Progressive neural architecture search. In ECCV, 2018.
11

Published as a conference paper at ICLR 2024
J. P. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon,
and L. Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and
in-distribution generalization. In International Conference on Machine Learning, pages 7721–7735.
PMLR, 2021.
B. Moniri, D. Lee, H. Hassani, and E. Dobriban. A theory of non-linear feature learning with one
gradient step in two-layer neural networks. arXiv preprint arXiv:2310.07891, 2023.
A. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in neural networks with
canonical correlation. Advances in Neural Information Processing Systems, 31, 2018.
A. H. Murphy and E. S. Epstein. Verification of probabilistic predictions: A brief review. Journal of
Applied Meteorology and Climatology, 6(5):748–755, 1967.
P. Nakkiran and Y. Bansal. Distributional generalization: A new kind of generalization. arXiv
preprint arXiv:2009.08092, 2020.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images
with unsupervised feature learning. 2011.
A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, and J. Clune. Synthesizing the preferred inputs
for neurons in neural networks via deep generator networks. Advances in neural information
processing systems, 29, 2016.
E. Nichani, A. Damian, and J. D. Lee. Provable guarantees for nonlinear feature learning in three-layer
neural networks. arXiv preprint arXiv:2305.06986, 2023.
C. Olah, A. Mordvintsev, and L. Schubert. Feature visualization. Distill, 2017. doi: 10.23915/distill.
00007. https://distill.pub/2017/feature-visualization.
C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, and A. Mordvint-
sev.
The building blocks of interpretability.
Distill, 2018.
doi: 10.23915/distill.00010.
https://distill.pub/2018/building-blocks.
V. Papyan, X. Han, and D. L. Donoho. Prevalence of neural collapse during the terminal phase of
deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663,
2020.
V. Pareto. Cours d’´economie politique, volume 1. Librairie Droz, 1964.
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, et al.
Learning transferable visual models from natural language supervision.
In
International Conference on Machine Learning, pages 8748–8763. PMLR, 2021.
M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. Svcca: Singular vector canonical correlation
analysis for deep learning dynamics and interpretability. Advances in neural information processing
systems, 30, 2017.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
International conference on machine learning, pages 6105–6114. PMLR, 2019.
Z. Wang, A. Engel, A. Sarwate, I. Dumitriu, and T. Chiang. Spectral evolution and invariance in
linear-width neural networks. arXiv preprint arXiv:2211.06506, 2022.
M. Welling and Y. W. Teh.
Bayesian learning via stochastic gradient langevin dynamics.
In
Proceedings of the 28th international conference on machine learning (ICML-11), pages 681–688.
Citeseer, 2011.
Z. Wen and Y. Li. Toward understanding the feature learning process of self-supervised contrastive
learning. In International Conference on Machine Learning, pages 11112–11122. PMLR, 2021.
S. Xie, R. B. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated residual transformations for deep
neural networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 5987–5995, 2017.
12

Published as a conference paper at ICLR 2024
J. Xu, Y. Pan, X. Pan, S. C. H. Hoi, Z. Yi, and Z. Xu. Regnet: Self-regulated network for image
classification. IEEE transactions on neural networks and learning systems, PP, 2022.
G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In
International Conference on Machine Learning, pages 11727–11737. PMLR, 2021.
M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European
conference on computer vision, pages 818–833. Springer, 2014.
X. Zhang, X. Zhou, M. Lin, and J. Sun. Shufflenet: An extremely efficient convolutional neural
network for mobile devices.
2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6848–6856, 2018.
13

Published as a conference paper at ICLR 2024
A
LIST OF NOTATIONS
A.1
CONCEPTUAL FRAMEWORK
A.1.1
MAIN PARAMETERS OF THE TOY MODEL
• pd: the proportion of all data that are dominant.
• pr: the proportion of all data that are rare. This parameter is equal to 1 −pd.
• c: the total model capacity. It represents how many features a single model can learn.
• td: the total number of dominant features available in the data for one class.
• tr: the total number of rare features available in the data for one class.
• nd: the total number of dominant features a single dominant data point has. nd ≤td.
• nr: the total number of rare features a single rare data point has. nr ≤tr.
A.1.2
OTHER NOTATIONS OF THE TOY MODEL
• Ψ(·): a function that returns the set of features learned by a model f or contained in a data
point x. The intersection of Ψ(·) of different models and data determines how the model
makes mistakes in our framework.
• err(f, x, y): the error function for a model f on the data (x, y).
• cr: The total capacity the model has for rare features. cr = 1
2prc.
• cd: The total capacity the model has for dominant features. cr = 1
2pdc.
• ζ: the agreement function that dictates the probability with which two models that do not
share features will agree.
A.2
INTERACTION TENSOR
• Φ ∈RN×d: Matrix that contains the representation of all data points for a single model.
• V ∈Rd×d: The principal components of Φ.
• Φproj ∈RN×k: Matrix that contains the projection of the representation of all data points.
• υi,a(x) ∈R: the coefficient of the representation of x in the ith model projected onto the ath
principal component.
• µi,a ∈R: the mean of υi,a(x) over the data distribution.
• σ2
i,a ∈R: the variance of υi,a(x) over the data distribution.
• ρ(i,j),(a,b) ∈R: the Pearson correlation between υi,a(x) and υj,b(x).
• Ki,j ∈[−1, 1]k×k: the feature correlation matrix of two models i and j.
• Λ ∈[−1, 1]M×M×k×k: the collection of all pairwise feature correlation matrices for all M
models. In other words, it contains the Ki,j for all pair of models.
B
IRREDUCIBLE ERROR AND DATA SCALING
In this section, we discuss the sources of irreducible error in our framework. Concretely, there are
two sources of irreducible error:
1. Inductive bias mismatch: these are errors that arise from the fact the models fundamentally
cannot learn some of the features present in the data via conventional training, e.g., stochastic
gradient descent.
2. Finite sample error: these are the errors that arise from insufficient sample size, where the
models do not observe all the features in the data.
14

Published as a conference paper at ICLR 2024
In both cases, the errors result from the models being unable to learn all the 2(td + tr) features in the
support of D. We will refer to the percentage of all features that are present in the training data as
coverage and use β to denote it. When irreducible error occurs, in the best case, the best possible
model can only learn up to βdtd dominant features and βrtr rare features for each class. Further,
while we have previously assumed that there are always more features than the model capacity c,
we will make the a mild but new assumption: if the model’s capacity is larger than the coverage,
the model will sample noise for the remaining capacity. Concretely, the model may be memorizing
noise patterns in the data that do not help generalization similar to (Allen-Zhu and Li, 2020). We can
characterize the expected accuracy when the irreducible error occurs (Proof in Appendix C.3).
Lemma B.1. Under the proposed framework, with coverage of βd and βr, the expected accuracy is
upper-bounded by:
Acc ≤pd
 
1 −1
2
 (1−βd)td
nd

 td
nd

!
+ pr
 
1 −1
2
 (1−βr)tr
nr

 tr
nr

!
.
(5)
Lemma B.1 provides an upper bound on the expected accuracy under this framework when the models
cannot learn all the features. To test the validity of this hypothesis, we simulate different coverage by
using training sets of different sizes. Specifically, we use training set size at 5% increment from 5%
to 100% on CIFAR 10 and ResNet18. In Figure 5a, we show the the upperbound in Equation 5 as a
function of coverage β (same value for both βr and βd). In Figure 5b, we show the test accuracy as
the function of training set size.
0.0
0.1
0.2
0.3
0.4
0.5
0.6
coverage
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00 coverage vs expected acc
(a)
0.0
0.2
0.4
0.6
0.8
1.0
training data %
0.3
0.4
0.5
0.6
0.7
0.8
accuracy
data fraction vs test acc
(b)
Figure 5: Plots of the predicted accuracy as a function of coverage (left) and real model accuracy as a
function of the percent of training data (right).
Note that the lemma describes the average-case test error rather than the worst-case test error that
classical bounds based on uniform convergence describe. Figure 5 shows that varying coverage can
approximate the behavior of scaling dataset size (Hestness et al., 2017). Nonetheless, we see that
some discrepancies between the two plots remain. Most notable is the fact that the test accuracy
seems to increase at a faster rate than the accuracy described by the framework when the dataset
is small. This difference exists likely because the relationship between training dataset size and
coverage is not linear. In particular, coverage increases faster when the dataset size is small but
saturates after the dataset size becomes large. One explanation for this phenomenon is that the models
learn features differently in the presence of different dataset sizes. Our framework currently does not
account for this effect but it is a promising direction for future works.
C
FULL PROOF
In this section, we provide the full proof of the theoretical results. For convenience, we repeat the
claims here.
15

Published as a conference paper at ICLR 2024
C.1
EXPECTED ACCURACY
Proposition C.1. Under the proposed model, the expected accuracy over the model distribution and
data distribution is:
Acc = pd
 
1 −1
2
 td−cd
nd

 td
nd

!
+ pr
 
1 −1
2
 tr−cr
nr

 tr
nr

!
Proof. We are interested in computing the expected accuracy over the entire data distribution D
and the entire hypothesis distribution FA. It is simpler to think about the average error instead of
accuracy (the prediction is random when the model and data do not share any features so the error is
1
2):
Ef∼FA

E(x,y)∼D [err(f, x, y)]

(6)
= Ef∼FA

E(x,y)∼D
1
21 {Ψ(f) ∩Ψ(x) = ∅}

(7)
= 1
2P [Ψ(f) ∩Ψ(x) = ∅]
(8)
= 1
2P [ a sampled f and a sampled x do not share any features ]
(9)
To avoid notational clutter, we will use f and x to denote a sampled f and a sampled x. The
probability of interest is thus:
P [ f and x do not share any features ] = 1 −P [ f and x share at least 1 features ]
(10)
We first partition the event space into two parts: 1. x is dominant, and 2. x is rare. Suppose that x is
dominant, we want to compute P [ f and x do not share any features | x is dominant ]. Since all f’s
have an equal probability of being sampled (as they contain the same numbers of problematically
indistinguishable features), the probability is equivalent to:
P [ f and x do not share any features | x is dominant ]
(11)
= P [ x does not have a fixed set of cd features | x is dominant ]
(12)
=
 td−cd
nd

 td
nd
 .
(13)
This is the configuration of x that does not contain the cd features of f. Analogously, we can compute:
P [ f and x do not share any features | x is rare ] =
 tr−cr
nr

 tr
nr
 .
Given the assumption about how models make mistakes, the expected for both parts of the event
space:
Accd =
1
2 · P [ f and x do not share any features | x is dominant ]
+ 1 · P [ f and x share at least 1 features | x is dominant ]
(14)
= 1
2
 td−cd
nd

 td
nd

+ 1 −
 td−cd
nd

 td
nd

= 1 −1
2
 td−cd
nd

 td
nd
 ,
(15)
Analogously we repeat the computation for rare data,
Accr = 1 −1
2
 tr−cr
nr

 tr
nr
 .
(16)
We now compute the expected accuracy over the entire event space,
Acc = pdAccd + prAccr
(17)
= pd
 
1 −1
2
 td−cd
nd

 td
nd

!
+ pr
 
1 −1
2
 tr−cr
nr

 tr
nr

!
(18)
16

Published as a conference paper at ICLR 2024
C.2
EXPECTED AGREEMENT
Proposition C.2. Under the proposed model, let
 n
r

= 0 when n < 0, r < 0 or n < r, and further
define:
q1 = pd
 
1 −
 th−cd
nd

 td
nd

!2
+ pr
 
1 −
 tr−cr
nr

 tr
nr

!2
,
q2(k) =
pd
 td−nd
cd
2
 td
cd
2
 X
a+b=k
 cd
a
 td−nd−cd
cd−a

 td−nd
cd

 cr
b
 tr−cr
cr−b

 tr
cr

!
+ pr
 tr−nr
cr
2
 tr
cr
2
 X
a+b=k
 cd
a
 td−cd
cd−a

 td
cd

 cr
b
 tr−nr−cr
cr−b

 tr−nr
cr

!
,
q3 = 1 −q1 −
c
X
k=1
q2(k),
the expected agreement between an i.i.d pair of model (f, g) drawn for the model distribution over
the data distribution is:
Agr = q1 + 1
2q3 +
c
X
k=1
ζ(k, c)q2(k)
Proof. We are interested computing the expected disagreement of (f, g) ∼FA × FA over the data
distribution x ∼D. Based on the features in f and g, We partition the event space into 3 subsets:
• A: f and g both share features with x.
n
Ψ(f) ∩Ψ(x) ̸= ∅
o \ n
Ψ(g) ∩Ψ(x) ̸= ∅
o
• B: f and g do not share any features with x but share features with each other.
n
|Ψ(f) ∩Ψ(g)| ̸= ∅
o \ n
Ψ(f) ∩Ψ(x) = ∅
o \ n
Ψ(g) ∩Ψ(x) = ∅
o
• C: The rest of the event space. In these events, we have either:
– f and g do not share any features with x or each other.
– only one of f and g share features with x.
Case A.
Since f and g are independent and identically distributed, it suffices to compute the
probability of one of them not sharing any features with x. We further partition the event space into
two part conditioned on whether the data point is dominant or rare (this is possible because x is
independent from f and g). Following the same logic as Equation 11:
P[ Ψ(f) ∩Ψ(x) ̸= ∅| x is dominant ]
(19)
=1 −P[Ψ(f) ∩Ψ(x) = ∅| x is dominant ]
(20)
=1 −
 td−cd
nd

 td
nd

(21)
Analogously,
P[Ψ(f) ∩Ψ(x) ̸= ∅| x is rare ] = 1 −
 tr−cr
nr

 tr
nr
 .
(22)
17

Published as a conference paper at ICLR 2024
By the independence of f and g:
P[ A | x is dominant ]
(23)
=P[Ψ(f) ∩Ψ(x) ̸= ∅| x is dominant ]2
(24)
=
 
1 −
 td−cd
nd

 td
nd

!2
,
(25)
and similarly,
P[A | x is rare ] =
 
1 −
 tr−cr
nr

 tr
nr

!2
.
(26)
Putting everything together:
q1 = P[A] = pd
 
1 −
 tr−cr
nr

 tr
nr

!2
+ pr
 
1 −
 tr−cr
nr

 tr
nr

!2
.
(27)
Case B.
Again, we partition the event space based on dominant and rare data. Then we further
partition the even space based on k, the number of features that f and g share with each other. First,
we compute the probability that both f and g do not share any features with x. By independence and
equation 11:
P[Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅| x is dominant ]
(28)
=P[Ψ(f) ∩Ψ(x) = ∅| x is dominant ]2
(29)
=
 td−cd
nd
2
 td
nd
2 .
(30)
Conditioned on that x is dominant and that f and g do not share any features with x, we now compute
the probability where f and g share exactly k features. By symmetry, this probability is equal to the
probability of sampling g that shares exactly k features with a fixed f. Since f and g cannot share any
feature with x, the total number of dominant features available is td −nd. This event space can be
further partitioned into disjoint events where g shares exactly a dominant features and b rare features
with f for (a, b) ∈{(0, k), (1, k −1), . . . , (k −1, 1), (k, 0)}. Since g always samples cd dominant
features and cr rare features, the two processes are independent from each other and respectively
follow hypergeometric distributions (i.e., marble picking problem):
P [|Ψ(f) ∩Ψ(g)| = k | Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅and x is dominant ]
=
X
a+b=k
 cd
a
 td−nd−cd
cd−a

 td−nd
cd

 cr
b
 tr−cr
cr−b

 tr
cr

.
The first term in the summation is the density of the hypergeometric distribution for sampling a
allowed dominant features, and the second term is the hypergeometric distribution for sampling b
allowed rare features.
The same reasoning process can be applied to when x is rare by modifying the available number of
rare features to tr −nr and keep the available number of dominant features as td:
P[Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅| x is rare ] =
 tr−cr
nr
2
 tr
nr
2
(31)
P [|Ψ(f) ∩Ψ(g)| = k | Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅and x is rare ]
(32)
=
X
a+b=k
 cd
a
 td−cd
cd−a

 td
cd

 cr
b
 tr−nr−cr
cr−b

 tr−nr
cr

.
(33)
18

Published as a conference paper at ICLR 2024
Putting everything together, we arrive at the probability:
q2(k) = P[|Ψ(f) ∩Ψ(g)| = k and Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅]
(34)
=
pd
 td−nd
cd
2
 td
cd
2
 X
a+b=k
 cd
a
 td−nd−cd
cd−a

 td−nd
cd

 cr
b
 tr−cr
cr−b

 tr
cr

!
(35)
+ pr
 tr−nr
cr
2
 tr
cr
2
 X
a+b=k
 cd
a
 td−cd
cd−a

 td
cd

 cr
b
 tr−nr−cr
cr−b

 tr−nr
cr

!
.
(36)
Note that there may be cases where the combination is undefined (e.g., td −nd −cd < 0 or
td −nd −cd < cd −a). These cases means that the configurations are impossible to exist, so their
corresponding probabilities are 0. We will define
 n
r

= 0 when n < 0, r < 0 or n < r to handle
these cases. The total probability of B is equal to the sum of q(k) from k = 1 to c since that is
equivalent of the event |Ψ(f) ∩Ψ(g)| > 0:
P[B] =
c
X
k=1
q2(k)
(37)
Case C.
This event is the complement of A ∪B so:
q3 = P[C] = 1 −P[A] −P[B] = 1 −q1 −
c
X
k=1
q2(k).
(38)
In A, we know the models agree with probability 1. In C, either both models will make a random
guess or one model will make a random guess and the other will classify x correctly. In both cases,
they will agree with probability 1
2. In B, we assumed that the probability agreement is modulated by
the agreement function ζ (Section 5). Combining these agreement conditions with the probability of
A, B, C gives:
Agr = 1 · P[A] + 1
2 · P[C] +
c
X
k=1
q2(k)ζ(k, c)
(39)
= q1 + 1
2q3 +
c
X
k=1
q2(k)ζ(k, c).
(40)
Replacing q3 with 1 −q1 −Pc
k=1 q2(k) and simplify yields the final results.
C.3
COVERAGE LEMMA
Lemma C.3. Under the proposed framework, with coverage of βd and βr, the expected accuracy is
upper-bounded by:
Acc ≤pd
 
1 −1
2
 (1−βd)td
nd

 td
nd

!
+ pr
 
1 −1
2
 (1−βr)tr
nr

 tr
nr

!
.
(41)
Proof. Lets call the set of all features Γ and set of features available for the models to learn bΓ. We can
naturally partition them based on dominant and rare features – Γd is the set of all dominant features
and Γr is the set of all rare features. By the coverage assumption |bΓr| = βr|Γr| and |bΓd| = βd|Γd|.
Notice that having different numbers of features available to the models and data means that the
distributions of model sharing features with conditioned on the data is no longer the identical for
different data. The conditional probability changes depending on how many features of the data
point is not in bΓ. On the other hand, conditional probability of data point sharing features with a
fixed model is the same for all models, because bΓ ⊆Γ — No matter what features are in Ψ(f), the
19

Published as a conference paper at ICLR 2024
probability that a sampled data point does not share any dominant features with it is
 td−cd
nd

/
 td
nd
3.
Recall that cd and cr represent how many features the model can learn which is upperbounded by
βdtd and βrtr. Since
 n
r

is monotonically increasing in n:
βdtd ≥cd =⇒td −βdtd ≤td −cd =⇒
(1 −βd)td
nd

≤
td −cd
nd

,
(42)
the same can be derived for rare data. Substituting in the expression for accuracy from Equation 3,
Acc = pd
 
1 −1
2
 td−cd
nd

 td
nd

!
+ pr
 
1 −1
2
 tr−cr
nr

 tr
nr

!
(43)
≤pd
 
1 −1
2
 (1−βd)td
nd

 td
nd

!
+ pr
 
1 −1
2
 (1−βr)tr
nr

 tr
nr

!
.
(44)
D
FURTHER DISCUSSIONS OF THE THEORETICAL MODEL
D.1
COMPARISON TO PRIOR AND CONCURRENT WORKS
An important difference between this model and the multi-view model from Allen-Zhu and Li (2020)
is that our model does not treat all features as having the same learning difficulty (i.e., probability of
being learned). Indeed, the experiments in Section 4 show that features demonstrate a wide range
of behaviors in terms of how often they occur in the data and how they interact with the models.
Another notable difference is that in Allen-Zhu and Li (2020), the multi-view portion of the dataset
contains all the features. In reality, the “easy” part of the data that a large portion of the models
classifies correctly actually contains much fewer features. These observations suggest that having
different types of features may be a more accurate description of nature. Nonetheless, we do not
describe the exact mechanism of how feature learning actually happens under our model since we are
not assuming any particular hypothesis class. Consequently, we do not use the same definition as
Allen-Zhu and Li (2020) as they adopt a very simplified model of features (i.e., orthogonal vectors
in the input space). The spirit of our model of feature learning is close to that of Allen-Zhu and Li
(2020) and we believe a similar iterative analysis can be applied to our model.
There is also a recent line of theoretical works that depart from the random feature model and try
to incorporate feature learning into the analysis (Damian et al., 2022; Wang et al., 2022; Moniri
et al., 2023; Nichani et al., 2023; Dandi et al., 2023). These works generally assume the data is
an isotropic Gaussian and the target function is a 2-layer neural network. The feature is usually a
linear function of the input and sometimes special training algorithms are needed (e.g., layer-wise
training). A notable development is Nichani et al. (2023) which analyzed 3-layer neural network
which learns non-linear features although it still requires layer-wise training. In general, there is
still a gap between the assumptions made in these works and real models and, to the best of our
knowledge, they do not account for ensemble or the possibility of learning different features. The
goal of our work is not to discuss the merits or limitations of these works but to study what actually
happens in practice on real dataset and real architectures. Due to the complex nature of deep learning,
we believe this more scientific approach is a good complement to the existing theoretical works.
It is also natural to question whether the simplification where a single feature is sufficient for deter-
mining the class is sensible. We believe that this simplification is realistic for a binary classification
problem and that using more features in determining the true class may make the model more expres-
sive but should not fundamentally alter the behavior of the system. Further, the true data distributions
are evidently more complex — dominant data can contain rare features, and, vice versa. In fact, both
features and data can lie on a continuous spectrum between “dominant” and “rare” (Figure 2b and
2c). These changes can be incorporated into the framework by modifying the distribution of features
but doing so can increase the complexity of the analysis and require tail-bounds to characterize the
system’s behavior.
3Here we assume the capacity is smaller than the number of available features. If the capacity is larger, then
model will learn all available features and the bound is tight.
20

Published as a conference paper at ICLR 2024
D.2
SOURCES OF RANDOMNESS
Another assumption we made is that when the model f does not share any feature with a data point x,
the model will make a random guess. At first look, this seems like a strong assumption that requires
the model to make a perfectly random guess. However, recall that we are computing the expectation
over the model distribution and the data distribution rather than a single fixed data point. For a single
model f, its prediction is effectively random if its average prediction over all the distribution of data
that do not share features with f is at the chance:
PD[f(x) = y | Ψ(f) ∩Ψ(x) = ∅] = E(x,y)∼D [1{f(x) = y} | Ψ(f) ∩Ψ(x) = ∅] = 1
2.
(45)
This means that f can be completely deterministic as long as its accuracy over all the data that
it doesn’t share feature with is random chance. This is in fact the only sensible outcome if we
assume that features are indeed what the models use to make predictions. In this case, the source of
randomness comes from the data, (x, y) ∼D.
We now analyze the case where we hold a single data point (x, y) fixed and generate the source of
randomness from the training algorithm f ∼FA (once again, the individual model can be completely
deterministic). When the data point is one with which f does not share features, we cannot expect
the models to make independent predictions since the models have similar inductive bias and can
make predictions in a correlated manner depending on x (e.g., noise in x):
PFA[f(x) = y | Ψ(f) ∩Ψ(x) = ∅] = Ef∼FA [1{f(x) = y} | Ψ(f) ∩Ψ(x) = ∅] ̸= 1
2.
(46)
Consequently, the agreement between a pair of models will not be random over the data distribution,
and this is exactly what the agreement function tries to model.
PFA×FA[f(x) = g(x) | Ψ(f) ∩Ψ(x) = ∅and Ψ(g) ∩Ψ(x) = ∅] = ζ(FA, x)
(47)
In the most general case, ζ is a function of the hypothesis distribution and a data point x, but the ones
we used in the main text assume that ζ is a function of the model’s features, since what type of data x
is irrelevant if neither models have the features to predict it so we can also drop that dependency.
D.3
LIMITATIONS
While our theoretical framework is able to explain some previously poorly understood phenomena,
several limitations still exist. We have discussed various limitations of our framework throughout the
paper but for ease of reading, we will discuss all of them here.
Optimization analysis
One of the important limitations of this work is that it only describes what
kind of features are present in the data and end up being learned by different models, but it does
not offer any insight on how these features are learned through gradient descent mechanistically.
Modeling the optimization procedure is evidently a challenging task (although this is true for deep
learning in general). One promising avenue is via techniques similar to Allen-Zhu and Li (2020).
Binary dichotomy of features
The current conceptual model assumes there are only two types
of features, which facilitate the analysis. Evidently, Figure 2b shows that the features in fact live
on a spectrum of frequency of appearance. Future work could resolve this by assuming long-tailed
distributions (e.g., Zipf’s distribution) instead of binary distributions to make the model closer to
reality.
Binary classification
The current conceptual model only analyzes a binary classification. We
discuss steps towards extending it to multi-class in Appendix D.4.
D.4
EXTENSION TO MULTI-CLASS
In order to extend this framework to multi-class, we would first have to decide on how the model
makes predictions based on the features it has learned and the features present in the data. In this
setting, perfect prediction based on a single feature may no longer be enough since different classes
can share features. Instead, one may need to introduce a new function for the probability of correct
classification based on the number of shared features between the model and the data point or the
21

Published as a conference paper at ICLR 2024
probability of making a mistake based on the features. This also means that we cannot no longer
assume the model will make a random guess since there are more than one possible wrong class and
how the model makes a prediction will depend on the features they share with these wrong classes.
Mathematically, this means that ζ is no longer independent of the data point x. The desired quantities
are still computable through combinatorics but the added complexity could make the derivation much
more complicated and an analytical expression may or may not be attainable, though the problem
may be amenable through tail-bounds.
E
EXPERIMENTAL DETAILS
In our experiments, we use two collections of k = 20 ResNet18 (He et al., 2016a) trained on the
CIFAR-10 dataset (Krizhevsky et al., 2009) following the experimental set up of Jiang et al. (2022).
The first collection of models is trained on random 10000 subsets of the whole training set (10k), and
the second collection of models is trained on random 45000 subsets of the whole training set (45k).
On average, the 10k models achieve 67.9% test accuracy and the 45k models achieve 84.5% test
accuracy. In addition, we repeat the same process for SVHN dataset Netzer et al. (2011) using random
45000 subsets of the whole training set (SVHN). The training details are outlined in Appendix E
and E.2. We compute Λ and Ωon the test set using the output of the penultimate layer (i.e., ψ
is the final linear layer). For clustering features, we choose k = 50 for the number of principal
components to use, γcorr to be the 90th percentile of Λ, and γdata to be the 90th percentile of all entries
in bΦproj
i
, i = 1, . . . , M. After clustering, we obtain T = 680 feature clusters for 45k. We primarily
show 45k here and leave the 10k and SVHN results to Appendix F, which are similar to the 45k
results qualitatively.
E.1
TRAINING DETAILS
For the ResNet18 experiments, we follow the same procedures as Jiang et al. (2022) which uses the
same architecture of ResNet18 as He et al. (2016a). We train the 20 models with:
• initial learning rate: 0.1
• weight decay: 0.0001
• minibatch size: 100
• data augmentation: No
The models in 45k samples 45000 data points from the training set without replacement. Likewise,
the models in 10k samples 10000 data points from the training set without replacement.
E.2
CLUSTERING ALGORITHM
Algorithm 1 iterates over all entries of Λ and assigns each feature to a cluster if its correlation with
the members of the cluster exceeds γcorr; otherwise, the algorithm creates a new cluster for that
feature. One notable property of the greedy clustering algorithm is that it does not generate a fixed
number of clusters. This is desirable in this case because if a feature does not have a high correlation
with any other features, we would like to isolate it as a unique cluster rather than grouping it together
with other features.
For the number of principal components, we recommend picking the number where after projecting
every activation vector onto the principal components, the linear layer can classify the projected
representation with approximately the same accuracy as the representation before projection. In our
setting, 50 principal components could retain 100% of the original performance. For γcorr , we found
that the qualitative results are not very sensitive to different values. We experiment with different
values of γcorr in Appendix F.10 and observed similar results.
E.3
HARDWARE
All experiments in the paper are done on single Nvidia RTX 2080’s and RTX A6000’s.
22

Published as a conference paper at ICLR 2024
Algorithm 1 ClusterFeatures
1: Input: Λ[M,M,k,k], γcorr
2: Assignment[M,k] ←new empty matrix
3: Maximum[M,k] ←new matrix filled with −1
4: CurrentFeature ←1
5: for i = 1 to k and j = 1 to m do
6:
if Assignment[i,j] is not empty then
7:
Skip to the next j
8:
Assignment[i,j] ←CurrentFeature
9:
for p = 1 to k do
10:
CorrMat ←Λ[i,p,:,:]
11:
FeatureRow ←CorrMat[j,:]
12:
for q = 1 to m do
13:
if FeatureRow[q] > Maximum[p,q] and FeatureRow[q] > γcorr then
14:
Assignment[p,q] ←CurrentFeature
15:
Maximum[p,q] ←FeatureRow[q]
16:
CurrentFeature ←CurrentFeature + 1
17: Return Assignment
F
ADDITIONAL FIGURES, SIMULATIONS, AND EXPERIMENTS
F.1
INTERACTION TENSOR
IT
1,1
IT
1,2
IT
1,3
· · ·
IT
1,N
IT
2,1
IT
2,2
IT
2,3
IT
2,N
IT
3,1
IT
3,2
IT
3,3
IT
3,N
...
...
IT
M,1 IT
M,2 IT
M,3
IT
M,N
I2
1,1
I2
1,2
I2
1,3
· · ·
I2
1,N
I2
2,1
I2
2,2
I2
2,3
I2
2,N
I2
3,1
I2
3,2
I2
3,3
I2
3,N
...
...
I2
M,1 I2
M,2 I2
M,3
I2
M,N
I1
1,1
I1
1,2
I1
1,3
· · ·
I1
1,N
I1
2,1
I1
2,2
I1
2,3
I1
2,N
I1
3,1
I1
3,2
I1
3,3
I1
3,N
...
...
I1
M,1 I1
M,2 I1
M,3
I1
M,N
feature (T)
data (N)
model (M)
Figure 6: An illustration of the interaction tensor, Ω. The three axes correspond to model, data, and
features. An entry It
m,n is 1 if both data n and model m contains feature t and is 0 otherwise.
F.2
EFFECT OF PCA
In Figure 8, we show the correlation matrices, Ki,j, for different model pairs from 10k and 45k. The
first column shows the self-correlation matrix between the features of the same model. Both matrices
are effectively diagonal which indicates that the principal components represent features with no
redundant information. This contrasts with Li et al. (2015) where the self-correlation matrices have
many off-diagonal entries. Off-diagonal entries for the self-correlation matrix indicate that either
there is redundant information or a single feature is distributed across multiple neurons, which is not
desirable for studying unique features. Another interesting effect of using PCA projected features is
that the features are naturally “aligned” because the principal components are already sorted by the
amount of variance they can explain. We can see that the correlation matrices’ entries (especially
towards the top features) are naturally more concentrated towards the diagonal. Furthermore, the
models with more data and higher test accuracy (45k) have more near diagonal entries. This indicates
that the models in 45k have learned nearly the same top features. This observation is consistent
with Li et al. (2015); Morcos et al. (2018) which find that better models tend to learn more similar
representations.
23

Published as a conference paper at ICLR 2024
Self-correlation (10k)
Correlation (10k)
Self-correlation (45k)
Correlation (45k)
Figure 8: Self-correlation (Ki,j where i = j) and correlation (Ki,j where i ̸= j) for random models
from 10k and 45k. Both self-correlation matrices contain only diagonal entries, indicating that the
features of the same model contain no redundant information. On the other hand, the correlation
between models in 45k exhibits more structure than the models in 10k, with the non-zero entries
more concentrated around the diagonal (zoom in for better visuals). This suggests that the features of
models learned in 45k contain more similar information compared to models learned in 10k.
e
K = 2
e
K = 3
e
K = 5
e
K = 10
G1
G2
G3
G4
G5
Acc
0.80± 0.01
0.84± 0.01
0.78± 0.01
0.81± 0.02
0.62± 0.02
0.60± 0.01
0.59± 0.03
0.66± 0.02
0.70± 0.03
Agr
0.85± 0.06
0.88± 0.05
0.83± 0.08
0.85± 0.07
0.70± 0.13
0.67± 0.15
0.69± 0.15
0.71± 0.13
0.75± 0.11
Diff
0.05
0.04
0.04
0.05
0.08
0.07
0.10
0.05
0.05
Table 2: Average accuracy and agreement on datasets with different interventions. eK represents
different number of superclasses and eK = 10 is the original CIFAR10. Gi presents the data in
the (i −1) × 20th to i × 20th percentile of blue intensity. The difference between accuracy and
agreement is approximately the same for different eK’s (thus GDE holds) but not for different Gi’s.
The uncertainty denotes standard deviation.
F.3
EMPIRICAL PROPERTIES OF PCA FEATURES
Features are semantically meaningful
As shown in Figure 1, our feature definitions are semanti-
cally meaningful and can be used for identifying common prototypes and rare images in each class.
new experiments on the density of feature in each group? Rare images contain more rare features.
This property of the defined features also allows us to find semantically similar images in the dataset.
To do so, we first define a similarity metric between two images:
s(x1, x2) = 2 |Ψ(x1) ∩Ψ(x2)|
|Ψ(x1)| + |Ψ(x2)|.
(48)
This function intuitively computes the overlap of features between two images normalized by their
total number of features. For any given image x, we can compute the similarity of x and the entire
dataset and find the ones with the highest similarities. In Figure 9, we show the nearest neighbors
of a random sample of images. We can see that our metric is able to identify semantically similar
neighbors for each image even if the images are not close in pixel space.
Note that for the second row of Figure 9, the first 7 neighbors have 100% feature overlap. The
property of our definition of feature may be of independent interest to other applications.
Individual features do not correspond to particular classes It may be tempting to think that
individual features may correspond to individual classes. In the extreme case, this would reduce to
neural collapse (Papyan et al., 2020) (which only happens after the model has been trained for an
extremely long time). We find that this is not the case. Instead, individual features do not correspond
to any particular classes (Figure 10). To illustrate this point further, we plot the frequency at which
the top features appear in each class, and observed that the dominant features often appear in many
different classes with different frequencies and would be missing from only one or two classes
(Figure 11). This suggests that individual features can represent multiple “concepts” in the data but
combinations of several features are much more interpretable (Figure 9).
This is perhaps not too surprising since in general we cannot expect the models to learn features that
humans consider to be good features. After all, the appeal for using neural networks is the difficulty
of designing hand-engineered features. Future works could investigate these observations further.
24

Published as a conference paper at ICLR 2024
Figure 9: Nearest neighbors of random images measured by feature overlap. The leftmost image
is the base image and the row contains its nearest neighbors. The similarity score is shown above
each image. We can see that feature overlap can reliable capture the semantic similarities between
different images even if the distance in pixels space is large. This phenomenon is particularly obvious
in the second row where we can see that several distinct images of frogs have 100% feature overlap.
Features capture prototypical examples.
We also observed (Figure 12) that our definition of
features can recover the notion of prototypical examples observed in Carlini et al. (2019); Jiang
et al. (2020). In particular, the images with the least features seem to correspond to the prototypical
examples (images where the objects are presented in a canonical way) whereas the images with
the most features seem to correspond to non-prototypical examples (images where the objects
are presented in a rare way). This means that these prototypical examples usually contain much
fewer (dominant) features whereas the non-prototypical examples contain much more rare features.
Exploring these connections would be an interesting future direction.
25

Published as a conference paper at ICLR 2024
Figure 10: Random data points containing individual features. The top figure shows the sample
images for dominant features and the bottom figure shows sample images for rare features. Neither
shows obvious patterns, although images with dominant features do seem to be visually less complex.
26

Published as a conference paper at ICLR 2024
Figure 11: Frequency of top feature in each class. n is the feature’s total number of occurrences.
F.4
CIFAR-10 10K SUBSET EXPERIMENTS
For 10k models, we see that the observations are largely consistent with the observations of 45k.
It is worth noting that in Figure 13e, the shared errors are generally smaller than Figure 3c and the
shared errors also exhibit more variance. This may be due to the fact that when the models have low
performance, their agreement behaves more randomly rather than how the agreement of 45k behaves.
We will see in Appendix F.6 that when the collection of models have different architecture, an even
strong correlation between number of shared features and amount of shared error is observed.
27

Published as a conference paper at ICLR 2024
Figure 12: Visualization of images with the least features (top) and the most features (bottom) for
each class of CIFAR 10 under our feature definition (defined in Section 3.1). Each row corresponds
to one class of CIFAR 10 (zoom in for better viewing quality). .
F.5
SVHN EXPERIMENTS
For models trained on SVHN, we observe similar phenomena from the other experimental settings on
CIFAR-10. Some notable differences include that there are much fewer low-confidence data points
compared to CIFAR-10 likely because the performance of ResNet18 is higher on SVHN and the
models classify most test points correctly. The absolute occurrences of different features are higher
because SVHN has more test data than CIFAR-10.
28

Published as a conference paper at ICLR 2024
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
confidence
0
20
40
60
80
100
120
140
160
number of features
conf vs. feature count (10k)
(a)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
0
1
2
3
4
5
6
7
8
occurrences (1e3)
1e3
feature frequency (10k)
(b)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
4.0
3.5
3.0
2.5
2.0
log prob (base 10)
log feature density by confidence (10k)
high confidence
low confidence
(c)
0
1
2
3
4
5
6
7
# of data with a feature (1e3)
1e3
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
# of models with a feature
n data vs. n models w/ feat (10k)
(d)
2
4
6
8
10
12
shared features
0.29
0.30
0.31
0.32
0.33
0.34
shared error
shared feature vs. shared error (10k)
(e)
Figure 13: Same set of plots for 10k models.
0.0
0.2
0.4
0.6
0.8
1.0
confidence
0
50
100
150
200
features num
conf vs feature num
(a)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
0.0
0.5
1.0
1.5
2.0
occurrences (1e3)
1e4
feature frequency
(b)
0.0
0.2
0.4
0.6
0.8
1.0
feature (1e2)
1e3
2.50
2.75
3.00
3.25
3.50
3.75
4.00
4.25
occurrences (base 10)
feature frequency over training
Init
ep 1
ep 5
ep 9
ep 13
ep 17
ep 21
ep 25
ep 29
(c)
0
1
2
3
4
5
6
7
feature (1e2)
1e2
4.0
3.5
3.0
2.5
2.0
log prob (base 10)
log feature density by confidence
high confidence
low confidence
(d)
0.0
0.5
1.0
1.5
2.0
# of data with a feature (1e3)
1e4
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
# of models with a feature
n data vs. n models w/ feat
(e)
2
4
6
8
10
shared features
0.36
0.38
0.40
0.42
0.44
0.46
0.48
shared error
shared feature vs. shared error
(f)
Figure 14: Same set of plots for SVHN models.
F.6
SHARED FEATURES AND SHARED ERROR FOR DIFFERENT ARCHITECTURES
We see earlier that when the architectures are the same, the models naturally tend to agree with each
other more. For both 45k and 10k, even the smallest shared error is much greater than chance.
Here we will use a wide range of different architectures trained of CIFAR 10 to test the validty of
our hypothesis that more shared features lead to more shared error. Figure 15 shows the number of
shared features plotted against the shared error between each pair of models. Similar to Figure 3c,
the shared error is almost monotonically increasing as a function of the number of shared features.
If two models share a large number of features, they would tend to share high proportion of errors.
If two models share a moderate number of features (4 to 11), the distribution of shared error once
again appears random. However, unlike the case of same architecture, when two architectures share a
29

Published as a conference paper at ICLR 2024
0
2
4
6
8
10
12
14
16
18
20
shared features
0.0
0.1
0.2
0.3
0.4
0.5
shared error
Share feature vs. shared error 
 on different architectures
Figure 15: Shared feature against shared error for models with different architectures.
small number of features (1 to 3), their shared errors tend to concentrate at much smaller values. This
observation indicates that while having high number of shared features generally leads to models
making similar mistakes, having low number of shared features does not mean two models will have
low number of shared error. Rather, different sets of features can still make similar mistakes. The
architectures we test include:
• PreActResNet18 (He et al., 2016b)
• PreActResNet34 (He et al., 2016b)
• PreActResNet50 (He et al., 2016b)
• VGG11 (Simonyan and Zisserman,
2014)
• VGG13 (Simonyan and Zisserman,
2014)
• VGG16 (Simonyan and Zisserman,
2014)
• RegNet X200 (Xu et al., 2022)
• RegNet X400 (Xu et al., 2022)
• ResNet34 (He et al., 2016a)
• ResNet50 (He et al., 2016a)
• ResNet101 (He et al., 2016a)
• ResNeXt29 (Xie et al., 2017)
• DenseNet121 (Huang et al., 2017)
• DenseNet169 (Huang et al., 2017)
• ShuffleNetV2 with scale factor 1 (Zhang
et al., 2018)
• ShuffleNetV2 with scale factor 1.5
(Zhang et al., 2018)
• ShuffleNetV2 with scale factor 0.5
(Zhang et al., 2018)
• ShuffleNetG2 (Zhang et al., 2018)
• SENet18 (Hu et al., 2020)
• SqueezeNet (Iandola et al., 2016)
• EfficientNetB0 (Tan and Le, 2019)
• PNASNetA (Liu et al., 2018)
• PNSNetA large (Liu et al., 2018)
• MobileNet V2 (Howard et al., 2017)
All models we use are from testbed created by (Miller et al., 2021).
30

Published as a conference paper at ICLR 2024
F.7
DIFFERENT CHOICES OF ζ
In this section, we investigate the effect of agreement function ζ on GDE. Instead of plotting accuracy
and agreement separately, we show the difference between accuracy and agreement:
Diff = Acc −Arg
The closer the difference is to 0, the closer the system is to satisfying GDE exactly. We test three
types of agreement functions, each with an adjustable parameter:
1. constant: This agreement function assumes that if two models share one or more features,
then they have a constant probability η ∈[0.5, 1] of agreeing.
ζconst(k, c, η) = η
In the main text, we use η = 0.9.
2. proportional: This agreement function assumes that the probability of agreement is
directly proportional to how many features two models share relative to the full model
capacity. The constant of proportionality is η ∈(0, ∞) and the probability is clipped to 1.
ζprop(k, c, η) = min

η k
c , 1

3. step: This agreement function assumes that there is a threshold η ∈N. If the number of
shared features is above η, then the probability of agreement is 1. Otherwise, the probability
of agreement is some constant θ ∈[0.5, 1].
ζstep(k, c, η, θ) = θ · 1{c ≤η} + 1{c > η}
For these simulation, we use θ = 0.8 since η has a much greater effect on GDE.
We vary the values of η for each agreement function and show the results for different values pd,
c, coupled nr, nd and coupled tr, td in Figure 16. Each row corresponds to a different agreement
function and from top to bottom are constant, proportional, and step.
For constant (top row), η ranges from 0.5 to 0.95. We see that for c, pd, nr ∝nd, the range of
variation in difference is consistently small when pd is sufficiently large. tr ∝td deviates from this
behavior where different η’s behave more differently as pd increases. For this scenario, we see that
larger η are closer to GDE.
For proportional, η ranges from 1 to 2.8. We see that the difference is generally large for all
values of the parameters. Suggesting that ζprop may not be a good approximation for how models
agree in practice.
For step, η is an integer that ranges from 0 to 9. We see that the differences are more robust
to different values of η than the other agreement functions. This suggests ζstep could be a good
approximation to how models agree in practice. This observation is consistent with Figure 3c,
Figure 13e, and Figure 15 — when models do not share many features, the shared error (therefore,
agreement) is spread out but have similar expected values; when models share a large number of
features, the probability of agreement increases significantly.
An important observation from these simulations is the importance of td and tr. These quantities
can be interpreted as proxies for the complexities of the entire dataset. tr in particular represents the
patterns in the data that are rare. The larger tr is, the more diverse or noisy the dataset is. According
to our framework, this quantity can have large impact on the behaviors of accuracy and agreement.
Another important observation is that pd needs to be sufficiently large for GDE to hold strongly. This
is roughly equivalent to requiring the feature distribution to be long-tailed, which is true in practice.
31

Published as a conference paper at ICLR 2024
5
10
15
20
25
30
35
40
45
c
0.02
0.00
0.02
0.04
0.06
0.08
0.10
const agreement function: c
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
0.2
0.4
0.6
0.8
1.0
pd
0.15
0.10
0.05
0.00
0.05
const agreement function: pd
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
0
10
20
30
40
50
nr
0.05
0.00
0.05
0.10
const agreement function: nr
nd
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
100 150 200 250 300 350 400 450 500
tr
0.02
0.00
0.02
0.04
0.06
0.08
0.10
0.12
const agreement function: tr
td
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
5
10
15
20
25
30
35
40
45
c
0.000
0.025
0.050
0.075
0.100
0.125
0.150
prop agreement function: c
1.00
1.30
1.60
1.90
2.20
2.50
2.80
0.2
0.4
0.6
0.8
1.0
pd
0.05
0.00
0.05
0.10
0.15
0.20
prop agreement function: pd
1.00
1.30
1.60
1.90
2.20
2.50
2.80
0
10
20
30
40
50
nr
0.025
0.050
0.075
0.100
0.125
0.150
0.175
prop agreement function: nr
nd
1.00
1.30
1.60
1.90
2.20
2.50
2.80
100 150 200 250 300 350 400 450 500
tr
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.22
0.24 prop agreement function: tr
td
1.00
1.30
1.60
1.90
2.20
2.50
2.80
5
10
15
20
25
30
35
40
45
c
0.02
0.00
0.02
0.04
0.06
0.08
step agreement function: c
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
9.00
0.2
0.4
0.6
0.8
1.0
pd
0.175
0.150
0.125
0.100
0.075
0.050
0.025
0.000
0.025
step agreement function: pd
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
9.00
0
10
20
30
40
50
nr
0.075
0.050
0.025
0.000
0.025
0.050
0.075
step agreement function: nr
nd
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
9.00
100 150 200 250 300 350 400 450 500
tr
0.02
0.00
0.02
0.04
step agreement function: tr
td
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
9.00
Figure 16: Difference between the analytical agreement and accuracy for different choices of
agreement functions ζ and different values of parameters. From top to bottom are constant,
proportional, and step.
F.8
MERGING CLASSES
For this experiment, we merge different classes of CIFAR 10 to form superclasses. Since the
individual images are not modified, we expect the majority of features that identify individual classes
to also identify the superclasses well (although there may be interferences between features of
different classes). Thus, we would expect the ratio between features stay approximately constant.
The specific superclasses are:
• 2 superclass: {airplane, automobile, bird, cat, deer}, {dog, frog, horse, ship, truck}
• 3 superclass: {airplane, automobile, bird}, {cat, deer, dog}, {frog, horse, ship, truck}
• 5 superclass: {airplane, automobile}, {bird, cat}, {deer, dog}, {frog, horse}, {ship, truck}
Finally, the 10 classes case corresponds to the regular CIFAR 10 classification. The experiments are
repeated for 6 random seeds.
F.9
PARTITIONING DATA
For this experiment, we partition CIFAR based on the intensity of blue pixels. More concretely, let
x ∈[0, 1]32×32×3 be an image where the last channel is the RGB value of the pixel. We compute its
blue intensity, b, as:
b(x) =
P32
i=1
P32
j=1 xi,j,2
P32
i=1
P32
j=1
P3
k=1 xi,j,k
.
(49)
Intuitively, this value captures how much the blue channel “weighs” in the whole image. We
compute this value for all xi and compute the CDF, F, of b(xi) over the training dataset Xtrain =
{x0, x1, . . . , x50000}. Then we partition the data into groups:
Gi = {x ∈X | 0.2i < F(b(x)) ≤0.2(i + 1)},
(50)
for i ∈{0, 1, 2, 3, 4}. For the test data, we partition according to the CDF of the training data, i.e.,
training and test use the same threshold. In Figure 17, we show random samples of images from each
partition based on the blue intensity. We can see that in group 0, the examples seem more visually
complex and diverse, which could lead to a larger number of rare features (i.e., larger tr, the total
number of rare features). The experiments are repeated for 6 random seeds.
32

Published as a conference paper at ICLR 2024
Figure 17: Visualization based on data partitioning based on blue intensity.
F.10
EMPIRICAL OBSERVATIONS UNDER DIFFERENT HYPERPARAMTERS
The construction of the interaction tensor involves several hyperparameters. In this section, we study
how sensitive the empirical observations are to the changes in hyperparameters. Specifically, we study
the effect of γcorr which decides whether two principal components of two models are capturing the
same feature, and k, the number of principal components used for each model. We show the results
for γcorr = 0.5 and γcorr = 0.7 in Figure 18 and Figure 19, and the results for k = 30 in Figure 20.
We observe that the most significant impact of γcorr is on the total number of features because having
a higher threshold would mean that more features would be counted as different features if they
do not have highly correlated responses to data; this would increase the total number of features.
Interestingly, the qualitative observations remain the same which means the conclusion is robust to
the choice of these hyperparamters.
Similarly, too many principal components would inflate the number of rare features (features at the
tail). Empirically, this would affect Observation 1 by inflating the length of the tail (but it is still
long-tailed), but it should not affect Observation 2-4. For 2, the high-confidence data would still have
a few dominant features. For Observation 3, since these inflated rare features do not usually appear
in more than one model and are not useful for classification, this observation is not changed either.
For Observation 4, following the previous reasoning, models with similar features would still make
similar mistakes.
Note that in our experiments, 30 is the minimum number of principal components required to retain
the full accuracy on CIFAR10 although this can change depending on the dataset (note that the plots
are almost identical to Figure 2 and Figure 3). Since more principal components only affect the rare
features, the effect of moderate overcounting is not significant. For the main experiments, we chose
50 principal components that are sufficient for retaining the full classification accuracy on the test
data but not too large.
33

Published as a conference paper at ICLR 2024
(a)
(b)
(c)
(d)
(e)
Figure 18: γcorr = 0.5
(a)
(b)
(c)
(d)
(e)
Figure 19: γcorr = 0.7
F.11
PROGRESSION OF FEATURES AT DIFFERENT DEPTH DEPTH
In the main text, we studied the distribution of features at the input of the final linear of the model.
As we mentioned earlier, this choice is to avoid superposition (Elhage et al., 2022) where a model
leverages the non-linearity to model multiple features using a few features. This phenomena cannot
happen at the final layer since only the linear component of the representation can be used to make
decision. In general, we want to emphasize that features are not always well-defined object. In all but
the simplest cases can we say what the feature of an input is concretely.
34

Published as a conference paper at ICLR 2024
(a)
(b)
(c)
(d)
(e)
Figure 20: 45k using 30 principal components per model.
In fact, it is not particularly productive to talk about features without talking about the model used to
classify the data. One may be interested in resorting to human interpretation but the human cognition
is in itself a classifier with features, and a machine learning model may or may not have the inductive
bias to automatically learn the same features. For example, humans may think the face of a puppy
is the defining feature of a puppy, but a neural network may consider the fur or paws as the most
important features.
In a deep neural network, we often say that the model learns hierarchical features. These features are
combined to form more complicated features (Olah et al., 2017). However, this also means that not all
information present in these features is necessary for performing classification. From this perspective,
the final layer features may be viewed as the “greatest lower bound“ of this feature hierarchy as
the model can no longer compress the feature beyond the final layer and whatever features present
at this layer is the feature used to predict the classes. As we can see in Figure 10 and Figure 11,
these features do not correspond to human perception in an obvious way and are thus not highly
interpretable.
That being said, looking at the feature distribution at different depths of the model could reveal
interesting observations about the evolution of features within the model at a macro level. The
ResNet18 architecture we used can be broken down into 4 residual blocks. We refer to the features at
the output of each block as blockn where n is the index of the layer (e.g., the results in the main
paper would be block4). In Figure 21, 22 and 23, we present the feature distribution and associated
artifacts for block1, block2 and block3. We observe that O.1, O.3 and O.4 generally still
hold across different depth. However, the separation in data with different confidence (O.2) tends to
happen at deeper layers. In the earlier layers, the features of data at different confidence levels are not
distinguishable, but this separation becomes more pronounced at deeper layers (subplot (b) and (c)).
In other words, the representations for data with high confidence become increasingly similar and
compressed the deeper they are into the model. The rare data that are less compressible are encoded
with additional dedicated features (rare features) that are not used for the dominant data.
35

Published as a conference paper at ICLR 2024
(a)
(b)
(c)
(d)
(e)
Figure 21: Features distribution of 45k at depth1.
(a)
(b)
(c)
(d)
(e)
Figure 22: Features distribution of 45k at depth2.
36

Published as a conference paper at ICLR 2024
(a)
(b)
(c)
(d)
(e)
Figure 23: Features distribution of 45k at depth3.
37

