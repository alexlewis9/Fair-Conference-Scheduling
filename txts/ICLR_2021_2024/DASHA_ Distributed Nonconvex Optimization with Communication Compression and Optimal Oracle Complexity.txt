Published as a conference paper at ICLR 2023
DASHA: DISTRIBUTED NONCONVEX OPTIMIZATION
WITH COMMUNICATION COMPRESSION AND OPTIMAL
ORACLE COMPLEXITY
Alexander Tyurin
KAUST
Saudi Arabia
alexandertiurin@gmail.com
Peter Richtárik
KAUST
Saudi Arabia
richtarik@gmail.com
ABSTRACT
We develop and analyze DASHA: a new family of methods for nonconvex dis-
tributed optimization problems. When the local functions at the nodes have a
ﬁnite-sum or an expectation form, our new methods, DASHA-PAGE, DASHA-MVR
and DASHA-SYNC-MVR, improve the theoretical oracle and communication com-
plexity of the previous state-of-the-art method MARINA by Gorbunov et al. (2020).
In particular, to achieve an ε-stationary point, and considering the random sparsiﬁer
RandK as an example, our methods compute the optimal number of gradients
O (
√m/ε√n) and O (σ/ε
3/2n) in ﬁnite-sum and expectation form cases, respectively,
while maintaining the SOTA communication complexity O (d/ε√n). Furthermore,
unlike MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send
compressed vectors only, which makes them more practical for federated learning.
We extend our results to the case when the functions satisfy the Polyak-Łojasiewicz
condition. Finally, our theory is corroborated in practice: we see a signiﬁcant
improvement in experiments with nonconvex classiﬁcation and training of deep
learning models.
1
INTRODUCTION
Nonconvex optimization problems are widespread in modern machine learning tasks, especially with
the rise of the popularity of deep neural networks (Goodfellow et al., 2016). In the past years, the
dimensionality of such problems has increased because this leads to better quality (Brown et al.,
2020) and robustness (Bubeck & Sellke, 2021) of the deep neural networks trained this way. Such
huge-dimensional nonconvex problems need special treatment and efﬁcient optimization methods
(Danilova et al., 2020).
Because of their high dimensionality, training such models is a computationally intensive undertaking
that requires massive training datasets (Hestness et al., 2017), and parallelization among several
compute nodes1 (Ramesh et al., 2021). Also, the distributed learning paradigm is a necessity in
federated learning (Koneˇcný et al., 2016), where, among other things, there is an explicit desire to
secure the private data of each client.
Unlike in the case of classical optimization problems, where the performance of algorithms is deﬁned
by their computational complexity (Nesterov, 2018), distributed optimization algorithms are typically
measured in terms of the communication overhead between the nodes since such communication
is often the bottleneck in practice (Koneˇcný et al., 2016; Wang et al., 2021). Many approaches
tackle the problem, including managing communication delays (Vogels et al., 2021), ﬁghting with
stragglers (Li et al., 2020a), and optimization over time-varying directed graphs (Nedi´c & Olshevsky,
2014). Another popular way to alleviate the communication bottleneck is to use lossy compression
of communicated messages (Alistarh et al., 2017; Mishchenko et al., 2019; Gorbunov et al., 2021;
Szlendak et al., 2021). In this paper, we focus on this last approach.
1Alternatively, we sometimes use the terms: machines, workers and clients.
1

Published as a conference paper at ICLR 2023
1.1
PROBLEM FORMULATION
In this work, we consider the optimization problem
min
x∈Rd
(
f(x) := 1
n
n
X
i=1
fi(x)
)
,
(1)
where fi : Rd →R is a smooth nonconvex function for all i ∈[n] := {1, . . . , n}. Moreover, we
assume that the problem is solved by n compute nodes, with the ith node having access to function fi
only, via an oracle. Communication is facilitated by an orchestrating server able to communicate
with all nodes. Our goal is to ﬁnd an ε-solution (ε-stationary point) of (1): a (possibly random) point
bx ∈Rd, such that E
h
∥∇f(bx)∥2i
≤ε.
1.2
GRADIENT ORACLES
We consider all of the following structural assumptions about the functions {fi}n
i=1, each with its
own natural gradient oracle:
1. Gradient Setting. The ith node has access to the gradient ∇fi : Rd →Rd of function fi.
2. Finite-Sum Setting. The functions {fi}n
i=1 have the ﬁnite-sum form
fi(x) = 1
m
m
X
j=1
fij(x),
∀i ∈[n],
(2)
where fij : Rd →R is a smooth nonconvex function for all j ∈[m]. For all i ∈[n], the ith node has
access to a mini-batch of B gradients, 1
B
P
j∈Ii ∇fij(·), where Ii is a multi-set of i.i.d. samples of
the set [m], and |Ii| = B.
3. Stochastic Setting. The function fi is an expectation of a stochastic function,
fi(x) = Eξ [fi(x; ξ)] ,
∀i ∈[n],
(3)
where fi : Rd × Ωξ →R. For a ﬁxed x ∈R, fi(x; ξ) is a random variable over some distribution
Di, and, for a ﬁxed ξ ∈Ωξ, fi(x; ξ) is a smooth nonconvex function. The ith node has access to a
mini-batch of B stochastic gradients 1
B
PB
j=1 ∇fi(·; ξij) of the function fi through the distribution
Di, where {ξij}B
j=1 is a collection of i.i.d. samples from Di.
1.3
ORACLE COMPLEXITY
In this paper, the oracle complexity of a method is the number of (stochastic) gradient calculations
per node to achieve an ε-solution. Every considered method performs some number T of communi-
cations rounds to get an ε-solution; thus, if every node (on average) calculates B gradients in each
communication round, then the oracle complexity equals O (Binit + BT) , where Binit is the number
of gradient calculations in the initialization phase of a method.
1.4
UNBIASED COMPRESSORS
The method proposed in this paper is based on unbiased compressors – a family of stochastic
mappings with special properties that we deﬁne now.
Deﬁnition 1.1. A stochastic mapping C : Rd →Rd is an unbiased compressor if there exists ω ∈R
such that
E [C(x)] = x,
E
h
∥C(x) −x∥2i
≤ω ∥x∥2 ,
∀x ∈Rd.
(4)
We denote this class of unbiased compressors as U(ω).
One can ﬁnd more information about unbiased compressors in (Beznosikov et al., 2020; Horváth
et al., 2019). The purpose of such compressors is to quantize or sparsify the communicated vectors in
order to increase the communication speed between the nodes and the server. Our methods will work
collection of stochastic mappings {Ci}n
i=1 satisfying the following assumption.
Assumption 1.2. Ci ∈U(ω) for all i ∈[n], and the compressors are independent.
2

Published as a conference paper at ICLR 2023
Table 1: General Nonconvex Case. The number of communication rounds (iterations) and the oracle
complexity of algorithms to get an ε-solution (E
h
∥∇f(bx)∥2i
≤ε), and the necessity (or not) of
algorithms to send non-compressed vectors periodically (see Section 3).
Setting
Method
T := # Communication Rounds(a) Oracle Complexity
Full?(b)
Gradient
MARINA
1+ω/√n
ε
T
Yes
DASHA (Cor. 6.2)
1+ω/√n
ε
T
No
Finite-Sum
(2)
VR-MARINA
1+ω/√n
ε
+
√
(1+ω)m
ε√nB
m + BT
Yes
DASHA-PAGE (Cor. 6.5)
1+ω/√n
ε
+
√m
ε√nB
m + BT
No
Stochastic
(3)
VR-MARINA (online)
1+ω/√n
ε
+
σ2
εnB +
√1+ωσ
ε3/2nB
Bω + BT
Yes
DASHA-MVR (Cor. 6.8)
1+ω/√n
ε
+
σ2
εnB +
σ
ε3/2nB
Bω
q
σ2
εnB
(c)
+ BT
No
DASHA-SYNC-MVR (Cor. 6.10)
1+ω/√n
ε
+
σ2
εnB +
σ
ε3/2nB
Bω + BT
Yes
(a) Only dependencies w.r.t. the following variables are shown: ω = quantization parameter, n = # of nodes,
m = # of local functions (only in ﬁnite-sum case (2)), σ2 = variance of stochastic gradients (only in stochastic case
(3)), B = batch size (only in ﬁnite-sum and stochastic case). To simplify bounds, we assume that ω +1 = Θ (d/ζC) ,
where d is dimension of x in (1) and ζC is the expected number of nonzero coordinates that each compressor Ci
returns (see Deﬁnition 1.3).
(b) Does the algorithm periodically send full (non-compressed) vectors? (see Section 3)
(c) One can always choose the parameter of RandK such that this term does not dominate (see Section 6.5).
1.5
COMMUNICATION COMPLEXITY
The quantity below characterizes the number of nonzero coordinates that a compressor C returns.
This notion is useful in case of sparsiﬁcation compressors.
Deﬁnition 1.3. The expected density of the compressor Ci is ζCi := supx∈Rd E [∥Ci(x)∥0], where
∥x∥0 is the number of nonzero components of x ∈Rd. Let ζC = maxi∈[n] ζCi.
In this paper, the communication complexity of a method is the number of coordinates sent to the
server per node to achieve an ε-solution. If every node (on average) sends ζ coordinates in each
communication round, then the communication complexity equals O (ζinit + ζT) , where T is the
number of communication rounds, and ζinit is the number of coordinates sent in the initialization
phase.
We would like to notice that the established communication complexities are compared to previous
upper bounds from (Gorbunov et al., 2021; Szlendak et al., 2021; Mishchenko et al., 2019; Alistarh
et al., 2017), and in this line of work, the comparisons of the communication complexities are made
with respect to the number of send coordinates. As far as we know, in this sense, no lower bounds
are proved, and it deserves a separate piece of work. However, Korhonen & Alistarh (2021) proved
the lower bounds of the communication complexity with respect to the number of send bits in the
constraint optimization setting that x ∈[0, 1]d, so our upper bounds can not be directly compared to
their result because we operate on a different level of abstraction.
2
RELATED WORK
• Uncompressed communication. This line of work is characterized by methods in which the nodes
send messages (vectors) to the server without any compression. In the ﬁnite-sum setting, the current
state-of-the-art methods were proposed by Sharma et al. (2019); Li et al. (2021b), showing that after
O (1/ε) communication rounds and
O

m +
√m
ε√n

(5)
3

Published as a conference paper at ICLR 2023
Table 2: Polyak-Łojasiewicz Case. The number of communications rounds (iterations) and oracle
complexity of algorithms to get an ε-solution (E [f(bx)] −f ∗≤ε), and the necessity (or not) of
algorithms to send non-compressed vectors periodically.
Setting
Method
T := # Communication Rounds (a)
Oracle Complexity
Full?(b)
Gradient
MARINA
ω + L(1+ω/√n)
µ
T
Yes
DASHA (Cor. I.10)
ω + L(1+ω/√n)
µ
T
No
Finite-Sum
(2)
VR-MARINA
ω + m
B + L(1+ω/√n)
µ
+
L√
(1+ω)m
µ√nB
BT
Yes
DASHA-PAGE (Cor. I.13)
ω + m
B + L(1+ω/√n)
µ
+
L√m
µ√nB
BT
No
Stochastic
(3)
VR-MARINA (online)
ω + L(1+ω/√n)
µ
+
σ2
µεnB +
√1+ωLσ
µ3/2√εnB
BT
Yes
DASHA-MVR (Cor. I.16)
ω + ω
q
σ2
µεnB
(c)
+ L(1+ω/√n)
µ
+
σ2
µεnB +
Lσ
µ3/2√εnB
BT
No
DASHA-SYNC-MVR (Cor. I.21)
ω + L(1+ω/√n)
µ
+
σ2
µεnB +
Lσ
µ3/2√εnB
BT
Yes
(a) Logarithmic factors are omitted and only dependencies w.r.t. the following variables are shown: L = the worst case smoothness
constant, µ = PŁ constant, ω = quantization parameter, n = # of nodes, m = # of local functions (only in ﬁnite-sum case (2)),
σ2 = variance of stochastic gradients (only in stochastic case (3)), B = batch size (only in ﬁnite-sum and stochastic case). To
simplify bounds, we assume that ω + 1 = Θ (d/ζC) , where d is dimension of x in (1) and ζC is the expected number of nonzero
coordinates that each compressor Ci returns (see Deﬁnition 1.3).
(b) Does the algorithm periodically send full (non-compressed) vectors? (see Section 3)
(c) One can always choose the parameter of RandK such that this term does not dominate (see Section 6.5).
calculations of ∇fij per node, these methods can return an ε-solution. Moreover, Sharma et al. (2019)
show that the same can be done in the stochastic setting after
O
σ2
εn +
σ
ε
3/2n

(6)
stochastic gradient calculations per node. Note that complexities (5) and (6) are optimal (Arjevani
et al., 2019; Fang et al., 2018; Li et al., 2021a). An adaptive variant was proposed by Khanduri et al.
(2020) based on the work of Cutkosky & Orabona (2019). See also (Khanduri et al., 2021; Murata &
Suzuki, 2021).
• Compressed communication. In practice, it is rarely affordable to send uncompressed messages
(vectors) from the nodes to the server due to limited communication bandwidth. Because of this,
researchers started to develop methods keeping in mind the communication complexity: the total
number of coordinates/ﬂoats/bits that the nodes send to the server to ﬁnd an ε-solution. Two important
families of compressors are investigated in the literature to reduce communication bottleneck: biased
and unbiased compressors. While unbiased compressors are superior in theory (Mishchenko et al.,
2019; Li et al., 2020b; Gorbunov et al., 2021), biased compressors often enjoy better performance
in practice (Beznosikov et al., 2020; Xu et al., 2020). Recently, Richtárik et al. (2021) developed
EF21, which is the ﬁrst method capable of working with biased compressors an having the theoretical
iteration complexity of gradient descent (GD), up to constant factors.
• Unbiased compressors. The theory around unbiased compressors is much more optimistic.
Alistarh et al. (2017) developed the QSGD method providing convergence rates of stochastic gradient
method with quantized vectors. However, the nonstrongly convex case was analyzed under the
strong assumption that all nodes have identical functions, and the stochastic gradients have bounded
second moment. Next, Mishchenko et al. (2019); Horváth et al. (2019) proposed the DIANA method
and proved convergence rates without these restrictive assumptions. Also, distributed nonconvex
optimization methods with compression were developed by Haddadpour et al. (2021); Das et al.
(2020). Finally, Gorbunov et al. (2021) proposed MARINA – the current state-of-the-art distributed
method in terms of theoretical communication complexity, inspired by the PAGE method of Li et al.
(2021a).
3
CONTRIBUTIONS
We develop a new family of distributed optimization methods DASHA for nonconvex optimization
problems with unbiased compressors. Compared to MARINA, our methods make more practical and
4

Published as a conference paper at ICLR 2023
simpler optimization steps. In particular, in MARINA, all nodes simultaneously send either compressed
vectors, with some probability p, or the gradients of functions {fi}n
i=1 (uncompressed vectors), with
probability 1−p. In other words, the server periodically synchronizes all nodes. In federated learning,
where some nodes can be inaccessible for a long time, such periodic synchronization is intractable.
Our method DASHA solves both problems: i) the nodes always send compressed vectors, and ii) the
server never synchronizes all nodes in the gradient setting.
Further, a simple tweak in the compressors (see Appendix D) results in support for partial participa-
tion in the gradient setting , which makes DASHA more practical for federated learning tasks. Let us
summarize our most important theoretical and practical contributions:
• New theoretical SOTA complexity in the ﬁnite-sum setting. Using our novel approach to
compress gradients, we improve the theoretical complexities of VR-MARINA (see Tables 1 and 2) in
the ﬁnite-sum setting. Indeed, if the number of functions m is large, our algorithm DASHA-PAGE
needs √ω + 1 times fewer communications rounds, while communicating compressed vectors only.
• New theoretical SOTA complexity in the stochastic setting. We develop a new method, DASHA-
SYNC-MVR, improving upon the previous state of the art (see Table 1). When ε is small, the number
of communication rounds is reduced by a factor of √ω + 1. Indeed, we improve the dominant term
which depends on ε
3/2 (the other terms depend on ε only). However, DASHA-SYNC-MVR needs to
periodically send uncompressed vectors with the same rate as VR-MARINA (online). Nevertheless,
we show that DASHA-MVR also improves the dominant term when ε is small, and this method
sends compressed vectors only. Moreover, we provide detailed experiments on practical machine
learning tasks: training nonconvex generalized linear models and deep neural networks, showing
improvements predicted by our theory. See Appendix A.
• Closing the gap between uncompressed and compressed methods. In Section 2, we mentioned
that the optimal oracle complexities of methods without compression in the ﬁnite-sum and stochastic
settings are (5) and (6), respectively. Considering the RandK compressor (see Deﬁnition F.1),
we show that DASHA-PAGE, DASHA-MVR and DASHA-SYNC-MVR attain these optimal oracle
complexities while attainting the state-of-the-art communication complexity as MARINA, which needs
to use the stronger gradient oracle! Therefore, our new methods close the gap between results from
(Gorbunov et al., 2021) and (Sharma et al., 2019; Li et al., 2021b).
4
ALGORITHM DESCRIPTION
We now describe our proposed family of optimization methods, DASHA (see Algorithm 1). DASHA
is inspired by MARINA and momentum variance reduction methods (MVR) (Cutkosky & Orabona,
2019; Tran-Dinh et al., 2021; Liu et al., 2020): the general structure repeats MARINA except for the
variance reduction strategy, which we borrow from MVR. Unlike MARINA, our algorithm never sends
uncompressed vectors, and the number of bits that every node sends is always the same. Moreover,
we reduce the variance from the oracle and the compressor separately, which helps us to improve the
theoretical convergence rates in the stochastic and ﬁnite-sum cases.
First, using the gradient estimator gt, the server in each communication round calculates the next
point xt+1 and broadcasts it to the nodes. Subsequently, all nodes in parallel calculate vectors ht+1
i
in one of three ways, depending on the available oracle. For the the gradient, ﬁnite-sum, and the
stochastic settings, we use GD-like, PAGE-like, and MVR-like strategies, respectively. Next, each
node compresses their message and uploads it to the server. Finally, the server aggregates all received
messages and calculates the next vector gt+1. We refer to Section H to get a better intuition about
DASHA.
We note that in the stochastic setting, our analysis of DASHA-MVR (Algorithm 1) provides a subopti-
mal oracle complexity w.r.t. ω (see Tables 1 and 2). In Appendix J we provide experimental evidence
that our analysis is tight. For this reason, we developed DASHA-SYNC-MVR (see Algorithm 2 in
Appendix C) that improves the previous state-of-the-art results and sends non-compressed vectors
with the same rate as VR-MARINA (online). Note that DASHA-MVR still enjoys the optimal oracle and
SOTA communication complexity (see Section 6.5); and this can be seen it in experiments.
5

Published as a conference paper at ICLR 2023
Algorithm 1 DASHA
1: Input: starting point x0 ∈Rd, stepsize γ > 0, momentum a ∈(0, 1], momentum b ∈(0, 1]
(only in DASHA-MVR), probability p ∈(0, 1] (only in DASHA-PAGE), batch size B (only in
DASHA-PAGE and DASHA-MVR), number of iterations T ≥1
2: Initialize g0
i ∈Rd, h0
i ∈Rd on the nodes and g0 = 1
n
Pn
i=1 g0
i on the server
3: for t = 0, 1, . . . , T −1 do
4:
xt+1 = xt −γgt
5:
Flip a coin ct+1 =
1,
with probability p
0,
with probability 1 −p (only in DASHA-PAGE)
6:
Broadcast xt+1 to all nodes
7:
for i = 1, . . . , n in parallel do
8:
ht+1
i
=













∇fi(xt+1)
(DASHA)
(
∇fi(xt+1)
if ct+1 = 1
ht
i + 1
B
P
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

if ct+1 = 0
(DASHA-PAGE)
1
B
PB
j=1 ∇fi(xt+1; ξt+1
ij ) + (1 −b)

ht
i −1
B
PB
j=1 ∇fi(xt; ξt+1
ij )

(DASHA-MVR)
9:
mt+1
i
= Ci
 ht+1
i
−ht
i −a (gt
i −ht
i)

10:
gt+1
i
= gt
i + mt+1
i
11:
Send mt+1
i
to the server
12:
end for
13:
gt+1 = gt + 1
n
Pn
i=1 mt+1
i
14: end for
15: Output: ˆxT chosen uniformly at random from {xt}T −1
k=0 (or xT under the PŁ-condition)
5
ASSUMPTIONS
We now provide the assumptions used throughout our paper.
Assumption 5.1. There exists f ∗∈R such that f(x) ≥f ∗for all x ∈R.
Assumption 5.2. The function f is L–smooth, i.e.,
∥∇f(x) −∇f(y)∥≤L ∥x −y∥
for all x, y ∈Rd.
Assumption 5.3. For all i ∈[n], the function fi is Li–smooth.2 We deﬁne bL2 := 1
n
Pn
i=1 L2
i .
The next assumption is used in the ﬁnite-sum setting (2).
Assumption 5.4. For all i ∈[n], j ∈[m], the function fij is Lij-smooth.
Let Lmax :=
maxi∈[n],j∈[m] Lij.
The two assumptions below are provided for the stochastic setting (3).
Assumption 5.5. For all i ∈[n] and for all x ∈Rd, the stochastic gradient ∇fi(x; ξ) is unbiased
and has bounded variance, i.e.,
Eξ [∇fi(x; ξ)] = ∇fi(x),
and
Eξ
h
∥∇fi(x; ξ) −∇fi(x)∥2i
≤σ2,
where σ2 ≥0.
Assumption 5.6. For all i ∈[n] and for all x, y ∈R, the stochastic gradient ∇fi(x; ξ) satisﬁes the
mean-squared smoothness property, i.e.,
Eξ
h
∥∇fi(x; ξ) −∇fi(y; ξ) −(∇fi(x) −∇fi(y))∥2i
≤L2
σ ∥x −y∥2 .
2Note that one can always take L2 = bL2 := 1
n
Pn
i=1 L2
i . However, the optimal constant L can be much
better because L2 ≤
  1
n
Pn
i=1 Li
2 ≤1
n
Pn
i=1 L2
i .
6

Published as a conference paper at ICLR 2023
6
THEORETICAL CONVERGENCE RATES
Now, we provide convergence rate theorems for DASHA, DASHA-PAGE and DASHA-MVR. All three
methods are listed in Algorithm 1 and differ in Line 8 only. At the end of the section, we provide a
theorem for DASHA-SYNC-MVR.
6.1
GRADIENT SETTING (DASHA)
Theorem 6.1. Suppose that Assumptions 5.1, 5.2, 5.3 and 1.2 hold. Let us take a = 1/ (2ω + 1)
, γ ≤

L +
q
16ω(2ω+1)
n
bL
−1
, and g0
i = h0
i = ∇fi(x0) for all i ∈[n] in Algorithm 1 (DASHA),
then E
h∇f(bxT )
2i
≤
2(f(x0)−f ∗)
γT
.
The corollary below simpliﬁes the previous theorem and reveals the communication complexity of
DASHA.
Corollary 6.2. Suppose that assumptions from Theorem 6.1 hold, and g0
i = h0
i = ∇fi(x0) for all
i ∈[n], then DASHA needs T := O
 
1
ε
"
 f(x0) −f ∗ 
L +
ω
√n bL
 #!
communication rounds
to get an ε-solution and the communication complexity is equal to O (d + ζCT) , where ζC is the
expected density from Deﬁnition 1.3.
In the previous corollary, we have free parameters ω and ζC. Now, we consider the RandK compressor
(see Deﬁnition F.1) and choose its parameters to get the communication complexity w.r.t. only d and
n.
Corollary 6.3. Suppose that assumptions of Corollary 6.2 hold. We take the unbiased compressor
RandK with K = ζC ≤d/√n, then the communication complexity equals O

d +
bL(f(x0)−f ∗)d
ε√n

.
6.2
FINITE-SUM SETTING (DASHA-PAGE)
Next, we provide the complexity bounds for DASHA-PAGE.
Theorem 6.4. Suppose that Assumptions 5.1, 5.2, 5.3, 5.4, and 1.2 hold. Let us take a = 1/ (2ω + 1),
probability p ∈(0, 1],
γ ≤
 
L +
s
48ω (2ω + 1)
n
(1 −p)L2max
B
+ bL2

+ 2 (1 −p) L2max
pnB
!−1
and g0
i = h0
i = ∇fi(x0) for all i ∈[n] in Algorithm 1 (DASHA-PAGE) then E
h∇f(bxT )
2i
≤
2(f(x0)−f ∗)
γT
.
Let us simplify the statement of Theorem 6.4 by choosing particular parameters.
Corollary 6.5. Let the assumptions from Theorem 6.4 hold, p = B/(m+B), and g0
i = h0
i = ∇fi(x0)
for all i ∈[n]. Then DASHA-PAGE needs
T := O




1
ε


 f(x0) −f ∗ 
L + ω
√n
bL +
 ω
√n +
r m
nB
 Lmax
√
B







communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the expected # of gradient calculations per node equals O (m + BT) , where ζC is the expected
density from Deﬁnition 1.3.
The corollary below reveals the communication and oracle complexities of Algorithm 1 (DASHA-
PAGE) with RandK.
7

Published as a conference paper at ICLR 2023
Corollary 6.6. Suppose that assumptions of Corollary 6.5 hold, B ≤
p
m/n, and we use the unbiased
compressor RandK with K = ζC = Θ (Bd/√m) . Then the communication complexity of Algorithm 1
is
O
 
d + Lmax
 f(x0) −f ∗
d
ε√n
!
,
(7)
and the expected # of gradient calculations per node equals
O
 
m + Lmax
 f(x0) −f ∗ √m
ε√n
!
.
(8)
Up to Lipschitz constants factors, bound (8) is optimal (Fang et al., 2018; Li et al., 2021a), and unlike
VR-MARINA, we recover the optimal bound with compression! At the same time, the communication
complexity (7) is the same as in DASHA (see Corollary 6.3) or MARINA.
6.3
STOCHASTIC SETTING (DASHA-MVR)
Let ht := 1
n
Pn
i=1 ht
i. This vector is not used in Algorithm 1, but appears in the theoretical results.
Theorem 6.7. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2ω+1,
b ∈(0, 1], γ ≤

L +
r
96ω(2ω+1)
n

(1−b)2L2σ
B
+ bL2

+ 4(1−b)2L2σ
bnB
−1
, and g0
i = h0
i for all i ∈[n]
in Algorithm 1 (DASHA-MVR). Then
E
h∇f(bxT )
2i
≤1
T


2
 f(x0) −f ∗
γ
+ 2
b
h0 −∇f(x0)
2
+ 32bω (2ω + 1)
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!

+
96ω (2ω + 1)
nB
+
4
bnB

b2σ2.
Corollary 6.8.
Suppose that assumptions from Theorem 6.7 hold,
momentum b
=
Θ

min
n
1
ω
q
nεB
σ2 , nεB
σ2
o
, and g0
i = h0
i =
1
Binit
PBinit
k=1 ∇fi(x0; ξ0
ik) for all i ∈[n], and batch
size Binit = Θ (B/b) , then Algorithm 1 (DASHA-MVR) needs
T := O




1
ε


 f(x0) −f ∗
 
L + ω
√n
bL +
 
ω
√n +
r
σ2
εn2B
!
Lσ
√
B
!

+ σ2
nεB




communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where ζC is the
expected density from Deﬁnition 1.3.
The following corollary reveals the communication and oracle complexity of DASHA-MVR.
Corollary 6.9. Suppose that assumptions of Corollary 6.8 hold, batch size B ≤
σ
√εn, we take RandK
with K = ζC = Θ

Bd√εn
σ

, and eL := max{L, Lσ, bL}. Then the communication complexity equals
O
 
dσ
√nε +
eL
 f(x0) −f ∗
d
√nε
!
,
(9)
and the expected # of stochastic gradient calculations per node equals
O
 
σ2
nε +
eL
 f(x0) −f ∗
σ
ε
3/2n
!
.
(10)
8

Published as a conference paper at ICLR 2023
Up to Lipschitz constant factors, the bound (10) is optimal (Arjevani et al., 2019; Sharma et al., 2019),
and unlike VR-MARINA (online), we recover the optimal bound with compression! At the same time,
the communication complexity (9) is the same as in DASHA (see Corollary 6.3) or MARINA for small
enough ε.
6.4
STOCHASTIC SETTING (DASHA-SYNC-MVR)
We now provide the complexities of Algorithm 2 (DASHA-SYNC-MVR) presented in Appendix C.
The main convergence rate Theorem I.19 is in the appendix.
Corollary 6.10.
Suppose that assumptions from Theorem I.19 hold,
probability p
=
min
n
ζC
d , nεB
σ2
o
, batch size B′ = Θ

σ2
nε

and h0
i = g0
i =
1
Binit
PBinit
k=1 ∇fi(x0; ξ0
ik) for all i ∈[n],
initial batch size Binit = Θ

max
n
σ2
nε, B d
ζC
o
, then DASHA-SYNC-MVR needs
T := O




1
ε


 f(x0) −f ∗
 
L + ω
√n
bL +
 
ω
√n +
s
d
ζCn +
r
σ2
εn2B
!
Lσ
√
B
!

+ σ2
nεB




communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where ζC is the
expected density from Deﬁnition 1.3.
Corollary 6.11. Suppose that assumptions of Corollary 6.10 hold, batch size B ≤
σ
√εn, we take
RandK with K = ζC = Θ

Bd√εn
σ

, and eL := max{L, Lσ, bL}. Then the communication complex-
ity equals
O
 
dσ
√nε +
eL
 f(x0) −f ∗
d
√nε
!
,
(11)
and the expected # of stochastic gradient calculations per node equals
O
 
σ2
nε +
eL
 f(x0) −f ∗
σ
ε
3/2n
!
.
(12)
Up to Lipschitz constant factors, the bound (12) is optimal (Arjevani et al., 2019; Sharma et al., 2019),
and unlike VR-MARINA (online), we recover the optimal bound with compression! At the same time,
the communication complexity (11) is the same as in DASHA (see Corollary 6.3) or MARINA for
small enough ε.
6.5
COMPARISON OF DASHA-MVR AND DASHA-SYNC-MVR
Let us consider RandK (note that ω + 1 = d/ζC). Comparing Corollary 6.8 to Corollary 6.10
(see Table 1), we see that DASHA-SYNC-MVR improved the size of the initial batch from
Θ

max

σ2
nε, Bω
q
σ2
nεB

to Θ

max
n
σ2
nε, Bω
o
. Fortunately, we can control the parameter K
in RandK, and Corollary 6.9 reveals that we can take K = Θ

Bd√εn
σ

, and get Bω
q
σ2
nεB ≤σ2
nε.
As a result, the “bad term” does not dominate the oracle complexity, and DASHA-MVR attains the
optimal oracle and SOTA communication complexity. The same reasoning applies to optimization
problems under PŁ-condition with K = Θ

Bd√µεn
σ

.
ACKNOWLEDGEMENTS
The work of P. Richtárik was partially supported by the KAUST Baseline Research Fund Scheme
and by the SDAIA-KAUST Center of Excellence in Data Science and Artiﬁcial Intelligence. The
work of A. Tyurin was supported by the Extreme Computing Research Center (ECRC) at KAUST.
9

Published as a conference paper at ICLR 2023
REFERENCES
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efﬁcient SGD via gradient quantization and encoding. In Advances in Neural Information Process-
ing Systems (NIPS), pp. 1709–1720, 2017.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Aleksandr Beznosikov, Samuel Horváth, Peter Richtárik, and Mher Safaryan. On biased compression
for distributed learning. arXiv preprint arXiv:2002.12410, 2020.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Sébastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. arXiv preprint
arXiv:2105.12806, 2021.
Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology (TIST), 2(3):1–27, 2011.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD.
arXiv preprint arXiv:1905.10018, 2019.
Marina Danilova, Pavel Dvurechensky, Alexander Gasnikov, Eduard Gorbunov, Sergey Guminov,
Dmitry Kamzolov, and Innokentiy Shibaev. Recent theoretical advances in non-convex optimiza-
tion. arXiv preprint arXiv:2012.06188, 2020.
Rudrajit Das, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S Dhillon. Improved convergence
rates for non-convex federated learning with compression. arXiv e-prints, pp. arXiv–2012, 2020.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. SPIDER: Near-optimal non-convex op-
timization via stochastic path integrated differential estimator. In NeurIPS Information Processing
Systems, 2018.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. MARINA: Faster non-
convex distributed learning with compression. In 38th International Conference on Machine
Learning, 2021.
Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated
learning with compression: Uniﬁed analysis and sharp guarantees. In International Conference on
Artiﬁcial Intelligence and Statistics, pp. 2350–2358. PMLR, 2021.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778, 2016.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.
Samuel Horváth, Chen-Yu Ho, L’udovít Horváth, Atal Narayan Sahu, Marco Canini, and Peter
Richtárik. Natural compression for distributed deep learning. arXiv preprint arXiv:1905.10988,
2019.
Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtárik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019.
10

Published as a conference paper at ICLR 2023
Prashant Khanduri, Pranay Sharma, Swatantra Kaﬂe, Saikiran Bulusu, Ketan Rajawat, and Pramod K
Varshney. Distributed stochastic non-convex optimization: Momentum-based variance reduction.
arXiv preprint arXiv:2005.00224, 2020.
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan Rajawat, and Pramod
Varshney. STEM: A stochastic two-sided momentum algorithm achieving near-optimal sample and
communication complexities for federated learning. Advances in Neural Information Processing
Systems, 34, 2021.
Jakub Koneˇcný, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efﬁciency. arXiv
preprint arXiv:1610.05492, 2016.
Janne H Korhonen and Dan Alistarh. Towards tight communication lower bounds for distributed
optimisation. Advances in Neural Information Processing Systems, 34:7254–7266, 2021.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, Toronto, 2009.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems,
2:429–450, 2020a.
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtárik. Acceleration for compressed gradient
descent in distributed and federated optimization.
In International Conference on Machine
Learning, 2020b.
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtárik. PAGE: A simple and optimal
probabilistic gradient estimator for nonconvex optimization. In International Conference on
Machine Learning, pp. 6286–6295. PMLR, 2021a.
Zhize Li, Slavomír Hanzely, and Peter Richtárik. ZeroSARAH: Efﬁcient nonconvex ﬁnite-sum
optimization with zero full gradient computation. arXiv preprint arXiv:2103.01447, 2021b.
Deyi Liu, Lam M Nguyen, and Quoc Tran-Dinh. An optimal hybrid variance-reduced algorithm for
stochastic composite nonconvex optimization. arXiv preprint arXiv:2008.09055, 2020.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takáˇc, and Peter Richtárik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
Tomoya Murata and Taiji Suzuki. Bias-variance reduced local SGD for less heterogeneous federated
learning. arXiv preprint arXiv:2102.03198, 2021.
Angelia Nedi´c and Alex Olshevsky. Distributed optimization over time-varying directed graphs.
IEEE Transactions on Automatic Control, 60(3):601–615, 2014.
Yurii Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.
Peter Richtárik, Igor Sokolov, and Ilyas Fatkhullin. EF21: A new, simpler, theoretically better, and
practically faster error feedback. arXiv preprint arXiv:2106.05203, 2021.
Pranay Sharma, Swatantra Kaﬂe, Prashant Khanduri, Saikiran Bulusu, Ketan Rajawat, and Pramod K
Varshney. Parallel restarted SPIDER–communication efﬁcient distributed nonconvex optimization
with optimal computation complexity. arXiv preprint arXiv:1912.06036, 2019.
Rafał Szlendak, Alexander Tyurin, and Peter Richtárik. Permutation compressors for provably faster
distributed nonconvex optimization. arXiv preprint arXiv:2110.03300, 2021.
11

Published as a conference paper at ICLR 2023
Quoc Tran-Dinh, Nhan H Pham, Dzung T Phan, and Lam M Nguyen. A hybrid stochastic optimization
framework for composite nonconvex optimization. Mathematical Programming, pp. 1–67, 2021.
Thijs Vogels, Lie He, Anastasiia Koloskova, Sai Praneeth Karimireddy, Tao Lin, Sebastian U Stich,
and Martin Jaggi. RelaySum for decentralized deep learning on heterogeneous data. Advances in
Neural Information Processing Systems, 34, 2021.
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A ﬁeld guide to federated
optimization. arXiv preprint arXiv:2107.06917, 2021.
Hang Xu, Chen-Yu Ho, Ahmed M Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos
Karatsenidis, Marco Canini, and Panos Kalnis. Compressed communication for distributed deep
learning: Survey and quantitative evaluation. Technical report, 2020.
12

Published as a conference paper at ICLR 2023
CONTENTS
1
Introduction
1
1.1
Problem formulation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Gradient oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Oracle complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.4
Unbiased compressors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.5
Communication complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Related Work
3
3
Contributions
4
4
Algorithm Description
5
5
Assumptions
6
6
Theoretical Convergence Rates
7
6.1
Gradient Setting (DASHA)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
6.2
Finite-Sum Setting (DASHA-PAGE)
. . . . . . . . . . . . . . . . . . . . . . . . .
7
6.3
Stochastic Setting (DASHA-MVR)
. . . . . . . . . . . . . . . . . . . . . . . . . .
8
6.4
Stochastic Setting (DASHA-SYNC-MVR) . . . . . . . . . . . . . . . . . . . . . . .
9
6.5
Comparison of DASHA-MVR and DASHA-SYNC-MVR . . . . . . . . . . . . . . . .
9
A Experiments
15
A.1
Gradient setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.2
Finite-sum setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.3
Stochastic setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.4
Deep neural network training . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
B
Experiments Details
18
C Description of DASHA-SYNC-MVR
19
D Partial Participation
19
E
Auxiliary Facts
19
F
Compressors Facts
20
G Polyak-Łojasiewicz Condition
20
H Intuition Behind DASHA
21
H.1
Different Sources of Contractions
. . . . . . . . . . . . . . . . . . . . . . . . . .
21
13

Published as a conference paper at ICLR 2023
H.2
The Source of Improvements in the Convergence Rates . . . . . . . . . . . . . . .
21
I
Theorems with Proofs
22
I.1
Case of DASHA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
I.2
Case of DASHA under PŁ-condition
. . . . . . . . . . . . . . . . . . . . . . . . .
28
I.3
Case of DASHA-PAGE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
I.4
Case of DASHA-PAGE under PŁ-condition . . . . . . . . . . . . . . . . . . . . . .
35
I.5
Case of DASHA-MVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
I.6
Case of DASHA-MVR under PŁ-condition . . . . . . . . . . . . . . . . . . . . . .
45
I.7
Case of DASHA-SYNC-MVR . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
I.8
Case of DASHA-SYNC-MVR under PŁ-condition
. . . . . . . . . . . . . . . . . .
55
J
Extra Experiments
58
14

Published as a conference paper at ICLR 2023
A
EXPERIMENTS
We have tested all developed algorithms on practical machine learnings problems3. Note that the goal
of our experiments is to justify the theoretical convergence rates from our paper. We compare the
new methods with MARINA on LIBSVM datasets (Chang & Lin, 2011) (under the 3-clause BSD
license) because MARINA is the only previous state-of-the-art method for the problem (1). Moreover,
we show the advantage of our method on an image recognition task with CIFAR10 (Krizhevsky et al.,
2009) and a deep neural network. In all experiments, we take parameters of algorithms predicted by
the theory (stated in the convergence rate theorems our paper and in (Gorbunov et al., 2021)), except
for the step sizes – we ﬁne-tune them using a set of powers of two {2i | i ∈[−10, 10]} – and use the
RandK compressor. We evaluate communication complexity; thus, each plot represents the relation
between the norm of a gradient or function value (vertical axis), and the total number of transmitted
bits per node (horizontal axis).
A.1
GRADIENT SETTING
We consider nonconvex functions
fi(x) := 1
m
m
X
j=1
 
1 −
1
1 + exp(yija⊤
ijx)
!2
to solve a classiﬁcation problem. Here, aij ∈Rd is the feature vector of a sample on the ith node,
yij ∈{−1, 1} is the corresponding label, and m is the number of samples on the ith node. All nodes
calculate full gradients. We take the mushrooms dataset (dimension d = 112, number of samples
equals 8124) from LIBSVM, randomly split the dataset between 5 nodes and take K = 10 in RandK.
One can see in Figure 1 that DASHA converges approximately 2 times faster.
0.0
0.2
0.4
0.6
0.8
1.0
1.2
#bit# / n
1e7
10−9
10−7
10−5
10−3
10−1
||∇f(xk)||2
Number of nodes: 5
MARINA: Step size: 0.25
MARINA: Step size: 0.5
MARINA: Step size: 1.0
DASHA: Step size: 0.25
DASHA: Step size: 0.5
DASHA: Step size: 1.0
Figure 1: Classiﬁcation task with the mushrooms dataset and gradient oracle.
3Code: https://github.com/mysteryresearcher/dasha
15

Published as a conference paper at ICLR 2023
A.2
FINITE-SUM SETTING
Now, we conduct the same experiments as in Section A.1 with real-sim dataset (dimension d =
20,958, number of samples equals 72,309) from LIBSVM in the ﬁnite-sum setting; moreover, we
compare VR-MARINA versus DASHA-PAGE with batch size B = 1 in both algorithms. Results in
Figure 2 coincide with Table 1 – our new method DASHA-PAGE converges faster than MARINA.
When K = 100, the improvement is not signiﬁcant because 1+ω/√n
ε
dominates
√m
ε√nB (see Table 1),
and both algorithms get the same theoretical convergence complexity.
0.0
0.5
1.0
1.5
#bits / n
1e8
10
6
10
5
10
4
|| f(xk)||2
K = 100
Number of nodes: 5
VR-MARINA: Step size: 0.03125
VR-MARINA: Step size: 0.0625
VR-MARINA: Step size: 0.125
DASHA-PAGE: Step size: 0.03125
DASHA-PAGE: Step size: 0.0625
DASHA-PAGE: Step size: 0.125
0
2
4
6
8
#bits / n
1e8
10
7
10
6
10
5
10
4
K = 500
VR-MARINA: Step size: 0.0625
VR-MARINA: Step size: 0.125
VR-MARINA: Step size: 0.25
DASHA-PAGE: Step size: 0.25
DASHA-PAGE: Step size: 0.5
DASHA-PAGE: Step size: 1.0
0
1
2
3
#bits / n
1e9
10
7
10
6
10
5
10
4
K = 2000
VR-MARINA: Step size: 0.25
VR-MARINA: Step size: 0.5
VR-MARINA: Step size: 1.0
DASHA-PAGE: Step size: 0.5
DASHA-PAGE: Step size: 1.0
DASHA-PAGE: Step size: 2.0
Figure 2: Classiﬁcation task with the real-sim dataset and K ∈{100; 500; 2, 000} in RandK in the
ﬁnite-sum setting.
16

Published as a conference paper at ICLR 2023
A.3
STOCHASTIC SETTING
In this experiment, we consider the following logistic regression functions with nonconvex regularizer
{fi}n
i=1 to solve a classiﬁcation problem:
fi(x1, x2) := Ej∼[m]

−log
 
exp
 a⊤
ijxyij

P
y∈{1,2} exp
 a⊤
ijxy

!
+ λ
X
y∈{1,2}
d
X
k=1
{xy}2
k
1 + {xy}2
k

,
where x1, x2 ∈Rd, {·}k is an indexing operation, aij ∈Rd is a feature of a sample on the ith
node, yij ∈{1, 2} is a corresponding label, m is the number of samples located on the ith node,
constant λ = 0.001. We take batch size B = 1 and compare VR-MARINA (online), DASHA-MVR,
and DASHA-SYNC-MVR that depend on a common ratio σ2/nεB 4. We ﬁx σ2/nεB ∈{104, 105} and
K ∈{200, 2000} in RandK compressors. We consider real-sim dataset from LIBSVM splitted
between 5 nodes. When we increase σ2/nεB from 104 to 105, we implicitly decrease ε because other
parameters are ﬁxed. In Figure 3, when ε is small, DASHA-MVR and DASHA-SYNC-MVR converge
faster than VR-MARINA (online).
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e9
10
6
10
5
10
4
|| f(xk)||2
K = 200
2
n  = 10000
VR-MARINA (online): Step size: 0.0625
DASHA-MVR: Step size: 0.0625
DASHA-SYNC-MVR: Step size: 0.125
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e10
10
6
10
5
10
4
K = 2000
VR-MARINA (online): Step size: 0.25
DASHA-MVR: Step size: 0.25
DASHA-SYNC-MVR: Step size: 0.5
0.00
0.25
0.50
0.75
1.00
1.25
1.50
#bits / n
1e10
10
7
10
6
10
5
10
4
|| f(xk)||2
2
n  = 100000
VR-MARINA (online): Step size: 0.015625
DASHA-MVR: Step size: 0.0625
DASHA-SYNC-MVR: Step size: 0.0625
0.00
0.25
0.50
0.75
1.00
1.25
1.50
#bits / n
1e11
10
7
10
6
10
5
10
4
VR-MARINA (online): Step size: 0.03125
DASHA-MVR: Step size: 0.125
DASHA-SYNC-MVR: Step size: 0.125
Figure 3: Classiﬁcation task with the real-sim dataset, σ2/nεB ∈{104, 105}, and K ∈{200, 2000}
in RandK in the stochastic setting.
4Indeed, in DASHA-SYNC-MVR and MARINA, the probability p = min{K/d, nεB/σ2}. In DASHA-MVR,
the momentum b = min{K/d
p
nεB/σ2, nεB/σ2}.
17

Published as a conference paper at ICLR 2023
A.4
DEEP NEURAL NETWORK TRAINING
Finally, we test our algorithms on an image recognition task, CIFAR10 (Krizhevsky et al., 2009),
with the ResNet-18 (He et al., 2016) deep neural network (the number of parameters d ≈107).
We split CIFAR10 among 5 nodes, and take K ≈2 · 106 in RandK. In all methods we ﬁne-
tune two parameters: step size γ ∈{0.05, 0.01, 0.005, 0.001} and ratio σ2/nεB ∈{2, 10, 20, 100}.
Moreover, we trained the neural network with SGD without compression as a baseline, with step size
γ ∈{1.0, 0.5, 0.1, 0.05, 0.01, 0.001}. All nodes have batch size B = 25.
Results are provided in Figure 4. We see that DASHA-MVR converges signiﬁcantly faster than other
algorithms in the terms of communication complexity. Moreover, DASHA-SYNC-MVR works better
than VR-MARINA (online) and SGD.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
#bits / n
1e12
10−3
1002
1001
100
f(xk) 0 f(x * )
Number )f ()des: 5
Va(illa SGD: S−e* size: 0.05
VR-MARINA (online): Step size: 0.01; Batch Size B': 10
DASHA-MVR: Step size: 0.01; Momentum b: 0.1
DASHA-SYNC-MVR: Step size: 0.01; Batch Size B': 10
Figure 4: Classiﬁcation task with CIFAR10 dataset and ResNet-18 deep neural network. Dimension
d ≈107 and K ≈2 · 106 in RandK.
B
EXPERIMENTS DETAILS
The code was written in Python 3.6.8 using PyTorch 1.9 (Paszke et al., 2019). A distributed
environment was emulated on a machine with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz and
64 cores. Deep learning experiments were conducted with NVIDIA A100 GPU with 40GB memory
(each deep learning experiment uses at most 5GB of this memory).
When the number of nodes n does not divide the number of samples N in a dataset, we randomly
ignore N mod n samples from a dataset (up to 4 when n = 5).
18

Published as a conference paper at ICLR 2023
C
DESCRIPTION OF DASHA-SYNC-MVR
In this section, we provide a description of DASHA-SYNC-MVR (see Algorithm 2). This algorithm
is closely related to DASHA-MVR (Algorithm 1), but DASHA-SYNC-MVR synchronizes all nodes
with some probability p. This synchronization procedure enabled us to ﬁx the convergence rate
suboptimality of DASHA-MVR w.r.t. ω.
Algorithm 2 DASHA-SYNC-MVR
1: Input: starting point x0 ∈Rd, stepsize γ > 0, momentum a ∈(0, 1], probability p ∈(0, 1],
batch size B′, number of iterations T ≥1.
2: Initialize g0
i , h0
i on the nodes and g0 = 1
n
Pn
i=1 g0
i on the server
3: for t = 0, 1, . . . , T −1 do
4:
xt+1 = xt −γgt
5:
ct+1 =
1, with probability p,
0, with probability 1 −p
6:
Broadcast xt+1 to all nodes
7:
for i = 1, . . . , n in parallel do
8:
if ct+1 = 1 then
9:
ht+1
i
=
1
B′
PB′
k=1 ∇fi(xt+1; ξt+1
ik )
10:
mt+1
i
= gt+1
i
= ht+1
i
11:
else
12:
ht+1
i
= 1
B
PB
j=1 ∇fi(xt+1; ξt+1
ij ) + ht
i −1
B
PB
j=1 ∇fi(xt; ξt+1
ij )
13:
mt+1
i
= Ci
 ht+1
i
−ht
i −a (gt
i −ht
i)

14:
gt+1
i
= gt
i + mt+1
i
15:
end if
16:
Send mt+1
i
to the server
17:
end for
18:
if ct+1 = 1 then
19:
gt+1 = 1
n
Pn
i=1 mt+1
i
20:
else
21:
gt+1 = gt + 1
n
Pn
i=1 mt+1
i
22:
end if
23: end for
24: Output: ˆxT chosen uniformly at random from {xt}T −1
k=0
D
PARTIAL PARTICIPATION
A partial participation mechanism, important for federated learning applications, can be easily
implemented in DASHA. Let us assume that the ith node either participates in a communication round
with probability p′, or sends nothing. From the view of unbiased compressors, it can mean that
instead of using a compressor C, we have use the following new stochastic mapping Cp′ :
Cp′(x) =
(
1
p′ C(x),
with probability p′,
0,
with probability 1 −p′.
(13)
The following simple result states that the new mapping Cp′ is also an unbiased compressor, which
means that our theory applies to this choice as well.
Theorem D.1. If C ∈U(ω), then Cp′ ∈U

ω+1
p′
−1

.
In the case of partial participation, all theorems from Section 6 will hold with ω replaced by
(ω+1)/p′ −1.
E
AUXILIARY FACTS
In this section, we recall well–known auxiliary facts that we use in the proofs.
19

Published as a conference paper at ICLR 2023
1. For all x, y ∈Rd, we have
∥x + y∥2 ≤2 ∥x∥2 + 2 ∥y∥2
(14)
2. Let us take a random vector ξ ∈Rd, then
E
h
∥ξ∥2i
= E
h
∥ξ −E [ξ]∥2i
+ ∥E [ξ]∥2 .
(15)
F
COMPRESSORS FACTS
Deﬁnition F.1. Let us take a random subset S from [d], |S| = K, K ∈[d]. We say that a stochastic
mapping C : Rd →Rd is RandK if
C(x) = d
K
X
j∈S
xjej,
where {ei}d
i=1 is the standard unit basis.
Informally, RandK randomly keeps K coordinates and zeroes out the other.
Theorem F.2. If C is RandK, then C ∈U
  d
k −1

.
See the proof in (Beznosikov et al., 2020).
In the next theorem, we show that Cp′(x) from (13) is an unbiased compressor.
Theorem D.1. If C ∈U(ω), then Cp′ ∈U

ω+1
p′
−1

.
Proof. First, we proof the unbiasedness:
E [Cp′(x)] = p′
 1
p′ C(x)

+ (1 −p′)0 = C(x),
∀x ∈Rd.
Next, we get a bound for the variance:
E
h
∥Cp′(x) −x∥2i
=
p′E
"
1
p′ C(x) −x

2#
+ (1 −p′) ∥x∥2
=
p′E
 1
p′2 ∥C(x)∥2 −2
 1
p′ C(x), x

+ ∥x∥2

+ (1 −p′) ∥x∥2
=
1
p′ E
h
∥C(x)∥2i
−(2 −p′) ∥x∥2 + (1 −p′) ∥x∥2
=
1
p′ E
h
∥C(x)∥2i
−∥x∥2 .
From C ∈U(ω), we have
E
h
∥Cp′(x) −x∥2i
≤
ω + 1
p′
∥x∥2 −∥x∥2 =
ω + 1
p′
−1

∥x∥2 .
G
POLYAK-ŁOJASIEWICZ CONDITION
In this section, we discuss our convergence rates under the (Polyak-Łojasiewicz) PŁ-condition:
Assumption G.1. A functions f satisfy (Polyak-Łojasiewicz) PŁ-condition:
∥∇f(x)∥2 ≥2µ(f(x) −f ∗),
∀x ∈R,
(16)
where f ∗= infx∈Rd f(x) > −∞.
20

Published as a conference paper at ICLR 2023
Here we use a different notion of an ε-solution: it is a (random) point bx, such that E [f(bx)] −f ∗≤ε.
Under this assumption, Algorithm 1 achieves a linear convergence rate O (ln (1/ε)) instead of a
sublinear convergence rate O (1/ε) in the gradient and ﬁnite-sum settings. Moreover, in the stochastic
setting, Algorithms 1 and 2 also improve dependence on ε. Related Theorems I.9, I.12, I.15 and I.20
are stated in Appendix I. Note that in the ﬁnite-sum and stochastic settings, Theorems I.12 and I.20
provide new SOTA theoretical convergence rates (see Table 2).
H
INTUITION BEHIND DASHA
In this section, we want to outline an intuition of differences between the proofs of DASHA and
MARINA that helps us to improve the convergence rates.
H.1
DIFFERENT SOURCES OF CONTRACTIONS
In both algorithms the proofs analyze EC
hgt+1 −∇f(xk+1)
2i
, a norm of a difference between a
gradient ∇f(xk+1) and a gradient estimator gt+1. For simplicity, we assume that n = 1, then for
MARINA, we have
EC
hgt+1 −∇f(xk+1)
2i
= p
∇f(xk+1) −∇f(xk+1)
2 + (1 −p)EC
hgt + C
 ∇f(xk+1) −∇f(xk)

−∇f(xk+1)
2i
= (1 −p)EC
hgt + C
 ∇f(xk+1) −∇f(xk)

−∇f(xk+1)
2i
(4),(15)
=
(1 −p)
gt −∇f(xk)
2 + (1 −p)EC
hC
 ∇f(xk+1) −∇f(xk)

−
 ∇f(xk+1) −∇f(xk)
2i
(4)
≤(1 −p)
gt −∇f(xk)
2 + (1 −p)ω
∇f(xk+1) −∇f(xk)
2 .
In order to get a contraction, i.e., EC
hgt+1 −∇f(xk+1)
2i
≤(1 −p)
gt −∇f(xk)
2 + · · · ,
MARINA has to send a full gradient ∇f(xk+1) with the probability p > 0.
Now, let us look how we get a contraction in DASHA:
EC
hgt+1 −∇f(xk+1)
2i
= EC
hgt + C
 ∇f(xk+1) −∇f(xk) −a
 gt −∇f(xk)

−∇f(xk+1)
2i
= EC
hgt + C
 ∇f(xk+1) −∇f(xk) −a
 gt −∇f(xk)

−∇f(xk+1)
2i
(4),(15)
=
(1 −a)2 gt −∇f(xk)
2
+ EC
hC
 ∇f(xk+1) −∇f(xk) −a
 gt −∇f(xk)

−
 ∇f(xk+1) −∇f(xk) −a
 gt −∇f(xk)
2i
(4)
≤(1 −a)2 gt −∇f(xk)
2 + ω
∇f(xk+1) −∇f(xk) −a
 gt −∇f(xk)
2
(14)
≤
 (1 −a)2 + 2ωa2 gt −∇f(xk)
2 + 2ω
∇f(xk+1) −∇f(xk)
2
≤(1 −a)
gt −∇f(xk)
2 + 2ω
∇f(xk+1) −∇f(xk)
2 .
In the last inequality we use that a ≤1/2ω+1. On can see that we get exactly the same recursion and
contraction. The source of contraction is a correction −a(gt −∇f(xk)) inside the compressor C.
H.2
THE SOURCE OF IMPROVEMENTS IN THE CONVERGENCE RATES
Let us brieﬂy explain why we get the improvements in the convergence rates of DASHA in the
ﬁnite-sum setting. The same intuitions implies to the stochastic setting.
In DASHA, we reduce variances from the compressors C and the random sampling It
j separately:
we have two different control variables ht
i and gt
i, two different parameters the probability p and
21

Published as a conference paper at ICLR 2023
the momentum a. For simplicity, let us assume that the number of nodes n = 1. Let us consider a
Lyapunov function from our proofs:
E

f(xt) −f ∗
+ γ (4ω + 1) E
hgt −ht2i
+ γ
1
p + 16ω (2ω + 1)

E
hht −∇f(xt)
2i
.
In contrast, MARINA (VR-MARINA) has only one control variable gt
i and on parameter p. A Lyapunov
function of MARINA is
E

f(xt) −f ∗
+ γ
2pE
hgt −∇f(xt)
2i
.
MARINA has a simpler Lyapunov function that leads to a suboptimal convergence rate. Intu-
itively, having one control variable and one parameter is not enough to reduce variances from
two different sources of randomness. So in DASHA, the parameter p =
B
m+B , while in MARINA
p = min
n
B
m+B , ζC
d
o
, because the parameter p of MARINA helps to reduce the variance from the
compressors C.
I
THEOREMS WITH PROOFS
Lemma I.1. Suppose that Assumption 5.2 holds and let xt+1 = xt −γgt. Then for any gt ∈Rd and
γ > 0, we have
f(xt+1) ≤f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
2
gt −∇f(xt)
2 .
(17)
The proof of Lemma I.1 is provided in (Li et al., 2021a).
There are two different sources of randomness in Algorithm 1: the ﬁrst one from vectors {ht+1
i
}n
i=1
and the second one from compressors {Ci}n
i=1. In this section, we deﬁne Eh [·] and EC [·] to be
conditional expectations w.r.t. {ht+1
i
}n
i=1 and {Ci}n
i=1, accordingly, conditioned on all previous
randomness.
Lemma I.2. Suppose that Assumption 1.2 holds and let us consider sequences gt+1
i
and ht+1
i
from
Algorithm 1, then
EC
hgt+1 −ht+12i
≤2ω
n2
n
X
i=1
ht+1
i
−ht
i
2 + 2a2ω
n2
n
X
i=1
gt
i −ht
i
2 + (1 −a)2 gt −ht2 ,
(18)
and
EC
hgt+1
i
−ht+1
i
2i
≤2ω
ht+1
i
−ht
i
2 +

2a2ω + (1 −a)2 gt
i −ht
i
2 ,
∀i ∈[n].
(19)
Proof. First, we estimate EC
hgt+1 −ht+12i
:
EC
hgt+1 −ht+12i
= EC


gt + 1
n
n
X
i=1
Ci
 ht+1
i
−ht
i −a
 gt
i −ht
i

−ht+1

2

(4),(15)
=
EC



1
n
n
X
i=1
Ci
 ht+1
i
−ht
i −a
 gt
i −ht
i

−1
n
n
X
i=1
 ht+1
i
−ht
i −a
 gt
i −ht
i


2

+ (1 −a)2 gt −ht2 .
Using the independence of compressors and (4), we get
EC
hgt+1 −ht+12i
22

Published as a conference paper at ICLR 2023
= 1
n2
n
X
i=1
EC
hCi
 ht+1
i
−ht
i −a
 gt
i −ht
i

−
 ht+1
i
−ht
i −a
 gt
i −ht
i
2i
+ (1 −a)2 gt −ht2
≤ω
n2
n
X
i=1
ht+1
i
−ht
i −a
 gt
i −ht
i
2 + (1 −a)2 gt −ht2
≤2ω
n2
n
X
i=1
ht+1
i
−ht
i
2 + 2a2ω
n2
n
X
i=1
gt
i −ht
i
2 + (1 −a)2 gt −ht2 .
Analogously, we can get the bound for EC
hgt+1
i
−ht+1
i
2i
:
EC
hgt+1
i
−ht+1
i
2i
= EC
hgt
i + Ci
 ht+1
i
−ht
i −a
 gt
i −ht
i

−ht+1
i
2i
= EC
hCi
 ht+1
i
−ht
i −a
 gt
i −ht
i

−
 ht+1
i
−ht
i −a
 gt
i −ht
i
2i
+ (1 −a)2 gt
i −ht
i
2
≤ω
ht+1
i
−ht
i −a
 gt
i −ht
i
2 + (1 −a)2 gt
i −ht
i
2
≤2ω
ht+1
i
−ht
i
2 + 2a2ω
gt
i −ht
i
2 + (1 −a)2 gt
i −ht
i
2
= 2ω
ht+1
i
−ht
i
2 +

2a2ω + (1 −a)2 gt
i −ht
i
2 .
Lemma I.3. Suppose that Assumptions 5.2 and 1.2 hold and let us take a = 1/ (2ω + 1) , then
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 8γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Proof. Due to Lemma I.1 and the update step from Line 4 in Algorithm 1, we have
E

f(xt+1)

≤
E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
2
gt −∇f(xt)
2

=
E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
2
gt −ht + ht −∇f(xt)
2

(20)
≤
E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
gt −ht2 +
ht −∇f(xt)
2i
.
In the last inequality we use Jensen’s inequality (14). Let us ﬁx some constants κ, η ∈[0, ∞) that we
will deﬁne later. Combining bounds (20), (18), (19) and using the law of total expectation, we get
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
gt −ht2 +
ht −∇f(xt)
2
23

Published as a conference paper at ICLR 2023
+ κE
"
2ω
n2
n
X
i=1
ht+1
i
−ht
i
2 + 2a2ω
n2
n
X
i=1
gt
i −ht
i
2 + (1 −a)2 gt −ht2
#
+ ηE
"
2ω
n
n
X
i=1
ht+1
i
−ht
i
2 +

2a2ω + (1 −a)2 1
n
n
X
i=1
gt
i −ht
i
2
#
= E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+

γ + κ (1 −a)2
E
hgt −ht2i
+
2κa2ω
n
+ η

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2κω
n
+ 2ηω

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
(21)
Now, by taking κ = γ
a, we can see that γ + κ (1 −a)2 ≤κ, and thus
E

f(xt+1)

+ γ
aE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ
aE
hgt −ht2i
+
2γaω
n
+ η

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2γω
an + 2ηω

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Next,
by taking η
=
2γω
n
and considering the choice of a,
one can show that

2γaω
n
+ η

2a2ω + (1 −a)2
≤η. Thus
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2γω (2ω + 1)
n
+ 4γω2
n

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 8γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
24

Published as a conference paper at ICLR 2023
The following lemma almost repeats the previous one. We will use it in the theorems with Assumption
G.1.
Lemma I.4. Suppose that Assumptions 5.2, 1.2 and G.1 hold and let us take a = 1/ (2ω + 1) and
γ ≤
a
2µ, then
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 20γω(2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Proof. Up to (21) we can follow the proof of Lemma I.3 to get
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+

γ + κ (1 −a)2
E
hgt −ht2i
+
2κa2ω
n
+ η

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2κω
n
+ 2ηω

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Now, by taking κ = 2γ
a , we can see that γ + κ (1 −a)2 ≤
 1 −a
2

κ, and thus
E

f(xt+1)

+ 2γ
a E
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+

1 −a
2
 2γ
a E
hgt −ht2i
+
4γaω
n
+ η

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
4γω
an + 2ηω

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Next,
by taking η
=
8γω
n
and considering the choice of a,
one can show that

4γaω
n
+ η

2a2ω + (1 −a)2
≤
 1 −a
2

η. Thus
E

f(xt+1)

25

Published as a conference paper at ICLR 2023
+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+

1 −a
2

2γ(2ω + 1)E
hgt −ht2i
+

1 −a
2
 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
4γω(2ω + 1)
n
+ 16γω2
n

E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+

1 −a
2

2γ(2ω + 1)E
hgt −ht2i
+

1 −a
2
 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 20γω(2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−ht
i
2
#
.
Finally, the assumption γ ≤
a
2µ implies an inequality 1 −a
2 ≤1 −γµ.
Lemma I.5. Suppose that Assumption 5.1 holds and
E

f(xt+1)

+ γΨt+1 ≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γΨt + γC,
(22)
where Ψt is a sequence of numbers, Ψt ≥0 for all t ∈[T], constant C ≥0, and constant γ > 0.
Then
E
h∇f(bxT )
2i
≤2
 f(x0) −f ∗
γT
+ 2Ψ0
T
+ 2C,
(23)
where a point bxT is chosen uniformly from a set of points {xt}T −1
t=0 .
Proof. By unrolling (22) for t from 0 to T −1, we obtain
γ
2
T −1
X
t=0
E
h∇f(xt)
2i
+ E

f(xT )

+ γΨT ≤f(x0) + γΨ0 + γTC.
We subtract f ∗, divide inequality by γT
2 , and take into account that f(x) ≥f ∗for all x ∈R, and
Ψt ≥0 for all t ∈[T], to get the following inequality:
1
T
T −1
X
t=0
E
h∇f(xt)
2i
≤2
 f(x0) −f ∗
γT
+ 2Ψ0
T
+ 2C.
It is left to consider the choice of a point bxT to complete the proof of the lemma.
Lemma I.6. Suppose that Assumptions 5.1 and G.1 hold and
E

f(xt+1)

+ γΨt+1 ≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ)γΨt + γC,
where Ψt is a sequence of numbers, Ψt ≥0 for all t ∈[T], constant C ≥0, constant µ > 0, and
constant γ ∈(0, 1/µ). Then
E

f(xT ) −f ∗
≤(1 −γµ)T   f(x0) −f ∗
+ γΨ0
+ C
µ .
(24)
26

Published as a conference paper at ICLR 2023
Proof. We subtract f ∗and use PŁ-condition (16) to get
E

f(xt+1) −f ∗
+ γΨt+1
≤
E

f(xt) −f ∗
−γ
2 E
h∇f(xt)
2i
+ γΨt + γC
≤
(1 −γµ)E

f(xt) −f ∗
+ (1 −γµ)γΨt + γC
=
(1 −γµ)
 E

f(xt) −f ∗
+ γΨt
+ γC.
Unrolling the inequality, we have
E

f(xt+1) −f ∗
+ γΨt+1
≤
(1 −γµ)t+1   f(x0) −f ∗
+ γΨ0
+ γC
t
X
i=0
(1 −γµ)i
≤
(1 −γµ)t+1   f(x0) −f ∗
+ γΨ0
+ C
µ .
It is left to note that Ψt ≥0 for all t ∈[T].
Lemma I.7. If 0 < γ ≤(L +
√
A)−1, L > 0, and A ≥0, then
1
2γ −L
2 −γA
2
≥0.
It is easy to verify with a direct calculation.
I.1
CASE OF DASHA
Despite the triviality of the following lemma, we provide it for consistency with Lemma I.14 and
Lemma I.11.
Lemma I.8. Suppose that Assumption 5.3 holds. Assuming that h0
i = ∇fi(x0) for all i ∈[n], for
ht+1
i
from Algorithm 1 (DASHA) we have
1.
Eh
hht+1 −∇f(xt+1)
2i
= 0.
2.
Eh
hht+1
i
−∇fi(xt+1)
2i
= 0,
∀i ∈[n].
3.
Eh
hht+1
i
−ht
i
2i
≤L2
i
xt+1 −xt2 ,
∀i ∈[n].
Theorem 6.1. Suppose that Assumptions 5.1, 5.2, 5.3 and 1.2 hold. Let us take a = 1/ (2ω + 1) and
γ ≤

L +
q
16ω(2ω+1)
n
bL
−1
, and h0
i = ∇fi(x0) for all i ∈[n] in Algorithm 1 (DASHA), then
E
h∇f(bxT )
2i
≤1
T
"
2
 f(x0) −f ∗
 
L +
r
16ω (2ω + 1)
n
bL
!
+ 2 (2ω + 1)
g0 −∇f(x0)
2 + 4ω
n
 
1
n
n
X
i=1
g0
i −∇fi(x0)
2
! #
.
Proof. Considering Lemma I.3, Lemma I.8, and the law of total expectation, we obtain
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2
27

Published as a conference paper at ICLR 2023
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 8γω (2ω + 1)
n
bL2 xt+1 −xt2
= E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−
 1
2γ −L
2 −8γω (2ω + 1)
n
bL2

E
hxt+1 −xt2i
.
Using assumption about γ, we can show that
1
2γ −L
2 −8γω(2ω+1)
n
bL2 ≥0 (see Lemma I.7), thus
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
.
In the view of Lemma I.5 with Ψt = (2ω + 1) E
h
∥gt −ht∥2i
+ 2ω
n E
h
1
n
Pn
i=1 ∥gt
i −ht
i∥2i
we can
conclude the proof.
Corollary 6.2. Suppose that assumptions from Theorem 6.1 hold, and g0
i = h0
i = ∇fi(x0) for all
i ∈[n], then DASHA needs T := O
 
1
ε
"
 f(x0) −f ∗ 
L +
ω
√n bL
 #!
communication rounds
to get an ε-solution and the communication complexity is equal to O (d + ζCT) , where ζC is the
expected density from Deﬁnition 1.3.
Proof. The communication complexities can be easily derived using Theorem 6.1. At each commu-
nication round of Algorithm 1, each node sends ζC coordinates. In the view of g0
i = ∇fi(x0) for
all i ∈[n], we additionally have to send d coordinates from the nodes to the server, thus the total
communication complexity would be O (d + ζCT) .
Corollary 6.3. Suppose that assumptions of Corollary 6.2 hold. We take the unbiased compressor
RandK with K = ζC ≤d/√n, then the communication complexity equals O

d +
bL(f(x0)−f ∗)d
ε√n

.
Proof. In the view of Theorem F.2, we have ω + 1 = d/K. Combining this and an inequality L ≤bL,
the communication complexity equals
O (d + ζCT)
=
O
 
d + 1
ε
"
 f(x0) −f ∗ 
KL + K ω
√n
bL
 #!
=
O
 
d + 1
ε
"
 f(x0) −f ∗  d
√nL + d
√n
bL
 #!
=
O
 
d + 1
ε
"
 f(x0) −f ∗  d
√n
bL
 #!
.
I.2
CASE OF DASHA UNDER PŁ-CONDITION
Theorem I.9. Suppose that Assumption 5.1, 5.2, 5.3, 1.2 and G.1 hold. Let us take a = 1/ (2ω + 1) ,
γ ≤min
(
L +
q
40ω(2ω+1)
n
bL
−1
, a
2µ
)
, and h0
i = ∇fi(x0) for all i ∈[n] in Algorithm 1
28

Published as a conference paper at ICLR 2023
(DASHA), then
E

f(xT ) −f ∗
≤(1 −γµ)T
 
 f(x0) −f ∗
+ 2γ(2ω + 1)
g0 −∇f(x0)
2 + 8γω
n
 
1
n
n
X
i=1
g0
i −∇fi(x0)
2
!!
.
Proof. Considering Lemma I.4, Lemma I.8, and the law of total expectation, we obtain
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 20γω(2ω + 1)
n
bL2 xt+1 −xt2
= E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−
 1
2γ −L
2 −20γω(2ω + 1)
n
bL2
 xt+1 −xt2 .
Using the assumption about γ, we can show that
1
2γ −L
2 −20γω(2ω+1)
n
bL2 ≥0 (see Lemma I.7), thus
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
.
In the view of Lemma I.6 with Ψt = 2(2ω + 1)E
h
∥gt −ht∥2i
+ 8ω
n E
h
1
n
Pn
i=1 ∥gt
i −ht
i∥2i
we
can conclude the proof.
We use e
O (·), when we provide a bound up to logarithmic factors.
Corollary I.10. Suppose that assumptions from Theorem I.9 hold, and g0
i = 0 for all i ∈[n], then
DASHA needs
T := e
O
 
ω + L
µ + ωbL
µ√n
!
.
(25)
communication rounds to get an ε-solution and the communication complexity is equal to O (ζCT) ,
where ζC is the expected density from Deﬁnition 1.3.
Proof. Clearly, using Theorem I.9, one can show that Algorithm 1 returns an ε-solution after
(25) communication rounds. At each communication round of Algorithm 1, each node sends
ζC coordinates, thus the total communication complexity would be O (ζCT) per node. Unlike
Corollary 6.2, in this corollary, we can initialize g0
i , for instance, with zeros because the corresponding
initialization error Ψ0 from the proof of Theorem I.9 would be under the logarithm.
29

Published as a conference paper at ICLR 2023
I.3
CASE OF DASHA-PAGE
Lemma I.11. Suppose that Assumptions 5.3 and 5.4 hold. For ht+1
i
from Algorithm 1 (DASHA-PAGE)
we have
1.
Eh
hht+1 −∇f(xt+1)
2i
≤(1 −p) L2
max
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2 .
2.
Eh
hht+1
i
−∇fi(xt+1)
2i
≤(1 −p) L2
max
B
xt+1 −xt2 + (1 −p)
ht
i −∇fi(xt)
2 ,
∀i ∈[n].
3.
Eh
hht+1
i
−ht
i
2i
≤
(1 −p)L2
max
B
+ 2L2
i
 xt+1 −xt2 + 2p
ht
i −∇fi(xt)
2 ,
∀i ∈[n].
Proof. Using the deﬁnition of ht+1, we obtain
Eh
hht+1 −∇f(xt+1)
2i
= (1 −p) Eh



ht + 1
n
n
X
i=1
1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−∇f(xt+1)

2

(15)
= (1 −p) Eh



1
n
n
X
i=1
1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−
 ∇f(xt+1) −∇f(xt)


2

+ (1 −p)
ht −∇f(xt)
2 .
From the unbiasedness and independence of mini-batch samples, we get
Eh
hht+1 −∇f(xt+1)
2i
≤(1 −p)
n2B2
n
X
i=1
Eh

X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−
 ∇fi(xt+1) −∇fi(xt)
2


+ (1 −p)
ht −∇f(xt)
2
= (1 −p)
n2B
n
X
i=1

1
m
m
X
j=1
 ∇fij(xt+1) −∇fij(xt)

−
 ∇fi(xt+1) −∇fi(xt)
2


+ (1 −p)
ht −∇f(xt)
2
≤(1 −p)
n2B
n
X
i=1

1
m
m
X
j=1
∇fij(xt+1) −∇fij(xt)
2


+ (1 −p)
ht −∇f(xt)
2
≤(1 −p) L2
max
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2 .
In the last inequality, we use Assumption 5.4. Using the same reasoning, we have
Eh
hht+1
i
−∇fi(xt+1)
2i
30

Published as a conference paper at ICLR 2023
= (1 −p) Eh



ht
i + 1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−∇fi(xt+1)

2

= (1 −p) Eh



1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−
 ∇f(xt+1) −∇f(xt)


2

+ (1 −p)
ht
i −∇fi(xt)
2
≤(1 −p) L2
max
B
xt+1 −xt2 + (1 −p)
ht
i −∇fi(xt)
2 .
Finally, we consider the last ineqaulity of the lemma:
Eh
hht+1
i
−ht
i
2i
= p
∇fi(xt+1) −ht
i
2 + (1 −p)Eh



ht
i + 1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−ht
i

2

(15)
= p
∇fi(xt+1) −ht
i
2
+ (1 −p)Eh



1
B
X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−
 ∇fi(xt+1) −∇fi(xt)


2

+ (1 −p)
∇fi(xt+1) −∇fi(xt)
2 .
Using the unbiasedness and independence of the gradients, we obtain
Eh
hht+1
i
−ht
i
2i
≤p
∇fi(xt+1) −ht
i
2
+ (1 −p)
B2
Eh

X
j∈It
i
 ∇fij(xt+1) −∇fij(xt)

−
 ∇fi(xt+1) −∇fi(xt)
2


+ (1 −p)
∇fi(xt+1) −∇fi(xt)
2
= p
∇fi(xt+1) −ht
i
2
+ (1 −p)
B

1
m
m
X
j=1
 ∇fij(xt+1) −∇fij(xt)

−
 ∇fi(xt+1) −∇fi(xt)
2


+ (1 −p)
∇fi(xt+1) −∇fi(xt)
2
≤p
∇fi(xt+1) −ht
i
2
+ (1 −p)
B

1
m
m
X
j=1
∇fij(xt+1) −∇fij(xt)
2


+ (1 −p)
∇fi(xt+1) −∇fi(xt)
2 .
From Assumptions 5.3 and 5.4, we can conclude that
Eh
hht+1
i
−ht
i
2i
≤p
∇fi(xt+1) −ht
i
2 + (1 −p)
L2
max
B
+ L2
i
 xt+1 −xt2
31

Published as a conference paper at ICLR 2023
= p
∇fi(xt+1) −∇fi(xt) + ∇fi(xt) −ht
i
2 + (1 −p)
L2
max
B
+ L2
i
 xt+1 −xt2
(14)
≤2p
∇fi(xt+1) −∇fi(xt)
2 + 2p
ht
i −∇fi(xt)
2 + (1 −p)
L2
max
B
+ L2
i
 xt+1 −xt2
≤2pL2
i
xt+1 −xt2 + 2p
ht
i −∇fi(xt)
2 + (1 −p)
L2
max
B
+ L2
i
 xt+1 −xt2
≤
(1 −p)L2
max
B
+ 2L2
i
 xt+1 −xt2 + 2p
ht
i −∇fi(xt)
2 .
Theorem 6.4. Suppose that Assumptions 5.1, 5.2, 5.3, 5.4, and 1.2 hold. Let us take a = 1/ (2ω + 1),
probability p ∈(0, 1], and
γ ≤
 
L +
s
48ω (2ω + 1)
n
(1 −p)L2max
B
+ bL2

+ 2 (1 −p) L2max
pnB
!−1
in Algorithm 1 (DASHA-PAGE) then
E
h∇f(bxT )
2i
≤1
T

2
 f(x0) −f ∗
×
 
L +
s
48ω (2ω + 1)
n
(1 −p)L2max
B
+ bL2

+ 2 (1 −p) L2max
pnB
!
+ 2 (2ω + 1)
g0 −h02 + 4ω
n
 
1
n
n
X
i=1
g0
i −h0
i
2
!
+ 2
p
h0 −∇f(x0)
2 + 32ω (2ω + 1)
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!

.
Proof. Let us ﬁx constants ν, ρ ∈[0, ∞) that we will deﬁne later.
Considering Lemma I.3,
Lemma I.11, and the law of total expectation, we obtain
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 8γω (2ω + 1)
n
E
"(1 −p)L2
max
B
+ 2bL2
 xt+1 −xt2 + 2p 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ νE
(1 −p) L2
max
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2

+ ρE
"
(1 −p) L2
max
B
xt+1 −xt2 + (1 −p) 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
32

Published as a conference paper at ICLR 2023
After rearranging the terms, we get
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−

1
2γ −L
2 −
8γω (2ω + 1)

(1−p)L2
max
B
+ 2bL2
n
−ν (1 −p) L2
max
nB
−ρ(1 −p) L2
max
B

E
hxt+1 −xt2i
+ (γ + ν(1 −p)) E
hht −∇f(xt)
2i
+
16γpω (2ω + 1)
n
+ ρ(1 −p)

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
Next, let us ﬁx ν = γ
p, to get
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
p E
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
−

1
2γ −L
2 −
8γω (2ω + 1)

(1−p)L2
max
B
+ 2bL2
n
−γ (1 −p) L2
max
pnB
−ρ(1 −p) L2
max
B

E
hxt+1 −xt2i
+
16γpω (2ω + 1)
n
+ ρ(1 −p)

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
By taking ρ = 16γω(2ω+1)
n
, we obtain
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
p E
hht+1 −∇f(xt+1)
2i
+ 16γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ 16γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
33

Published as a conference paper at ICLR 2023
−

1
2γ −L
2 −
8γω (2ω + 1)

(1−p)L2
max
B
+ 2bL2
n
−γ (1 −p) L2
max
pnB
−16γω (2ω + 1) (1 −p) L2
max
nB

E
hxt+1 −xt2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ 16γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−

1
2γ −L
2 −
24γω (2ω + 1)

(1−p)L2
max
B
+ bL2
n
−γ (1 −p) L2
max
pnB

E
hxt+1 −xt2i
.
Next, considering the choice of γ and Lemma I.7, we get
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
p E
hht+1 −∇f(xt+1)
2i
+ 16γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ 16γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
Finally, in the view of Lemma I.5 with
Ψt
=
(2ω + 1) E
hgt −ht2i
+ 2ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
1
pE
hht −∇f(xt)
2i
+ 16ω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
,
we can conclude the proof.
Corollary 6.5. Let the assumptions from Theorem 6.4 hold, p = B/(m+B), and g0
i = h0
i = ∇fi(x0)
for all i ∈[n]. Then DASHA-PAGE needs
T := O




1
ε


 f(x0) −f ∗ 
L + ω
√n
bL +
 ω
√n +
r m
nB
 Lmax
√
B







communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the expected # of gradient calculations per node equals O (m + BT) , where ζC is the expected
density from Deﬁnition 1.3.
Proof. Corollary 6.5 can be proved in the same way as Corollary 6.2. One only should note that the
expected number of gradients calculations at each communication round equals pm + (1 −p)B =
2mB
m+B ≤2B.
34

Published as a conference paper at ICLR 2023
Corollary 6.6. Suppose that assumptions of Corollary 6.5 hold, B ≤
p
m/n, and we use the unbiased
compressor RandK with K = ζC = Θ (Bd/√m) . Then the communication complexity of Algorithm 1
is
O
 
d + Lmax
 f(x0) −f ∗
d
ε√n
!
,
(7)
and the expected # of gradient calculations per node equals
O
 
m + Lmax
 f(x0) −f ∗ √m
ε√n
!
.
(8)
Proof. In the view of Theorem F.2, we have ω + 1 = d/K. Combining this, inequalities L ≤bL ≤
Lmax, and K = Θ

Bd
√m

= O

d
√n

, we can show that the communication complexity equals
O (d + ζCT)
=
O



d + 1
ε


 f(x0) −f ∗ 
KL + K ω
√n
bL + K
 ω
√n +
r m
nB
 Lmax
√
B







=
O



d + 1
ε


 f(x0) −f ∗  d
√nL + d
√n
bL + d
√nLmax







=
O



d + 1
ε


 f(x0) −f ∗  d
√nLmax






.
And the expected number of gradient calculations per node equals
O (m + BT)
=
O



m + 1
ε


 f(x0) −f ∗ 
BL + B ω
√n
bL + B
 ω
√n +
r m
nB
 Lmax
√
B







=
O



m + 1
ε


 f(x0) −f ∗ rm
n L +
rm
n
bL +
rm
n Lmax







=
O



m + 1
ε


 f(x0) −f ∗ rm
n Lmax






.
I.4
CASE OF DASHA-PAGE UNDER PŁ-CONDITION
Theorem I.12. Suppose that Assumption 5.1, 5.2, 5.3, 1.2, 5.4, and G.1 hold.
Let
us take a
=
1/ (2ω + 1) , probability p
∈
(0, 1], batch size B
∈
[m], and
γ ≤min
(
L +
r
200ω(2ω+1)
n

(1−p)L2
max
B
+ 2bL2

+ 4(1−p)L2max
pnB
−1
, a
2µ, p
2µ
)
in Algorithm 1
(DASHA-PAGE), then
E

f(xT ) −f ∗
≤
(1 −γµ)T



(f(x0) −f ∗) + 2γ(2ω + 1)
g0 −h02 + 8γω
n
1
n
n
X
i=1
g0
i −h0
i
2
35

Published as a conference paper at ICLR 2023
+
2γ
p
h0 −∇f(x0)
2 + 80γω (2ω + 1)
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!



.
Proof. Let us ﬁx constants ν, ρ ∈[0, ∞) that we will deﬁne later.
Considering Lemma I.4,
Lemma I.11, and the law of total expectation, we obtain
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 20γω(2ω + 1)
n
E
"(1 −p)L2
max
B
+ 2bL2
 xt+1 −xt2 + 2p 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ νE
(1 −p) L2
max
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2

+ ρE
"
(1 −p) L2
max
B
xt+1 −xt2 + (1 −p) 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
After rearranging the terms, we get
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−
 1
2γ −L
2 −20γω(2ω + 1)
n
(1 −p)L2
max
B
+ 2bL2

−ν (1 −p) L2
max
nB
−ρ(1 −p) L2
max
B

E
hxt+1 −xt2i
+ (γ + ν(1 −p)) E
hht −∇f(xt)
2i
+
40pγω (2ω + 1)
n
+ ρ(1 −p)

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
By taking ν = 2γ
p and ρ = 80γω(2ω+1)
n
, one can see that γ+ν(1−p) ≤
 1 −p
2

ν and 40pγω(2ω+1)
n
+
ρ(1 −p) ≤
 1 −p
2

ρ, thus
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
36

Published as a conference paper at ICLR 2023
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −p
2
 2γ
p E
hht −∇f(xt)
2i
+

1 −p
2
 80γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−
 1
2γ −L
2 −20γω(2ω + 1)
n
(1 −p)L2
max
B
+ 2bL2

−2γ (1 −p) L2
max
pnB
−80γω (2ω + 1) (1 −p) L2
max
nB

E
hxt+1 −xt2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −p
2
 2γ
p E
hht −∇f(xt)
2i
+

1 −p
2
 80γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−
 1
2γ −L
2 −100γω(2ω + 1)
n
(1 −p)L2
max
B
+ 2bL2

−2γ (1 −p) L2
max
pnB

E
hxt+1 −xt2i
.
Next, considering the choice of γ and Lemma I.7, we get
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ (1 −γµ) 2γ
p E
hht −∇f(xt)
2i
+ (1 −γµ) 80γω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
In the view of Lemma I.6 with
Ψt
=
2(2ω + 1)E
hgt −ht2i
+ 8ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2
pE
hht −∇f(xt)
2i
+ 80ω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
,
we can conclude the proof of the theorem.
Corollary I.13. Suppose that assumptions from Theorem I.12 hold, probability p = B/(m + B),
and h0
i = g0
i = 0 for all i ∈[n], then DASHA-PAGE needs
T := e
O
 
ω + m
B + L
µ + ωbL
µ√n +
 ω
√n +
√m
√
nB
 Lmax
µ
√
B
!
(26)
communication rounds to get an ε-solution, the communication complexity is equal to O (ζCT) , and
the expected number of gradient calculations per node equals O (BT) , where ζC is the expected
density from Deﬁnition 1.3.
37

Published as a conference paper at ICLR 2023
Proof. Clearly, using Theorem I.12, one can show that Algorithm 1 returns an ε-solution after
(26) communication rounds. At each communication round of Algorithm 1, each node sends ζC
coordinates, thus the total communication complexity would be O (ζCT) . Moreover, the expected
number of gradients calculations at each communication round equals pm+(1−p)B = 2mB
m+B ≤2B,
thus the total expected number of gradients that each node calculates is O (BT) . Unlike Corollary 6.5,
in this corollary, we can initialize h0
i and g0
i , for instance, with zeros because the corresponding
initialization error Ψ0 from the proof of Theorem I.12 would be under the logarithm.
I.5
CASE OF DASHA-MVR
We introduce new notations: ∇fi(xt+1; ξt+1
i
) = 1
B
PB
j=1 ∇fi(xt+1; ξt+1
ij ) and ∇f(xt+1; ξt+1) =
1
n
Pn
i=1 ∇fi(xt+1; ξt+1
i
).
Lemma I.14. Suppose that Assumptions 5.3, 5.5 and 5.6 hold. For ht+1
i
from Algorithm 1 (DASHA-
MVR) we have
1.
Eh
hht+1 −∇f(xt+1)
2i
≤2b2σ2
nB
+ 2 (1 −b)2 L2
σ
nB
xt+1 −xt2 + (1 −b)2 ht −∇f(xt)
2 .
2.
Eh
hht+1
i
−∇fi(xt+1)
2i
≤2b2σ2
B
+ 2 (1 −b)2 L2
σ
B
xt+1 −xt2 + (1 −b)2 ht
i −∇fi(xt)
2 ,
∀i ∈[n].
3.
Eh
hht+1
i
−ht
i
2i
≤2b2σ2
B
+ 2
 
(1 −b)2 L2
σ
B
+ L2
i
!
xt+1 −xt2 + 2b2 ht
i −∇fi(xt)
2 ,
∀i ∈[n].
Proof. First, let us proof the bound for Eh
hht+1 −∇f(xt+1)
2i
:
Eh
hht+1 −∇f(xt+1)
2i
= Eh
h∇f(xt+1; ξt+1) + (1 −b)
 ht −∇f(xt; ξt+1)

−∇f(xt+1)
2i
(15)
= Eh
hb
 ∇f(xt+1; ξt+1) −∇f(xt+1)

+ (1 −b)
 ∇f(xt+1; ξt+1) −∇f(xt+1) + ∇f(xt) −∇f(xt; ξt+1)
2i
+ (1 −b)2 ht −∇f(xt)
2
(14)
≤2b2Eh
h∇f(xt+1; ξt+1) −∇f(xt+1)
2i
+ 2 (1 −b)2 Eh
h∇f(xt+1; ξt+1) −∇f(xt+1) + ∇f(xt) −∇f(xt; ξt+1)
2i
+ (1 −b)2 ht −∇f(xt)
2
= 2b2
n2
n
X
i=1
Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt+1)
2i
+ 2 (1 −b)2
n2
n
X
i=1
Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt; ξt+1
i
) −
 ∇fi(xt+1) −∇fi(xt)
2i
+ (1 −b)2 ht −∇f(xt)
2
=
2b2
n2B2
n
X
i=1
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt+1)
2i
+ 2 (1 −b)2
n2B2
n
X
i=1
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt; ξt+1
ij ) −
 ∇fi(xt+1) −∇fi(xt)
2i
38

Published as a conference paper at ICLR 2023
+ (1 −b)2 ht −∇f(xt)
2 .
Using Assumptions 5.5 and 5.6, we obtain
Eh
hht+1 −∇f(xt+1)
2i
≤2b2σ2
nB
+ 2 (1 −b)2 L2
σ
nB
xt+1 −xt2 + (1 −b)2 ht −∇f(xt)
2 .
Similarly, we can get the bound for Eh
hht+1
i
−∇fi(xt+1)
2i
:
Eh
hht+1
i
−∇fi(xt+1)
2i
= Eh
h∇fi(xt+1; ξt+1
i
) + (1 −b)
 ht
i −∇fi(xt; ξt+1
i
)

−∇fi(xt+1)
2i
= E
hb
 ∇fi(xt+1; ξt+1
i
) −∇fi(xt+1)

+ (1 −b)
 ∇fi(xt+1; ξt+1
i
) −∇fi(xt+1) + ∇f(xt) −∇fi(xt; ξt+1
i
)
2i
+ (1 −b)2 ht
i −∇fi(xt)
2
≤2b2σ2
B
+ 2 (1 −b)2 L2
σ
B
xt+1 −xt2 + (1 −b)2 ht
i −∇fi(xt)
2 .
Now, we proof the last inequality of the lemma:
Eh
hht+1
i
−ht
i
2i
= Eh
h∇fi(xt+1; ξt+1
i
) + (1 −b)
 ht
i −∇fi(xt; ξt+1
i
)

−ht
i
2i
(15)
= Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt+1) + (1 −b)
 ∇fi(xt) −∇fi(xt; ξt+1
i
)
2i
+
∇fi(xt+1) −∇fi(xt) −b
 ht
i −∇fi(xt)
2
= Eh
hb
 ∇fi(xt+1; ξt+1
i
) −∇fi(xt+1)

+ (1 −b)
 ∇fi(xt+1; ξt+1
i
) −∇fi(xt; ξt+1
i
) −(∇fi(xt+1) −∇fi(xt))
2i
+
∇fi(xt+1) −∇fi(xt) −b
 ht
i −∇fi(xt)
2
(14)
≤2b2Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt+1)
2i
+ 2 (1 −b)2 Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt; ξt+1
i
) −(∇fi(xt+1) −∇fi(xt))
2i
+ 2
∇fi(xt+1) −∇fi(xt)
2 + 2b2 ht
i −∇fi(xt)
2
= 2b2
B2
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt+1)
2i
+ 2 (1 −b)2
B2
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt; ξt+1
ij ) −(∇fi(xt+1) −∇fi(xt))
2i
+ 2
∇fi(xt+1) −∇fi(xt)
2 + 2b2 ht
i −∇fi(xt)
2
In the view of Assumptions 5.3, 5.5 and 5.6, we obtain
Eh
hht+1
i
−ht
i
2i
≤2b2σ2
B
+ 2 (1 −b)2 L2
σ
B
xt+1 −xt2 + 2L2
i
xt+1 −xt2 + 2b2 ht
i −∇fi(xt)
2 .
Theorem 6.7. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2ω+1, b ∈(0, 1], and γ ≤

L +
r
96ω(2ω+1)
n

(1−b)2L2σ
B
+ bL2

+ 4(1−b)2L2σ
bnB
−1
, in Algorithm 1
(DASHA-MVR). Then
E
h∇f(bxT )
2i
≤1
T

2
 f(x0) −f ∗
39

Published as a conference paper at ICLR 2023
×

L +
v
u
u
t96ω (2ω + 1)
n
 
(1 −b)2 L2σ
B
+ bL2
!
+ 4 (1 −b)2 L2σ
bnB


+ 2 (2ω + 1)
g0 −h02 + 4ω
n
 
1
n
n
X
i=1
g0
i −h0
i
2
!
+ 2
b
h0 −∇f(x0)
2 + 32bω (2ω + 1)
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!


+
96ω (2ω + 1)
nB
+
4
bnB

b2σ2.
Proof. Let us ﬁx constants ν, ρ ∈[0, ∞) that we will deﬁne later.
Considering Lemma I.3,
Lemma I.14, and the law of total expectation, we obtain
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 8γω (2ω + 1)
n
E
"
2b2σ2
B
+ 2
 
(1 −b)2 L2
σ
B
+ bL2
!
xt+1 −xt2 + 2b2 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ νE
"
2b2σ2
nB
+ 2 (1 −b)2 L2
σ
nB
xt+1 −xt2 + (1 −b)2 ht −∇f(xt)
2
#
+ ρE
"
2b2σ2
B
+ 2 (1 −b)2 L2
σ
B
xt+1 −xt2 + (1 −b)2 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
After rearranging the terms, we get
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−

1
2γ −L
2 −
16γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−2ν (1 −b)2 L2
σ
nB
−2ρ (1 −b)2 L2
σ
B

E
hxt+1 −xt2i
+
 γ + ν(1 −b)2
E
hht −∇f(xt)
2i
+
16b2γω (2ω + 1)
n
+ ρ(1 −b)2

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
40

Published as a conference paper at ICLR 2023
+ 2
8γω (2ω + 1)
nB
+ ν
nB + ρ
B

b2σ2.
By taking ν = γ
b , one can see that γ + ν(1 −b)2 ≤ν, and
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
b E
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
b E
hht −∇f(xt)
2i
−

1
2γ −L
2 −
16γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−2γ (1 −b)2 L2
σ
bnB
−2ρ (1 −b)2 L2
σ
B

E
hxt+1 −xt2i
+
16b2γω (2ω + 1)
n
+ ρ(1 −b)2

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ 2
8γω (2ω + 1)
nB
+
γ
bnB + ρ
B

b2σ2.
Next, we ﬁx ρ =
16bγω(2ω+1)
n
. With this choice of ρ and for all b ∈[0, 1], we can show that
16b2γω(2ω+1)
n
+ ρ(1 −b)2 ≤ρ, thus
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
b E
hht+1 −∇f(xt+1)
2i
+ 16bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
b E
hht −∇f(xt)
2i
+ 16bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−

1
2γ −L
2 −
16γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−2γ (1 −b)2 L2
σ
bnB
−32bγω (2ω + 1) (1 −b)2 L2
σ
nB

E
hxt+1 −xt2i
+ 2
8γω (2ω + 1)
nB
+
γ
bnB + 16bγω (2ω + 1)
nB

b2σ2
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
b E
hht −∇f(xt)
2i
+ 16bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
41

Published as a conference paper at ICLR 2023
−

1
2γ −L
2 −
48γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−2γ (1 −b)2 L2
σ
bnB

E
hxt+1 −xt2i
+
48γω (2ω + 1)
nB
+ 2γ
bnB

b2σ2.
In the last inequality we use b ∈(0, 1]. Next, considering the choice of γ and Lemma I.7, we get
E

f(xt+1)

+ γ (2ω + 1) E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
b E
hht+1 −∇f(xt+1)
2i
+ 16bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ (2ω + 1) E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
b E
hht −∇f(xt)
2i
+ 16bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+
48γω (2ω + 1)
nB
+ 2γ
bnB

b2σ2.
In the view of Lemma I.5 with
Ψt
=
(2ω + 1) E
hgt −ht2i
+ 2ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
1
b E
hht −∇f(xt)
2i
+ 16bω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
and C =

48ω(2ω+1)
nB
+
2
bnB

b2σ2, we can conclude the proof.
Corollary 6.8.
Suppose that assumptions from Theorem 6.7 hold,
momentum b
=
Θ

min
n
1
ω
q
nεB
σ2 , nεB
σ2
o
, and g0
i = h0
i =
1
Binit
PBinit
k=1 ∇fi(x0; ξ0
ik) for all i ∈[n], and batch
size Binit = Θ (B/b) , then Algorithm 1 (DASHA-MVR) needs
T := O




1
ε


 f(x0) −f ∗
 
L + ω
√n
bL +
 
ω
√n +
r
σ2
εn2B
!
Lσ
√
B
!

+ σ2
nεB




communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where ζC is the
expected density from Deﬁnition 1.3.
Proof. In the view of Theorem 6.7, we have
E
h∇f(bxT )
2i
= O




1
T


 f(x0) −f ∗

L + ω
√n
r
(1 −b)2 L2σ
B + bL2 +
s
(1 −b)2
bn
Lσ
√
B


42

Published as a conference paper at ICLR 2023
+ 1
b
h0 −∇f(x0)
2 + bω2
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!


+
ω2
n + 1
bn

b2 σ2
B



.
Note, that 1
b = Θ

max

ω
q
σ2
nεB ,
σ2
nεB

≤Θ

max
n
ω2,
σ2
nεB
o
, thus
E
h∇f(bxT )
2i
= O




1
T


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
r
σ2
εn2B
Lσ
√
B
!
+ 1
b
h0 −∇f(x0)
2 + bω2
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!

+ ε



.
Thus we can take
T = O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
r
σ2
εn2B
Lσ
√
B
!
+ 1
b
h0 −∇f(x0)
2 + bω2
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!





.
Note, that h0
i = g0
i =
1
Binit
PBinit
k=1 ∇fi(x0; ξ0
ik) for all i ∈[n]. Let us bound E
hh0 −∇f(x0)
2i
:
E
hh0 −∇f(x0)
2i
=
E



1
n
n
X
i=1
1
Binit
Binit
X
k=1
∇fi(x0; ξ0
ik) −∇f(x0)

2

=
1
n2B2
init
n
X
i=1
Binit
X
k=1
E
h∇fi(x0; ξ0
ik) −∇fi(x0)
2i
≤
σ2
nBinit
.
Likewise, 1
n
Pn
i=1 E
hh0
i −∇fi(x0)
2i
≤
σ2
Binit . All in all, we have
T = O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
r
σ2
εn2B
Lσ
√
B
!
+
σ2
bnBinit
+ bω2σ2
nBinit






43

Published as a conference paper at ICLR 2023
= O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
r
σ2
εn2B
Lσ
√
B
!
+ σ2
nB + b2ω2σ2
nB






= O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
r
σ2
εn2B
Lσ
√
B
!

+ σ2
nεB



.
In the view of Algorithm 1 and the fact that we use a mini-batch of stochastic gradients, the number
of stochastic gradients that each node calculates equals Binit + 2BT = O(Binit + BT).
Corollary 6.9. Suppose that assumptions of Corollary 6.8 hold, batch size B ≤
σ
√εn, we take RandK
with K = ζC = Θ

Bd√εn
σ

, and eL := max{L, Lσ, bL}. Then the communication complexity equals
O
 
dσ
√nε +
eL
 f(x0) −f ∗
d
√nε
!
,
(9)
and the expected # of stochastic gradient calculations per node equals
O
 
σ2
nε +
eL
 f(x0) −f ∗
σ
ε
3/2n
!
.
(10)
Proof. In the view of Theorem F.2, we have ω+1 = d/K. Moreover, K = Θ

Bd√εn
σ

= O

d
√n

,
thus the communication complexity equals
O (d + ζCT)
=
O



d + 1
ε


 f(x0) −f ∗
 
KL + K ω
√n

bL + Lσ
√
B

+ K
r
σ2
εn2B
Lσ
√
B
!

+ K σ2
nεB




=
O



d + 1
ε


 f(x0) −f ∗  d
√nL + d
√n

bL + Lσ
√
B

+ d
√nLσ


+ dσ
√nε




=
O



d + dσ
√nε + 1
ε


 f(x0) −f ∗  d
√n
eL







=
O




dσ
√nε + 1
ε


 f(x0) −f ∗  d
√n
eL






.
And the expected number of stochastic gradient calculations per node equals
O (Binit + BT)
= O



B σ2
Bnε + Bω
r
σ2
nεB + 1
ε


 f(x0) −f ∗
 
BL + B ω
√n

bL + Lσ
√
B

+ B
r
σ2
εn2B
Lσ
√
B
!






44

Published as a conference paper at ICLR 2023
= O




σ2
nε +
σ2
nε
√
B
+ 1
ε


 f(x0) −f ∗  σ
√εnL +
σ
√εn

bL + Lσ
√
B

+
σ
√εnLσ







= O




σ2
nε + 1
ε


 f(x0) −f ∗  σ
√εn
eL






.
I.6
CASE OF DASHA-MVR UNDER PŁ-CONDITION
Theorem
I.15.
Suppose
that
Assumption
5.1,
5.2,
5.3,
1.2,
5.5,
5.6
and
G.1
hold.
Let
us
take
a
=
1/ (2ω + 1) ,
b
∈
(0, 1]
and
γ
≤
min









L +
s
400ω(2ω+1)

(1−b)2L2σ
B
+bL2

n
+ 8(1−b)2L2σ
bnB



−1
, a
2µ, b
2µ







in Algorithm 1 (DASHA-
MVR), then
E

f(xT ) −f ∗
≤
(1 −γµ)T




 f(x0) −f ∗
+ 2γ(2ω + 1)
g0 −h02 + 8γω
n
 
1
n
n
X
i=1
g0
i −h0
i
2
!
+
2γ
b
h0 −∇f(x0)
2 + 80bγω (2ω + 1)
n
 
1
n
n
X
i=1
h0
i −∇fi(x0)
2
!




+
1
µ
200ω (2ω + 1)
nB
+
4
bnB

b2σ2.
Proof. Let us ﬁx constants ν, ρ ∈[0, ∞) that we will deﬁne later.
Considering Lemma I.4,
Lemma I.14, and the law of total expectation, we obtain
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
ht −∇f(xt)
2

+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ 20γω(2ω + 1)
n
E
"
2b2σ2
B
+ 2
 
(1 −b)2 L2
σ
B
+ bL2
!
xt+1 −xt2 + 2b2 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ νE
"
2b2σ2
nB
+ 2 (1 −b)2 L2
σ
nB
xt+1 −xt2 + (1 −b)2 ht −∇f(xt)
2
#
+ ρE
"
2b2σ2
B
+ 2 (1 −b)2 L2
σ
B
xt+1 −xt2 + (1 −b)2 1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
.
45

Published as a conference paper at ICLR 2023
After rearranging the terms, we get
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
−

1
2γ −L
2 −
40γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−2ν (1 −b)2 L2
σ
nB
−2ρ (1 −b)2 L2
σ
B

E
hxt+1 −xt2i
+
 γ + ν(1 −b)2
E
hht −∇f(xt)
2i
+
40b2γω (2ω + 1)
n
+ ρ(1 −b)2

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ 2
20γω (2ω + 1)
nB
+ ν
nB + ρ
B

b2σ2.
By taking ν = 2γ
b , one can see that γ + ν(1 −b)2 ≤
 1 −b
2

ν, and
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ 2γ
b E
hht+1 −∇f(xt+1)
2i
+ ρE
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −b
2
 2γ
b E
hht −∇f(xt)
2i
−

1
2γ −L
2 −
40γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−4γ (1 −b)2 L2
σ
bnB
−2ρ (1 −b)2 L2
σ
B

E
hxt+1 −xt2i
+
40b2γω (2ω + 1)
n
+ ρ(1 −b)2

E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+ 2
20γω (2ω + 1)
nB
+ 2γ
bnB + ρ
B

b2σ2.
Next, we ﬁx ρ =
80bγω(2ω+1)
n
. With this choice of ρ and for all b ∈(0, 1], we can show that
40b2γω(2ω+1)
n
+ ρ(1 −b)2 ≤
 1 −b
2

ρ, thus
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ 2γ
b E
hht+1 −∇f(xt+1)
2i
+ 80bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
46

Published as a conference paper at ICLR 2023
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −b
2
 2γ
b E
hht −∇f(xt)
2i
+

1 −b
2
 80bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−

1
2γ −L
2 −
40γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−4γ (1 −b)2 L2
σ
bnB
−160bγω (2ω + 1) (1 −b)2 L2
σ
nB
!
E
hxt+1 −xt2i
+ 2
20γω (2ω + 1)
nB
+ 2γ
bnB + 80bγω (2ω + 1)
nB

b2σ2
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −b
2
 2γ
b E
hht −∇f(xt)
2i
+

1 −b
2
 80bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
−

1
2γ −L
2 −
200γω (2ω + 1)

(1−b)2L2
σ
B
+ bL2
n
−4γ (1 −b)2 L2
σ
bnB

E
hxt+1 −xt2i
+
200γω (2ω + 1)
nB
+ 4γ
bnB

b2σ2.
In the last inequality we use b ∈(0, 1]. Next, considering the choice of γ and Lemma I.7, we get
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ 2γ
b E
hht+1 −∇f(xt+1)
2i
+ 80bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht+1
i
−∇fi(xt+1)
2
#
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ (1 −γµ) 2γ
b E
hht −∇f(xt)
2i
+ (1 −γµ) 80bγω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
+
200γω (2ω + 1)
nB
+ 4γ
bnB

b2σ2.
In the view of Lemma I.6 with
Ψt
=
2(2ω + 1)E
hgt −ht2i
+ 8ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2
b E
hht −∇f(xt)
2i
+ 80bω (2ω + 1)
n
E
"
1
n
n
X
i=1
ht
i −∇fi(xt)
2
#
47

Published as a conference paper at ICLR 2023
and C =

200ω(2ω+1)
nB
+
4
bnB

b2σ2, we can conclude the proof.
Corollary I.16.
Suppose that assumptions from Theorem I.15 hold,
momentum b
=
Θ

min

1
ω
q
µnεB
σ2 , µnεB
σ2

, and h0
i = g0
i = 0 for all i ∈[n], then Algorithm 1 needs
T := e
O
 
ω + ω
s
σ2
µnεB +
σ2
µnεB + L
µ + ωbL
µ√n +
 ω
√n +
σ
n√Bµε
 Lσ
µ
√
B
!
(27)
communication rounds to get an ε-solution, the communication complexity is equal to O (ζCT) , and
the number of stochastic gradient calculations per node equals O(BT), where ζC is the expected
density from Deﬁnition 1.3.
Proof. Considering the choice of b, we have 1
µ

200ω(2ω+1)
nB
+
4
bnB

b2σ2 = O (ε) . Therefore, is it
enough to take the number of communication rounds equals (27) to get an ε-solution. In the view
of Algorithm 1 and the fact that we use a mini-batch of stochastic gradients, the communication
complexity is equal to O (ζCT) and the number of stochastic gradients that each node calculates
equals O(BT). Unlike Corollary 6.8, in this corollary, we can initialize h0
i and g0
i , for instance, with
zeros because the corresponding initialization error Ψ0 from the proof of Theorem I.15 would be
under the logarithm.
I.7
CASE OF DASHA-SYNC-MVR
Comparing Algorithm 1 and Algorithm 2, one can see that Algorithm 2 has the third source of
randomness from ct+1. In this section, we deﬁne Ep [·] to be a conditional expectation w.r.t. ct+1
conditioned on all previous randomness. And we deﬁne Et+1 [·] to be a conditional expectation
w.r.t. ct+1, {Ci}n
i=1, {ht+1
i
}n
i=1 conditioned on all previous randomness. Note, that Et+1 [·] =
Eh [EC [Ep [·]]] .
Lemma I.17. Suppose that Assumptions 5.3, 5.5 and 1.2 hold and let us consider sequences
{gt+1
i
}n
i=1 and {ht+1
i
}n
i=1 from Algorithm 2, then
Et+1
hgt+1 −ht+12i
≤
2ω(1 −p)

L2
σ
B + bL2
n
xt+1 −xt2 + 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2 ,
and
Et+1
hgt+1
i
−ht+1
i
2i
≤2ω(1 −p)
L2
σ
B + L2
i
 xt+1 −xt2 + (1 −p)

2a2ω + (1 −a)2 gt
i −ht
i
2 ,
∀i ∈[n].
Proof. First,
we
estimate
Et+1
hgt+1 −ht+12i
.
Let
us
denote
ht+1
i,0
=
1
B
PB
j=1 ∇fi(xt+1; ξt+1
ij ) + ht
i −1
B
PB
j=1 ∇fi(xt; ξt+1
ij ).
Et+1
hgt+1 −ht+12i
= Et+1
h
Ep
hgt+1 −ht+12ii
= (1 −p)Et+1


gt + 1
n
n
X
i=1
Ci
 ht+1
i,0 −ht
i −a
 gt
i −ht
i

−1
n
n
X
i=1
ht+1
i,0

2

(4),(15)
=
(1 −p)Eh

EC



1
n
n
X
i=1
Ci
 ht+1
i,0 −ht
i −a
 gt
i −ht
i

−1
n
n
X
i=1
 ht+1
i,0 −ht
i −a
 gt
i −ht
i


2



48

Published as a conference paper at ICLR 2023
+ (1 −p) (1 −a)2 gt −ht2 .
Using the independence of compressors and (4), we get
Et+1
hgt+1 −ht+12i
= (1 −p)
n2
n
X
i=1
Eh
h
EC
hCi
 ht+1
i,0 −ht
i −a
 gt
i −ht
i

−
 ht+1
i,0 −ht
i −a
 gt
i −ht
i
2ii
+ (1 −p) (1 −a)2 gt −ht2
≤ω(1 −p)
n2
n
X
i=1
Eh
hht+1
i,0 −ht
i −a
 gt
i −ht
i
2i
+ (1 −p) (1 −a)2 gt −ht2
≤2ω(1 −p)
n2
n
X
i=1
Eh
hht+1
i,0 −ht
i
2i
+ 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2
= 2ω(1 −p)
n2
n
X
i=1
Eh



1
B
B
X
j=1
∇fi(xt+1; ξt+1
ij ) −1
B
B
X
j=1
∇fi(xt; ξt+1
ij )

2

+ 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2
(15)
= 2ω(1 −p)
n2
n
X
i=1
 
Eh



1
B
B
X
j=1
 ∇fi(xt+1; ξt+1
ij ) −∇fi(xt; ξt+1
ij )

−
 ∇fi(xt+1) −∇fi(xt)


2

+
∇fi(xt+1) −∇fi(xt)
2
!
+ 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2
= 2ω(1 −p)
n2
n
X
i=1
 
1
B2
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt; ξt+1
ij ) −
 ∇fi(xt+1) −∇fi(xt)
2i
+
∇fi(xt+1) −∇fi(xt)
2
!
+ 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2
≤
2ω(1 −p)

L2
σ
B + bL2
n
xt+1 −xt2 + 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2 ,
where in the inequalities we use Assumptions 1.2, 5.5 and 5.3, and (14). Analogously, we can get the
bound for Et+1
hgt+1
i
−ht+1
i
2i
for all i ∈[n]:
Et+1
hgt+1
i
−ht+1
i
2i
= Et+1
h
Ep
hgt+1
i
−ht+1
i
2ii
= (1 −p)Et+1
hgt
i + Ci
 ht+1
i,0 −ht
i −a
 gt
i −ht
i

−ht+1
i,0
2i
≤2ω(1 −p)
L2
σ
B + L2
i
 xt+1 −xt2 + 2a2ω(1 −p)
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt
i −ht
i
2 .
49

Published as a conference paper at ICLR 2023
We introduce new notations: ∇fi(xt+1; ξt+1
i
) = 1
B
PB
j=1 ∇fi(xt+1; ξt+1
ij ) and ∇f(xt+1; ξt+1) =
1
n
Pn
i=1 ∇fi(xt+1; ξt+1
i
).
Lemma I.18. Suppose that Assumptions 5.5 and 5.6 hold and let us consider sequence {ht+1
i
}n
i=1
from Algorithm 2, then
Et+1
hht+1 −∇f(xt+1)
2i
≤pσ2
nB′ + (1 −p)L2
σ
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2 .
Proof.
Et+1
hht+1 −∇f(xt+1)
2i
= pEh



1
n
n
X
i=1
1
B′
B′
X
k=1
∇fi(xt+1; ξt+1
ik ) −∇f(xt+1)

2

+ (1 −p)Eh
h∇f(xt+1; ξt+1) + ht −∇f(xt; ξt+1) −∇f(xt+1)
2i
≤pσ2
nB′ + (1 −p)Eh
h∇f(xt+1; ξt+1) + ht −∇f(xt; ξt+1) −∇f(xt+1)
2i
,
where we use Assumption 5.5. Next, using Assumption 5.6 and (15), we have
Et+1
hht+1 −∇f(xt+1)
2i
≤pσ2
nB′ + (1 −p)Eh
h∇f(xt+1; ξt+1) + ht −∇f(xt; ξt+1) −∇f(xt+1)
2i
= pσ2
nB′ + (1 −p)Eh
h∇f(xt+1; ξt+1) −∇f(xt; ξt+1) −
 ∇f(xt+1) −∇f(xt)
2i
+ (1 −p)
ht −∇f(xt)
2
= pσ2
nB′ + (1 −p)
n2
n
X
i=1
Eh
h∇fi(xt+1; ξt+1
i
) −∇fi(xt; ξt+1
i
) −
 ∇fi(xt+1) −∇fi(xt)
2i
+ (1 −p)
ht −∇f(xt)
2
= pσ2
nB′ + (1 −p)
n2B2
n
X
i=1
B
X
j=1
Eh
h∇fi(xt+1; ξt+1
ij ) −∇fi(xt; ξt+1
ij ) −
 ∇fi(xt+1) −∇fi(xt)
2i
+ (1 −p)
ht −∇f(xt)
2
≤pσ2
nB′ + (1 −p)L2
σ
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2 .
Theorem I.19. Suppose that Assumptions 5.1, 5.2, 5.3, 5.5, 5.6 and 1.2 hold. Let us take a =
1
2ω+1,
probability p ∈(0, 1], batch size B′ ≥1 and
γ ≤
 
L +
s
12ω(2ω + 1)(1 −p)
n
L2σ
B + bL2

+ 2(1 −p)L2σ
pnB
!−1
,
in Algorithm 2. Then
E
h∇f(bxT )
2i
≤1
T

2
 f(x0) −f ∗
×
 
L +
s
12ω(2ω + 1)(1 −p)
n
L2σ
B + bL2

+ 2(1 −p)L2σ
pnB
!
+ 2 (2ω + 1)
g0 −h02 + 4ω
n
 
1
n
n
X
i=1
g0
i −h0
i
2
!
50

Published as a conference paper at ICLR 2023
+ 2
p
h0 −∇f(x0)
2

+ 2σ2
nB′ .
Proof. Let us ﬁx constants κ, η, ν ∈[0, ∞) that we will deﬁne later. Using Lemma I.1, we can get
(20). Considering (20), Lemma I.17, Lemma I.18, and the law of total expectation, we obtain
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
gt −ht2 + γ
ht −∇f(xt)
2

+ κE


2ω(1 −p)

L2
σ
B + bL2
n
xt+1 −xt2 + 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2


+ ηE
"
2ω(1 −p)
L2
σ
B + bL2
 xt+1 −xt2 + (1 −p)

2a2ω + (1 −a)2 1
n
n
X
i=1
gt
i −ht
i
2
#
+ νE
 pσ2
nB′ + (1 −p)L2
σ
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2

.
After rearranging the terms, we get
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
−

1
2γ −L
2 −
2κω(1 −p)

L2
σ
B + bL2
n
−2ηω(1 −p)
L2
σ
B + bL2

−ν(1 −p)L2
σ
nB

E
hxt+1 −xt2i
+
 γ + κ(1 −p)(1 −a)2
E
hgt −ht2i
+
2κa2ω(1 −p)
n
+ η(1 −p)

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ (γ + ν(1 −p)) E
hht −∇f(xt)
2i
+ νpσ2
nB′ .
Let us take ν = γ
p, κ = γ
a, a =
1
2ω+1, and η = 2γω
n . Thus γ+κ(1−p)(1−a)2 ≤κ, γ+ν(1−p) = ν,
2κa2ω(1−p)
n
+ η(1 −p)

2a2ω + (1 −a)2
≤η, and
E

f(xt+1)

+ γ(2ω + 1)E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
p E
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
51

Published as a conference paper at ICLR 2023
−

1
2γ −L
2 −
2γω(2ω + 1)(1 −p)

L2
σ
B + bL2
n
−
4γω2(1 −p)

L2
σ
B + bL2
n
−γ(1 −p)L2
σ
pnB

E
hxt+1 −xt2i
+ γ(2ω + 1)E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ γσ2
nB′
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
−

1
2γ −L
2 −
6γω(2ω + 1)(1 −p)

L2
σ
B + bL2
n
−γ(1 −p)L2
σ
pnB

E
hxt+1 −xt2i
+ γ(2ω + 1)E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ γσ2
nB′ .
In the view of the choice of γ, we obtain
E

f(xt+1)

+ γ(2ω + 1)E
hgt+1 −ht+12i
+ 2γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ γ
p E
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ γ(2ω + 1)E
hgt −ht2i
+ 2γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ γ
p E
hht −∇f(xt)
2i
+ γσ2
nB′ .
Finally, using Lemma I.5 with
Ψt
=
(2ω + 1)E
hgt −ht2i
+ 2ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
1
pE
hht −∇f(xt)
2i
and C =
σ2
nB′ , we can conclude the proof.
Corollary 6.10.
Suppose that assumptions from Theorem I.19 hold,
probability p
=
min
n
ζC
d , nεB
σ2
o
, batch size B′ = Θ

σ2
nε

and h0
i = g0
i =
1
Binit
PBinit
k=1 ∇fi(x0; ξ0
ik) for all i ∈[n],
initial batch size Binit = Θ

max
n
σ2
nε, B d
ζC
o
, then DASHA-SYNC-MVR needs
T := O




1
ε


 f(x0) −f ∗
 
L + ω
√n
bL +
 
ω
√n +
s
d
ζCn +
r
σ2
εn2B
!
Lσ
√
B
!

+ σ2
nεB




52

Published as a conference paper at ICLR 2023
communication rounds to get an ε-solution, the communication complexity is equal to O (d + ζCT) ,
and the number of stochastic gradient calculations per node equals O(Binit + BT), where ζC is the
expected density from Deﬁnition 1.3.
Proof. Considering Theorem I.19 and the choice of B′, we have
E
h∇f(bxT )
2i
≤1
T

2
 f(x0) −f ∗



L +
v
u
u
t12ω(2ω + 1)(1 −p)

L2σ
B + bL2

n
+ 2(1 −p)L2σ
pnB




+ 2
p
h0 −∇f(x0)
2

+ 2σ2
nB′
≤1
T

2
 f(x0) −f ∗



L +
v
u
u
t12ω(2ω + 1)(1 −p)

L2σ
B + bL2

n
+ 2(1 −p)L2σ
pnB




+ 2
p
h0 −∇f(x0)
2

+ 2
3ε.
Due to p = min
n
ζC
d , nεB
σ2
o
, we have
E
h∇f(bxT )
2i
≤O




1
T

2
 f(x0) −f ∗



L +
v
u
u
t12ω(2ω + 1)(1 −p)

L2σ
B + bL2

n
+ 2d(1 −p)L2σ
ζCnB
+ 2σ2(1 −p)L2σ
εn2B2




+ 2
 d
ζC
+ σ2
nεB
 h0 −∇f(x0)
2





+ 2
3ε
≤O




1
T

2
 f(x0) −f ∗



L +
v
u
u
tω2(1 −p)

L2σ
B + bL2

n
+ d(1 −p)L2σ
ζCnB
+ 2σ2(1 −p)L2σ
εn2B2




+ 2
 d
ζC
+ σ2
nεB
 h0 −∇f(x0)
2





+ 2
3ε.
Therefore, we can take
T = O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
s
d
ζCn
Lσ
√
B
+
r
σ2
εn2B
Lσ
√
B
!
+
 d
ζC
+ σ2
nεB
 h0 −∇f(x0)
2





.
Note, that
E
hh0 −∇f(x0)
2i
=
E



1
n
n
X
i=1
1
Binit
Binit
X
k=1
∇fi(x0; ξ0
ik) −∇f(x0)

2

53

Published as a conference paper at ICLR 2023
=
1
n2B2
init
n
X
i=1
Binit
X
k=1
E
h∇fi(x0; ξ0
ik) −∇fi(x0)
2i
≤
σ2
nBinit
.
Next, by taking Binit = max
n
σ2
nε, B d
ζC
o
and using the last ineqaulity, we have
T = O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
s
d
ζCn
Lσ
√
B
+
r
σ2
εn2B
Lσ
√
B
!
+
 d
ζC
+ σ2
nεB

min
σ2ζC
ndB , ε







= O




1
ε


 f(x0) −f ∗
 
L + ω
√n

bL + Lσ
√
B

+
s
d
ζCn
Lσ
√
B
+
r
σ2
εn2B
Lσ
√
B
!

+ σ2
nεB



.
Finally, it is left to estimate the communication and oracle complexity. On average, the number
of coordinates that each node in Algorithm 2 sends at each communication round equals pd +
(1 −p)ζC ≤
ζC
d d +

1 −ζC
d

ζC ≤2ζC. Therefore, the communication complexity is equal to
O (d + ζCT) . Considering the fact that we use a mini-batch of stochastic gradients, on average,
the number of stochastic gradients that each node calculates at each communication round equals
pB′ + (1 −p)2B ≤O

nεB
σ2 · σ2
nε

+ 2B = O (B) . Considering the initial batch size Binit, the
number of stochastic gradients that each node calculates equals O(Binit + BT).
Corollary 6.11. Suppose that assumptions of Corollary 6.10 hold, batch size B ≤
σ
√εn, we take
RandK with K = ζC = Θ

Bd√εn
σ

, and eL := max{L, Lσ, bL}. Then the communication complex-
ity equals
O
 
dσ
√nε +
eL
 f(x0) −f ∗
d
√nε
!
,
(11)
and the expected # of stochastic gradient calculations per node equals
O
 
σ2
nε +
eL
 f(x0) −f ∗
σ
ε
3/2n
!
.
(12)
Proof. In the view of Theorem F.2, we have ω+1 = d/K. Moreover, K = Θ

Bd√εn
σ

= O

d
√n

,
thus the communication complexity equals
O (d + ζCT)
=
O



d + 1
ε


 f(x0) −f ∗
 
KL + K ω
√n

bL + Lσ
√
B

+ K
rω
n
Lσ
√
B
+ K
r
σ2
εn2B
Lσ
√
B
!

+ K σ2
nεB




=
O



d + 1
ε


 f(x0) −f ∗  d
√nL + d
√n

bL + Lσ
√
B

+ d
√nLσ


+ dσ
√nε




=
O



d + dσ
√nε + 1
ε


 f(x0) −f ∗  d
√n
eL







54

Published as a conference paper at ICLR 2023
=
O




dσ
√nε + 1
ε


 f(x0) −f ∗  d
√n
eL






.
And the expected number of stochastic gradient calculations per node equals
O (Binit + BT)
= O




σ2
nε + B d
ζC
+ 1
ε


 f(x0) −f ∗
 
BL + B ω
√n

bL + Lσ
√
B

+ B
rω
n
Lσ
√
B
+ B
r
σ2
εn2B
Lσ
√
B
!






= O




σ2
nε +
σ
√nε + 1
ε


 f(x0) −f ∗  σ
√εnL +
σ
√εn

bL + Lσ
√
B

+
σ
√εnLσ







= O




σ2
nε + 1
ε


 f(x0) −f ∗  σ
√εn
eL






.
I.8
CASE OF DASHA-SYNC-MVR UNDER PŁ-CONDITION
Theorem
I.20.
Suppose
that
Assumption
5.1,
5.2,
1.2,
5.5,
5.6
and
G.1
hold.
Let
us
take
a
=
1/ (2ω + 1) ,
probability
p
∈
(0, 1]
and
γ
≤
min







L +
s
40ω(2ω+1)(1−p)

L2σ
B +bL2

n
+ 4(1−p)L2σ
pnB



−1
, a
2µ, p
2µ





in Algorithm 1, then
E

f(xT ) −f ∗
≤
(1 −γµ)T




 f(x0) −f ∗
+ 2γ(2ω + 1)
g0 −h02 + 8γω
n
 
1
n
n
X
i=1
g0
i −h0
i
2
!
+
2γ
p
h0 −∇f(x0)
2



+ 2σ2
nµB′ .
Proof. Let us ﬁx constants κ, η, ν ∈[0, ∞) that we will deﬁne later. Using Lemma I.1, we can get
(20). Considering (20), Lemma I.17, Lemma I.18, and the law of total expectation, we obtain
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
≤E

f(xt) −γ
2
∇f(xt)
2 −
 1
2γ −L
2
 xt+1 −xt2 + γ
gt −ht2 + γ
ht −∇f(xt)
2

+ κE


2ω(1 −p)

L2
σ
B + bL2
n
xt+1 −xt2 + 2a2ω(1 −p)
n2
n
X
i=1
gt
i −ht
i
2 + (1 −p) (1 −a)2 gt −ht2


+ ηE
"
2ω(1 −p)
L2
σ
B + bL2
 xt+1 −xt2 + (1 −p)

2a2ω + (1 −a)2 1
n
n
X
i=1
gt
i −ht
i
2
#
55

Published as a conference paper at ICLR 2023
+ νE
 pσ2
nB′ + (1 −p)L2
σ
nB
xt+1 −xt2 + (1 −p)
ht −∇f(xt)
2

After rearranging the terms, we get
E

f(xt+1)

+ κE
hgt+1 −ht+12i
+ ηE
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ νE
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
−

1
2γ −L
2 −
2κω(1 −p)

L2
σ
B + bL2
n
−2ηω(1 −p)
L2
σ
B + bL2

−ν(1 −p)L2
σ
nB

E
hxt+1 −xt2i
+
 γ + κ(1 −p)(1 −a)2
E
hgt −ht2i
+
2κa2ω(1 −p)
n
+ η(1 −p)

2a2ω + (1 −a)2
E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ (γ + ν(1 −p)) E
hht −∇f(xt)
2i
+ νpσ2
nB′ .
Let us take ν = 2γ
p , κ = 2γ
a , a =
1
2ω+1, and η = 8γω
n . Thus γ + κ(1 −p)(1 −a)2 ≤
 1 −a
2

κ,
γ + ν(1 −p) =
 1 −p
2

ν, 2κa2ω(1−p)
n
+ η(1 −p)

2a2ω + (1 −a)2
≤
 1 −a
2

η, and
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ 2γ
p E
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
−

1
2γ −L
2 −
4γω(2ω + 1)(1 −p)

L2
σ
B + bL2
n
−
16γω2(1 −p)

L2
σ
B + bL2
n
−2γ(1 −p)L2
σ
pnB

E
hxt+1 −xt2i
+

1 −a
2

2γ(2ω + 1)E
hgt −ht2i
+

1 −a
2
 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −p
2
 2γ
p E
hht −∇f(xt)
2i
+ 2γσ2
nB′
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
−

1
2γ −L
2 −
20γω(2ω + 1)(1 −p)

L2
σ
B + bL2
n
−2γ(1 −p)L2
σ
pnB

E
hxt+1 −xt2i
+

1 −a
2

2γ(2ω + 1)E
hgt −ht2i
+

1 −a
2
 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+

1 −p
2
 2γ
p E
hht −∇f(xt)
2i
56

Published as a conference paper at ICLR 2023
+ 2γσ2
nB′ .
In the view of the choice of γ and Lemma I.7, one can show that 1
2γ −L
2 −
40γω(2ω+1)(1−p)

L2
σ
B +bL2

n
−
4γ(1−p)L2
σ
pnB
≥0, 1 −a
2 ≤1 −γµ, and 1 −p
2 ≤1 −γµ, thus
E

f(xt+1)

+ 2γ(2ω + 1)E
hgt+1 −ht+12i
+ 8γω
n E
"
1
n
n
X
i=1
gt+1
i
−ht+1
i
2
#
+ 2γ
p E
hht+1 −∇f(xt+1)
2i
≤E

f(xt)

−γ
2 E
h∇f(xt)
2i
+ (1 −γµ) 2γ(2ω + 1)E
hgt −ht2i
+ (1 −γµ) 8γω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+ (1 −γµ) 2γ
p E
hht −∇f(xt)
2i
+ 2γσ2
nB′ .
In the view of Lemma I.6 with
Ψt
=
2(2ω + 1)E
hgt −ht2i
+ 8ω
n E
"
1
n
n
X
i=1
gt
i −ht
i
2
#
+
2
pE
hht −∇f(xt)
2i
and C = 2σ2
nB′ , we can conclude the proof.
Corollary I.21.
Suppose that assumptions from Theorem I.20 hold,
probability p
=
min
n
ζC
d , µnεB
σ2
o
, batch size B′ = Θ

σ2
µnε

, and h0
i = g0
i = 0 for all i ∈[n], then DASHA-
SYNC-MVR needs
T := e
O
 
ω + d
ζC
+
σ2
µnεB + L
µ + ωbL
µ√n +
 
ω
√n +
s
d
ζCn +
σ
n√Bµε
!
Lσ
µ
√
B
!
(28)
communication rounds to get an ε-solution, the communication complexity is equal to O (ζCT) , and
the number of stochastic gradient calculations per node equals O(BT), where ζC is the expected
density from Deﬁnition 1.3.
Proof. Considering the choice of B′, we have
2σ2
nµB′ = O (ε) . Therefore, is it enough to take the
number of communication rounds equals (28) to get an ε-solution.
It is left to estimate the communication and oracle complexity. On average, in Algorithm 2, at
each communication round the number of coordinates that each node sends equals pd + (1 −
p)ζC ≤ζC
d d +

1 −ζC
d

ζC ≤2ζC. Therefore, the communication complexity is equal to O (ζCT) .
Considering the fact that we use a mini-batch of stochastic gradients, on average, the number of
stochastic gradients that each node calculates at each communication round equals pB′+(1−p)2B =
O

µnεB
σ2
·
σ2
µnε

+ 2B = O (B) , thus the number of stochastic gradients that each node calculates
equals O(BT). Unlike Corollary 6.10, in this corollary, we can initialize h0
i and g0
i , for instance,
with zeros because the corresponding initialization error Ψ0 from the proof of Theorem I.20 would
be under the logarithm.
57

Published as a conference paper at ICLR 2023
J
EXTRA EXPERIMENTS
DASHA-MVR improves VR-MARINA (online) when ε is small (see Tables 1 and 2 and experiments
in Section A). However, our analysis shows that DASHA-MVR gets a term Bω
q
σ2
εnB in the oracle
complexity and a term ω
q
σ2
µεnB in the number of communication rounds in general nonconvex
and PŁ settings accordingly. Both terms can be a bottleneck in some regimes; now, we verify this
dependence in the PŁ setting.
We take a synthetically generated stochastic quadratic optimization problem with one node (n = 1):
min
x∈Rd

f(x; ξ) := x⊤(A + ξI) x −b⊤x
	
,
where A ∈Rd×d, b ∈Rd, A = A⊤≻0, and ξ ∼Normal
 0, σ2
.
We generate A in such way, that µ ≈1.0 ≤L ≈2.0, take d = 104, σ2 = 1.0, RandK with K = 1
(ω ≈d), batch size B = 1, and
σ2
µεnB = 104. With this particular choice of parameters, ω
q
σ2
µεnB
would dominate in the number of communication rounds T = ω + ω
q
σ2
µεnB + L(1+ω/√n)
µ
+
σ2
µεnB +
Lσ
µ3/2√εnB .
Results are provided in Figure 5. We consider DASHA-MVR with a momentum b from Corollary I.16
and b = min
n
1
ω, µnεB
σ2
o
. With the latter choice of momentum b, DASHA-MVR converges at the
same rate as DASHA-SYNC-MVR or VR-MARINA (online) but to an ε-solution with a smaller ε. On the
other hand, the former choice of momentum b guarantees the convergence to the correct ε-solution,
but with a slower rate. Overall, the experiment provides the pieces of evidence that our choice of b is
correct and that our analysis in Theorem I.15 is tight.
If we decrease ω from 104 to 103 (see Figure 6), or σ2 from 1.0 to 0.1 (see Figure 7), or µ from 1.0
to 0.1 (see Figure 8), then the gap between algorithms closes.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-MVR b = min[1 ,
n B
2 ]
DASHA-SYNC-MVR
Figure 5: Comparison of algorithms on a synthetic stochastic quadratic optimization task
58

Published as a conference paper at ICLR 2023
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
#bits / n
1e9
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 10
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 6: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
K = 10
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 7: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
σ2 = 0.1
59

Published as a conference paper at ICLR 2023
0.0
0.5
1.0
1.5
2.0
2.5
3.0
#bits / n
1e8
10
8
10
6
10
4
10
2
100
102
104
|| f(xk)||2
K = 1
VR-MARINA (online)
DASHA-MVR
DASHA-SYNC-MVR
Figure 8: Comparison of algorithms on a synthetic stochastic quadratic optimization task with
µ = 0.1
60

