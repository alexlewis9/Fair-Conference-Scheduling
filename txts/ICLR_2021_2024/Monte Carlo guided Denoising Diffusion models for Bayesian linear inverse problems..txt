Published as a conference paper at ICLR 2024
MONTE CARLO GUIDED DENOISING DIFFUSION MODELS FOR
BAYESIAN LINEAR INVERSE PROBLEMS
Gabriel Cardoso*
Ecole polytechnique
IHU Liryc
Yazid Janati*
Ecole polytechnique
Sylvain Le Corff
Sorbonne UniversitÃ©
Eric Moulines
Ecole polytechnique
ABSTRACT
Ill-posed linear inverse problems arise frequently in various applications, from computational
photography to medical imaging. A recent line of research exploits Bayesian inference with
informative priors to handle the ill-posedness of such problems. Amongst such priors, score-
based generative models (SGM) have recently been successfully applied to several different
inverse problems. In this paper, we exploit the particular structure of the prior defined by
the SGM to define a sequence of intermediate linear inverse problems. As the noise level
decreases, the posterior distributions of these inverse problems get closer to the target posterior
of the original inverse problem. To sample from these distributions, we propose the use of
Sequential Monte Carlo (SMC) methods. The proposed algorithm, MCGdiff, is shown to
be theoretically grounded and we provide numerical simulations showing that it outperforms
competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.
1
INTRODUCTION
This paper is concerned with linear inverse problems y = Ax + ÏƒyÎµ, where y âˆˆRd
y is a vector of indirect
observations, x âˆˆRdx is the vector of unknowns, A âˆˆRdyÃ—dx is the linear forward operator and Îµ âˆˆRdy is
an unknown noise vector. This general model is used throughout computational imaging, including various
tomographic imaging applications such as common types of magnetic resonance imaging Vlaardingerbroek &
Boer (2013), X-ray computed tomography Elbakri & Fessler (2002), radar imaging Cheney & Borden (2009),
and basic image restoration tasks such as deblurring, super-resolution, and image inpainting GonzÃ¡lez et al.
(2009). The classical approach to solving linear inverse problems relies on prior knowledge about x, such as its
smoothness, sparseness in a dictionary, or its geometric properties. These approaches attempt to estimate a bx by
minimizing a regularized inverse problem, Ë†x = argminx{âˆ¥y âˆ’Axâˆ¥2 + Reg(x)}, where Reg is a regularization
term that balances data fidelity and noise while enabling efficient computations. However, a common difficulty in
the regularized inverse problem is the selection of an appropriate regularizer, which has a decisive influence on the
quality of the reconstruction.
Whereas regularized inverse problems continue to dominate the field, many alternative statistical formulations
have been proposed; see Besag et al. (1991); Idier (2013); Marnissi et al. (2017) and the references therein - see
Stuart (2010) for a mathematical perspective. A main advantage of statistical approaches is that they allow for
uncertainty quantification in the reconstructed solution; see Dashti & Stuart (2017). The Bayesâ€™ formulation
of the regularized inverse problem is based on considering the indirect measurement Y , the state X and the
noise Îµ as random variables, and to specify p(y|x) the likelihood (the conditional distribution of Y at X) and
the prior p(x) (the distribution of the state). One can use Bayesâ€™ theorem to obtain the posterior distribution
p(x|y) âˆp(y|x)p(x), where "âˆ" means that the two sides are equal to each other up to a multiplicative constant
that does not depend on x. Moreover, the use of an appropriate method for Bayesian inference allows the
*Both authors contributed equally.
Correspondence: {gabriel.victorino_cardoso,yazid.janati}@polytechnique.edu
1

Published as a conference paper at ICLR 2024
quantification of the uncertainty in the reconstructed solution x. A variety of priors are available, including but not
limited to Laplace Figueiredo et al. (2007), total variation (TV) Kaipio et al. (2000) and mixture-of-Gaussians
Fergus et al. (2006). In the last decade, a variety of techniques have been proposed to design and train generative
models capable of producing perceptually realistic samples from the original data, even in challenging high-
dimensional data such as images or audio Kingma et al. (2019); Kobyzev et al. (2020); Gui et al. (2021). Denoising
diffusion models have been shown to be particularly effective generative models in this context Sohl-Dickstein
et al. (2015); Song et al. (2021c;a;b); Benton et al. (2022). These models convert noise into the original data
domain through a series of denoising steps. A popular approach is to use a generic diffusion model that has
been pre-trained, eliminating the need for re-training and making the process more efficient and versatile Trippe
et al. (2023); Zhang et al. (2023). Although this was not the main motivation for developing these models, they
can of course be used as prior distributions in Bayesian inverse problems. This simple observation has led to a
new, fast-growing line of research on how linear inverse problems can benefit from the flexibility and expressive
power of the recently introduced deep generative models; see Arjomand Bigdeli et al. (2017); Wei et al. (2022);
Su et al. (2022); Kaltenbach et al. (2023); Shin & Choi (2023); Zhihang et al. (2023); SahlstrÃ¶m & Tarvainen
(2023).
CONTRIBUTIONS
â€¢ We propose MCGdiff, a novel algorithm for sampling from the Bayesian posterior of Gaussian linear inverse
problems with denoising diffusion model priors. MCGdiff specifically exploits the structure of both the linear
inverse problem and the denoising diffusion generative model to design an efficient SMC sampler.
â€¢ We establish under sensible assumptions that the empirical distribution of the samples produced by MCGdiff
converges to the target posterior when the number of particles goes to infinity. To the best of our knowledge,
MCGdiff is the first provably consistent algorithm for conditional sampling from the denoising diffusion
posteriors.
â€¢ To evaluate the performance of MCGdiff, we perform numerical simulations on several examples for which
the target posterior distribution is known. Simulation results support our theoretical results, i.e. the empirical
distribution of samples from MCGdiff converges to the target posterior distribution. This is not the case for
the competing methods (using the same denoising diffusion generative priors) which are shown, when run with
random initialization of the denoising diffusion, to generate a significant number of samples outside the support of
the target posterior. We also illustrate samples from MCGdiff in imaging inverse problems.
Background and notations.
This section provides a concise overview of the diffusion model framework and
notations used in this paper. We cover the elements that are important for understanding our approach, and we
recommend that readers refer to the original papers for complete details and derivations Sohl-Dickstein et al.
(2015); Ho et al. (2020); Song et al. (2021c;a). A denoising diffusion model is a generative model consisting of a
forward and a backward process. The forward process involves sampling X0 âˆ¼qdata from the data distribution,
which is then converted to a sequence X1:n of recursively corrupted versions of X0. The backward process
involves sampling Xn according to an easy-to-sample reference distribution on Rdx and generating X0 âˆˆRdx by
a sequence of denoising steps. Following Sohl-Dickstein et al. (2015); Song et al. (2021a), the forward process
can be chosen as a Markov chain with joint distribution
q0:n(x0:n) = qdata(x0) Qn
t=1 qt(xt|xtâˆ’1),
qt(xt|xtâˆ’1) = N(xt; (1 âˆ’Î²t)1/2xtâˆ’1, Î²t Idx) ,
(1.1)
where Idx is the identity matrix of size dx, {Î²t}tâˆˆN âŠ‚(0, 1) is a non-increasing sequence and N(x; Âµ, Î£) is the
p.d.f. of the Gaussian distribution with mean Âµ and covariance matrix Î£ (assumed to be non-singular) evaluated at
x. For all t > 0, set Â¯Î±t = Qt
â„“=1(1 âˆ’Î²â„“) with the convention Î±0 = 1. We have for all 0 â‰¤s < t â‰¤n,
qt|s(xt|xs) :=
R Qt
â„“=s+1 qâ„“(xâ„“|xâ„“âˆ’1)dxs+1:tâˆ’1 = N(xt; (Â¯Î±t/Â¯Î±s)1/2xs, (1 âˆ’Â¯Î±t/Â¯Î±s) Idx) .
(1.2)
For the standard choices of Â¯Î±t, the sequence of distributions (qt)tâˆˆN converges weakly to the standard normal
distribution as t â†’âˆ, which we chose as the reference distribution. For the reverse process, Song et al. (2021a;b)
introduce an inference distribution qÏƒ
1:n|0(x1:n|x0), depending on a sequence {Ïƒt}tâˆˆN of hyperparameters satisfy-
2

Published as a conference paper at ICLR 2024
ing Ïƒ2
t âˆˆ[0, 1 âˆ’Â¯Î±tâˆ’1] for all t âˆˆNâˆ—, and defined as qÏƒ
1:n|0(x1:n|x0) = qÏƒ
n|0(xn|x0) Q2
t=n qÏƒ
tâˆ’1|t,0(xtâˆ’1|xt, x0) ,
where qÏƒ
n|0(xn|x0) = N
Ã„
xn; Â¯Î±1/2
n x0, (1 âˆ’Â¯Î±n) Idx
Ã¤
and qÏƒ
tâˆ’1|t,0(xtâˆ’1|xt, x0) = N  xtâˆ’1; Âµt(x0, xt), Ïƒ2
t Idx
 ,
with Âµt(x0, xt) = Â¯Î±1/2
tâˆ’1x0 + (1 âˆ’Â¯Î±tâˆ’1 âˆ’Ïƒ2
t )1/2(xt âˆ’Â¯Î±1/2
t
x0)(1 âˆ’Â¯Î±t)1/2 . For t âˆˆ[1 : n âˆ’1], we define
by backward induction the sequence qÏƒ
t|0(xt|x0) =
R
qÏƒ
t|t+1,0(xt|xt+1, x0)qÏƒ
t+1|0(xt+1|x0)dxt+1. It is shown in
(Song et al., 2021a, Lemma 1) that for all t âˆˆ[1 : n], the distributions of the forward and inference process condi-
tioned on the initial state coincide, i.e. that qÏƒ
t|0(xt|x0) = qt|0(xt|x0). The backward process is derived from the
inference distribution by replacing, for each t âˆˆ[2 : n], x0 in the definition qÏƒ
tâˆ’1|t,0(xtâˆ’1|xt, x0) with a prediction
where Ï‡Î¸
0|t(xt) := Â¯Î±âˆ’1/2
t
 xt âˆ’(1 âˆ’Â¯Î±t)1/2eÎ¸(xt, t)
where eÎ¸(x, t) is typically a neural network parameterized
by Î¸. More formally, the backward distribution is defined as pÎ¸
0:n(x0:n) = pn(xn) Qnâˆ’1
t=0 pÎ¸
t (xt|xt+1) , where
pn(xn) = N(xn; 0dx, Idx) and for all t âˆˆ[1 : n âˆ’1],
pÎ¸
t(xt|xt+1) := qÏƒ
t|t+1,0(xt|xt+1, Ï‡Î¸
0|t+1(xt+1)) = N(xt, mÎ¸
t+1(xt+1), Ïƒ2
t+1Idx) ,
(1.3)
where mt+1(xt+1) := Âµ(Ï‡Î¸
0|t+1(xt+1), xt+1) and 0dx is the null vector of size dx. At step 0, we set p0(x0|x1) :=
N(x0; Ï‡Î¸
0|1(x1), Ïƒ2
1Idx). The parameter Î¸ is obtained (Song et al., 2021a, Theorem 1) by solving the following
optimization problem:
Î¸âˆ—âˆˆarg minÎ¸
Pn
t=1(2dxÏƒ2
t Î±t)âˆ’1 R
âˆ¥Ïµ âˆ’eÎ¸(âˆšÎ±tx0 + âˆš1 âˆ’Î±tÏµ, t)âˆ¥2
2N(Ïµ; 0dx, Idx)qdata(dx0)dÏµ .
(1.4)
Thus, eÎ¸âˆ—(Xt, t) might be seen as the predictor of the noise added to X0 to obtain Xt (in the forward pass) and
justifies the â€predictionâ€ terminology. The time 0 marginal pÎ¸âˆ—
0 (x0) =
R
pÎ¸âˆ—
0:n(x0:n)dx1:n which we will refer to as
the prior is used as an approximation of qdata and the time s marginal is pÎ¸âˆ—
s (xs) =
R
pÎ¸âˆ—
0:n(x0:n)dx1:sâˆ’1dxs+1:n.
In the rest of the paper, we drop the dependence on the parameter Î¸âˆ—. We define for all v âˆˆRâ„“, w âˆˆRk, the
concatenation operator vâŒ¢w = [vT , wT ]T âˆˆRâ„“+k. For i âˆˆ[1 : â„“], we let v[i] the i-th coordinate of v.
Related works.
The subject of Bayesian problems is very vast, and it is impossible to discuss here all the
results obtained in this very rich literature. One of such domains is image restoration problems, such as deblurring,
denoising inpainting, which are challenging problems in computer vision that involves restoring a partially
observed degraded image. Deep learning techniques are widely used for this task Arjomand Bigdeli et al. (2017);
Yeh et al. (2018); Xiang et al. (2023); Wei et al. (2022) with many of them relying on auto-encoders, VAEs
Ivanov et al. (2018); Peng et al. (2021); Zheng et al. (2019), GANs Yeh et al. (2018); Zeng et al. (2022), or
autoregressive transformers Yu et al. (2018); Wan et al. (2021). In what follows, we focus on methods based on
denoising diffusion that has recently emerged as a way to produce high-quality realistic samples from the original
data distribution on par with the best GANs in terms of image and audio generation, without the intricacies of
adversarial training; see Sohl-Dickstein et al. (2015); Song et al. (2021c; 2022). Diffusion-based approaches
do not require specific training for degradation types, making them much more versatile and computationally
efficient. In Song et al. (2022), noisy linear inverse problems are proposed to be solved by diffusing the degraded
observation forward, leading to intermediate observations {ys}n
s=0, and then running a modified backward process
that promotes consistency with ys at each step s. The Denoising-Diffusion-Restoration model (DDRM) Kawar et al.
also modifies the backward process so that the unobserved part of the state follows the backward process while the
observed part is obtained as a noisy weighted sum between the noisy observation and the prediction of the state.
As observed by Lugmayr et al. (2022), DDRM is very efficient, but the simple blending used occasionally causes
inconsistency in the restoration process. DPS Chung et al. (2023) considers a backward process targeting the
posterior. DPS approximates the score of the posterior using the Tweedie formula, which incorporates the learned
score of the prior. The approximation error is quantified and shown to decrease when the noise level is large, i.e.,
when the posterior is close to the prior distribution. As shown in Section 3 with a very simple example, neither
DDRM nor DPS can be used to sample the target posterior and therefore do not solve the Bayesian recovery problem
(even if we run DDRM and DPS several time with independent initializations). Indeed, we show that DDRM and DPS
produce samples under the "prior" distribution (which is generally captured very well by the denoising diffusion
model), but which are not consistent with the observations (many samples land in areas with very low likelihood).
3

Published as a conference paper at ICLR 2024
In Trippe et al. (2023), the authors introduce SMCdiff, a Sequential Monte Carlo-based denoising diffusion
model that aims at solving specifically the inpainting problem. SMCdiff produces a particle approximation of
the conditional distribution of the non observed part of the state conditionally on a forward-diffused trajectory
of the observation. The resulting particle approximation is shown to converge to the true posterior of the SGM
under the assumption that the joint laws of the forward and backward processes coincide, which fails to be true in
realistic setting. In comparison with SMCdiff, MCGdiff is a versatile approach that solves any Bayesian linear
inverse problem while being consistent under mild assumptions. In parallel to our work, Wu et al. (2023) also
developed a similar SMC based methodology but with a different proposal kernel.
2
THE MCGD I F F ALGORITHM
In this section, we present our methodology for the inpainting problem equation 2.1, both with noise and without
noise. The more general case is treated in Section 2.1. Let dy âˆˆ[1 : dx âˆ’1]. In what follows we denote the dy
top coordinates of a vector x âˆˆRdx by x and the remaining coordinates by x, so that x = xâŒ¢x. The inpainting
problem is defined as
Y = X + ÏƒyÎµ ,
Îµ âˆ¼N(0, Idy) ,
Ïƒ â‰¥0 ,
(2.1)
where X are the first dy coordinates of a random variable X âˆ¼p0. The goal is then to recover the law of the
complete state X given a realisation y of the incomplete observation Y and the model equation 2.1.
Noiseless case.
We begin by the case Ïƒy = 0. As the first dy coordinates are observed exactly, we aim at infering
the remaining coordinates of X,which correspond to X. As such, given an observation y, we aim at sampling
from the posterior Ï•y
0(x0) âˆp0(yâŒ¢x0) with integral form
Ï•y
0(x0) âˆ
R
pn(xn)
Â¶Qnâˆ’1
s=1 ps(xs|xs+1)
Â©
p0(yâŒ¢x0|x1)dx1:n .
(2.2)
To solve this problem, we propose to use SMC algorithms Doucet et al. (2001); CappÃ© et al. (2005); Chopin
et al. (2020), where a set of N random samples, referred to as particles, is iteratively updated to approximate
the posterior distribution. The updates involve, at iteration s, selecting promising particles from the pool
of particles Î¾1:N
s+1 = (Î¾1
s+1, . . . , Î¾N
s+1) based on a weight function eÏ‰s, and then apply a Markov transition py
s
to obtain the samples Î¾1:N
s
. The transition py
s(xs|xs+1) is designed to follow the backward process while
guiding the dy top coordinates of the pool of particles Î¾1:N
s
towards the measurement y. Note that under the
backward dynamics equation 1.3, Xt and Xt are independent conditionally on Xt+1 with transition kernels
respectively pt(xt|xt+1) := N(xt; mt+1(xt+1), Ïƒ2
t+1Idy) and pt(xt|xt+1) := N(xt; mt+1(xt+1), Ïƒ2
t+1Idxâˆ’dy)
where mt+1(xt+1) âˆˆRdy and mt+1(xt+1) âˆˆRdxâˆ’dy are such that mt+1(xt+1) = mt+1(xt+1)
âŒ¢mt+1(xt+1)
and the above kernels satisfy pt(xt|xt+1) = pt(xt|xt+1)pt(xt|xt+1). We consider the following proposal kernels
for t âˆˆ[1 : n âˆ’1],
py
t (xt|xt+1) âˆpt(xt|xt+1)qt|0(xt|y) ,
where
qt|0(xt|y) := N(xt; Â¯Î±1/2
t
y, (1 âˆ’Â¯Î±t)Idy) .
(2.3)
For the final step, we define py
0(x0|x1) = p0(x0|x1). Using standard Gaussian conjugation formulas, we
obtain
py
t (xt|xt+1) = pt(xt|xt+1) Â· N
Ã„
xt; KtÎ±1/2
t
y + (1 âˆ’Kt)mt+1(xt+1), (1 âˆ’Â¯Î±t)Kt Â· Idy
Ã¤
,
where Kt := Ïƒ2
t+1
(Ïƒ2
t+1+1âˆ’Î±t). For this procedure to target the posterior Ï•y
0, the weight function eÏ‰s is chosen as
follows; we set eÏ‰nâˆ’1(xn) :=
R
pnâˆ’1(xnâˆ’1|xn)qnâˆ’1|0(xnâˆ’1|y)dxnâˆ’1 = N
Ã„
Î±1/2
nâˆ’1y; mn(xn), Ïƒ2
n + 1 âˆ’Î±nâˆ’1
Ã¤
and for t âˆˆ[1 : n âˆ’2],
eÏ‰t(xt+1) :=
R
pt(xt|xt+1)qt|0(xt|y)dxt
qt+1|0(xt+1|y)
=
N
Ã„
Î±1/2
t
y; mt+1(xs+1), (Ïƒ2
t+1 + 1 âˆ’Î±t)Idy
Ã¤
N
Ã„
Î±1/2
t+1y; xt+1, (1 âˆ’Î±t+1)Idy
Ã¤
.
(2.4)
4

Published as a conference paper at ICLR 2024
For the final step, we set eÏ‰0(x1) := p0(y|x1)q1|0(x1|y). The overall SMC algorithm targeting Ï•y
0 using the
instrumental kernel equation 2.3 and weight function equation 2.4 is summarized in Algorithm 1. We now provide
Algorithm 1: MCGdiff (Ïƒ = 0)
Input: Number of particles N
Output: Î¾1:N
0
Î¾1:N
n
i.i.d.
âˆ¼N(0dx, Idx);
// Operations involving index i are repeated for each i âˆˆ[1 : N]
for s â†n âˆ’1 : 0 do
if s = n âˆ’1 then
eÏ‰nâˆ’1(Î¾i
n) = N Â¯Î±1/2
n y; mn(Î¾i
n), 2 âˆ’Â¯Î±n
;
else
eÏ‰s(Î¾i
s+1) = N
Ã„
Â¯Î±1/2
s
y; ms+1(Î¾i
s+1), Ïƒ2
s+1 + 1 âˆ’Â¯Î±s
Ã¤ N
Ã„
Â¯Î±1/2
s+1y; Î¾i
s+1, 1 âˆ’Â¯Î±s+1
Ã¤
;
Ii
s+1 âˆ¼Cat {eÏ‰s(Î¾j
s+1)/ PN
k=1 eÏ‰s(Î¾k
s+1)}N
j=1
,
zi
s âˆ¼N(0dy, Idy),
zi
s âˆ¼N(0dxâˆ’dy, Idxâˆ’dy);
Î¾i
s = Ks Â¯Î±1/2
s
y + (1 âˆ’Ks)ms+1(Î¾
Ii
s+1
s+1 ) + (1 âˆ’Î±s)1/2K1/2
s
zi
s,
Î¾i
s = ms+1(Î¾
Ii
s+1
s+1 ) + Ïƒs+1zi
s;
Set Î¾i
s = Î¾i
s
âŒ¢Î¾i
s;
a justification to Algorithm 1. Let {gy
s}n
s=1 be a sequence of positive functions with gy
n â‰¡1. Consider the
sequence of distributions {Ï•y
s}n
s=1 defined as follows; Ï•y
n(xn) âˆgy
n(xn)pn(xn) and for t âˆˆ[1 : n âˆ’1]
Ï•y
t (xt) âˆ
R
gy
t+1(xt+1)âˆ’1gy
t (xt)pt(xt|xt+1)Ï•y
t+1(dxt+1) .
(2.5)
By construction, the time t marginal equation 2.5 is Ï•y
t (xt) âˆpt(xt)gy
t (xt) for all t âˆˆ[1 : n]. Then, using Ï•y
1
and equation 2.2, we have that Ï•y
0(x0) âˆ
R
gy
1(x1)âˆ’1p0(y|x1)p0(x0|x1)Ï•y
1(dx1).
The recursion equation 2.5 suggests a way of obtaining a particle approximation of Ï•y
0; by sequentially approxi-
mating each Ï•y
t we can effectively derive a particle approximation of the posterior.To construct the intermediate
particle approximations we use the framework of auxiliary particle filters (APF) (Pitt & Shephard, 1999). We
focus on the case gy
t (xt) = qt|0(xt|y) which corresponds to Algorithm 1. The initial particle approximation Ï•y
n is
obtained by drawing N i.i.d. samples Î¾1:N
n
from pn and setting Ï•N
n = N âˆ’1 PN
i=1 Î´Î¾in where Î´Î¾ is the Dirac mass
at Î¾. Assume that the empirical approximation of Ï•y
t+1 is Ï•N
t+1 = N âˆ’1 PN
i=1 Î´Î¾i
t+1 , where Î¾1:N
t+1 are N random
variables. Substituting Ï•N
t+1 into the recursion equation 2.5 and introducing the instrumental kernel equation 2.3,
we obtain the mixture
bÏ•N
t (xt) = PN
i=1 eÏ‰t(Î¾i
t+1)py
t (xt|Î¾i
t+1) PN
j=1 eÏ‰t(Î¾j
t+1) .
(2.6)
Then, a particle approximation of equation 2.6 is obtained by sampling N conditionally i.i.d. ancestor indices
I1:N
t+1
i.i.d.
âˆ¼Cat({eÏ‰t(Î¾i
t+1)/ PN
j=1 eÏ‰t(Î¾j
t+1)}N
i=1), and then propagating each ancestor particle Î¾
Ii
t+1
t+1 according to
the instrumental kernel equation 2.3. The final particle approximation is given by Ï•N
0 = N âˆ’1 PN
i=1 Î´Î¾i
0, where
Î¾i
0 âˆ¼p0(Â·|Î¾Ii
1
1 ), Ii
1 âˆ¼Cat({eÏ‰0(Î¾k
1) PN
j=1 eÏ‰0(Î¾j
1)}N
k=1). The sequence of distributions {pt}n
t=0 approximating
the marginals of the forward process initialized at p0 defines a path that bridges between pn and the prior
p0 such that the discrepancy between pt and pt+1 is small. SMC samplers based on this path are robust to
multi-modality and offer an interesting alternative to the geometric and tempering paths traditionally used in
the SMC literature, see Dai et al. (2022). Our proposals Ï•y
t (xt) âˆpt(xt)qt|0(xt|y) inherit the behavior of
{pt}tâˆˆN and bridge the initial distribution Ï•y
n and posterior Ï•y
0. Indeed, as y is a noiseless observation of
X0 âˆ¼p0, we may consider Â¯Î±1/2
t
y + (1 âˆ’Â¯Î±t)1/2Îµt, with Îµt âˆ¼N(0dy, Idy), as a noisy observation of Xt âˆ¼pt
and thus, Ï•y
t is the associated posterior. We illustrate this intuition by considering the following Gaussian mixture
(GM) example. We assume that p0(x0) = PM
i=1 wi Â· N(x0; Âµi, Idx) where M > 1 and {wi}M
i=1 are drawn
uniformly on the simplex. The marginals of the forward process are available in closed form and are given by
5

Published as a conference paper at ICLR 2024
t = 450
t = 100
t = 80
t = 70
t = 50
t = 20
t = 15
t = 5
Figure 1: Display of samples from Ï•y
t (xt) âˆpt(xt)qt|0(xt|y) for the GM prior. Samples from Ï•y
t (yellow), those
from the prior (purple) and those from the posterior Ï•y
0 (light blue) with n = 500.
pt(xt) = PM
i=1 wi Â· N(xt; Â¯Î±1/2
t
Âµi, Idx), which shows that the discrepancy between pt and pt+1 is small as long
as Â¯Î±1/2
t
âˆ’Â¯Î±1/2
t+1 is close to 0. The posteriors {Ï•y
t }tâˆˆ[0:n] are also available in closed form and displayed in Figure 1,
which illustrates that our choice of potentials ensures that the discrepancy between consecutive posteriors is small.
The idea of using the forward diffused observation to guide the observed part of the state, as we do here through
qt(xt|y), has been exploited in prior works but in a different way. For instance, in Song et al. (2021c; 2022) the
observed part of the state is directly replaced by the forward noisy observation and, as it has been noted Trippe
et al. (2023), this introduces an irreducible bias. Instead, MCGdiff weights the backward process by the density
of the forward one conditioned on y, resulting in a natural and consistent algorithm.
We now establish the convergence of MCGdiff with a general sequence of potentials {gy
s}n
s=1. We consider the
following assumption on the sequence of potentials {gy
t }n
t=1.
(A1)
sup
xâˆˆRdx
p0(y|x)/gy
1(x) < âˆand sup
xâˆˆRdx
Z
gy
t (xt)pt(xt|x)dxt
gy
t+1(x) < âˆfor all t âˆˆ[1 : n âˆ’1].
The following exponential deviation inequality is standard and is a direct application of (Douc et al., 2014, Theorem
10.17). In particular, it implies a O(1/
âˆš
N) bound on the mean squared error âˆ¥Ï•N
0 (h) âˆ’Ï•y
0(h)âˆ¥2.
Proposition 2.1. Assume (A1). There exist constants c1,n, c2,n âˆˆ(0, âˆ) such that, for all N âˆˆN, Îµ > 0
and bounded function h : Rdx 7â†’R, P Ï•N
0 (h) âˆ’Ï•y
0(h)
 â‰¥Îµ â‰¤c1,n exp(âˆ’c2,nNÎµ2|h|2
âˆ) where |h|âˆ:=
supxâˆˆRdx |h(x)|.
We also furnish our estimator with an explicit non-asymptotic bound on its bias. Define Î¦N
0 = EÏ•N
0 ] where
Ï•N
0
= N âˆ’1 PN
i=1 Î´Î¾i
0 is the particle approximation produced by Algorithm 1 and the expectation is with
respect to the law of (Î¾1:N
0:n , I1:N
1:n ). Define for all t âˆˆ[1 : n], Ï•â‹†
t (xt) âˆpt(xt)
R
Î´y(dx0)p0|t(x0|xt)dx0 , where
p0|t(x0|xt) :=
R Â¶Qtâˆ’1
s=0 ps(xs|xs+1)
Â©
dx1:tâˆ’1.
Proposition 2.2. It holds that
KL(Ï•y
0 âˆ¥Î¦N
0 ) â‰¤Cy
0:n(N âˆ’1)âˆ’1 + Dy
0:nN âˆ’2 ,
(2.7)
where Dy
0:n > 0, Cy
0:n := Pn
t=1
R Zt/Z0
gy
t (zt)
Â¶R
Î´y(dx0)p0|t(x0|zt)dx0
Â©
Ï•â‹†
t (dzt) and Zt :=
R
gy
t (xt)pt(dxt) for
all t âˆˆ[1 : n] and Z0 :=
R
Î´y(dx0)p0(x0)dx0. If furthermore (A1) holds then both Cy
0:n and Dy
0:n are finite.
The proof of Proposition 2.2 is postponed to Appendix B.1. (A1) is an assumption on the equivalent of the weights
{eÏ‰t}n
t=0 with a general sequence of potentials {gy
t }n
t=1 and is not restrictive as it can be satisfied by setting for
example gy
s(xs) = qs|0(xs|y) + Î´ where Î´ > 0. The resulting algorithm is then only a slight modification of the
one described above, see Appendix B.1 for more details. It is also worth noting that Proposition 2.2 combined
with Pinskerâ€™s inequality implies that the bias of MCGdiff goes to 0 with the number of particle samples N
for fixed n. We have chosen to present a bound in Kullbackâ€“Leibler (KL) divergence, inspired by Andrieu
et al. (2018); Huggins & Roy (2019), as it allows an explicit dependence on the modeling choice {gy
s}n
s=1, see
Lemma B.2. Finally, unlike the theoretical guarantees established for SMCdiff in Trippe et al. (2023), proving
6

Published as a conference paper at ICLR 2024
the asymptotic exactness of our methodology w.r.t. to the generative model posterior does not require having
ps+1(xs+1)ps(xs|xs+1) = ps(xs)qs+1(xs+1|xs) for all s âˆˆ[0 : n âˆ’1], which does not hold in practice.
Noisy case.
We consider the case Ïƒy > 0. The posterior density is given by Ï•y
0(x0) âˆgy
0(x0)p0(x0), where
gy
0(x0) := N(y; x0, Ïƒ2
yIdy). In what follows, assume that there exists Ï„ âˆˆ[1 : n] such that Ïƒ2 = (1 âˆ’Â¯Î±Ï„)Â¯Î±Ï„.
We denote ËœyÏ„ = Â¯Î±1/2
Ï„
y. We can then write that
gy
0(x0) = Â¯Î±1/2
Ï„
Â· N(ËœyÏ„; Â¯Î±1/2
Ï„
x0, (1 âˆ’Â¯Î±Ï„) Â· Idy) = Â¯Î±1/2
Ï„
Â· qÏ„|0(ËœyÏ„|x0) ,
(2.8)
which hints that the likelihood function gy
0 is closely related to the forward process equation 1.1. We may then
write the posterior Ï•y
0(x0) as Ï•y
0(x0) âˆqÏ„|0(ËœyÏ„|x0)p0(x0) âˆ
R
Î´ËœyÏ„ (dxÏ„)qÏ„|0(xÏ„|x0)p0(x0)dxÏ„. Next, assume
that the forward process equation 1.1 is the reverse of the backward one equation 1.3, i.e. that
pt(xt)qt+1(xt+1|xt) = pt+1(xt+1)pt(xt|xt+1) ,
âˆ€t âˆˆ[0 : n âˆ’1] .
(2.9)
This is similar to the assumption made in SMCdiff Trippe et al. (2023). Then, it is easily seen that it implies
p0(x0)qÏ„|0(xÏ„|x0) = pÏ„(xÏ„)p0|Ï„(x0|xÏ„) and thus
Ï•y
0(x0) =
Z
p0|Ï„(x0|xÏ„)Î´ËœyÏ„ (dxÏ„)pÏ„(xÏ„)dxÏ„
Z
Î´ËœyÏ„ (dzÏ„)pÏ„(zÏ„)dzÏ„ =
Z
p0|Ï„(x0|Ëœy
âŒ¢
Ï„ xÏ„)Ï•ËœyÏ„
Ï„ (dxÏ„) , (2.10)
where Ï•ËœyÏ„
Ï„ (xÏ„) âˆpÏ„(ËœyâŒ¢
Ï„ xÏ„). equation 2.10 highlights that solving the inverse problem equation 2.1 with Ïƒy > 0
is equivalent to solving an inverse problem on the intermediate state XÏ„ âˆ¼pÏ„ with noiseless observation ËœyÏ„
of the dy top coordinates and then propagating the resulting posterior back to time 0 with the backward kernel
p0|Ï„. The assumption equation 2.9 does not always holds in realistic settings.Therefore, while equation 2.10
also holds only approximately in practice, we can still use it as inspiration for designing potentials when the
assumption is not valid. Consider then {gy
t }n
t=Ï„ and sequence of probability measures {Ï•y
t }n
t=Ï„ defined for all
t âˆˆ[Ï„ : n] as Ï•y
t (xt) âˆgy
t (xt)pt(xt), where gy
t (xt) := N(xt; Â¯Î±1/2
t
y, (1 âˆ’(1 âˆ’Îº)Â¯Î±t/Â¯Î±Ï„)Idy), Îº â‰¥0. In the
case of Îº = 0, we have gy
t (xt) = qt|Ï„(xt|ËœyÏ„) for t âˆˆ[Ï„ + 1 : n] and Ï•y
Ï„ = Ï•ËœyÏ„
Ï„ . The recursion equation 2.5
holds for t âˆˆ[Ï„ : n] and assuming Îº > 0, we find that Ï•y
0(x0) âˆgy
0(x0)
R
gy
Ï„(xÏ„)âˆ’1p0|Ï„(x0|xÏ„)Ï•y
Ï„(dxÏ„) , which
resembles the recursion equation 2.10. In practice we take Îº to be small in order to mimick the Dirac delta mass
at xÏ„ in equation 2.10. Having a particle approximation Ï•N
Ï„ = N âˆ’1 PN
i=1 Î´Î¾iÏ„ of Ï•y
Ï„ by adapting Algorithm 1,
we estimate Ï•y
0 with Ï•N
0 = PN
i=1 Ï‰i
0Î´Î¾i
0 where Î¾i
0 âˆ¼p0|Ï„(Â·|Î¾i
Ï„) and Ï‰i
0 âˆgy
0(Î¾i
0)/gy
Ï„(Î¾Ii
Ï„
Ï„ ). In the next section we
extend this methodology to general linear Gaussian observation models. Finally, equation 2.10 allows us to extend
SMCdiff to handle noisy inverse problems in a principled manner which is detailed in Appendix A.
2.1
EXTENSION TO GENERAL LINEAR INVERSE PROBLEMS
Consider Y = AX + ÏƒyÎµ where A âˆˆRdyÃ—dx, Îµ âˆ¼N(0dy, Idy) and Ïƒy â‰¥0 and the singular value decomposition
(SVD) A = USVT , where V âˆˆRdxÃ—dy, U âˆˆRdyÃ—dy are two orthonormal matrices, and S âˆˆRdyÃ—dy is diagonal.
For simplicity, it is assumed that the singular values satisfy s1 > Â· Â· Â· > sdy > 0. Set b = dx âˆ’dy. Let V âˆˆRdxÃ—b
be an orthonormal matrix of which the columns complete those of V into an orthonormal basis of Rdx, i.e.
VT V = Ib and VT V = 0b,dy. We define V = [V, V] âˆˆRdxÃ—dx. In what follows, for a given x âˆˆRdx we
write x âˆˆRdy for its top dy coordinates and x âˆˆRb for the remaining coordinates. Setting X := VT X and
Y := Sâˆ’1UT Y and multiplying the measurement equation by Sâˆ’1UT yields
Y = X + ÏƒySâˆ’1ËœÎµ ,
eÎµ âˆ¼N(0, Idy) .
In this section, we focus on solving this linear inverse problem in the orthonormal basis defined by V using the
methodology developed in the previous sections. This prompts us to define the diffusion based generative model
in this basis. As V is an orthonormal matrix, the law of X0 = VT X0 is p0(x0) := p0(Vx0). By definition of p0
7

Published as a conference paper at ICLR 2024
and the fact that âˆ¥Vxâˆ¥2 = âˆ¥xâˆ¥2 for all x âˆˆRdx we have that
p0(x0) =
R
p0(Vx0|x1)
Â¶Qnâˆ’1
s=1 ps(dxs|xs+1)
Â©
pn(dxn) =
R
Î»0(x0|x1)
Â¶Qnâˆ’1
s=1 Î»s(dxs|xs+1)
Â©
pn(dxn)
where for all s âˆˆ[1 : n], Î»sâˆ’1(xsâˆ’1|xs) := N(xsâˆ’1; ms(xs), Ïƒ2
sIdx), where ms(xs) := VT ms(Vxs). The tran-
sition kernels {Î»s}nâˆ’1
s=0 define a diffusion based model in the basis V. We write ms(xs) for the first dy coordinates
of ms(xs) and ms(xs) the last b coordinates and denote by ps the time s marginal of the backward process.
Noiseless. In this case the target posterior is Ï•y
0(x0) âˆp0(yâŒ¢x0). The extension of algorithm 1 is straight
forward; it is enough to replace y with y (=Sâˆ’1UT y) and the backward kernels {pt}nâˆ’1
t=0 with {Î»t}nâˆ’1
t=0 .
Noisy. The posterior density is then Ï•y
0(x0) âˆgy
0 (x0)p0(x0), where gy
0 (x0) = Qdy
i=1 N(y[i]; x0[i], (Ïƒy/si)2).
As in Section 2, assume that there exists {Ï„i}dy
i=1 âŠ‚[1 : n] such that Â¯Î±Ï„iÏƒ2
y = (1 âˆ’Â¯Î±Ï„i)s2
i and define for all
i âˆˆ[1 : dy], Ëœyi := Â¯Î±1/2
Ï„i y[i]. Then we can write the potential gy
0 in a similar fashion to equation 2.8 as the product
of forward processes from time 0 to each time step Ï„i, i.e. gy
0 (x0) = Qdy
i=1 Â¯Î±1/2
Ï„i N(Ëœyi; Â¯Î±1/2
Ï„i x0[i], (1âˆ’Â¯Î±Ï„i)). Writ-
ing the potential this way allows us to generalize equation 2.10 as follows. Denote for â„“âˆˆ[1 : dx], x\â„“âˆˆRdxâˆ’1
the vector x with its â„“-th coordinate removed. Define
Ï•Ëœy
Ï„1:n(dxÏ„1:n) âˆ
Â¶Qdyâˆ’1
i=1 Î»Ï„i|Ï„i+1(xÏ„i|xÏ„i+1)Î´Ëœyi(dxÏ„i[i])dx\i
Ï„i
Â©
pÏ„dy (xÏ„dy )Î´Ëœydy (dxÏ„dy [dy])dx\dy
Ï„dy ,
which corresponds to the posterior of a noiseless inverse problem on the joint states XÏ„1:n âˆ¼pÏ„1:n with noiseless
observations ËœyÏ„i of XÏ„i[i] for all i âˆˆ[1 : dy].
Proposition 2.3. Assume that ps+1(xs+1)Î»s(xs|xs+1) = ps(xs)qs+1(xs+1|xs) for all s âˆˆ[0 : n âˆ’1]. Then it
holds that Ï•y
0(x0) âˆ
R
Î»0|Ï„1(x0|xÏ„1)Ï•Ëœy
Ï„1:n(dxÏ„1:n).
The proof of Proposition 2.3 is given in Appendix B.2. We have shown that sampling from Ï•y
0 is equiv-
alent to sampling from Ï•Ëœy
Ï„1:n then propagating the final state XÏ„1 to time 0 according to Î»0|Ï„1.
There-
fore, as in equation 2.8, we define {gy
t }n
t=Ï„1 and {Ï•y
t }n
t=Ï„1 for all t âˆˆ[Ï„1 : n] by Ï•y
t (xt) âˆgy
t (xt)pt(xt)
and gy
t (xt) := QÏ„(t)
i=1 N
Ã„
xt; Â¯Î±1/2
t
yi, 1 âˆ’(1 âˆ’Îº)Â¯Î±t/Â¯Î±Ï„i
Ã¤
, Îº > 0.
We obtain a particle approximation of
Ï•y
Ï„1 using a particle filter with proposal kernel and weight function Î»y
t (xt|xt+1) âˆgy
t (xt)pt(xt|xt+1),
eÏ‰t(xt+1) =
R
gy
t (xt)pt(dxt|xt+1)gy
t+1(xt+1), which are both available in closed form.
3
NUMERICS
A prerequisite for quantitative evaluation in ill-posed inverse problems in a Bayesian setting is to have access
to samples of the posterior distribution. This generally requires having at least an unnormalized proxy of the
posterior density, so that one can run MCMC samplers such as the No U-turn sampler (NUTS) Hoffman &
Gelman (2011). Therefore, this section focus on mixture models of two types of basis distribution, the Gaussian
and the Funnel distributions. We then present a brief illustration of MCGdiff on image data. However, in this
setting, the actual posterior distribution is unknown and the main goal is to explore the potentially multimodal
posterior distribution, which makes a comparison with a "real image" meaningless. Therefore, metrics such as
FrÃ©chet Inception Distance (FID) and LPIPS score, which require comparison to a ground truth, are not useful for
evaluating Bayesian reconstruction methods in such settings.1
Mixture Models.
We refer to the Funnel mixture prior as FM prior (see Appendix B.3). For GM prior, we
consider a mixture of 25 components with known means and variances. For FM prior, we consider a mixture of 20
components consisting of rotated and translated funnel distributions. For a given pair (dx, dy), we sample a prior
distribution by randomly sampling the weights of the mixture and for the FM case the translation and rotation
of each component. We then randomly sample measurement models (y, A, Ïƒy) âˆˆRdy Ã— RdyÃ—dx Ã— [0, 1]. For
each pair of prior distribution and measurement model, we generate 104 samples from MCGdiff, DPS, DDRM,
RNVP, and from the posterior either analytically (GM) or using NUTS (FM). We calculate for each algorithm the
sliced Wasserstein (SW) distance between the resulting samples and the posterior samples. Table 1 shows the CLT
1The code for the experiments is available at https://github.com/gabrielvc/mcg_diff.
8

Published as a conference paper at ICLR 2024
x2
MCGdiff
DDRM
DPS
RNVP
MCGdiff
DDRM
DPS
RNVP
PCA2
x1
PCA1
Figure 2: The first and last four columns correspond respectively to GM with (dx, dy) = (800, 1) and FM with
(dx, dy) = (10, 1). The blue and red dots represent respectively samples from the exact posterior and those
generated by each of the algorithms used (names on top).
d
dy
MCGdiff
DDRM
DPS
RNVP
80
1
1.39 Â± 0.45
5.64 Â± 1.10
4.98 Â± 1.14
6.86 Â± 0.88
80
2
0.67 Â± 0.24
7.07 Â± 1.35
5.10 Â± 1.23
7.79 Â± 1.50
80
4
0.28 Â± 0.14
7.81 Â± 1.48
4.28 Â± 1.26
7.95 Â± 1.61
800
1
2.40 Â± 1.00
7.44 Â± 1.15
6.49 Â± 1.16
7.74 Â± 1.34
800
2
1.31 Â± 0.60
8.95 Â± 1.12
6.88 Â± 1.01
8.75 Â± 1.02
800
4
0.47 Â± 0.19
8.39 Â± 1.48
5.51 Â± 1.18
7.81 Â± 1.63
d
dy
MCGdiff
DDRM
DPS
RNVP
6
1
1.95 Â± 0.43
4.20 Â± 0.78
5.43 Â± 1.05
6.16 Â± 0.65
6
3
0.73 Â± 0.33
2.20 Â± 0.67
3.47 Â± 0.78
4.70 Â± 0.90
6
5
0.41 Â± 0.12
0.91 Â± 0.43
2.07 Â± 0.63
3.52 Â± 0.93
10
1
2.45 Â± 0.42
3.82 Â± 0.64
4.30 Â± 0.91
6.04 Â± 0.38
10
3
1.07 Â± 0.26
4.94 Â± 0.87
5.38 Â± 0.84
5.91 Â± 0.64
10
5
0.71 Â± 0.12
2.32 Â± 0.74
3.74 Â± 0.77
5.11 Â± 0.69
Table 1: Sliced Wasserstein for the GM (left) and FM (right) case.
95% confidence intervals obtained over 20 seeds. Figure 2 illustrates the samples for the different algorithms for
a given seed. We see that MCGdiff outperforms all the other algorithms in each setting tested. The complete
details of the numerical experiments are available in Appendix B.3 as well as additional visualisations.
Image datasets.
Figure 3 shows samples of MCGdiff in different datasets (Celeb, Churches, Bedroom and
Flowers) for different inverse problems, namely Inpaiting (Inp), super resolution (SR), Gaussian 2D deblur
(G2Deb) and Colorization (Col). Visual comparison with competing algorithms and different datasets are shown
in Appendix B.3 as well as numerical details concerning Figure 3.
4
CONCLUSION
In this paper, we present MCGdiff a novel method for solving Bayesian linear Gaussian inverse problems
with SGM priors.We show that MCGdiff is theoretically grounded and provided numerical experiments that
reflect the adequacy of MCGdiff in a Bayesian framework, as opposed to recent works. This difference is of
the uttermost importance when the relevance of the generated samples is hard to verify, as in safety critical
applications. MCGdiff is a first step towards robust approaches for addressing the challenges of Bayesian linear
inverse problems with SGM priors.
Y
MCGdiff
MCGdiff
MCGdiff
MCGdiff
Y
Inp
Ïƒy = 0
Celeb
SR
Ïƒy = 0.05
Bedroom
Col
Ïƒy = 0
Flowers
G2Deb
Ïƒy = 0.1
Church
Figure 3: Illustration of the samples of MCGdiff for different datasets and different inverse problems.
9

Published as a conference paper at ICLR 2024
REFERENCES
Christophe Andrieu, Anthony Lee, and Matti Vihola. Uniform ergodicity of the iterated conditional SMC and
geometric ergodicity of particle Gibbs samplers. Bernoulli, 24(2):842 â€“ 872, 2018. doi: 10.3150/15-BEJ785.
URL https://doi.org/10.3150/15-BEJ785.
Siavash Arjomand Bigdeli, Matthias Zwicker, Paolo Favaro, and Meiguang Jin. Deep mean-shift priors for image
restoration. Advances in Neural Information Processing Systems, 30, 2017.
Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. From denoising
diffusions to denoising markov models. arXiv preprint arXiv:2211.03595, 2022.
Julian Besag, Jeremy York, and Annie MolliÃ©. Bayesian image restoration, with two applications in spatial
statistics. Annals of the institute of statistical mathematics, 43:1â€“20, 1991.
Olivier CappÃ©, Eric Moulines, and Tobias Ryden. Inference in Hidden Markov Models (Springer Series in
Statistics). Springer-Verlag, Berlin, Heidelberg, 2005. ISBN 0387402640.
Margaret Cheney and Brett Borden. Fundamentals of radar imaging. SIAM, 2009.
Nicolas Chopin, Omiros Papaspiliopoulos, et al. An introduction to sequential Monte Carlo, volume 4. Springer,
2020.
Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion
posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=OnD9zGAGT0k.
Chenguang Dai, Jeremy Heng, Pierre E Jacob, and Nick Whiteley. An invitation to sequential monte carlo
samplers. Journal of the American Statistical Association, 117(539):1587â€“1600, 2022.
Masoumeh Dashti and Andrew M Stuart. The bayesian approach to inverse problems. In Handbook of uncertainty
quantification, pp. 311â€“428. Springer, 2017.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=HkpbnH9lx.
Randal Douc, Eric Moulines, and David Stoffer. Nonlinear time series: Theory, methods and applications with R
examples. CRC press, 2014.
Arnaud Doucet, Nando De Freitas, Neil James Gordon, et al. Sequential Monte Carlo methods in practice,
volume 1. Springer, 2001.
Idris A Elbakri and Jeffrey A Fessler. Statistical image reconstruction for polyenergetic x-ray computed tomogra-
phy. IEEE transactions on medical imaging, 21(2):89â€“99, 2002.
Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis, and William T Freeman. Removing camera shake
from a single photograph. In Acm Siggraph 2006 Papers, pp. 787â€“794. 2006.
MÃ¡rio AT Figueiredo, JosÃ© M Bioucas-Dias, and Robert D Nowak. Majorizationâ€“minimization algorithms for
wavelet-based image restoration. IEEE Transactions on Image processing, 16(12):2980â€“2991, 2007.
Rafael Corsino GonzÃ¡lez, Richard E. Woods, and Barry R. Masters. Digital image processing, third edition.
Journal of Biomedical Optics, 14:029901, 2009.
Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adversarial networks:
Algorithms, theory, and applications. IEEE transactions on knowledge and data engineering, 2021.
10

Published as a conference paper at ICLR 2024
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840â€“6851, 2020.
Matthew Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths in hamiltonian
monte carlo. Journal of Machine Learning Research, 15, 11 2011.
Jonathan H. Huggins and Daniel M. Roy. Sequential Monte Carlo as approximate sampling: bounds, adaptive
resampling via âˆ-ESS, and an application to particle Gibbs. Bernoulli, 25(1):584 â€“ 622, 2019. doi: 10.3150/
17-BEJ999. URL https://doi.org/10.3150/17-BEJ999.
JÃ©rÃ´me Idier. Bayesian approach to inverse problems. John Wiley & Sons, 2013.
Oleg Ivanov, Michael Figurnov, and Dmitry Vetrov. Variational autoencoder with arbitrary conditioning. arXiv
preprint arXiv:1806.02382, 2018.
Jari P Kaipio, Ville Kolehmainen, Erkki Somersalo, and Marko Vauhkonen. Statistical inversion and monte carlo
sampling methods in electrical impedance tomography. Inverse problems, 16(5):1487, 2000.
Sebastian Kaltenbach, Paris Perdikaris, and Phaedon-Stelios Koutsourelakis. Semi-supervised invertible neural
operators for bayesian inverse problems. Computational Mechanics, pp. 1â€“20, 2023.
Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In
Advances in Neural Information Processing Systems.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann
LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,
May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.
Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and TrendsÂ® in
Machine Learning, 12(4):307â€“392, 2019.
Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and review of
current methods. IEEE transactions on pattern analysis and machine intelligence, 43(11):3964â€“3979, 2020.
Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint:
Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 11461â€“11471, 2022.
Yosra Marnissi, Yuling Zheng, Emilie Chouzenoux, and Jean-Christophe Pesquet. A variational bayesian approach
for image restorationâ€”application to image deblurring with poissonâ€“gaussian noise. IEEE Transactions on
Computational Imaging, 3(4):722â€“737, 2017.
Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image inpainting with
hierarchical vq-vae. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 10775â€“10784, 2021.
Michael K Pitt and Neil Shephard. Filtering via simulation: Auxiliary particle filters. Journal of the American
statistical association, 94(446):590â€“599, 1999.
Teemu SahlstrÃ¶m and Tanja Tarvainen. Utilizing variational autoencoders in the bayesian inverse problem of
photoacoustic tomography. SIAM Journal on Imaging Sciences, 16(1):89â€“110, 2023.
Hyomin Shin and Minseok Choi. Physics-informed variational inference for uncertainty quantification of stochastic
differential equations. Journal of Computational Physics, pp. 112183, 2023.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256â€“2265. PMLR,
2015.
11

Published as a conference paper at ICLR 2024
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International
Conference on Learning Representations, 2021a.
URL https://openreview.net/forum?id=
St1giarCHLP.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based
diffusion models. Advances in Neural Information Processing Systems, 34:1415â€“1428, 2021b.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential equations. In International Conference on Learning
Representations, 2021c. URL https://openreview.net/forum?id=PxTIG12RRHS.
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with
score-based generative models.
In International Conference on Learning Representations, 2022.
URL
https://openreview.net/forum?id=vaRCHVj0uGI.
Andrew M Stuart. Inverse problems: a bayesian perspective. Acta numerica, 19:451â€“559, 2010.
Jingwen Su, Boyan Xu, and Hujun Yin. A survey of deep learning approaches to image restoration. Neurocomput-
ing, 487:46â€“65, 2022.
Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, and Tommi S.
Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. In
The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.
net/forum?id=6TxBxqNME1Y.
Marinus T Vlaardingerbroek and Jacques A Boer. Magnetic resonance imaging: theory and practice. Springer
Science & Business Media, 2013.
Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao. High-fidelity pluralistic image completion with
transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4692â€“4701,
2021.
Xinyi Wei, Hans van Gorp, Lizeth Gonzalez-Carabarin, Daniel Freedman, Yonina C Eldar, and Ruud JG van Sloun.
Deep unfolding with normalizing flow priors for inverse problems. IEEE Transactions on Signal Processing,
70:2962â€“2971, 2022.
Luhuan Wu, Brian L. Trippe, Christian A. Naesseth, David M. Blei, and John P. Cunningham. Practical and
asymptotically exact conditional sampling in diffusion models, 2023.
Hanyu Xiang, Qin Zou, Muhammad Ali Nawaz, Xianfeng Huang, Fan Zhang, and Hongkai Yu. Deep learning for
image inpainting: A survey. Pattern Recognition, 134:109046, 2023.
Raymond A Yeh, Teck Yian Lim, Chen Chen, Alexander G Schwing, Mark Hasegawa-Johnson, and Minh N Do.
Image restoration with deep generative models. In 2018 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 6772â€“6776. IEEE, 2018.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with
contextual attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5505â€“5514, 2018.
Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Aggregated contextual transformations for
high-resolution image inpainting. IEEE Transactions on Visualization and Computer Graphics, 2022.
Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, and Shiyu Chang. Towards coherent image
inpainting using denoising diffusion implicit models. arXiv preprint arXiv:2304.03322, 2023.
12

Published as a conference paper at ICLR 2024
Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic image completion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 1438â€“1447, 2019.
Xu Zhihang, Xia Yingzhi, and Liao Qifeng. A domain-decomposed vae method for bayesian inverse problems.
arXiv preprint arXiv:2301.05708, 2023.
13

Published as a conference paper at ICLR 2024
A
SMCD I F F EXTENSION
The identity equation 2.10 allows us to extend SMCdiff Trippe et al. (2023) to handle noisy inverse problems as
we now show. We have that
Ï•ËœyÏ„
Ï„ (xÏ„) =
R
pÏ„(ËœyâŒ¢
Ï„ xÏ„|xÏ„+1)
Â¶Qnâˆ’1
s=Ï„+1 ps(dxs|xs+1)
Â©
pn(dxn)
R
pÏ„(ËœyâŒ¢
Ï„ zÏ„)dzÏ„
=
Z
bËœyÏ„
Ï„:n(xÏ„:n|xÏ„+1:n)f ËœyÏ„
Ï„+1:n(dxÏ„+1:n)dxÏ„+1:n ,
where
bÏ„:n(xÏ„:n|xÏ„+1:n) =
pÏ„(ËœyâŒ¢
Ï„ xÏ„|xÏ„+1)
Â¶Qnâˆ’1
s=Ï„+1 ps(xs|xs+1)ps(xs|xs+1)
Â©
pn(xn)
LËœyÏ„
Ï„:n(xÏ„+1:n)
,
f ËœyÏ„
Ï„+1:n(xÏ„+1:n) =
LËœyÏ„
Ï„:n(xÏ„+1:n)
R
pÏ„(ËœyâŒ¢
Ï„ zÏ„)dzÏ„
,
and
LËœyÏ„
Ï„:n(xÏ„+1:n) =
Z
pÏ„(Ëœy
âŒ¢
Ï„ zÏ„|xÏ„+1
âŒ¢zÏ„+1)
( nâˆ’1
Y
s=Ï„+1
ps(dzs|xs+1
âŒ¢zs+1)ps(xs|xs+1
âŒ¢zs+1)
)
pn(dzn) .
Next, equation 2.9 implies that
Z
ps+1(xs+1
âŒ¢zs+1)ps(dzs|xs+1
âŒ¢zs+1)ps(xs|xs+1
âŒ¢zs+1)dzs:s+1 =
Z
ps(xs
âŒ¢zs)qs+1(xs+1|xs)qs+1(zs+1|zs)dzs:s+1 ,
and applied repeatedly, we find that
LËœyÏ„ (xÏ„+1:n) =
Z
pÏ„(Ëœy
âŒ¢
Ï„ xÏ„)dxÏ„ Â·
Z
Î´ËœyÏ„ (dxÏ„)
n
Y
s=Ï„+1
qs(xs|xsâˆ’1) .
and thus, f ËœyÏ„
Ï„:n(xÏ„+1:n) =
R
Î´ËœyÏ„ (dxÏ„) Qn
s=Ï„+1 qs(xs|xsâˆ’1). In order to approximate Ï•ËœyÏ„
Ï„
we first diffuse the
noised observation up to time n, resulting in xÏ„+1:n, and then estimate bËœyÏ„
Ï„+1:n(Â·|xÏ„+1:n) using a particle filter with
ps(xs|xs+1) as transition kernel at step s âˆˆ[Ï„ + 1 : n] and gs : zs 7â†’psâˆ’1(xsâˆ’1|xs
âŒ¢zs) as potential, similarly
to SMCdiff.
B
PROOFS
B.1
PROOF OF PROPOSITION 2.2
PRELIMINARY DEFINITIONS.
We preface the proof with notations and definitions of a few quantities that will be used throughout.
For a probability measure Âµ and f a bounded measurable function, we write Âµ(f) :=
R
f(x)Âµ(dx) the expectation
of f under Âµ and if K(dx|z) is a transition kernel we write K(f)(z) :=
R
f(x)K(dx|z).
Define the smoothing distribution
Ï•y
0:n(dx0:n) âˆÎ´y(dx0)p0:n(x0:n)dx0dx1:n ,
(B.1)
14

Published as a conference paper at ICLR 2024
which admits the posterior Ï•y
0 as time 0 marginal. Its particle estimate known as the poor man smoother is given
by
Ï•N
0:n(dx0:n) = N âˆ’1
X
k0:nâˆˆ[1:N]n+1
Î´yâŒ¢Î¾k0
0 (dx0)
n
Y
s=1
1ks = Iksâˆ’1
s
	Î´Î¾ks
s (dxs) .
(B.2)
We also let Î¦N
0:n be the probability measure defined for any B âˆˆB(Rdx)âŠ—n+1 by
Î¦N
0:n(B) = EÏ•N
0:n(B) ,
where the expectation is with respect to the probability measure
P N
0:n
 d(x1:N
0:n , a1:N
1:n ) =
N
Y
i=1
py
n(dxi
n)
n
Y
â„“=2
ï£±
ï£²
ï£³
N
Y
j=1
N
X
k=1
Ï‰k
â„“âˆ’1Î´k(daj
â„“)py
â„“âˆ’1(dxj
â„“âˆ’1|x
aj
â„“
â„“)
ï£¼
ï£½
ï£¾
Ã—
N
Y
j=1
N
X
k=1
Ï‰k
0Î´k(daj
1)py
0(dxj
0|x
aj
1
1 )Î´y(dxj
0) ,
(B.3)
where Ï‰i
t := eÏ‰t(Î¾i
t+1)/ PN
j=1 eÏ‰t(Î¾j
t+1) and which corresponds to the joint law of all the random variables
generated by Algorithm 1. It then follows by definition that for any C âˆˆB(Rdx),
Z
Î¦N
0:n(dz0:n)1C(z0) = E
Ã¯Z
Ï•N
0:n(dz0:n)1C(z0)
Ã²
= EÏ•N
0 (C) = Î¦N
0 (C) .
Define also the law of the conditional particle cloud
PN d(x1:N
0:n , a1:N
1:n )
z0:n
 = Î´zn(dxN
n )
Nâˆ’1
Y
i=1
py
n(dxi
n)
Ã—
n
Y
â„“=2
Î´zâ„“âˆ’1(dxN
â„“âˆ’1)Î´N(daN
â„“âˆ’1)
Nâˆ’1
Y
j=1
N
X
k=1
Ï‰k
â„“âˆ’1Î´k(daj
â„“)py
â„“âˆ’1(dxj
â„“âˆ’1|x
aj
â„“
â„“)
Ã— Î´z0(dxN
0 )Î´N(daN
1 )
Nâˆ’1
Y
j=1
N
X
k=1
Ï‰k
0Î´k(daj
1)py
0(dxj
0|x
aj
1
1 )Î´y(dxj
0) .
(B.4)
In what follows Ez0:n refers to expectation with respect to PN(Â·|z0:n). Finally, for s âˆˆ[0 : nâˆ’1] we let â„¦N
s denote
the sum of the filtering weights at step s, i.e. â„¦N
s = PN
i=1 eÏ‰s(Î¾i
s+1). We also write Z0 =
R
p0(x0)Î´y(dx0)dx0
and for all â„“âˆˆ[1 : n], Zâ„“=
R
qâ„“|0(xâ„“|y)pâ„“(dxâ„“).
The proof of Proposition 2.2 relies on two Lemmata stated below and proved in Appendix B.1; in Lemma B.1 we
provide an expression for the Radon-Nikodym derivative dÏ•y
0:n/dÎ¦y
0:n and in Lemma B.2 we explicit its leading
term.
Lemma B.1. Ï•y
0:n and Î¦N
0:n are equivalent and we have that
Î¦N
0:n(dz0:n) = Ez0:n
Ã±
N nZ0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
Ï•y
0:n(dz0:n) .
(B.5)
15

Published as a conference paper at ICLR 2024
Lemma B.2. It holds that
Zn
Z0
Ez0:n
"nâˆ’1
Y
s=0
N âˆ’1â„¦N
s
#
=
Ã…N âˆ’1
N
Ã£n
+ (N âˆ’1)nâˆ’1
N n
n
X
s=1
Zs/Z0
qs|0(zs|y)
Z
p0|s(x0|zs)Î´y(dx0)dx0 + Dy
0:n
N 2 .
(B.6)
where Dy
0:n is a positive constant.
Before proceeding with the proof of Proposition 2.2, let us note that having z 7â†’eÏ‰â„“(z) bounded on Rdx for all
â„“âˆˆ[0 : n âˆ’1] is sufficient to guarantee that Cy
0:n and Dy
0:n are finite since in this case it follows immediately that
Ez0:n
Ã®Qnâˆ’1
s=0 N âˆ’1â„¦N
s
Ã³
is bounded and so is the right hand side of equation B.6. This can be achieved with a slight
modification of equation 2.5 and equation ??. Indeed, consider instead the following recursion for s âˆˆ[0 : n]
where Î´ > 0,
Ï•y
n(xn) âˆ qn|0(xn|y) + Î´pn(xn) ,
Ï•y
s(xs) âˆ
Z
Ï•y
s+1(xs+1)ps(dxs|xs+1)
qs(xs|y) + Î´
qs+1(xs+1|y) + Î´ dxs+1 .
Then we have that
Ï•y
0(x0) âˆ
Z
Ï•y
1(x1)p0(x0|x1)
p0(y|x1)
q1|0(x1|y) + Î´ dx1 .
We can then use Algorithm 1 to produce a particle approximation of Ï•y
0 using the following transition and weight
function,
py,Î´
s (xs|xs+1) =
Î³s(y|xs+1)
Î³s(y|xs+1) + Î´ py
s(xs|xs+1) +
Î´
Î³s(y|xs+1) + Î´ ps(xs|xs+1) ,
eÏ‰s(xs+1) =  Î³s(y|xs+1) + Î´ qs+1|0(xs+1|y) + Î´ ,
where Î³s(y|xs+1) =
R
qs|0(xs|y)ps(xs|xs+1)dxs is available in closed form and py
s is defined in equation 2.3.
eÏ‰s is thus clearly bounded for all s âˆˆ[0 : n âˆ’1] and it is still possible to sample from py,Î´
s
since it is simply a
mixture between the transition equation 2.3 and the â€œpriorâ€ transition.
Proof of Proposition 2.2. Consider the forward Markov kernel
âˆ’â†’
B1:n(z0, dz1:n) =
p1:n(dz1:n)p0(z0|z1)
R
p1:n(dËœz1:n)p0(Ëœz0|Ëœz1) ,
(B.7)
which satisfies
Ï•y
0:n(dz0:n) = Ï•y
0(dz0)âˆ’â†’
B1:n(z0, dz1:n) .
By Lemma B.1 we have for any C âˆˆB(Rdx) that
Î¦N
0 (C) =
Z
Î¦N
0:n(dz0:n)1C(z0)
=
Z
1C(z0)Ez0:n
Ã±
N nZ0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
Ï•y
0:n(dz0:n)
=
Z
1C(z0)
Z âˆ’â†’
B1:n(z0, dz1:n)Ez0:n
Ã±
N nZ0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
Ï•y
0(dz0) ,
16

Published as a conference paper at ICLR 2024
which shows that the Radon-Nikodym derivative dÎ¦N
0 /dÏ•y
0 is,
dÎ¦N
0
dÏ•y
0
(z0) =
Z âˆ’â†’
B1:n(z0, dz1:n)Ez0:n
Ã±
N nZ0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
.
Applying Jensenâ€™s inequality twice yields
dÎ¦N
0
dÏ•y
0
(z0) â‰¥
N nZ0/Zn
R âˆ’â†’
B1:n(z0, dz1:n)Ez0:n
Ã®Qnâˆ’1
s=0 â„¦N
s
Ã³ ,
and it then follows that
KL(Ï•y
0 âˆ¥Î¦N
0 ) â‰¤
Z
log
 
Zn
Z0
Z âˆ’â†’
B1:n(z0, dz1:n)Ez0:n
"nâˆ’1
Y
s=0
N âˆ’1â„¦N
s
#!
Ï•y
0(dz0) .
Finally, using Lemma B.2 and the fact that log(1 + x) < x for x > 0 we get
KL(Ï•y
0 âˆ¥Î¦N
0 ) â‰¤
Cy
0:n
N âˆ’1 + Dy
0:n
N 2
where
Cy
0:n :=
n
X
s=1
Z
Zs/Z0
qs|0(zs|y)
Ã„
p0|s(x0|zs)Î´y(dx0)dx0
Ã¤
Ï•y
s(dzs) ,
and Ï•y
s(zs) âˆps(zs)
R
p0|s(z0|zs)Î´y(dz0)dz0.
PROOF OF LEMMA B.1 AND LEMMA B.2
Proof of Lemma B.1. We have that
Î¦N
0:n(dz0:n)
= N âˆ’1
Z
P N
0:n(dx1:N
0:n , da1:N
1:n )
X
k0:nâˆˆ[1:N]n+1
Î´yâŒ¢xk0
0 (dz0)
n
Y
s=1
1ks = aksâˆ’1
s
	Î´xks
s (dzs)
= N âˆ’1
Z X
k0:n
X
a1:N
1:n
Î´yâŒ¢xk0
0 (dz0)
n
Y
s=1
1ks = aksâˆ’1
s
	Î´xks
s (dzs)
Ã—
N
Y
j=1
py
n(dxj
n)
( n
Y
â„“=2
N
Y
i=1
Ï‰ai
â„“
â„“âˆ’1py
â„“âˆ’1(dxi
â„“âˆ’1|xai
â„“
â„“)
) N
Y
r=1
Ï‰ar
1
0 py
â„“âˆ’1(dxr
0|xar
1
1 )Î´y(xr
0)
= N âˆ’1
Z X
k0:n
X
a1:N
1:n
py
n(dxkn
n )Î´xkn
n (dzn)
Y
jÌ¸=kn
py
n(dxj
n)
n
Y
â„“=2
ÃŸ
Y
iÌ¸=kâ„“âˆ’1
Ï‰ai
â„“
â„“âˆ’1py
â„“âˆ’1(dxi
â„“âˆ’1|xai
â„“
â„“)
Ã— 1akâ„“âˆ’1
â„“
= kâ„“} ËœÏ‰â„“âˆ’1(x
a
kâ„“âˆ’1
â„“
â„“
)
â„¦N
â„“âˆ’1
py
â„“âˆ’1(dxkâ„“âˆ’1
â„“
|x
a
kâ„“âˆ’1
â„“
â„“
)Î´x
kâ„“âˆ’1
â„“âˆ’1 (dzâ„“âˆ’1)
â„¢
Ã—
ÃŸ Y
rÌ¸=k0
Ï‰ar
1
0 py
0(dxr
0|xar
1
1 )Î´y(dxr
0)
â„¢
1ak0
1 = k1
	 ËœÏ‰0(x
ak0
1
1
)
â„¦N
0
py
0(dxk0
0 |x
ak0
1
0
)Î´yâŒ¢xk0
0 (dz0) .
17

Published as a conference paper at ICLR 2024
Then, using that for all s âˆˆ[2 : n]
eÏ‰sâˆ’1(xks
s )py
sâˆ’1(dxksâˆ’1
sâˆ’1 |xks
s ) =
qsâˆ’1|0(xksâˆ’1
sâˆ’1 |y)
qs|0(xks
s |y)
ps(dxksâˆ’1
sâˆ’1 |xks
s ) ,
we recursively get that
py
n(dxkn
n )Î´xkn
n (dzn)
n
Y
s=2
1aksâˆ’1
s
= ks} ËœÏ‰sâˆ’1(xa
ksâˆ’1
s
s
)
â„¦N
sâˆ’1
py
sâˆ’1(dxksâˆ’1
sâˆ’1 |xa
ksâˆ’1
s
s
)Î´x
ksâˆ’1
sâˆ’1 (dzsâˆ’1)
Ã— 1ak0
1 = k1
	 ËœÏ‰0(x
ak0
1
1
)
â„¦N
0
py
0(dxk0
0 |x
ak0
1
1
)Î´yâŒ¢xk0
0 (dz0)
=
qn|0(zn|y)pn(dzn)
Zn
Î´zn(dxkn
n )
n
Y
s=2
1aksâˆ’1
s
= ks}
qsâˆ’1|0(zsâˆ’1|y)
â„¦N
sâˆ’1qs|0(zs|y)psâˆ’1(dzsâˆ’1|zs)Î´zsâˆ’1(dxksâˆ’1
sâˆ’1 )
Ã— 1ak0
1 = k1
	
p0(y|z1)
â„¦N
0 q1|0(z1|y)p0(dz0|z1)Î´y(dz0)Î´z0(dxk0
0 )
= Z0
Zn
Ï•y
0:n(dz0:n)Î´zn(dxkn
n )
n
Y
s=1
1aksâˆ’1
s
= ks}
1
â„¦N
sâˆ’1
Î´zsâˆ’1(dxksâˆ’1
sâˆ’1 ) .
Thus, we obtain
Î¦N
0:n(dz0:n) = N âˆ’1
Z X
k0:n
X
a1:N
1:n
Ï•y
0:n(dz0:n) Z0/Zn
Qnâˆ’1
s=0 â„¦N
s
Î´zn(dxkn
n )
Y
jÌ¸=kn
py
n(dxj
n)
Ã—
n
Y
â„“=2
1akâ„“âˆ’1
â„“
= kâ„“
	Î´zâ„“âˆ’1(dxkâ„“âˆ’1
â„“âˆ’1 )
Y
iÌ¸=kâ„“âˆ’1
Ï‰ai
â„“
â„“âˆ’1py
â„“âˆ’1(dxi
â„“âˆ’1|xai
â„“
â„“)
Ã— 1ak0
1 = k1
	Î´z0(dxk0
0 )
Y
iÌ¸=k0
Ï‰ai
1
0 p0(xi
0|xai
1
1 )Î´y(dxi
0)
= N âˆ’1 X
k0:n
Ï•y
0:n(dz0:n)Ek0:n
z0:n
Ã±
Z0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
,
where for all k0:n âˆˆ[1 : N]n+1 Ek0:n
z0:n denotes the expectation under the Markov kernel
PN
k0:n
 d(x1:N
0:n , a1:N
1:n )
z0:n
 = Î´zn(dxkn
n )
Y
iÌ¸=kn
py
n(dxi
n)
Ã—
n
Y
â„“=2
Î´zâ„“âˆ’1(dxkâ„“âˆ’1
â„“âˆ’1 )Î´kâ„“(dakâ„“âˆ’1
â„“
)
Y
jÌ¸=kâ„“âˆ’1
N
X
k=1
Ï‰k
â„“âˆ’1Î´k(daj
â„“)py
â„“âˆ’1(dxj
â„“âˆ’1|x
aj
â„“
â„“)
Ã— Î´z0(dxk0
0 )Î´k1(dak0
1 )
Y
jÌ¸=k0
N
X
k=1
Ï‰k
0Î´k(daj
1)py
0(dxj
0|x
aj
1
1 )Î´y(dx0) .
Note however that for all (k0:n, â„“0:n) âˆˆ([1 : N]n+1)2,
Ek0:n
z0:n
Ã±
1
Qnâˆ’1
s=0 â„¦N
s
Ã´
= Eâ„“0:n
z0:n
Ã±
1
Qnâˆ’1
s=0 â„¦N
s
Ã´
18

Published as a conference paper at ICLR 2024
and thus it follows that
Î¦N
0:n(dz0:n) = Ez0:n
Ã±
N nZ0/Zn
Qnâˆ’1
s=0 â„¦N
s
Ã´
Ï•y
0:n(dz0:n) .
(B.8)
Denote by {Fs}n
s=0 the filtration generated by a conditional particle cloud sampled from the kernel PN equa-
tion B.4, i.e. for all â„“âˆˆ[0 : n âˆ’1]
Fs = Ïƒ Î¾1:N
s:n , I1:N
s+1:n
 .
and Fn = Ïƒ Î¾1:N
n
. Define for all bounded f and â„“âˆˆ[0 : n âˆ’1]
Î³N
â„“:n(f) =
( nâˆ’1
Y
s=â„“+1
N âˆ’1â„¦N
s
)
N âˆ’1
N
X
k=1
eÏ‰â„“(Î¾k
â„“+1)f(Î¾k
â„“+1) ,
(B.9)
with the convention Î³N
â„“:n(f) = 1 if â„“â‰¥n. Define also the transition Kernel
Qy
â„“âˆ’1|â„“+1 : Rdx Ã— B(Rdx) âˆ‹(xâ„“+1, A) 7â†’
Z
1A(xâ„“)eÏ‰â„“âˆ’1(xâ„“)py
â„“(dxâ„“|xâ„“+1) .
(B.10)
Using eqs. (2.3) and (2.4), it is easily seen that for all â„“âˆˆ[0 : n âˆ’1],
eÏ‰â„“(xâ„“+1)Qy
â„“âˆ’1|â„“+1(f)(xâ„“+1) =
1
qâ„“+1|0(xâ„“+1|y)
Z
qâ„“|0(xs|y)eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)pâ„“(dxâ„“|xâ„“+1) .
(B.11)
Define 1 : x âˆˆRdx 7â†’1. We may thus write that Î³N
â„“:n(f) = N âˆ’1Î³N
â„“+1:n(1) PN
k=1 eÏ‰â„“(Î¾k
â„“+1)f(Î¾k
â„“+1).
Lemma B.3. For all â„“âˆˆ[0 : n âˆ’1] it holds that
Ez0:n
Î³N
â„“âˆ’1:n(f) = N âˆ’1
N
Ez0:n
Ã®
Î³N
â„“:n
Ã„
Qy
â„“âˆ’1|â„“+1(f)
Ã¤Ã³
+ 1
N Ez0:n
Î³N
â„“:n(1) eÏ‰â„“âˆ’1(zâ„“)f(zâ„“) .
Proof. By the tower property and the fact that Î³N
â„“:n(f) is Fâ„“+1-measurable, we have that
Ez0:n
Î³N
â„“âˆ’1:n(f) = Ez0:n
"
N âˆ’1Î³N
â„“+1:n(1)â„¦N
â„“Ez0:n
"
N âˆ’1
N
X
k=1
eÏ‰â„“âˆ’1(Î¾k
â„“)f(Î¾k
â„“)
Fâ„“+1
##
.
Note that for all â„“âˆˆ[0 : n âˆ’1], (Î¾1
â„“, . . . , Î¾Nâˆ’1
â„“
) are identically distributed conditionally on Fâ„“+1 and
Ez0:n
Ã¯
eÏ‰â„“âˆ’1(Î¾j
â„“)f(Î¾j
â„“)
Fâ„“+1
Ã²
=
1
â„¦N
â„“
N
X
k=1
eÏ‰â„“(Î¾k
â„“+1)
Z
eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)py
â„“(dxâ„“|Î¾k
â„“+1) ,
leading to
Ez0:n
"
N âˆ’1
N
X
k=1
eÏ‰â„“âˆ’1(Î¾k
â„“)f(Î¾k
â„“)
Fâ„“+1
#
= N âˆ’1
Nâ„¦N
â„“
N
X
k=1
eÏ‰â„“(Î¾k
â„“+1)
Z
eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)py
â„“(dxâ„“|Î¾k
â„“+1) + 1
N eÏ‰â„“âˆ’1(zâ„“)f(zâ„“) ,
and the desired recursion follows.
19

Published as a conference paper at ICLR 2024
Proof of Lemma B.2. We proceed by induction and show for all â„“âˆˆ[0 : n âˆ’2]
Ez0:n
Î³N
â„“:n(f)]
=
Ã…N âˆ’1
N
Ã£nâˆ’â„“R
pâ„“+1(dxâ„“+1)qâ„“+1|0(xâ„“+1|y)eÏ‰â„“(xâ„“+1)f(xâ„“+1)
Zn
+ (N âˆ’1)nâˆ’â„“âˆ’1
N nâˆ’â„“
Ã¯
(Zâ„“+1/Zn)f(zâ„“+1)eÏ‰â„“(zâ„“+1)
+
n
X
s=â„“+2
Zs/Zn
qs|0(zs|y)
Z
eÏ‰â„“(xâ„“+1)qâ„“+1|0(xâ„“+1|y)f(xâ„“+1)pâ„“+1|s(dxâ„“+1|zs)
Ã²
+ Dy
â„“:n
N 2 .
(B.12)
where f is a bounded function and Dy
â„“:n is a a positive constant. The desired result in Lemma B.2 then follows by
taking â„“= 0 and f = 1.
Assume that equation B.12 holds at step â„“. To show that it holds at step â„“âˆ’1 we use Lemma B.3 and we compute
Ez0:n
Ã®
Î³N
â„“:n
Ã„
Qy
â„“âˆ’1|â„“+1(f)
Ã¤Ã³
and Ez0:n
Î³N
â„“:n(1) eÏ‰â„“âˆ’1(zâ„“)f(zâ„“).
Using the following identities which follow from equation B.11
Z
qâ„“+1|0(xâ„“+1|y)eÏ‰â„“(xâ„“+1)Qy
â„“âˆ’1|â„“+1(f)(xâ„“+1)pâ„“+1(dxâ„“+1)
=
Z
qâ„“|0(xâ„“|y)eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)pâ„“(dxâ„“) ,
and
Z
eÏ‰â„“(xâ„“+1)qâ„“+1|0(xâ„“+1|y)Qy
â„“âˆ’1|â„“+1(f)(xâ„“+1)pâ„“+1|s(dxâ„“+1|xs)
=
Z
eÏ‰â„“âˆ’1(xâ„“)qâ„“|0(xâ„“|y)f(xâ„“)pâ„“|s(dxâ„“|xs) ,
we get by equation B.12 that
N âˆ’1
N
Ez0:n
Ã®
Î³N
â„“:n
Ã„
Qy
â„“âˆ’1|â„“+1(f)
Ã¤Ã³
=
Ã…N âˆ’1
N
Ã£nâˆ’â„“+1 R
qâ„“|0(xâ„“|y)eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)pâ„“(dxâ„“)
Zn
+ (N âˆ’1)nâˆ’â„“
N nâˆ’â„“+1
Ã¯
Zâ„“+1/Zn
qâ„“+1|0(zâ„“+1|y)
Z
qâ„“|0(xâ„“|y)eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)pâ„“(dxâ„“|zâ„“+1)
+
n
X
s=â„“+2
Zs/Zn
qs|0(zs|y)
Z
eÏ‰â„“âˆ’1(xâ„“)qâ„“|0(xâ„“|y)f(xâ„“)pâ„“|s(dxâ„“|zs)
Ã²
+ Dy
â„“:n
N 2
=
Ã…N âˆ’1
N
Ã£nâˆ’â„“+1 R
qâ„“|0(xâ„“|y)eÏ‰â„“âˆ’1(xâ„“)f(xâ„“)pâ„“(dxâ„“)
Zn
+ (N âˆ’1)nâˆ’â„“
N nâˆ’â„“+1
n
X
s=â„“+1
Zs/Zn
qs|0(zs|y)
Z
eÏ‰â„“âˆ’1(xâ„“)qâ„“|0(xs|y)f(xâ„“)pâ„“|s(dxâ„“|zs) + Dy
â„“:n
N 2 .
(B.13)
20

Published as a conference paper at ICLR 2024
The induction step is finished by using again equation B.12 and noting that
1
N Ez0:n
Î³N
â„“:n(1) eÏ‰â„“âˆ’1(zâ„“)f(zâ„“) = (N âˆ’1)nâˆ’â„“
N nâˆ’â„“+1
 Zâ„“/Zn
eÏ‰â„“âˆ’1(zâ„“)f(zâ„“) +
eDy
â„“:n
N 2 .
and then setting Dy
â„“âˆ’1:n = Dy
â„“:n + eDy
â„“:n.
It remains to compute the initial value at â„“= n âˆ’2. Note that
Ez0:n
Î³N
nâˆ’1:n(f) = N âˆ’1
N
Z
py
n(dxn)eÏ‰nâˆ’1(xn)f(xn) + 1
N eÏ‰nâˆ’1(zn)f(zn)
(B.14)
and thus by Lemma B.3 and similarly to the previous computations
Ez0:n
Î³N
nâˆ’2:n(f)
=
Ã…N âˆ’1
N
Ã£2 Z
py
n(dxn)eÏ‰nâˆ’1(xn)Qy
nâˆ’2|n(f)(xn) + N âˆ’1
N 2
Ã¯
eÏ‰nâˆ’1(zn)Qy
nâˆ’2|n(f)(zn)
+ eÏ‰nâˆ’2(znâˆ’1)f(znâˆ’1)
Z
py
n(dxn)eÏ‰nâˆ’1|n(xn)
Ã²
+
Dy
nâˆ’2fa:n
N 2
=
Ã…N âˆ’1
N
Ã£2 R
qnâˆ’1|0(xnâˆ’1|y)eÏ‰nâˆ’2(xnâˆ’1)pnâˆ’1(dxnâˆ’1)
Zn
+ N âˆ’1
N 2
Ã¯ Znâˆ’1/Zn
eÏ‰nâˆ’2(znâˆ’1)f(znâˆ’1)
+
1
qn|0(xn|y)
Z
qnâˆ’1|0(xnâˆ’1|y)eÏ‰nâˆ’2(xnâˆ’1)f(xnâˆ’1)pnâˆ’1(dxnâˆ’1|zn)
Ã²
+ Dy
nâˆ’2:n
N 2
.
B.2
PROOF OF PROPOSITION 2.3 AND LEMMA B.4
In this section and only in this section we make the following assumption
(A2) For all s âˆˆ[0 : n âˆ’1], ps(xs)qs+1(xs+1|xs) = ps+1(xs+1)Î»s(xs|xs+1) .
We also consider ÏƒÎ´ = 0. In what follows we let Ï„dy+1 = n and we write Ï„1:dy = {Ï„1, . . . , Ï„dy} and Ï„1:dy = [1 :
n] \ Ï„1:t. Define the measure
Î“y
0:n(dx0:n) = pn(dxn)
Y
sâˆˆÏ„1:dy
Î»s(dxs|xs+1)
dy
Y
i=1
Î»Ï„i(xÏ„i|xÏ„i+1)dx\i
Ï„iÎ´y[i](dxÏ„i[i]) .
(B.15)
Under (A2) it has the following alternative forward expression,
Î“y
0:n(dx0:n) = p0(dx0)
Y
sâˆˆÏ„1:dy
qs+1(dxs+1|xs)
dy
Y
i=1
qÏ„i(xÏ„i|xÏ„iâˆ’1)dx\i
Ï„iÎ´y[i](dxÏ„i[i]) .
(B.16)
Since the forward kernels decompose over the dimensions of the states, i.e.
qs+1(xs+1|xs) =
dx
Y
â„“=1
qâ„“
s+1(xs+1[â„“]|xs[â„“])
21

Published as a conference paper at ICLR 2024
where qâ„“
s+1(xs+1[â„“]|xs[â„“]) = N(xs+1[â„“]; (Î±s+1/Î±s)1/2xs[â„“], 1 âˆ’(Î±s+1/Î±s)), we can write
Î“y
0:n(x0:n) = p0(x0)
dx
Y
â„“=1
Î“y
1:n|0,â„“
 x1[â„“], . . . , xn[â„“]
x0[â„“] ,
(B.17)
where for â„“âˆˆ[1 : dy]
Î“y
1:n|0,â„“
 x1[â„“], . . . , xn[â„“]|x0[â„“] = qâ„“
Ï„â„“(y[â„“]|xÏ„â„“âˆ’1[â„“])
Y
sÌ¸=Ï„â„“
qâ„“
s(dxs[â„“]|xsâˆ’1[â„“]) ,
(B.18)
and for â„“âˆˆ[dy + 1 : dx],
Î“y
1:n|0,â„“(x1[â„“], . . . , xn[â„“]|x0[â„“]) =
nâˆ’1
Y
s=0
qâ„“
s+1(xs+1[â„“]|xs[â„“]) .
(B.19)
With these quantities in hand we can now prove Proposition 2.3.
Proof of Proposition 2.3. Note that for â„“âˆˆ[1 : dy],
N(y[â„“]; Î±Ï„â„“x0[â„“], 1 âˆ’Î±Ï„â„“) = qâ„“
Ï„â„“|0(y[â„“]|x0[â„“]) =
Z
qâ„“
Ï„â„“(y[â„“]|xÏ„â„“âˆ’1[â„“])
Y
sÌ¸=Ï„â„“
qâ„“
s(dxs[â„“]|xsâˆ’1[â„“])
=
Z
Î“y
1:n|0,â„“
 d(x1[â„“], . . . , xn[â„“])|x0[â„“]
and thus by equation ?? we have that
p0(x0)gy
0(x0) âˆp0(x0)
dy
Y
â„“=1
N(y[â„“]; Î±Ï„â„“x0[â„“], 1 âˆ’Î±Ï„â„“)
= p0(x0)
dy
Y
â„“=1
Z
Î“y
1:n|0,â„“
 d(x1[â„“], . . . , xn[â„“])|x0[â„“]
= p0(x0)
dx
Y
â„“=1
Z
Î“y
1:n|0,â„“
 d(x1[â„“], . . . , xn[â„“])|x0[â„“] .
By equation B.16 it follows that
Ï•y
0(x0) =
1
R
Î“y
0:n(Ëœx0:n)dËœx0:n
Z
Î“y
0:n(x0:n)dx1:n ,
and hence by equation B.16 and equation B.15 we get
Ï•y
0(x0) âˆ
Z
pÏ„dy (xÏ„dy )Î´y[dy](dxÏ„dy [dy])dx\dy
Ï„dy
ï£±
ï£²
ï£³
dyâˆ’1
Y
i=1
Î»Ï„i|Ï„i+1(xÏ„i|xÏ„i+1)Î´y[i](dxÏ„i[i])dx\i
Ï„i
ï£¼
ï£½
ï£¾Î»0|Ï„1(x0|xÏ„1) .
This completes the proof.
Let Î³y
0,s denote the joint time 0 and s marginal of the measure equation B.15, i.e.
Î³y
0,s(x0, xs) =
Z
Î“y
0:n(x0:n)dx1:sâˆ’1dxs+1:n
(B.20)
22

Published as a conference paper at ICLR 2024
We now prove the following result.
Lemma B.4. Assume (A2) and let Ï„0 := 0, Ï„dy+1 := n. For all k âˆˆ[1 : dy],
(i) If s âˆˆ[Ï„k + 1 : Ï„k+1],
Î³y
0,s(x0, xs) =
Z
Î³y
0,s+1(x0, xs+1)qÏƒ
s|s+1,0(xs|xs+1, x0)gy
s (xs)
dy
Y
â„“=k+1
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])dxs+1 .
(ii) If s = Ï„k,
Î³y
0,s(x0, xs) =
Z
Î³y
0,s+1(x0, xs+1)qÏƒ
s|s+1,0(xs|xs+1, x0)
Ã—
kâˆ’1
Y
i=1
gy
s,i(xs[i])
dy
Y
â„“=k+1
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])dxs+1 .
Proof of Lemma B.4. Let k âˆˆ[1 : dy] and assume that s âˆˆ[Ï„k + 1 : Ï„k+1 âˆ’2]. By (A2), equation B.16,
equation B.18 and equation B.19 we have that
Î³y
0,s(x0, xs) = p0(x0)qs|0(xs|x0)
k
Y
i=1
qi
Ï„i|0(y[i]|x0[i])qi
s|Ï„i(xs[i]|y[i])
Ã—
dy
Y
â„“=k+1
qâ„“
s|0(xs[â„“]|x0[â„“])qâ„“
Ï„â„“|s(y[â„“]|xs[â„“]) ,
and thus, using the following identity valid for â„“âˆˆ[k + 1 : dy]
qâ„“
s|0(xs[â„“]|x0[â„“])qâ„“
Ï„â„“|s(y[â„“]|xs[â„“])
= qâ„“
s|0(xs[â„“]|x0[â„“])
Z
qâ„“
Ï„â„“|s+1(y[â„“]|xs+1[â„“])qâ„“
s+1(xs+1[â„“]|xs[â„“])dxs+1[â„“]
=
Z
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])qâ„“
Ï„â„“|s+1(y[â„“]|xs+1[â„“])qâ„“
s+1|0(xs+1[â„“]|x0[â„“])dxs+1[â„“] ,
23

Published as a conference paper at ICLR 2024
and that qs|0(xs|x0)qs+1(xs+1|xs) = qÏƒ
s|s+1,0(xs|xs+1, x0)qs+1|0(xs+1|x0) we get that
Î³y
0,s(x0, xs)
=
Z
p0(x0)qs|0(xs|x0)qs+1(dxs+1|xs)
Ã—
k
Y
i=1
qi
Ï„i|0(y[i]|x0[i])qi
s|Ï„i(xs[i]|y[i])qi
s+1|Ï„i(dxs+1[i]|y[i])
Ã—
dy
Y
â„“=k+1
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])qâ„“
Ï„â„“|s+1(y[â„“]|xs+1[â„“])qâ„“
s+1|0(xs+1[â„“]|x0[â„“])dxs+1[â„“]
=
Z
Î³y
0,s+1(x0, xs+1)qÏƒ
s|s+1,0(xs|xs+1, x0)gy
s (xs)
dy
Y
â„“=k+1
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])dxs+1 .
If s = Ï„k+1 then
Î³y
0,s(x0, xs) = p0(x0)qs|0(xs|x0)
k
Y
i=1
qi
Ï„i|0(y[i]|x0[i])qi
s|Ï„i(xs[i]|y[i])
Ã— qk+1
Ï„k+1|0(y[k + 1]|x0[k + 1])
dy
Y
â„“=k+2
qâ„“
s|0(xs[â„“]|x0[â„“])qâ„“
Ï„â„“|s(y[â„“]|xs[â„“]) ,
(B.21)
and similarly to the previous case we get
Î³0,s(x0, xs)
=
Z
Î³y
0,s+1(x0, xs+1)qÏƒ
s|s+1,0(xs|xs+1, x0)gy
s (xs)
dy
Y
â„“=k+2
qÏƒ,â„“
s|s+1,0(xs[â„“]|xs+1[â„“], x0[â„“])dxs+1 .
Finally, if s = Ï„k+1 âˆ’1, then
Î³y
0,s(x0, xs) = p0(x0)qs|0(xs|x0)
k
Y
i=1
qi
Ï„i|0(y[i]|x0[i])qi
s|Ï„i(xs[i]|y[i])
Ã— qk+1
s|0 (xs[k + 1]|x0[k + 1])qk+1
Ï„k+1|s(y[k + 1]|xs[k + 1])
dy
Y
â„“=k+2
qâ„“
s|0(xs[â„“]|x0[â„“])qâ„“
Ï„â„“|s(y[â„“]|xs[â„“]) ,
and using
qk+1
s|0 (xs[k + 1]|x0[k + 1])qk+1
Ï„k+1|s(y[k + 1]|xs[k + 1])
= qÏƒ,k+1
s|Ï„k+1,0(xs[k + 1]|xÏ„k+1[k + 1], x0[k + 1])qk+1
Ï„k+1|0(y[k + 1]|x0[k + 1])
24

Published as a conference paper at ICLR 2024
we find that
Î³0,s(x0, xs)
=
Z
Î³y
0,Ï„k+1(x0, xÏ„k+1)qÏƒ
s|Ï„k+1,0(xs|xÏ„k+1, x0)gy
s (xs)
dy
Y
â„“=k+1
qÏƒ,â„“
s|s+1,0(xs[â„“]|xÏ„k+1[â„“], x0[â„“])dxÏ„k+1 .
B.3
ALGORITHMIC DETAILS AND NUMERICS
B.3.1
GMM
For a given dimension dx, we consider qdata a mixture of 25 Gaussian random variables. The Gaussian random
variables have mean Âµi,j := (8i, 8j, Â· Â· Â· , 8i, 8j) âˆˆRdx for (i, j) âˆˆ{âˆ’2, âˆ’1, 0, 1, 2}2 and unit variance. The
mixture (unnormalized) weights Ï‰i,j are independently drawn according to a Ï‡2 distribution. The Îº paramater of
MCGdiff is Îº2 = 10âˆ’4. We use 20 steps of DDIM for the numerical examples and for all algorithms.
Score:
Note that qs(xs) =
R
qs|0(xs|x0)qdata(x0)dx0. As qdata is a mixture of Gaussians, qs(xs) is also a
mixture of Gaussians with means Î±1/2
s
Âµi,j and unitary variances. Therefore, using automatic differentiation
libraries, we can calculate âˆ‡log qs(xs). Setting e(xs, s) = âˆ’(1 âˆ’Î±s)1/2âˆ‡log qs(xs) leads to the optimum of
equation 1.4.
Forward process scaling:
We chose the sequence of {Î²s}1000
s=1 as a linearly decreasing sequence between
Î²1 = 0.2 and Î²1000 = 10âˆ’4.
Measurement model:
For a pair of dimensions (dx, dy) the measurement model (y, A, Ïƒy) is drawn as fol-
lows:
â€¢ A: We first draw ËœA âˆ¼N(0dyÃ—dx, IdyÃ—dx) and compute the SVD decomposition of ËœA = USVT .
Then, we sample for (i, j) âˆˆ{âˆ’2, âˆ’1, 0, 1, 2}2, si,j according to a uniform in [0, 1]. Finally, we set
A = U Diag({si,j}(i,j)âˆˆ{âˆ’2,âˆ’1,0,1,2}2)VT .
â€¢ Ïƒy: We draw Ïƒy uniformly in the interval [0, max(s1, Â· Â· Â· , sdy)].
â€¢ y: We then draw xâˆ—âˆ¼qdata and set y := Axâˆ—+ ÏƒyÏµ where Ïµ âˆ¼N(0dy, Idy).
Posterior:
Once we have drawn both qdata and (y, A, Ïƒy), the posterior can be exactly calculated using Bayes
formula and gives a mixture of Gaussians with mixture components ci,j and associated weights ËœÏ‰i,j
ci,j := N(Î£  AT y/Ïƒ2
y + Âµi,j
 , Î£) ,
ËœÏ‰i := Ï‰iN(y; AÂµi,j, Ïƒ2 Idx +AAT ) ,
where Î£ :=  Idx +Ïƒâˆ’2
y AT Aâˆ’1.
Variational Inference:
The RNVP entries in the numerical examination are obtained by Variational Inference
using the RNVP architecture for the normalizing flow from Dinh et al. (2017). Given a normalizing flow fÏ•
with Ï• âˆˆRj, j âˆˆNâˆ—, the training procedure consists of optimizing the ELBO, i.e., solving the optimization
problem
Ï•âˆ—= arg max
Ï•âˆˆRj
Nnf
X
k=1
log |JfÏ•(Ïµi)| + log Ï€âˆ—(fÏ•(Ïµi)) ,
(B.22)
where Nnf âˆˆNâˆ—is the minibatch-size, JfÏ• the Jacobian of fÏ• w.r.t Ï•, and Ïµ1:Nnf âˆ¼N(0, I)âŠ—Nnf . All the
experiments were performed using a 10 layers RNVP. Equation (B.22) is solved using Adam algorithm Kingma &
25

Published as a conference paper at ICLR 2024
dx = 1
dx = 2
dx = 4
KL
0
50
100
150
200
101
102
103
104
0
50
100
150
200
101
102
103
104
0
50
100
150
200
101
102
103
104
dy = 8
KL
0
50
100
150
200
102
103
104
0
50
100
150
200
102
103
104
0
50
100
150
200
102
103
104
dy = 80
KL
0
50
100
150
200
103
104
0
50
100
150
200
103
104
0
50
100
150
200
103
104
dy = 800
Iteration
Figure 4: Evolution of KL with the number of iterations for all pairs of (dx, dy) tested in the GMM case.
Ba (2015) with a learning rate of 10âˆ’3 and 200 iterations with Nnf = 10. The losses for each pair (dx, dy) is
shown in fig. 4, where one can see that the majority of the losses have converged.
Choosing DDIM timesteps for a given measurement model:
Given a number of DDIM samples R, we choose
the timesteps 1 = t1 < Â· Â· Â· < tR = 1000 âˆˆ[1 : 1000] as to try to satisfy the two following constraints:
â€¢ For all i âˆˆ[1 : dy] there exists a tj such that ÏƒyÎ±1/2
tj
â‰ˆ(1 âˆ’Î±tj)1/2si,
â€¢ For all i âˆˆ[1 : R âˆ’1], Î±1/2
ti
âˆ’Î±1/2
ti+1 â‰ˆÎ´ for some Î´ > 0.
26

Published as a conference paper at ICLR 2024
MCGdiff
DDRM
DPS
RNVP
x2
x2
x2
x1
Figure 5: First two dimensions for the GMM case with dx = 8. The rows represent dy = 1, 2, 4 respectively. The
blue dots represent samples from the exact posterior, while the red dots correspond to samples generated by each
of the algorithms used (the names of the algorithms are given at the top of each column).
The
first
constraint
comes
naturally
from
the
definition
of
Ï„i.
Since
the
poten-
tials
have
mean
Î±1/2
ti y,
the
second
condition
constrains
the
intermediate
laws
remain
â€œcloseâ€.
An
algorithm
that
approximately
satisfies
both
constraints
is
given
below.
Algorithm 2: Timesteps choice
Input: Number of DDIM steps R, Ïƒy, {si}dy
i=1, {Î±i}1000
i=1
Output: {tj}R
j=1
Set SÏ„ = {}.
for j â†[1 : dy] do
Set ËœÏ„j = arg minâ„“âˆˆ[1:1000] |ÏƒyÎ±1/2
â„“
âˆ’(1 âˆ’Î±â„“)1/2)sj|.
Add ËœÏ„j to SÏ„ if ËœÏ„j /âˆˆSÏ„.
Set nm = R âˆ’#SÏ„ âˆ’1 and Î´ = (Î±1/2
1
âˆ’Î±1/2
1000)/nm.
Set t1 = 1, e = 1 and ie = 1. for â„“â†[2 : 1000] do
if Î±1/2
e
âˆ’Î±1/2
â„“
> Î´ or â„“âˆˆSÏ„ then
Set e = â„“, ie = ie + 1 and Ï„ie = â„“.
Set Ï„R = 1000.
Additional numerics:
We now proceed to illustrate in Figures 5 to 7 the first 2 components for one of the
measurement models for all the different combinations of (dx, dy) combinations used in table 1. We also show in
fig. 8 the evolution of each observed coordinate in the noise case with dy = 4. We can see that it follows closely
the forward path of the diffused observations indicated by the blue line.
27

Published as a conference paper at ICLR 2024
MCGdiff
DDRM
DPS
RNVP
x2
x2
x2
x1
Figure 6: First two dimensions for the GMM case with dx = 80. The rows represent dy = 1, 2, 4 respectively.
The blue dots represent samples from the exact posterior, while the red dots correspond to samples generated by
each of the algorithms used (the names of the algorithms are given at the top of each column).
MCGdiff
DDRM
DPS
RNVP
x2
x2
x2
x1
Figure 7: First two dimensions for the GMM case with dx = 800. The rows represent dy = 1, 2, 4 respectively.
The blue dots represent samples from the exact posterior, while the red dots correspond to samples generated by
each of the algorithms used (the names of the algorithms are given at the top of each column).
28

Published as a conference paper at ICLR 2024
d
dy
MCGdiff
DDRM
DPS
RNVP
8
1
1.43 Â± 0.55
5.88 Â± 1.16
4.86 Â± 1.01
9.43 Â± 0.99
8
2
0.49 Â± 0.24
5.20 Â± 1.32
5.79 Â± 1.96
8.93 Â± 1.29
8
4
0.38 Â± 0.25
2.51 Â± 1.29
3.48 Â± 1.52
6.71 Â± 1.54
80
1
1.39 Â± 0.45
5.64 Â± 1.10
4.98 Â± 1.14
6.86 Â± 0.88
80
2
0.67 Â± 0.24
7.07 Â± 1.35
5.10 Â± 1.23
7.79 Â± 1.50
80
4
0.28 Â± 0.14
7.81 Â± 1.48
4.28 Â± 1.26
7.95 Â± 1.61
800
1
2.40 Â± 1.00
7.44 Â± 1.15
6.49 Â± 1.16
7.74 Â± 1.34
800
2
1.31 Â± 0.60
8.95 Â± 1.12
6.88 Â± 1.01
8.75 Â± 1.02
800
4
0.47 Â± 0.19
8.39 Â± 1.48
5.51 Â± 1.18
7.81 Â± 1.63
Table 2: Extended GMM sliced wasserstein table.
xs[1]
100
101
102
103
20
10
0
10
20
100
101
102
103
20
10
0
10
20
xs[2]
xs[3]
100
101
102
103
20
10
0
10
20
100
101
102
103
20
10
0
10
20
xs[4]
s
s
Figure 8: Illustration of the particle cloud of the 4 first observed coordinate in the case (dy, dx) = (4, 800) with
100 DDIM steps. The red points represent the particle cloud, while the purple points at the origin represent the
posterior distribution. The blue curve corresponds to the curve s â†’Î±1/2
s
y[â„“] and the blue dot on the curve to
Î±1/2
Ï„â„“y[â„“].
Table 2 is an extended version of table 1.
29

Published as a conference paper at ICLR 2024
dy = 2
dy = 6
dy = 10
x2
x1
Figure 9: Purple points are samples from the prior and yellow samples from the diffusion with 25 DDIM steps.
d
SW
2
0.79 Â± 0.15
6
0.87 Â± 0.07
10
0.96 Â± 0.06
Table 3: Sliced Wasserstein between learned diffusion and target prior.
B.3.2
FMM
A funnel distribution is defined by the following density
N(x1; 0, 1)
d
Y
i=1
N(xi; 0, exp(x1/2)) .
To generate a Funnel mixture model of 20 components in dimension d, we start by firstly sampling (Âµi, Ri)20
i=1
uniformly in ([âˆ’20, 20]d Ã— SO(Rd))Ã—20. The mixture will consist of 20 Funnel random variables translated by
Âµi and rotated by Ri, with unnormalized weights Ï‰i,j that are independently drawn uniformly in [0, 1].
Score
The denoising diffusion network e(Î¸) in dimension d is defined as a 5 layers Resnet network where each
Resnet block consists of the chaining of three blocks where each block has the following layers:
â€¢ Linear (512, 1024),
â€¢ 1d Batch Norm,
â€¢ ReLU activation.
The Resnet is preceeded by an input embedding from dimension d to 512 and in the end an output embedding
layer projects the output of the resnet from 512 to d. The time t is embedded using positional embedding into
dimension 512 and is added to the input at each Resnet block. The network is trained using the same loss as in Ho
et al. (2020) for 104 iterations using a batch size of 512 samples. A learning rate of 10âˆ’3 is used for the Adam
optimizer Kingma & Ba (2015). Figure 9 illustrate the outcome of the learned diffusion generative model and the
target prior. In table 3 we show the CLT 95% intervals for the SW between the learned diffusion generative model
and the target prior.
Forward process scaling
We chose the sequence of {Î²s}1000
s=1 as a linearly decreasing sequence between
Î²1 = 0.2 and Î²1000 = 10âˆ’4.
30

Published as a conference paper at ICLR 2024
dx = 1
dx = 3
dx = 5
KL
0
50
100
150
200
101
102
103
104
0
50
100
150
200
101
102
103
104
0
50
100
150
200
101
102
103
104
dy = 6
KL
0
50
100
150
200
101
102
103
104
0
50
100
150
200
102
103
104
0
50
100
150
200
102
103
104
dy = 10
Iteration
Figure 10: Evolution of KL with the number of iterations for all pairs of (dx, dy) tested in the FMM case.
Measurement model
The measurement model was generated in the same way as for the GMM case.
Posterior
The posterior samples were generated by running the No U-turn sampler (Hoffman & Gelman (2011))
with a chain of length 104 and taking the last sample of the chain. This was done in parallel to generate 104
samples. The mass matrix and learning rate were set by first running Stanâ€™s warmup and taking the last values of
the warmup phase.
Variational inference:
Variational inference in FMM shares the same details as the GMM case. The analogous
of fig. 4 is displayed at fig. 10.
Additional plots:
We now proceed to illustrate in Figures 11 to 13 the first 2 components for one of the
measurement models for all the different combinations of (dx, dy) combinations used in table 1.
31

Published as a conference paper at ICLR 2024
MCGdiff
DDRM
DPS
RNVP
PCA2
PCA2
PCA2
PCA1
Figure 11: First two dimensions for the FMM case with dx = 10. The rows represent dy = 1, 3, 5 respectively.
The blue dots represent samples from the exact posterior, while the red dots correspond to samples generated by
each of the algorithms used (the names of the algorithms are given at the top of each column).
MCGdiff
DDRM
DPS
RNVP
PCA2
PCA2
PCA2
PCA1
Figure 12: First two dimensions for the FMM case with dx = 6. The rows represent dy = 1, 3, 5 respectively.
The blue dots represent samples from the exact posterior, while the red dots correspond to samples generated by
each of the algorithms used (the names of the algorithms are given at the top of each column).
32

Published as a conference paper at ICLR 2024
MCGdiff
DDRM
DPS
RNVP
PCA2
PCA1
Figure 13: First two dimensions for the FMM case with dx = 2 and dy = 1. The blue dots represent samples
from the exact posterior, while the red dots correspond to samples generated by each of the algorithms used (the
names of the algorithms are given at the top of each column).
33

Published as a conference paper at ICLR 2024
B.3.3
IMAGE DATASETS
We now present samples from MCGdiff in different image dataset and different kinds of inverse problems.
Super Resolution
We start by super resolution. We set Ïƒy = 0.05 for all the datasets and Î¶coeff = 0.1 for DPS .
We use 100 steps of DDIM with Î· = 1. The results are shown in Figure 14. We use a downsampling ratio of 4 for
the CIFAR-10 dataset, 8 for both Flowers and Cats datasets and 16 for the others. The dimension of the datasets
are recalled in table 4. We display in fig. 14 samples from MCGdiff, DPSand DDRMover several different image
datasets (table 4). For each algorithm, we generate 1000 samples and we show the pair of samples that are the
furthest apart in L2 norm from each other in the pool of samples. For MCGdiff we ran several parallel particle
filters with N = 64 to generate 1000 samples.
CIFAR-10
Flowers
Cats
Bedroom
Church
CelebaHQ
(W, H, C)
(32, 32, 3)
(64, 64, 3)
(128, 128, 3)
(256, 256, 3)
(256, 256, 3)
(256, 256, 3)
Table 4: The datasets used for the inverse problems over image datasets.
Gaussian 2D debluring
We consider a Gaussian 2D square kernel with sizes (w/6, h/6) and standard deviation
w/30 where (w, h) are the width and height of the image. We set Ïƒy = 0.1 for all the datasets and Î¶coeff = 0.1 for
DPS . We use 100 steps of DDIM with Î· = 1. We display in fig. 15 samples from MCGdiff, DPSand DDRMover
several different image datasets (table 4). For each algorithm, we generate 1000 samples and we show the pair of
samples that are the furthest appart in L2 norm from each other in the pool of samples. For MCGdiff we ran
several parallel particle filters with N = 64 to generate 1000 samples.
Inpainting on CelebA
We consider the inpainting problem on the CelebA dataset with several different masks
in fig. 16. We show in fig. 17 the evolution of the particle cloud with s.
1000
900
800
700
600
500
400
300
200
100
50
4
Figure 17: Evolution of the particle cloud for one of the masks. The numbers on top and bottom indicate the step
s of the approximation.
34

Published as a conference paper at ICLR 2024
CIFAR-10
Flowers
Cats
Bedroom
Church
CelebaHQ
sample
observation
DPS
DPS
DDRM
DDRM
MCGdiff
MCGdiff
Figure 14: Ratio 4 for CIFAR, 8 for flowers and Cats and 16 for CELEB
35

Published as a conference paper at ICLR 2024
CIFAR 10
Flowers
Cats
Bedroom
Church
CelebaHQ
sample
observation
DPS
DPS
DDRM
DDRM
MCGdiff
MCGdiff
Figure 15
36

Published as a conference paper at ICLR 2024
Original
MCGdiff
MCGdiff
MCGdiff
MCGdiff
DPS
DDRM
SMCdiff
Figure 16: Inpainting with different masks on the CelebA test set.
37

