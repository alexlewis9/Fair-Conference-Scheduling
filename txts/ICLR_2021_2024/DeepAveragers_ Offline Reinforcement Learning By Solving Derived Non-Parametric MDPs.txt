Published as a conference paper at ICLR 2021
DEEPAVERAGERS: OFFLINE REINFORCEMENT LEARN-
ING BY SOLVING DERIVED NON-PARAMETRIC MDPS
Aayam Shrestha, Stefan Lee, Prasad Tadepalli, Alan Fern
Oregon State University
Corvallis, OR 97330, USA
{shrestaa, leestef, tadepall, alan.fern}@oregonstate.edu
ABSTRACT
We study an approach to ofÔ¨Çine reinforcement learning (RL) based on optimally
solving Ô¨Ånitely-represented MDPs derived from a static dataset of experience.
This approach can be applied on top of any learned representation and has the
potential to easily support multiple solution objectives as well as zero-shot adjust-
ment to changing environments and goals. Our main contribution is to introduce
the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions
for ofÔ¨Çine RL. DAC-MDPs are a non-parametric model that can leverage deep
representations and account for limited data by introducing costs for exploiting
under-represented parts of the model. In theory, we show conditions that allow
for lower-bounding the performance of DAC-MDP solutions. We also investigate
the empirical behavior in a number of environments, including those with image-
based observations. Overall, the experiments demonstrate that the framework can
work in practice and scale to large complex ofÔ¨Çine RL problems.
1
INTRODUCTION
Research in automated planning and control has produced powerful algorithms to solve for optimal,
or near-optimal, decisions given accurate environment models. Examples include the classic value-
and policy-iteration algorithms for tabular representations or more sophisticated symbolic variants
for graphical model representations (e.g. Boutilier et al. (2000); Raghavan et al. (2012)). In concept,
these planners address many of the traditional challenges in reinforcement learning (RL). They can
perform ‚Äúzero-shot transfer‚Äù to new goals and changes to the environment model, accurately account
for sparse reward or low-probability events, and solve for different optimization objectives (e.g.
robustness). Effectively leveraging these planners, however, requires an accurate model grounded
in observations and expressed in the planner‚Äôs representation. On the other hand, model-based
reinforcement learning (MBRL) aims to learn grounded models to improve RL‚Äôs data efÔ¨Åciency.
Despite developing grounded environment models, the vast majority of current MBRL approaches
do not leverage near-optimal planners to help address the above challenges. Rather, the models are
used as black-box simulators for experience augmentation and/or Monte-Carlo search. Alternatively,
model learning is sometimes treated as purely an auxiliary task to support representation learning.
The high-level goal of this paper is to move toward MBRL approaches that can effectively leverage
near-optimal planners for improved data efÔ¨Åciency and Ô¨Çexibility in complex environments. How-
ever, there are at least two signiÔ¨Åcant challenges. First, there is a mismatch between the deep model
representations typically learned in MBRL (e.g. continuous state mappings) and the representations
assumed by many planners (e.g. discrete tables or graphical models). Second, near-optimal planners
are well-known for exploiting model inaccuracies in ways that hurt performance in the real environ-
ment, e.g. (Atkeson, 1998). This second challenge is particularly signiÔ¨Åcant for ofÔ¨Çine RL, where
the training experience for model learning is Ô¨Åxed and limited.
We address the Ô¨Årst challenge above by focusing on tabular representations, which are perhaps the
simplest, but most universal representation for optimal planning. Our main contribution is an ofÔ¨Çine
MBRL approach based on optimally solving a new model called the Deep Averagers with Costs
MDP (DAC-MDP). A DAC-MDP is a non-parametric model derived from an experience dataset
and a corresponding (possibly learned) latent state representation. While the DAC-MDP is deÔ¨Åned
over the entire continuous latent state space, its full optimal policy can be computed by solving
a standard (Ô¨Ånite) tabular MDP derived from the dataset. This supports optimal planning via any
1

Published as a conference paper at ICLR 2021
Derived Finite 
DAC-MDP Core
Experiential
Dataset
Solved DAC-MDP
via Value Iteration
DAC-MDP Over 
Continuous State Space
ùíîùüè
ùíîùüê
ùíîùüë
ùíîùüí
Compile 
DAC-MDP
Value
Iteration
ùíîùüè
ùíîùüê
ùíîùüë
ùíîùüí
Unseen States Online
1-step kNN 
look-ahead 
ùíîùüè
ùíîùüê
ùíîùüë
ùíîùüí
Offline Reinforcement Learning Setting
(ùë†1, ùëé1,ùë†2, ùëü)
(ùë†1, ùëé2,ùë†2, ùëü)
‚ãÆ
(ùë†2, ùëé1,ùë†4, ùëü)
Figure 1: Overview of OfÔ¨Çine RL via DAC-MDPs. Given a static experience dataset, we Ô¨Årst compile it into
a Ô¨Ånite tabular MDP which is at most the size of the dataset. This MDP contains the ‚Äúcore‚Äù states of the full
continuous DAC-MDP. The Ô¨Ånite core-state MDP is then solved via value iteration, resulting in a policy and
Q-value function for the core states. This Ô¨Ånite Q-function is used to deÔ¨Åne a non-parametric Q-function for
the continuous DAC-MDP, which allows for Q-values and hence a policy to be computed for previously unseen
states.
tabular MDP solver, e.g. value iteration. To scale this approach to typical ofÔ¨Çine RL problems,
we develop a simple GPU implementation of value iteration that scales to millions of states. As an
additional engineering contribution, this implementation will be made public.
To address the second challenge of model inaccuracy due to limited data, DAC-MDPs follow the
pessimism in the face of uncertainty principle, which has been shown effective in a number of prior
contexts (e.g. (Fonteneau et al., 2013)). In particular, DAC-MDPs extend Gordon‚Äôs Averagers
framework (Gordon, 1995) with additional costs for exploiting transitions that are under-represented
in the data. Our second contribution is to give a theoretical analysis of this model, which provides
conditions under which a DAC-MDP solution will perform near optimally in the real environment.
Our Ô¨Ånal contribution is to empirically investigate the DAC-MDP approach using simple latent rep-
resentations derived from random projections and those learned by Q-iteration algorithms. Among
other results, we demonstrate the ability to scale to Atari-scale problems, which is the Ô¨Årst demon-
stration of optimal planning being effectively applied across multiple Atari games. In addition, we
provide case studies in 3D Ô¨Årst-person navigation that demonstrate the Ô¨Çexibility and adaptability
afforded by integrating optimal planning into ofÔ¨Çine MBRL. These results show the promise of our
approach for marrying advances in representation learning with optimal planning.
2
FORMAL PRELIMINARIES
A Markov Decision Process (MDP) is a tuple ‚ü®S, A, T, R‚ü©(Puterman, 1994), with state set S, ac-
tion set A, transition function T(s, a, s‚Ä≤), and reward function R(s, a). A policy œÄ maps states
to actions and has Q-function QœÄ(s, a) giving the expected inÔ¨Ånite-horizon Œ≤-discounted reward
of following œÄ after taking action a in s.
The optimal policy œÄ‚àómaximizes the Q-function
over all policies and state-action pairs. Q‚àócorresponds to the optimal Q-function that satisÔ¨Åes
œÄ‚àó(s) = arg maxa Q‚àó(s, a). Q‚àócan be computed given the MDP by repeated application of the
Bellman Backup Operator B, which for any Q-function Q, returns a new Q-function given by,
B[Q](s, a) = R(s, a) + Œ≥Es‚Ä≤‚àºT (s,a,¬∑)
h
max
a
Q(s‚Ä≤, a)
i
.
(1)
The objective of RL is to Ô¨Ånd a near-optimal policy without prior knowledge of the MDP. In the
online RL setting, this is done by actively exploring actions in the environment. Rather, in the
ofÔ¨Çine RL (Levine et al., 2020), which is the focus of this paper, learning is based on a static dataset
D = {(si, ai, ri, s‚Ä≤
i)}, where each tuple gives the reward ri and next state s‚Ä≤
i observed after taking
action ai in state si.
In strict ofÔ¨Çine RL setting, the Ô¨Ånal policy selection must be done using only the dataset, without
direct access to the environment. This includes all hyperparameter tuning and the choice of when to
stop learning. Evaluations of ofÔ¨Çine RL, however, often blur this distinction, for example, reporting
performance of the best policy obtained across various hyperparameter settings as evaluated via new
online experiences (Gulcehre et al., 2020). Here we consider an evaluation protocol that makes
the amount of online access to the environment explicit. In particular, the ofÔ¨Çine RL algorithm is
2

Published as a conference paper at ICLR 2021
allowed to use the environment to evaluate Ne policies (e.g. an average over repeated trials for each
policy), which, for example, may derive from different hyperparameter choices. The best of the
evaluated policies can then be selected. Note that Ne = 1 corresponds to pure ofÔ¨Çine RL.
3
DEEP AVERAGERS WITH COSTS MDPS (DAC-MDPS)
From a practical perspective our approach carries out the following steps as illustrated in Figure 1.
(1) We start with a static experience dataset, where the states are assumed to come from a continuous
latent state space. For example, states encoded via random or learned deep representations. (2) Next
we compile the dataset into a tabular MDP over the ‚Äúcore states‚Äù of the DAC-MDP (those in the
dataset). This compilation uses k-nearest neighbor (kNN) queries to deÔ¨Åne the reward and transition
functions (Equation 2) functions of the core states. (3) Next we use a GPU implementation of value
iteration to solve for the tabular MDP‚Äôs optimal Q-function. (4) Finally, this tabular Q-function is
used to deÔ¨Åne the Q-function over the entire DAC-MDP (Equation 3). Previously unseen states at
test time are assigned Q-values and in turn a policy action via kNN queries over the core states.
Conceptually, our DAC-MDP model is inspired by Gordon‚Äôs (1995) early work that showed the con-
vergence of (ofÔ¨Çine) approximate value iteration for a class of function approximators; averagers,
which includes methods such as k nearest neighbor (kNN) regression, among others. It was also ob-
served that approximate value iteration using an averager was equivalent to solving an MDP derived
from the ofÔ¨Çine data. That observation, however, was not investigated experimentally and has yet to
be integrated with deep representation learning. Here we develop and evaluate such an integration.
The quality of an averagers MDP, and model-learning in general, depends on the size and distribution
of the dataset. In particular, an optimal planner can exploit inaccuracies in the underrepresented parts
of the state-action space, which can lead to poor performance in the real environment. The DAC-
MDPs aim to avoid this by augmenting the derived MDPs with costs/penalties on under-represented
transitions. This turns out to be essential to achieving good performance on challenging benchmarks.
3.1
DAC-MDP DEFINITION
A DAC-MDP is deÔ¨Åned in terms of an experience dataset D = {(si, ai, ri, s‚Ä≤
i)} from the true
MDP M with continuous latent state space S and Ô¨Ånite action space A. The DAC-MDP Àú
M =
(S, A, ÀúR, ÀúT) shares the same state and action spaces as M, but deÔ¨Ånes the reward and transition
functions in terms of empirical averages over the k nearest neighbors of (s, a) in D.
The distance metric d(s, a, s‚Ä≤, a‚Ä≤) gives the distance between pairs (s, a) and (s‚Ä≤, a‚Ä≤). This met-
ric considers (s,a) pairs with different actions to be inÔ¨Ånitely distant. Otherwise, the distance be-
tween pairs involving the same action is the euclidean distance between their states. In particular,
the distance between (s, a) and a data tuple (si, ai, ri, s‚Ä≤
i) is given by d(s, a, si, ai). Also, we let
kNN(s, a) denote the set of indices of the k nearest neighbors to (s, a) in D, noting that the depen-
dence on D and d is left implicit. Given hyperparameters k (smoothing factor) and C (cost factor)
we can now specify the DAC-MDP reward and transition function.
ÀúR(s, a) = 1
k
X
i‚ààkNN(s,a)
ri ‚àíC ¬∑ d(s, a, si, ai),
ÀúT(s, a, s‚Ä≤) = 1
k
X
i‚ààkNN(s,a)
I[s‚Ä≤ = s‚Ä≤
i]
(2)
The reward for (s, a) is simply the average reward of the nearest neighbors with a penalty for each
neighbor that grows linearly with the distance to a neighbor. Thus, the farther (s, a) is to its nearest
neighbor set, the less desirable its immediate reward will be. The transition function is simply the
empirical distribution over destination states of the nearest neighbor set.
Importantly, even though a DAC-MDP has an inÔ¨Ånite continuous state space, it has a special Ô¨Ånite
structure. Since the transition function ÀúT only allows transitions to states appearing as destination
states in D. We can view Àú
M as having a Ô¨Ånite core set of states SD = {s‚Ä≤
i | (si, ai, ri, s‚Ä≤
i) ‚ààD}.
States in this core do not transition to non-core states and each non-core state immediately transitions
to the core for any action. Hence, the value of core states is not inÔ¨Çuenced by non-core states.
Further, once the core values are known, we can compute the values of any non-core state via one-
step look ahead using ÀúT. Thus, we can optimally solve a DAC-MDP by solving just its Ô¨Ånite core.
SpeciÔ¨Åcally, let ÀúQ be the optimal Q-function of Àú
M. We can compute ÀúQ for the core states by solving
the Ô¨Ånite MDP Àú
MD = (SD, A, ÀúR, ÀúT). We can then compute ÀúQ for any non-core state on demand
3

Published as a conference paper at ICLR 2021
via the following one-step look-ahead expression.1 This allows us to compute the optimal policy of
Àú
M, denoted ÀúœÄ, using any solver of Ô¨Ånite MDPs.
ÀúQ(s, a) = 1
k
X
i‚ààkNN(s,a)
ri + Œ≥ max
a
ÀúQ(s‚Ä≤
i, a) ‚àíC ¬∑ d(s, a, si, ai)
(3)
3.2
DAC-MDP PERFORMANCE LOWER BOUND
We are ultimately interested in how well the optimal DAC-MDP Àú
M policy ÀúœÄ performs in the true
MDP M. Without further assumptions, ÀúœÄ can be arbitrarily sub-optimal in M, due to potential ‚Äúnon-
smoothness‚Äù of values, limited data, and limited neighborhood sizes. We now provide a lower-bound
on the performance of ÀúœÄ in M that quantiÔ¨Åes the dependence on these quantities. Smoothness is
characterized via a Lipschitz smoothness assumptions on B[ ÀúQ], where B is the Bellman operator for
the true MDP and ÀúQ is the optimal Q-function for Àú
M using hyperparameters k and C. In particular,
we assume that there is a constant L(k, C), such that for any state-action pairs (s, a) and (s‚Ä≤, a‚Ä≤)
B[ ÀúQ](s, a) ‚àíB[ ÀúQ](s‚Ä≤, a‚Ä≤)
 ‚â§L(k, C) ¬∑ d(s, a, s‚Ä≤, a‚Ä≤).
This quantiÔ¨Åes the smoothness of the Q-function obtained via one-step look-ahead using the true
dynamics The coverage of the dataset is quantiÔ¨Åed in terms of the worst case average distance to
a kNN set deÔ¨Åned by ¬Ødmax=max(s,a)
1
k
P
i‚ààkNN(s,a) d(s, a, si, ai). The bound also depends on
Qmax=max(s,a) ÀúQ(s, a) and MN,k, which is the maximum number of distinct kNN sets over a
dataset of size N. For L2 distance over a d-dimensional space,MN,k is bounded by O

(kN)
d
2 +1
.
Theorem 3.1. For any data set D of size N, let ÀúQ and ÀúœÄ be the optimal Q-function and policy for the
corresponding DAC-MDP with parameters k and C. If B[ ÀúQ] is Lipshitz continuous with constant
L(k, C), then with probability at least 1 ‚àíŒ¥,
V ÀúœÄ ‚â•V ‚àó‚àí2
 L(k, C) ¬∑ ¬Ødmax + Qmaxœµ(k, N, Œ¥)

1 ‚àíŒ≥
,
œµ(k, N, Œ¥) =
r
1
2k ln 2MN,k
Œ¥
,
which for L2 distance over a d-dimensional space yields œµ(k, N, Œ¥) = O
q
1
k
 d ln kN + ln 1
Œ¥

.
The full proof is in the Appendix A.2. The Ô¨Årst term in the bound characterizes how well the dataset
represents the MDP from a nearest neighbor perspective. In general, ¬Ødmax decreases as the dataset
becomes larger. The second term characterizes the variance of stochastic state transitions in the
dataset and decreases with the smoothness parameter k. Counter-intuitively we see that for Eu-
clidean distance, the second term can grow logarithmically in the dataset size.This worst-case be-
havior is due to not making assumptions about the distribution of source states-actions in the dataset.
Thus, we can‚Äôt rule out adversarial choices that negatively inÔ¨Çuence the kNN sets of critical states.
Practical Issues: There are some key practical issues of the framework regarding the scalability of
VI and selection of hyperparameters. We exploit the Ô¨Åxed sparse structure of DAC-MDPs along
with parallel compute afforded by GPUs to gracefully scale our VI to millions of states. This can
provide anywhere between 20-1000x wall clock speedup over its serial counterpart. To reduce the
sensitivity to the smoothness parameter k, we use weighted averaging based on distances to nearest
neighbors instead of uniform averaging in Equation 2. Furthermore, we use different smoothness
parameter k and kœÄ to build the DAC-MDP and compute the policy respectively. We Ô¨Ånd that using
larger values of kœÄ is generally beneÔ¨Åcial to reduce the variance. Finally we use a simple rule-of-
thumb of setting cost factor C to be in the order of magnitude of observed rewards. These choices
are further elaborated in detail in appendix A.1
3.3
REPRESENTATION LEARNING
While DAC-MDPs can be deÔ¨Åned over raw observations, for complicated observation spaces, such
as images, performance will likely be poor in practice for simple distance functions. Thus, com-
bining the DAC-MDP framework with deep representation learning is critical for observation-rich
1Note that we can get ÀúV using value iteration on Àú
MD.
ÀúQ can then be computed via 1-step lookup as
ÀúQ(s, a) = ÀúR(s, a) + Œ≥ P
s‚Ä≤‚ààÀú
T (s,a) ÀúT(s, a, s‚Ä≤) ¬∑ ÀúV (s)
4

Published as a conference paper at ICLR 2021
Figure 2: (a) Greedy Policy performance for CartPole with varying (a) cost paramter C. (b) smoothness
parameter k. (c) policy smoothing parameter kœÄ
environments. Since the primary focus of this paper is on the introduction of DAC-MDPs, rather
than representation learning, below we describe three basic representation-learning approaches used
in our experiments. An important direction for future work is to investigate the effectiveness of other
alternatives and to develop representation-learning speciÔ¨Åcally for the DAC-MDP framework.
Random Projection (RND): This baseline simply produces a representation vector as the out-
put of a chosen network architecture with randomly initialized weights (e.g. a CNN with random
weights for images). OfÔ¨Çine DQN (DQN): From the Ô¨Årst term of Theorem 3.1 we see that de-
creasing the Lipschitz constant of B[ ÀúQ] can improve the performance bound. This suggests that
a representation that is smooth with respect to the Q-function can be beneÔ¨Åcial. For this purpose,
our second representation directly uses the penultimate layer of DQN (Mnih et al., 2013) after it has
been trained on our ofÔ¨Çine dataset D. Since the penultimate layer is just a linear mapping away from
the DQN Q-values, it is reasonable to believe that this representation will be smooth with respect to
meaningful Q-functions for the environment. BCQ - Imitation (BCQ): The ofÔ¨Çine RL algorithm
BCQ (Fujimoto et al., 2019) trains an imitation network to emulate the action distribution of the
behavioral policy. This policy is used to help avoid backing up action values for under-represented
actions. We use the penultimate layer of the imitation network trained on our dataset as the third
type of representation. This representation is not as well-motivated by our performance bounds, but
is intuitively appealing and provides a policy-centric contrast to Q-value modeling.
4
EXPERIMENTS
The experiments are divided into three sections. First, we present exploratory experiments in Cart-
pole to illustrate the impact of DAC-MDP parameters using an idealized representation. Second, we
demonstrate the scalability of the approach on Atari games with image-based observations. Finally,
we demonstrate use-cases enabled by our planning-centric approach in a 3D Ô¨Årst-person domain.
Exploring DAC-MDP Parameters: We explore the impact of the DAC-MDP parameters in Cart-
pole. To separate the impact of state-representation and DAC-MDP parameters, we use the ideal
state representation consisting of the true 4-dimensional system state. We generate three datasets
of size 100k each: (1) RandomBag (V œÄŒ≤ = 20).2: Trajectories collected by random poli-
cies. (2) OptimalBag (V œÄŒ≤ = 140): Trajectories collected by a well-trained DQN policy. (3)
MixedBag (V œÄŒ≤ = 500): Trajectories collected by a mixed bag of œµ-greedy DQN policies using
œµ ‚àà[0, 0.1, 0.2, 0.4, 0.6, 1].
We Ô¨Årst illustrate the impact of the cost parameter C by Ô¨Åxing k = kœÄ = 1 and varying C from 0 to
1e6. Figure 2a shows that for all datasets when there is no penalty for under-represented transitions,
i.e. C = 0 , the policy is extremely poor. This shows that even for this relatively simple problem
with a relatively large dataset, the MDPs from the original cost-free averagers framework, can yield
low-quality policies. At the other extreme, when C is very large, the optimal policy tries to stay as
close as possible to transitions in the dataset. This results in good performance for the OptimalBag
dataset, since all actions are near optimal. However, the policies resulting from the MixedBag and
RandomBag datasets fail. This is due to those datasets containing a large number of sub-optimal
actions, which the policy should actually avoid for purposes of maximizing reward. Between these
extremes the performance is relatively insensitive to the choice of C.
Figure 2b explores the impact of varying k using kœÄ = 1 and C = 1. The main observation is that
there is a slight disadvantage to using k = 1, which deÔ¨Ånes a deterministic Ô¨Ånite MDP, compared
to k > 1, especially for the non-optimal datasets. This indicates that the optimal planner is able
2V œÄŒ≤ is the expected performance of the behavioral policy for the dataset.
5

Published as a conference paper at ICLR 2021
-20.0
-10.0
0.0
10.0
Avg Rewards
Pong
DAC
BCQ
BCQ
10.0
20.0
30.0
Breakout
300.0
400.0
500.0
600.0
SpaceInvaders
1000.0
2K
Qbert
0.5
1.0
Iterations (1e5)
25K
50K
75K
100K
Avg Rewards
Atlantis
0.5
1.0
Iterations (1e5)
25.0
30.0
35.0
Bowling
0.5
1.0
Iterations (1e5)
20.0
25.0
30.0
Freeway
0.5
1.0
Iterations (1e5)
50.0
100.0
150.0
Amidar
-20.0
-10.0
0.0
10.0
Avg Rewards
Pong
DAC
DQN
DQN
1.0
2.0
3.0
4.0
Breakout
100.0
200.0
300.0
SpaceInvaders
0.0
1000.0
2K
Qbert
0.5
1.0
Iterations (1e5)
20K
40K
60K
Avg Rewards
Atlantis
0.5
1.0
Iterations (1e5)
0.0
20.0
40.0
60.0
Bowling
0.5
1.0
Iterations (1e5)
0.0
10.0
20.0
30.0
Freeway
0.5
1.0
Iterations (1e5)
0.0
50.0
100.0
Amidar
Figure 3: Results on Atari 100K (left)BCQ (right) DQN. Each agent is trained for 100K iterations(training
steps), and evaluated on 10 episodes every 10K steps. At each of these evaluation checkpoints, we use the
internal representation to compile DAC-MDPs. We then evaluate the DAC-MDPs for Ne = 6. Runs averaged
over 5 seeds and error bars plot the 95% conÔ¨Ådence interval.
Pong
Breakout SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
Online DQN normalized score
BCQ
DAC
BCQ, Ne = 1
DAC
BCQ, Ne = 6
DAC
BCQ, Ne = 20
Pong
Breakout SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
1.0
0.5
0.0
0.5
1.0
1.5
Online DQN normalized score
DQN
DAC
DQN, Ne = 1
DAC
DQN, Ne = 6
DAC
DQN, Ne = 20
Figure 4: Results for different sets of candidate policies Ne on 100K dataset. Here we plot the Ô¨Ånal perfor-
mance of BCQ representation (left) and DQN representation (right) along with the DAC-MDP performances
for different values of Ne. Runs averaged over 5 seeds. Error bars plot the 95% conÔ¨Ådence interval.
to beneÔ¨Åt by reasoning about the stochastic transitions of the DAC-MDP for k > 1. Otherwise the
results are relatively insensitive to k. We set k = 5 for remaining experiments unless otherwise
stated. Finally, Figure 2c varies the policy smoothing parameter kœÄ from 1 to 31. Similar to the
results for varying k, there is a beneÔ¨Åt to using a value of kœÄ > 1, but otherwise the sensitivity is
low. Thus, even for this relatively benign problem setting, some amount of averaging at test time
appears beneÔ¨Åcial. We Ô¨Ånd that DAC-MDPs are slightly more sensitive to these parameters in low
data regime. Similar experiments for low data regime are presented in Appendix A.4.
Experiments on Atari: We choose 8 stochastic Atari games ranging from small to medium action
spaces: Pong, Breakout, Space Invaders, Q Bert, Atlantis, Bowling, Freeway, and Amidar. These
are stochastic in nature as sticky actions have been enabled. Following recent work (Fujimoto et al.,
2019), we generate datasets by Ô¨Årst training a DQN agent for each game. Next, to illustrate perfor-
mance on small datasets and scalability to larger datasets, we generated two datasets of sizes 100K
and 2.5 million for each game using an œµ-greedy version of the DQN policy. In particular, each
episode had a 0.8 probability of using œµ = 0.2 and œµ = 0.001 otherwise. This ensures a mixture of
both near optimal trajectories and more explorative trajectories.
We Ô¨Årst trained ofÔ¨Çine DQN and BCQ agents on the 100K dataset for 100K iterations and evaluated
on 10 test episodes every 10K iterations. At each evaluation iteration, we construct DAC-MDPs
using the latent state representation at that point. We consider an ofÔ¨Çine RL setting where Ne=6,
i.e. we evaluate 6 policies (on 10 episodes each) corresponding to 6 different DAC-MDPs, set to
a parameter combination of k=5, C‚àà{1, 100, 1M}, and kœÄ‚àà{11, 51}. For each representation and
evalutaion point, the best performing DAC-MDP is recorded. The entire 100K iteration protocol
was repeated 3 times. Figure 3 shows the averaged curves with 90% conÔ¨Ådence intervals.
The main observation is that for both BCQ and DQN representations the corresponding DAC-MDP
performance (labeled DAC-BCQ and DAC-DQN) is usually better than BCQ or DQN policy at each
point. Further, the DAC-MDP performance is usually better than the maximum performance of
pure BCQ/DQN across the iterations. These results point to an interesting possibility of using DAC-
MDPs to select the policy at the end of training at any Ô¨Åxed number of training iterations, rather than
using online evaluations along the curve. Interestingly, the Ô¨Årst point on each curve corresponds to
a random network representation which is often non-trivial in many domains. However, we do see
that in many domains the DAC-MDP performance further improves as the representation is learned,
showing that the DAC-MDP framework is able to leverage improved representations.
Figures 4 investigates the performance at the Ô¨Ånal iteration for different values of Ne. For Ne = 1
we use k = 5, kœÄ = 11, C = 1 and for Ne = 20 we expand on the parameter set used for
6

Published as a conference paper at ICLR 2021
Pong
Breakout
SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.0
0.5
1.0
1.5
2.0
Online DQN normalized score
DAC
RND
DQN
DAC
DQN
BCQ
DAC
BCQ
Figure 5: Atari results for 2.5M dataset. We show the Ô¨Ånal performance of BCQ and DQN trained for 2.5M
iterations. We also use the same representation for the DAC-MDPs named as DAC-BCQ and DAC-DQN
respectively. All DAC-MDPs are evaluated with Ne = 6.
Ne = 6 [C‚àà{1, 10, 100, 1000, 1M}, and kœÄ‚àà{5, 11, 31, 51}]. First we see that in the majority of
cases, even Ne = 1 is able to perform as well or better than BCQ or DQN and that increasing Ne
often signiÔ¨Åcantly improves performance. We do see that for three games, SpaceInvaders, QBert,
and Atlantis, DAC-BCQ (Ne = 1) is signiÔ¨Åcantly worse than the BCQ policy. This illustrates
the potential challenge of hyperparameter selection in the ofÔ¨Çine framework without the beneÔ¨Åt of
additional online evaluations.
Finally we show results for all representation on the 2.5M dataset in Figure 5. In all but one case
(Breakout DAC-DQN), we see that DAC-MDP improves performance or is no worse than the cor-
responding BCQ and DQN policies. Further in most cases, the DAC-MDP performance improves
over the Online DQN agent used to generate the dataset which was trained on 10M online transi-
tions. While random representations perform reasonably in some cases, they are signiÔ¨Åcantly out-
performed by learned representations. These results show, for the Ô¨Årst time, that optimal planning
can result in non-trivial performance improvement across a diverse set of Atari games. Given the
simplicity of the representation learning and the difference between random and learned representa-
tions, these results suggest signiÔ¨Åcant promise for further beneÔ¨Åts from improved representations.
Figure 6: Agent and top view for 3D Navigation domains. (left) Simple Room, (center) Box and Pillar Room
and (right) Tunnel Room.
Illustrative Use Cases in 3D Navigation: We now show the Ô¨Çexibility of an optimal planning
approach using three use cases in Gym Mini-World (Chevalier-Boisvert, 2018), a Ô¨Årst-person con-
tinuous 3D navigation environment. In all scenarios the agent has three actions: Move Forward,
Turn Right, and Turn Left. Episodes lasted for a maximum of 100 time steps. The raw observa-
tions are 84x84 RGB images. We use random CNN representations in all experiments, which were
effective for this environment. Our ofÔ¨Çine dataset was collected by following a random policy for
100K time steps. In these experiments we use Ne = 2 with k = 5, kœÄ ‚àà{11, 51}, and C = 0.01.
All the rooms used for the experiments and their optimal policies are visualized in Ô¨Åg 15a. We also
compare our approach against baselines(DQN/BCQ) in Appendix A.6 and Ô¨Ånd that DAC-MDPs
perform better and more consistently across the tasks.
Case 1: Adapting to ModiÔ¨Åed Action Space. We designed a simple room with a blue box in a Ô¨Åxed
position and an agent that is spawned in a random initial position. The agent gets a -1 reward for
bumping into the walls and a terminal reward of +1 for reaching the blue box. The DAC-MDP
achieves an average total reward of 0.98 using the ofÔ¨Çine dataset. Next, we simulate the event that
an actuator is damaged, so that the agent is unable to turn left. For most ofÔ¨Çine RL approaches
this would require retraining the agent on a modiÔ¨Åed dataset from scratch. Rather our DAC-MDP
approach can account for such a scenario by simply attaching a huge penalty to all left actions. The
solution to this modiÔ¨Åed DAC-MDP achieves an average score of 0.96 and is substantially different
than the original since it must Ô¨Ånd revolving paths with just the turn right action.
Case 2: Varying Horizons. We create a new room by adding a pillar in the simple room described
above. Additionally, the agent gets a small non-terminal reward of 0.02 for bumping into the pillar.
7

Published as a conference paper at ICLR 2021
Depending on the placement of the agent and the expected lifetime of the agent it may be better
to go for the single +1 reward or to repeatedly get the 0.02 reward. This expected lifetime can
be simulated via the discount factor, which will result in different agent behavior. Typical ofÔ¨Çine
RL approaches would again need to be retrained for each discount factor of interest. Rather, the
DAC-MDP approach simply allows for changing the discount factor used by VI and solving for
the new policy in seconds. Using a small discount factor (0.95) our short-term agent achieves an
average score of 0.91 by immediately attempting to get to the box. However, the long-term agent
with larger discount factor (0.995) achieves a score of 1.5 by repeatedly collecting rewards from the
pillar. Qualitative observation suggests that both policies perform close to optimally.
Case 3: Robust/Safe Policies. Our third room adds a narrow passage to the box with cones on the
side. Tunnel Room simulates the classic dilemma of whether to choose a risky or safe path. The
agent can risk getting a -10 reward by bumping into the cones or can optionally follow a longer
path around an obstacle to reach the goal. Even with a relatively large dataset, small errors in the
model can lead an agent along the risky path to occasionally hit a cone. To build robustness, we Ô¨Ånd
an optimal solution to a modiÔ¨Åed DAC-MDP where at each step there is a 10% chance of taking
a random action. This safe policy avoids the riskier paths where a random ‚Äúslip‚Äù might lead to
disaster and always chooses the safer path around the obstacle. This achieved an average score of
0.72. In contrast, the optimal policy for the original DAC-MDP achieved an average score of 0.42
and usually selected the riskier path to achieve the goal sooner (encouraged by the discount factor).
Initial Run
Secondary Objective
Objective
0
10
20
30
40
Wall Time (minutes)
DAC-RND
DQN
BCQ
Figure 7: Comparison of Computation Cost
for different appraoches. [100k Dataset]
Computation Time Analysis: DAC-MDP has explicit con-
vergence guarantees; however, the stopping condition of
DQN and BCQ is not clear. We follow previous works
and use batch updates equal to the size of the dataset. We
deÔ¨Åne the Ô¨Årst run for a set of hyperparameters/objective
as an algorithm‚Äôs initial run. We assume that the rep-
resentation is learned in the initial run and frozen for
secondary objectives. Here secondary objectives can in-
clude new hyperparameter sets or goals. Fig 7 compares
the wall time of different approaches. Here we can see
that DAC-MDPs are 2-4x faster for the initial run. For
secondary objectives, however, we can see a wall-time
speedup of anywhere between 25-40x. In the initial run of DAC-MDPs, KNN lookups3 account for
most of the computation time. Since we can reuse the KNN lookups for any secondary objective,
the VI solver dominates the runtime for the new run, which is much faster. For a large dataset of
2.5M, it takes 60-90 minutes to compile the DAC-MDP and less than 3 minutes to solve the MDP.
The memory consumption scales linearly with the state count, action space, and the smoothness
parameter k. Fully compiled MDPs for 2.5M approximately ranges from 2-6GB depending on
the action space. The current implementation is not fully optimized for memory however one can
perform state space aggregation in favor of memory efÔ¨Åciency.
5
RELATED WORK
Averagers Framework. Our work is inspired by the original averagers framework of Gordon (1995)
which showed the equivalence of a derived MDP to a type of approximate dynamic programming.
A construction similar to the derived MDP was explored in later work (Jong & Stone, 2007) within
an online RL setting. A model similar to Gordon‚Äôs derived MDP was later theoretically studied
for efÔ¨Åcient exploration in online continuous-state RL (Pazis & Parr, 2013). Due to its exploration
goals, that work introduced bonuses rather than penalties for unknown parts of the space. Ormoneit
& Glynn (2002) adapts Ô¨Åtted value iteration Gordon & Mitchell (1999) to kernel based reinforce-
ment learning. Furthermore Ormoneit & Sen (2002) studied the theoretical convergence and con-
sistency properties of this algorithm when combined with kernel-based regressors. Within the same
framework Ernst et al. (2005) employed tree-based regression algorithms to show scalability in high-
dimensional spaces. However, none of these approaches have been shown to scale to image based
domains. Also, our work is the Ô¨Årst to integrate the averagers derived MDP with pessimistic costs
3Currently, KNN lookups are performed by using CPU based KD-trees and has a throughput of Àú5k
lookups/sec. However, leveraging GPU optimized KD-trees ((Martin, 2012), Hu et al. (2015)) seems promising
to reduce the wall time of the algorithm further.
8

Published as a conference paper at ICLR 2021
in order to make it applicable to ofÔ¨Çine RL. Our work is also the Ô¨Årst to demonstrate the framework
in combination with deep representations for scaling to complex observation spaces such as images.
Optimal Planning with Deep Representations. Recent work (Corneil et al., 2018) uses variational
autoencoders with Gumbel softmax discretization to learn discrete latent representations. These are
used to form a tabular MDP followed by optimal planning in an online RL setting. More recently,
van der Pol et al. (2020) introduced a contrastive representation-learning and model-learning ap-
proach that is used to construct and solve a tabular MDP. Experimental results in both of these works
were limited to a small number of domains and small datasets. Unlike these works, which primar-
ily focus on representation learning, our focus here has been on providing a theoretically-grounded
model, which can be combined with representation-learning. Works like Kurutach et al. (2018) are
mostly focused on learning plannable representations than planning. Instead, they train a GAN to
generate a series of waypoints which can then be is leveraged by simple feedback controllers. Also,
Yang et al. (2020) and Agarwal et al. (2020) attempt to incorporate long term relationships into their
learned latent space and employ search algorithms such as A* and eliptical planners.
Episodic Control and Transition Graphs. Several prior approaches for online RL, sometimes
called episodic control, construct different types of explicit transition graphs from data that are used
to drive model-free RL (Blundell et al., 2016; Hansen, 2017; Pritzel et al., 2017; Lin et al., 2018;
Zhu et al., 2020; Marklund et al., 2020). None of these methods, however, use solutions produced
by planning as the Ô¨Ånal policy, but rather as a way of improving or bootstrapping the value estimates
of model-free RL. The memory structures produced by these methods have a rough similarity to
deterministic DAC-MDPs with k = 1.
Pessimism for OfÔ¨Çine Model-Free Based RL. Fonteneau et al. (2013) studied a ‚Äútrajectory based‚Äù
simulation model for ofÔ¨Çine policy evaluation. Similar in concept to our work, pessimistic costs
were used based on transition distances to ‚Äúpiece together‚Äù disjoint observations, which allowed for
theoretical lower-bounds on value estimates. That work, however, did not construct an MDP model
that could be used for optimal planning. Most recently, in concurrent work to ours, pessimistic
MDPs have been proposed for ofÔ¨Çine model-based RL (Kidambi et al., 2020; Yu et al., 2020). Both
approaches deÔ¨Åne MDPs that penalizes for model uncertainty based on an assumed ‚Äúuncertainty
oracle‚Äù signal and derive performance bounds under the assumption of optimal planning. In practice,
however, due to the difÔ¨Åculty of planning with learned deep-network models, the implementations
rely on model-free RL, which introduces an extra level of approximation.
Model free RL for OfÔ¨Çine settings: Agarwal et al. (2019) show that, for ofÔ¨Çine settings, stronger
off policy methods stemming from distributional RL like REM and QR-DQN perform better to
classic DQN. However recent ofÔ¨Çine RL methods directly tackle the problem of distributional shift
in ofÔ¨Çine settings as pointed out by Atkeson (1998), and more recently Fonteneau et al. (2013).
Works like Sutton et al. (2016),Nachum et al. (2019), Buckman et al. (2020) among others can
directly estimate the policy gradient for ofÔ¨Çine learning. Moreover, Fujimoto et al. (2019), Wang
et al. (2020), Kumar et al. (2020), Wu et al. (2019) directly constrains off policy learning towards
the dataset, in turn, introducing pessimism in their estimates.
6
SUMMARY
This work is an effort to push the integration of deep representation learning with near-optimal
planners. To this end, we proposed the Deep Averagers with Costs MDP (DAC-MDP) for ofÔ¨Çine
RL as a principled way to leverage optimal planners for tabular MDPs. The key idea is to deÔ¨Åne
a non-parametric continuous-state MDP based on the data, whose solution can be represented as
a solution to a tabular MDP derived from the data. This construction can be integrated with any
deep representation learning approach and addresses model inaccuracy by adding costs for exploit-
ing under-represented parts of the model. Using a planner based on a GPU-implementation of value
iteration, we demonstrate scalability to complex image-based environments such as Atari with rel-
atively simple representations derived from ofÔ¨Çine model-free learners. We also illustrate potential
use-cases of our planning-based approach for zero-shot adaptation to changes in the environment
and optimization objectives. Overall, the DAC-MDP framework has demonstrated the potential for
principled integrations of planning and representation learning. There are many interesting direc-
tions to explore further, including integrating new representation-learning approaches and exploiting
higher-order structure for the next level of scalability.
9

Published as a conference paper at ICLR 2021
REFERENCES
A. Agarwal, Sham M. Kakade, A. Krishnamurthy, and W. Sun. Flambe: Structural complexity and
representation learning of low rank mdps. ArXiv, abs/2006.10814, 2020.
Rishabh Agarwal, D. Schuurmans, and Mohammad Norouzi. An optimistic perspective on ofÔ¨Çine
reinforcement learning. ICML, 2019.
Christopher G Atkeson. Nonparametric model-based reinforcement learning. In Advances in neural
information processing systems, pp. 1008‚Äì1014, 1998.
Richard Bellman and Samuel G. Kneale. Dynamic programming. Princeton University Press, 1958.
Charles Blundell, B. Uria, A. Pritzel, Y. Li, Avraham Ruderman, Joel Z. Leibo, Jack W. Rae, Daan
Wierstra, and Demis Hassabis. Model-free episodic control. ArXiv, abs/1606.04460, 2016.
Craig Boutilier, Richard Dearden, and Mois¬¥es Goldszmidt. Stochastic dynamic programming with
factored representations. ArtiÔ¨Åcial intelligence, 121(1-2):49‚Äì107, 2000.
J. Buckman, Carles Gelada, and Marc G. Bellemare. The importance of pessimism in Ô¨Åxed-dataset
policy optimization. ArXiv, abs/2009.06799, 2020.
Maxime Chevalier-Boisvert. gym-miniworld environment for openai gym. https://github.
com/maximecb/gym-miniworld, 2018.
Dane S. Corneil, W. Gerstner, and J. Brea. EfÔ¨Åcient model-based deep reinforcement learning with
variational state tabulation. ArXiv, abs/1802.04325, 2018.
D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. J. Mach.
Learn. Res., 6:503‚Äì556, 2005.
R. Fonteneau, S. Murphy, L. Wehenkel, and D. Ernst. Batch mode reinforcement learning based on
the synthesis of artiÔ¨Åcial trajectories. Annals of Operations Research, 208:383‚Äì416, 2013.
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch
deep reinforcement learning algorithms. ArXiv, abs/1910.01708, 2019.
G. Gordon. Stable function approximation in dynamic programming. In ICML, 1995.
G. Gordon and Tom Michael Mitchell. Approximate solutions to markov decision processes. In
Ph.D. Thesis, MIT, 1999.
Caglar Gulcehre, Ziyu Wang, A. Novikov, T. L. Paine, Sergio Gomez Colmenarejo, Konrad Zolna,
Rishabh Agarwal, Josh Merel, Daniel J. Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold,
J. Li, Mohammad Norouzi, Matt Hoffman, OÔ¨År Nachum, G. Tucker, Nicolas Heess, and N. D.
Freitas. Rl unplugged: Benchmarks for ofÔ¨Çine reinforcement learning. ArXiv, abs/2006.13888,
2020.
Steven Stenberg Hansen. Deep episodic value iteration for model-based meta-reinforcement learn-
ing. ArXiv, abs/1705.03562, 2017.
Linjia Hu, S. Nooshabadi, and M. Ahmadi. Massively parallel kd-tree construction and nearest
neighbor search algorithms. 2015 IEEE International Symposium on Circuits and Systems (IS-
CAS), pp. 2752‚Äì2755, 2015.
Nicholas K Jong and Peter Stone. Model-based function approximation in reinforcement learning.
In Proceedings of the 6th international joint conference on Autonomous agents and multiagent
systems, pp. 1‚Äì8, 2007.
R. Kidambi, A. Rajeswaran, Praneeth Netrapalli, and T. Joachims. Morel : Model-based ofÔ¨Çine
reinforcement learning. ArXiv, abs/2005.05951, 2020.
Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
CoRR,
abs/1412.6980, 2015.
10

Published as a conference paper at ICLR 2021
Aviral Kumar, Aurick Zhou, G. Tucker, and Sergey Levine. Conservative q-learning for ofÔ¨Çine
reinforcement learning. NeurIPS, abs/2006.04779, 2020.
Thanard Kurutach, A. Tamar, Ge Yang, S. Russell, and P. Abbeel. Learning plannable representa-
tions with causal infogan. In NeurIPS, 2018.
Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. OfÔ¨Çine reinforcement learning: Tutorial,
review, and perspectives on open problems. ArXiv, abs/2005.01643, 2020.
Zichuan Lin, Tianqi Zhao, G. Yang, and L. Zhang. Episodic memory deep q-networks. ArXiv,
abs/1805.07603, 2018.
Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, J. Veness, Matthew J. Hausknecht, and
Michael H. Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. ArXiv, abs/1709.06009, 2018.
Henrik Marklund, Suraj Nair, and Chelsea Finn. Exact (then approximate) dynamic programming
for deep reinforcement learning. In Bian and Invariances Workshop, ICML, 2020.
W. W. Martin. Implementation of kd-trees on the gpu to achieve real time graphics processing. In
CSci Senior Seminar Conference, 2012.
Yoan Miche, Antti Sorjamaa, Patrick Bas, Olli Simula, Christian Jutten, and Amaury Lendasse. Op-
elm: optimally pruned extreme learning machine. IEEE transactions on neural networks, 21(1):
158‚Äì162, 2009.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A.
Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, Andrei A. Rusu, J. Veness, Marc G. Bellemare, A. Graves,
Martin A. Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, S. Petersen, C. Beattie, A. Sadik,
Ioannis Antonoglou, H. King, D. Kumaran, Daan Wierstra, S. Legg, and Demis Hassabis. Human-
level control through deep reinforcement learning. Nature, 518:529‚Äì533, 2015.
OÔ¨År Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, L. Li, and D. Schuurmans. Algaedice: Policy
gradient from arbitrary experience. OPTRL, abs/1912.02074, 2019.
¬¥Ars√¶ll ¬¥or J¬¥ohannsson. Gpu-based markov decision process solver. In Msc. Thesis Reykjavik Uni-
versity, 2009.
Dirk Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems. IEEE
Trans. Autom. Control., 47:1624‚Äì1636, 2002.
Dirk Ormoneit and ¬¥S. Sen. Kernel-based reinforcement learning. In Machine Learning, 2002.
Jason Pazis and Ronald Parr. Pac optimal exploration in continuous space markov decision pro-
cesses. In Twenty-Seventh AAAI Conference on ArtiÔ¨Åcial Intelligence, 2013.
A. Pritzel, B. Uria, S. Srinivasan, Adri`a Puigdom`enech Badia, Oriol Vinyals, Demis Hassabis, Daan
Wierstra, and Charles Blundell. Neural episodic control. In ICML, 2017.
Martin L. Puterman. Markov decision processes: Discrete stochastic dynamic programming. In
Wiley Series in Probability and Statistics, 1994.
Aswin Raghavan, Saket Joshi, Alan Fern, Prasad Tadepalli, and Roni Khardon. Planning in factored
action spaces with symbolic dynamic programming. In AAAI. Citeseer, 2012.
S. Ruiz and B. Hern¬¥andez. A parallel solver for markov decision process in crowd simulations. In
2015 Fourteenth Mexican International Conference on ArtiÔ¨Åcial Intelligence (MICAI), pp. 107‚Äì
116, 2015.
R. Sutton, A. R. Mahmood, and Martha White. An emphatic approach to the problem of off-policy
temporal-difference learning. ML, abs/1503.04269, 2016.
11

Published as a conference paper at ICLR 2021
Csaba D Toth, Joseph O‚ÄôRourke, and Jacob E Goodman. Handbook of discrete and computational
geometry. CRC press, 2017.
Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, and M. Welling. Plannable approximations to
mdp homomorphisms: Equivariance under actions. In AAMAS, 2020.
Ziyu Wang, A. Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, B. Shahriari,
N. Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and N. D. Freitas. Critic regularized
regression. Neurips, abs/2006.15134, 2020.
Y. Wu, G. Tucker, and OÔ¨År Nachum. Behavior regularized ofÔ¨Çine reinforcement learning. ArXiv,
abs/1911.11361, 2019.
Zhimin Wu, E. Hahn, A. G¬®unay, L. Zhang, and Y. Liu. Gpu-accelerated value iteration for the
computation of reachability probabilities in mdps. In ECAI, 2016.
G. Yang, A. Zhang, Ari S. Morcos, Joelle Pineau, P. Abbeel, and R. Calandra. Plan2vec: Unsuper-
vised representation learning by latent plans. ArXiv, abs/2005.03648, 2020.
Tianhe Yu, G. Thomas, Lantao Yu, S. Ermon, J. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma.
Mopo: Model-based ofÔ¨Çine policy optimization. ArXiv, abs/2005.13239, 2020.
Guangxiang Zhu, Zichuan Lin, G. Yang, and C. Zhang. Episodic reinforcement learning with asso-
ciative memory. In ICLR, 2020.
12

Published as a conference paper at ICLR 2021
A
APPENDIX
A.1
PRACTICAL ISSUES
We now describe some of the key practical issues of the framework and how we address them.
Optimal Planning. We use value iteration (VI) (Bellman & Kneale, 1958) to optimally solve the
core-state MDPs, which can have a number of states equal to the size of the dataset N. While in
general storing a transition matrix with N states can take O(n2) space, the core-state MDPs have
sparse transitions with each state having at most k successors, which our implementation exploits.
Further, VI is highly parallelizable on GPUs, since the update of each state at each VI iteration
can be done independently. We developed a simple GPU implementation of VI, which can provide
between 20-1000x wall clock speedup over its serial counterpart. Our current implementation can
solve MDPs with a million states in less than 30 seconds and easily scale to MDPs with several
million states and medium action spaces (up to 10). Thus, this implementation is adequate for the
dataset sizes that can be expected in many ofÔ¨Çine RL settings, where often we are interested in
performing well given limited data. For extremely large datasets, we expect that future work can
develop effective sub-sampling approaches, similar to those used for scaling other non-parametric
models, e.g. Miche et al. (2009). We will release an open-source implementation and further details
are in Appendix A.7.
Weighted Averaging: The above DAC-MDP construction and theory is based on uniform averaging
across kNN sets. In practice, however, most non-parametric regression techniques use weighted
averaging, where the inÔ¨Çuence of a neighbor decreases with its distance. Our implementation also
uses weighted averaging, which we found has little impact on overall performance (see Appendix),
but can reduce sensitivity to the choice of k, especially for smaller datasets. In our implementation
we compute normalized weights over kNN sets according to the inverse of distances. In particular,
the modiÔ¨Åed DAC-MDP reward and transition functions become,
ÀúR(s, a) =
X
i‚ààkNN(s,a)
Œ±(s, a, si, ai)(ri ‚àíC ¬∑ d(s, a, si, ai))
(4)
ÀúT(s, a, s‚Ä≤) =
X
i‚ààkNN(s,a)
Œ±(s, a, si, ai) I[s‚Ä≤ = s‚Ä≤
i],
(5)
where Œ±(s, a, si, ai) =
d‚Ä≤(s,a,si,ai)
P
j‚ààkNN(s,a) d‚Ä≤(s,a,sj,aj), d‚Ä≤(s, a, si, ai) =
1
d(s,a,si,ai)+Œ¥d and Œ¥d = 1e‚àí5
Optimized Policy Computation. After solving the DAC-MDP, the policy computation for new
states requires computing Q-values for each action via Equation 3, which involves a kNN query
for each action. We Ô¨Ånd that we can reduce this computation by a factor of |A| via the following
heuristic that performs just one kNN query at the state level. Here kNN(s) returns the data tuple of
indices whose source states are the k nearest neighbors of s.
ÀúœÄ(s) = max
a‚ààA
1
k
X
i‚ààkNN(s)
Œ±(s, si) Àú
Q‚àó(s‚Ä≤
i, a)
(6)
where, d‚Ä≤(s, si) =
1
d(s,si)+Œ¥d , Œ±(s, si) =
d‚Ä≤(s,si)
P
i‚ààkNN(s) d‚Ä≤(s,si) and Œ¥d is set to 1e‚àí5. We have found
that this approach rarely hurts performance and results in the same action choices. However, when
there is limited data, there is some evidence that this computation can actually help since it avoids
splitting the dataset across actions (see the Appendix for ablation study).
Hyperparameter Selection. The choice of the cost parameter C can signiÔ¨Åcantly inÔ¨Çuence the
DAC-MDP performance. At one extreme, if C is large, then the resulting policy will focus on
‚Äústaying close‚Äù to the dataset. At the other extreme when C = 0, there is no penalty for exploiting
the under-represented parts of the model. In general, the best choice will lie between the extremes
and must be heuristically and/or empirically selected. Here, We use the simple rule-of-thumb of
setting C to be in the order of magnitude of the observed rewards, to avoid exploitation but not
dominate the policy. In addition, if the online evaluation budget Ne (see Section 2) is greater than
one, we also consider values that are orders of magnitude apart to span qualitatively different ranges.
The DAC-MDP construction used the same smoothness parameter k for both building the MDP and
computing the policy for unknown states via Equation 3. We have found it is useful to use different
13

Published as a conference paper at ICLR 2021
values and in particular, there can be a beneÔ¨Åt for using a larger value for the later use to reduce
variance. Thus, our experiments will specify a value of k used to construct the MDP and a value kœÄ
used to compute the policy. Our default setting is k = 5 and kœÄ = 11, with more values possibly
considered depending on the evaluation budget Ne.
A.2
THEORETICAL PROOFS
Let ÀúœÄ bet the optimal policy of a DAC-MDP. We wish to bound the value V ÀúœÄ(s) of ÀúœÄ in the true
MDP in terms of the optimal value V ‚àó(s) for any state s. Using the following lemma from Pazis &
Parr (2013) it is sufÔ¨Åcient to bound the Bellman Error ÀúQ(s, a) ‚àíB[ ÀúQ](s, a) of ÀúQ across all s and a
with respect to the true MDP.
Lemma A.1. (Pazis & Parr, 2013),Theorem 3.12 For any Q-function Q with greedy policy œÄ, if for
all s ‚ààS and a ‚ààA, ‚àíœµ‚àí‚â§Q(s, a) ‚àíB[Q](s, a) ‚â§œµ+, then for all s ‚ààS,
V œÄ(s) ‚â•V ‚àó(s) ‚àíœµ‚àí+ œµ+
1 ‚àíŒ≥
.
In general, however, the Bellman Error can be arbitrarily large without further assumptions. Thus,
in this work, we make Lipschitz smoothness assumptions on B[ ÀúQ]. In particular, we assume that
there is a constant L(k, C) such that for any state-action pairs (s, a) and (s‚Ä≤, a‚Ä≤) we have
B[ ÀúQ](s, a) ‚àíB[ ÀúQ](s‚Ä≤, a‚Ä≤)
 ‚â§L(k, C) ¬∑ d(s, a, s‚Ä≤, a‚Ä≤).
Given the smoothness assumption for any (s, a), data sample (si, ai, ri, s‚Ä≤
i), and a constant L we
deÔ¨Åne a constant ‚àÜi(s, a) such that
B[ ÀúQ](s, a) = B[ ÀúQ](si, ai) ‚àí‚àÜi(s, a).
Note that based on the smoothness assumption we have that |‚àÜi(s, a)| ‚â§L(k, C) ¬∑ d(s, a, si, ai).
Using this deÔ¨Ånition we introduce a new operator ÀÜB, which will allow us to relate ÀúB to B.
ÀÜB[Q](s, a) = 1
k
X
i‚ààkNN(s,a)
ri + Œ≥ max
a‚Ä≤ Q(s‚Ä≤
i, a‚Ä≤) ‚àí‚àÜi(s, a).
(7)
The following lemma shows that this operator approximates B with high probability.
Lemma A.2. For any data set D of size N, value of k, and any cost parameter C, if ÀúQ is the
optimal Q-function of the corresponding DAC MDP, then with probability at least 1 ‚àíŒ¥, for all
(s, a) ‚ààS √ó A,
ÀÜB[ ÀúQ](s, a) ‚àíB[ ÀúQ](s, a) ‚â§Qmaxœµ(k, N, Œ¥) for all (s, a) ‚ààS √ó A
œµ(k, N, Œ¥) =
r
1
2k ln 2MN,k
Œ¥
where Qmax = max(s,a) ÀúQ(s, a) and n is the dimensionality of the state-action encoding.
Proof. In order to capture the variance of ÀÜB[ ÀúQ] associated with the transition dynamics, for each
state-action pair (s, a) and each nearest neighbor index i ‚ààkNN(s, a), deÔ¨Åne a random variable
Xi(s, a) = ri + Œ≥ maxa‚Ä≤ ÀúQ(S‚Ä≤
i, a‚Ä≤) ‚àí‚àÜi(s, a), where S‚Ä≤
i ‚àºT(si, ai, ¬∑). Note that each term of
ÀÜB[ ÀúQ](s, a) is a single sample of one Xi(s, a). That is,
ÀÜB[ ÀúQ](s, a) = 1
k
X
i‚ààkNN(s,a)
xi(s, a), where xi(s, a) ‚àºXi(s, a).
14

Published as a conference paper at ICLR 2021
Also note that according to the deÔ¨Ånition of ‚àÜi(s, a) the expected value of each Xi(s, a) is equal to
B[ ÀúQ](s, a).
E [Xi(s, a)] = ri + Œ≥E
h
max
a‚Ä≤
ÀúQ(S‚Ä≤
i, a‚Ä≤)
i
‚àí‚àÜi(s, a)
= B[ ÀúQ](si, ai) ‚àí‚àÜi(s, a)
= B[ ÀúQ](s, a)
Accordingly E
h
ÀÜB[ ÀúQ](s, a)
i
= B[ ÀúQ](s, a).
From the above, we can apply the Hoeffding inequality, which states that for k independent random
variables X1, . . . , Xk with bounded support ai ‚â§Xi ‚â§bi, if ¬ØX = 1
k
P
i Xi is the empirical mean,
then for all œµ > 0,
Pr
  ¬ØX ‚àíE
 ¬ØX
 ‚â•œµ

‚â§2 exp

‚àí2k2œµ2
P
i(bi ‚àíai)2

.
Applying this bound to ÀÜB[ ÀúQ](s, a) implies that:
Pr
 ÀÜB[ ÀúQ](s, a) ‚àíB[ ÀúQ](s, a)
 ‚â•œµ

‚â§2 exp
‚àí2kœµ2
Q2max

,
which can be equivalently written as
Pr
  ÀÜB[ ÀúQ](s, a) ‚àíB[ ÀúQ](s, a)
 ‚â•Qmax
r
1
2k ln 2
Œ¥‚Ä≤
!
‚â§Œ¥‚Ä≤.
This bound holds for individual s, a pairs. However, we need to bound the probability across all
s, a pairs. To do this note that the computed value ÀÜB[ ÀúQ](s, a) is based on the nearest neighbor set
kNN(s, a) and let MN,k denote an upper bound on the possible number of those sets across all
S √ó A. To ensure that the bound holds simultaneously for all such sets, we can apply the union
bound using Œ¥‚Ä≤ = Œ¥/MN,k. This bounds the probability over all state-action pairs simultaneously by
Œ¥.
Pr
  ÀÜB[ ÀúQ](s, a) ‚àíB[ ÀúQ](s, a)
 ‚â•Qmax
r
1
2k ln 2MN,k
Œ¥
!
‚â§Œ¥.
Theorem 3.1 For any data set D of size N, let ÀúQ and ÀúœÄ be the optimal Q-function and policy for the
corresponding DAC-MDP with parameters k and C. If B[ ÀúQ] is Lipschitz continuous with constant
L(k, C), then with probability at least 1 ‚àíŒ¥,
V ÀúœÄ ‚â•V ‚àó‚àí2
 L(k, C) ¬∑ ¬Ødmax + Qmaxœµ(k, N, Œ¥)

1 ‚àíŒ≥
,
œµ(k, N, Œ¥) =
r
1
2k ln 2MN,k
Œ¥
,
which for L2 distance over a d-dimensional space yields œµ(k, N, Œ¥) = O
q
1
k
 d ln kN + ln 1
Œ¥

.
Proof. The proof Ô¨Årst will bound the Bellman error of ÀúQ from above an below and then apply
Lemma A.1. We Ô¨Årst decompose the Bellman error into two parts by adding and subtracting ÀÜB[ ÀúQ].
ÀúQ(s, a) ‚àíB[ ÀúQ](s, a) =

ÀúQ(s, a) ‚àíÀÜB[ ÀúQ](s, a)

|
{z
}
Œæd(s,a)
+

ÀÜB[ ÀúQ](s, a) ‚àíB[ ÀúQ](s, a)

|
{z
}
Œæsim(s,a)
The Ô¨Årst term corresponds to the error due to the non-zero distance of the k nearest neighbors and
the second term is due to sampling error.
15

Published as a conference paper at ICLR 2021
Noting that ÀúB and ÀÜB only differ in one term, Œæd(s, a) can be simpliÔ¨Åed as follows.
Œæd(s, a) = ÀúQ(s, a) ‚àíÀÜB[ ÀúQ](s, a)
= ÀúB[ ÀúQ](s, a) ‚àíÀÜB[ ÀúQ](s, a)
= 1
k
X
i‚ààkNN(s,a)
‚àÜi(s, a) ‚àíC ¬∑ d(s, a, si, ai)
From this and the fact tht |‚àÜi(s, a)| ‚â§L(k, C) ¬∑ d(s, a, si, ai) we can immediately derive upper and
lower bounds on Œæd(s, a) for all s and a.
‚àí(L(k, C) + C) ¬∑ ¬Ødmax ‚â§Œæd(s, a) ‚â§(L(k, C) ‚àíC) ¬∑ ¬Ødmax
We can bound Œæsim(s, a) by a direct application of Lemma A.2. SpeciÔ¨Åcally, with probability at least
1 ‚àíŒ¥, for all s and a, |Œæsim(s, a)| ‚â§œµ(k, N, Œ¥). Putting these together we get that with probability
at least 1 ‚àíŒ¥, for all s and a,
‚àí
 (L(k, C) + C) ¬∑ ¬Ødmax + œµ(k, N, Œ¥)

‚â§ÀúQ(s, a)‚àíB[ ÀúQ](s, a) ‚â§(L(k, C) ‚àíC)¬∑ ¬Ødmax+œµ(k, N, Œ¥).
The proof is completed by applying Lemma A.1 to this bound.
Note: For a Euclidean distance metric, Toth et al. (2017)[Chapter 27] has established an upper bound
on MN,k.
MN,k = O

N ‚åàd/2‚åâk‚åäd/2‚åã+1
= O

(kN)d/2+1
.
A.3
ABLATION STUDY
We highlight the deviation of the practical implementation of DAC-MDPs from theory in section
A.1, namely, weighted averaging based on KNN distances (WA) and, KNN over states instead
of state-action pairs (sKNN). The theory for DAC-MDP is derived for uniform averaging. i.e.,
the probability distribution to the candidate next states from KNN state action pairs is uniformly
distributed irrespective of their relative distances. In contrast, weighted averaging normalizes the
probability distribution according to their relative distances from the query state-action pair. Sec-
ondly, the theory states that we query the K nearest neighbors for each state-action pair to calculate
the q values for any unseen state. This entails that |A| numbers of KNN calls have to be made
for each action decision. However, we can reduce this by simply querying k nearest neighbors for
states over state-action pairs. We query for K- nearest states when sKNN option is turned on. We
conduct the ablation study on the full[100k] dataset as well as a smaller dataset comprising of only
10% of the full dataset. Below in Figure 8 we show ablation study for each of these choices in the
standard CartPole domain.
We Ô¨Ånd that for a larger dataset, neither weighted averaging nor the state KNN approximation
affects the performance of DAC-MDPs. However, there is a noticeable drop in performance for
optimalBag dataset for smaller dataset sizes when state KNN approximation is turned off. This
suggests that when the dataset is mostly comprised of subsamples of optimal trajectories, the data is
not divided uniformly over the actions, resulting in less accurate estimates, especially when the data
is limited and skewed towards a particular policy.
A.4
ADDITIONAL CARTPOLE EXPERIMENTS
To further investigate the impact of DAC-MDP parameters, we conduct similar experiments to sec-
tion 4 for different dataset sizes on CartPole. In addition to the greedy policy performance we also
track the performance of œµ-greedy run of the policy to further distinguish the quality/robustness of
the policies learned.
We see a very similar trend (as in 100k) for the choice of cost parameter C even for the low data
regime of 10k. Figure 9 shows that for all datasets, when there is no penalty for under-represented
transitions,i.e. C = 0, the policy is extremely poor. At the other extreme, when C is very large,
the optimal policy tries to stay as close as possible to transitions in the dataset. This results in good
performance for the OptimalBag dataset, since all actions are near optimal. However, the policies
16

Published as a conference paper at ICLR 2021
RandomBag
MixedBag
OptimalBag
0
100
200
300
400
500
Greedy Policy
Ablation for dataset size 100k
RandomBag
MixedBag
OptimalBag
0
50
100
150
200
250
300
350
400
Greedy Policy
Ablation for dataset size 10k
RandomBag
MixedBag
OptimalBag
Dataset Version
0
50
100
150
200
250
300
350
400
e - Greedy Policy (e = 0.1)
Ablation
DAC-MDP
DAC-MDP, no WA
DAC-MDP, no sKNN
DAC-MDP, no WA, no sKNN
RandomBag
MixedBag
OptimalBag
Dataset Version
0
50
100
150
200
250
300
e - Greedy Policy (e = 0.1)
Ablation
DAC-MDP
DAC-MDP, no WA
DAC-MDP, no sKNN
DAC-MDP, no WA, no sKNN
Figure 8: (a) Ablation study for WA and sKNN in CartPole Domain. Greedy and eps-greedy policy returns for
different sets of hyperparameters and dataset versions of size 100k. (left) and 10K (right) Hyperparameters:
[k = 5, kœÄ = 11, C = 1, Ne = 1]
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
100
200
300
e - Greedy Policy (e = 0.1)
Average Return vs Cost Parameter C
0
1e-03
0.1
1
10
1e+03
1e+05
1e+07
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
100
200
e - Greedy Policy (e = 0.1)
Average return vs MDP build parameter k
1
5
11
51
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
100
200
300
e - Greedy Policy (e = 0.1)
Average return vs policy lift parameter k
1
5
11
51
101
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
100
200
300
e - Greedy Policy (e = 0.1)
0
1e-03
0.1
1
10
1e+03
1e+05
1e+07
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
100
200
300
e - Greedy Policy (e = 0.1)
1
5
11
51
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
100
200
300
400
e - Greedy Policy (e = 0.1)
1
5
11
51
101
Figure 9: Eps-Greedy performance for CartPole on different dataset sizes. (a) Top row: dataset size 10k (b)
Bottom Row: dataset size 50k
resulting from the MixedBag and RandomBag datasets fail. This is due to those datasets contain-
ing a large number of sub-optimal actions, which the policy should actually avoid for purposes of
maximizing reward. Between these extremes, the performance is relatively insensitive to the choice
of C.
DAC-MDP, however, is more sensitive towards the choice of k for small dataset regime. Figure 9
second column, explores the impact of varying k using Ô¨Åxed kœÄ = 1 and C = 1. The main observa-
tion is that there is a slight disadvantage to using k = 1, which deÔ¨Ånes a deterministic Ô¨Ånite MDP,
compared to k > 1, especially for the non-optimal datasets. Moreover, there is also a slight disad-
vantage in using larger k when the dataset is not dense. It is to be noted that although RandomBag
dataset contains the same amount of experience, it is much denser than other datasets as random
trajectories are quite short compared to optimal trajectories. This indicates that the optimal planner
17

Published as a conference paper at ICLR 2021
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
200
400
Greedy Policy
Average Return vs Cost Parameter C
0
1e-03
0.1
1
10
1e+03
1e+05
1e+07
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
200
400
Greedy Policy
Average return vs MDP build parameter k
1
5
11
51
RandomBag
MixedBag
OptimalBag
Dataset Version [10k]
0
200
400
Greedy Policy
Average return vs policy lift parameter k
1
5
11
51
101
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
200
400
Greedy Policy
0
1e-03
0.1
1
10
1e+03
1e+05
1e+07
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
200
400
Greedy Policy
1
5
11
51
RandomBag
MixedBag
OptimalBag
Dataset Version [50k]
0
200
400
Greedy Policy
1
5
11
51
101
Figure 10: Greedy performance for CartPole on different dataset sizes. (a) Top row: datset size 10k (b) Bottom
Row: dataset size 50k
is able to beneÔ¨Åt by reasoning about the stochastic transitions of the DAC-MDP for k > 1. However,
DAC-MDP suffers from high values of k when dataset is sparse.
Finally Figure 9 third column, varies the policy smoothing parameter kœÄ from 1 to 101. Similar to
the results for varying smoothness parameter k, there is a beneÔ¨Åt to using a value of kœÄ > 1, but
should not be chosen to be too large depending on how large the dataset is.
Pong
Breakout
SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
pre-trained DQN normalized score
Performance comparison for Different sets of candidate policies. Ne [Dataset Size:100k]
BCQ, Ne = 1
BCQ, Ne = 6
BCQ, Ne = 20
DAC
BCQ, Ne = 1
DAC
BCQ, Ne = 6
DAC
BCQ, Ne = 20
Pong
Breakout
SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
1
0
1
2
3
pre-trained DQN normalized score
DQN, Ne = 1
DQN, Ne = 6
DQN, Ne = 20
DAC
DQN, Ne = 1
DAC
DQN, Ne = 6
DAC
DQN, Ne = 20
Figure 11: (a) Greedy performance for Atari using different learnt representations and evaluation candidate
policies Ne[100k dataset]. Runs averaged over 3 runs. Error bars show the 95% conÔ¨Ådence interval.
A.5
ADDITIONAL ATARI EXPERIMENTS
Policy search for DQN and BCQ: In our experiments we consider an evaluation protocol that
makes the amount of online access to the environment explicit. In particular, the ofÔ¨Çine RL algorithm
is allowed to use the environment to evaluate Ne policies (e.g., each evaluation can be an average
over repeated trials), which, for example, may derived from different hyperparameter choices. It is
not clear how many of these evaluations will be actually needed for Q-learning algorithms such as
DQN that is primarily designed for online learning. Even approaches focusing on ofÔ¨Çine learning are
18

Published as a conference paper at ICLR 2021
Pong
Breakout
SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.0
0.5
1.0
1.5
2.0
Online DQN normalized score
Performance comparison for Different sets of candidate policies. Ne [Dataset Size:2.5M]
BCQ, Ne = 1
BCQ, Ne = 6
BCQ, Ne = 20
DAC
BCQ, Ne = 1
DAC
BCQ, Ne = 6
DAC
BCQ, Ne = 20
Pong
Breakout
SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.5
0.0
0.5
1.0
1.5
2.0
2.5
Online DQN normalized score
DQN, Ne = 1
DQN, Ne = 6
DQN, Ne = 20
DAC
DQN, Ne = 1
DAC
DQN, Ne = 6
DAC
DQN, Ne = 20
Figure 12: (a) Greedy performance for Atari using different learnt representations and evaluation candidate
policies Ne [2.5M dataset]
not spared from hyperparameter search and stopping criteria. Hence it is not clear how to evaluate
Q-iterating policies such as DQN or BCQ for different values of Ne where we are already using the
best parameters reported from previous works.
However, we can still assume that we know the value of Ne before hand and tune the learning
process accordingly. More speciÔ¨Åcally given a Ô¨Åxed number of training iterations the evaluation
frequency can be set such that we complete Ne evaluations by the end of the training. We can then
use the last Ne evaluation checkpoints to obtain the set of candidate policies. We can then choose
the best among them. It is worth noting that, even if we disregard the hyperparameter tuning, it is
still not entirely fair to compare BCQ,Ne = 6 directly with DAC-BCQ, Ne=6 as the DAC-MDP
only has access to the very last representation. Moreover, DAC-MDPs do not need stopping criteria
and are more robust to small representational changes. We show the policy search results for both
DQN and BCQ for 100k dataset as well as 2.5M dataset in Figure 11 and Figure 12 respectively.
5.0
10.0
15.0
Avg Rewards
Pong
50.0
100.0
Breakout
400.0
600.0
SpaceInvaders
1000.0
2K
3K
Qbert
5
10
Iterations (1e5)
50K
100K
150K
Avg Rewards
Atlantis
5
10
Iterations (1e5)
25.0
30.0
Bowling
5
10
Iterations (1e5)
28.0
30.0
32.0
Freeway
5
10
Iterations (1e5)
100.0
150.0
200.0
Amidar
DAC
BCQ
BCQ
(a)
Pong
Breakout SpaceInvaders
Qbert
Atlantis
Bowling
Freeway
Amidar
0.5
0.0
0.5
1.0
1.5
2.0
BCQ
DAC
BCQ, Ne = 1
DAC
BCQ, Ne = 6
DAC
BCQ, Ne = 20
(b)
Figure 13: Results on Medium dataset size of 1M for (a) BCQ representation and BCQ agent is trained for
1m timesteps, and evaluated on 10 episodes every 50k steps. At each of 6 uniformly distributed evaluation
checkpoints, we use the internal representation to compile DAC-MDPs. We then evaluate the DAC-MDPs for
Ne = 6. (b) Final BCQ policy along with the corresponding performance of DAC-BCQ for different values of
Ne. Runs averaged over 3 runs, error bars so the 95% conÔ¨Ådence interval.
Additionally we also run the experiment on dataset size 1M at different evaluation checkpoints as
done in the main paper. We trained ofÔ¨Çine BCQ agent on the dataset for 1M iterations. This
allows us to compare to these baselines as well as use the BCQ latent state representations for
19

Published as a conference paper at ICLR 2021
DAC-MDP construction. The performance of the corresponding BCQ policies is evaluated using
10 test episodes every 50K iterations. At some select position of BCQ evaluation iteration, we
constructed DAC-MDPs using the latent state representation at that point. In particular, this Ô¨Årst
experiment considers the ofÔ¨Çine RL setting where Ne = 6, meaning that we can evaluate 6 poli-
cies(using 10 episodes each) corresponding to 6 different DAC-MDP parameter settings comprised
of the combinations of k = 5,C ‚àà1, 100, 1M, andkœÄ ‚àà11, 51. For each representation the best
performing DAC-MDP setting at each point is then recorded. The entire 1M iteration protocol was
repeated 3 times and Figure 13a show the averaged curves with 90%conÔ¨Ådence intervals. Figure 13b
investigates the performance of Ô¨Ånal iteration for different values of Ne. All hyperparameters and
normalization were selected as in the 100K experiment. We see that in the DAC-MDPs can perform
as good as BCQ or better in most of the evaluation checkpoints even for the larger dataset size and
training iterations.
A.6
ADDITIONAL 3D NAVIGATION EXPERIMENTS
We also compare DAC-MDPs performance against DQN and BCQ baselines for Ô¨Årst person 3D
navigation Domain. In all scenarios the agent has three actions: Move Forward,Turn Right, and
Turn Left and a maximum episode time steps of 100. The raw observations are 84x84 RGB images.
We use the same settings for DAC-MDP as described in the main paper. We use the same baselines
DQN and BCQ from Atari experiments with different set of hyperparameters as detailed in Appendix
B.2
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
1.0
0.5
0.0
0.5
1.0
Avg Rewards
Simple Room
DQN
BCQ
DAC-RND
(a)
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
1.0
0.5
0.0
0.5
1.0
Avg Rewards
Simple Room
DQN [LF]
DQN [RLF]
DAC-RND [LF]
DAC-RND [RLF]
(b)
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
1.0
0.5
0.0
0.5
1.0
Avg Rewards
Simple Room
BCQ [LF]
BCQ [RLF]
DAC-RND [LF]
DAC-RND [RLF]
(c)
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
0.0
0.2
0.4
0.6
0.8
1.0
Avg Rewards
Box and Pillar Room, [max episode length = 20]
DQN
BCQ
DAC-RND
(d)
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
1.0
0.5
0.0
0.5
1.0
1.5
Avg Rewards
Box and Pillar Room  [max episode length = 100]
DQN
BCQ
DAC-RND
(e)
0.2
0.4
0.6
0.8
1.0
Training Steps 1e5
1.0
0.5
0.0
0.5
1.0
Avg Rewards
Tunnel Room
DQN
BCQ
DAC-RND (Opt)
DAC-RND (Safe)
(f)
Figure 14: Comparison of DAC-MDPs using randomly initialized representation networks (DAC-RND) with
baselines DQN and BCQ for 3D navigation Domains. (a) Simple Room Domain, (b) Simple Room Domain:
DQN vs DAC-RND on right actuator failure[LF] (c) BCQ vs DAC-RND on right actuator failure[LF](d) Box
and Pillar Room: short horizon, maximum episode length of 20, (e) Box and Pillar Room: Long horizon, max-
imum episode lengths of 100, (f) Tunnel Room. Rewards across the rooms are clipped at (-1,2) for normalized
results.
Case 1: Adapting to ModiÔ¨Åed Action Space. Here an agent is spawned in a random initial position
with a goal(blue box) in a Ô¨Åxed position. We call this Simple Room (V œÄŒ≤ = ‚àí9.36)4. The agent
gets a -1 reward for bumping into the walls and a terminal reward of +1 for reaching the goal. We run
DQN and BCQ on the same dataset in the simple room. Ô¨Ågure this shows that DAC-MDP performs
better. For the simulated event of damaged actuator, we simply discard the damaged action from the
policy action space. While this allows for a zero-shot transfer learning in this scenario, the methods
4V œÄŒ≤ is the expected performance of the behavioral policy for the dataset.
20

Published as a conference paper at ICLR 2021
suffer with higher variance for DQN and fails to perform well in case of BCQ as pointed in Ô¨Ågure
this and this. The DAC-MDP however can adapt to these changes very robustly.
Case 2: Varying Horizons. We create a new room by adding a pillar in the simple room described
above. Additionally, the agent gets a small non-terminal reward of 0.02 for bumping into the pillar.
We call this BoxAndPillar Room (V œÄŒ≤ = ‚àí9.84). Depending on the placement of the agent and
the expected lifetime of the agent it may be better to go for the single +1 reward or to repeatedly get
the 0.02 reward. This expected lifetime can be simulated via the discount factor, which will result
in different agent behavior. As the baselines do not allow for zero shot transfer in this setting, we
retrain the DQN and BCQ agents for different discount factors: 0.95 for short horizon and 0.99 for
long horizon task. We then evaluate these policies on BoxAndPillar room with different maximum
episode steps. 20 and 100 for short and long horizon settings respectively.
Here as pointed in Ô¨Ågure this and this we see that the baselines cannot account for the long horizon
planning effectively and mostly choose to plan for the shorter horizon even with a larger discount
factor. hence it is clear that the discount factor do not translate to effective planning horizon in these
baselines. However, the DAC-MDP approach using a small discount factor (0.95) our short-term
agent achieves an average score of 0.91 learns to immediately get to the box along with the long-
term agent repeatedly collecting rewards from the pillar. Moreover DAC-MDP simply allows for
changing the discount factor used by VI and solving for the new policy in seconds for this secondary
objective.
Case 3: Robust/Safe Policies. The Tunnel room (V œÄŒ≤ = ‚àí25) adds a narrow passage to the box
with cones on the side. This room simulates the classic dilemma of whether to choose a risky or safe
path. The agent can risk getting a -10 reward by bumping into the cones or can optionally follow a
longer path around an obstacle to reach the goal. Even with a relatively large dataset, small errors
in the model can lead an agent along the risky path to occasionally hit a cone. We train a DQN
and BCQ agent using the same dataset and see that they do not perform as well as DAC-DQN in
the experiments. Moreover, it is not straight forward on how to add stochasticity in the baselines
to achieve a more robust policy. In contrast, we can easily Ô¨Ånd an optimal solution to a modiÔ¨Åed
DAC-MDP where at each step there is a 10% chance of taking a random action. This safe policy
avoids the riskier paths where a random ‚Äúslip‚Äù might lead to disaster and always chooses the safer
path around the obstacle, and also achieves an average score of 0.72.
(a)
(b)
(c)
(d)
Figure 15: Visualization of DAC-MDP policies for 3D Navigation Domain (a) Simple Room Solid arrow: pol-
icy rollout of standard DAC-MDP. Dotted arrow: policy rollout of modiÔ¨Åed DAC-MDP ;Right Turn Penalized .
(b) Simple Room Solid arrow: policy rollout of standard DAC-MDP. Dotted arrow: policy rollout of modiÔ¨Åed
DAC-MDP; Left Turn Penalized . (c) Box and Pillar Room Dotted Arrow: policy rollout of DAC-MDP solved
with small discount factor [short-term planning]. Solid Arrow: Policy rollout of DAC-MDP solved with Large
Discount Factor [long-term planning] (d) Tunnel Room Dotted Arrow: Policy rollout of standard DAC-MDP.
No stochasticity Introduced. [optimal policy]. Solid Arrow: Policy rollout of modiÔ¨Åed DAC-MDP with added
stochasticity in dynamics. [safe policy]
Hence we see that typical Deep RL methods like DQN and BCQ do not excel for secondary objec-
tives. There are settings such as failed actuator where they can perform zero-shot transfer learning
but fail to give any guarantees, moreover they can fail sometimes as shown empirically. In cases
where zero-shot transfer is not possible such as different horizons, they can be retrained for these
secondary objectives, but this comes at a huge cost (if and when they succeed) as compared to just
solving for a new objective in DAC-MDP as shown in the computational analysis in the main paper.
21

Published as a conference paper at ICLR 2021
Furthermore, these baseline methods are not able to tackle objectives such as robustness in any triv-
ial manner. In contrast DAC-MDP approaches have a Ô¨Åne-tuned control over the learned MDP and
its planning parameters.
A.7
SCALING VALUE ITERATION WITH GPUS
Value iteration (VI) (Bellman & Kneale, 1958) successively approximates the value function, start-
ing from an arbitrary initial estimate by turning the Bellman optimality equation into an update rule.
While VI is simple and able to calculate the exact solution for any MDP, one of the major disad-
vantages is that they are computationally inefÔ¨Åcient. A large number of bellman backups has to be
computed for each state before convergence and the memory required grows exponentially with the
number of states in the MDPs. Hence, it is only natural to optimize value iteration using parallel
computing platforms such as GPUs. ¬¥or J¬¥ohannsson (2009) was one of the Ô¨Årst to introduce a GPU
based MDP solver. In particular, they introduce two algorithms: Block Divided Iteration and Result
Divided Iteration. However, there have been works leveraging GPU optimized value iteration for
speciÔ¨Åc domains. (eg. Ruiz & Hern¬¥andez (2015), Wu et al. (2016)), there does not exist a standard
public library for GPU optimized value iteration. To facilitate this, we implement a version that is a
hybrid between the Block Divided Iteration and Result Divided Iteration approaches using a simple
CUDA kernel as described in pseudo-code 15.
Pseudocode 1 GPU Value Iteration Kernel
1: procedure BELLMANBACKUP(‚àóTP , ‚àóTI, ‚àóR, ‚àóV, ‚àóV ‚Ä≤, ‚àóQ‚Ä≤, ‚àóŒ¥)
2:
i ‚Üêget thread id()
3:
v, vmax ‚Üê0, 0
4:
for j ‚ààrange(A) do
5:
for k ‚ààrange(kb) do
‚ñ∑kb is initialized externally as per the MDP build parameter kb
6:
Pss‚Ä≤ ‚ÜêTP [i, j, k]
7:
Is‚Ä≤ ‚ÜêTI[i, j, k]
8:
v ‚Üêv + Pss‚Ä≤V [Is‚Ä≤]
9:
end for
10:
Q‚Ä≤[i, j] ‚Üêv
11:
vmax ‚Üêmax(v, vmax)
12:
end for
13:
V ‚Ä≤[i] ‚Üêvmax
14:
Œ¥[i] = abs(V [i] ‚àíV ‚Ä≤[i])
15: end procedure
Pseudocode 2 GPU Value Iteration Function
1: procedure VALUEITERATION(tranDict, rewardDict, Œ¥min)
2:
Tp, TI, R = get sparse representation(tranDict, rewardDict)
3:
Tp, TI, R = allocate gpu memory(Tp, TI, R)
4:
V, Q‚Ä≤, Œ¥ = allocate gpu memory for value vectors()
5:
while min(Œ¥) > Œ¥min do
6:
V ‚Ä≤ = allocate gpu memory(V.size())
7:
RunGPUKernel(Tp, TI, R, V, V ‚Ä≤, Q‚Ä≤, Œ¥)
8:
V = V ‚Ä≤)
9:
Œ¥[i] = abs(V [i] ‚àíV ‚Ä≤[i])
10:
release memory for(V ‚Ä≤)
11:
end while
12: end procedure
For a compact representation of the Transition matrix, we use a list of lists as our sparse represen-
tations. Here the Transition Matrix is divided into two matrices, one that holds the index of next
states with a non-zero transition probability (TI) and the other, which holds the actual transition
5Note that the different versions of GPU implementation does not affect the convergence properties of Value
Iteration as each approach is still carrying out exact value iteration.
22

Published as a conference paper at ICLR 2021
probability(TP ). Each thread takes a single row from these two matrices and the Reward matrix to
compute each state-action pair‚Äôs Q values and the new value of a state. The value vector is shared
among the threads and synchronized after each bellman backup operation.
To benchmark the GPU implementation‚Äôs performance, we compare its run-time with a standard
serial implementation of Value iteration across MDPs of varying sizes. These MDPs are DAC-MDPs
generated by different samples from a large pool of datasets with continuous state vectors. The serial
implementation is run on an Intel Xeon processor and does not use any CPU multi-processing. We
plot the relative performance gain across different GPUs with varying CUDA cores. We consider 3
GPUs, namely, GTX 1080ti, RTX 8000, and Tesla V100, each with a CUDA core count of 3584,
4608 and 6912, respectively. The GPU optimized implementation provides anywhere between 20-
1000X boost in solving speed over its serial counterpart, as shown in Figure 16. Currently, our
implementation of VI can solve MDPs with a million states less than 30 seconds.
1k
10k
100k
1M
MDP Size
10
100
1000
Speed Up
GPU Speedup (Intel CPU baseline)
GTX 1080ti
Quadro RTX 8000
Tesla V100
Figure 16: Compares the performance results for serial VI solvers with its GPU optimized implementation.
The serial VI solver is benchmarked using Intel Xeon CPU. We plot the performance gain over different MDP
sizes.
23

Published as a conference paper at ICLR 2021
B
EXPERIMENTAL DETAILS
B.1
ATARI PREPROCESSING
The Atari 2600 environment is processed in teh same manner as previous work (Mnih et al., 2015;
Machado et al., 2018) and we use consistent preprocesssing across all tasks and algorithms.
Table 1: Atari pre-processing details
Name
Value
Sticky actions
Yes
Sticky action probability
0.25
Grey-scaling
True
Observation down-sampling
(84,84)
Frames stacked
4
Frameskip (Action repetitions)
4
Reward clipping
[-1,1]
Terminal condition
Game Over
Max frames per episode
108K
B.2
ARCHITECTURE AND HYPER-PARAMETERS
Same architecture and hyperparameters were used as in Fujimoto et al. (2019) with slight modiÔ¨Åca-
tions in the architecture.
Table 2: Architecture used by each Network
Layer
Number of outputs
Other details
Input frame size
(4x84x84)
‚àí
Downscale convolution 1
12800
kernel 8x8, depth 32, stride 4x4
Downscale convolution 2
5184
kernel 4x4, depth 32, stride 2x2
Downscale convolution 3
3136
kernel 3x3, depth 32, stride 1x1
Hidden Linear Layer 1
512
-
Hidden Linear Layer 2
16
-
Output Layer
|A|
-
Table 3: All Hyperparameters for DQN and BCQ [Atari]
Hyper-parameter
Value
Network optimizer
Adam Kingma & Ba (2015)
Learning rate
0.0000625
Adam œµ
0.00015
Discount Œ≥
0.99
Mini-batch size
32
Target network update frequency
8k training updates
Evaluation œµ
0.001
Threshold œÑ (BCQ)
0.3
Table 4: All Hyperparameters for DQN and BCQ [3D Nav]
Hyper-parameter
Value
Network optimizer
Adam Kingma & Ba (2015)
Learning rate
0.0003
Discount Œ≥
0.99/0.95
Mini-batch size
64
Target network update frequency
100 training updates
Evaluation œµ
0.001
Threshold œÑ (BCQ)
0.3
Table 5: All Hyperparameters for DQN ‚àó
24

Published as a conference paper at ICLR 2021
Hyper-parameter
Value
Replay buffer size
1 million
Training frequency
Every 4th time step
Warmup time steps
20k time steps
Initial œµ
1.0
Final œµ
0.01
œµ decay period
250k training iterations
25

