Published as a conference paper at ICLR 2021
COUPLED OSCILLATORY RECURRENT NEURAL NET-
WORK (CORNN): AN ACCURATE AND (GRADIENT)
STABLE ARCHITECTURE FOR LEARNING LONG TIME
DEPENDENCIES
T. Konstantin Rusch
Seminar for Applied Mathematics (SAM)
Department of Mathematics
ETH Zürich
Zürich, 8092, Switzerland
trusch@ethz.ch
Siddhartha Mishra
Seminar for Applied Mathematics (SAM)
Department of Mathematics
ETH Zürich
Zürich, 8092, Switzerland
smishra@ethz.ch
ABSTRACT
Circuits of biological neurons, such as in the functional parts of the brain can be
modeled as networks of coupled oscillators. Inspired by the ability of these systems
to express a rich set of outputs while keeping (gradients of) state variables bounded,
we propose a novel architecture for recurrent neural networks. Our proposed RNN
is based on a time-discretization of a system of second-order ordinary differential
equations, modeling networks of controlled nonlinear oscillators. We prove precise
bounds on the gradients of the hidden states, leading to the mitigation of the
exploding and vanishing gradient problem for this RNN. Experiments show that
the proposed RNN is comparable in performance to the state of the art on a variety
of benchmarks, demonstrating the potential of this architecture to provide stable
and accurate RNNs for processing complex sequential data.
1
INTRODUCTION
Recurrent neural networks (RNNs) have achieved tremendous success in a variety of tasks involving
sequential (time series) inputs and outputs, ranging from speech recognition to computer vision
and natural language processing, among others. However, it is well known that training RNNs to
process inputs over long time scales (input sequences) is notoriously hard on account of the so-called
exploding and vanishing gradient problem (EVGP) (Pascanu et al., 2013), which stems from the
fact that the well-established BPTT algorithm for training RNNs requires computing products of
gradients (Jacobians) of the underlying hidden states over very long time scales. Consequently, the
overall gradient can grow (to inﬁnity) or decay (to zero) exponentially fast with respect to the number
of recurrent interactions.
A variety of approaches have been suggested to mitigate the exploding and vanishing gradient problem.
These include adding gating mechanisms to the RNN in order to control the ﬂow of information
in the network, leading to architectures such as long short-term memory (LSTM) (Hochreiter &
Schmidhuber, 1997) and gated recurring units (GRU) (Cho et al., 2014), that can overcome the
vanishing gradient problem on account of the underlying additive structure. However, the gradients
might still explode and learning very long term dependencies remains a challenge (Li et al., 2018).
Another popular approach for handling the EVGP is to constrain the structure of underlying recurrent
weight matrices by requiring them to be orthogonal (unitary), leading to the so-called orthogonal
RNNs (Henaff et al., 2016; Arjovsky et al., 2016; Wisdom et al., 2016; Kerg et al., 2019) and references
therein. By construction, the resulting Jacobians have eigen- and singular-spectra with unit norm,
alleviating the EVGP. However as pointed out by Kerg et al. (2019), imposing such constraints on the
recurrent matrices may lead to a signiﬁcant loss of expressivity of the RNN resulting in inadequate
performance on realistic tasks.
In this article, we adopt a different approach, based on observation that coupled networks of controlled
non-linear forced and damped oscillators, that arise in many physical, engineering and biological
1

Published as a conference paper at ICLR 2021
systems, such as networks of biological neurons, do seem to ensure expressive representations while
constraining the dynamics of state variables and their gradients. This motivates us to propose a novel
architecture for RNNs, based on time-discretizations of second-order systems of non-linear ordinary
differential equations (ODEs) (1) that model coupled oscillators. Under veriﬁable hypotheses, we
are able to rigorously prove precise bounds on the hidden states of these RNNs and their gradients,
enabling a possible solution of the exploding and vanishing gradient problem, while demonstrating
through benchmark numerical experiments, that the resulting system still retains sufﬁcient expressivity,
i.e. ability to process complex inputs, with a competitive performance, with respect to the state of the
art, on a variety of sequential learning tasks.
2
THE PROPOSED RNN
Our proposed RNN is based on the following second-order system of ODEs,
y′′ = σ (Wy + Wy′ + Vu + b) −γy −ϵy′.
(1)
Here, t ∈[0, 1] is the (continuous) time variable, u = u(t) ∈Rd is the time-dependent input signal,
y = y(t) ∈Rm is the hidden state of the RNN with W, W ∈Rm×m, V ∈Rm×d are weight
matrices, b ∈Rm is the bias vector and 0 < γ, ϵ are parameters, representing oscillation frequency
and the amount of damping (friction) in the system, respectively. σ : R 7→R is the activation
function, set to σ(u) = tanh(u) here. By introducing the so-called velocity variable z = y′(t) ∈Rm,
we rewrite (1) as the ﬁrst-order system:
y′ = z,
z′ = σ (Wy + Wz + Vu + b) −γy −ϵz.
(2)
We ﬁx a timestep 0 < ∆t < 1 and deﬁne our proposed RNN hidden states at time tn = n∆t ∈[0, 1]
(while omitting the afﬁne output state) as the following IMEX (implicit-explicit) discretization of the
ﬁrst order system (2):
yn = yn−1 + ∆tzn,
zn = zn−1 + ∆tσ (Wyn−1 + Wzn−1 + Vun + b) −∆tγyn−1 −∆tϵz¯n,
(3)
with either ¯n = n or ¯n = n −1. Note that the only difference in the two versions of the RNN (3) lies
in the implicit (¯n = n) or explicit (¯n = n −1) treatment of the damping term −ϵz in (2), whereas
both versions retain the implicit treatment of the ﬁrst equation in (2).
Motivation and background.
To see that the underlying ODE (2) models a coupled network
of controlled forced and damped nonlinear oscillators, we start with the single neuron (scalar)
case by setting d = m = 1 in (1) and assume an identity activation function σ(x) = x. Setting
W = W = V = b = ϵ = 0 leads to the simple ODE, y′′ + γy = 0, which exactly models simple
harmonic motion with frequency γ, for instance that of a mass attached to a spring (Guckenheimer
& Holmes, 1990). Letting ϵ > 0 in (1) adds damping or friction to the system (Guckenheimer &
Holmes, 1990). Then, by introducing non-zero V in (1), we drive the system with a driving force
proportional to the input signal u(t). The parameters V, b modulate the effect of the driving force,
W controls the frequency of oscillations and W the amount of damping in the system. Finally, the
tanh activation mediates a non-linear response in the oscillator. In the coupled network (2) with
m > 1, each neuron updates its hidden state based on the input signal as well as information from
other neurons. The diagonal entries of W (and the scalar hyperparameter γ) control the frequency
whereas the diagonal entries of W (and the hyperparameter ϵ) determine the amount of damping for
each neuron, respectively, whereas the non-diagonal entries of these matrices modulate interactions
between neurons. Hence, given this behavior of the underlying ODE (2), we term the RNN (3) as a
coupled oscillatory Recurrent Neural Network (coRNN).
The dynamics of the ODE (2) (and the RNN (3)) for a single neuron are relatively straightforward.
As we illustrate in Fig. 6 of supplementary material SM§C, input signals drive the generation of
(superpositions of) oscillatory wave-forms, whose amplitude and (multiple) frequencies are controlled
by the tunable parameters W, W, V, b. Adding a tanh activation does not change these dynamics
much. This is in contrast to truncating tanh to leading non-linear order by setting σ(x) = x −x3/3,
which yields a Dufﬁng type oscillator that is characterized by chaotic behavior (Guckenheimer &
Holmes, 1990). Adding interactions between neurons leads to further accentuation of this generation
of superposed wave forms (see Fig. 6 in SM§C) and even with very simple network topologies, one
2

Published as a conference paper at ICLR 2021
sees the emergence of non-trivial non-oscillatory hidden states from oscillatory inputs. In practice,
a network of a large number of neurons is used and can lead to extremely rich global dynamics.
Hence, we argue that the ability of a network of (forced, driven) oscillators to access a very rich set
of output states may lead to high expressivity of the system, allowing it to approximate outputs from
complicated sequential inputs.
Oscillator networks are ubiquitous in nature and in engineering systems (Guckenheimer & Holmes,
1990; Strogatz, 2015) with canonical examples being pendulums (classical mechanics), business
cycles (economics), heartbeat (biology) for single oscillators and electrical circuits for networks of
oscillators. Our motivating examples arise in neurobiology, where individual biological neurons can
be viewed as oscillators with periodic spiking and ﬁring of the action potential. Moreover, functional
circuits of the brain, such as cortical columns and prefrontal-striatal-hippocampal circuits, are being
increasingly interpreted by networks of oscillatory neurons, see Stiefel & Ermentrout (2016) for an
overview. Following well-established paths in machine learning, such as for convolutional neural
networks (LeCun et al., 2015), our focus here is to abstract the essence of functional brain circuits
being networks of oscillators and design an RNN based on much simpler mechanistic systems, such
as those modeled by (2), while ignoring the complicated biological details of neural function.
Related work.
There is an increasing trend of basing RNN architectures on ODEs and dynamical
systems. These approaches can roughly be classiﬁed into two branches, namely RNNs based on
discretized ODEs and continuous-time RNNs. Examples of continuous-time approaches include
neural ODEs (Chen et al., 2018) with ODE-RNNs (Rubanova et al., 2019) as its recurrent extension
as well as E (2017) and references therein, to name just a few. We focus, however, in this article
on an ODE-inspired discrete-time RNN, as the proposed coRNN is derived from a discretization of
the ODE (1). A good example for a discrete-time ODE-based RNNs is the so-called anti-symmetric
RNN of Chang et al. (2019), where the RNN architecture is based on a stable ODE resulting from
a skew-symmetric hidden weight matrix, thus constraining the stable (gradient) dynamics of the
network. This approach has much in common with previously mentioned unitary/orthogonal/non-
normal RNNs in constraining the structure of the hidden-to-hidden layer weight matrices. However,
adding such strong constraints might reduce expressivity of the resulting RNN and might lead to
inadequate performance on complex tasks. In contrast to these approaches, our proposed coRNN
does not explicitly constrain the weight matrices but relies on the dynamics of the underlying ODE
(and the IMEX discretization (3)), to provide gradient stability. Moreover, no gating mechanisms as
in LSTMs/GRUs are used in the current version of coRNN. There is also an increasing interest in
designing hybrid methods, which use a discretization of an ODE (in particular a Hamiltonian system)
in order to learn the continuous representation of the data, see for instance Greydanus et al. (2019);
Chen et al. (2020). Overall, our approach here differs from these papers in our use of networks of
oscillators to build the RNN.
3
RIGOROUS ANALYSIS OF THE PROPOSED RNN
An attractive feature of the underlying ODE system (2) lies in the fact that the resulting hidden
states (and their gradients) are bounded (see SM§D for precise statements and proofs). Hence, one
can expect that a suitable discretization of the ODE (2) that preserves these bounds will not have
exploding gradients. We claim that one such structure preserving discretization is given by the IMEX
discretization that results in the RNN (3) and proceed to derive bounds on this RNN below.
Following standard practice we set y(0) = z(0) = 0 and purely for the simplicity of exposition, we
set the control parameters, ϵ = γ = 1 and ¯n = n in (3) leading to,
yn
= yn−1 + ∆tzn,
zn
= zn−1
1+∆t +
∆t
1+∆tσ(An−1) −
∆t
1+∆tyn−1, An−1 := Wyn−1 + Wzn−1 + Vun + b.
(4)
Analogous results and proofs for the case where ¯n = n −1 and for general values of ϵ, γ are provided
in SM§F.
Bounds on the hidden states.
As with the underlying ODE (2), the hidden states of the RNN (3)
are bounded, i.e.
3

Published as a conference paper at ICLR 2021
Proposition 3.1 Let yn, zn be the hidden states of the RNN (4) for 1 ≤n ≤N, then the hidden
states satisfy the following (energy) bounds:
y⊤
n yn + z⊤
n zn ≤nm∆t = mtn ≤m.
(5)
The proof of the energy bound (5) is provided in SM§E.1 and a straightforward variant of the proof
(see SM§E.2) yields an estimate on the sensitivity of the hidden states to changing inputs. As with
the underlying ODE (see SM§D) , this bound rules out chaotic behavior of hidden states.
Bounds on hidden state gradients.
We train the RNN (3) to minimize the loss function,
E := 1
N
N
X
n=1
En,
En = 1
2∥yn −¯yn∥2
2,
(6)
with ¯y being the underlying ground truth (training data). During training, we compute gradients of
the loss function (6) with respect to the weights and biases Θ = [W, W, V, b], i.e.
∂E
∂θ = 1
N
N
X
n=1
∂En
∂θ ,
∀θ ∈Θ.
(7)
Proposition 3.2 Let yn, zn be the hidden states generated by the RNN (4). We assume that the time
step ∆t << 1 can be chosen such that,
max
∆t(1 + ∥W∥∞)
1 + ∆t
, ∆t∥W∥∞
1 + ∆t

= η ≤∆tr,
1
2 ≤r ≤1.
(8)
Denoting δ =
1
1+∆t, the gradient of the loss function E (6) with respect to any parameter θ ∈Θ is
bounded as,

∂E
∂θ
 ≤3
2
 m + ¯Y √m

,
(9)
with ¯Y = max
1≤n≤N ∥¯yn∥∞be a bound on the underlying training data.
Sketch of the proof. Denoting Xn = [yn, zn], we can apply the chain rule repeatedly (for instance as
in Pascanu et al. (2013)) to obtain,
∂En
∂θ =
X
1≤k≤n
∂En
∂Xn
∂Xn
∂Xk
∂+Xk
∂θ
|
{z
}
∂E(k)
n
∂θ
.
(10)
Here, the notation ∂+Xk
∂θ
refers to taking the partial derivative of Xk with respect to the parameter θ,
while keeping the other arguments constant. This quantity can be readily calculated from the structure
of the RNN (4) and is presented in the detailed proof provided in SM§E.3. From (6), we can directly
compute that ∂En
∂Xn = [yn −¯yn, 0] .
Repeated application of the chain rule and a direct calculation with (4) yields,
∂Xn
∂Xk
=
Y
k<i≤n
∂Xi
∂Xi−1
,
∂Xi
∂Xi−1
=

I + ∆tBi−1
∆tCi−1
Bi−1
Ci−1

,
(11)
where I is the identity matrix and
Bi−1 = δ∆t (diag(σ′(Ai−1))W −I) ,
Ci−1 = δ (I + ∆t diag(σ′(Ai−1))W) .
(12)
It is straightforward to calculate using the assumption (8) that ∥Bi−1∥∞< η and ∥Ci−1∥∞≤η + δ.
Using the deﬁnitions of matrix norms and (8), we obtain:

∂Xi
∂Xi−1

∞
≤max (1 + ∆t(∥Bi−1∥∞+ ∥Ci−1∥∞), ∥Bi−1∥∞+ ∥Ci−1∥∞)
≤max (1 + ∆t(δ + 2η), δ + 2η) ≤1 + 3∆tr.
(13)
4

Published as a conference paper at ICLR 2021
Therefore, using (11), we have

∂Xn
∂Xk

∞
≤
Y
k<i≤n

∂Xi
∂Xi−1

∞
≤(1 + 3∆tr)n−k ≈1 + 3(n −k)∆tr.
(14)
Note that we have used an expansion around 1 and neglected terms of O(∆t2r) as ∆t << 1. We
remark that the bound (13) is the crux of our argument about gradient control as we see from the
structure of the RNN that the recurrent matrices have close to unit norm. The detailed proof is
presented in SM§E.3. As the entire gradient of the loss function (6), with respect to the weights and
biases of the network, is bounded above in (9), the exploding gradient problem is mitigated for this
RNN.
On the vanishing gradient problem.
The vanishing gradient problem (Pascanu et al., 2013) arises
if
 ∂E(k)
n
∂θ
, deﬁned in (10), →0 exponentially fast in k, for k << n (long-term dependencies). In that
case, the RNN does not have long-term memory, as the contribution of the k-th hidden state to error
at time step tn is inﬁnitesimally small. We already see from (14) that
 ∂Xn
∂Xk

∞≈1 (independently
of k). Thus, we should not expect the products in (10) to decay fast. In fact, we will provide a much
more precise characterization of this gradient. To this end, we introduce the following order-notation,
β = O(α), for α, β ∈R+
if there exists constants C, C such that Cα ≤β ≤Cα.
M = O(α), for M ∈Rd1×d2, α ∈R+
if there exists constant C such that ∥M∥≤Cα.
(15)
For simplicity of notation, we will also set ¯yn = un ≡0, for all n, b = 0 and r = 1 in (8) and we
will only consider θ = Wi,j for some 1 ≤i, j ≤m in the following proposition.
Proposition 3.3 Let yn be the hidden states generated by the RNN (4). Under the assumption that
yi
n = O(√tn), for all 1 ≤i ≤m and (8), the gradient for long-term dependencies satisﬁes,
∂E(k)
n
∂θ
= O

ˆcδ∆t
3
2

+ O

ˆcδ(1 + δ)∆t
5
2

+ O(∆t3), ˆc = sech2 √
k∆t(1 + ∆t)

, k << n.
(16)
This precise bound (16) on the gradient shows that although the gradient can be small, i.e O(∆t
3
2 ), it
is in fact independent of k, ensuring that long-term dependencies contribute to gradients at much later
steps and mitigating the vanishing gradient problem. The detailed proof is presented in SM§E.5.
Summarizing, we see that the RNN (3) indeed satisﬁed similar bounds to the underlying ODE (2)
that resulted in upper bounds on the hidden states and its gradients. However, the lower bound on
the gradient (16) is due to the speciﬁc choice of this discretization and does not appear to have a
continuous analogue, making the speciﬁc choice of discretization of (2) crucial for mitigating the
vanishing gradient problem.
4
EXPERIMENTS
We present results on a variety of learning tasks with coRNN (3) with ¯n = n −1, as this version
resulted in marginally better performance than the version with ¯n = n. Details of the training
procedure for each experiment can be found in SM§B. We wish to clarify here that we use a
straightforward hyperparameter tuning protocol based on a validation set and do not use additional
performance enhancing tools, such as dropout (Srivastava et al., 2014), gradient clipping (Pascanu
et al., 2013) or batch normalization (Ioffe & Szegedy, 2015), which might further improve the
performance of coRNNs.
Adding problem.
We start with the well-known adding problem (Hochreiter & Schmidhuber,
1997), proposed to test the ability of an RNN to learn (very) long-term dependencies. The input is
a two-dimensional sequence of length T, with the ﬁrst dimension consisting of random numbers
drawn from U([0, 1]) and with two non-zero entries (both set to 1) in the second dimension, chosen at
random locations, but one each in both halves of the sequence. The output is the sum of two numbers
5

Published as a conference paper at ICLR 2021
0
100
200
300
400
500
Training steps (hundreds)
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
MSE
T = 500
0
100
200
300
400
500
Training steps (hundreds)
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
MSE
T = 2000
coRNN
expRNN
FastRNN
anti.sym. RNN
tanh RNN
Baseline
0
100
200
300
400
500
Training steps (hundreds)
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
0.200
MSE
T = 5000
Figure 1: Results of the adding problem for coRNN, expRNN, FastRNN, anti.sym. RNN and tanh
RNN based on three different sequence lengths T, i.e. T = 500, T = 2000 and T = 5000.
of the ﬁrst dimension at positions, corresponding to the two 1 entries in the second dimension. We
compare the proposed coRNN to three recently proposed RNNs, which were explicitly designed to
learn LTDs, namely the FastRNN (Kusupati et al., 2018), the antisymmetric (anti.sym.) RNN (Chang
et al., 2019) and the expRNN (Lezcano-Casado & Martínez-Rubio, 2019), and to a plain vanilla
tanh RNN, with the goal of beating the baseline mean square error (MSE) of 0.167 (which stems
from the variance of the baseline output 1). All methods have 128 hidden units (dimensionality of
the hidden state y) and the same training protocol is used in all cases. Fig. 1 shows the results for
different lengths T of the input sequences. We can see that while the tanh RNN is not able to beat the
baseline for any sequence length, the other methods successfully learn the adding task for T = 500.
However, in this case, coRNN converges signiﬁcantly faster and reaches a lower test MSE than other
tested methods. When setting the length to the much more challenging case of T = 2000, we see
that only coRNN and the expRNN beat the baseline. However, the expRNN fails to reach a desired
test MSE of 0.01 within training time. In order to further demonstrate the superiority of coRNN
over recently proposed RNN architectures for learning LTDs, we consider the adding problem for
T = 5000 and observe that coRNN converges very quickly even in this case, while expRNN fails to
consistently beat the baseline. We thus conclude that the coRNN mitigates the vanishing/exploding
gradient problem even for very long sequences.
Table 1: Test accuracies on sMNIST and psMNIST (we provide our own psMNIST result for the
FastGRNN, as no ofﬁcial result for this task has been published so far).
Model
sMNIST
psMNIST
# units
# params
uRNN (Arjovsky et al., 2016)
95.1%
91.4%
512
9k
LSTM (Helfrich et al., 2018)
98.9%
92.9%
256
270k
GRU (Chang et al., 2017)
99.1%
94.1%
256
200k
anti.sym. RNN (Chang et al., 2019)
98.0%
95.8%
128
10k
DTRIV∞(Casado, 2019)
99.0%
96.8%
512
137k
FastGRNN (Kusupati et al., 2018)
98.7%
94.8%
128
18k
coRNN (128 units)
99.3%
96.6%
128
34k
coRNN (256 units)
99.4%
97.3%
256
134k
Sequential (permuted) MNIST.
Sequential MNIST (sMNIST) (Le et al., 2015) is a benchmark
for RNNs, in which the model is required to classify an MNIST (LeCun et al., 1998) digit one pixel
at a time leading to a classiﬁcation task with a sequence length of T = 784. In permuted sequential
MNIST (psMNIST), a ﬁxed random permutation is applied in order to increase the time-delay
between interdependent pixels and to make the problem harder. In Table 1, we compare the test
accuracy for coRNN on sMNIST and psMNIST with recently published best case results for other
recurrent models, which were explicitly designed to solve long-term dependencies together with
baselines corresponding to gated and unitary RNNs. To the best of our knowledge the proposed
coRNN outperforms all single-layer recurrent architectures, published in the literature, for both the
sMNIST and psMNIST. Moreover in Fig. 2, we present the performance (with respect to number of
epochs) of different RNN architectures for psMNIST with the same ﬁxed random permutation and the
6

Published as a conference paper at ICLR 2021
same number of hidden units, i.e. 128. As seen from this ﬁgure, coRNN clearly outperforms the other
architectures, some of which were explicitly designed to learn LTDs, handily for this permutation.
0
20
40
60
80
epoch
0.5
0.6
0.7
0.8
0.9
1.0
test accuracy
coRNN
DTRIV∞
FastGRNN
anti.sym. RNN
LSTM
Figure 2: Performance on psM-
NIST for different models, all
with 128 hidden units and the
same ﬁxed random permutation.
10000
20000
30000
40000
50000
Learning steps
0.00
0.02
0.04
0.06
0.08
0.10
line: ∆t(1+∥W∥∞)
1+∆t
dashed: ∆t∥W∥∞
1+∆t
η = ∆t
1
2
adding
noisy CIFAR
psMNIST
sMNIST
Figure 3: Weight assumptions
(8), with r = 1
2, evaluated dur-
ing training for all LTD experi-
ments (mean and standard devia-
tion of 10 different runs for each
task).
1.00
1.25
1.50
1.75
γ
6
8
10
12
14
16
ϵ
55.0
55.5
56.0
56.5
57.0
57.5
58.0
58.5
59.0
59.5
Test accuracy in %
Figure 4: Ablation study on the
hyperparameters ϵ, γ in (3) us-
ing the noise padded CIFAR-10
experiment.
Noise padded CIFAR-10.
Another challenging test problem for learning LTDs is the recently
proposed noise padded CIFAR-10 experiment by Chang et al. (2019), in which CIFAR-10 data points
(Krizhevsky et al., 2009) are fed to the RNN row-wise and ﬂattened along the channels resulting in
sequences of length 32. To test the long term memory, entries of uniform random numbers are added
such that the resulting sequences have a length of 1000, i.e. the last 968 entries of each sequence are
only noise to distract the network. Table 2 shows the result for coRNN together with other recently
published best case results. We observe that coRNN readily outperforms other RNN architectures on
this benchmark, while requiring only 128 hidden units.
Table 2: Test accuracies on noise padded CIFAR-10.
Model
test accuracy
# units
# params
LSTM (Kag et al., 2020)
11.6%
128
64k
Incremental RNN (Kag et al., 2020)
54.5%
128
12k
FastRNN (Kag et al., 2020)
45.8%
128
16k
anti.sym. RNN (Chang et al., 2019)
48.3%
256
36k
Gated anti.sym. RNN (Chang et al., 2019)
54.7%
256
37k
Lipschitz RNN (Erichson et al., 2020)
55.2%
256
134k
coRNN
59.0%
128
46k
Human activity recognition.
This experiment is based on the human activity recognition data set
provided by Anguita et al. (2012). The data set is a collection of tracked human activities, which were
measured by an accelerometer and gyroscope on a Samsung Galaxy S3 smartphone. Six activities
were binarized to obtain two merged classes {Sitting, Laying, Walking_Upstairs} and {Standing,
Walking, Walking_Downstairs}, leading to the HAR-2 data set, which was ﬁrst proposed in Kusupati
et al. (2018). Table 3 shows the result for coRNN together with other very recently published best
case results on the same data set. We can see that coRNN readily outperforms all other methods. We
also ran this experiment on a tiny coRNN with very few parameters, i.e. only 1k. We can see that
even in this case, the tiny coRNN beats all baselines. We thus conclude that coRNN can efﬁciently
be used on resource-constrained IoT micro-controllers.
IMDB sentiment analysis.
The IMDB data set (Maas et al., 2011) is a collection of 50k movie
reviews, where 25k reviews are used for training (with 7.5k of these reviews used for validating) and
25k reviews are used for testing. The aim of this binary sentiment classiﬁcation task is to decide
whether a movie review is positive or negative. We follow the standard procedure by initializing
the word embedding with pretrained 100d GloVe (Pennington et al., 2014) vectors and restrict the
7

Published as a conference paper at ICLR 2021
Table 3: Test accuracies on HAR-2.
Model
test accuracy
# units
# params
GRU (Kusupati et al., 2018)
93.6%
75
19k
LSTM (Kag et al., 2020)
93.7%
64
16k
FastRNN (Kusupati et al., 2018)
94.5%
80
7k
FastGRNN (Kusupati et al., 2018)
95.6%
80
7k
anti.sym. RNN (Kag et al., 2020)
93.2%
120
8k
incremental RNN (Kag et al., 2020)
96.3%
64
4k
coRNN
97.2%
64
9k
tiny coRNN
96.5%
20
1k
dictionary to 25k words. Table 4 shows the results for coRNN and other recently published models,
which are trained similarly and have the same number of hidden units, i.e. 128. We can see that
coRNN compares favorable with gated baselines (which are known to perform very well on this task),
while at the same time requiring signiﬁcantly less parameters.
Table 4: Test accuracies on IMDB.
Model
test accuracy
# units
# params
LSTM (Campos et al., 2018)
86.8%
128
220k
Skip LSTM(Campos et al., 2018)
86.6%
128
220k
GRU (Campos et al., 2018)
86.2%
128
164k
Skip GRU (Campos et al., 2018)
86.6%
128
164k
ReLU GRU (Dey & Salemt, 2017)
84.8%
128
99k
coRNN
87.4%
128
46k
Further experimental results.
To shed further light on the performance of coRNN, we consider
the following issues. First, the theory suggested that coRNN mitigates the exploding/vanishing
gradient problem as long as the assumptions (8) on the time step ∆t and weight matrices W, W hold.
Clearly one can choose a suitable ∆t to enforce (8) before training, but do these assumptions remain
valid during training? In SM§E.4, we argue, based on worst-case estimates, that the assumptions
will remain valid for possibly a large number of training steps. More pertinently, we can verify
experimentally that (8) holds during training. This is demonstrated in Fig. 3, where we show that (8)
holds for all LTD tasks during training. Thus, the presented theory applies and one can expect control
over hidden state gradients with coRNN. Next, we recall that the frequency parameter γ and damping
parameter ϵ play a role for coRNNs (see SM§F for the theoretical dependence and Table 8 for best
performing values of ϵ, γ for each numerical experiment within the range considered in Table 7).
How sensitive is the performance of coRNN to the choice of these 2 parameters? To investigate
this dependence, we focus on the noise padded CIFAR-10 experiment and show the results of an
ablation study in Fig. 4, where the test accuracy for different coRNNs based on a two dimensional
hyperparameter grid (ϵ, γ) ∈[0.8, 1.8] × [5.7, 17, 7] (i.e., sufﬁciently large intervals around the best
performing values of ϵ, γ from Table 8) is plotted. We observe from the ﬁgure that although there
are reductions in test accuracy for non-optimal values of (ϵ, γ), there is no large variation and the
performance is rather robust with respect to these hyperparameters. Finally, note that we follow
standard practice and present best reported results with coRNN as well as other competing RNNs in
order to compare the relative performance. However, it is natural to investigate the dependence of
these best results on the random initial (before training) values of the weight matrices. To this end, in
Table 5 of SM, we report the mean and standard deviation (over 10 retrainings) of the test accuracy
with coRNN on various learning tasks and ﬁnd that the mean value is comparable to the best reported
value, with low standard deviations. This indicates further robustness of the performance of coRNNs.
8

Published as a conference paper at ICLR 2021
Table 5: Distributional information (mean and standard deviation) on the results for each classiﬁcation
experiment presented in the paper based on 10 re-trainings of the best performing coRNN using
random initialization of the trainable parameters.
Experiment
Mean
Standard deviation
sMNIST (256 units)
99.17%
0.07%
psMNIST (256 units)
96.10%
1.20%
Noise padded CIFAR-10
58.56%
0.35%
HAR-2 (64 units)
96.01%
0.53%
IMDB
86.65%
0.31%
5
DISCUSSION
Inspired by many models in physics, biology and engineering, we proposed a novel RNN architecture
(3) based on a model (1) of a network of controlled forced and damped oscillators. For this RNN, we
rigorously showed that under veriﬁable hypotheses on the time step and weight matrices, the hidden
states are bounded (5) and obtained precise bounds on the gradients (Jacobians) of the hidden states,
(9) and (16). Thus by design, this architecture can mitigate the exploding and vanishing gradient
problem (EVGP) for RNNs. We present a series of numerical experiments that include sequential
image classiﬁcation, activity recognition and sentiment analysis, to demonstrate that the proposed
coRNN keeps hidden states and their gradients under control, while retaining sufﬁcient expressivity
to perform complex tasks. Thus, we provide a novel and promising strategy for designing RNN
architectures that are motivated by the functioning of natural systems, have rigorous bounds on hidden
state gradients and are robust, accurate, straightforward to train and cheap to evaluate.
This work can be extended in different directions. For instance in this article, we have mainly focused
on the learning of tasks with long-term dependencies and observed that coRNNs are comparable
in performance to the best published results in the literature. Given that coRNNs are built with
networks of oscillators, it is natural to expect that they will perform very well on tasks with oscillatory
inputs/outputs, such as the time series analysis of high-resolution biomedical data, for instance
EEG (electroencephalography) and EMG (electromyography) data and seismic activity data from
geoscience. This will be pursued in a follow-up article. Similarly, applications of coRNN to language
modeling will be covered in future work.
However, it is essential to point out that coRNNs might not be suitable for every learning task involving
sequential inputs/outputs. As a concrete example, we consider the problem of predicting time series
corresponding to a chaotic dynamical system. We recall that by construction, the underlying ODE (2)
(and the discretization (3)) do not allow for super-linear (in time) separation of trajectories for nearby
inputs. Thus, we cannot expect that coRNNs will be effective at predicting chaotic time series and it
is indeed investigated and demonstrated for a Lorenz-96 ODE in SM§A, where we observe that the
coRNN is outperformed by LSTMs in the chaotic regime.
Our main theoretical focus in this paper was to demonstrate the possible mitigation of the exploding
and vanishing gradient problem. On the other hand, we only provided some heuristics and numerical
evidence on why the proposed RNN still has sufﬁcient expressivity. A priori, it is natural to think
that the proposed RNN architecture might introduce a strong bias towards oscillatory functions.
However, as we argue in SM§C, the proposed coRNN can be signiﬁcantly more expressive, as the
damping, forcing and coupling of several oscillators modulates nonlinear response to yield a very
rich and diverse set of output states. This is also evidenced by the ability of coRNNs to deal with
many tasks in our numerical experiments, which do not have an explicit oscillatory structure. This
sets the stage for a rigorous investigation of universality of the proposed coRNN architecture, as in
the case of echo state networks in Grigoryeva & Ortega (2018). A possible approach would be to
leverage the ability of the proposed RNN to convert general inputs into a rich set of superpositions of
harmonics (oscillatory wave forms). Moreover, the proposed RNN was based on the simplest model
of coupled oscillators (1). Much more detailed models of oscillators are available, particularly those
that arise in the modeling of biological neurons, Stiefel & Ermentrout (2016) and references therein.
An interesting variant of our proposed RNN would be to base the RNN architecture on these more
elaborate models, resulting in analogues of the spiking neurons model of Maass (2001) for RNNs.
9

Published as a conference paper at ICLR 2021
REFERENCES
Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge L Reyes-Ortiz. Human
activity recognition on smartphones using a multiclass hardware-friendly support vector machine.
In International Workshop on Ambient Assisted Living, pp. 216–223. Springer, 2012.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, pp. 1120–1128, 2016.
Víctor Campos, Brendan Jou, Xavier Giró-i-Nieto, Jordi Torres, and Shih-Fu Chang. Skip RNN:
learning to skip state updates in recurrent neural networks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings, 2018.
Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. In Advances in
Neural Information Processing Systems, pp. 9154–9164, 2019.
Bo Chang, Minmin Chen, Eldad Haber, and Ed H. Chi. Antisymmetricrnn: A dynamical system view
on recurrent neural networks. In 7th International Conference on Learning Representations, ICLR
2019, New Orleans, LA, USA, May 6-9, 2019, 2019.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.
In Advances in Neural Information Processing Systems, pp. 77–87, 2017.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in Neural Information Processing Systems, pp. 6571–6583,
2018.
Zhengdao Chen, Jianyu Zhang, Martín Arjovsky, and Léon Bottou. Symplectic recurrent neural
networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020, 2020.
Kyunghyun Cho, B van Merrienboer, Caglar Gulcehre, F Bougares, H Schwenk, and Yoshua Bengio.
Learning phrase representations using rnn encoder-decoder for statistical machine translation. In
Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 2014.
Rahul Dey and Fathi M Salemt. Gate-variants of gated recurrent unit (gru) neural networks. In 2017
IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS), pp. 1597–1600.
IEEE, 2017.
Weinan E. A proposal on machine learning via dynamical systems. Commun. Math. Stat, 5:1–11,
2017.
N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, and Michael W Mahoney. Lipschitz
recurrent neural networks. arXiv preprint arXiv:2006.12070, 2020.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems, pp. 15379–15389, 2019.
Lyudmila Grigoryeva and Juan-Pablo Ortega. Echo state networks are universal. Neural Networks,
108:495 – 508, 2018. ISSN 0893-6080.
J. Guckenheimer and P. Holmes. Nonlinear oscillations, dynamical systems, and bifurcations of
vector ﬁelds. Springer Verlag, New York, 1990.
S. Shinomoto H. Sakaguchi and Y. Kuramoto. Local and global self-entrainment in oscillator lattices.
Progress of Theoretical Physics, 77:1005–1010, 1987.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pp. 1969–1978. PMLR,
2018.
10

Published as a conference paper at ICLR 2021
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd Interna-
tional Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research,
pp. 2034–2042, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine
Learning, ICML, volume 37 of JMLR Workshop and Conference Proceedings, pp. 448–456.
JMLR.org, 2015.
Anil Kag, Ziming Zhang, and Venkatesh Saligrama. Rnns incrementally evolving on an equilibrium
manifold: A panacea for vanishing and exploding gradients? In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.
Giancarlo Kerg, Kyle Goyette, Maximilian Puelma Touzel, Gauthier Gidel, Eugene Vorontsov,
Yoshua Bengio, and Guillaume Lajoie. Non-normal recurrent neural network (nnrnn): learning
long time dependencies while improving expressivity with transient dynamics. In Advances in
Neural Information Processing Systems, pp. 13591–13601, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Aditya Kusupati, Manish Singh, Kush Bhatia, Ashish Kumar, Prateek Jain, and Manik Varma.
Fastgrnn: A fast, accurate, stable and tiny kilobyte sized gated recurrent neural network. In
Advances in Neural Information Processing Systems, pp. 9017–9028, 2018.
Thomas Laurent and James von Brecht. A recurrent neural network without chaos. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net, 2017.
Quoc V Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of
rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436–444, 2015.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Mario Lezcano-Casado and David Martínez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. volume 97 of Proceedings of
Machine Learning Research, pp. 3794–3803, Long Beach, California, USA, 09–15 Jun 2019.
PMLR.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5457–5466, 2018.
Edward N Lorenz. Predictability: A problem partly solved. In Proc. Seminar on Predictability,
volume 1, 1996.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, volume 1, pp.
142–150. Association for Computational Linguistics, 2011.
W. Maass. Fast sigmoidal networks via spiking neurons. Neural Computation, 9:279–304, 2001.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural
networks. In Proceedings of the 30th International Conference on International Conference on
Machine Learning, volume 28 of ICML’13, pp. III–1310–III–1318. JMLR.org, 2013.
11

Published as a conference paper at ICLR 2021
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 1532–1543, 2014.
Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. Latent ordinary differential equations
for irregularly-sampled time series. In Advances in Neural Information Processing Systems 32, pp.
5320–5330. 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overﬁtting. The Journal of Machine
Learning Research, 15(1):1929–1958, 2014.
K. M. Stiefel and G. B. Ermentrout. Neurons as oscillators. Journal of Neurophysiology, 116:
2950–2960, 2016.
S. Strogatz. Nonlinear Dynamics and Chaos. Westview, Boulder CO, 2015.
S. H. Strogatz. Exploring complex networks. Nature, 410:268–276, 2001.
A. T. Winfree. Biological rhythms and the behavior of populations of coupled oscillators. Journal of
Theoretical Biology, 16:15–42, 1967.
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In Advances in Neural Information Processing Systems, pp.
4880–4888, 2016.
12

Published as a conference paper at ICLR 2021
Supplementary Material for:
Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable
architecture for learning long time dependencies
A
CHAOTIC TIME-SERIES PREDICTION.
According to proposition E.1, coRNN does not exhibit chaotic behavior by design. While this
property is highly desirable for learning long-term dependencies (a slight perturbation of the input
should not result in an unbounded perturbation of the prediction), it impairs the performance on tasks,
where the network has to learn actual chaotic dynamics. To test this numerically, we consider the
following version of the Lorenz 96 system: (Lorenz, 1996):
x′
j = (xi+1 −xi−2)xi−1 −xi + F,
(17)
where xj ∈R for all j = 1, . . . , 5 and F is an external force controlling the level of chaos in the
system. Fig. 5 shows a trajectory of the system (17) plotted on the x1x2-plane for a small external
0.4
0.6
0.8
1.0
1.2
x1
0.6
0.8
1.0
1.2
x2
F = 0.9
0
5
10
x1
−5
0
5
10
x2
F = 8
Figure 5: Exemplary (x1, x2)-trajectories of the Lorenz 96 system (17) for different forces F.
force of F = 0.9 as well as a trajectory for a large external force of F = 8. We can see that while
for F = 0.9 the system does not exhibit chaotic behavior, the dynamics for F = 8 is already highly
chaotic.
Our task consists of predicting the 25-th next state of a trajectory of the system (17). We provide
128 trajectories of length 2000 for each of the training, validation and test sets. The trajectories are
generated by numerically solving the system (17) and evaluating it at 2000 equidistantly distributed
discrete time points with distance 0.01. The initial value for each trajectory is chosen uniform at
random on [F −1/2, F + 1/2]5 around the equilibrium point (F, . . . , F) of the system (17).
Since LSTMs are known to be able to produce chaotic dynamics, even in the autonomous (zero-entry)
case (Laurent & von Brecht, 2017), we expect them to perform signiﬁcantly better than coRNN if the
underlying system exhibits strong chaotic behavior. Table 6 shows the normalized root mean square
error (NRMSE) (RMSE divided by the root mean square of the target trajectory) on the test set for
coRNN and LSTM. We can see that indeed for the non-chaotic case of using an external force of
F = 0.9 LSTM and coRNN perform similarly. However, when the dynamics get chaotic (in this case
using an external force of F = 8), the LSTM clearly outperforms coRNN.
Table 6: Test NRMSE on the Lorenz 96 system (17) for coRNN and LSTM.
Model
F = 0.9
F = 8
# units
# params
LSTM
2.0 × 10−2
6.8 × 10−2
44
9k
coRNN
2.0 × 10−2
9.8 × 10−2
64
9k
13

Published as a conference paper at ICLR 2021
B
TRAINING DETAILS
The IMDB task was conducted on an NVIDIA GeForce GTX 1080 Ti GPU, while all other experi-
ments were run on a Intel Xeon E3-1585Lv5 CPU. The weights and biases of coRNN are randomly
initialized according to U(−
1
√nin ,
1
√nin ), where nin denotes the input dimension of each afﬁne
transformation. Instead of treating the parameters ∆t, γ and ϵ as ﬁxed hyperparameters, we can
also treat them as trainable network parameters by constraining ∆t to [0, 1] by using a sigmoidal
activation function and ϵ, γ > 0 by the use of ReLU for instance. However, in this case no major
difference in performance is obtained. The hyperparameters are optimized with a random search
algorithm, where the results of the best performing coRNN (based on the validation set) are reported.
The ranges of the hyperparameters for the random search algorithm are provided in Table 7. Table 8
shows the rounded hyperparameters of the best performing coRNN architecture resulting from the
random search algorithm for each learning task. We used 100 training epochs for sMNIST, psMNIST
and noise padded CIFAR-10 with additional 20 epochs in which the learning rate was reduced by a
factor of 10. Additionally, we used 100 epochs for the IMDB task and 250 epochs for the HAR-2
task.
Table 7: Setting for the hyperparameter optimization of coRNN. Intervals denote ranges of the
corresponding hyperparameter for the grid search algorithm, while ﬁxed numbers mean that no
hyperparameter optimization was done in this case.
task
learning rate
batch size
∆t
γ
ϵ
Adding
2 × 10−2
50
[10−2, 10−1]
[1, 100]
[1, 100]
sMNIST (nhid = 128)
[10−4, 10−1]
120
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
sMNIST (nhid = 256)
[10−4, 10−1]
120
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
psMNIST (nhid = 128)
[10−4, 10−1]
120
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
psMNIST (nhid = 256)
[10−4, 10−1]
120
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
Noise padded CIFAR-10
[10−4, 10−1]
100
[10−2, 10−1]
[1, 100]
[1, 100]
HAR-2
[10−4, 10−1]
64
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
IMDB
[10−4, 10−1]
64
[10−2, 10−1]
[10−1, 10]
[10−1, 10]
Table 8: Rounded hyperparameters of the best performing coRNN architecture.
task
learning rate
batch size
∆t
γ
ϵ
Adding (T = 5000)
2 × 10−2
50
1.6 × 10−2
94.5
9.5
sMNIST (nhid = 128)
3.5 × 10−3
120
5.3 × 10−2
1.7
4
sMNIST (nhid = 256)
2.1 × 10−3
120
4.2 × 10−2
2.7
4.7
psMNIST (nhid = 128)
3.7 × 10−3
120
8.3 × 10−2
1.3 × 10−1
4.1
psMNIST (nhid = 256)
5.4 × 10−3
120
7.6 × 10−2
4 × 10−1
8.0
Noise padded CIFAR-10
7.5 × 10−3
100
3.4 × 10−2
1.3
12.7
HAR-2
1.7 × 10−2
64
10−1
2 × 10−1
6.4
IMDB
6.0 × 10−4
64
5.4 × 10−2
4.9
4.8
C
HEURISTICS OF NETWORK FUNCTION
At the level of a single neuron, the dynamics of the RNN is relatively straightforward. We start with
the scalar case, i.e. m = d = 1 and illustrate different hidden states y as a function of time, for
different input signals, in Fig. 6. In this ﬁgure, we consider two different input signals, one oscillatory
signal given by u(t) = cos(4t) and another is a combination of step functions. First, we plot the
solution y(t) of (1), with the parameters V, b, W, W, ϵ = 0 and γ = 1. This simply corresponds to
the case of a simple harmonic oscillator (SHO) and the solution is described by a sine wave with the
natural frequency of the oscillator. Next, we introduce forcing by the input signal by setting V = 1
and the activation function is the identity σ(x) = x, leading to a forced damped oscillator (FDO). As
seen from Fig. 6, in the case of an oscillatory signal, this leads to a very minor change over the SHO,
14

Published as a conference paper at ICLR 2021
whereas for the step function, the change is only in the amplitude of the wave. Next, we add damping
by setting ϵ = 0.25 and see that the resulting forced damped oscillator (FDO), merely damps the
amplitude of the waves, without changing their frequency. Then, we consider the case of controlled
oscillator (CFDO) by setting W = −2, V = 2, b = 0.25, W = 0.75. As seen from Fig. 6, this leads
to a signiﬁcant change in the wave form in both cases. For the oscillatory input, the output is now a
superposition of many different forms, with different amplitudes and frequencies (phases) whereas for
the step function input, the phase is shifted. Already, we can see that for a linear controlled oscillator,
the output can be very complicated with the superposition of different waves. This holds true when
the activation function is set to σ(x) = tanh(x) (which is our proposed coRNN). For both inputs,
the output is a modulated version of the one generated by CFDO, expressed as a superposition of
waves. On the other hand, we also plot the solution with a Dufﬁng type oscillator (DUFF) by setting
the activation function as,
σ(x) = x −x3
3 .
(18)
In this case, the solution is very different from the CFDO and coRNN solutions and is heavily damped
(either in the output or its derivative). On the other hand, given the chaotic nature of the dynamical
system in this case, a slight change in the parameters led to the output blowing up. Thus, a bounded
nonlinearity seems essential in this context.
Coupling neurons together further accentuates this generation of superpositions of different wave-
forms, as seen even with the simplest case of a network with two neurons, shown in Fig. 6 (Bottom
row). For this ﬁgure, we consider two neurons, i.e m = 2 and two different network topologies.
For the ﬁrst, we only allow the ﬁrst neuron to inﬂuence the second one and not vice versa. This is
enforced with the weight matrices,
W =

−2
0
3
−2

,
W =

0.75
0
−1
0.75

.
We also set V = [2, 2]⊤, b = [0.25, 0.25]⊤. Note that in this case (we name as ORD (for ordered
connections)), the output of the ﬁrst neuron should be exactly the same as in the uncoupled (UC)
case, whereas there is a distinct change in the output of the second neuron and we see that the ﬁrst
neuron has modulated a sharp change in the resulting output wave form. It is well illustrated by the
emergence of an approximation to the step function (Bottom Right of Fig. 6), even though the input
signal is oscillatory.
Next, we consider the case of fully connected (FC) neurons by setting the weight matrices as,
W =

−2
1
3
−2

,
W =

0.75
0.3
−1
0.75

.
The resulting outputs for the ﬁrst neuron are now slightly different from the uncoupled case. On the
the other hand, the approximation of step function output for the second neuron is further accentuated.
Even these simple examples illustrate the functioning of a network of controlled oscillators well. The
input signal is converted into a superposition of waves with different frequencies and amplitudes, with
these quantities being controlled by the weights and biases in (1). Thus, very complicated outputs
can be generated by modulating the number, frequencies and amplitudes of the waves. In practice, a
network of a large number of neurons is used and can lead to extremely rich global dynamics, along
the lines of emergence of synchronization or bistable heterogeneous behavior seen in systems of
idealized oscillators and explained by their mean ﬁeld limit, see H. Sakaguchi & Kuramoto (1987);
Winfree (1967); Strogatz (2001). Thus, we argue that the ability of the network of (forced, driven)
oscillators to access a very rich set of output states can lead to high expressivity of the system. The
training process selects the weights that modulate frequencies, phases and amplitudes of individual
neurons and their interaction to guide the system to its target output.
D
BOUNDS ON THE DYNAMICS OF THE ORDINARY DIFFERENTIAL EQUATION
(1)
In this section, we present bounds that show how the continuous time dynamics of the ordinary differ-
ential equation (2), modeling non-linear damped and forced networks of oscillators, is constrained.
We start with the following estimate on the energy of the solutions of the system (2).
15

Published as a conference paper at ICLR 2021
0
1
2
3
4
5
6
7
8
9
10
TIME
-1
-0.5
0
0.5
1
0
1
2
3
4
5
6
7
8
9
10
TIME
-1
-0.5
0
0.5
1
SHO
FHO
FDO
CFDO
DUFF
coRNN
0
1
2
3
4
5
6
7
8
9
10
TIME
0
0.2
0.4
0.6
0.8
1
1.2
1.4
U
0
1
2
3
4
5
6
7
8
9
10
TIME
-1
-0.5
0
0.5
1
1.5
2
Y
SHO
FHO
FDO
CFDO
DUFF
coRNN
0
1
2
3
4
5
6
7
8
9
10
TIME
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Y1
UC
ORD
FC
0
1
2
3
4
5
6
7
8
9
10
TIME
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Y2
UC
ORD
FC
Figure 6: Illustration of the hidden state y of coRNN (3) with a scalar input signal u (Top, Middle,
Left) with one neuron with state y (Top and Middle, Right) and two neurons with states y1 (Bottom
left), and y2 (Bottom right), corresponding to scalar input signal, shown in Top Left. Legend is SHO
(simple harmonic oscillator), FHO (forced oscillator), FDO (forced and damped oscillator), CFDO
(controlled forced and damped oscillator), DUFF (Dufﬁng type) UC (Uncoupled), Ord (ordered
coupling) and FC (fully coupled). Legend explained in the text.
Proposition D.1 Let y(t), z(t) be the solutions of the ODE system (2) at any time t ∈[0, T] and
assume that the damping parameter ϵ ≥1
2 and the initial data for (2) is given by,
y(0) = z(0) ≡0.
Then, the solutions are bounded as,
y(t)⊤y(t) ≤mt
γ ,
z(t)⊤z(t) ≤mt,
∀t ∈(0, T].
(19)
To prove this proposition, we multiply the ﬁrst equation in (2) with y(t)⊤and the second equation in
(2) with 1
γ z(t)⊤to obtain,
d
dt
y(t)⊤y(t)
2
+ z(t)⊤z(t)
2γ

= z(t)⊤σ(A(t))
γ
−ϵ
γ z(t)⊤z(t),
(20)
with
A(t) = Wy(t) + Wz(t) + Vu(t) + b.
Using the elementary Cauchy’s inequality repeatedly in (20) results in,
d
dt
y(t)⊤y(t)
2
+ z(t)⊤z(t)
2γ

≤σ(A)⊤σ(A)
2γ
+ 1
γ
1
2 −ϵ

z⊤z
≤m
2γ
(as |σ| ≤1 and ϵ ≥1
2).
Integrating the above inequality over the time interval [0, t] and using the fact that the initial data are
y(0) = z(0) ≡0, we obtain the bounds (19).
The above proposition and estimate (19) clearly demonstrate that the dynamics of the network of
coupled non-linear oscillators (1) is bounded. The fact that the nonlinear activation function σ = tanh
is uniformly bounded in its arguments played a crucial role in deriving the energy bound (19). A
straightforward adaptation of this argument leads to the following proposition about the sensitivity of
the system to inputs,
16

Published as a conference paper at ICLR 2021
Proposition D.2 Let y(t), z(t) be the solutions of the ODE system (2) with respect to the input signal
u(t). Let ¯y(t), ¯z(t) be the solutions of the ODE system (2), but with respect to the input signal ¯u(t).
Assume that the damping parameter ϵ ≥1
2 and the initial data are given by,
y(0) = z(0) = ¯y(0) = ¯z(0) ≡0.
Then we have the following bound,
(y(t) −¯y(t))⊤(y(t) −¯y(t)) ≤4mt
γ ,
(z(t) −¯z(t))⊤(z(t) −¯z(t)) ≤4mt,
∀t ∈(0, T].
(21)
Thus from the bound (21), there can be atmost linear separation (in time) with respect to the
trajectories of the ODE (2) for different input signals. Hence, chaotic behavior, which is characterized
by the (super-)exponential separation of trajectories is ruled out by the structure of the ODE system
(2). Note that this property of the ODE system was primarily a result of the uniform boundedness of
the activation function σ. Using a different activation function such as ReLU might enable to obtain
an exponential separation of trajectories that is a prerequisite for a chaotic dynamical system.
D.1
GRADIENT DYNAMICS FOR THE ODE SYSTEM (2)
Let θ denote the i, j-th entry of the Weight matrices W, W, V or the i-th entry of the bias vector b.
We are interested in ﬁnding out how the gradients of the hidden state y (and the auxiliary hidden state
z) with respect to parameter θ, vary with time. Note that these gradients are precisely the objects of
interest in the training of an RNN, based on a discretization of the ODE system (2). To this end, we
differentiate (2) with respect to the parameter θ and denote
yθ(t) = ∂y
∂θ (t), zθ(t) = ∂z
∂θ (t),
to obtain,
y′
θ = zθ,
z′
θ = diag(σ′(A)) [Wyθ + Wzθ] + Zi,j
m, ¯m(A)ρ −γyθ −ϵzθ.
(22)
As introduced before, Zi,j
m, ¯m(A) ∈Rm× ¯m is a matrix with all elements are zero except for the
(i, j)-th entry which is set to σ′(A(t))i, i.e. the i-th entry of σ′(A), and we have,
ρ = y,
¯m = m,
if θ = Wi,j,
ρ = z,
¯m = m,
if θ = Wi,j,
ρ = u,
¯m = d,
if θ = Vi,j,
ρ = 1,
¯m = 1,
if θ = bi.
We see from (22) that the ODEs governing the gradients with respect to the parameter θ also represent
a system of oscillators but with additional coupling and forcing terms, proportional to the hidden
states y, z or input signal u. As we have already proved with estimate (19) that the hidden states
are always bounded and the input signal is assumed to be bounded, it is natural to expect that the
gradients of the states with respect to θ are also bounded. We make this statement explicit in the
following proposition, which for simplicity of exposition, we consider the case of θ = Wi,j, as the
other values of θ are very similar in their behavior.
Proposition D.3 Let θ = Wi,j and y, z be the solutions of the ODE system (2). Assume that the
weights and the damping parameter satisfy,
∥W∥∞+ ∥W∥∞≤ϵ,
then we have the following bounds on the gradients,
yθ(t)⊤yθ(t) + 1
γ
 zθ(t)⊤zθ(t)

≤

yθ(0)⊤yθ(0) + 1
γ
 zθ(0)⊤zθ(0)

eCt + mt2
2γ2 ,
t ∈(0, T],
C = max
∥W∥1
γ
, 1 + ∥W∥1

.
(23)
17

Published as a conference paper at ICLR 2021
The proof of this proposition follows exactly along the same lines as the proof of proposition D.1 and
we skip the details, while noting the crucial role played by the energy bound (19).
We remark that the bound (23) indicates that as long as the initial gradients with respect to θ are
bounded and the weights are controlled by the damping parameter, the hidden state gradients remain
bounded in time.
E
SUPPLEMENT TO THE RIGOROUS ANALYSIS OF CORNN
In this section, we supplement the section on the rigorous analysis of the proposed RNN (4). We start
with
E.1
PROOF OF PROPOSITION 3.1
We multiply (y⊤
n−1, z⊤
n ) to (3) and use the elementary identities,
a⊤(a −b) = a⊤a
2
−b⊤b
2
+ 1
2(a −b)⊤(a −b),
b⊤(a −b) = a⊤a
2
−b⊤b
2
−1
2(a −b)⊤(a −b),
to obtain the following,
y⊤
n yn + z⊤
n zn
2
= y⊤
n−1yn−1 + z⊤
n−1zn−1
2
+ (yn −yn−1)⊤(yn −yn−1)
2
−(zn −zn−1)⊤(zn −zn−1)
2
+ ∆tz⊤
n σ(An−1) −∆tz⊤
n zn
≤y⊤
n−1yn−1 + z⊤
n−1zn−1
2
+ ∆t (1/2 + ∆t/2 −1) z⊤
n zn + ∆t
2 σ⊤(An−1)σ(An−1)
≤y⊤
n−1yn−1 + z⊤
n−1zn−1
2
+ m∆t
2
as σ2 ≤1 and ϵ > ∆t << 1.
Iterating the above inequality n times leads to the energy bound,
y⊤
n yn + z⊤
n zn ≤y⊤
0 y0 + z⊤
0 z0 + nm∆t = mtn,
(24)
as y0 = z0 = 0.
E.2
SENSITIVITY TO INPUTS
Next, we examine how changes in the input signal u affect the dynamics. We have the following
proposition:
Proposition E.1 Let yn, zn be the hidden states of the trained RNN (4) with respect to the input
u = {un}N
n=1 and let yn, zn be the hidden states of the same RNN (4), but with respect to the input
u = {un}N
n=1, then the differences in the hidden states are bounded by,
(yn −yn)⊤(yn −yn) + (zn −zn)⊤(zn −zn) ≤4mtn.
(25)
The proof of this proposition is completely analogous to the proof of proposition 3.1, we subtract
yn
= yn−1 + ∆tzn,
zn
= zn−1
1+∆t +
∆t
1+∆tσ(An−1) −
∆t
1+∆tyn−1, An−1 := Wyn−1 + Wzn−1 + Vun + b.
(26)
from (4) and multiply

(yn −yn)⊤, (zn −zn)⊤
to the difference. The estimate (25) follows
identically to the proof of (5) (presented above) by realizing that σ(An−1) −σ(An−1) ≤2.
Note that the bound (25) ensures that the hidden states can only separate linearly in time for changes
in the input. Thus, chaotic behavior, such as for Dufﬁng type oscillators, characterized by at least
exponential separation of trajectories, is ruled out for this proposed RNN, showing that it is stable
with respect to changes in the input. This is largely on account of the fact that the activation function
σ in (3) is globally bounded.
18

Published as a conference paper at ICLR 2021
E.3
PROOF OF PROPOSITION 3.2
From (6), we readily calculate that,
∂En
∂Xn
= [yn −¯yn, 0] .
(27)
Similarly from (3), we calculate,
∂+Xk
∂θ
=


























∆t2
1+∆tZi,j
m,m(Ak−1)yk−1
⊤
,

∆t
1+∆tZi,j
m,m(Ak−1)yk−1
⊤⊤
if
θ = (i, j)−th entry of W,

∆t2
1+∆tZi,j
m,m(Ak−1)zk−1
⊤
,

∆t
1+∆tZi,j
m,m(Ak−1)zk−1
⊤⊤
if
θ = (i, j)−th entry of W,

∆t2
1+∆tZi,j
m,d(Ak−1)uk
⊤
,

∆t
1+∆tZi,j
m,d(Ak−1)uk
⊤⊤
if
θ = (i, j)−th entry of V,

∆t2
1+∆tZi,1
m,1(Ak−1)
⊤
,

∆t
1+∆tZi,1
m,1(Ak−1)
⊤⊤
if
θ = i−th entry of b,
(28)
where Zi,j
m, ¯m(Ak−1) ∈Rm× ¯m is a matrix with all elements are zero except for the (i, j)-th entry
which is set to σ′(Ak−1)i, i.e. the i-th entry of σ′(Ak−1). We easily see that ∥Zi,j
m, ¯m(Ak−1)∥∞≤1
for all i, j, m, ¯m and all choices of Ak−1.
Now, using deﬁnitions of matrix and vector norms and applying (14) in (10), together with (27) and
(28), we obtain the following estimate on the norm:

∂E(k)
n
∂θ
 ≤







(∥yn∥∞+ ∥¯yn∥∞)(1 + 3(n −k)∆tr)δ∆t∥yk−1∥∞,
if
θ is entry of W,
(∥yn∥∞+ ∥¯yn∥∞)(1 + 3(n −k)∆tr)δ∆t∥zk−1∥∞,
if
θ is entry of W,
(∥yn∥∞+ ∥¯yn∥∞)(1 + 3(n −k)∆tr)δ∆t∥uk∥∞,
if
θ is entry of V,
(∥yn∥∞+ ∥¯yn∥∞)(1 + 3(n −k)∆tr)δ∆t,
if
θ is entry of b.
(29)
We will estimate the above term, just for the case of θ is an entry of W, the rest of the terms are very
similar to estimate.
For simplicity of notation, we let k −1 ≈k and aim to estimate the term,

∂E(k)
n
∂θ
 ≤∥yn∥∞∥yk∥∞(1 + 3(n −k)∆tr)δ∆t + ∥¯yn∥∞∥yk∥∞(1 + 3(n −k)∆tr)δ∆t
≤m
√
nk∆t(1 + 3(n −k)∆tr)δ∆t + ∥¯yn∥∞
√
mk
√
∆t(1 + 3(n −k)∆tr)δ∆t
(by (5))
≤m
√
nkδ∆t2 + 3m
√
nk(n −k)δ∆tr+2 + ∥¯yn∥∞
√
mk
√
∆t(1 + 3(n −k)∆tr)δ∆t.
(30)
To further analyze the above estimate, we recall that n∆t = tn ≤1 and consider two different
regimes. Let us start by considering short-term dependencies by letting k ≈n, i.e n −k = c with
constant c ∼O(1), independent of n, k. In this case, a straightforward application of the above
assumptions in the bound (30) yields,

∂E(k)
n
∂θ
 ≤m
√
nkδ∆t2 + 3m
√
nk(n −k)δ∆tr+2 + ∥¯yn∥∞
√m√tnδ∆t + ∥¯yn∥∞
√m√tncδ∆tr+1
≤mtnδ∆t + mctnδ∆tr+1 + ∥¯yn∥∞
√m√tnδ∆t + ∥¯yn∥∞
√m√tncδ∆tr+1
≤tnmδ∆t + ∥¯yn∥∞
√m√tnδ∆t
(for ∆t << 1 as r ≥1/2)
≤mδ∆t + ∥¯yn∥∞
√mδ∆t.
(31)
19

Published as a conference paper at ICLR 2021
Next, we consider long-term dependencies by setting k << n and estimating,

∂E(k)
n
∂θ
 ≤m
√
nkδ∆t2 + 3m
√
nk(n −k)δ∆tr+2 + ∥¯yn∥∞
√mδ∆t
3
2 + 3∥¯yn∥∞
√mnδ∆tr+ 3
2
≤m√tnδ∆t
3
2 + 3mt
3
2nδ∆tr+ 1
2 + ∥¯yn∥∞
√mδ∆t
3
2 + 3∥¯yn∥∞
√mtnδ∆tr+ 1
2
≤mδ∆t
3
2 + 3mδ∆tr+ 1
2 + ∥¯yn∥∞
√mδ∆t
3
2 + 3∥¯yn∥∞
√mδ∆tr+ 1
2
(as tn < 1)
≤3mδ∆tr+ 1
2 + 3∥¯yn∥∞
√mδ∆tr+ 1
2
(as r ≤1 and ∆t << 1).
(32)
Thus, in all cases, we have that,

∂E(k)
n
∂θ
 ≤3δ∆t
 m + √m∥¯yn∥∞

(as r ≥1/2).
(33)
Applying the above estimate in (10) allows us to bound the gradient by,

∂En
∂θ
 ≤
X
1≤k≤n

∂E(k)
n
∂θ
 ≤3δtn
 m + √m∥¯yn∥∞

.
(34)
Therefore, the gradient of the loss function (6) can be bounded as,

∂E
∂θ
 ≤1
N
N
X
n=1

∂En
∂θ

≤3δ
"
m∆t
N
N
X
n=1
n +
√m∆t
N
N
X
n=1
∥¯yn∥∞n
#
≤3δ
"
m∆t
N
N
X
n=1
n +
√m ¯Y ∆t
N
N
X
n=1
n
#
≤3
2δ(N + 1)∆t
 m + ¯Y √m

≤3
2δ(tN + ∆t)
 m + ¯Y √m

≤3
2δ(1 + ∆t)
 m + ¯Y √m

(as tN = 1)
≤3
2
 m + ¯Y √m

,
(35)
which is the desired estimate (9).
E.4
ON THE ASSUMPTION (8) AND TRAINING
Note that all the estimates were based on the fact that we were able to choose a time step ∆t in (3)
that enforces the condition (8). For any ﬁxed weights W, W, we can indeed choose such a value of ϵ
to satisfy (8). However, we train the RNN to ﬁnd the weights that minimize the loss function (6).
Can we ﬁnd a hyperparameter ∆t such that (8) is satisﬁed at every step of the stochastic gradient
descent method for training?
To investigate this issue, we consider a simple gradient descent method of the form:
θℓ+1 = θℓ−ζ ∂E
∂θ (θℓ).
(36)
Note that ζ is the constant (non-adapted) learning rate. We assume for simplicity that θ0 = 0 (other
choices lead to the addition of a constant). Then, a straightforward estimate on the weight is given by,
20

Published as a conference paper at ICLR 2021
|θℓ+1| ≤|θℓ| + ζ

∂E
∂θ (θℓ)

≤|θℓ| + ζ 3
2
 m + ¯Y √m

(by (35))
≤|θ0| + ℓζ 3
2
 m + ¯Y √m

= ℓζ 3
2
 m + ¯Y √m

.
(37)
In order to calculate the minimum number of steps L in the gradient descent method (36) such that
the condition (8) is satisﬁed, we set ℓ= L in (37) and applying it to the condition (8) leads to the
straightforward estimate,
L ≥
1
ζ 3
2
 m + ¯Y √m

m∆t1−rδ .
(38)
Note that the parameter δ < 1, while in general, the learning rate ζ << 1. Thus, as long as r ≤1,
we see that the assumption (8) holds for a large number of steps of the gradient descent method. We
remark that the above estimate (38) is a large underestimate on L. In the experiments presented in
this article, we are able to take a very large number of training steps, while the gradients remain
within a range (see Fig. 3).
E.5
PROOF OF PROPOSITION 3.3
We start with the following decomposition of the recurrent matrices:
∂Xi
∂Xi−1
= Mi−1 + ∆t ˜
Mi−1,
Mi−1 :=

I
∆tCi−1
Bi−1
Ci−1

,
˜
Mi−1 :=

Bi−1
0
0
0

,
with B, C deﬁned in (12). By the assumption (8), one can readily check that ∥˜
Mi−1∥∞≤∆t, for
all k ≤i ≤n −1.
We will use an induction argument to show the following representation formula for the product of
Jacobians,
∂Xn
∂Xk
=
Y
k<i≤n
∂Xi
∂Xi−1
=


I
∆t
n−1
P
j=k
kQ
i=j
Ci
Bn−1 +
kP
j=n−2
 j+1
Q
i=n−1
Ci

Bj
kQ
i=n−1
Ci

+ O(∆t).
(39)
We start by the outermost product and calculate,
∂Xn
∂Xn−1
∂Xn−1
∂Xn−2
=

Mn−1 + ∆t ˜
Mn−1
 
Mn−2 + ∆t ˜
Mn−2

= Mn−1Mn−2 + ∆t( ˜
Mn−1Mn−2 + Mn−1 ˜
Mn−2) + O(∆t2).
By direct multiplication, we obtain,
Mn−1Mn−2 =

I
∆t (Cn−2 + Cn−1Cn−2)
Bn−1 + Cn−1Bn−2
Cn−1Cn−2

+ ∆t

Cn−1Bn−2
0
0
Bn−1Cn−2

.
Using the deﬁnitions in (12) and (8), we can easily see that

Cn−1Bn−2
0
0
Bn−1Cn−2

= O(∆t).
Similarly, it is easy to show that
˜
Mn−1Mn−2, Mn−1 ˜
Mn−2 ∼O(∆t).
21

Published as a conference paper at ICLR 2021
Plugging all the above estimates yields,
∂Xn
∂Xn−1
∂Xn−1
∂Xn−2
=

I
∆t (Cn−2 + Cn−1Cn−2)
Bn−1 + Cn−1Bn−2
Cn−1Cn−2

+ O(∆t2),
which is exactly the form of the leading term (39).
Iterating the above calculations (n −k) times and realizing that (n −k)∆t2 ≈n∆t2 = tn∆t yields
the formula (39).
Recall that we have set θ = Wi,j, for some 1 ≤i, j ≤m in proposition 3.3. Directly calculating
with (27), (28) and the representation formula (39) yields the formula,
∂E(k)
n
∂θ
= y⊤
n ∆t2δZi,j
m,m(Ak−1)yk−1 + y⊤
n ∆t2δC∗Zi,j
m,m(Ak−1)yk−1 + O(∆t3),
(40)
with matrix C∗deﬁned as,
C∗:=
n−1
X
j=k
k
Y
i=j
Ci,
and Zi,j
m,m(Ak−1) ∈Rm×m is a matrix with all elements are zero except for the (i, j)-th entry which
is set to σ′(ai
k−1), i.e. the i-th entry of σ′(Ak−1).
Note that the formula (40) can be explicitly written as,
∂E(k)
n
∂θ
= δ∆t2σ′(ai
k−1)yi
nyj
k−1 + δ∆t2σ′(ai
k−1)
m
X
ℓ=1
C∗
ℓiyℓ
nyj
k−1 + O(∆t3),
(41)
with yj
n denoting the j-th element of vector yn, and
ai
k−1 :=
m
X
ℓ=1
Wiℓyℓ
k−1 +
m
X
ℓ=1
Wiℓzℓ
k−1.
(42)
By the assumption (8), we can readily see that
∥W∥∞, ∥W∥∞≤1 + ∆t.
Therefore by the fact that σ′ = sech2, the assumption yi
k = O(√tk) and (42), we obtain,
ˆc = sech2(
√
k∆t(1 + ∆t) ≤σ′(ak−1
i
) ≤1.
(43)
Using (43) in (41), we obtain,
δ∆t2σ′(ai
k−1)yi
nyj
k−1 = O

ˆcδ∆t
5
2

.
(44)
Using the deﬁnition of Ci, we can expand the product in C∗and neglect terms of order O(∆t4), to
obtain
k
Y
i=j
Ci = (O(1) + O((j −k + 1)δ∆t2))I.
Summing over j and using the fact that k << n, we obtain that
C∗= (O(n) + O(δ∆t0))I.
(45)
Plugging (45) and (43) into (41) leads to,
δ∆t2σ′(ai
k−1)
m
X
ℓ=1
C∗
ℓiyℓ
nyj
k−1 = O

ˆcδ∆t
3
2

+ O

ˆcδ2∆t
5
2

.
(46)
Combining (44) and (46) yields the desired estimate (16).
22

Published as a conference paper at ICLR 2021
Remark.
A careful examination of the above proof reveals that the constants hidden in the prefactors
of the leading term O

ˆcδ∆t
3
2

of (16) stem from the formula (46). Here, we have used the
assumption that yi
k = O(√tk). Note that this assumption implicitly assumes that the energy bound
(5) is equidistributed among all the elements of the vector yk and results in the obfuscation of the
constants in the leading term of (16). Given that the energy bound (5) is too coarse to allow for
precise upper and lower bounds on each individual element of the hidden state vector yk, we do not
see any other way of, in general, determining the distribution of energy among individual entries of
the hidden state vector. Thus, assuming equidistribution seems reasonable. On the other hand, in
practice, one has access to all the terms in formula (46) for each numerical experiment and if one is
interested, then one can directly evaluate the precise bound on the leading term of the formula (16).
F
RIGOROUS ESTIMATES FOR THE RNN (3) WITH ¯n = n −1 AND GENERAL
VALUES OF ϵ, γ
In this section, we will provide rigorous estimates, similar to that of propositions 3.1, E.1 and 3.2 for
the version of coRNN (3) that results by setting ¯n = n −1 in (3) leading to,
yn = yn−1 + ∆tzn,
zn = zn−1 + ∆tσ (Wyn−1 + Wzn−1 + Vun + b) −∆tγyn−1 −∆tϵzn−1.
(47)
Note that (47) can be equivalently written as,
yn = yn−1 + ∆tzn,
zn = (1 −ϵ∆t) zn−1 + ∆tσ (Wyn−1 + Wzn−1 + Vun + b) −∆tγyn−1.
(48)
We will also consider the case of non-unit values of the control parameters γ and ϵ below.
Bounds on Hidden states.
We start the following bound on the hidden states of (47),
Proposition F.1 Let the damping parameter ϵ > 1
2 and the time step ∆t in the RNN (47) satisfy the
following condition,
∆t < 2ϵ −1
γ + ϵ2 .
(49)
Let yn, zn be the hidden states of the RNN (47) for 1 ≤n ≤N, then the hidden states satisfy the
following (energy) bounds:
y⊤
n yn + 1
γ z⊤
n zn ≤mtn
γ .
(50)
We set An−1 = Wyn−1 + Wzn−1 + Vun−1 + b and as in the proof of proposition 3.1, we multiply
(y⊤
n−1, 1
γ z⊤
n ) to (47) and use elementary identities and rearrange terms to obtain,
y⊤
n yn
2
+ z⊤
n zn
2γ
= y⊤
n−1yn−1
2
+ z⊤
n−1zn−1
2γ
+ (yn −yn−1)⊤(yn −yn−1)
2
−(zn −zn−1)⊤(zn −zn−1)
2γ
+ ∆t
γ z⊤
n σ(An−1) −ϵ∆t
γ z⊤
n zn + ϵ∆t
γ z⊤
n (zn −zn−1) .
We use a rescaled version of the well-known Cauchy’s inequality
ab ≤ca2
2 + b2
2c,
for a constant c > 0 to be determined, to rewrite the above identity as,
y⊤
n yn
2
+ z⊤
n zn
2γ
≤y⊤
n−1yn−1
2
+ z⊤
n−1zn−1
2γ
+ (yn −yn−1)⊤(yn −yn−1)
2
+
ϵ∆t
2cγ −1
2γ

(zn −zn−1)⊤(zn −zn−1) + ∆t
2γ σ(An−1)⊤σ(An−1)
+
∆t
2γ + cϵ∆t
2γ
−ϵ∆t
γ

z⊤
n zn.
23

Published as a conference paper at ICLR 2021
Using the ﬁrst equation in (47), the above inequality reduces to,
y⊤
n yn
2
+ z⊤
n zn
2γ
≤y⊤
n−1yn−1
2
+ z⊤
n−1zn−1
2γ
+
ϵ∆t
2cγ −1
2γ

(zn −zn−1)⊤(zn −zn−1) + ∆t
2γ σ(An−1)⊤σ(An−1)
+
∆t2
2
+ ∆t
2γ + cϵ∆t
2γ
−ϵ∆t
γ

z⊤
n zn.
As long as,
∆t ≤min
c
ϵ, (2 −c)ϵ −1
γ

,
(51)
we can easily check that,
y⊤
n yn
2
+ z⊤
n zn
2γ
≤y⊤
n−1yn−1
2
+ z⊤
n−1zn−1
2γ
+ ∆t
2γ σ(An−1)⊤σ(An−1)
≤y⊤
n−1yn−1
2
+ z⊤
n−1zn−1
2γ
+ m∆t
2γ
(σ ≤1).
Iterating the above bound till n = 0 and using the zero initial data yields the desired (50) as long as
we ﬁnd a c such that the condition (51) is satisﬁed. To do so, we equalize the two terms on the right
hand side of (51) to obtain,
c = ϵ(2ϵ −1)
γ + ϵ2 .
From the assumption (49) and the fact that ϵ > 1
2, we see that such a c > 0 always exists for any
value of γ > 0 and (51) is satisﬁed, which completes the proof.
We remark that the same bound on the hidden states is obtained for both versions of coRNN, i.e. (3)
with ¯n = n and (47). However, the difference lies in the constraint on the time step ∆t. In contrast to
(49), a careful examination of the proof of proposition 3.1 reveals that the condition on the time step
for the stability of (3) with ¯n = n is given by,
∆t < 2ϵ −1
γ
,
(52)
and is clearly less stringent than the condition (51) for the stability of (47). For instance, in the
prototypical case of γ = ϵ = 1, the stability of (3) with ¯n = n is ensured for any ∆t < 1. On
the other hand, the stability of (47) is ensured as long as ∆t < 1
2. However, it is essential to recall
that these conditions are only sufﬁcient to ensure stability and are by no means necessary. Thus in
practice, the coRNN version (47) is found to be stable in the same range of time steps as the version
(3) with ¯n = n.
On the exploding and vanishing gradient problems for coRNN (47)
Next, we have the following
upper bound on the hidden state gradients for the version (47) of coRNN,
Proposition F.2 Let yn, zn be the hidden states generated by the RNN (47). We assume that the
damping parameter ϵ > 1
2 and the time step ∆t can be chosen such that in addition to (51) it also
satisﬁes,
max {∆t(γ + ∥W∥∞), ∆t∥W∥∞} = η ≤˜C∆tr,
1
2 ≤r ≤1,
(53)
and with the constant ˜C independent of the other parameters of the RNN (47). Then the gradient of
the loss function E (6) with respect to any parameter θ ∈Θ is bounded as,

∂E
∂θ
 ≤3( ˜C)
 m + ¯Y √m

2γ
,
(54)
with the constant ˜C, deﬁned in (53) and ¯Y = max
1≤n≤N ∥¯yn∥∞be a bound on the underlying training
data
24

Published as a conference paper at ICLR 2021
The proof of this proposition is completely analogous to the proof of proposition 3.2 and we omit the
details here.
Note that the bound (54) enforces that hidden state gradients cannot explode for version (47) of
coRNN. A similar statement for the vanishing gradient problem is inferred from the proposition
below.
Proposition F.3 Let yn be the hidden states generated by the RNN (47). Under the assumption that
yi
n = O(
q
tn
γ ), for all 1 ≤i ≤m and (53), the gradient for long-term dependencies satisﬁes,
∂E(k)
n
∂θ
= O
 ˆc
γ ∆t
3
2

+O
 ˆc
γ δ(1 + δ)∆t
5
2

+O(∆t3), ˆc = sech2 √
k∆t(1 + ∆t)

k << n.
(55)
The proof is a repetition of the steps of the proof of proposition 3.3, with suitable modiﬁcations for
the structure of the RNN and non-unit ϵ, γ and we omit the tedious calculations here. Note that (55)
rules out the vanishing gradient problem for the coRNN version (47).
25

