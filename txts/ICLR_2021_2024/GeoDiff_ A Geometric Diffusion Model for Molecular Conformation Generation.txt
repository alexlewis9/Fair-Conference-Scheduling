Published as a conference paper at ICLR 2022
GEODIFF: A GEOMETRIC DIFFUSION MODEL FOR
MOLECULAR CONFORMATION GENERATION
Minkai Xu1,2, Lantao Yu3, Yang Song3, Chence Shi1,2, Stefano Ermon3‚àó, Jian Tang1,4,5‚àó
1Mila - Qu√©bec AI Institute, Canada 2Universit√© de Montr√©al, Canada
3Stanford University, USA 4HEC Montr√©al, Canada 5CIFAR AI Research Chair
{minkai.xu,chence.shi}@umontreal.ca
{lantaoyu,yangsong,ermon}@cs.stanford.edu
jian.tang@hec.ca
ABSTRACT
Predicting molecular conformations from molecular graphs is a fundamental prob-
lem in cheminformatics and drug discovery. Recently, signiÔ¨Åcant progress has been
achieved with machine learning approaches, especially with deep generative mod-
els. Inspired by the diffusion process in classical non-equilibrium thermodynamics
where heated particles will diffuse from original states to a noise distribution, in
this paper, we propose a novel generative model named GEODIFF for molecular
conformation prediction. GEODIFF treats each atom as a particle and learns to
directly reverse the diffusion process (i.e., transforming from a noise distribution
to stable conformations) as a Markov chain. Modeling such a generation process
is however very challenging as the likelihood of conformations should be roto-
translational invariant. We theoretically show that Markov chains evolving with
equivariant Markov kernels can induce an invariant distribution by design, and
further propose building blocks for the Markov kernels to preserve the desirable
equivariance property. The whole framework can be efÔ¨Åciently trained in an end-to-
end fashion by optimizing a weighted variational lower bound to the (conditional)
likelihood. Experiments on multiple benchmarks show that GEODIFF is superior or
comparable to existing state-of-the-art approaches, especially on large molecules.1
1
INTRODUCTION
Graph representation learning has achieved huge success for molecule modeling in various tasks rang-
ing from property prediction (Gilmer et al., 2017; Duvenaud et al., 2015) to molecule generation (Jin
et al., 2018; Shi et al., 2020), where typically a molecule is represented as an atom-bond graph.
Despite its effectiveness in various applications, a more intrinsic and informative representation
for molecules is the 3D geometry, also known as conformation, where atoms are represented as
their Cartesian coordinates. The 3D structures determine the biological and physical properties of
molecules and hence play a key role in many applications such as computational drug and material de-
sign (Thomas et al., 2018; Gebauer et al., 2021; Jing et al., 2021; Batzner et al., 2021). Unfortunately,
how to predict stable molecular conformation remains a challenging problem. Traditional methods
based on molecular dynamics (MD) or Markov chain Monte Carlo (MCMC) are very computationally
expensive, especially for large molecules (Hawkins, 2017).
Recently, signiÔ¨Åcant progress has been made with machine learning approaches, especially with
deep generative models. For example, Simm & Hernandez-Lobato (2020); Xu et al. (2021b) studied
predicting atomic distances with variational autoencoders (VAEs) (Kingma & Welling, 2013) and
Ô¨Çow-based models (Dinh et al., 2017) respectively. Shi et al. (2021) proposed to use denoising score
matching (Song & Ermon, 2019; 2020) to estimate the gradient Ô¨Åelds over atomic distances, through
which the gradient Ô¨Åelds over atomic coordinates can be calculated. Ganea et al. (2021) studied
generating conformations by predicting both bond lengths and angles. As molecular conformations
are roto-translational invariant, these approaches circumvent directly modeling atomic coordinates by
leveraging intermediate geometric variables such as atomic distances, bond and torsion angles, which
1Code is available at https://github.com/MinkaiXu/GeoDiff.
1

Published as a conference paper at ICLR 2022
are roto-translational invariant. As a result, they are able to achieve very compelling performance.
However, as all these approaches seek to indirectly model the intermediate geometric variables, they
have inherent limitations in either training or inference process (see Sec. 2 for a detailed description).
Therefore, an ideal solution would still be directly modeling the atomic coordinates and at the same
time taking the roto-translational invariance property into account.
In this paper, we propose such a solution called GEODIFF, a principled probabilistic framework
based on denoising diffusion models (Sohl-Dickstein et al., 2015). Our approach is inspired by the
diffusion process in nonequilibrium thermodynamics (De Groot & Mazur, 2013). We view atoms
as particles in a thermodynamic system, which gradually diffuse from the original states to a noisy
distribution in contact with a heat bath. At each time step, stochastic noises are added to the atomic
positions. Our high-level idea is learning to reverse the diffusion process, which recovers the target
geometric distribution from the noisy distribution. In particular, inspired by recent progress of
denoising diffusion models on image generation (Ho et al., 2020; Song et al., 2020), we view the
noisy geometries at different timesteps as latent variables, and formulate both the forward diffusion
and reverse denoising process as Markov chains. Our goal is to learn the transition kernels such
that the reverse process can recover realistic conformations from the chaotic positions sampled
from a noise distribution. However, extending existing methods to geometric generation is highly
non-trivial: a direct application of diffusion models on the conformation generation task lead to poor
generation quality. As mentioned above, molecular conformations are roto-translational invariant,
i.e., the estimated (conditional) likelihood should be unaffected by translational and rotational
transformations (K√∂hler et al., 2020). To this end, we Ô¨Årst theoretically show that a Markov process
starting from an roto-translational invariant prior distribution and evolving with roto-translational
equivariant Markov kernels can induce an roto-translational invariant density function. We further
provide practical parameterization to deÔ¨Åne a roto-translational invariant prior distribution and a
Markov kernel imposing the equivariance constraints. In addition, we derive a weighted variational
lower bound of the conditional likelihood of molecular conformations, which also enjoys the roto-
translational invariance and can be efÔ¨Åciently optimized.
A unique strength of GEODIFF is that it directly acts on the atomic coordinates and entirely bypasses
the usage of intermediate elements for both training and inference. This general formulation enjoys
several crucial advantages. First, the model can be naturally trained end-to-end without involving
any sophisticated techniques like bilevel programming (Xu et al., 2021b), which beneÔ¨Åts from small
optimization variances. Besides, instead of solving geometries from bond lengths or angles, the
one-stage sampling fashion avoids accumulating any intermediate error, and therefore leads to more
accurate predicted structures. Moreover, GEODIFF enjoys a high model capacity to approximate the
complex distribution of conformations. Thus, the model can better estimate the highly multi-modal
distribution and generate structures with high quality and diversity.
We conduct comprehensive experiments on multiple benchmarks, including conformation generation
and property prediction tasks. Numerical results show that GEODIFF consistently outperforms
existing state-of-the-art machine learning approaches, and by a large margin on the more challenging
large molecules. The signiÔ¨Åcantly superior performance demonstrate the high capacity to model the
complex distribution of molecular conformations and generate both diverse and accurate molecules.
2
RELATED WORK
Recently, various deep generative models have been proposed for conformation generation. Among
them, CVGAE (Mansimov et al., 2019) Ô¨Årst proposed a VAE model to directly generate 3D atomic
coordinates, which fails to preserve the roto-translation equivariance property of conformations and
suffers from poor performance. To address this problem, the majority of subsequent models are
based on intermediate geometric elements such as atomic distances and torsion angles. A favorable
property of these elements is the roto-translational invariance, (e.g. atomic distances does not
change when rotating the molecule), which has been shown to be an important inductive bias for
molecular geometry modeling (K√∂hler et al., 2020). However, such a decomposition suffers from
several drawbacks for either training or sampling. For example, GRAPHDG (Simm & Hernandez-
Lobato, 2020) and CGCF (Xu et al., 2021a) proposed to predict the interatomic distance matrix
by VAE and Flow respectively, and then solve the geometry through the Distance Geometry (DG)
technique (Liberti et al., 2014), which searches reasonable coordinates that matches with the predicted
2

Published as a conference paper at ICLR 2022
distances. CONFVAE further improves this pipeline by designing an end-to-end framework via bilevel
optimization (Xu et al., 2021b). However, all these approaches suffer from the accumulated error
problem, meaning that the noise in the predicted distances will misguide the coordinate searching
process and lead to inaccurate or even erroneous structures. To overcome this problem, CONFGF (Shi
et al., 2021; Luo et al., 2021) proposed to learn the gradient of the log-likelihood w.r.t coordinates.
However, in practice the model is still aided by intermediate geometric elements, in that it Ô¨Årst
estimates the gradient w.r.t interatomic distances via denoising score matching (DSM) (Song &
Ermon, 2019; 2020), and then derives the gradient of coordinates using the chain rule. The problem
is, by learning the distance gradient via DSM, the model is fed with perturbed distance matrices,
which may violate the triangular inequality or even contain negative values. As a consequence, the
model is actually learned over invalid distance matrices but tested with valid ones calculated from
coordinates, making it suffer from serious out-of-distribution (Hendrycks & Gimpel, 2016) problem.
Most recently, another concurrent work (Ganea et al., 2021) proposed a highly systematic (rule-based)
pipeline named GEOMOL, which learns to predict a minimal set of geometric quantities (i.e. length
and angles) and then reconstruct the local and global structures of the conformation in a sophisticated
procedure. Besides, there has also been efforts to use reinforcement learning for conformation
search Gogineni et al. (2020). Nevertheless, this method relies on rigid rotor approximation and can
only model the torsion angles, and thus fundamentally differs from other approaches.
3
PRELIMINARIES
3.1
NOTATIONS AND PROBLEM DEFINITION
Notations. In this paper each molecule with n atoms is represented as an undirected graph G = ‚ü®V, E‚ü©,
where V = {vi}n
i=1 is the set of vertices representing atoms and E = {eij | (i, j) ‚äÜ|V| √ó |V|} is
the set of edges representing inter-atomic bonds. Each node vi ‚ààV describes the atomic attributes,
e.g., the element type. Each edge eij ‚ààE describes the corresponding connection between vi and
vj, and is labeled with its chemical type. In addition, we also assign the unconnected edges with a
virtual type. For the geometry, each atom in V is embedded by a coordinate vector c ‚ààR3 into the
3-dimensional space, and the full set of positions (i.e., the conformation) can be represented as a
matrix C = [c1, c2, ¬∑ ¬∑ ¬∑ , cn] ‚ààRn√ó3.
Problem DeÔ¨Ånition. The task of molecular conformation generation is a conditional generative
problem, where we are interested in generating stable conformations for a provided graph G. Given
multiple graphs G, and for each G given its conformations C as i.i.d samples from an underlying
Boltzmann distribution (No√© et al., 2019), our goal is learning a generative model pŒ∏(C|G), which is
easy to draw samples from, to approximate the Boltzmann function.
3.2
EQUIVARIANCE
Equivariance is ubiquitous in machine learning for atomic systems, e.g., the vectors of atomic dipoles
or forces should rotate accordingly w.r.t. the conformation coordinates (Thomas et al., 2018; Weiler
et al., 2018; Fuchs et al., 2020; Miller et al., 2020; Simm et al., 2021; Batzner et al., 2021). It has
shown effectiveness to integrate such inductive bias into model parameterization for modeling 3D
geometry, which is critical for the generalization capacity (K√∂hler et al., 2020; Satorras et al., 2021a).
Formally, a function F : X ‚ÜíY is equivariant w.r.t a group G if:
F ‚ó¶Tg(x) = Sg ‚ó¶F(x),
(1)
where Tg and Sg are transformations for an element g ‚ààG, acting on the vector spaces X and
Y, respectively. In this work, we consider the SE(3) group, i.e., the group of rotation, translation
in 3D space. This requires the estimated likelihood unaffected with translational and rotational
transformations, and we will elaborate on how our method satisfy this property in Sec. 4.
4
GEODIFF METHOD
In this section, we elaborate on the proposed equivariant diffusion framework. We Ô¨Årst present
a high level description of our 3D diffusion formulation in Sec. 4.1, based on recent progress of
denoising diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020). Then we emphasize several
3

Published as a conference paper at ICLR 2022
ùë™!
ùë™"
ùë™!"#
‚ãØ
ùëù! ùê∂"#$ ùí¢, ùê∂")
ùëûùê∂" ùê∂"#$)
ùë™$
‚ãØ
Figure 1: Illustration of the diffusion and reverse process of GEODIFF. For diffusion process,
noise from Ô¨Åxed posterior distributions q(Ct|Ct‚àí1) is gradually added until the conformation is de-
stroyed. Symmetrically, for generative process, an initial state CT is sampled from standard Gaussian
distribution, and the conformation is progressively reÔ¨Åned via the Markov kernels pŒ∏(Ct‚àí1|G, Ct).
non-trivial challenges of building diffusion models for geometry generation scenario, and show how
we technically tackle these issues. SpeciÔ¨Åcally, in Sec. 4.2, we present how we parameterize pŒ∏(C|G)
so that the conditional likelihood is roto-translational invariant, and in Sec. 4.3, we introduce our
surgery of the training objective to make the optimization also invariant of translation and rotation.
Finally, we brieÔ¨Çy show how to draw samples from our model in Sec. 4.4.
4.1
FORMULATION
Let C0 denotes the ground truth conformations and let Ct for t = 1, ¬∑ ¬∑ ¬∑ , T be a sequence of latent
variables with the same dimension, where t is the index for diffusion steps. Then a diffusion
probabilistic model (Sohl-Dickstein et al., 2015) can be described as a latent variable model with two
processes: the forward diffusion process, and the reverse generative process. Intuitively, the diffusion
process progressively injects small noises to the data C0, while the generative process learns to revert
the diffusion process by gradually eliminating the noise to recover the ground truth. We provide a
high-level schematic of the processes in Fig. 1.
Diffusion process. Following the physical insight, we model the particles C as an evolving thermo-
dynamic system. With time going by, the equilibrium conformation C0 will gradually diffuse to the
next chaotic states Ct, and Ô¨Ånally converge into a white noise distribution after T iterations. Different
from typical latent variable models, in diffusion model this forward process is deÔ¨Åned as a Ô¨Åxed
(rather than trainable) posterior distribution q(C1:T |C0). SpeciÔ¨Åcally, we deÔ¨Åne it as a Markov chain
according to a Ô¨Åxed variance schedule Œ≤1, . . . , Œ≤T :
q(C1:T |C0) =
T
Y
t=1
q(Ct|Ct‚àí1),
q(Ct|Ct‚àí1) = N(Ct;
p
1 ‚àíŒ≤tCt‚àí1, Œ≤tI).
(2)
Note that, in this work we do not impose speciÔ¨Åc (invariance) requirement upon the diffusion process,
as long as it can efÔ¨Åciently draw noisy samples for training the generative process pŒ∏(C0).
Let Œ±t = 1 ‚àíŒ≤t and ¬ØŒ±t = Qt
s=1 Œ±s, a special property of the forward process is that q(Ct|C0) of
arbitrary timestep t can be calculated in closed form q(Ct|C0) = N(Ct; ‚àö¬ØŒ±tC0, (1 ‚àí¬ØŒ±t)I)2. This
indicates with sufÔ¨Åciently large T, the whole forward process will convert C0 to whitened isotropic
Gaussian, and thus it is natural to set p(CT ) as a standard Gaussian distribution.
Reverse Process. Our goal is learning to recover conformations C0 from the white noise CT , given
speciÔ¨Åed molecular graphs G. We consider this generative procedure as a reverse dynamics of the
above diffusion process, starting from the noisy particles CT ‚àºp(CT ). We formulate this reverse
dynamics as a conditional Markov chain with learnable transitions:
pŒ∏(C0:T ‚àí1|G, CT ) =
T
Y
t=1
pŒ∏(Ct‚àí1|G, Ct),
pŒ∏(Ct‚àí1|G, Ct) = N(Ct‚àí1; ¬µŒ∏(G, Ct, t), œÉ2
t I).
(3)
Herein ¬µŒ∏ are parameterized neural networks to estimate the means, and œÉt can be any user-deÔ¨Åned
variance. The initial distribution p(CT ) is set as a standard Gaussian. Given a graph G, its 3D structure
is generated by Ô¨Årst drawing chaotic particles CT from p(CT ), and then iteratively reÔ¨Åned through
the reverse Markov kernels pŒ∏(Ct‚àí1|G, Ct).
2Detailed derivations are provided in the Appendix A.
4

Published as a conference paper at ICLR 2022
Having formulated the reverse dynamics, the marginal likelihood can be calculated by pŒ∏(C0|G) =
R
p(CT )pŒ∏(C0:T ‚àí1|G, CT )dC1:T . Herein a non-trivial problem is that the likelihood should be
invariant w.r.t translation and rotation, which has proved to be a critical inductive bias for 3D object
generation (K√∂hler et al., 2020; Satorras et al., 2021a). In the following subsections, we will elaborate
on how we parameterize the Markov kernels pŒ∏(Ct‚àí1|G, Ct) to achieve this desired property, and also
how to maximize this likelihood by taking the invariance into account.
4.2
EQUIVARIANT REVERSE GENERATIVE PROCESS
Instead of directly leveraging existing methods, we consider building the density pŒ∏(C0) that is
invariant to rotation and translation transformations. Intuitively, this requires the likelihood to be
unaffected by translations and rotations. Formally, let Tg be some roto-translational transformations
of a group element g ‚ààSE(3), then we have the following statement:
Proposition 1. Let p(xT ) be an SE(3)-invariant density function, i.e., p(xT ) = p(Tg(xT )). If
Markov transitions p(xt‚àí1|xt) are SE(3)-equivariant, i.e., p(xt‚àí1|xt) = p(Tg(xt‚àí1)|Tg(xt)), then
we have that the density pŒ∏(x0) =
R
p(xT )pŒ∏(x0:T ‚àí1|xT )dx1:T is also SE(3)-invariant.
This proposition indicates that the dynamics starting from an invariant standard density along an
equivariant Gaussian Markov kernel can result in an invariant density. Now we provide a practical
implementation of GEODIFF based on the recent denoising diffusion framework (Ho et al., 2020).
Invariant Initial Density p(CT ). We Ô¨Årst introduce the invariant distribution p(CT ), which will
also be employed in the equivariant Markov chain. We borrow the idea from K√∂hler et al. (2020)
to consider systems with zero center of mass (CoM), termed CoM-free systems. We deÔ¨Åne p(CT )
as a ‚ÄúCoM-free standard density‚Äù ÀÜœÅ(C), built upon an isotropic normal density œÅ(C): for evaluating
the likelihood ÀÜœÅ(C) we can Ô¨Årstly translate C to zero CoM and then calculate œÅ(C), and for sampling
from ÀÜœÅ(C) we can Ô¨Årst sample from œÅ(C) and then move the CoM to zero.
We provide a formal theoretical analysis of ÀÜœÅ(C) in Appendix A. Intuitively, the isotropic Gaussian
is manifestly invariant to rotations around the zero CoM. And by considering CoM-free system,
moving the particles to zero CoM can always ensure the translational invariance. Consequently, ÀÜœÅ(C)
is constructed as a roto-transitional invariant density.
Equivariant Markov Kernels p(Ct‚àí1|G, Ct). Similar to the prior density, we also consider equip-
ping all intermediate structures Ct as CoM-free systems. SpeciÔ¨Åcally, given mean ¬µŒ∏(G, Ct, t) and
variance œÉt, the likelihood of Ct‚àí1 will be calculated by ÀÜœÅ( Ct‚àí1‚àí¬µŒ∏(G,Ct,t)
œÉt
). The CoM-free Gaussian
ensures the translation invariance in the Markov kernels. Consequently, to achieve the equivariant
property deÔ¨Åned in Proposition 1, we focus on the rotation equivariance.
Then in general, the key requirement is to ensure the means ¬µŒ∏(G, Ct, t) to be roto-translation
equivariant w.r.t Ct. Following Ho et al. (2020), we consider the following parameterization of ¬µŒ∏:
¬µŒ∏(Ct, t) =
1
‚àöŒ±t

Ct ‚àí
Œ≤t
‚àö1 ‚àí¬ØŒ±t
œµŒ∏(G, Ct, t)

,
(4)
where œµŒ∏ are neural networks with trainable parameters Œ∏. Intuitively, the model œµŒ∏ learns to predict the
noise necessary to decorrupt the conformations. This is analogous to the physical force Ô¨Åelds (Sch√ºtt
et al., 2017; Zhang et al., 2018; Hu et al., 2021; Shuaibi et al., 2021), which also gradually push
particles towards convergence around the equilibrium states.
Now the problem is transformed to constructing œµŒ∏ to be roto-translational equivariant. We draw
inspirations from recent equivariant networks (Thomas et al., 2018; Satorras et al., 2021b) to design an
equivariant convolutional layer, named graph Ô¨Åeld network (GFN). In the l-th layer, GFN takes node
embeddings hl ‚ààRn√ób (b denotes the feature dimension) and corresponding coordinate embeddings
xl ‚ààRn√ó3 as inputs, and outputs hl+1 and xl+1 as follows:
mij = Œ¶m
 hl
i, hl
j, ‚à•xl
i ‚àíxl
j‚à•2, eij; Œ∏m

(5)
hl+1
i
= Œ¶h

hl
i,
X
j‚ààN(i)
mij; Œ∏h

(6)
xl+1
i
=
X
j‚ààN(i)
1
dij
(ci ‚àícj) Œ¶x (mij; Œ∏x)
(7)
5

Published as a conference paper at ICLR 2022
where Œ¶ are feed-forward networks and dij denotes interatomic distances. N(i) denotes the neighbor-
hood of ith node, including both connected atoms and other ones within a radius threshold œÑ, which
enables the model to explicitly capture long-range interactions and support molecular graphs with
disconnected components. Initial embeddings h0 are combinations of atom and timestep embeddings,
and x0 are atomic coordinates. The main difference between proposed GFN and other GNNs lies
in equation 7, where x is updated as a combination of radial directions weighted by Œ¶x : Rb ‚ÜíR.
Such vector Ô¨Åeld xL enjoys the roto-translation equivariance property. Formally, we have:
Proposition 2. Parameterizing œµŒ∏(G, C, t) as a composition of L GFN layers, and take the xL after
L updates as the output. Then the noise vector Ô¨Åeld œµŒ∏ is SE(3) equivariant w.r.t the 3D system C.
Intuitively, given hl already invariant and xl equivariant, the message embedding m will also be
invariant since it only depends on invariant features. Since x is updated with the relative differences
ci ‚àícj weighted by invariant features, it will be translation-invariant and rotation-equivariant. Then
inductively, composing œµŒ∏ with L GFN layers enables equivariance with Ct. We provide the formal
proof of equivariance properties in Appendix A.
4.3
IMPROVED TRAINING OBJECTIVE
Having formulated the generative process and the model parameterization, now we consider the
practical training objective for the reverse dynamics. Since directly optimizing the exact log-likelihood
is intractable, we instead maximize the usual variational lower bound (ELBO)3:
E

log pŒ∏(C0|G)

= E
h
log Eq(C1:T |C0)
pŒ∏(C0:T |G)
q(C1:T |C0)
i
‚â•‚àíEq
h
T
X
t=1
DKL(q(Ct‚àí1|Ct, C0)‚à•pŒ∏(Ct‚àí1|Ct, G))
i
:= ‚àíLELBO
(8)
where q(Ct‚àí1|Ct, C0) is analytically tractable as N(
‚àö¬ØŒ±t‚àí1Œ≤t
1‚àí¬ØŒ±t
C0 +
‚àöŒ±t(1‚àí¬ØŒ±t‚àí1)
1‚àí¬ØŒ±t
Ct, 1‚àí¬ØŒ±t‚àí1
1‚àí¬ØŒ±t Œ≤t)3. Most
recently, Ho et al. (2020) showed that under the parameterization in equation 4, the ELBO of the
diffusion model can be further simpliÔ¨Åed by calculating the KL divergences between Gaussians as
weighted L2 distances between the means œµŒ∏ and œµ3. Formally, we have:
Proposition 3. (Ho et al., 2020) Under the parameterization in equation 4, we have:
LELBO =
T
X
t=1
Œ≥tE{C0,G}‚àºq(C0,G),œµ‚àºN(0,I)
h
‚à•œµ ‚àíœµŒ∏(G, Ct, t)‚à•2
2
i
(9)
where Ct = ‚àö¬ØŒ±tC0 + ‚àö1 ‚àí¬ØŒ±tœµ. The weights Œ≥t =
Œ≤t
2Œ±t(1‚àí¬ØŒ±t‚àí1) for t > 1, and Œ≥1 =
1
2Œ±1 .
The intuition of this objective is to independently sample chaotic conformations of different timesteps
from q(Ct‚àí1|Ct, C0), and use œµŒ∏ to model the noise vector œµ. To yield a better empirical performance,
Ho et al. (2020) suggests to set all weights Œ≥t as 1, which is in line with the the objectives of recent
noise conditional score networks (Song & Ermon, 2019; 2020).
As œµŒ∏ is designed to be equivariant, it is natural to require its supervision signal œµ to be equivariant
with Ct. Note that once this is achieved, the ELBO will also become invariant. However, the œµ in
the forward diffusion process is not imposed with such equivariance, violating the above properties.
Here we propose two approaches to obtain the modiÔ¨Åed noise vector ÀÜœµ, which, after replacing œµ in the
L2 distance calculation in equation 9, achieves the desired equivariance:
Alignment approach. Considering the fact that œµ can be calculated by Ct‚àí‚àö¬ØŒ±tC0
‚àö1‚àí¬ØŒ±t
, we can Ô¨Årst rotate
and translate C0 to ÀÜC0 by aligning w.r.t Ct, and then compute ÀÜœµ as Ct‚àí‚àö¬ØŒ±t ÀÜC0
‚àö1‚àí¬ØŒ±t . Since the aligned
conformation ÀÜC0 is equivariant with Ct, the processed ÀÜœµ will also enjoy the equivariance. SpeciÔ¨Åcally,
the alignment is implemented by Ô¨Årst translating C0 to the same CoM of Ct and then solve the optimal
rotation matrix by Kabsch alignment algorithm (Kabsch, 1976).
3The detailed derivations and full proofs are provided in Appendix A.
6

Published as a conference paper at ICLR 2022
Chain-rule approach. Another meaningful observation is that by reparameterizing the Gaussian
distribution q(Ct|C0) as Ct = ‚àö¬ØŒ±tC0 + ‚àö1 ‚àí¬ØŒ±tœµ, œµ can be viewed as a weighted score function
‚àö1 ‚àí¬ØŒ±t‚àáCt q(Ct|C0). Shi et al. (2021) recently shows that generally this score function ‚àáCt q(Ct|¬∑)
can be designed to be equivariant by decomposing it into ‚àÇCtdt ‚àádtq(Ct|¬∑) with the chain rule, where
dt can be any invariant features of the structures Ct such as the inter-atomic distances. We refer
readers to Shi et al. (2021) for more details. The insight is that as gradient of invariant variables w.r.t
equivariant variables, the partial derivative ‚àÇCtdt will always be equivalent with Ct. In this work,
under the common assumption that d also follows a Gaussian distribution (Kingma & Welling, 2013),
our practical implementation is to Ô¨Årst approximately calculate ‚àádtq(Ct|C0) as dt‚àí‚àö¬ØŒ±td0
1‚àí¬ØŒ±t
, and then
compute the modiÔ¨Åed noise vector ÀÜœµ as ‚àö1 ‚àí¬ØŒ±t ‚àÇCtdt( dt‚àí‚àö¬ØŒ±td0
1‚àí¬ØŒ±t
) = ‚àÇCtdt¬∑(dt‚àí‚àö¬ØŒ±td0)
‚àö1‚àí¬ØŒ±t
.
4.4
SAMPLING
Algorithm 1 Sampling Algorithm of GEODIFF.
Input: the molecular graph G, the learned reverse model œµŒ∏.
Output: the molecular conformation C.
1: Sample CT ‚àºp(CT ) = N(0, I)
2: for s = T, T ‚àí1, ¬∑ ¬∑ ¬∑ , 1 do
3:
Shift Cs to zero CoM
4:
Compute ¬µŒ∏(Cs, G, s) from œµŒ∏(Cs, G, s) using equation 4
5:
Sample Cs‚àí1 ‚àºN(Cs‚àí1; ¬µŒ∏(Cs, G, s), œÉ2
t I)
6: end for
7: return C0 as C
With a learned reverse dynamics
œµŒ∏(G, Ct, t), the transition means
¬µŒ∏(G, Ct, t) can be calculated by
equation 4. Thus, given a graph
G, its geometry C0 is generated
by Ô¨Årst sampling chaotic parti-
cles CT
‚àº
p(CT ), and then
progressively sample Ct‚àí1
‚àº
pŒ∏(Ct‚àí1|G, Ct) for t = T, T ‚àí
1, ¬∑ ¬∑ ¬∑ , 1. This process is Marko-
vian, which gradually shifts the
previous noisy positions towards
equilibrium states. We provide the pseudo code of the whole sampling process in Algorithm 1.
5
EXPERIMENT
In this section, we empirically evaluate GEODIFF on the task of equilibrium conformation generation
for both small and drug-like molecules. Following existing work (Shi et al., 2021; Ganea et al.,
2021), we test the proposed method as well as the competitive baselines on two standard benchmarks:
Conformation Generation (Sec. 5.2) and Property Prediction (Sec. 5.3). We Ô¨Årst present the
general experiment setups, and then describe task-speciÔ¨Åc evaluation protocols and discuss the results
in each section. The implementation details are provided in Appendix C.
5.1
EXPERIMENT SETUP
Datasets. Following prior works (Xu et al., 2021a;b), we also use the recent GEOM-QM9 (Ramakr-
ishnan et al., 2014) and GEOM-Drugs (Axelrod & Gomez-Bombarelli, 2020) datasets. The former
one contains small molecules while the latter one are medium-sized organic compounds. We borrow
the data split produced by Shi et al. (2021). For both datasets, the training split consists of 40, 000
molecules with 5 conformations for each, resulting in 200, 000 conformations in total. The valid
split share the same size as training split. The test split contains 200 distinct molecules, with 22, 408
conformations for QM9 and 14, 324 ones for Drugs.
Baselines. We compare GEODIFF with 6 recent or established state-of-the-art baselines. For the ML
approaches, we test the following models with highest reported performance: CVGAE (Mansimov
et al., 2019), GRAPHDG (Simm & Hernandez-Lobato, 2020), CGCF (Xu et al., 2021a), CONF-
VAE (Xu et al., 2021b) and CONFGF (Shi et al., 2021). We also test the classic RDKIT (Riniker &
Landrum, 2015) method, which is arguably the most popular open-source software for conformation
generation. We refer readers to Sec. 2 for a detailed discussion of these models.
5.2
CONFORMATION GENERATION
Evaluation metrics. The task aims to measure both quality and diversity of generated conformations
by different models. We follow Ganea et al. (2021) to evaluate 4 metrics built upon root-mean-square
7

Published as a conference paper at ICLR 2022
Table 1: Results on the GEOM-Drugs dataset, without FF optimization.
COV-R (%) ‚Üë
MAT-R (√Ö) ‚Üì
COV-P (%) ‚Üë
MAT-P (√Ö) ‚Üì
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median
CVGAE
0.00
0.00
3.0702
2.9937
-
-
-
-
GRAPHDG
8.27
0.00
1.9722
1.9845
2.08
0.00
2.4340
2.4100
CGCF
53.96
57.06
1.2487
1.2247
21.68
13.72
1.8571
1.8066
CONFVAE
55.20
59.43
1.2380
1.1417
22.96
14.05
1.8287
1.8159
GEOMOL
67.16
71.71
1.0875
1.0586
-
-
-
-
CONFGF
62.15
70.93
1.1629
1.1596
23.42
15.52
1.7219
1.6863
GEODIFF-A
88.36
96.09
0.8704
0.8628
60.14
61.25
1.1864
1.1391
GEODIFF-C
89.13
97.88
0.8629
0.8529
61.47
64.55
1.1712
1.1232
* The COV-R and MAT-R results of CVGAE, GRAPHDG, CGCF, and CONFGF are borrowed from Shi
et al. (2021). The results of GEOMOL are borrowed from a most recent study Zhu et al. (2022). Other
results are obtained by our own experiments. The results of all models for the GEOM-QM9 dataset
(summarized in Tab. 5) are collected in the same way.
deviation (RMSD), which is deÔ¨Åned as the normalized Frobenius norm of two atomic coordinates
matrices, after alignment by Kabsch algorithm (Kabsch, 1976). Formally, let Sg and Sr denote the
sets of generated and reference conformers respectively, then the Coverage and Matching metrics (Xu
et al., 2021a) following the conventional Recall measurement can be deÔ¨Åned as:
COV-R(Sg, Sr) =
1
|Sr|

n
C ‚ààSr| RMSD(C, ÀÜC) ‚â§Œ¥, ÀÜC ‚ààSg
o,
(10)
MAT-R(Sg, Sr) =
1
|Sr|
X
C‚ààSr
min
ÀÜC‚ààSg
RMSD(C, ÀÜC),
(11)
where Œ¥ is a pre-deÔ¨Åned threshold. The other two metrics COV-P and MAT-P inspired by Precision
can be deÔ¨Åned similarly but with the generated and reference sets exchanged. In practice, Sg is set
as twice of the size of Sr for each molecule. Intuitively, the COV scores measure the percentage
of structures in one set covered by another set, where covering means the RMSD between two
conformations is within a certain threshold Œ¥. By contrast, the MAT scores measure the average
RMSD of conformers in one set with its closest neighbor in another set. In general, higher COV rates
or lower MAT score suggest that more realistic conformations are generated. Besides, the Precision
metrics depend more on the quality, while the Recall metrics concentrate more on the diversity. Either
metrics can be more appealing considering the speciÔ¨Åc scenario. Following previous works (Xu et al.,
2021a; Ganea et al., 2021), Œ¥ is set as 0.5√Ö and 1.25√Ö for QM9 and Drugs datasets respectively.
Results & discussion. The results are summarized in Tab. 1 and Tab. 5 (left in Appendix. D). As
noted in Sec. 4.3, GEODIFF can be trained with two types of modiÔ¨Åed ELBO, named alignment
and chain-rule approaches. We denote models learned by these two objectives as GEODIFF-A and
GEODIFF-C respectively. As shown in the tables, GEODIFF consistently outperform the state-of-the-
art ML models on all datasets and metrics, especially by a signiÔ¨Åcant margin for more challenging
large molecules (Drugs dataset). The results demonstrate the superior capacity of GEODIFF to model
the multi modal distribution, and generative both accurate and diverse conformations. We also notice
that in general GEODIFF-C performs slightly better than GEODIFF-A, which suggests that chain-rule
approach leads to a better optimization procedure. We thus take GEODIFF-C as the representative in
the following comparisons. We visualize samples generated by different models in Fig. 2 to provide a
qualitative comparison, where GEODIFF is shown to capture better both local and global structures.
On the more challenging Drugs dataset, we further test RDKIT. As shown in Tab. 2, our observation
is in line with previous studies (Shi et al., 2021) that the state-of-the-art ML models (shown in Tab. 1)
perform better on COV-R and MAT-R. However, for the new Precision-based metrics we found that
ML models are still not comparable. This indicates that ML models tend to explore more possible
representatives while RDKIT concentrates on a few most common ones, prioritizes quality over
diversity. Previous works (Mansimov et al., 2019; Xu et al., 2021b) suggest that this is because
RDKIT involves an additional empirical force Ô¨Åeld (FF) (Halgren, 1996) to optimize the structure,
and we follow them to also combine GEODIFF with FF to yield a more fair comparison. Results in
8

Published as a conference paper at ICLR 2022
Reference
ConfGF
GeoDiff
GraphDG
Graph
Figure 2: Examples of generated structures from Drugs dataset. For every model, we show the
conformation best-aligned with the ground truth. More examples are provided in Appendix E.
Table 2: Results on the GEOM-Drugs dataset, with FF optimization.
COV-R (%) ‚Üë
MAT-R (√Ö) ‚Üì
COV-P (%) ‚Üë
MAT-P (√Ö) ‚Üì
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median
RDKIT
60.91
65.70
1.2026
1.1252
72.22
88.72
1.0976
0.9539
GEODIFF + FF
92.27
100.00
0.7618
0.7340
84.51
95.86
0.9834
0.9221
Tab. 2 demonstrate that GEODIFF +FF can keep the superior diversity (Recall metrics) while also
enjoy signiÔ¨Åcantly improved accuracy ((Precision metrics)).
5.3
PROPERTY PREDICTION
Table 3: MAE of predicted ensemble properties in eV.
Method
E
Emin
‚àÜœµ
‚àÜœµmin
‚àÜœµmax
RDKIT
0.9233
0.6585
0.3698
0.8021
0.2359
GRAPHDG
9.1027
0.8882
1.7973
4.1743
0.4776
CGCF
28.9661
2.8410
2.8356
10.6361
0.5954
CONFVAE
8.2080
0.6100
1.6080
3.9111
0.2429
CONFGF
2.7886
0.1765
0.4688
2.1843
0.1433
GEODIFF
0.25974
0.1551
0.3091
0.7033
0.1909
Evaluation metrics. This task estimates
the molecular ensemble properties (Axel-
rod & Gomez-Bombarelli, 2020) over a set
of generated conformations. This can pro-
vide an direct assessment on the quality
of generated samples. In speciÔ¨Åc, we fol-
low Shi et al. (2021) to extract a split from
GEOM-QM9 covering 30 molecules, and
generate 50 samples for each. Then we use
the chemical toolkit PSI4 (Smith et al., 2020) to calculate each conformer‚Äôs energy E and HOMO-
LUMO gap œµ, and compare the average energy E, lowest energy Emin, average gap ‚àÜœµ, minimum
gap ‚àÜœµmin, and maximum gap ‚àÜœµmax with the ground truth.
Results & discussions. The mean absolute errors (MAE) between calculated properties and the
ground truth are reported in Tab. 3. CVGAE is excluded due to the poor performance, which is also
reported in Simm & Hernandez-Lobato (2020); Shi et al. (2021). The properties are highly sensitive
to geometric structure, and thus the superior performance demonstrate that GEODIFF can consistently
predict more accurate conformations across different molecules.
6
CONCLUSION
We propose GEODIFF, a novel probabilistic model for generating molecular conformations. GEODIFF
marries denoising diffusion models with geometric representations, where we parameterize the reverse
generative dynamics as a Markov chain, and novelly impose roto-translational invariance into the
density with equivariant Markov kernels. We derive a tractable invariant objective from the variational
lower bound to optimize the likelihood. Comprehensive experiments over multiple tasks demonstrate
that GEODIFF is competitive with the existing state-of-the-art models. Future work includes further
improving or accelerating the model with other recent progress of diffusion models, and extending
our method to other challenging structures such as proteins.
9

Published as a conference paper at ICLR 2022
ACKNOWLEDGEMENT
Minkai thanks Huiyu Cai, David Wipf, Zuobai Zhang, and Zhaocheng Zhu for their helpful discus-
sions and comments. This project is supported by the Natural Sciences and Engineering Research
Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants be-
tween Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award,
Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&D Project (AI4D-CORE-06).
This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-
3583139727. The Stanford team is supported by NSF(#1651565, #1522054, #1733686), ONR
(N000141912145), AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship.
REFERENCES
Mohammed AlQuraishi. End-to-end differentiable learning of protein structure. Cell systems, 8(4):
292‚Äì301, 2019.
Simon Axelrod and Rafael Gomez-Bombarelli. Geom: Energy-annotated molecular conformations
for property prediction and molecular generation. arXiv preprint arXiv:2006.05531, 2020.
Simon Batzner, Tess E Smidt, Lixin Sun, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari,
and Boris Kozinsky. Se (3)-equivariant graph neural networks for data-efÔ¨Åcient and accurate
interatomic potentials. arXiv preprint arXiv:2101.03164, 2021.
Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. Implicit functions in feature space for 3d
shape reconstruction and completion. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6970‚Äì6981, 2020.
Sybren Ruurds De Groot and Peter Mazur. Non-equilibrium thermodynamics. Courier Corporation,
2013.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In
ICLR, 2017.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al√°n
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
Ô¨Ångerprints. In Advances in neural information processing systems, pp. 2224‚Äì2232, 2015.
Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-
translation equivariant attention networks. NeurIPS, 2020.
Octavian-Eugen Ganea, Lagnajit Pattanaik, Connor W Coley, Regina Barzilay, Klavs F Jensen,
William H Green, and Tommi S Jaakkola. Geomol: Torsional geometric generation of molecular
3d conformer ensembles. arXiv preprint arXiv:2106.07802, 2021.
Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert M√ºller, and Kristof T
Sch√ºtt. Inverse design of 3d molecular structures with conditional generative neural networks.
arXiv preprint arXiv:2109.04824, 2021.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263‚Äì1272. JMLR. org, 2017.
T. Gogineni, Ziping Xu, Exequiel Punzalan, Runxuan Jiang, Joshua A Kammeraad, Ambuj Tewari,
and P. Zimmerman. Torsionnet: A reinforcement learning approach to sequential conformer search.
ArXiv, abs/2006.07078, 2020.
Thomas A Halgren. Merck molecular force Ô¨Åeld. v. extension of mmff94 using experimental data,
additional computational data, and empirical rules. Journal of Computational Chemistry, 17(5-6):
616‚Äì641, 1996.
Paul CD Hawkins. Conformation generation: the state of the art. Journal of Chemical Information
and Modeling, 57(8):1747‚Äì1756, 2017.
10

Published as a conference paper at ICLR 2022
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiÔ¨Åed and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arXiv:2006.11239, 2020.
Weihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec,
Devi Parikh, and Larry Zitnick. Forcenet: A graph neural network for large-scale quantum
chemistry simulation. 2021.
John Ingraham, Adam J Riesselman, Chris Sander, and Debora S Marks. Learning protein structure
with a differentiable simulator. In International Conference on Learning Representations, 2019.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. arXiv preprint arXiv:1802.04364, 2018.
Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror.
Learning from protein structure with geometric vector perceptrons. In International Conference on
Learning Representations, 2021.
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin ≈Ω√≠dek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583‚Äì589, 2021.
Wolfgang Kabsch. A solution for the best rotation to relate two sets of vectors. Acta Crystallographica
Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography, 32(5):922‚Äì923,
1976.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, 2013.
Jonas K√∂hler, Leon Klein, and Frank Noe. Equivariant Ô¨Çows: Exact likelihood generative learning for
symmetric densities. In Proceedings of the 37th International Conference on Machine Learning,
2020.
Leo Liberti, Carlile Lavor, Nelson Maculan, and Antonio Mucherino. Euclidean distance geometry
and applications. SIAM review, 56(1):3‚Äì69, 2014.
Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. ArXiv,
abs/2103.01458, 2021.
Shitong Luo, Chence Shi, Minkai Xu, and Jian Tang. Predicting molecular conformation via dynamic
graph score matching. Advances in Neural Information Processing Systems, 34, 2021.
Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry
prediction using a deep generative graph neural network. arXiv preprint arXiv:1904.00314, 2019.
B. Miller, M. Geiger, T. Smidt, and F. No√©. Relevance of rotationally equivariant convolutions for
predicting molecular properties. ArXiv, abs/2008.08461, 2020.
Frank No√©, Simon Olsson, Jonas K√∂hler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457), 2019.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. ScientiÔ¨Åc data, 1(1):1‚Äì7, 2014.
Sereina Riniker and Gregory A. Landrum. Better informed distance geometry: Using what we know
to improve conformation generation. Journal of Chemical Information and Modeling, 55(12):
2562‚Äì2574, 2015.
Victor Garcia Satorras, Emiel Hoogeboom, Fabian B Fuchs, Ingmar Posner, and Max Welling. E (n)
equivariant normalizing Ô¨Çows for molecule generation in 3d. arXiv preprint arXiv:2105.09016,
2021a.
11

Published as a conference paper at ICLR 2022
Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks,
2021b.
Kristof Sch√ºtt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre
Tkatchenko, and Klaus-Robert M√ºller. Schnet: A continuous-Ô¨Ålter convolutional neural network
for modeling quantum interactions. In Advances in Neural Information Processing Systems, pp.
991‚Äì1001. Curran Associates, Inc., 2017.
Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin ≈Ω√≠dek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706‚Äì710, 2020.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
Ô¨Çow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382,
2020.
Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang.
Learning gradient Ô¨Åelds for molecular
conformation generation. ArXiv, 2021.
Muhammed Shuaibi, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi,
and C Lawrence Zitnick. Rotation invariant graph neural networks using spin convolutions. arXiv
preprint arXiv:2106.09575, 2021.
Gregor Simm and Jose Miguel Hernandez-Lobato. A generative model for molecular distance
geometry.
In Hal Daum√© III and Aarti Singh (eds.), Proceedings of the 37th International
Conference on Machine Learning, volume 119, pp. 8949‚Äì8958. PMLR, 2020.
Gregor N. C. Simm, Robert Pinsler, G√°bor Cs√°nyi, and Jos√© Miguel Hern√°ndez-Lobato. Symmetry-
aware actor-critic for 3d molecular design. In International Conference on Learning Representa-
tions, 2021.
Daniel G. A. Smith, L. Burns, A. Simmonett, R. Parrish, M. C. Schieber, Raimondas Galvelis,
P. Kraus, H. Kruse, Roberto Di Remigio, Asem Alenaizan, A. M. James, S. Lehtola, Jonathon P
Misiewicz, et al. Psi4 1.4: Open-source software for high-throughput quantum chemistry. The
Journal of chemical physics, 2020.
Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, pp. 11918‚Äì11930, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.
NeurIPS, 2020.
N. Thomas, T. Smidt, Steven M. Kearnes, Lusann Yang, L. Li, Kai Kohlhoff, and P. Riley. Tensor
Ô¨Åeld networks: Rotation- and translation-equivariant neural networks for 3d point clouds. ArXiv,
2018.
M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen. 3d steerable cnns: Learning
rotationally equivariant features in volumetric data. In NeurIPS, 2018.
Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, and Jian Tang. Learning neural generative
dynamics for molecular conformation generation. In International Conference on Learning
Representations, 2021a.
Minkai Xu, Wujie Wang, Shitong Luo, Chence Shi, Yoshua Bengio, Rafael Gomez-Bombarelli,
and Jian Tang. An end-to-end framework for molecular conformation generation via bilevel
programming. arXiv preprint arXiv:2105.07246, 2021b.
12

Published as a conference paper at ICLR 2022
Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and Weinan E. Deep Potential Molecular
Dynamics: A Scalable Model with the Accuracy of Quantum Mechanics. Physical Review Letters,
120(14):143001, 2018.
Jinhua Zhu, Yingce Xia, Chang Liu, Lijun Wu, Shufang Xie, Tong Wang, Yusong Wang, Wengang
Zhou, Tao Qin, Houqiang Li, et al. Direct molecular conformation generation. arXiv preprint
arXiv:2202.01356, 2022.
13

Published as a conference paper at ICLR 2022
A
PROOFS
A.1
PROPERTIES OF THE DIFFUSION MODEL
We include proofs for several key properties of the probabilistic diffusion model here to be self-
contained. For more detailed discussions, please refer to Ho et al. (2020). Let {Œ≤0, ..., Œ≤T } be a
sequence of variances, and Œ±t = 1 ‚àíŒ≤t and ¬ØŒ±t = Qt
s=1 Œ±s. The two following properties are crucial
for deriving the Ô¨Ånal tractable objective in equation 9.
Property 1. Tractable marginal of the forward process:
q(Ct|C0) =
Z
q(C1:t|C0) dC1:(t‚àí1) = N(Ct; ‚àö¬ØŒ±tC0, (1 ‚àí¬ØŒ±t)I).
Proof. Let œµi‚Äôs be independent standard Gaussian random variables. Then, by deÔ¨Ånition of the
Markov kernels q(Ct|Ct‚àí1) in equation 2, we have
Ct
= ‚àöŒ±tCt‚àí1 + ‚àöŒ≤tœµt
= ‚àöŒ±tŒ±t‚àí1Ct‚àí2 +
p
Œ±tŒ≤t‚àí1œµt‚àí1 + ‚àöŒ≤tœµt
= ‚àöŒ±tŒ±t‚àí1Œ±t‚àí1Ct‚àí3 +
p
Œ±tŒ±t‚àí1Œ≤t‚àí2œµt‚àí2 +
p
Œ±tŒ≤t‚àí1œµt‚àí1 + ‚àöŒ≤tœµt
= ¬∑ ¬∑ ¬∑
= ‚àö¬ØŒ±tC0 +
p
Œ±tŒ±t‚àí1 ¬∑ ¬∑ ¬∑ Œ±2Œ≤1œµ1 + ¬∑ ¬∑ ¬∑ +
p
Œ±tŒ≤t‚àí1œµt‚àí1 + ‚àöŒ≤tœµt
(12)
Therefore q(Ct|C0) is still Gaussian, and the mean of Ct is ‚àö¬ØŒ±tC0, and the variance matrix is
(Œ±tŒ±t‚àí1 ¬∑ ¬∑ ¬∑ Œ±2Œ≤1 + ¬∑ ¬∑ ¬∑ + Œ±tŒ≤t‚àí1 + Œ≤t)I = (1 ‚àí¬ØŒ±t)I. Then we have:
q(Ct|C0) = N(Ct; ‚àö¬ØŒ±tC0, (1 ‚àí¬ØŒ±t)I).
This property provides convenient closed-form evaluation of Ct knowing C0:
Ct = ‚àö¬ØŒ±tC0 +
‚àö
1 ‚àí¬ØŒ±tœµ,
where œµ ‚àºN(0, I).
Besides, it is worth noting that,
q(CT |C0) = N(CT ; ‚àö¬ØŒ±T C0, (1 ‚àí¬ØŒ±T )I),
where ¬ØŒ±T = QT
t=1(1 ‚àíŒ≤t) approaches zero with large T, which indicates the diffusion process can
Ô¨Ånally converge into a whitened noisy distribution.
Property 2. Tractable posterior of the forward process:
q(Ct‚àí1|Ct, C0) = N(Ct‚àí1;
‚àö¬ØŒ±t‚àí1Œ≤t
1 ‚àí¬ØŒ±t
C0 +
‚àöŒ±t(1 ‚àí¬ØŒ±t‚àí1)
1 ‚àí¬ØŒ±t
Ct, (1 ‚àí¬ØŒ±t‚àí1)
1 ‚àí¬ØŒ±t
Œ≤tI).
Proof. Let ÀúŒ≤t = 1‚àí¬ØŒ±t‚àí1
1‚àí¬ØŒ±t Œ≤t, then we can derive the posterior by Bayes rule:
q(Ct‚àí1|Ct, C0)
= q(Ct|Ct‚àí1) q(Ct‚àí1|C0)
q(Ct|C0)
= N(Ct; ‚àöŒ±tCt‚àí1, Œ≤tI) N(Ct‚àí1; ‚àö¬ØŒ±t‚àí1C0, (1 ‚àí¬ØŒ±t‚àí1)I)
N(Ct; ‚àö¬ØŒ±tC0, (1 ‚àí¬ØŒ±t)I)
= (2œÄŒ≤t)‚àíd
2 (2œÄ(1 ‚àí¬ØŒ±t‚àí1))‚àíd
2 (2œÄ(1 ‚àí¬ØŒ±t))
d
2 √ó
exp

‚àí‚à•Ct ‚àí‚àöŒ±tCt‚àí1‚à•2
2Œ≤t
‚àí‚à•Ct‚àí1 ‚àí‚àö¬ØŒ±t‚àí1C0‚à•2
2(1 ‚àí¬ØŒ±t‚àí1)
+ ‚à•Ct ‚àí‚àö¬ØŒ±tC0‚à•2
2(1 ‚àí¬ØŒ±t)

= (2œÄ ÀúŒ≤t)‚àíd
2 exp
 
‚àí1
2ÀúŒ≤t
Ct‚àí1 ‚àí
‚àö¬ØŒ±t‚àí1Œ≤t
1 ‚àí¬ØŒ±t
C0 ‚àí
‚àöŒ±t(1 ‚àí¬ØŒ±t‚àí1)
1 ‚àí¬ØŒ±t
Ct

2!
(13)
Then we have the posterior q(Ct‚àí1|Ct, C0) as the given form.
14

Published as a conference paper at ICLR 2022
A.2
PROOF OF PROPOSITION 1
Let Tg be some roto-translational transformations of a group element g ‚ààSE(3), and let p(xT ) be a
density which is SE(3)-invariant, i.e., p(xT ) = p(Tg(xT )). If the Markov transitions p(xt‚àí1|xt) are
SE(3)-equivariant, i.e., p(xt‚àí1|xt) = p(Tg(xt‚àí1)|Tg(xt)), then we have that the density pŒ∏(x0) =
R
p(xT )pŒ∏(x0:T ‚àí1|xT )dx1:T is also SE(3)-invariant.
Proof.
pŒ∏(Tg(x0)) =
Z
p(Tg(xT ))pŒ∏(Tg(x0:T ‚àí1)|Tg(xT ))dx1:T
=
Z
p(Tg(xT ))Œ†T
t=1pŒ∏(Tg(xt‚àí1)|Tg(xt))dx1:T
=
Z
p(xT )Œ†T
t=1pŒ∏(Tg(xt‚àí1)|Tg(xt))dx1:T
(invariant prior p(xT ))
=
Z
p(xT )Œ†T
t=1pŒ∏(xt‚àí1|xt)dx1:T
(equivariant kernels p(xt‚àí1|xt))
=
Z
p(xT )pŒ∏(x0:T ‚àí1|xT )dx1:T
= pŒ∏(x0)
(14)
A.3
PROOF OF PROPOSITION 2
In this section we prove that the output x of GFN deÔ¨Åned in equation 5, 6 and 7 is translationally
invariant and rotationally equivariant with the input C. Let g ‚ààR3 denote any translation transforma-
tions and orthogonal matrices R ‚ààR3√ó3 denote any rotation transformations. let Rx be shorthand
for (Rx1, ¬∑ ¬∑ ¬∑ , RxN). Formally, we aim to prove that the model satisÔ¨Åes:
Rxl+1, hl+1 = GFN(Rxl, RC + g, hl).
(15)
This equation indicates that, given xl already rotationally equivalent with C, and hl already invariant,
then such property can propagate through a single GFN layer to xl+1 and hl+1.
Proof. Firstly, given that hl already invariant to SE(3) transformations, we have that the messages
mij calculated from equation 5 will also be invariant. This is because it sorely relies on the
distance between two atoms, which are manifestly invariant to rotations ‚à•Rxl
i ‚àíRxl
j‚à•2 = (xl
i ‚àí
xl
j)‚ä§R‚ä§R(xl
i ‚àíxl
j) = (xl
i ‚àíxl
j)‚ä§I(xl
i ‚àíxl
j) = ‚à•xl
i ‚àíxl
j‚à•2. Formally, the invariance of messages
in equation 5 can be written as:
mi,j = Œ¶m

hl
i, hl
j,
Rxl
i ‚àíRxl
j
2 , eij

= Œ¶m

hl
i, hl
j,
xl
i ‚àíxl
j
2 , eij

.
(16)
And similarly, the ht+1 updated from equation 6 will also be invariant.
Next, we prove that the vector x updated from equation 7 preserves rotational equivariance and
translational invariance. Given mij already invariant as proven above, we have that:
X
j‚ààN(i)
1
dij
(Rci + g ‚àíRcj ‚àíg) Œ¶x (mi,j) = R
X
j‚ààN(i)
1
dij
(ci ‚àícj) Œ¶x (mi,j) = Rxl+1
i
.
(17)
Therefore, we have that rotating and translating c results in the same rotation and no translation on
xl+1 by updating through equation 7.
Thus we can conclude that the property deÔ¨Åned in equation 15 is satisÔ¨Åed.
Having proved the equivariance property of a single GFN layer, then inductively, we can draw
conclusion that a composition of L GFN layers will also preserve the same equivariance.
15

Published as a conference paper at ICLR 2022
A.4
PROOF OF PROPOSITION 3
We Ô¨Årst derive the variational lower bound (ELBO) objective in equation 8. The ELBO can be
calculated as follows:
E log pŒ∏(C0|G) = E log Eq(C1:T |C0)
hpŒ∏(C0:T ‚àí1|G, CT ) √ó p(CT )
q(C1:T |C0)
i
‚â•Eq log pŒ∏(C0:T ‚àí1|G, CT ) √ó p(CT )
q(C1:T |C0)
= Eq
h
log p(CT ) ‚àí
T
X
t=1
log pŒ∏(Ct‚àí1|G, Ct)
q(Ct|Ct‚àí1)
i
= Eq
h
log p(CT ) ‚àílog pŒ∏(C0|G, C1)
q(C1|C0)
‚àí
T
X
t=2

log pŒ∏(Ct‚àí1|G, Ct)
q(Ct‚àí1|Ct, C0) + log q(Ct‚àí1|C0)
q(Ct|C0)
i
= Eq
h
log
p(CT )
q(CT |C0) ‚àílog pŒ∏(C0|G, C1) ‚àí
T
X
t=2
log pŒ∏(Ct‚àí1|G, Ct)
q(Ct‚àí1|Ct, C0)
i
= ‚àíEq
h
KL
 q(CT |C0)‚à•p(CT )

+
T
X
t=2
KL
 q(Ct‚àí1|Ct, C0)‚à•pŒ∏(Ct‚àí1|G, Ct)

‚àílog pŒ∏(C0|G, C1)
i
.
(18)
It can be noted that the Ô¨Årst term KL
 q(CT |C0)‚à•p(CT )

is a constant, which can be omit-
ted in the objective.
Furthermore, for brevity, we also merge the Ô¨Ånal term log pŒ∏(C0|G, C1)
into the second term (sum over KL divergences),
and Ô¨Ånally derive that LELBO
=
PT
t=1 DKL(q(Ct‚àí1|Ct, C0)‚à•pŒ∏(Ct‚àí1|G, Ct)) as in equation 8.
Now we consider how to compute the KL divergences as the proposition 3. Since both q(Ct‚àí1|Ct, C0)
and pŒ∏(Ct‚àí1|G, Ct) are Gaussian share the same covariance matrix ÀúŒ≤tI, the KL divergence between
them can be calculated by the squared ‚Ñì2 distance between their means weighed by a certain weights
1
2 ÀúŒ≤t . By the expression of q(Ct|C0), we have the reparameterization that Ct = ‚àö¬ØŒ±tC0 + ‚àö1 ‚àí¬ØŒ±tœµ.
Then we can derive:
Eq KL
 q(Ct‚àí1|Ct, C0)‚à•pŒ∏(G, Ct‚àí1|Ct)

=
1
2ÀúŒ≤t
EC0

‚àö¬ØŒ±t‚àí1Œ≤t
1 ‚àí¬ØŒ±t
C0 +
‚àöŒ±t(1 ‚àí¬ØŒ±t‚àí1)
1 ‚àí¬ØŒ±t
Ct ‚àí
1
‚àöŒ±t

Ct ‚àí
Œ≤t
‚àö1 ‚àí¬ØŒ±t
œµŒ∏(Ct, G, t)

2
=
1
2ÀúŒ≤t
EC0,œµ

‚àö¬ØŒ±t‚àí1Œ≤t
1 ‚àí¬ØŒ±t
¬∑ Ct ‚àí‚àö1 ‚àí¬ØŒ±tœµ
‚àö¬ØŒ±t
+
‚àöŒ±t(1 ‚àí¬ØŒ±t‚àí1)
1 ‚àí¬ØŒ±t
Ct ‚àí
1
‚àöŒ±t

Ct ‚àí
Œ≤t
‚àö1 ‚àí¬ØŒ±t
œµŒ∏(Ct, G, t)

2
=
1
2ÀúŒ≤t
¬∑
Œ≤2
t
Œ±t(1 ‚àí¬ØŒ±t)EC0,œµ
0 ¬∑ Ct + œµ ‚àíœµŒ∏(Ct, G, t)
2
=
Œ≤2
t
2 1‚àí¬ØŒ±t‚àí1
1‚àí¬ØŒ±t Œ≤tŒ±t(1 ‚àí¬ØŒ±t)
EC0,œµ
œµ ‚àíœµŒ∏(Ct, G, t)
2
= Œ≥tEC0,œµ
œµ ‚àíœµŒ∏(Ct, t)
2 ,
(19)
where Œ≥t represent the wights
Œ≤t
2Œ±t(1‚àí¬ØŒ±t‚àí1). And we Ô¨Ånish the proof.
A.5
ANALYSIS OF THE INVARIANT DENSITY IN SEC. 4.2
Given a geometric system x ‚ààRN¬∑3, we obtain the CoM-free ÀÜx by subtracting its CoM. This can be
considered as a linear transformation:
ÀÜx = Qx, where Q = I3 ‚äó

IN ‚àí1
N 1N1T
N

(20)
where Ik denotes the k √ó k identity matrix and 1k denotes the k-dimensional vector Ô¨Ålled with ones.
It can be noted that Q is a symmetric projection operator, i.e., Q2 = Q and QT = Q. And we also
16

Published as a conference paper at ICLR 2022
have that rank[Q] = (N ‚àí1) ¬∑ 3. Furthermore, let U represent the space of CoM-free systems, we
can easily have that Qy = y for any y ‚ààU since the CoM of y is already zero.
Formally, let n = N ¬∑ 3 and set Rn with an isotropic normal distribution œÅ = N(0, In), then the
CoM-free density can be formally written as ÀÜœÅ = N(0, QInQT ) = N(0, QQT ). Thus, sampling
from ÀÜœÅ can be trivially achieved by sampling from œÅ and then projecting with Q. And ÀÜœÅ(y) can be
calculated by œÅ(y) since for any y ‚ààU we have ‚à•y‚à•2
2 = ‚à•Qy‚à•2
2, and thus œÅ(y) = ÀÜœÅ(y).
And in this paper, with the SE(3)-equivariant Markov kernels of the reverse process, any CoM-free
system will transit to another CoM-free system. And thus we can induce a well-deÔ¨Åned Markov chain
on the subspace spanned by Q.
B
OTHER RELATED WORK
Protein structure generation. There has also been many recent works working on protein structure
folding. For example, Boltzmann generators No√© et al. (2019) use Ô¨Çow-based models to generate
the structure of protein main chains. AlQuraishi (2019) uses recurrent networks to model the amino
acid sequences. Ingraham et al. (2019) proposed neural networks to learn an energy simulator to
infer the protein structures. Most recently, AlphaFold Senior et al. (2020); Jumper et al. (2021) has
signiÔ¨Åcantly improved the performance of protein structure generation. Nevertheless, proteins are
mainly linear backbone structures while general molecules are highly branched with various rings,
making protein folding approaches unsuitable for our setting.
Point cloud generation. Recently, some other works (Luo & Hu, 2021; Chibane et al., 2020) has
also been proposed for 3D structure generation with diffusion-based models, but focus on the point
cloud problem. Unfortunately, in general, point clouds are not considered as graphs with various
atom and bond information, and equivariance is also not widely considered, making these methods
fundamentally different from our model.
C
EXPERIMENT DETAILS
In this section, we introduce the details of our experiments. In practice, the means œµŒ∏ are parameterized
as compositions of both typical invariant MPNNs (Sch√ºtt et al., 2017) and the proposed equivariant
GFNs in Sec. 4.2. As a default setup, the MPNNs for parameterizing the means œµŒ∏ are all implemented
with 4 layers, and the hidden embedding dimension is set as 128. After the MPNNs, we can obtain
the informative invariant atom embeddings, which we denote as h0. Then the embeddings h0 are
fed into equivariant layers and updated with equation 5, equation 6, and equation 7 to obtain the
equivariant output. For the training of GEODIFF, we train the model on a single Tesla V100 GPU
with a learning rate of 0.001 until convergence and Adam (Kingma & Welling, 2013) as the optimizer.
The practical training time is ~48 hours. The other hyper-parameters of GEODIFF are summarized in
Tab. 4, including highest variance level Œ≤T , lowest variance level Œ≤T , the variance schedule, number
of diffusion timesteps T, radius threshold for determining the neighbor of atoms œÑ, batch size, and
number of training iterations.
Table 4: Additional hyperparameters of our GEODIFF.
Task
Œ≤1
Œ≤T
Œ≤ scheduler
T
œÑ
Batch Size
Train Iter.
QM9
1e-7
2e-3
sigmoid
5000
10√Ö
64
1M
Drugs
1e-7
2e-3
sigmoid
5000
10√Ö
32
1M
D
ADDITIONAL EXPERIMENTS
D.1
RESULTS FOR GEOM-QM9
The results on the GEOM-QM9 dataset are reported in Tab. 5.
17

Published as a conference paper at ICLR 2022
Table 5: Results on the GEOM-QM9 dataset, without FF optimization.
COV-R (%) ‚Üë
MAT-R (√Ö) ‚Üì
COV-P (%) ‚Üë
MAT-P (√Ö) ‚Üì
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median
CVGAE
0.09
0.00
1.6713
1.6088
-
-
-
-
GRAPHDG
73.33
84.21
0.4245
0.3973
43.90
35.33
0.5809
0.5823
CGCF
78.05
82.48
0.4219
0.3900
36.49
33.57
0.6615
0.6427
CONFVAE
77.84
88.20
0.4154
0.3739
38.02
34.67
0.6215
0.6091
GEOMOL
71.26
72.00
0.3731
0.3731
-
-
-
-
CONFGF
88.49
94.31
0.2673
0.2685
46.43
43.41
0.5224
0.5124
GEODIFF-A
90.54
94.61
0.2104
0.2021
52.35
50.10
0.4539
0.4399
GEODIFF-C
90.07
93.39
0.2090
0.1988
52.79
50.29
0.4448
0.4267
Table 6: Additional results on the GEOM-Drugs dataset, without FF optimization.
COV-R (%) ‚Üë
MAT-R (√Ö) ‚Üì
COV-P (%) ‚Üë
MAT-P (√Ö) ‚Üì
Models
Mean
Median
Mean
Median
Mean
Median
Mean
Median
GEODIFF (T=1000)
82.96
96.29
0.9525
0.9334
48.27
46.03
1.3205
1.2724
D.2
ABLATION STUDY WITH FEWER DIFFUSION STEPS
We also test our method with fewer diffusion steps. SpeciÔ¨Åcally, we test the setting with T = 1000,
Œ≤1 =1e-7 and Œ≤T =9e-3. The results on the more challenging Drugs dataset are shown in Tab. 6.
Compared with the results in Tab. 1, we can observe that when setting the diffusion steps as 1000,
though slightly weaker than the performance with 5000 decoding steps, the model can already
outperforms all existing baselines. Note that, the most competitive baseline CONFGF (Shi et al., 2021)
also requires 5000 sampling steps, which indicates that our model can achieve better performance
with fewer computational costs compared with the state-of-the-art method.
E
MORE VISUALIZATIONS
We provide more visualization of generated structures in Fig. 3. The molecules are chosen from the
test split of GEOM-Drugs dataset.
18

Published as a conference paper at ICLR 2022
Graph
Conformations
Figure 3: Visualization of drug-like conformations generated by GEODIFF.
19

