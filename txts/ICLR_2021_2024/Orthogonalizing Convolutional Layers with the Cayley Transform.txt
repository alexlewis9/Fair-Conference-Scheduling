Published as a conference paper at ICLR 2021
Orthogonalizing Convolutional Layers with the
Cayley Transform
Asher Trockman
Computer Science Department
Carnegie Mellon University
ashert@cs.cmu.edu
J. Zico Kolter
Computer Science Department
Carnegie Mellon University &
Bosch Center for AI
zkolter@cs.cmu.edu
Abstract
Recent work has highlighted several advantages of enforcing orthogonality in the
weight layers of deep networks, such as maintaining the stability of activations,
preserving gradient norms, and enhancing adversarial robustness by enforcing low
Lipschitz constants. Although numerous methods exist for enforcing the orthogo-
nality of fully-connected layers, those for convolutional layers are more heuristic
in nature, often focusing on penalty methods or limited classes of convolutions. In
this work, we propose and evaluate an alternative approach to directly parameterize
convolutional layers that are constrained to be orthogonal. Speciﬁcally, we pro-
pose to apply the Cayley transform to a skew-symmetric convolution in the Fourier
domain, so that the inverse convolution needed by the Cayley transform can be
computed eﬃciently. We compare our method to previous Lipschitz-constrained
and orthogonal convolutional layers and show that it indeed preserves orthogonal-
ity to a high degree even for large convolutions. Applied to the problem of certiﬁed
adversarial robustness, we show that networks incorporating the layer outperform
existing deterministic methods for certiﬁed defense against ℓ2-norm-bounded ad-
versaries, while scaling to larger architectures than previously investigated. Code
is available at https://github.com/locuslab/orthogonal-convolutions.
1
Introduction
Encouraging orthogonality in neural networks has proven to yield several compelling beneﬁts. For
example, orthogonal initializations allow extremely deep vanilla convolutional neural networks to be
trained quickly and stably (Xiao et al., 2018; Saxe et al., 2013). And initializations that remain closer
to orthogonality throughout training seem to learn faster and generalize better (Pennington et al.,
2017). Unlike Lipschitz-constrained layers, orthogonal layers are gradient-norm-preserving (Anil
et al., 2019), discouraging vanishing and exploding gradients and stabilizing activations (Rodríguez
et al., 2017). Orthogonality is thus a potential alternative to batch normalization in CNNs and
can help to remember long-term dependencies in RNNs (Arjovsky et al., 2016; Vorontsov et al.,
2017).
Constraints and penalty terms encouraging orthogonality can improve generalization in
practice (Bansal et al., 2018; Sedghi et al., 2018), improve adversarial robustness by enforcing low
Lipschitz constants, and allow deterministic certiﬁcates of robustness (Tsuzuku et al., 2018).
Despite evidence for the beneﬁts of orthogonality constraints, and while there are many methods to
orthogonalize fully-connected layers, the orthogonalization of convolutions has posed challenges.
More broadly, current Lipschitz-constrained convolutions rely on spectral normalization and kernel
reshaping methods (Tsuzuku et al., 2018), which only allow loose bounds and can cause vanishing
gradients. Sedghi et al. (2018) showed how to clip the singular values of convolutions and thus
enforce orthogonality, but relied on costly alternating projections to achieve tight constraints. Most
recently, Li et al. (2019) introduced the Block Convolution Orthogonal Parameterization (BCOP),
which cannot express the full space of orthogonal convolutions.
In contrast to previous work, we provide a direct, expressive, and scalable parameterization of
orthogonal convolutions. Our method relies on the Cayley transform, which is well-known for pa-
rameterizing orthogonal matrices in terms of skew-symmetric matrices, and can be easily extended to
1

Published as a conference paper at ICLR 2021
non-square weight matrices. The transform requires eﬃciently computing the inverse of a particular
convolution in the Fourier domain, which we show works well in practice.
We demonstrate that our Cayley layer is indeed orthogonal in practice when implemented in 32-bit
precision, irrespective of the number of channels. Further, we compare it to alternative convolu-
tional and Lipschitz-constrained layers: we include them in several architectures and evaluate their
deterministic certiﬁable robustness against an ℓ2-norm-bounded adversary. Our layer provides state-
of-the-art results on this task. We also demonstrate that the layers empirically endow a considerable
degree of robustness without adversarial training. Our layer generally outperforms the alternatives,
particularly for larger architectures.
2
Related Work
Orthogonality in neural networks. The beneﬁts of orthogonal weight initializations for dynamical
isometry, i.e., ensuring signals propagate through deep networks, are explained by Saxe et al. (2013)
and Pennington et al. (2017), with limited theoretical guarantees investigated by Hu et al. (2020).
Xiao et al. (2018) provided a method to initialize orthogonal convolutions, and demonstrated that
it allows the training of extremely deep CNNs without batch normalization or residual connections.
Further, Qi et al. (2020) developed a novel regularization term to encourage orthogonality throughout
training and showed its eﬀectiveness for training very deep vanilla networks. The signal-preserving
properties of orthogonality can also help with remembering long-term dependencies in RNNs, on
which there has been much work (Helfrich et al., 2018; Arjovsky et al., 2016).
One way to orthogonalize weight matrices is with the Cayley transform, which is often used in
Riemannian optimization (Absil et al., 2009). Helfrich et al. (2018) and Maduranga et al. (2019)
avoid vanishing/exploding gradients in RNNs using the scaled Cayley transform. Similarly, Lezcano-
Casado & Martínez-Rubio (2019) use the exponential map, which the Cayley transform approximates.
Li et al. (2020) derive an iterative approximation of the Cayley transform for orthogonally-constrained
optimizers and show it speeds the convergence of CNNs and RNNs. However, they merely orthog-
onalize a matrix obtained by reshaping the kernel, which is not the same as an orthogonal convo-
lution (Sedghi et al., 2018). Our contribution is unique here in that we parameterize orthogonal
convolutions directly, as opposed to reshaping kernels.
Bounding neural network Lipschitzness. Orthogonality imposes a strict constraint on the Lipschitz
constant, which itself comes with many beneﬁts: Lower Lipschitz constants are associated with
improved robustness (Yang et al., 2020) and better generalization bounds (Bartlett et al., 2017).
Tsuzuku et al. (2018) showed that neural network classiﬁcations can be certiﬁed as robust to ℓ2-
norm-bounded perturbations given a Lipschitz bound and suﬃciently conﬁdent classiﬁcations. Along
with Szegedy et al. (2013), they noted that the Lipschitz constant of neural networks can be bounded
if the constants of the layers are known. Thus, there is substantial work on Lipschitz-constrained
and regularized layers, which we review in Sec. 5. However, Anil et al. (2019) realized that mere
Lipschitz constraints can attenuate gradients, unlike orthogonal layers.
There have been other ideas for calculating and controlling the minimal Lipschitzness of neural
networks, e.g., through regularization (Hein & Andriushchenko, 2017), extreme value theory (Weng
et al., 2018), or using semi-deﬁnite programming (Latorre et al., 2020; Chen et al., 2020; Fazlyab
et al., 2019), but constructing bounds from Lipschitz-constrained layers is more scalable and ef-
ﬁcient. Besides Tsuzuku et al. (2018)’s strategy for deterministic certiﬁable robustness, there are
many approaches to deterministically verifying neural network defenses using SMT solvers (Huang
et al., 2017; Ehlers, 2017; Carlini & Wagner, 2017), integer programming approaches (Lomuscio &
Maganti, 2017; Tjeng & Tedrake, 2017; Cheng et al., 2017), or semi-deﬁnite programming (Raghu-
nathan et al., 2018). Wong et al. (2018)’s approach to minimize an LP-based bound on the robust loss
is more scalable, but networks made from Lipschitz-constrained components can be more eﬃcient
still, as shown by Li et al. (2019) who outperform their approach. However, none of these methods
yet perform as well as probabilistic methods (Cohen et al., 2019).
Consequently, orthogonal layers appear to be an important component to enhance the convergence
of deep networks while encouraging robustness and generalization.
2

Published as a conference paper at ICLR 2021
3
Background
Orthogonality. Since we are concerned with orthogonal convolutions, we review orthogonal matri-
ces: A matrix Q ∈Rn×n is orthogonal if QT Q = QQT = I. However, in building neural networks,
layers do not always have equal input and output dimensions: more generally, a matrix U ∈Rm×n is
semi-orthogonal if U T U = I or UU T = I. Importantly, if m ≥n, then U is also norm-preserving:
∥Ux∥2 = ∥x∥2 for all x ∈Rn. If m < n, then the mapping is merely non-expansive (a contraction),
i.e., ∥Ux∥2 ≤∥x∥2. A matrix having all singular values equal to 1 is orthogonal, and vice versa.
Orthogonal convolutions.
The same concept of orthogonality applies to convolutional layers,
which are also linear transformations. A convolutional layer conv : Rc×n×n →Rc×n×n with
c = cin = cout input and output channels is orthogonal if and only if ∥conv(X)∥F = ∥X∥F for all
input tensors X ∈Rc×n×n; the notion of semi-orthogonality extends similarly for cin ̸= cout. Note
that orthogonalizing each convolutional kernel as in Lezcano-Casado & Martínez-Rubio (2019);
Lezcano-Casado (2019) does not yield an orthogonal (norm-preserving) convolution.
Lipschitzness under the ℓ2 norm. A consequence of orthogonality is 1-Lipschitzness. A function
f : Rn →Rm is L-Lipschitz with respect to the ℓ2 norm iﬀ∥f(x) −f(y)∥2 ≤L∥x −y∥2 for all
x, y ∈Rn. If L is the smallest such constant for f, then it’s called the Lipschitz constant of f, denoted
by Lip(f). An useful property for certiﬁable robustness is that the Lipschitz constant of the com-
position of f and g is upper-bounded by the product of their constants: Lip(f ◦g) ≤Lip(f)Lip(g).
Since simple neural networks are fundamentally just composed functions, this allows us to bound
their Lipschitz constants, albeit loosely. We can extend this idea to residual networks using the
fact that Lip(f + g) ≤Lip(f) + Lip(g), which motivates using a convex combination in residual
connections. More details can be found in Li et al. (2019); Szegedy et al. (2013).
Lipschitz bounds for provable robustness. If we know the Lipschitz constant of the neural network,
we can certify that a classiﬁcation with suﬃciently a large margin is robust to ℓ2 perturbations below
a certain magnitude. Speciﬁcally, denote the margin of a classiﬁcation with label t as
Mf(x) = max(0, yt −max
i̸=t yi),
(1)
which can be interpreted as the distance between the correct logit and the next largest logit. Then if
the logit function f has Lipschitz constant L, and Mf(x) >
√
2Lϵ, then f(x) is certiﬁably robust
to perturbations {δ : ∥δ∥2 ≤ϵ}. Tsuzuku et al. (2018) and Li et al. (2019) provide proofs.
4
The Cayley transform of a Convolution
Before describing our method, we ﬁrst review discrete convolutions and the Cayley transform;
then, we show the need for inverse convolutions and how to compute them eﬃciently in the Fourier
domain, which lets us parameterize orthogonal convolutions via the Cayley transform. The key
idea in our method is that multi-channel convolution in the Fourier domain reduces to a batch
of matrix-vector products, and making each of those matrices orthogonal makes the convolution
orthogonal.
We describe our method in more detail in Appendix A and provide a minimal
implementation in PyTorch in Appendix E.
An unstrided convolutional layer with cin input channels and cout output channels has a weight tensor
W of shape Rcout×cin×n×n and takes an input X of shape Rcin×n×n to produce an output Y of shape
Rcout×n×n, i.e., convW : Rcin×n×n →Rcout×n×n. It is easiest to analyze convolutions when they are
circular: if the kernel goes out of bounds of X, it wraps around to the other side—this operation can
be carried out eﬃciently in the Fourier domain. Consequently, we focus on circular convolutions.
We deﬁne convW (X) as the circular convolutional layer with weight tensor W ∈Rcout×cin×n×n
applied to an input tensor X ∈Rcin×n×n yielding an output tensor Y = convW (X) ∈Rcout×n×n.
Equivalently, we can view convW (X) as the doubly block-circulant matrix C ∈Rcoutn2×cinn2
corresponding to the circular convolution with weight tensor W applied to the unrolled input tensor
vec X ∈Rcinn2×1. Similarly, we denote by convT
W (X) the transpose CT of the same convolution,
which can be obtained by transposing the ﬁrst two channel dimensions of W and ﬂipping each of
the last two (kernel) dimensions vertically and horizontally, calling the result W ′, and computing
convW ′(X). We denote conv−1
W (X) as the inverse of the convolution, i.e., with corresponding
matrix C−1, which is more diﬃcult to eﬃciently compute.
3

Published as a conference paper at ICLR 2021
Now we review how to perform a convolution in the spatial domain. We refer to a pixel as a cin
or cout-dimensional slice of a tensor, like X[:, i, j]. Each of the n2 (i, j) output pixels Y [:, i, j] are
computed as follows: for each c ∈[cout], compute Y [c, i, j] by centering the tensor W[c] on the
(i, j)th pixel of the input and taking a dot product, wrapping around pixels of W that go out-of-
bounds. Typically, W is zero except for a k × k region of the last two (spatial) dimensions, which we
call the kernel or the receptive ﬁeld. Typically, convolutional layers have small kernels, e.g., k = 3.
Considering now matrices instead of tensors, the Cayley transform is a bijection between skew-
symmetric matrices A and orthogonal matrices Q without −1 eigenvalues:
Q = (I −A)(I + A)−1.
(2)
A matrix is skew-symmetric if A = −AT , and we can skew-symmetrize any square matrix B by
computing the skew-symmetric part A = B −BT . The Cayley transform of such a skew-symmetric
matrix is always orthogonal, which can be seen by multiplying Q by its transpose and rearranging.
We can also apply the Cayley transform to convolutions, noting they are also linear transformations
that can be represented as doubly block circulant matrices. While it is possible to construct the matrix
C corresponding to a convolution convW and apply the Cayley transform to it, this is highly ineﬃcient
in practice: Convolutions can be easily skew-symmetrized by computing convW (X) −convT
W (X),
but ﬁnding their inverse is challenging; instead, we manipulate convolutions in the Fourier domain,
taking advantage of the convolution theorem and the eﬃciency of the fast Fourier transform.
According to the 2D convolution theorem (Jain, 1989), the circular convolution of two matrices in
the Fourier domain is simply their elementwise product. We will show that the convolution theorem
extends to multi-channel convolutions of tensors, in which case convolution reduces to a batch of
complex matrix-vector products rather than elementwise products: inverting these smaller matrices
is equivalent to inverting the convolution, and ﬁnding their skew-Hermitian part is equivalent to
skew-symmetrizing the convolution, which allows us to compute the Cayley transform.
We deﬁne the 2D Discrete (Fast) Fourier Transform for tensors of order ≥2 as a mapping FFT :
Rm1×...×mr×n×n →Cm1×...×mr×n×n deﬁned by FFT(X)[i1, ..., ir] = FnX[i1, ..., ir]Fn for
il ∈1, ..., ml and l ∈1, ..., r and r ≥0, where Fn[i, j] =
1
√n exp( −2πı
n )(i−1)(j−1). That is, we
treat all but the last two dimensions as batch dimensions. We denote ˜X = FFT(X) for a tensor X.
Using the convolution theorem, in the Fourier domain the cth output channel is the sum of the elemen-
twise products of the cin input and weight channels: that is, ˜Y [c] = Pcin
k=1 ˜W[c, k] ⊙˜X[k]. Equiva-
lently, working in the Fourier domain, the (i, j)th pixel of the cth output channel is the dot product
of the (i, j)th pixel of the cth weight with the (i, j)th input pixel: ˜Y [c, i, j] = ˜W[c, :, i, j] · ˜X[:, i, j].
From this, we can see that the whole (i, j)th Fourier-domain output pixel is the matrix-vector product
FFT(convW (X))[:, i, j] = ˜W[:, :, i, j] ˜X[:, i, j].
(3)
This interpretation gives a way to compute the inverse convolution as required for the Cayley
transform, assuming cin = cout:
FFT(conv−1
W (X))[:, i, j] = ˜W[:, :, i, j]−1 ˜X[:, i, j].
(4)
Given this method to compute inverse convolutions, we can now parameterize an orthogonal convo-
lution with a skew-symmetric convolution through the Cayley transform, highlighted in Algorithm 1:
In line 1, we use the Fast Fourier Transform on the weight and input tensors. In line 4, we com-
pute the Fourier domain weights for the skew-symmetric convolution (the Fourier representation
is skew-Hermitian, thus the use of the conjugate transpose). Next, in lines 4–5 we compute the
inverses required for FFT(conv−1
I+A(x)) and use them to compute the Cayley transform written as
(I+A)−1−A(I+A)−1 in line 6. Finally, we get our spatial domain result with the inverse FFT, which
is always exactly real despite working with complex matrices in the Fourier domain (see Appendix A).
4.1
Properties of our approach
It is important to note that the inverse in the Cayley transform always exists: Because A is skew-
symmetric, it has all imaginary eigenvalues, so I + A has all nonzero eigenvalues and is thus
nonsingular. Since only square matrices can be skew-symmetrized and inverted, Algorithm 1 only
4

Published as a conference paper at ICLR 2021
Algorithm 1: Orthogonal convolution via the Cayley transform.
Input: A tensor X ∈Rcin×n×n and convolution weights W ∈Rcout×cin×n×n, with cin = cout.
Output: A tensor Y ∈Rcout×n×n, the orthogonal convolution parameterized by W applied to X.
1 ˜W := FFT(W) ∈Ccout×cin×n×n, ˜X := FFT(X) ∈Ccin×n×n
2 for all i, j ∈1, . . . , n // In parallel
3 do
4
˜A[:, :, i, j] := ˜W[:, :, i, j] −˜W[:, :, i, j]∗
5
˜Y [:, i, j] := (I + ˜A[:, :, i, j])−1 ˜X[:, i, j]
6
˜Z[:, i, j] := ˜Y [:, i, j] −˜A[:, :, i, j] ˜Y [:, i, j]
7 end
8 return FFT−1( ˜Z).real
works for cin = cout, but can be extended to the rectangular case where cout ≥cin by padding the
matrix with zeros and then projecting out the ﬁrst cin columns after the transform, resulting in a
norm-preserving semi-orthogonal matrix; the case cin ≥cout follows similarly, but the resulting
matrix is merely non-expansive. With eﬃcient implementation in terms of the Schur complement
(Appendix A.1, Eq. A22), this only requires inverting a square matrix of order min(cin, cout).
We saw that learning was easier if we parameterized W in Algorithm 1 by W = gV/∥V ∥F for a
learnable scalar g and tensor V , as in weight normalization (Salimans & Kingma, 2016).
Comparison to BCOP. While the Block Convolution Orthogonal Parameterization (BCOP) can only
express orthogonal convolutions with ﬁxed k × k-sized kernels, a Cayley convolutional layer can
represent orthogonal convolutions with a learnable kernel size up to the input size, and it does this
without costly projections unlike Sedghi et al. (2018). However, our parameterization as presented
is limited to orthogonal convolutions without -1 eigenvalues. Hence, our parameterization is incom-
plete; besides kernel size restrictions, BCOP was also demonstrated to incompletely represent the
space of orthogonal convolutions, though the details of the problem were unresolved (Li et al., 2019).
Our method can represent such orthogonal convolutions by multiplying the Cayley transform by a
ﬁxed diagonal matrix with ±1 entries (Gallier, 2006; Helfrich et al., 2018); however, we cannot
optimize over the discrete set of such scaling matrices, so our method cannot optimize over all
orthogonal convolutions, nor all special orthogonal convolutions. In our experiments, we did not
ﬁnd improvements from adding randomly initialized scaling matrices as in Helfrich et al. (2018).
Limitations of our method. As our method requires computing an inverse convolution, it is generally
incompatible with strided convolutions; e.g., a convolution with stride 2 cannot be inverted since it
involves noninvertible downsampling. It is possible to apply our method to stride-2 convolutions
by simultaneously increasing the number of output channels by 4× to compensate for the 2×
downsampling of the two spatial dimensions, though we found this to be computationally ineﬃcient.
Instead, we use the invertible downsampling layer from (Jacobsen et al., 2018) to emulate striding.
The convolution resulting from our method is circular, which is the same as using the circular padding
mode instead of zero padding in, e.g., PyTorch, and will not have a large impact on performance if
subjects tend to be centered in images in the data set. BCOP (Li et al., 2019) and Sedghi et al. (2018)
also restricted their attention to circular convolutions.
Our method is substantially more expensive than plain convolutional layers, though in most practical
settings it is more eﬃcient than existing work: We plot the runtimes of our Cayley layer, BCOP, and
plain convolutions in a variety of settings in Figure 6 for comparison, and we also report runtimes in
Tables 4 and 5 (see Appendix C).
Runtime comparison Our Cayley layer does cincout FFTs on n×n matrices (i.e., the kernels padded
to the input size), and cin FFTs for each n × n input. These have complexity O(cincoutn2 log n) and
O(coutn2 log n) respectively. The most expensive step is computing the inverse of n2 square matrices
of order c = min(cin, cout), with complexity O(n2c3), similarly to the method of Sedghi et al. (2018).
We note like the authors that parallelization could eﬀectively make this O(n2 log n + c3), and it
is quite feasible in practice. As in Li et al. (2020), the inverse could be replaced with an iterative
approximation, but we did not ﬁnd it necessary for our relatively small architectures.
5

Published as a conference paper at ICLR 2021
For comparison, the related layers BCOP and RKO (Sec. 5) take only O(c3) to orthogonalize the
convolution, and OSSN takes O(n2c3) (Li et al., 2019). In practice, we found our Cayley layer takes
anywhere from 1/2× to 4× as long as BCOP, depending on the architecture (see Appendix C).
5
Experiments
Our experiments have two goals: First, we show that our layer remains orthogonal in practice.
Second, we compare the performance of our layer versus alternatives (particularly BCOP) on two
adversarial robustness tasks on CIFAR-10: We investigate the certiﬁable robustness against an
ℓ2-norm-bounded adversary using the idea of Lipschitz Margin Training (Tsuzuku et al., 2018),
and then we look at robustness in practice against a powerful adversary. We ﬁnd that our layer is
always orthogonal and performs relatively well in the robustness tasks. Separately, we show our
layer improves on the Wasserstein distance estimation task from Li et al. (2019) in Appendix D.2.
For alternative layers, we adopt the naming scheme for previous work on Lipschitz-constrained
convolutions from Li et al. (2019), and we compare directly against their implementations. We
outline the methods below.
RKO. A convolution can be represented as a matrix-vector product, e.g., using a doubly block-
circulant matrix and the unrolled input. Alternatively, one could stack each k ×k receptive ﬁeld, and
multiply by the cout × k2cin reshaped kernel matrix (Cisse et al., 2017). The spectral norm of this
reshaped matrix is bounded by the convolution’s true spectral norm (Tsuzuku et al., 2018). Conse-
quently, reshaped kernel methods orthogonalize this reshaped matrix, upper-bounding the singular
values of the convolution by 1. Cisse et al. (2017) created a penalty term based on this matrix; instead,
like Li et al. (2019), we orthogonalize the reshaped matrix directly, called reshaped kernel orthog-
onalization (RKO). They used an iterative algorithm for orthogonalization (Björck & Bowie, 1971);
for comparison, we implement RKO using the Cayley transform instead of Björck orthogonalization,
called CRKO.
OSSN. A prevalent idea to constrain the Lipschitz constants of convolutions is to approximate the
maximum singular value and normalize it out: Miyato et al. (2018) used the power method on
the matrix W associated with the convolution, i.e., si+1 := W T Wsi, and σmax ≈∥Wsn∥/∥sn∥.
Gouk et al. (2018) improved upon this idea by applying the power method directly to convolutions,
using the transposed convolution for W T . However, this one-sided spectral normalization is quite
restrictive; dividing out σmax can make other singular values vanishingly small.
SVCM. Sedghi et al. (2018) showed how to exactly compute the singular values of convolutional
layers using the Fourier transform before the SVD, and proposed a singular value clipping method.
However, the clipped convolution can have an arbitrarily large kernel size, so they resorted to
alternating projections between orthogonal convolutions and k × k-kernel convolutions, which can
be expensive. Like Li et al. (2019), we found that ≈50 projections are needed for orthogonalization.
BCOP. The Block Convolution Orthogonal Parameterization extends the orthogonal initialization
method of Xiao et al. (2018). It diﬀerentiably parameterizes k × k orthogonal convolutions with an
orthogonal matrix and 2(k −1) symmetric projection matrices. The method only parameterizes the
subspace of orthogonal convolutions with k × k-sized kernels, but is quite expressive empirically.
Internally, orthogonalization is done with the method by Björck & Bowie (1971).
Note that BCOP and SVCM are the only other orthogonal convolutional layers, and SVCM only for
a large number of projections. RKO, CRKO, and OSSN merely upper-bound the Lipschitz constant
of the layer by 1.
5.1
Training and Architectural Details
Training details. For all experiments, we used CIFAR-10 with standard augmentation, i.e., random
cropping and ﬂipping. Inputs to the model are always in the range [0, 1]; we implement normalization
as a layer for compatibility with AutoAttack. For each architecture/convolution pair, we tried learning
rates in {10−5, 10−4, 10−3, 10−2, 10−1}, choosing the one with the best test accuracy. Most often,
0.001 is appropriate. We found that a piecewise triangular learning rate, as used in top performers in
the DAWNBench competition (Coleman et al., 2017), performed best. Adam (Kingma & Ba, 2014)
showed a signiﬁcant improvement over plain SGD, and we used it for all experiments.
6

Published as a conference paper at ICLR 2021
    0     
0.5
     1    
Conv(x) 2 / x 2
0
20
40
60
80
100
256
64
Before
Training
After
Training
(a)
1-1e-6
1
1+1e-6    
Conv(x) 2 / x 2
3
64
Before
Training
After
Training
(b)
1-1e-6
1
1+1e-6    
Conv(x) 2 / x 2
64
128
Before
Training
After
Training
(c)
1-1e-6
1
1+1e-6    
Conv(x) 2 / x 2
128
128
Before
Training
After
Training
(d)
1-1e-6
1
1+1e-6    
Conv(x) 2 / x 2
512
512
Before
Training
After
Training
(e)
Figure 1: (Titles: cin →cout). Our layer remains orthogonal in practice even for large convolutions
(d, e), and is norm-preserving even when cout > cin (b, c); it is nonexpansive when cin > cout (a).
Loss function. Inspired by Tsuzuku et al. (2018), Anil et al. (2019) and Li et al. (2019) used
multi-class hinge loss where the margin is the robustness certiﬁcate
√
2Lϵ0. We corroborate their
ﬁnding that this works better than cross-entropy, and similarly use ϵ0 = 0.5. Varying ϵ0 controls a
tradeoﬀbetween accuracy and robustness (see Fig. 5).
Initialization. We found that the standard uniform initialization in PyTorch performed well for our
layer. We adjusted the variance, but signiﬁcant diﬀerences required order-of-magnitude changes.
For residual networks, we tried Fixup initialization (Zhang et al., 2019), but saw no signiﬁcant
improvement. We hypothesize this is due to (1) the learnable scaling parameter inside the Cayley
transform, which changes signiﬁcantly during training and (2) the dynamical isometry inherent with
orthogonal layers. For alternative layers, we used the initializations from Li et al. (2019).
Architecture considerations. For fair comparison with previous work, we use the “large” network
from Li et al. (2019), which was ﬁrst implemented in Kolter & Wong (2017)’s work on certiﬁable
robustness. We also compare the performance of the diﬀerent layers in a 1-Lipschitz-constrained
version of ResNet9 (He et al., 2016) and WideResNet10-10 (Zagoruyko & Komodakis, 2016). The
architectures we could investigate were limited by compute and memory, as all the layers compared are
relatively expensive. For RKO, OSSN, SVCM, and BCOP, we use Björck orthogonalization (Björck
& Bowie, 1971) for fully-connected layers, as reported in Li et al. (2019); Anil et al. (2019). For our
Cayley convolutional layer and CRKO, we orthogonalize the fully-connected layers with the Cayley
transform to be consistent with our method. We found the gradient-norm-preserving GroupSort
activation function from Anil et al. (2019) to be more eﬀective than ReLU, and we used a group size
of 2, i.e., MaxMin.
Strided convolutions. For the KWLarge network, we used “invertible downsampling”, which em-
ulates striding by rearranging the inputs to have 4× more channels while halving the two spatial
dimensions and reducing the kernel size to ⌊k/2⌋(Jacobsen et al., 2018). For the residual networks,
we simply used a version of pooling, noting that average pooling is still non-expansive when multi-
plied by its kernel size, which allows us to use more of the network’s capacity. We also halved the
kernel size of the last pooling layer, instead adding another fully-connected layer; empirically, this
resulted in higher local Lipschitz constants.
Ensuring Lipschitz constraints. Batch normalization layers scale their output, so they can’t be
included in our 1-Lipschitz-constrained architecture; the gradient-norm-preserving properties of
our layers compensate for this. We ensure residual connections are non-expansive by making them a
convex combination with a new learnable parameter α, i.e., g(x) = αf(x)+(1−α)x, for α ∈[0, 1].
To ensure the latter constraint, we use sigmoid(α). We can tune the overall Lipschitz bound to a
given L using the Lipschitz composition property, multiplying each of the m layers by L1/m.
5.2
Adversarial Robustness
For certiﬁable robustness, we report the fraction of certiﬁable test points: i.e., those with classiﬁcation
margin Mf(x) greater than
√
2Lϵ, where ϵ = 36/255. For empirical defense, we use both vanilla
projected gradient descent and AutoAttack by Croce & Hein (2020). For PGD, we use α = ϵ/4.0 with
10 iterations. Within AutoAttack, we use both APGD-CE and APGD-DLR, ﬁnding the decision-
based attacks provided no improvements. We report on ϵ = 36/255 for consistency with Li et al.
(2019) and previous work on deterministic certiﬁable robustness (Wong et al., 2018). Additionally,
7

Published as a conference paper at ICLR 2021
KWLarge: Trained for provable robustness
ϵ
Test Acc.
Cayley
BCOP
RKO
CRKO
OSSN
SVCM
.85·Cayley
0
Clean
75.33±.41
75.11±.37
74.47±.28
73.92±.27 71.69±.34 72.43±.84
74.35±.33
36
255
PGD
67.66±.31
67.29±.35
68.32±.22
68.03±.28 65.13±.10 66.43±.62
67.29±.52
AutoAttack
65.13±.48
64.62±.31
66.10±.26
65.95±.25 62.92±.16 64.27±.67
65.00±.58
Certiﬁed
59.16±.36
58.29±.19
57.50±.17
57.48±.34 55.71±.57 52.11±.90
59.99±.40
Emp.Lip
0.740±.01
0.740±.02
0.667±.01
0.668±.01 0.716±.01 0.570±.02
0.648±.01
Table 1: Trained without normalizing inputs, mean/s.d. from 5 experiments reported. Our Cayley
layer outperforms other methods in both test and ℓ2 certiﬁable robust accuracy.
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
0
10
20
30
40
50
60
70
80
90
100
% Accuracy
Cayley / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
BCOP / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
RKO / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
CRKO / KWLarge
Train
Test
Cert. 36
Cert. 64
Cert. 128
Est. L
Figure 2: The provable robustness vs. clean accuracy tradeoﬀenabled by scaling the Lipschitz
upper-bound for KWLarge.
we found it useful to report on empirical local Lipschitz constants throughout training using the
PGD-like method from Yang et al. (2020).
5.3
Results
Practical orthogonality. We show that our layer remains very close to orthogonality in practice, both
before and after learning, when implemented in 32-bit precision. We investigated Cayley layers from
one of our ResNet9 architectures, running them on random tensors to see if their norm is preserved,
which is equivalent to orthogonality. We found that ∥Conv(x)∥/∥x∥, the extent to which our layer
is gradient norm preserving, is always extremely close to 1. We illustrate the small discrepancies,
easily bounded between 0.99999 and 1.00001, in Figure 1. Cayley layers which do not change or
increase the number of channels are guaranteed to be orthogonal, which we see in practice for graphs
(b, c, d, e). Those which decrease the number of channels can only be non-expansive, and in fact the
layer seems to become slightly more norm-preserving after training (a). In short, our Cayley layer
can capture the full beneﬁts of orthogonality.
Certiﬁable robustness. We use our layer and alternatives within the KWLarge architecture for
a more direct comparison to previous work on deterministic certifable robustness (Li et al., 2019;
Wong et al., 2018). As in (Li et al., 2019), we got the best performance without normalizing inputs,
and can thus say that all networks compared here are at most 1-Lipschitz.
Our layer outperforms BCOP on this task (see Table 1), and is thus state-of-the-art, getting on average
75.33% clean test accuracy and 59.16% certiﬁable robust accuracy against adversarial perturbations
with norm less than ϵ = 36/255. In contrast, BCOP gets 75.11% test accuracy and 58.29% certiﬁable
robust accuracy. The reshaped kernel methods perform only a percent or two worse on this task,
while the spectral normalization and clipping methods lag behind.
We assumed that a layer is only meaningfully better than the other if both the test and robust accuracy
are improved; otherwise, the methods may simply occupy diﬀerent parts of the tradeoﬀcurve. Since
reshaped kernel methods can encourage smaller Lipschitz constants than orthogonal layers (Sedghi
et al., 2018), we investigated the clean vs. certiﬁable robust accuracy tradeoﬀenabled by scaling
the Lipschitz upper bound of the network, visualized in Figure 2.
To that end, in light of the
competitiveness of RKO, we chose a Lipschitz upper-bound of 0.85 which gave our Cayley layer
similar test accuracy; this allowed for even higher certiﬁable robustness of 59.99%, but lower test
8

Published as a conference paper at ICLR 2021
ResNet9
WideResNet10-10
ϵ
Test Acc.
Cayley
BCOP
RKO
CRKO
Cayley BCOP RKO CRKO
0
Clean
81.70±.12
80.72±.18 80.06±.15 79.38±.18
82.99
81.39 81.50
78.81
36
255
PGD
73.77±.19
73.27±.18 73.37±.12 72.52±.11
76.02
74.56 74.72
72.28
AutoAttack
71.17±.20
70.50±.06 71.01±.13 70.10±.07
73.16
71.86 72.24
69.97
Table 2: Empirical adversarial robustness for residual networks, mean and standard deviation for
ResNet9 from 3 experiments. Cayley layers perform competitively on clean and robust accuracy.
accuracy of 74.35%. Overall, we were surprised by the similarity between the four top-performing
methods after scaling Lipschitz constants.
We were not able to improve certiﬁable accuracy with ResNets. However, it was useful to increase
the kernel size: we found 5 was an improvement in accuracy, while 7 and 9 were slightly worse.
(Since our method operates in the Fourier domain, increases in kernel size incur no extra cost.)
We also saw an improvement from scaling up the width of each layer of KWLarge, and our Cayley
layer was substantially faster than BCOP as the width of KWLarge increased (see Appendix C).
Multiplying the width by 3 and increasing the kernel size to 5, we were able to get 61.13% certiﬁed
robust accuracy with our layer, and 60.55% with BCOP.
Empirical robustness.
Previous work has shown that adversarial robustness correlates with
lower Lipschitz constants. Thus, we investigated the robustness endowed by our layer against ℓ2
gradient-based adversaries. Here, we got better accuracy with the standard practice of normalizing
inputs. Our layer outperformed the others in ResNet9 and WideResNet10-10 architectures; results
were less decisive for KWLarge (see Appendix B). For the WideResNet, we got 82.99% clean
accuracy and 73.16% robust accuracy for ϵ = 36/255. For comparison, the state-of-the-art achieves
91.08% clean accuracy and 72.91% robust accuracy for ϵ = 0.5 using a ResNet50 with adversarial
training and additional unlabeled data (Augustin et al., 2020). We visualize the tradeoﬀs for our
residual networks in Figure 3, noting that they empirically have smaller local Lipschitz constants
than KWLarge. While our layer outperforms others for the default Lipschitz bound of 1, and is con-
sistently slightly better than BCOP, RKO can perform similarly well for larger bounds. This provides
some support for studies showing that hard constraints like ours may not match the performance of
softer constraints, such as RKO and penalty terms (Bansal et al., 2018; Vorontsov et al., 2017).
6
Conclusion
In this paper, we presented a new, expressive parameterization of orthogonal convolutions using
the Cayley transform. Unlike previous approaches to Lipschitz-constrained convolutions, ours gives
deep networks the full beneﬁts of orthogonality, such as gradient norm preservation. We showed
empirically that our method indeed maintains a high degree of orthogonality both before and after
learning, and also scales better to some architectures than previous approaches. Using our layer,
we were able to improve upon the state-of-the-art in deterministic certiﬁable robustness against an
ℓ2-norm-bounded adversary, and also showed that it endows networks with considerable inherent
robustness empirically. While our layer oﬀers beneﬁts theoretically, we observed that heuristics
involving orthogonalizing reshaped kernels were also quite eﬀective for empirical robustness. Or-
thogonal convolutions may only show their true advantage in gradient norm preservation for deeper
networks than we investigated. In light of our experiments in scaling the Lipschitz bound, we hy-
pothesize that not orthogonality, but insead the ability of layers such as ours to exert control over
the Lipschitz constant, may be best for the robustness/accuracy tradeoﬀ. Future work may avoid
expensive inverses using approximations or the exponential map, or compare various orthogonal and
Lipschitz-constrained layers in the context of very deep networks.
Acknowledgments
We thank Shaojie Bai, Chun Kai Ling, Eric Wong, and the anonymous reviewers for helpful feedback
and discussions. This work was partially supported under DARPA grant number HR00112020006.
9

Published as a conference paper at ICLR 2021
References
P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms on matrix manifolds.
Princeton University Press, 2009.
Cem Anil, James Lucas, and Roger Grosse.
Sorting out lipschitz function approximation.
In
International Conference on Machine Learning, pp. 291–301, 2019.
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks,
2016.
Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on in- and
out-distribution improves explainability, 2020.
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep networks? In Advances in Neural Information Processing Systems, pp.
4261–4271, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240–6249, 2017.
Åke Björck and Clazett Bowie.
An iterative algorithm for computing the best estimate of an
orthogonal matrix. SIAM Journal on Numerical Analysis, 8(2):358–364, 1971.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017.
Tong Chen, Jean B Lasserre, Victor Magron, and Edouard Pauwels. Semialgebraic optimization
for lipschitz constants of relu networks. Advances in Neural Information Processing Systems, 33,
2020.
Chih-Hong Cheng, Georg Nührenberg, and Harald Ruess. Maximum resilience of artiﬁcial neural
networks. In International Symposium on Automated Technology for Veriﬁcation and Analysis,
pp. 251–268. Springer, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certiﬁed adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Cody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi Nardi, Peter Bailis,
Kunle Olukotun, Chris Ré, and Matei Zaharia. Dawnbench: An end-to-end deep learning bench-
mark and competition. Training, 100(101):102, 2017.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. arXiv preprint arXiv:2003.01690, 2020.
Ruediger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In Interna-
tional Symposium on Automated Technology for Veriﬁcation and Analysis, pp. 269–286. Springer,
2017.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Eﬃcient
and accurate estimation of lipschitz constants for deep neural networks. In Advances in Neural
Information Processing Systems, pp. 11427–11438, 2019.
Jean Gallier. Remarks on the cayley representation of orthogonal matrices and on perturbing the
diagonal of a matrix to make it invertible. arXiv preprint math/0606320, 2006.
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. arXiv preprint arXiv:1804.04368, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.
10

Published as a conference paper at ICLR 2021
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classiﬁer
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266–2276, 2017.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pp. 1969–1978, 2018.
Wei Hu, Lechao Xiao, and Jeﬀrey Pennington. Provable beneﬁt of orthogonal initialization in
optimizing deep linear networks. In International Conference on Learning Representations, 2020.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety veriﬁcation of deep neural
networks. In International Conference on Computer Aided Veriﬁcation, pp. 3–29. Springer, 2017.
Jörn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.
arXiv preprint arXiv:1802.07088, 2018.
Anil K Jain. Fundamentals of digital image processing. Prentice-Hall, Inc., 1989.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex outer
adversarial polytope. arXiv preprint arXiv:1711.00851, 1(2):3, 2017.
Fabian Latorre, Paul Rolland, and Volkan Cevher. Lipschitz constant estimation of neural networks
via sparse polynomial optimization. arXiv preprint arXiv:2004.08688, 2020.
Mario Lezcano-Casado. Trivializations for gradient-based optimization on manifolds. arXiv preprint
arXiv:1909.09501, 2019.
Mario Lezcano-Casado and David Martínez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. arXiv preprint arXiv:1901.08428,
2019.
Jun Li, Li Fuxin, and Sinisa Todorovic. Eﬃcient riemannian optimization on the stiefel manifold via
the cayley transform. arXiv preprint arXiv:2002.01113, 2020.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jörn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. In Advances in
neural information processing systems, pp. 15390–15402, 2019.
Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward relu
neural networks. arXiv preprint arXiv:1706.07351, 2017.
Kehelwala DG Maduranga, Kyle E Helfrich, and Qiang Ye. Complex unitary recurrent neural
networks using scaled cayley transform. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 33, pp. 4528–4535, 2019.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019.
Jeﬀrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems, pp. 4785–4795, 2017.
11

Published as a conference paper at ICLR 2021
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In International Conference on Machine Learning, pp. 7824–7835. PMLR,
2020.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certiﬁed defenses against adversarial exam-
ples. arXiv preprint arXiv:1801.09344, 2018.
Pau Rodríguez, Jordi Gonzàlez, Guillem Cucurull, Josep M. Gonfaus, and Xavier Roca. Regularizing
cnns with locally constrained decorrelations, 2017.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing systems, pp. 901–
909, 2016.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. arXiv
preprint arXiv:1805.10408, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vincent Tjeng and Russ Tedrake. Verifying neural networks with mixed integer programming. arXiv
preprint arXiv:1711.07356, pp. 945–950, 2017.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certiﬁcation
of perturbation invariance for deep neural networks. In Advances in neural information processing
systems, pp. 6541–6550, 2018.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. arXiv preprint arXiv:1702.00071, 2017.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
arXiv preprint arXiv:1801.10578, 2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400–8409, 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeﬀrey Penning-
ton. Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaud-
huri. A closer look at accuracy vs. robustness, 2020.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Fuzhen Zhang. The Schur complement and its applications, volume 4. Springer Science & Business
Media, 2006.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. arXiv preprint arXiv:1901.09321, 2019.
12

Published as a conference paper at ICLR 2021
A
Orthogonalizing Convolutions in the Fourier Domain
Our method relies on the fact that a multi-channel circular convolution can be block-diagonalized by
a suitable Discrete Fourier Transform matrix. We show how this follows from the 2D convolution
theorem (Jain, 1989, p. 145) below.
Deﬁnition A.1. Fn is the DFT matrix for sequences of length n; we drop the subscript when it can
be inferred from context.
Deﬁnition A.2. We deﬁne convW (X) as in Section 4; if cin = cout = 1, we drop the channel axes,
i.e., for X, W ∈Rn×n, the 2D circular convolution of X with W is convW (X) ∈Rn×n.
Theorem A.1. If C ∈Rn2×n2 represents a 2D circular convolution with weights W ∈Rn×n
operating on a vectorized input vec(X) ∈Rn2×1, with X ∈Rn×n, then it can be diagonalized as
(F ⊗F)C(F ∗⊗F ∗) = D.
Proof. According to the 2D convolution theorem, we can implement a single-channel 2D circular
convolution by computing the elementwise product of the DFT of the ﬁlter and input signals:
FWF ⊙FXF = FconvW (X)F.
(A1)
This elementwise product is easier to work mathematically with if we represent it as a diagonal-
matrix-vector product after vectorizing the equation:
diag(vec(FWF)) vec(FXF) = vec(FconvW (X)F).
(A2)
We can then rearrange this using vec(ABC) = (CT ⊗A) vec(B) and the symmetry of F:
diag(vec(FWF))(F ⊗F) vec(X) = (F ⊗F) vec(convW (X)).
(A3)
Left-multiplying by the inverse of F ⊗F and noting C vec(X) = vec(convW (X)), we get the result
(F ∗⊗F ∗) diag(vec(FWF))(F ⊗F) = C
⇒
diag(vec(FWF)) = (F ⊗F)C(F ∗⊗F ∗),
(A4)
which shows that the (doubly-block-ciculant) matrix C is diagonalized by F ⊗F. An alternate proof
can be found in Jain (1989, p. 150).
Now we can consider the case where we have a 2D circular convolution C ∈Rcoutn2×cinn2 with cin
input channels and cout output channels. Here, C has cout × cin blocks, each of which is a circular
convolution Cij ∈Rn2×n2. The input image is vec X =

vecT X1, . . . , vecT Xcin
T ∈Rcinn2×1,
where Xi is the ith channel of X.
Corollary A.1.1. If C ∈Rcoutn2×cinn2 represents a 2D circular convolution with cin input channels
and cout output channels, then it can be block diagonalized as FcoutCF∗
cin = D, where Fc =
Sc,n2 (Ic ⊗(F ⊗F)), Sc,n2 is a permutation matrix, Ik is the identity matrix of order k, and D is
block diagonal with n2 blocks of size cout × cin.
Proof. We ﬁrst look at each of the blocks of C individually, referring to ˆD as the block matrix before
applying the S permutations, i.e., ˆD = ST
cout,n2DScin,n2, so that:
ˆDij = [(Icout ⊗(F ⊗F)) C (Icin ⊗(F ∗⊗F ∗))]ij = (F ⊗F)Cij(F ∗⊗F ∗)
= diag(vec(FWijF)),
(A5)
where Wij are the weights of the (ij)th single-channel convolution, using Theorem A.1. That is, ˆD
is a block matrix of diagonal matrices. Then, let Sa,b be the perfect shuﬄe matrix that permutes the
block matrix of diagonal matrices to a block diagonal matrix. Sa,b can be constructed by subselecting
rows of the identity matrix. Using slice notation:
Sa,b =


Iab(1 : b : ab, :)
Iab(2 : b : ab, :)
...
Iab(b : b : ab, :)

.
(A6)
13

Published as a conference paper at ICLR 2021
As an example:
S2,4


a 0 0 0 e 0 0 0 i 0 0 0
0 b 0 0 0 f 0 0 0 j 0 0
0 0 c 0 0 0 g 0 0 0 k 0
0 0 0 d 0 0 0 h 0 0 0 l
m 0 0 0 q 0 0 0 u 0 0 0
0 n 0 0 0 r 0 0 0 v 0 0
0 0 o 0 0 0 s 0 0 0 w 0
0 0 0 p 0 0 0 t 0 0 0 x


|
{z
}
ˆ
D
ST
3,4 =


a e i 0 0 0 0 0 0 0 0 0
m q u 0 0 0 0 0 0 0 0 0
0 0 0 b f j 0 0 0 0 0 0
0 0 0 n r v 0 0 0 0 0 0
0 0 0 0 0 0 c g k 0 0 0
0 0 0 0 0 0 o s w 0 0 0
0 0 0 0 0 0 0 0 0 d h l
0 0 0 0 0 0 0 0 0 p t x


|
{z
}
D
.
(A7)
Then, with the perfect shuﬄe matrix, we can compute the block diagonal matrix D as:
Scout,n2 ˆDST
cin,n2 = Scout,n2 (Icout ⊗(F ⊗F)) C (Icin ⊗(F ∗⊗F ∗)) ST
cin,n2
= FcoutCF∗
cin = D.
(A8)
The eﬀect of left and right-multiplying with the perfect shuﬄe matrix is to create a new matrix D
from ˆD such that [Dk]ij = [ ˆDij]kk, where the subscript inside the brackets refers to the kth diagonal
block and the (ij)th block respectively.
Remark. It is much more simple to compute D (here wfft) in tensor form given the convolution
weights w as a cout × cin × n × n tensor:
wfft = fft2(w).reshape(cout, cin, n**2).permute(2, 0, 1).
Deﬁnition A.3. The Cayley transform is a bijection between skew-Hermitian matrices and unitary
matrices; for real matrices, it is a bijection between skew-symmetric and orthogonal matrices. We
apply the Cayley transform to an arbitrary matrix by ﬁrst computing its skew-Hermitian part: we
deﬁne the function cayley : Cm×m →Cm×m by cayley(B) = (Im −B + B∗)(Im + B −B∗)−1,
where we compute the skew-Hermitian part of B inline as B −B∗. Note that the Cayley transform of
a real matrix is always real, i.e., Im(B) = 0 ⇒Im(cayley(B)) = 0, in which case B−B∗= B−BT
is a skew-symmetric matrix.
We now note a simple but important fact that we will use to show that our convolutions are always
exactly real despite manipulating their complex representations in the Fourier domain.
Lemma A.2. Say J ∈Cm×m is unitary so that J∗J = I, and B = J ˜BJ∗for B ∈Rm×m and
˜B ∈Cm×m. Then cayley(B) = Jcayley( ˜B)J∗.
Proof. First note that B = J ˜BJ∗implies BT = B∗= (J ˜BJ∗)∗= J ˜B∗J∗. Then
cayley(B) = (I −B + BT )(I + B −BT ) = (I −J ˜BJ∗+ J ˜B∗J∗)(I + J ˜BJ∗−J ˜B∗J∗)−1
= J(I −˜B + ˜B∗)J∗h
J(I + ˜B −˜B∗)J∗i−1
= J(I −˜B + ˜B∗)J∗h
J(I + ˜B −˜B∗)−1J∗i
= J(I −˜B + ˜B∗)(I + ˜B −˜B∗)−1J∗
= Jcayley( ˜B)J∗.
(A9)
For the rest of this section, we drop the subscripts of F and S when they can be inferred from context.
Theorem A.3. When cin = cout = c, applying the Cayley transform to the block diagonal matrix D
results in a real, orthogonal multi-channel 2D circular convolution:
cayley(C) = F∗cayley(D)F.
Proof. Note that F is unitary:
FF∗= S(Ic ⊗(F ⊗F))(Ic ⊗(F ∗⊗F ∗))ST = SIcn2ST = SST = Icn2,
(A10)
since S is a permutation matrix and is thus orthogonal. Then apply Lemma A.2, where we have
J = F∗, B = C, and ˜B = D, to see the result. Note that cayley(C) is real because C is real; that is,
even though we apply the Cayley transform to skew-Hermitian matrices in the Fourier domain, the
resulting convolution is real.
14

Published as a conference paper at ICLR 2021
Remark. While we deal with skew-Hermitian matrices in the Fourier domain, we are still eﬀec-
tively parameterizing the Cayley transform in terms of skew-symmetric matrices: as in the note in
Lemma A.2, we can see that
C = F∗DF ⇒C −CT = C −C∗= F∗DF −F∗D∗F = F∗(D −D∗)F,
(A11)
where C is real, D is complex, and C −CT is skew-symmetric (in the spatial domain) despite
computing it with a skew-Hermitian matrix D −D∗in the Fourier domain.
Remark. Since D is block diagonal, we only need to apply the Cayley transform (and thus invert) its
n2 blocks of size c × c, which are much smaller than the whole matrix:
cayley(D) = diag(cayley(D1), . . . , cayley(Dn2)).
(A12)
A.1
Semi-Orthogonal Convolutions
In many cases, convolutional layers do not have cin = cout, in which case they cannot be orthogonal.
Rather, we must resort to enforcing semi-orthogonality. We can semi-orthogonalize convolutions
using the same techniques as above.
Lemma A.4. Right-padding the multi-channel 2D circular convolution matrix C (from cin to cout
channels) with dn2 columns of zeros is equivalent to padding each diagonal block of the correspond-
ing block-diagonal matrix D on the right with d columns of zeros:
[C
0dn2] = F∗diag([D1
0d] , . . . , [Dn2
0d])F,
(A13)
where 0k refers to k columns of zeros and a compatible number of rows.
Proof. For a ﬁxed column j, note that
[Dk]ij = 0 for all i, k ⇐⇒[ ˆDij]kk = 0 for all i, k ⇐⇒Cij = 0 for all i,
(A14)
since ˆDij = (F ⊗F)Cij(F ∗⊗F ∗) = 0 only when Cij = 0. Apply this for j = cin+1, . . . , cin+d.
Lemma A.5. Projecting out d blocks of columns of C is equivalent to projecting out d columns of
each of the diagonal blocks of D:
C

Idn2
0

= F∗diag

D1

Id
0

, . . . , Dn2

Id
0

F
(A15)
Proof. This proceeds similarly to the previous lemma: removing columns of each of the n2 matrices
D1, . . . , Dn2 implies removing the corresponding blocks of columns of ˆD, and thus of C.
Theorem A.6. If C is a 2D multi-channel convolution with cin ≤cout, then letting d = cout −cin,
cayley ([C
0dn2])

Idn2
0

=
F∗diag

cayley ([D1
0d])

Id
0d

, . . . , cayley ([Dn2
0d])

Id
0d

F,
(A16)
which is a real 2D multi-channel semi-orthogonal circular convolution.
Proof. For the ﬁrst step, we use Lemma A.4 for right padding, getting
[C
0dn2] = F∗diag([D1
0d] , . . . , [Dn2
0d])F.
(A17)
Then, noting that [C
0dn2] is a convolution matrix with cin = cout, we can apply Theorem A.3 (and
the following remark) to get:
cayley ([C
0dn2]) = F∗diag (cayley ([D1
0d]) , . . . , cayley ([Dn2
0d])) F.
(A18)
Since cayley ([C
0dn2]) is still a real convolution matrix, we can apply Lemma A.5 to get the
result.
15

Published as a conference paper at ICLR 2021
This demonstrates that we can semi-orthogonalize convolutions with cin ̸= cout by ﬁrst padding
them so that cin = cout; despite performing padding, the Cayley transform, and projections on
complex matrices in the Fourier domain, we have shown that the resulting convolution is still real.
In practice, we do not literally perform padding nor projections; we explain how to do an equivalent
but more eﬃcient comptutation on each diagonal block Dk ∈Ccout×cin below.
Proposition A.7. We can eﬃciently compute the Cayley transform for semi-orthogonalization, i.e.,
cayley ([W
0d])
 Id
0d

, when cin ≤cout by writing the inverse in terms of the Schur complement.
Proof. We can partition W ∈Ccout×cin into its top part U ∈Ccin×cin and bottom part V ∈
C(cout−cin)×cin, and then write the padded matrix [W
0cout−cin] ∈Ccout×cout as
[W
0cout−cin] = [ U 0
V 0 ].
(A19)
Taking the skew-Hermitian part and applying the Cayley transform, then projecting, we get:
cayley ([ U 0
V 0 ])
h
Icin
0
i
=
 Icout −[ U 0
V 0 ] + [ U 0
V 0 ]∗  Icout + [ U 0
V 0 ] −[ U 0
V 0 ]∗−1 h
Icin
0
i
=
h Icin−U+U ∗
V ∗
−V
Icout−cin
ih Icin+U−U ∗
−V ∗
V
Icout−cin
i−1h
Icin
0
i
.
(A20)
We focus on computing the inverse while keeping only the ﬁrst cin columns. We use the inversion
formula noted in Zhang (2006, p. 13) for a block partitioned matrix M,
M −1h
Icin
0
i
=
 P Q
R S
−1h
Icin
0
i
=
h
(M/S)−1
−(M/S)−1QS−1
−S−1R(M/S)−1 S−1+S−1R(M/S)−1QS−1
ih
Icin
0
i
=
h
(M/S)−1
−S−1R(M/S)−1
i
,
(A21)
where we assume M takes the form of the inverse in Eq. A20, and M/S = P −QS−1R is the Schur
complement. Using this formula for the ﬁrst cin columns of the inverse in Eq. A20, and computing
the Schur complement Icin + U −U ∗+ V ∗I−1
cout−cinV , we ﬁnd
cayley ([ U 0
V 0 ]) =
h Icin−U+U ∗
V ∗
−V
Icout−cin
ih
(Icin+U−U ∗+V ∗V )−1
−V (Icin+U−U ∗+V ∗V )−1
i
=
h (Icin−U+U ∗−V ∗V )(Icin+U−U ∗+V ∗V )−1
−2V (Icin+U−U ∗+V ∗V )−1
i
∈Ccout×cin,
(A22)
which is semi-orthogonal and requires computing only one inverse of size cin ≤cout. Note that this
inverse always exists because U −U ∗is skew-Hermitian, so it has purely imaginary eigenvalues,
and V ∗V is positive semideﬁnite and has all real non-negative eigenvalues.
That is, the sum
Icin + U −U ∗+ V ∗V has all nonzero eigenvalues and is thus nonsingular.
Proposition A.8. We can also compute semi-orthogonal convolutions when cin ≥cout using the
method described above because cayley ([ CT 0 ])T = cayley ([ C
0 ]).
Proof. We use that (A−1)T = (AT )−1 and (I −A)(I + A)−1 = (I + A)−1(I −A) to see
cayley ([ C
0 ])T =

I −[ C
0 ] + [ C
0 ]T  
I + [ C
0 ] −[ C
0 ]T −1T
=

I + [ C
0 ]T −[ C
0 ]
−1 
I −[ C
0 ]T + [ C
0 ]

= cayley

[ C
0 ]T 
= cayley ([ CT 0 ]) .
(A23)
We have thus shown how to (semi-)orthogonalize real multi-channel 2D circular convolutions eﬃ-
ciently in the Fourier domain. A minimal implementation of our method can be found in Appendix E.
The techniques described above could also be used with other orthogonalization methods, or for cal-
culating the determinants or singular values of convolutions.
16

Published as a conference paper at ICLR 2021
B
Additional Results
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
0
10
20
30
40
50
60
70
80
90
100
% Accuracy
Cayley / ResNet9
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
BCOP / ResNet9
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
RKO / ResNet9
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
CRKO / ResNet9
Train
Test
PGD (36)
PGD (64)
PGD (128)
Est. L
Figure 3: The robustness/accuracy tradeoﬀfrom changing the Lipschitz upper bound for ResNet9.
KWLarge: Trained for test & PGD accuracy
ϵ
Test Acc.
Cayley
BCOP
RKO
CRKO
OSSN
SVCM
.85·Cayley
0
Clean
79.87±.33
80.14±.32
79.65±.20
79.53±.22 78.66±.21 79.19±.30
79.55±.19
36
255
PGD
66.09±.47
65.31±.55
68.66±.20
68.55±.22 68.08±.20 68.35±.26
66.75±.34
AutoAttack 62.02±.60
60.63±.72
65.16±.14
65.40±.28 64.90±.17 64.92±.15
62.67±.32
Certiﬁed
38.67±.23
36.92±.30
34.67±.32
35.87±.43 36.29±.65 29.92±1.1
41.97±.23
Emp.Lip
2.245±.04
2.272±.03
2.891±.02
1.866±.04 1.923±.06 1.698±.12
1.999±.03
Table 3: Trained with normalizing inputs, mean and standard deviation from 5 experiments reported.
The normalization layer increases the Lipschitz bound of the network to ≈4.1, viz., CIFAR-10
standard deviation.
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
0
10
20
30
40
50
60
70
80
90
100
% Accuracy
Cayley / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
BCOP / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
RKO / KWLarge
4
3
2
1 0
1
2
3
4
5
6
7
log2(Lipschitz UB)
CRKO / KWLarge
Train
Test
PGD (36)
PGD (64)
PGD (128)
Est. L
Figure 4: Robustness vs. accuracy tradeoﬀfrom changing the Lipschitz upper bound for KWLarge.
For KWLarge, our results on empirical robustness were mixed: while our Cayley layer outperforms
BCOP in robust accuracy, the RKO methods are overall more robust by around 2%, for only a
marginal decrease in clean accuracy.
We note the lower empirical local Lipschitzness of RKO
methods, which may explain their higher robustness: Figure 4 shows that the best choice of Lipschitz
upper-bound for Cayley and BCOP layers may be less than 1 for this architecture.
3
4
5
6
7
8
9
10
11
log2(Margin
0)
0
10
20
30
40
50
60
70
80
90
100
% Accuracy
Cayley / KWLarge
Train
Test
PGD (36)
PGD (64)
PGD (128)
Est. L
Figure 5: Eﬀect of varying ϵ0 for Lipschitz margin training for KWLarge.
17

Published as a conference paper at ICLR 2021
C
Empirical runtimes
KWLarge Empirical Runtimes
↓Layer
Width Mult. →
1
2
3
6
8
Cayley
Test Acc.
75.97
76.97
76.94
76.80
78.28
Certiﬁed
59.51
60.86
61.13
60.37
61.03
Avg. sec/Epoch
9.01
17.50
32.30
133.43
260.31
BCOP
Test Acc.
75.28
75.81
76.35
76.55
–.–
Certiﬁed
58.63
59.34
59.69
59.45
–.–
Avg. sec/Epoch
20.75
53.72
135.95
1050.61
–.–
RKO
Test Acc.
74.85
75.74
76.05
76.29
–.–
Certiﬁed
57.59
58.74
59.07
58.69
–.–
Avg. sec/Epoch
16.02
50.15
131.43
1034.96
–.–
Table 4: Here we multiplied the input channels and output channels of each layer of KWLarge by
width; we report on changes in accuracy and average runtime per epoch (100 epochs). Width 1 was
on a Nvidia RTX 2080 Ti, while 2, 3, 6, and 8 were on a Nvidia Quadro RTX 8000. In this case,
we were unable to scale BCOP to width 8 due to time and memory constraints. Generally, the wider
networks may need more epochs to converge.
Avg. sec/Epoch
Architecture
Cayley
BCOP
RKO
Plain Conv.
Both Plain
ResNet9
43.92
48.04
45.73
14.56
13.27
WideResNet10-10
210.6
109.4
99.46
48.40
46.10
Table 5: Our Cayley layer was not as fast for residual networks, possibly because they have convo-
lutions with more channels and also larger spatial dimension, which is a multiplicative factor in our
runtime analysis. This is especially true for the WideResNet. For plain conv, we replaced the Cayley
convolutional layer with a plain circular convolution, leaving the Cayley fully-connected layers. For
both plain, we also used plain fully-connected layers.
18

Published as a conference paper at ICLR 2021
0
100
200
300
400
500
# Channels (in & out)
0
20
40
60
80
Avg. ms (CUDA)
Runtime vs. # Channels, n = 8
(a)
0
100
200
300
400
500
# Channels (in & out)
0
100
200
 
 
Runtime vs. # Channels, n = 16
Cayley
BCOP
Plain
(b)
0
50
100
150
200
250
Spatial Size (n)
0
50
100
Avg. ms (CUDA)
Runtime vs. Spatial Size,  cin = cout = 16
(c)
0
50
100
150
200
250
Spatial Size (n)
0
100
200
300
 
 
Runtime vs. Spatial Size,  cin = cout = 32
Cayley
BCOP
Plain
(d)
2
4
6
8
10
12
14
16
Kernel Size (k)
0
100
200
300
400
Avg. ms (CUDA)
Runtime vs. Kernel Size,  cin = cout = 32, n = 16
(e)
0
100
200
300
400
500
# Output Channels (cout)
0
10
20
30
 
 
Runtime vs. # Output Channels,  cin = 32, n = 16
Cayley
BCOP
Plain
(f)
Figure 6: Our Cayley layer is particulaly eﬃcient for inputs with small spatial dimension (width
and height, i.e., n) (see (a), (c)), large kernel size k (see (e)), and cases where the number of input
and output channels are not equal (cin ̸= cout) (see (f)). For very large spatial size (image width and
height) (see (d)), or the combination of relatively large spatial size and many channels (see (b)),
BCOP (Li et al., 2019) tends to be more eﬃcient. Since convolutional layers in neural networks tend
to decrease the spatial dimensionality while increasing the number of channels, and also often have
unequal numbers of input and output channels, our Cayley layer is often more eﬃcient in practice.
In all cases, orthogonal convolutional layers are signiﬁcantly slower than plain convolutional layers.
Each runtime was recorded using the autograd proﬁler in PyTorch (Paszke et al., 2019) by
summing the CUDA execution times. The batch size was ﬁxed at 128 for all graphs, and each data
point was averaged over 32 iterations. We used a Nvidia Quadro RTX 8000.
19

Published as a conference paper at ICLR 2021
D
Additional Baseline Experiments
D.1
Robustness Experiments
KWLarge
ϵ
Test Acc.
CayleyBCOP
0
Clean
74.35±.29
36
255
PGD
66.91±.10
AutoAttack
64.36±.17
Certiﬁed
57.94±.22
Emp.Lip
0.732±.01
Table 6: Additional baseline for KWLarge
trained for provable robustness. Mean and
s.d. over 5 trials.
ResNet9
ϵ
Test Acc.
Plain Conv.
CayleyBCOP
0
Clean
89.22±.18
80.05±.20
36
255
PGD
70.86±.06
72.58±.25
AutoAttack
68.20±.10
69.90±.35
Table 7: Additional baselines for ResNet9 trained
for empirical adversarial robustness.
Mean and
s.d. over 3 trials.
The main competing orthogonal convolutional layer, BCOP (Li et al., 2019), uses Björck (Björck &
Bowie, 1971) orthogonalization for internal parameter matrices; they also used it in their experiments
for orthogonal fully-connected layers. Similarly to how we replaced the method in RKO with the
Cayley transform for our CRKO (Cayley RKO) experiments, we replaced Björck with the Cayley
transform in BCOP and used a Cayley linear layer for CayleyBCOP experiments, reported in Tables 6
and 7. We see slightly decreased performance over all metrics, similarly to the relationship between
RKO and CRKO.
For additional comparison, we also report on a plain convolutional baseline in Table 7. For this
model, we used a plain circular convolutional layer and a Cayley linear layer, which still imparts a
considerable degree of robustness. With the plain convolutional layer, the model gains a considerable
degree of accuracy but loses some robustness. We did not report a plain convolutional baseline for
the provable robustness experiments on KWLarge, as it would require a more sophisticated technique
to bound the Lipschitz constants of each layer, which is outside the scope of our investigation.
D.2
Wasserstein Distance Estimation
Cayley
BCOP
RKO
OSSN
Wasserstein Distance:
10.72
10.08
9.18
7.50
Table 8: For BCOP, RKO and OSSN, we report the best bound over all trials from the experiments
in the repository containing BCOP’s implementation (Li et al., 2019). We ran one trial of the
Wasserstein GAN experiment, replacing the BCOP and Björck layers with our Cayley convolutional
and linear layers, and achieved a signiﬁcantly tighter bound. We only report on experiments using
the GroupSort (MaxMin) activation (Anil et al., 2019) and on the STL-10 dataset.
We repeated the Wasserstein distance estimation experiment from Li et al. (2019), simply replacing
the BCOP layer with our Cayley convolutional layer, and the Björck linear layer with our Cayley
fully-connected layer. We took the best Wasserstein distance bound from one trial of each of the four
learning rates considered in BCOP (0.1, 0.01, 0.001, 0.0001); see Table 8.
20

Published as a conference paper at ICLR 2021
E
Example Implementations
In PyTorch 1.8, our layer can be implemented as follows.
def cayley(W):
if len(W.shape) == 2:
return cayley(W[None])[0]
_, cout, cin = W.shape
if cin > cout:
return cayley(W.transpose(1, 2)).transpose(1, 2)
U, V = W[:, :cin], W[:, cin:]
I = torch.eye(cin, dtype=W.dtype, device=W.device)[None, :, :]
A = U - U.conj().transpose(1, 2) + V.conj().transpose(1, 2) @ V
inv = torch.inverse(I + A)
return torch.cat((inv @ (I - A), -2 * V @ inv), axis=1)
class CayleyConv(nn.Conv2d):
def fft_shift_matrix(self, n, s):
shift = torch.arange(0, n).repeat((n, 1))
shift = shift + shift.T
return torch.exp(2j * np.pi * s * shift / n)
def forward(self, x):
cout, cin, _, _ = self.weight.shape
batches, _, n, _ = x.shape
if not hasattr(self, "shift_matrix"):
s = (self.weight.shape[2] - 1) // 2
self.shift_matrix = self.fft_shift_matrix(n, -s)[:, :(n//2 + 1)] \
.reshape(n * (n // 2 + 1), 1, 1).to(x.device)
xfft = torch.fft.rfft2(x).permute(2, 3, 1, 0) \
.reshape(n * (n // 2 + 1), cin, batches)
wfft = self.shift_matrix * torch.fft.rfft2(self.weight, (n, n)) \
.reshape(cout, cin, n * (n // 2 + 1)).permute(2, 0, 1).conj()
yfft = (cayley(wfft) @ xfft).reshape(n, n // 2 + 1, cout, batches)
y = torch.fft.irfft2(yfft.permute(3, 2, 0, 1))
if self.bias is not None:
y += self.bias[:, None, None]
return y
To make the layer support stride-2 convolutions, have CayleyConv inherit from the following class
instead, which depends on the einops package:
class StridedConv(nn.Conv2d):
def __init__(self, *args, **kwargs):
if "stride" in kwargs and kwargs["stride"] == 2:
args = list(args)
args[0] = 4 * args[0] # 4x in_channels
args[2] = args[2] // 2 # //2 kernel_size; optional
args = tuple(args)
super().__init__(*args, **kwargs)
downsample = "b c (w k1) (h k2) -> b (c k1 k2) w h"
self.register_forward_pre_hook(lambda _, x: \
einops.rearrange(x[0], downsample, k1=2, k2=2) \
if self.stride == (2, 2) else x[0])
More details on our implementation and experiments can be found at:
https://github.com/locuslab/orthogonal-convolutions.
21

