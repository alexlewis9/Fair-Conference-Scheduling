Ordering-based Conditions for Global Convergence
of Policy Gradient Methods
Jincheng Mei
Google DeepMind
jcmei@google.com
Bo Dai
Google DeepMind
bodai@google.com
Alekh Agarwal
Google Research
alekhagarwal@google.com
Mohammad Ghavamzadeh
Amazon∗
ghavamza@amazon.com
Csaba Szepesvári
Google DeepMind
University of Alberta
szepi@google.com
Dale Schuurmans
Google DeepMind
University of Alberta
daes@ualberta.ca
Abstract
We prove that, for ﬁnite-arm bandits with linear function approximation, the global
convergence of policy gradient (PG) methods depends on inter-related properties
between the policy update and the representation. First, we establish a few key
observations that frame the study: (i) Global convergence can be achieved under
linear function approximation without policy or reward realizability, both for the
standard Softmax PG and natural policy gradient (NPG). (ii) Approximation error
is not a key quantity for characterizing global convergence in either algorithm.
(iii) The conditions on the representation that imply global convergence are differ-
ent between these two algorithms. Overall, these observations call into question
approximation error as an appropriate quantity for characterizing the global conver-
gence of PG methods under linear function approximation. Second, motivated by
these observations, we establish new general results: (i) NPG with linear function
approximation achieves global convergence if and only if the projection of the
reward onto the representable space preserves the optimal action’s rank, a quantity
that is not strongly related to approximation error. (ii) The global convergence
of Softmax PG occurs if the representation satisﬁes a non-domination condition
and can preserve the ranking of rewards, which goes well beyond policy or reward
realizability. We provide experimental results to support these theoretical ﬁndings.
1
Introduction
Policy gradient (PG) is a foundational concept in reinforcement learning (RL), centrally used in both
policy-based and actor-critic methods [25]. Despite the non-convexity of the policy optimization
objective [4], global convergence of PG methods has been recently established in the tabular case
for standard conﬁgurations such as the softmax parameterization [4, 22] and stochastic on-policy
sampling [20]. In practice, when an RL agent is faced with a problem with large state and/or action
spaces, function approximation is needed to generalize across related states and actions. The behavior
of PG methods in these settings is relatively under-explored. In this paper, we study this question for
the case of linear function approximation, and establish a surprising result that
the classical Softmax PG method converges whenever there exists an adequate linear function that
ranks actions in the same order as the ground-truth reward function.
∗The work was done prior to joining Amazon, while the author was at Google Research.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Understanding the behavior of PG methods under function approximation is crucial for describing the
behavior of RL in practice, since one rarely faces domains small enough to explicitly enumerate over
states and actions in parameterizing the policy. It is well known that, standard Softmax PG converges
to stationary points if a “compatible” function approximation is used [25]; i.e., one that is able to
exactly represent policy value functions. However, when exact policy values are non-realizable,
“approximation error” is typically considered to be the key quantity for characterizing how well a
function approximation captures relevant problem quantities, including transition dynamics, rewards
and policy values. This paper shows that such an approximation error perspective is overly demanding
when attempting to characterize the conditions that lead to global convergence of PG methods.
Using the concept of approximation error, global convergence results for PG methods have been
recently established in an additive form,
sub-optimality gap ≤optimization error + approximation error,
(1)
implying that if the approximation error is small, a diminishing optimization error implies a small
sub-optimality gap. A representative result is the global convergence of natural policy gradient
(NPG) [4, Table 2], where the optimization error will diminish as the algorithm updates. There have
also been global convergence results for other PG variants under linear function approximation that
follow a similar approximation error analysis [3, 8, 10, 28, 5, 1, 2]. However, an additive bound like
Eq. (1) has the inherent weakness that the approximation error will never be zero if the function
approximation is not able to perfectly represent the desired quantities. This prevents such a strategy
from establishing global convergence in cases where the approximation error is non-zero but a PG
method still reaches the best representable solution.
Therefore, in spite of this recent progress, using approximation error in PG global convergence with
function approximations has left two major gaps in the literature. First, it has not been investigated
whether small approximation error is necessary to achieve convergence to an optimal representable
policy [4], diverting attention from feature designs that achieve useful properties beyond small
approximation error. Second, it is not clear if standard Softmax PG (other than NPG) converges
globally under small approximation errors. In particular, NPG contains a least squares regression
step [4, Eq. (17)] that can be naturally characterized with an approximation error quantity. However,
standard Softmax PG does not have such a projection step [25], and the results in [4] do not apply
to this update. Whether standard Softmax PG can achieve global convergence with even linearly
realizable rewards (zero approximation error) is still an open problem.
In this paper, we address the above questions and contribute the following results. First, we provide
negative answers to questions on the role of approximation error in determining global convergence
of PG methods:
(i) Global convergence can be achieved under linear function approximation with non-zero approxi-
mation error, for both the standard Softmax PG and natural policy gradient (NPG) updates.
(ii) Approximation error is not a key quantity for characterizing global convergence in either case.
(iii) The conditions that imply global convergence are different between these two algorithms.
Second, these results lead us to question whether approximation error is an appropriate quantity to
consider the global convergence of PG methods under linear function approximation. We establish
new general results that characterize the conditions for global convergence of PG methods:
(i) NPG with log-linear function approximation achieves global convergence if and only if the
projection of the reward onto the representable space preserves the optimal action’s rank. This
result signiﬁcantly extends previous results that use approximation error in the analysis [4, 3],
since preserving the rank of the optimal action is not strongly related to approximation error
(except in the realizable limit).
(ii) We show that the global convergence of Softmax PG follows if the representation satisﬁes a
non-domination condition and can preserve the ranking of rewards, which goes well beyond
policy or reward realizability. As a byproduct, we resolve an open question by showing that even
for linearly realizable reward function, Softmax PG cannot always converge to globally optimal
policies when the non-domination condition for representation is violated.
2

2
Settings
We study the policy optimization problem under one state with K actions. Given a reward vector
r ∈RK, the problem is to ﬁnd a parametric policy πθ to maximize the expected reward,
sup
θ∈Rd π⊤
θ r,
(2)
where θ ∈Rd with d < K is the parameter, and πθ = softmax(Xθ) is called a “log-linear policy”
[4, 28] such that for all action a ∈[K] := {1, 2, . . . , K},
πθ(a) =
exp{[Xθ](a)}
P
a′∈[K] exp{[Xθ](a′)},
(3)
where X ∈RK×d is the feature matrix with full column rank d < K. There are two major difﬁculties
with the policy optimization problem. First, Eq. (2) is a non-concave maximization w.r.t. θ, due to
the softmax transform [22, Proposition 1]. Second, the policy and reward can be unrealizable, in the
sense that the parametric log-linear policy πθ = softmax(Xθ) cannot well approximate every policy
π in the K-dimensional probability simplex, and the score Xθ ∈RK cannot well approximate the
true mean reward r ∈RK. Such limitations arise in the linear function approximation case because
πθ and Xθ are restricted to low-dimensional manifolds via θ ∈Rd for d < K.
To solve Eq. (2), we consider the standard Softmax PG [25] and NPG [13, 4] methods, shown in
Algorithms 1 and 2. Softmax PG is an instance of gradient ascent, obtained by the chain rule,
d π⊤
θtr
dθt
= d Xθt
dθt
 d πθt
d Xθt
⊤d π⊤
θtr
dπθt
= X⊤(diag(πθt) −πθtπ⊤
θt) r.
(4)
On the other hand, NPG conducts updates using least squares regression (i.e., projection),
 X⊤X
−1 X⊤r = arg min
w∈Rd
∥Xw −r∥2
2.
(5)
As representative policy-based methods, in their general forms, Softmax PG and NPG lay the
foundation for widely used RL methods, including REINFORCE [26], actor-critic [16, 7, 12], TRPO
and PPO [23, 24]. The above Eqs. (4) and (5) are their updates applied to the one-state setting.
Algorithm 1 Softmax policy gradient (PG)
Input: Learning rate η > 0.
Output: Policies πθt = softmax(Xθt).
Initialize parameter θ1 ∈Rd.
while t ≥1 do
θt+1 ←θt+η·X⊤(diag(πθt)−πθtπ⊤
θt)r.
end while
Algorithm 2 Natural policy gradient (NPG)
Input: Learning rate η > 0.
Output: Policies πθt = softmax(Xθt).
Initialize parameter θ1 ∈Rd.
while t ≥1 do
θt+1 ←θt + η · (X⊤X)−1X⊤r.
end while
To understand the difﬁculty of the optimization problem in Eq. (2), it is helpful to consider previous
work that has analyzed the convergence of PG methods.
In the tabular setting, where d = K, X = Id, and πθ = softmax(θ) with θ ∈RK, both the
rewards and optimal policy can be arbitrarily well approximated. In this case it is known that NPG
enjoys a O(1/t) global convergence rate [4, Table 1], which has been recently improved to O(e−c·t)
[14, 20, 17, 27]. For the case of function approximation, such results have subsequently been extended
to log-linear policies, where approximation error is used to characterize the projection step of Eq. (5)
[4, 28]. In particular, NPG achieves the following sub-optimality gap for all t ≥1 [4, Table 2],
(π∗−πθt)⊤r ≤c1/
√
t + c2 · ϵapprox,
(c1 > 0, c2 > 0)
(6)
where c1 and c2 are problem speciﬁc constants, π∗is the globally optimal policy, πθt is produced by
NPG, and ϵapprox is the approximation error, i.e., the minimum error with which the policy values
can be approximated using the features [4, Table 2]. The “optimization error” term c1/
√
t in Eq. (6)
has since been improved to O(e−c3·t) with c3 > 0 in [28, 5]. Note that if ϵapprox > 0 then Eq. (6)
is insufﬁcient for establishing π⊤
θtr →r(a∗) := maxa∈[K] r(a) as t →∞even when such global
convergence is achieved.
The understanding for the standard Softmax PG is even less clear. In the tabular case, it is known that
Softmax PG achieves global convergence asymptotically, i.e., π⊤
θtr →r(a∗) as t →∞[4], with an
O(1/t) rate of convergence that exhibits undesirable problem and initialization dependent constants
3

[21, 18]. Directly extending this global convergence result to the case of function approximation,
i.e., log-linear policies, is impossible without any additional assumptions on the features, since
there can be exponentially many sub-optimal local maxima in the worst case [9]. In fact, even with
linearly realizable rewards (zero approximation error), whether standard Softmax PG achieves global
convergence still remains unsolved [4]. One intuitive reason why this is a difﬁcult result to establish
is that standard Softmax PG uses the gradient Eq. (4) rather than projection (regression) to perform
updates, which is less directly connected to the concept of approximation error.
3
The Limitations of Approximation Error in Characterizing Convergence
It is known that there exist representations X ∈RK×d with d < K and r ∈RK that create expo-
nentially many sub-optimal local maxima in Eq. (2) [9, Theorem 1], which makes it impossible to
ensure global convergence of PG methods without imposing any structure on the function approxi-
mation. Before identifying speciﬁc conditions that ensure global convergence, we ﬁrst explain how
approximation error cannot be a useful structural measure for this purpose, by demonstrating that
zero approximation error is not a necessary condition for global convergence, and illustrating problem
instances with comparable approximation error that render starkly different convergence behaviors
across different PG methods. Speciﬁcally, we illustrate these points with a set of concrete scenarios,
each with 4 actions and 2-dimensional feature vectors describing each action. Since d < K, not
every policy can be expressed in these representations, hence the problem instances are unrealizable.
3.1
Global Convergence is Achievable with Non-zero Approximation Error
The results of [9, Theorem 1] do not imply that sub-optimal local maxima always appear, as shown in
the following.
Example 1. K = 4, d = 2, X⊤=
 0
−1
0
2
−2
0
1
0

and r = (9, 8, 7, 6)⊤. The approximation
error is ϵapprox = min
w∈Rd ∥Xw −r∥2 =
X
 X⊤X
−1 X⊤r −r

2 =
√
202.6 ≈14.2338.
Note that the approximation error is larger than any sub-optimality gap, i.e., for any policy π,
(π∗−π)⊤r ≤3 < ϵapprox,
(7)
hence the bound in Eq. (6) does not imply global convergence for NPG in this example. Yet, despite
the non-zero approximation error and the inability of existing results including Eq. (6) to establish
global convergence on Example 1, both Algorithms 1 and 2 can be shown to reach a global maximum.
Proposition 1. Denote a∗:= arg maxa∈[K] r(a). With constant η > 0 and any initialization
θ1 ∈Rd, both Algorithms 1 and 2 guarantee π⊤
θtr →r(a∗) as t →∞on Example 1.
All proofs can be found in the appendix due to space limits. The fact that Softmax PG achieves global
convergence in Example 1 is much harder to establish than for NPG, since Eq. (4) involves a complex
non-linearity given the presence of the softmax, unlike the linear least squares Eq. (5) used in NPG.
To illustrate the intuition behind Proposition 1 we use a visualization of the optimization landscape.
Visualization.
A visualization of the optimization landscape of Example 1 is shown in Figure 1(a).
The bottom two-dimensional plane is the parameter space Rd where d = 2. For each θ ∈Rd, we
calculate πθ using Eq. (3) and π⊤
θ r using Eq. (2), and use π⊤
θ r as the vertical axis value of θ.
Figure 1: Visualizing the landscapes in the example problem instances.
To verify Proposition 1, we run Softmax PG and NPG on Example 1 with the same θ1 = (6, 8)⊤∈R2.
In Figure 1(a), the optimization trajectories show 85 iterations of NPG and 8.5 × 106 iterations of
4

Softmax PG, both with learning rate η = 0.2. It can be clearly seen that both Softmax PG and
NPG eventually achieve expected reward π⊤
θtr →9 = r(a∗), demonstrating global convergence
(Figure 3(c) later shows that the sub-optimality gap (π∗−πθt)⊤r approaches 0).
In summary, Example 1 shows that both Softmax PG and NPG are able to achieve global convergence
on unrealizable problem instances with non-zero approximation error. This raises the question:
Is non-zero approximation error useful for characterizing global convergence?
3.2
Global Convergence is Irrelevant to Non-zero Approximation Error
We answer the above question negatively. By comparing alternative problem instances with similar
approximation errors but different convergence behaviors, we illustrate how approximation error is
not able to distinguish between scenarios where global versus local convergence is obtained.
Example 2. K = 4, d = 2, X⊤=
 0
0
−1
2
−2
1
0
0

∈Rd×K, and r = (9, 8, 7, 6)⊤∈RK. The
approximation error is
X
 X⊤X
−1 X⊤r −r

2 =
√
205 ≈14.3178.
The only difference between Examples 1 and 2 is that the second and third columns of X⊤have been
exchanged. The approximation error remains similar to that of Example 1. Using the upper bound of
Eq. (6), one might therefore expect similar sub-optimality gaps (π∗−πθt)⊤r to be demonstrated
by the algorithms, since the r.h.s. contains similar approximation errors. However, as shown in
Figure 1(b), using the same initialization and learning rate, Softmax PG obtains π⊤
θtr →8 = r(2) <
r(a∗) as it converges to a sub-optimal deterministic policy, while NPG continues to succeed.
Lest one believe that NPG is globally convergent, the following example, where the ﬁrst and second
columns of X⊤are swapped, illustrate an analogous failure for NPG but not Softmax PG.
Example 3. K = 4, d = 2, X⊤=
−1
0
0
2
0
−2
1
0

∈Rd×K, and r = (9, 8, 7, 6)⊤∈RK. The
approximation error is
X
 X⊤X
−1 X⊤r −r

2 =
√
212 ≈14.5602.
Here again the approximation error is close to that of Example 1. Yet, Figure 1(c) shows that NPG
achieves π⊤
θtr →8 < r(a∗) as it converges to a sub-optimal solution, while Softmax PG succeeds.
In summary, the Examples 1, 2 and 3 all have similar approximation errors, yet Softmax PG achieves
global convergence on Example 1 but reaches a bad local maxima on Example 2, while NPG succeeds
on Example 1 and fails on Example 3. Note that these examples can be re-scaled to have exactly the
same approximation errors while demonstrating the same convergence behavior of the algorithms.
From these ﬁndings we conclude that, if there is any quantity that can predict whether global versus
local convergence is obtained by Softmax PG or NPG, that the quantity cannot be approximation
error alone. This motivates to investigate the question: what is the right quantity to characterize
global convergence for unrealizable problems?
3.3
Global Convergence Characterization is Algorithm Dependent
We make one more key point. From Figure 1(b) and Figure 1(c), NPG achieves global convergence
on Example 2 but fails on Example 3, while, conversely, Softmax PG succeeds on Example 3 and fails
on Example 2. This difference indicates that whatever condition characterizes global convergence,
it must be algorithm dependent, even for the closely related algorithms Softmax PG and NPG.
Therefore, one has to study the conditions for Softmax PG and NPG respectively (rather than one
condition for both algorithms), which motivates the reﬁned question:
What conditions characterize global convergence of Softmax PG and NPG in unrealizable problems?
4
New Characterizations of Global Convergence for PG Methods
From these observations, it is clear that whatever quantity characterizes the global convergence of
PG methods, it cannot be based solely on approximation error and it must be algorithm dependent.
Therefore, we study distinct global convergence conditions for Softmax PG and NPG respectively.
5

4.1
Reward Order Preservation with Adequate Features is Sufﬁcient for PG Convergence
We now investigate a global convergence condition for Softmax PG under log-linear policies.
Intuition.
Consider Example 1, where Softmax PG achieves global convergence. From the land-
scape shown in Figure 1(a), there appears to be a monotonic path from any initialization point that
allows gradient ascent to reach the optimal plateau with reward r(a∗) = 9. Intuitively, this arises be-
cause the actions’ rewards seem to be nicely “ordered”. For example, starting from θ1 = (6, 8)⊤∈Rd
such that π⊤
θ1r ≈6, Softmax PG is able to improve its expected reward eventually to π⊤
θtr ≈7, since
there exists a sub-optimal plateau with a higher reward 7 right beside the lowest plateau with reward
6. Next, Softmax PG continues to improve its expected reward eventually to π⊤
θtr ≈8 by “climbing”
toward another neighboring plateau with a higher reward. Finally, this process ends with Softmax PG
successfully arriving at the optimal plateau with reward r(a∗) = 9.
By contrast, in Example 2, as shown in Figure 1(b), Softmax PG gets stuck on a bad plateau with
a local maximum reward of 8. Visually, Softmax PG stops improving its expected reward on this
sub-optimal plateau, because it is “surrounded” by two lower plateaus with rewards 6 and 7, which
breaks the nice “ordering” of the expected reward landscape and traps the gradient ascent trajectory
on a sub-optimal plateau from which there is no monotonic ascent to global optimality.
Verifying reward order preservation.
Based on the above intuition and observations, we conjec-
ture that the ordering structure between the different rewards is a key property behind the global
convergence of Softmax PG. We can verify this conjecture in each of the Examples 1 to 3 by deter-
mining whether the feature matrix X ∈RK×d allows the same action ordering as the reward vector
r ∈RK to be realized. For Example 1, note that with w = (−1, −1)⊤∈Rd, we have
r′ := Xw = (2, 1, −1, −2)⊤∈RK,
(8)
which preserves the ordering of r ∈RK, such that for all i, j ∈[K], r(i) > r(j) if and only if
r′(i) > r′(j). Similarly, for Example 3, if we let w = (−3, −1)⊤then we have r′ := Xw =
(3, 2, −1, −6)⊤, which also preserves the order of r over actions. Softmax PG converges to a globally
optimal reward in both of these examples.
By contrast, for Example 2, it is impossible to ﬁnd any w ∈Rd such that Xw preserves the order of
the rewards r. To see why, consider any w = (w(1), w(2))⊤and note that
r′ := Xw = (−2 · w(2), w(2), −w(1), 2 · w(1))⊤.
(9)
To preserve the reward order, we require both −2 · w(2) > w(2) (which would imply w(2) < 0)
and −w(1) > 2 · w(1) (which would imply w(1) < 0), but these two conditions imply w(2) < 0 <
−w(1), which must reverse the order of the second and third actions. This is an example where PG
can fail to reach a global optimum.
Main Softmax PG result.
We formalize the above intuition by proving the following main result,
which establishes that reward order preservation with adequate representations is a sufﬁcient condition
for the global convergence of Softmax PG under log-linear function approximation.
Theorem 1 (Reward order preservation, non-domination features). Given any reward r ∈RK and
feature matrix X ∈RK×d. Denote xi ∈Rd as the i-th row vector of X. If (i) x⊤
i xi > x⊤
i xj for all
j ̸= i, and (ii) there exists at least one w ∈Rd, s.t., r′ := Xw preserves the order of r, i.e., for all
i, j ∈[K], r(i) > r(j) if and only if r′(i) > r′(j), then for any initialization θ1 ∈Rd, Algorithm 1
with a constant learning rate η > 0 achieves global convergence of π⊤
θtr →r(a∗) as t →∞.
A few remarks about this theorem are in order.
Examples 1 to 3 all satisfy the non-domination condition (i) on X, and their differences lie in
satisfying reward order preservation or not. However, the following example shows that if the
condition (i) on X is removed, then global convergence is not always achievable for even linearly
realizable rewards (with zero approximation error).
Proposition 2. Let K = 3, d = 2, X⊤=
 0
−10
0
−2
4
1

∈Rd×K, and r = Xw = (4, 2, −2)⊤,
where w = (−1, −2)⊤∈Rd. With initialization θ1 = (−ln 2, ln 2)⊤, Algorithm 1 does not achieve
global convergence, i.e., πθt(1) ̸→1 as t →∞.
6

Generalization of tabular and linear realizability. When d = K and X = Id, i.e., the softmax
tabular parameterization πθ = softmax(θ), it is always true that Xr = r preserves the order of r.
Consequently, Theorem 1 recovers the global convergence result for PG in the softmax tabular setting
[4, 22] as a special case. More generally, for non-domination features, when the reward is linearly
realizable, such that Xw = r for some w ∈Rd, the global convergence of Softmax PG also follows
from Theorem 1, since r preserves its own order when the approximation error is zero.
Corollary 1 (Linearly realizable rewards, non-domination features). Given any reward r ∈RK and
feature matrix X ∈RK×d. Denote xi ∈Rd as i-th row vector of X. If (i) x⊤
i xi > x⊤
i xj for all
j ̸= i, and (ii) there exists w ∈Rd, s.t., Xw = r, then for any initialization θ1 ∈Rd, Algorithm 1
with a constant learning rate η > 0 achieves global convergence of π⊤
θtr →r(a∗) as t →∞.
It is worth mentioning that Proposition 2 and Corollary 1 together answer a question which still remain
unsolved in PG literature [4]: with linearly realizable rewards (zero approximation error), whether
standard Softmax PG achieves global convergence? Proposition 2 shows that linearly realizable
reward on its own is not enough to guarantee global convergence, while Corollary 1 shows that with
adequate features, linearly realizable reward implies global convergence. Note that the NPG global
convergence result in [4], such as Eq. (6), does not apply to standard Softmax PG.
Ordering does not determine approximation. As already illustrated in Section 3, approximation
error is not adequate for capturing the global convergence of Softmax PG. It is important to emphasize
that the existence of an order preserving reward r′ is very different from having a small approximation
error. When the approximation error is zero, then an order preserving reward (equal to r) always
exists. However, in general, r′ can take very different values than r, and hence have a very large
approximation error, yet still enable global convergence as shown in Examples 1 and 3.
Proof idea. The idea behind the proof of the main theorem consists of three parts. We provide a
sketch of the proof here; the full proof is given in Appendix A. First, starting from any initialization
θt ∈Rd, Algorithm 1 guarantees that πθt will approach a (generalized) one-hot policy as t →∞. To
see why, ﬁrst note that π⊤
θ r is β-smooth over θ ∈Rd with some β > 0 (Lemma 3 in Appendix B),
since the softmax transform is smooth [4, 22] and the feature matrix X has bounded values. This
implies that using a sufﬁciently small constant learning rate 0 < η ≤2/β we obtain,
π⊤
θt+1r −π⊤
θtr ≥
1
2 β ·

d π⊤
θtr
dθt

2
2
≥0.
(10)
Note that π⊤
θ r is upper bounded by r(a∗). According to the monotone convergence, π⊤
θtr →c ≤
r(a∗) as t →∞. This fact combined with Eq. (10) implies

d π⊤
θtr
dθt

2 →0 as t →∞. Next, a special
co-variance structure of softmax PG (Lemma 4) shows that

d π⊤
θtr
dθt

2 →0 implies that ∥θt∥2 →∞
and πθt approaches a (generalized) one-hot policy as t →∞.
Lemma 1. Under the same conditions as Theorem 1, and r(i) ̸= r(j) for all i ̸= j (unique action
reward), Algorithm 1 assures ∥θt∥2 →∞and πθt(i) →1 for an action i ∈[K] as t →∞.
Remark 1. Removing the unique action reward condition in Lemma 1 makes πθt approach a
generalized one-hot policy (rather than a strict one-hot in Lemma 1) as t →∞as a result.
According to Lemma 1, θt grows unboundedly. Intuitively, this can be seen in Figure 1(a), where
there are no stationary points in a ﬁnite region.
Second, for any vector r′ that preserves the order of r, we establish the following key lemma.
Lemma 2 (Non-negative covariance of order preservation). If r′ ∈RK preserves the order of
r ∈RK, i.e., for all i, j ∈[K], r(i) > r(j) iff r′(i) > r′(j), then for any policy π ∈∆(K),
r′⊤ diag(π) −ππ⊤
r = Covπ (r′, r) ≥0.
(11)
Now consider the direction w ∈Rd such that r′ := Xw preserves the order of r. We have,
w⊤θt+1 = w⊤θt + η · w⊤X⊤ diag(πθt) −πθtπ⊤
θt

r
(by Algorithm 1)
(12)
= w⊤θt + η · r′⊤ diag(πθt) −πθtπ⊤
θt

r
(r′ := Xw)
(13)
≥w⊤θt.
(by Lemma 2)
(14)
7

Third, take a sub-optimal action i ∈[K] with r(i) < r(a∗), and we show that the assumption
πθt(i) →1 as t →∞leads to a contradiction.
To that end, ﬁrst observe that this assumption implies that for all large enough time t ≥1,
 Xθt
∥θt∥2

(i) = max
a∈[K]
 Xθt
∥θt∥2

(a),
(15)
which means that the sub-optimal action i ∈[K] always has the largest score (since its probability
πθt(i) →1 is always the largest). Moreover, differences between actions’ scores are unbounded, due
to πθt(i)
πθt(j) = exp

[Xθt](i) −[Xθt](j)
	
→∞for all other actions j ̸= i.
0
-1
1
-1
1
Figure 2: Idea illustration.
Consider Example 1 for illustration. The top
view of Figure 1(a) is shown in Figure 2(a).
Take i = 2 and r(i) = 8, and assume
θt
∥θt∥2
stays in the green (sub-optimal) region of Fig-
ure 2(a), excluding its boundaries. This green
region is partitioned in Figure 2(b), where the
dark sub-region contains v2 ∈Rd such that
[Xv2](a∗) is the second largest component
among all a ̸= 2, and the light sub-region is
the remaining.
The argument is completed by addressing the
two cases: (i) If
θt
∥θt∥2 stays in the dark sub-
region where v1 ∈Rd belongs to, then π⊤
θtr > r(i) = 8 must occur in ﬁnite t < ∞, implying
πθt(i) ̸→1, contradicting the assumption. Intuitively, the contradiction occurs because the dark
sub-region is closer to a higher plateau with reward 9, and scaling up θt’s magnitude in this sub-region
eventually ensures π⊤
θtr > r(i) = 8. (ii) If
θt
∥θt∥2 stays in the light sub-region which contains v2 ∈Rd,
then w⊤θt > u⊤θt must occur in ﬁnite time t < ∞, implying that
θt
∥θt∥2 will enter the dark sub-
region, reducing to the ﬁrst case. This argument depends on Eq. (12) and a key observation showing
that u⊤θt+1 < u⊤θt, where u is a “worse” direction such that [Xu](a−) = maxa∈[K] [Xu](a) for
some a−∈[K] with r(a−) < r(i).
To summarize,
θt
∥θt∥2 cannot always stay in the green sub-optimal region in Figure 2(a), which implies
that
θt
∥θt∥2 must eventually enter the optimal region that contains w and stay in that region. By
Lemma 1 we then obtain πθt(a∗) →1 and π⊤
θtr →r(a∗) as t →∞(see appendix).
4.2
Optimal Action Preservation is Necessary and Sufﬁcient for NPG Convergence
Next, we investigate the global convergence conditions for NPG under log-linear policies. Unlike
Softmax PG, the key property for determining global convergence of NPG is whether the projection
of the rewards r onto the feature representation X preserves the top ranking of the optimal action.
Intuition and demonstration.
First consider Example 1 where NPG successfully converges to a
global maximum. From Algorithm 2, a simple calculation shows,
Xθt+1 = Xθt + η · X(X⊤X)−1X⊤r = Xθt + η · 1
5 · (22, −4, −11, 8)⊤,
(16)
which implies that the optimal action a∗= 1 always receives the largest update to its score [Xθt](a∗)
in each iteration. Next, take a sub-optimal action a = 2, as an example, and observe that,
πθt+1(a∗)
πθt+1(a) = πθt(a∗)
πθt(a) · exp

η · (ˆr(a∗) −ˆr(a))
	
= πθt(a∗)
πθt(a) · exp
n
η · 26
5
o
(17)
by Eq. (3), where ˆr := X(X⊤X)−1X⊤r. Using a constant learning rate η > 0 and applying Eq. (17),
we have that πθt(a∗) grows exponentially with t, indicating that πθt(a∗) →1 as t →∞since the
same argument works for any sup-optimal action a ̸= a∗. Moreover, the rate is O(e−c·t), since
(π∗−πθt)⊤r ≤2 · ∥r∥∞· (1 −πθt(a∗)). The O(e−c·t) rate matches the results in softmax tabular
settings [14, 20, 17, 27].
8

Second, consider Example 2 where NPG fails to converge to a global maximum. Using similar
calculations to Eq. (16) we obtain,
Xθt+1 = Xθt + η · ˆr = Xθt + η · 1
5 · (−3, 18, −9, −6)⊤
(18)
which implies that a sub-optimal action a = 2 always receives the largest update on its score [Xθt](2)
in each iteration. The failure in Figure 1(b) is then veriﬁed by similar arguments around Eq. (17).
Main NPG result.
Based on these observations, it is evident that for NPG to converge globally, it
is important for the optimal action to eventually always receive the largest update to its score, which
makes it critical that the least square projection X(X⊤X)−1X⊤r preserves the top ranking of the
optimal action. We formalize this intuition by establishing the following main result.
Theorem 2 (Optimal action preservation condition). For a constant learning rate η > 0, a necessary
and sufﬁcient condition for Algorithm 2 to achieve global convergence π⊤
θtr →r(a∗) as t →∞from
any initialization θ1 ∈Rd is that ˆr(a∗) > ˆr(a) for all a ̸= a∗, such that a∗:= arg maxa∈[K] r(a),
and ˆr := X
 X⊤X
−1 X⊤r is the least squares projection of r onto the column space of X. If the
condition is satisﬁed, then the rate of convergence is (π∗−πθt)⊤r ∈O(e−c·t) for some c > 0.
Proof idea. When the optimal action preservation is satisﬁed, similar arguments to Eqs. (16) and (17)
guarantee that πθt(a∗) grows exponentially with t, indicating that πθt(a∗) →1 as t →∞.
The constant c > 0 in Theorem 2 depends on the gap of ˆr, i.e., ˆr(a∗) −maxa̸=a∗ˆr(a), which
ﬁnds similarities to NPG results in tabular settings [14, 15]. The main difference is that the gap
of true reward r in tabular cases is replaced with the gap of least square projection ˆr in function
approximation settings in Theorem 2. This similarity is an evidence for improving the rate to
super-linear by using geometrically increasing step sizes, as in tabular settings [17, 27, 19, 28, 6].
One-sided Approximation Error.
For NPG, [4, Lemma 6.2] introduces a “one-sided approxima-
tion error” quantity, which aims to overestimate the advantage of the optimal action a∗,
ϵt := r(a∗) −π⊤
θtr −w⊤ xa∗−X⊤πθt

= r(a∗) −π⊤
θtr −
 ˆr(a∗) −π⊤
θtˆr

.
(19)
This quantity relaxes the notion of approximation error and still guarantees the global convergence
of NPG, since if P
t=1 ϵt ∈o(T), then NPG with η ∈O(1/
√
T) achieves global convergence [4,
Lemma 6.2]. We note however that Eq. (19) has two limitations: (i) Eq. (19) depends on the entire
update trajectory {θt}t≥1, which is hard to verify. By contrast, the optimal action preservation
condition in Theorem 2 only involves problem quantities X and r. (ii) It is not clear whether Eq. (19)
is a necessary condition for global convergence, while optimal action preservation is proved above to
be both necessary and sufﬁcient.
5
Simulation Study
We conducted additional simulations to check the theoretical results. First, we check whether the
strict inequality of ˆr(a∗) > ˆr(a) for all a ̸= a∗in Theorem 2 is required for NPG global convergence.
Example 4. K = 4, d = 2, X⊤=
 0
−1
0
1
−1
0
1
0

∈Rd×K, and r = (9, 8, 7, 6)⊤∈RK. The
best ﬁt for r is ˆr = X
 X⊤X
−1 X⊤r = (1, 1, −1, −1)⊤.
Example 4 has ˆr(a∗) = ˆr(1) = ˆr(2), which violates the strict inequality condition of ˆr(a∗) > ˆr(a)
for all a ̸= a∗. The consequence is that NPG guarantees πθt(a∗)
πθt(2) =
πθ1(a∗)
πθ1(2) for all t ≥1, which
makes it impossible for πθt(a∗) →1 as t →∞. This is observed in Figure 3(a), supporting that the
strictly inequality condition in Theorem 2 is indeed necessary. The initialization is θ1 = (4, 10)⊤,
and η = 0.2. We run 150 iterations for NPG and 1.5 × 107 iterations for Softmax PG.
Second, we run 150 iterations of NPG on Example 1. As shown in Figure 3(b), the quantity
log (π∗−πθt)⊤r is a linear function of time t, implying that (π∗−πθt)⊤r ∈O(e−c·t) with c > 0.
This supports the convergence rate results in Theorem 2. Here θ1 and η are the same as in Figure 1(a).
Third, we check whether the condition in Theorem 1 is required for Softmax PG global convergence.
9

0
20
40
60
80
100
120
-40
-30
-20
-10
0
NPG
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
-20
-19
-18
-17
-16
-15
-14
-13
-12
-11
-10
-9
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
PG
Figure 3: Simulations for verifying theoretical results.
Example 5. K = 6, d = 2, X⊤=
 0
−1
−1
0
1
1
−1
0
1
1
0
−1

, and r = (9, 8, 7, 6, 5, 4)⊤.
Similar to Eq. (9), it is impossible to ﬁnd any w ∈Rd, such that r′ := Xw preserves the order of r in
Example 5. However, as shown in Figure 3(d), Softmax PG achieves π⊤
θt →r(a∗) = 9, indicating
that the reward order preservation condition in Theorem 1 is sufﬁcient but not necessary for PG to
achieve global convergence. The initialization is θ1 = (10, −2)⊤, and η = 0.2. We run 100 iterations
for NPG and 2 × 106 iterations for Softmax PG. Note that NPG behaves erratically on Example 5
(which does not satisfy its global convergence conditions), by ﬁrst entering then leaving the optimal
plateau, eventually approaching a sub-optimal solution.
Finally, we run 108 iterations of Softmax PG on Example 1, using the same η and θ1 as in Figure 1(a).
Figure 3(c) shows that the slope of log (π∗−πθt)⊤r over log t approaches −1, indicating that the
global convergence rate is (π∗−πθt)⊤r ∈O(1/t), matching the softmax tabular setting results [22].
6
Discussions
Checking ordering-based conditions. Checking the existence of w ∈Rd in Theorem 1 is known
as linear feasibility in literature [11], i.e., determining whether a set of inequalities has a non-empty
intersection. In particular, suppose X ∈RK×d, and r ∈RK is sorted, i.e., r(1) ≥r(2) ≥· · · ≥
r(K). Denote xi ∈Rd as the i-th row vector of X. The linear feasibility problem in this case is to
check if there exists w ∈Rd, such that for all i ∈[K −1], x⊤
i w ≥x⊤
i+1w. Linear feasibility can
be cast as linear programming (LP) using a dummy objective and keeping the constraints, hence
any LP technique, such as the ellipsoid method, can be used to solve it [11]. On the other hand,
checking the optimal action preservation in Theorem 2 requires the same information as in calculating
approximation error ∥ˆr −r∥2 = min
w∈Rd ∥Xw −r∥2, since arg maxa∈[K] ˆr(a) = arg maxa∈[K] r(a)
can be immediately veriﬁed after calculating the projection ˆr := X⊤(X⊤X)−1X⊤r.
Generalization to Markov decision processes (MDPs). Our work provides some new and useful
insights for understanding more complex settings, but it requires further investigation to resolve this
highly non-trivial problem for general MDPs. See Appendix C for detailed discussions.
7
Conclusions and Future Work
We believe this work opens new directions for understanding PG-based methods under function
approximation, going well beyond the conventional approximation error based analysis. The major
technical ﬁndings involve ordering-based conditions and relevant techniques (covariance and global
convergence). Identifying exact necessary and sufﬁcient conditions for the global convergence of
Softmax PG remains future work. Extending the results and techniques to general MDPs is another
important and challenging next step. Combining function approximation with recent results on
stochastic on-policy sampling [20] is another interesting direction for agnostic learning. Investigating
whether these new global convergence conditions might be used to achieve better representation
learning is of great interest for algorithm design. Generalizing the proof techniques to other scenarios
where non-linear transforms (activation functions) interact with low-dimensional features through
gradient descent, such as in neural networks, is another lofty ambition.
10

Acknowledgments and Disclosure of Funding
The authors would like to thank anonymous reviewers for their valuable comments. CS, DS gratefully
acknowledge funding from the Canada CIFAR AI Chairs Program, Amii and NSERC.
References
[1] Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellért Weisz.
Politex: Regret bounds for policy iteration using expert prediction. In International Conference on Machine
Learning, pages 3692–3702. PMLR, 2019.
[2] Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz. Exploration-enhanced politex.
arXiv preprint arXiv:1908.10479, 2019.
[3] Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun. PC-PG: Policy cover directed exploration for
provable policy gradient learning. Advances in neural information processing systems, 33:13399–13412,
2020.
[4] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient
methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research,
22(98):1–76, 2021.
[5] Carlo Alfano and Patrick Rebeschini. Linear convergence for natural policy gradient with log-linear policy
parametrization. arXiv preprint arXiv:2209.15382, 2022.
[6] Carlo Alfano, Rui Yuan, and Patrick Rebeschini. A novel framework for policy mirror descent with general
parametrization and linear convergence. arXiv preprint arXiv:2301.13139, 2023.
[7] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor–critic
algorithms. Automatica, 45(11):2471–2482, 2009.
[8] Semih Cayci, Niao He, and Rayadurgam Srikant. Linear convergence of entropy-regularized natural policy
gradient with linear function approximation. arXiv preprint arXiv:2106.04096, 2021.
[9] Minmin Chen, Ramki Gummadi, Chris Harris, and Dale Schuurmans. Surrogate objectives for batch policy
optimization in one-step decision making. Advances in Neural Information Processing Systems, 32, 2019.
[10] Zaiwei Chen, Sajad Khodadadian, and Siva Theja Maguluri. Finite-sample analysis of off-policy natural
actor–critic with linear function approximation. IEEE Control Systems Letters, 6:2611–2616, 2022.
[11] Martin Grötschel, László Lovász, and Alexander Schrijver. Geometric algorithms and combinatorial
optimization, volume 2. Springer Science & Business Media, 2012.
[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International conference on machine
learning, pages 1861–1870. PMLR, 2018.
[13] Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems, pages
1531–1538, 2002.
[14] Sajad Khodadadian, Prakirt Raj Jhunjhunwala, Sushil Mahavir Varma, and Siva Theja Maguluri. On the
linear convergence of natural policy gradient algorithm. In 2021 60th IEEE Conference on Decision and
Control (CDC), pages 3794–3799. IEEE, 2021.
[15] Sajad Khodadadian, Prakirt Raj Jhunjhunwala, Sushil Mahavir Varma, and Siva Theja Maguluri. On linear
and super-linear convergence of natural policy gradient algorithm. Systems & Control Letters, 164:105214,
2022.
[16] Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing
systems, 12, 1999.
[17] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling
complexity, and generalized problem classes. Mathematical programming, 198(1):1059–1106, 2023.
[18] Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Softmax policy gradient methods can take
exponential time to converge. In Conference on Learning Theory, pages 3107–3110. PMLR, 2021.
[19] Yan Li, Guanghui Lan, and Tuo Zhao. Homotopic policy mirror descent: Policy convergence, implicit
regularization, and improved sample complexity. arXiv preprint arXiv:2201.09457, 2022.
[20] Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, and Dale Schuurmans. The
role of baselines in policy gradient optimization. Advances in Neural Information Processing Systems,
35:17818–17830, 2022.
[21] Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvári, and Dale Schuurmans. Escaping the
gravitational pull of softmax. Advances in Neural Information Processing Systems, 33:21130–21140, 2020.
11

[22] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of
softmax policy gradient methods. In International Conference on Machine Learning, pages 6820–6829.
PMLR, 2020.
[23] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy
optimization. In International conference on machine learning, pages 1889–1897, 2015.
[24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[25] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems, pages 1057–1063, 2000.
[26] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229–256, 1992.
[27] Lin Xiao. On the convergence rates of policy gradient methods. Journal of Machine Learning Research,
23(282):1–36, 2022.
[28] Rui Yuan, Simon S Du, Robert M Gower, Alessandro Lazaric, and Lin Xiao. Linear convergence of natural
policy gradient methods with log-linear policies. arXiv preprint arXiv:2210.01400, 2022.
12

