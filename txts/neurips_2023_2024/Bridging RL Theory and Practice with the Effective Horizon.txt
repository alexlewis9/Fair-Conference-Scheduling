Bridging Reinforcement Learning Theory
and Practice with the Effective Horizon
Cassidy Laidlaw
Stuart Russell
Anca Dragan
Unversity of California, Berkeley
{cassidy_laidlaw,russell,anca}@cs.berkeley.edu
Abstract
Deep reinforcement learning (RL) works impressively in some environments and
fails catastrophically in others. Ideally, RL theory should be able to provide an
understanding of why this is, i.e. bounds predictive of practical performance.
Unfortunately, current theory does not quite have this ability. We compare standard
deep RL algorithms to prior sample complexity bounds by introducing a new
dataset, BRIDGE. It consists of 155 deterministic MDPs from common deep
RL benchmarks, along with their corresponding tabular representations, which
enables us to exactly compute instance-dependent bounds. We choose to focus
on deterministic environments because they share many interesting properties of
stochastic environments, but are easier to analyze. Using BRIDGE, we find that
prior bounds do not correlate well with when deep RL succeeds vs. fails, but
discover a surprising property that does. When actions with the highest Q-values
under the random policy also have the highest Q-values under the optimal policy
(i.e. when it is optimal to be greedy on the random policy’s Q function), deep
RL tends to succeed; when they don’t, deep RL tends to fail. We generalize this
property into a new complexity measure of an MDP that we call the effective
horizon, which roughly corresponds to how many steps of lookahead search would
be needed in that MDP in order to identify the next optimal action, when leaf nodes
are evaluated with random rollouts. Using BRIDGE, we show that the effective
horizon-based bounds are more closely reflective of the empirical performance of
PPO and DQN than prior sample complexity bounds across four metrics. We also
find that, unlike existing bounds, the effective horizon can predict the effects of
using reward shaping or a pre-trained exploration policy. Our code and data are
available at https://github.com/cassidylaidlaw/effective-horizon.
1
Introduction
Deep reinforcement learning (RL) has produced impressive results in robotics [1], strategic games
[2], and control [3]. However, the same deep RL algorithms that achieve superhuman performance
in some environments completely fail to learn in others. Sometimes, using techniques like reward
shaping or pre-training help RL, and in other cases they don’t. Our goal is to provide a theoretical
understanding of why this is—a theoretical analysis that is predictive of practical RL performance.
Unfortunately, there is a large gap between the current theory and practice of RL. Despite RL theorists
often focusing on algorithms using strategic exploration (e.g., UCB exploration bonuses; Azar et al.
[4], Jin et al. [5]), the most commonly-used deep RL algorithms, which explore randomly, resist
such analysis. In fact, theory suggests that RL with random exploration is exponentially hard in
the worst case [6], but this is not predictive of practical performance. Some theoretical research
has explored instance-dependent bounds, identifying properties of MDPs when random exploration
should perform better than this worst case [7, 8]. However, it is not clear whether these properties
correlate with when RL algorithms work vs. fail—and our results will reveal that they tend not to.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Current state
. . .
. . .
k timesteps
m random rollouts
from each leaf node
102
106
1010
Our bounds
103
105
107
PPO sample complexity
ρ = 0.81
Figure 1: We introduce the effective horizon, a property of
MDPs that controls how difficult RL is. Our analysis is mo-
tivated by Greedy Over Random Policy (GORP), a simple
Monte Carlo planning algorithm (left) that exhaustively ex-
plores action sequences of length k and then uses m random
rollouts to evaluate each leaf node. The effective horizon
combines both k and m into a single measure. We prove
sample complexity bounds based on the effective horizon that
correlate closely with the real performance of PPO, a deep
RL algorithm, on our BRIDGE dataset of 155 deterministic
MDPs (right).
If the current theory literature cannot ex-
plain the empirical performance of deep
RL, what can?
Ideally, a theory of RL
should provably show why deep RL suc-
ceeds while using random exploration. It
should also be able to predict which envi-
ronments are harder or easier to solve empir-
ically. Finally, it should be able to explain
when and why tools like reward shaping or
initializing with a pre-trained policy help
make RL perform better.
We present a new theoretical complexity
measure for MDPs called the effective hori-
zon that satisfies all of the above criteria.
Intuitively, the effective horizon measures
approximately how far ahead an algorithm
must exhaustively plan in an environment
before evaluating leaf nodes with random
rollouts.
In order to assess previous bounds and eventually arrive at such a property, we start by creating
a new dataset, BRIDGE, of deterministic MDPs from common deep RL benchmarks. A major
difficulty with evaluating instance-dependent bounds is that they can’t be calculated without tabular
representations, so prior work work has typically relied on small toy environments for justification.
To get a more realistic picture, we choose 155 MDPs across different benchmarks and compute their
tabular representations—some with over 100 million states which must be exhaustively explored and
stored. This is a massive engineeering challenge, but it enables connecting theoretical and empirical
results at an unprecedented scale. We focus on deterministic MDPs in BRIDGE and in this paper
because they are simpler to analyze but still have many of the interesting properties of stochastic
MDPs, like reward sparsity and credit assignment challenges. Many deep RL benchmarks are (nearly)
deterministic, so we believe our analysis is highly relevent to practical RL.
Our journey to the effective horizon began with identifying a surprising property that holds in many
of the environments in BRIDGE: one can learn to act optimally by acting randomly. More specifically,
actions with the highest Q-values under the uniformly random policy also have the highest Q-values
under the optimal policy. The random policy is about as far as one can get from the optimal policy, so
this property may seem unlikely to hold. However, about two-thirds of the environments in BRIDGE
satisfy the property. This proportion rises to four-fifths among environments that PPO [9], a popular
deep RL algorithm, can solve efficiently (Table 1). Conversely, when this property does not hold,
PPO is more likely to fail than succeed—and when it does succeed, so does simply applying a few
steps of lookahead on the Q-function of the random policy (Figure 6). We found it remarkable that,
at least in the environments in BRIDGE, modern algorithms seem to boil down to not much more than
acting greedily on the random policy Q-values.
The property that it is optimal to act greedily with respect to the random policy’s Q-function has
important implications for RL theory and practice. Practically, it suggests that very simple algorithms
designed to estimate the random policy’s Q-function could efficiently find an optimal policy. We
introduce such an algorithm, Greedy Over Random Policy (GORP), which also works in the case
where one may need to apply a few steps of value iteration to the random policy’s Q-function before
acting greedily. Empirically, GORP finds an optimal policy in fewer timesteps than DQN (another
deep RL algorithm) in more than half the environments in BRIDGE. Theoretically, it is simple to
analyze GORP, which consists almost entirely of estimating the random policy’s Q-function via a
sample average over i.i.d. random rollouts. Since GORP works well empirically and can be easily
understood theoretically, we thoroughly analyze it in the hopes of finding sample complexity bounds
that can explain the performance of deep RL.
Our analysis of Greedy Over Random Policy leads to a single metric, the effective horizon, that
measures the complexity of model-free RL in an MDP. As shown in Figure 1, GORP is an adaptation
of a Monte Carlo planning algorithm to the reinforcement learning setup (where the transitions are
unknown): it mimics exhaustively planning ahead k steps and then sampling m random rollouts from
2

each leaf node. The effective horizon H combines the depth k and number of rollouts m. We call
it the effective horizon because worst-case sample complexity bounds for random exploration are
exponential in the horizon T, while we prove sample complexity bounds exponential only in the
effective horizon H. For most BRIDGE environments, H ≪T, explaining the efficiency of RL in
these MDPs.
In the environments in BRIDGE, we find that the effective horizon-based sample complexity bounds
satisfy all our desiderata above for a theory of RL. They are more predictive of the empirical sample
complexities of PPO and DQN than several other bounds from the literature across four metrics,
including measures of correlation, tightness, and accuracy (Table 2). Furthermore, the effective
horizon can predict the effects of both reward shaping (Table 3a) and initializing using a pre-trained
policy learned from human data or transferred from a similar environment (Table 3b). In contrast,
none of the existing bounds we compare to depend on both the reward function and initial policy;
thus, they are unable to explain why reward shaping, human data, and transfer learning can help
RL. Although our results focus on deterministic MDPs, we plan to extend our work to stochastic
environments in the future and already have some promising results in that direction.
2
Preliminaries
We begin by presenting the reinforcement learning (RL) setting we consider. An RL algorithm
acts in a deterministic, tabular, episodic Markov decision process (MDP) with finite horizon. The
MDP comprises a set of states S, a set of actions A, a horizon T ∈N and optional discount factor
γ ∈[0, 1], a start state s1, transition function f : S × A →S, and a reward function R : S × A →R.
Throughout the paper we use γ = 1 but all our theory applies equally when γ < 1.
An RL agent interacts with the MDP for a number of episodes, starting at a fixed start state s1. At
each step t ∈[T] of an episode (using the notation [n] = {1, . . . , n}), the agent observes the state st,
picks an action at, receives reward R(st, at), and transitions to the next state st+1 = f(st, at). A
policy π is a set of functions π1, . . . , πt : S →∆(A), which defines for each state and timestep a
distribution πt(a | s) over actions. If a policy is deterministic at some state, then with slight abuse of
notation we denote a = πt(s) to be the action taken by πt in state s.
We denote a policy’s Q-function Qπ
t : S × A →R and value function V π
t : S →R for each t ∈[T].
In this paper, we also use a Q-function which is generalized to sequences of actions. We use the
shorthand at:t+k to denote the sequence at, . . . , at+k, and define the action-sequence Q-function as
Qπ
t (st, at:t+k) = Eπ
" T
X
t′=t
γt′−t R(st′, at′) | st, at:t+k
#
.
The objective of an RL algorithm is to find an optimal policy π∗, which maximizes J(π) = V π
1 (s1),
the expected discounted sum of rewards over an episode, also known as the return of the policy π.
Generally, an RL algorithm can be run for any number of timesteps n (i.e., counting one episode as
T timesteps), returning a policy πn. We define the sample complexity N of an RL algorithm as the
minimum number of timesteps needed such that the algorithm has at least a 50-50 chance of returning
an optimal policy:
N = min {n ∈N | P (J(πn) = J∗) ≥1/2} .
Here, the probability is with respect to any randomness in the algorithm itself. One can estimate the
sample complexity N empirically by running an algorithm several times, calculating the number of
samples n needed to reach the optimal policy during each run, and then taking the median.
The following simple theorem gives upper and lower bounds for the worst-case sample complexity in
a deterministic MDP, depending on A and T.
Theorem 2.1. There is an RL algorithm which can solve any deterministic MDP with sample
complexity N ≤T⌈AT /2⌉. Conversely, for any RL algorithm and any values of T and A, there must
be some deterministic MDP for which its sample complexity N ≥T(⌈AT /2⌉−1).
All proofs are deferred to Appendix A. In this case, the idea of the proof is quite simple, and will later
be useful to motivate our idea of the effective horizon: in an MDP where exactly one sequence of
actions leads to a reward, an RL algorithm may have to try almost every sequence of actions to find
the optimal policy; there are AT such sequences. As we develop sample complexity bounds based on
3

the effective horizon in Section 5, we can compare them to the worst-case bounds in Theorem 2.1.
Why deterministic MDPs?
We focus on deterministic (as opposed to stochastic) MDPs in this
study for several reasons. First, analyzing deterministic MDPs avoids the need to consider general-
ization within RL algorithms. In common stochastic MDPs, one often needs neural-network based
policies, whereas in a deterministic MDP one can simply learn a sequence of actions. Since neural
network generalization is not well understood even in supervised learning, analyzing generalization
in RL is an especially difficult task. Second, deterministic MDPs still display many of the interesting
properties of stochastic MDPs. For instance, deterministic MDPs have worst case exponential sample
complexity when using naive exploration; environments with dense rewards are easier to solve
empirically than those with sparse rewards; credit assignment can be challenging; and there is a wide
range of how tractable environments are for deep RL, even for environments with similar horizons,
state spaces, and action spaces.
Finally, many common RL benchmark environments are deterministic or nearly-deterministic. For
instance, the ALE Atari games used to evaluate DQN [3], Rainbow [10], and MuZero [11] are
all deterministic after the first state, which is selected randomly from one of only 30 start states.
The widely used DeepMind Control Suite [12] is based on the MuJoCo simulator [13], which is
also deterministic given the initial state (some of the environments do use a randomized start state).
MiniGrid environments [14], which are commonly used for evaluating exploration [15], environment
design [16], and language understanding [17], are also deterministic after the initial state. Thus, our
investigation of deterministic environments is highly relevant to common deep RL practice.
3
Related Work
Before delving into our contributions, we briefly summarize existing work in theoretical RL and prior
sample complexity bounds. Our novel bounds are contrasted with existing ones in Sections 5.1 and 6;
for a detailed comparison with full definitions and proofs, please see Appendix D.
Recent RL theoretical results largely focus on strategic exploration using techniques like UCB
exploration bonuses [18, 4, 19, 5, 20, 21, 22]. Such bounds suggest RL is tractable for smaller or
low-dimensional state spaces. In deterministic MDPs, the UCB-based R-MAX algorithm [23, 18] has
sample complexity bounded by SAT.
Some prior work has focused on sample complexity bounds for random exploration. Liu and Brunskill
[7] give bounds based on the covering length L of an MDP, which is the number of episodes needed
to visit all state-action pairs at least once with probability at least 1/2 while taking actions at random.
This yields a sample complexity bound of TL for deterministic MDPs. Other work suggests that it
may not be necessary to consider rewards all the way to the end of the episode to select an optimal
action [24, 25, 8]. One can define a “effective planning window” of W timesteps ahead that must be
considered, resulting in a sample complexity bound of T 2AW for deterministic MDPs. Finally, Dann
et al. [6] define a “myopic exploration gap” that controls the sample complexity of using ϵ-greedy
exploration, a form of naive exploration. However, in Appendix D.4, we demonstrate why their
bounds are impractical and often vacuous.
There have been a few prior attempts to bridge the RL theory-practice gap. bsuite [26], MDP
playground [27], and SEGAR [28] are collections of environments that are designed to empirically
evaluate deep RL algorithms across various axes of environment difficulty. However, they do not
provide theoretical explanations for why environments with varying properties are actually easier or
harder. Furthermore, their environments are artificially constructed to have understandable properties.
In contrast, we aim to find the mathematical reasons that deep RL succeeds or fails in “in-the-wild”
environments like Atari and Procgen. Conserva and Rauber [29] calculate two regret bounds and
compare the bounds to the empirical performance of RL algorithms. However, they consider tabular
RL algorithms in simple artificial environments with less than a thousand states, while we experiment
with deep RL algorithms on real benchmark environments with tens of millions of states.
Our GORP algorithm and the effective horizon are inspired by rollout and Monte Carlo planning
algorithms, which have a long history [30, 31, 32, 33, 24, 34, 35, 36]. These algorithms were used
effectively in Backgammon [32], Go [37], and real-time strategy games [38] before the start of deep
RL. GORP and related Monte Carlo rollout algorithms are sometimes referred to as “one-step” or
“multi-step” lookahead. Bertsekas [39, 40] suggests that one-step lookahead, possibly after a few
4

steps of value iteration, often leads to fast convergence to optimal policies because it is equivalent to
a step of Newton’s method for finding a fixed-point of the Bellman equation [41, 42]. Our analysis
suggests the success of deep RL is due to similar properties. However, we go beyond previous
work by introducing GORP, which approximates a step of policy iteration in model-free RL—a
setting where on-line planning approaches are not applicable. Furthermore, unlike previous work in
Monte-Carlo planning, we combine our theoretical contributions with extensive empirical analysis to
verify that our assumptions hold in common environments.
4
The BRIDGE Dataset
Acting greedily
PPO finds optimal policy
with respect to
in ≤5M timesteps?
Qπrand is optimal?
Yes
No
Yes
80 MDPs
24 MDPs
No
15 MDPs
36 MDPs
Table 1: The distribution of the MDPs in our
BRIDGE dataset according to two criteria: first,
whether PPO empirically converges to an optimal
policy in 5 million timesteps, and second, whether
acting greedily with respect to the Q-function of the
random policy is optimal. We find that a surprising
number of environments satisfy the latter property,
especially when only considering those where PPO
succeeds.
In order to assess how well existing bounds pre-
dict practical performance, and gain insight about
novel properties of MDPs that could be predictive,
we constructed BRIDGE (Bridging the RL Interdis-
ciplinary Divide with Grounded Environments), a
dataset of 155 popular deep RL benchmark environ-
ments with full tabular representations. One might
assume that the ability to calculate the instance-
dependent bounds we just presented in Section 3
already exists; however, it turns out that for many
real environments even the number of states S is
unknown! This is because a significant engineering
effort is required to analyze large-scale environments
and calculate their properties.
In BRIDGE, we tackle this problem by computing
tabular representations for all the environments using
a program that exhaustively enumerates all states,
calculating the reward and transition functions at every state-action pair. We do this for 67 Atari
games from the Arcade Learning Enivornment [43], 55 levels from the Procgen Benchmark [44], and
33 gridworlds from MiniGrid [14] (Figure 5). The MDPs have state space sizes S ranging across 7
orders of magnitude from tens to tens of millions, 3 to 18 discrete actions, and horizons T ranging
from 10 to 200, which are limited in some cases to avoid the state space becoming too large. See
Appendix E for the full details of the BRIDGE dataset.
A surprisingly common property
To motivate the effective horizon, which is introduced in
the next section, we describe a property that we find holds in many of the MDPs in BRIDGE.
Consider the random policy πrand, which assigns equal probability to every action in every state, i.e.,
πrand
t
(a | s) = 1/A. We can use dynamic programming on a tabular MDP to calculate the random
policy’s Q-function Qπrand. We denote by Π(Qπrand) the set of policies which act greedily with respect
to this Q-function; that is,
Π

Qπrand
=
n
π | ∀s, t
πt(s) ∈arg max
a∈A Qπrand
t
(s, a)
o
.
Perhaps surprisingly, we find that all the policies in Π(Qπrand) are optimal in about two-thirds of the
MDPs in BRIDGE. This proportion is even higher when considering only the environments where
PPO empirically succeeds in finding an optimal policy (Table 1). Thus, it seems that this property
may be the key to what makes many of these environments tractable for deep RL.
5
The Effective Horizon
We now theoretically analyze why RL should be tractable in environments where, as we observe in
BRIDGE, it is optimal to act greedily with respect to the random policy’s Q-function. This leads to a
more general measure of an environment’s complexity for model-free RL: the effective horizon.
Our analysis centers around a simple algorithm, GORP (Greedy Over Random Policy), shown in
Algorithm 1. GORP constructs an optimal policy iteratively; each iteration i aims to calculate an
optimal policy πi for timestep t = i. In the case where we set k = 1 and πexpl = πrand, GORP can
solve environments which have the property we observe in BRIDGE. It does this at each iteration i
by simulating m random rollouts for each action from the state reached at timestep t = i. Then, it
5

averages the m rollouts’ returns to obtain a Monte Carlo estimate of Qπrand for each action. Finally, it
greedily picks the action with the highest estimated Q-value.
Algorithm 1 The Greedy Over Random Policy (GORP)
algorithm, used to motivate the effective horizon.
1: procedure GORP(k, m, πexpl)
2:
for i = 1, . . . , T do
3:
for ai:i+k−1 ∈Ak do
4:
sample m episodes following π1, . . . , πi−1,
then actions ai:i+k−1, and finally πexpl.
5:
ˆQi(si, ai:i+k−1) ←
1
m
Pm
j=1
PT
t=i γt−iR(sj
t, aj
t).
6:
end for
7:
πi(si) ←arg maxai∈A
maxai+1:i+k−1∈Ak−1 ˆQi(si, ai, ai+1:i+k−1).
8:
end for
9:
return π
10: end procedure
Besides taking advantage of the surprising
property we found in BRIDGE, GORP has
other properties which help us bridge the
theory-practice gap in RL. It explores ran-
domly, like common deep RL algorithms,
meaning that it can give us insight into why
random exploration works much better than
the worst-case given in Theorem 2.1. Also,
unlike other RL algorithms, it has cleanly
separated exploration and learning stages,
making it much easier to analyze than al-
gorithms in which exploration and learning
are entangled.
Furthermore, GORP is extensible beyond
environments satisfying the property we
found in BRIDGE. First, it can solve MDPs
where one may have to apply a few steps
of value iteration to the random policy’s Q-
function before acting randomly. Second, it can use an “exploration policy” πexpl different from
the random policy πrand. These two generalizations are captured in the following definition. In
the definition, we use the notation that a step of Q-value iteration transforms a Q-function Q to
Q′ = QVI(Q), where
Q′
t(s, a) = Rt(s, a) + max
a′∈A Qt+1 (f(s, a), a′) .
Definition 5.1 (k-QVI-solvable). Given an exploration policy πexpl (πexpl = πrand unless otherwise
noted), let Q1 = Qπexpl and Qi+1 = QVI(Qi) for i = 1, . . . , T −1. We say an MDP is k-QVI-solvable
for some k ∈[T] if every policy in Π(Qk) is optimal.
We will see that running GORP with k > 1 will allow it to find an optimal policy in MDPs that are
k-QVI-solvable. Although the sample complexity of GORP scales with Ak, we find that nearly all of
the environments in BRIDGE are k-QVI-solvable for very small values of k (Figure 6).
We now use GORP to define the effective horizon of an MDP. Note that the total number of timesteps
sampled by GORP with parameters k and m is T 2Akm = T 2Ak+logA m. Thus, analogously to how
the horizon T appears in the exponent of the worst-case sample complexity bound O(TAT ), we
define the effective horizon as the exponent of A in the sample complexity of GORP:
Definition 5.2 (Effective horizon). Given k ∈[T], let Hk = k + logA mk, where mk is the minimum
value of m needed for Algorithm 1 with parameter k to return the optimal policy with probability at
least 1/2, or ∞if no value of m suffices. The effective horizon is H = mink Hk.
By definition, the sample complexity of GORP can be given using the effective horizon:
Lemma 5.3. The sample complexity of GORP with optimal choices of k and m is T 2AH.
As we noted in the introduction, when H ≪T, as we find is often true in practice, this is far better
than the worst-case bound given in Theorem 2.1 which scales with AT .
Definition 5.2 does not give a method for actually calculating the effective horizon. It turns out we
can bound the effective horizon using a generalized gap notion like those found throughout the RL
theory literature. We denote by ∆k
t the gap of the Q-function Qk from Definition 5.1, where
∆k
t (s) = max
a∈A Qk
t (s, a) −
max
a′̸∈arg maxa Qk
t (s,a) Qk
t (s, a′).
The following theorem gives bounds on the effective horizon in terms of this gap.
Theorem 5.4. Suppose that an MDP is k-QVI-solvable and that all rewards are nonnegative, i.e.
R(s, a) ≥0 for all s, a. Let ∆k denote the gap of the Q-function Qk as defined in Definition 5.1.
6

s1
s2
R = 0
. . .0
s2
0
. . .
R = 0
. . .
0
. . .
0
. . .
0
sT
R = 1
sT
0
sT
0
sT
0
(a) Sparse rewards:
when
only one sequence of actions
gives a reward of 1 and all
others give 0, the effective
horizon H = ˜
O(T ).
s1
s2
R = 1
. . .0
s2
0
. . .
R = 1
. . .
0
. . .
0
. . .
0
sT
R = 1
sT
0
sT
0
sT
0
(b) Dense rewards: when ev-
ery optimal action gives a re-
ward of 1 and suboptimal ac-
tions give no reward, the ef-
fective horizon H = ˜
O(1).
s1
s2
R = 0
s2
0
. . .
R = 0
. . .
0
. . .0
sT
R = T
sT
T −1
sT
1
sT
0
(c) Delayed rewards: when
the rewards in (b) are all
delayed to the end of the
episode, the effective hori-
zon remains ˜
O(1).
Q1
10(s, NOOP) = .118
Q1
10(s, UP) = .136
Q1
10(s, DOWN) = .116
Q∗
10(s, NOOP) = 4
Q∗
10(s, UP) = 5
Q∗
10(s, DOWN) = 4
(d) For the first 50 timesteps of the Atari
game Freeway, we can bound H ≤10.2,
which is much lower than the horizon T =
50.
Figure 2: Examples of calculating the effective horizon H using Theorem 5.4; see Section 5.1 for the details.
Then
Hk ≤k +
max
t∈[T ],s∈Sopt
i ,a∈A logA
Qk
t (s, a)V ∗
t (s)
∆k
t (s)2

+ logA 6 log
 2TAk
,
where Sopt
i
is the set of states visited by some optimal policy at timestep i and V ∗
t (s) = maxπ V π
t (s)
is the optimal value function.
A full proof of Theorem 5.4 is given in Appendix A. Intuitively, the smaller the gap ∆k
t (s), the more
precisely we must estimate the Q-values in GORP in order to pick an optimal action.
The GORP algorithm is very amenable to theoretical analysis because it reduces the problem of
finding an optimal policy to the problem of estimating several k-step Q-values, each of which is a
simple mean of i.i.d. random variables. There are endless tail bounds that can be applied to analysis
of GORP; we use some of these to obtain even tighter bounds on the effective horizon in Appendix C.
Why should the effective horizon, which is defined in terms of our GORP algorithm, also explain the
performance of deep RL algorithms like PPO and DQN which are very different from GORP? In
Appendix B, we present two algorithms, PG-GORP and FQI-GORP, which are more similar to PPO
and DQN but whose sample complexities can still be bounded with the effective horizon. We also
give additional bounds on the effective horizon and lower bounds on sample complexity.
5.1
Examples of the effective horizon
To gain some intuition for the bound in Theorem 5.4, consider the examples in Figure 2. MDP (a)
has extremely sparse rewards, with a reward of 1 only given for a single optimal action sequence.
However, note that this MDP is still 1-QVI-solvable by Definition 5.1. The maximum of the bound
in Theorem 5.4 is at t = 1 with the optimal action, where Q1
1(s, a) = 1/AT −1, V ∗
1 (s) = 1, and
∆1
1(s) = 1/AT −1. Ignoring logarithmic factors and constants gives H ≲1 + logA AT −1 = T. That
is, in the case of MDP (a), the effective horizon is no better than the horizon.
Next, consider MDP (b), which has dense rewards of 1 for every optimal action. Again, this MDP
is 1-QVI-solvable. The maximum in the bound is obtained at t = 1 and the optimal action with
Q1
1(s, a) ≤2, V ∗(s) = T, and the gap ∆1
1(s, a) ≥1. Again ignoring logarithmic factors gives in
this case H ≲1+logA T = ˜O(1). In this case, the effective horizon is much shorter than the horizon,
and barely depends on it! This again reflects our intuition that in this case, finding the optimal policy
via RL should be much easier.
MDP (c) is similar to MDP (b) except that all rewards are delayed to the end of the episode. In this
case, the Q function is the same as in MDP (b) so the effective horizon remains ˜O(1). This may seem
counterintuitive since one needs to consider rewards T timesteps ahead to act optimally. However, the
way GORP uses random rollouts to evaluate leaf nodes means that it can implicitly consider rewards
quite far in the future even without needing to exhaustively plan that many timesteps ahead.
Finally, consider MDP (d), the first 50 timesteps of the Atari game Freeway, which is included in the
BRIDGE dataset. This MDP is also 1-QVI-solvable and the maximum in the bound is obtained in
the state shown in Figure 2d at timestep t = 10. Plugging in the Q values shown in the figure gives
H ≤10.2, which is far lower than the horizon T = 50. The low effective horizon reflects how this
MDP is much easier than the worst case in practice. Both PPO and DQN are able to solve it with a
sample complexity of less than 1.5 million timesteps, while the worst case bound would suggest a
7

sample complexity greater than 50 × 350/2 ≈1025 timesteps!
Comparison to other bounds
Intuitively, why might the effective horizon give better sample
complexity bounds than previous works presented in Section 3? The MDP in Figure 2b presents a
problem for the covering length and UCB-based bounds, both of which are Ω(AT ). The exponential
sample complexity arises because these bounds depend on visiting every state in the MDP during
training. In contrast, GORP doesn’t need to visit every state to find an optimal policy. The effective
horizon of ˜O(1) for MDP (b) reflects this, showing that our effective horizon-based bounds can
actually be much smaller than the state space size, which is on the order of AT for MDP (b).
The effective planning window (EPW) does manage to capture the same intuition as the effective
horizon in the MDP in Figure 2b: in this case, W = 1. However, the analysis based on the EPW is
unsatisfactory because it entirely ignores rewards beyond the planning window. Thus, in MDP (c) the
EPW W = T, making EPW-based bounds no better than the worst case. In contrast, the effective
horizon-based bound remains the same between MDPs (b) and (c), showing that it can account for the
ability of RL algorithms to use rewards beyond the window where exhaustive planning is possible.
6
Experiments
We now show that sample complexity bounds based on the effective horizon predict the empirical
performance of deep RL algorithms far better than other bounds in the literature. For each MDP in
the BRIDGE dataset, we run deep RL algorithms to determine their empirical sample complexity. We
also use the tabular representations of the MDPs to calculate the effective horizon and other sample
complexity bounds for comparison.
Deep RL algorithms
We run both PPO and DQN for five million timesteps for each MDP in
BRIDGE, and record the empirical sample complexity (see Appendix F for hyperparameters and
experiment details). PPO converges to the optimal policy in 95 of the 155 MDPs, and DQN does in
117 of 155. At least one of the two finds the optimal policy in 119 MDPs.
Sample complexity bounds
We also compute sample complexity bounds for each MDP in BRIDGE.
These include the worst-case bound of TAT from Theorem 2.1, the effective-horizon-based bound of
T 2AH from Lemma 5.3, as well as three other bounds from the literature, introduced in Section 3
and proved in Appendix D: the UCB-based bound SAT, the covering-length-based bound TL, and
the effective planning window (EPW)-based bound of T 2AW .
Evaluation metrics
To determine which sample complexity bounds best reflect the empirical
performance of PPO and DQN, we compute a few summary metrics for each bound. First, we
measure the Spearman (rank) correlation between the sample complexity bounds and the empirical
sample complexity over environments where the algorithm converged to the optimal policy. The
correlation (higher is better) is a useful for measuring how well the bounds can rank the relative
difficulty of RL in different MDPs.
Second, we compute the median ratio between the sample complexity bound and the empirical
sample complexity for environments where the algorithm converged. The ratio between the bound
Nbound and empirical value Nemp is calculated as max{Nbound/Nemp, Nemp/Nbound}. For instance, a
median ratio of 10 indicates that half the sample complexity bounds were within a factor of 10 of
the empirical sample complexity. Lower values indicate a better bound; this metric is useful for
determining whether the sample complexity bounds are vacuous or tight.
Finally, we consider the binary classification task of predicting whether the algorithm will converge
at all within five million steps using the sample complexity bounds. That is, we consider simply
thresholding each sample complexity bound and predicting that only environments with bounds
below the threshold will converge. We compute the area under the ROC curve (AUROC) for this
prediction task as well as the accuracy with the optimal threshold. Higher AUROC and accuracy
both indicate a better bound.
Results
The results of our experiments are shown in Table 2. The effective horizon-based bounds
have higher correlation with the empirical sample complexity than the other bounds for both PPO
and DQN. While the EPW-based bounds are also reasonably correlated, they are significantly off
in absolute terms: the typical bound based on the EPW is 3-4 orders of magnitude off, while the
effective horizon yields bounds that are typically within 1-2 orders of magnitude. The UCB-based
8

PPO
DQN
Bound
Correl.
Median ratio
AUROC
Acc.
Correl.
Median ratio
AUROC
Acc.
Worst-case (T⌈AT /2⌉)
0.24
7.2 × 1010
0.57
0.63
0.15
5.5 × 1010
0.67
0.76
Covering length (TL)
0.35
6.3 × 106
0.78
0.72
0.27
3.9 × 106
0.86
0.85
EPW (T 2AW )
0.69
1.1 × 105
0.78
0.75
0.58
8.0 × 104
0.88
0.85
UCB (SAT)
0.26
20
0.68
0.67
0.31
31
0.67
0.77
Effective horizon (T 2AH)
0.81
31
0.92
0.86
0.74
67
0.92
0.86
Other deep RL algorithm
0.77
2.3
0.84
0.85
0.77
2.3
0.86
0.99
GORP empirical
0.79
7.3
0.77
0.82
0.65
11
0.80
0.94
Table 2: Effective horizon-based sample complexity bounds are the most predictive of the real performance
of PPO and DQN according to the four metrics we describe in Section 6. The effective horizon bounds are
about as good at predicting the sample complexity of PPO and DQN as one algorithm’s sample complexity is at
predicting the other’s.
PPO
DQN
Bound
Correl.
Ratio
Correl.
Ratio
EPW
0.20
2.1
0.70
12
Effective horizon
0.48
2.4
0.35
12
Other bounds
0.00
1.3
0.00
1.9
(a) Reward shaping.
PPO
Bound
Correl.
Ratio
Covering length
-0.36
2.5 × 104
Effective horizon
0.57
2.7
Other bounds
0.00
2.2
(b) Initializing with a policy trained on human
data or transferred from similar environments.
Table 3: The effective horizon explains the effects of reward shaping and initializing with a pretrained policy
by accurately predicting their effects on the empirical sample complexity of PPO and DQN. Correlation and
median ratio are measured between the predicted change in sample complexity and the empirical change. See
Section 6 for further discussion.
bounds are somewhat closer to the empirical sample complexity, but are not well correlated; this
makes sense since the UCB bounds depend on strategic exploration, while PPO and DQN use random
exploration. Finally, the effective horizon bounds are able to more accurately predict whether PPO or
DQN will find an optimal policy, as evidenced by the AUROC and accuracy metrics.
As an additional baseline, we also calculate the four evaluation metrics when using the empirical
sample complexity of PPO to predict the empirical sample complexity of DQN, or vice-versa, and
when using the empirical sample complexity of GORP to predict PPO or DQN’s performance (bottom
two rows of Table 2). While these are not provable bounds, they provide another point of comparison
for each metric. The effective horizon-based bounds correlate about as well with PPO and DQN’s
sample complexities as they do with each other’s. The empirical performance of GORP is typically
even closer to that of PPO and DQN than the effective horizon-based bounds.
Reward shaping, human data, and transfer learning
In order for RL theory to be useful
practically, it should help practitioners make decisions about which techniques to apply in order to
improve their algorithms’ performance. We show how the effective horizon can be used to explain
the effect of three such techniques: reward shaping, using human data, and transfer learning.
Potential-based reward shaping [45] is a classic technique which can speed up the convergence of
RL algorithms. It is generally used to transform a sparse-reward MDP like the one in Figure 2a to a
dense-reward MDP like in Figure 2b without changing the optimal policy. If the effective horizon is a
good measure of the difficulty of RL in an environment, then it should be able to predict whether (and
by how much) the sample complexity of practical algorithms changes when reward shaping is applied.
We develop 77 reward-shaped versions of the original 33 Minigrid environments and run PPO and
DQN. Results in Table 3a show the effective horizon accurately captures the change in sample
complexity from using reward shaping. We use similar metrics to those in Table 2: the correlation
between the predicted and empirical ratio of the shaped sample complexity to the unshaped sample
complexity, and the median ratio between the predicted change and the actual change.
Out of the five bounds we consider, three—worst case, covering length, and UCB—don’t even depend
on the reward function. The EPW does depend on the reward function and captures some of the
effect of reward shaping for DQN, but does worse at predicting the effect on PPO. In comparison, the
effective horizon does well for both algorithms, showing that it can accurately capture how reward
shaping affects RL performance.
9

0
50M
1k
2k
3k
Beam Rider
0
50M
0
250
Breakout
0
50M
0
1k
Enduro
0
50M
-20
0
20
Pong
0
50M
0
20k
Qbert
0
50M
0
2k
Seaquest
0
50M
0
2k
Space Invaders
GORP
PPO
DQN
Figure 3: Learning curves for PPO, DQN, and GORP on full-horizon Atari games. We use 5 random seeds for
all algorithms. The solid line shows the median return throughout training while the shaded region shows the
range of returns over random seeds.
Another tool used to speed up RL is initializing with a pre-trained policy, which is used practically
to make RL work on otherwise intractable tasks. Can the effective horizon also predict whether
initializing RL with a pre-trained policy will help? We initialize PPO with pre-trained policies for 82
of the MDPs in BRIDGE, then calculate new sample complexity bounds based on using the pre-trained
policies as an exploration policy πexpl. Table 3b shows that the effective horizon accurately predicts
the change in sample complexity due to using a pre-trained policy. Again, three bounds—worst case,
EPW, and UCB—do not depend on the exploration policy at all, while the covering length gives
wildly inaccurate predictions for its effect. In contrast, the effective horizon is accurate at predicting
the changes in sample complexities when using pre-trained policies.
Long-horizon environments
We also perform experiments on full-length Atari games to evaluate
the predictive power of the effective horizon in longer-horizon environments. It is intractable to con-
struct tabular representations of these environments and thus we cannot compute instance-dependent
sample complexity bounds. However, it is still possible to compare the empirical performance of
PPO, DQN, and GORP. If the performance of GORP is close to that of PPO and DQN, then this
suggests that the effective horizon, which is defined in terms of GORP, can explain RL performance
in these environments as well. Figure 3 compares the learning curves of PPO, DQN, and GORP in
deterministic versions of the seven Atari games from Mnih et al. [46] (see Appendix F.1 for details).
GORP performs better than both PPO and DQN in two games and better than at least one deep RL
algorithm in an additional three games. This provides evidence that the effective horizon is also
predictive of RL performance in long-horizon environments.
7
Discussion
Overall, our results suggest the effective horizon is a key measure of the difficulty of solving an MDP
via reinforcement learning. The intuition behind the effective horizon presented in Section 5 and the
empirical evidence in Section 6 both support its importance for better understanding RL.
Limitations
While we have presented a thorough theoretical and empirical justification of the
effective horizon, there are still some limitations to our analysis. First, we focus on deterministic
MDPs with discrete action spaces, leaving the extension to stochastic environments and those with
continuous action spaces an open question. Furthermore, the effective horizon is not easily calculable
without full access to the MDP’s tabular representation. Despite this, it serves as a useful perspective
for understanding RL’s effectiveness and potential improvement areas. An additional limitation is
that the effective horizon cannot capture the performance effects of generalization—the ability to
use actions that work well at some states for other similar states. For an example where the effective
horizon fails to predict generalization, see Appendix G.1. However, the effective horizon is still quite
predictive of deep RL performance even without modeling generalization.
Implications and future work
We hope that this paper helps to bring the theoretical and empirical
RL communities closer together in pursuit of shared understanding. Theorists can extend our analysis
of the effective horizon to new algorithms or explore related properties, using our BRIDGE dataset to
ground their investigations by testing assumptions in real environments. Empirical RL researchers
can use the effective horizon as a foundation for new algorithms. For instance, Brandfonbrener et al.
[47] present an offline RL algorithm similar to GORP with k = 1; our theoretical understanding of
GORP might provide insights for improving it or developing related algorithms.
10

Acknowledgments and Disclosure of Funding
We would like to thank Jacob Steinhardt, Kush Bhatia, Lawrence Chan, Paria Rashidinejad, Ruiqi
Zhong, and Yaodong Yu for feedback on drafts as well as Jiantao Jiao and Banghua Zhu for helpful
discussions.
This work was partially supported by the Open Philanthropy Project, the Office of Naval Researcher’s
Young Investigator Program (YIP), and Weill Neurohub. Cassidy Laidlaw is supported by an Open
Philanthropy AI Fellowship and a National Defense Science and Engineering Graduate (NDSEG)
Fellowship. Stuart Russell is supported by an AI 2050 Senior Fellowship from Schmidt Futures.
References
[1] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334–1373, January
2016. ISSN 1532-4435.
[2] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,
Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Tim-
othy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hass-
abis.
Mastering the game of Go with deep neural networks and tree search.
Nature,
529(7587):484–489, January 2016.
ISSN 1476-4687.
doi: 10.1038/nature16961.
URL
https://www.nature.com/articles/nature16961. Bandiera_abtest: a Cg_type: Na-
ture Research Journals Number: 7587 Primary_atype: Research Publisher: Nature Publish-
ing Group Subject_term: Computational science;Computer science;Reward Subject_term_id:
computational-science;computer-science;reward.
[3] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G.
Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig
Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Ku-
maran, Daan Wierstra, Shane Legg, and Demis Hassabis.
Human-level control through
deep reinforcement learning. Nature, 518(7540):529–533, February 2015. ISSN 1476-4687.
doi: 10.1038/nature14236.
URL https://www.nature.com/articles/nature14236.
Bandiera_abtest: a Cg_type: Nature Research Journals Number: 7540 Primary_atype: Re-
search Publisher: Nature Publishing Group Subject_term: Computer science Subject_term_id:
computer-science.
[4] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax Regret Bounds for
Reinforcement Learning. arXiv:1703.05449 [cs, stat], July 2017. URL http://arxiv.org/
abs/1703.05449. arXiv: 1703.05449.
[5] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is Q-learning Provably
Efficient? arXiv:1807.03765 [cs, math, stat], July 2018. URL http://arxiv.org/abs/
1807.03765. arXiv: 1807.03765.
[6] Chris Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, and Karthik Sridharan. Guaran-
tees for Epsilon-Greedy Reinforcement Learning with Function Approximation. In Proceedings
of the 39th International Conference on Machine Learning, pages 4666–4689. PMLR, June
2022. URL https://proceedings.mlr.press/v162/dann22a.html. ISSN: 2640-3498.
[7] Yao Liu and Emma Brunskill. When Simple Exploration is Sample Efficient: Identifying
Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms, April 2019. URL
http://arxiv.org/abs/1805.09045. arXiv:1805.09045 [cs, stat].
[8] Dhruv Malik, Aldo Pacchiano, Vishwak Srinivasan, and Yuanzhi Li. Sample Efficient Reinforce-
ment Learning In Continuous State Spaces: A Perspective Beyond Linearity. In Proceedings of
the 38th International Conference on Machine Learning, pages 7412–7422. PMLR, July 2021.
URL https://proceedings.mlr.press/v139/malik21c.html. ISSN: 2640-3498.
[9] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
Policy Optimization Algorithms. arXiv:1707.06347 [cs], August 2017. URL http://arxiv.
org/abs/1707.06347. arXiv: 1707.06347.
11

[10] Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney,
Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining Improve-
ments in Deep Reinforcement Learning, October 2017. URL http://arxiv.org/abs/1710.
02298. arXiv:1710.02298 [cs].
[11] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy
Lillicrap, and David Silver. Mastering Atari, Go, Chess and Shogi by Planning with a Learned
Model. Nature, 588(7839):604–609, December 2020. ISSN 0028-0836, 1476-4687. doi: 10.
1038/s41586-020-03051-4. URL http://arxiv.org/abs/1911.08265. arXiv:1911.08265
[cs, stat].
[12] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin
Riedmiller. DeepMind Control Suite, January 2018. URL http://arxiv.org/abs/1801.
00690. arXiv:1801.00690 [cs].
[13] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
5026–5033, October 2012. doi: 10.1109/IROS.2012.6386109. ISSN: 2153-0866.
[14] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic Gridworld Envi-
ronment for OpenAI Gym, 2018. URL https://github.com/maximecb/gym-minigrid.
Publication Title: GitHub repository.
[15] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State
Entropy Maximization with Random Encoders for Efficient Exploration. In Proceedings of
the 38th International Conference on Machine Learning, pages 9443–9454. PMLR, July 2021.
URL https://proceedings.mlr.press/v139/seo21a.html. ISSN: 2640-3498.
[16] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew
Critch, and Sergey Levine. Emergent Complexity and Zero-shot Transfer via Unsupervised
Environment Design. In Advances in Neural Information Processing Systems, volume 33, pages
13049–13061. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/985e9a46e10005356bbaf194249f6856-Abstract.html.
[17] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan
Saharia, Thien Huu Nguyen, and Yoshua Bengio. BabyAI: A Platform to Study the Sample
Efficiency of Grounded Language Learning, December 2019. URL http://arxiv.org/abs/
1810.08272. arXiv:1810.08272 [cs].
[18] Sham Machandranath Kakade. On the sample complexity of reinforcement learning. PhD thesis,
University of London, University College London (United Kingdom), 2003.
[19] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire.
Contextual Decision Processes with low Bellman rank are PAC-Learnable. In Proceedings of
the 34th International Conference on Machine Learning, pages 1704–1713. PMLR, July 2017.
URL https://proceedings.mlr.press/v70/jiang17c.html. ISSN: 2640-3498.
[20] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably Efficient Reinforcement
Learning with Linear Function Approximation. arXiv:1907.05388 [cs, math, stat], August
2019. URL http://arxiv.org/abs/1907.05388. arXiv: 1907.05388.
[21] Simon Du, Sham Kakade, Jason Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong
Wang. Bilinear Classes: A Structural Framework for Provable Generalization in RL. In
Proceedings of the 38th International Conference on Machine Learning, pages 2826–2836.
PMLR, July 2021. URL https://proceedings.mlr.press/v139/du21a.html. ISSN:
2640-3498.
[22] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
Bellman Eluder Dimension:
New
Rich Classes of RL Problems, and Sample-Efficient Algorithms.
In Advances in
Neural Information Processing Systems, volume 34, pages 13406–13418. Curran As-
sociates, Inc., 2021.
URL https://proceedings.neurips.cc/paper/2021/hash/
6f5e4e86a87220e5d361ad82f1ebc335-Abstract.html.
12

[23] Ronen I. Brafman and Moshe Tennenholtz. R-MAX - A General Polynomial Time Algorithm
for Near-Optimal Reinforcement Learning. Journal of Machine Learning Research, 3(Oct):213–
231, 2002. ISSN ISSN 1533-7928. URL https://www.jmlr.org/papers/v3/brafman02a.
html.
[24] Michael Kearns, Yishay Mansour, and Andrew Y. Ng. A Sparse Sampling Algorithm for
Near-Optimal Planning in Large Markov Decision Processes. Machine Learning, 49(2):193–
208, November 2002. ISSN 1573-0565. doi: 10.1023/A:1017932429737. URL https:
//doi.org/10.1023/A:1017932429737.
[25] Nan Jiang, Satinder Singh, and Ambuj Tewari. On structural properties of MDPs that bound
loss due to shallow planning. In Proceedings of the Twenty-Fifth International Joint Conference
on Artificial Intelligence, IJCAI’16, pages 1640–1647, New York, New York, USA, July 2016.
AAAI Press. ISBN 978-1-57735-770-4.
[26] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva,
Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy,
Richard Sutton, David Silver, and Hado Van Hasselt. Behaviour Suite for Reinforcement
Learning, February 2020. URL http://arxiv.org/abs/1908.03568. arXiv:1908.03568
[cs, stat].
[27] Raghu Rajan, Jessica Lizeth Borja Diaz, Suresh Guttikonda, Fabio Ferreira, André Biedenkapp,
Jan Ole von Hartz, and Frank Hutter.
MDP Playground: A Design and Debug Testbed
for Reinforcement Learning, June 2021.
URL http://arxiv.org/abs/1909.07750.
arXiv:1909.07750 [cs, stat].
[28] R. Devon Hjelm, Bogdan Mazoure, Florian Golemo, Felipe Frujeri, Mihai Jalobeanu, and
Andrey Kolobov. The Sandbox Environment for Generalizable Agent Research (SEGAR),
March 2022. URL http://arxiv.org/abs/2203.10351. arXiv:2203.10351 [cs].
[29] Michelangelo Conserva and Paulo Rauber. Hardness in Markov Decision Processes: Theory
and Practice, October 2022. URL http://arxiv.org/abs/2210.13075. arXiv:2210.13075
[cs].
[30] B. Abramson.
Expected-outcome:
a general model of static evaluation.
IEEE
Transactions
on
Pattern
Analysis
and
Machine
Intelligence,
12(2):182–193,
February 1990.
ISSN 1939-3539.
doi:
10.1109/34.44404.
URL https://
ieeexplore.ieee.org/abstract/document/44404?casa_token=uj3nSMaKLJcAAAAA:
d7LSY9mmrlNL8qhkLN6WEKgrZpmv1pRnvwj0cAolitRmNtbTUFegNNstXuXCEwBnrPayNKBO.
Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence.
[31] Bernd
Brügmann.
Monte
Carlo
Go.
1993.
URL
https://
www.semanticscholar.org/paper/Monte-Carlo-Go-Max-Planck/
5f8b18be86be69077e66377e03198d21d06833f3.
[32] Gerald Tesauro and Gregory Galperin.
On-line Policy Improvement using Monte-
Carlo Search.
In Advances in Neural Information Processing Systems, volume 9.
MIT Press,
1996.
URL https://proceedings.neurips.cc/paper/1996/hash/
996009f2374006606f4c0b0fda878af1-Abstract.html.
[33] Dimitri P. Bertsekas, John N. Tsitsiklis, and Cynara Wu. Rollout Algorithms for Combinatorial
Optimization. Journal of Heuristics, 3(3):245–262, December 1997. ISSN 1572-9397. doi:
10.1023/A:1009635226865. URL https://doi.org/10.1023/A:1009635226865.
[34] Hyeong Soo Chang, Michael C. Fu, Jiaqiao Hu, and Steven I. Marcus. An Adaptive Sampling
Algorithm for Solving Markov Decision Processes. Operations Research, 53(1):126–139,
February 2005. ISSN 0030-364X. doi: 10.1287/opre.1040.0145. URL https://pubsonline.
informs.org/doi/10.1287/opre.1040.0145. Publisher: INFORMS.
[35] Levente Kocsis and Csaba Szepesvári. Bandit Based Monte-Carlo Planning. In Johannes
Fürnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006,
Lecture Notes in Computer Science, pages 282–293, Berlin, Heidelberg, 2006. Springer. ISBN
978-3-540-46056-5. doi: 10.1007/11871842_29.
13

[36] Rémi Coulom. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search. In
H. Jaap van den Herik, Paolo Ciancarini, and H. H. L. M. (Jeroen) Donkers, editors, Computers
and Games, Lecture Notes in Computer Science, pages 72–83, Berlin, Heidelberg, 2007.
Springer. ISBN 978-3-540-75538-8. doi: 10.1007/978-3-540-75538-8_7.
[37] B. Bouzy and B. Helmstetter. Monte-Carlo Go Developments. In H. Jaap Van Den Herik,
Hiroyuki Iida, and Ernst A. Heinz, editors, Advances in Computer Games: Many Games, Many
Challenges, IFIP — The International Federation for Information Processing, pages 159–174.
Springer US, Boston, MA, 2004. ISBN 978-0-387-35706-5. doi: 10.1007/978-0-387-35706-5_
11. URL https://doi.org/10.1007/978-0-387-35706-5_11.
[38] Michael Chung, Michael Buro, and Jonathan Schaeffer. Monte Carlo Planning in RTS Games.
January 2005.
[39] Dimitri Bertsekas. Rollout, Policy Iteration, and Distributed Reinforcement Learning. Athena
Scientific, Belmont, Massachusetts, first edition edition, August 2020. ISBN 978-1-886529-07-
6.
[40] Dimitri P. Bertsekas. Lessons from AlphaZero for Optimal, Model Predictive, and Adaptive
Control. Athena Scientific, March 2022. ISBN 978-1-886529-17-5.
[41] D.
Kleinman.
On
an
iterative
technique
for
Riccati
equation
computa-
tions.
IEEE
Transactions
on
Automatic
Control,
13(1):114–115,
Febru-
ary
1968.
ISSN
1558-2523.
doi:
10.1109/TAC.1968.1098829.
URL
https://ieeexplore.ieee.org/abstract/document/1098829?casa_token=
tKFm80m5gI4AAAAA:E3oaouksJy-5g-HuySnACwNdxyHpRDk8IbPTssB0B-PK0qZng_
-v9Fsk7qzcyYyYVudFUrI8. Conference Name: IEEE Transactions on Automatic Control.
[42] Martin L. Puterman and Shelby L. Brumelle. On the Convergence of Policy Iteration in Station-
ary Dynamic Programming. Mathematics of Operations Research, 4(1):60–69, 1979. ISSN
0364-765X. URL https://www.jstor.org/stable/3689239. Publisher: INFORMS.
[43] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment:
An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47:
253–279, June 2013. ISSN 1076-9757. doi: 10.1613/jair.3912. URL https://www.jair.
org/index.php/jair/article/view/10819.
[44] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging Procedural
Generation to Benchmark Reinforcement Learning, July 2020. URL http://arxiv.org/
abs/1912.01588. arXiv:1912.01588 [cs, stat].
[45] Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In In Proceedings of the Sixteenth International
Conference on Machine Learning, pages 278–287. Morgan Kaufmann, 1999.
[46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning, December
2013. URL http://arxiv.org/abs/1312.5602. arXiv:1312.5602 [cs].
[47] David Brandfonbrener, William F. Whitney, Rajesh Ranganath, and Joan Bruna. Offline RL
Without Off-Policy Evaluation, December 2021. URL http://arxiv.org/abs/2106.08909.
arXiv:2106.08909 [cs, stat].
[48] Ronald J. Williams.
Simple Statistical Gradient-Following Algorithms for Connectionist
Reinforcement Learning. In Richard S. Sutton, editor, Reinforcement Learning, The Springer
International Series in Engineering and Computer Science, pages 5–32. Springer US, Boston,
MA, 1992. ISBN 978-1-4615-3618-5. doi: 10.1007/978-1-4615-3618-5_2. URL https:
//doi.org/10.1007/978-1-4615-3618-5_2.
[49] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-Based Batch Mode Reinforcement
Learning. Journal of Machine Learning Research, 6(18):503–556, 2005. ISSN 1533-7928.
URL http://jmlr.org/papers/v6/ernst05a.html.
14

[50] Martin Riedmiller. Neural Fitted Q Iteration – First Experiences with a Data Efficient Neural
Reinforcement Learning Method. In João Gama, Rui Camacho, Pavel B. Brazdil, Alípio Mário
Jorge, and Luís Torgo, editors, Machine Learning: ECML 2005, Lecture Notes in Computer
Science, pages 317–328, Berlin, Heidelberg, 2005. Springer. ISBN 978-3-540-31692-3. doi:
10.1007/11564096_32.
[51] Irina Shevtsova. On the absolute constants in the Berry-Esseen type inequalities for identi-
cally distributed summands, November 2011. URL http://arxiv.org/abs/1111.6554.
arXiv:1111.6554 [math].
[52] Eyal Even-Dar and Yishay Mansour. Learning Rates for Q-learning. Journal of Machine
Learning Research, 5(Dec):1–25, 2003. ISSN ISSN 1533-7928. URL https://www.jmlr.
org/papers/v5/evendar03a.html.
[53] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The Dependence of Effective
Planning Horizon on Model Accuracy. In Proceedings of the 2015 International Conference on
Autonomous Agents and Multiagent Systems, AAMAS ’15, pages 1181–1189, Richland, SC,
May 2015. International Foundation for Autonomous Agents and Multiagent Systems. ISBN
978-1-4503-3413-6.
[54] Alex Braylan, Mark Hollenbeck, Elliot Meyerson, and Risto Miikkulainen. Frame skip is a
powerful parameter for learning to play Atari. In Workshops at the twenty-ninth AAAI conference
on artificial intelligence, 2015.
[55] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna
Wallach, Hal Daumé III, and Kate Crawford. Datasheets for Datasets, December 2021. URL
http://arxiv.org/abs/1803.09010. arXiv:1803.09010 [cs].
[56] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. OpenAI Gym, June 2016. URL http://arxiv.org/abs/1606.
01540. arXiv:1606.01540 [cs].
[57] Ruohan Zhang, Calen Walshe, Zhuode Liu, Lin Guan, Karl S. Muller, Jake A. Whritner, Luxin
Zhang, Mary M. Hayhoe, and Dana H. Ballard. Atari-HEAD: Atari Human Eye-Tracking
and Demonstration Dataset, September 2019. URL http://arxiv.org/abs/1903.06754.
arXiv:1903.06754 [cs, stat].
[58] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah
Dormann. Stable-baselines3: Reliable reinforcement learning implementations. The Journal of
Machine Learning Research, 22(1):12348–12355, 2021. URL https://dl.acm.org/doi/
abs/10.5555/3546258.3546526. Publisher: JMLRORG.
[59] Antonin Raffin.
RL Baselines3 Zoo, 2020.
URL https://github.com/DLR-RM/
rl-baselines3-zoo. Publication Title: GitHub repository.
15

Appendix
A
Proofs of main results
A.1
Proof of Theorem 2.1
Theorem 2.1. There is an RL algorithm which can solve any deterministic MDP with sample
complexity N ≤T⌈AT /2⌉. Conversely, for any RL algorithm and any values of T and A, there must
be some deterministic MDP for which its sample complexity N ≥T(⌈AT /2⌉−1).
Proof. Consider the following RL algorithm:
procedure EXHAUSTIVESEARCH
T ←Shuffle(AT )
J is an array of size ⌈AT /2⌉
for i = 1, . . . , ⌈AT /2⌉do
run one episode, taking the actions in T [i]
J [i] ←PT
t=1 γtR(st, at)
end for
i∗←arg maxi J [i]
return the policy which takes the actions in T [i∗]
end procedure
Since the MDP is deterministic, an RL algorithm only needs to find an optimal sequence of ac-
tions. Clearly, there is at least a 1/2 chance that some optimal sequence of actions is in the first
⌈AT /2⌉elements of T . If this is the case, then EXHAUSTIVESEARCH will return an optimal pol-
icy corresponding to that optimal sequence. Since the number of environment timesteps taken by
EXHAUSTIVESEARCH is equal to T⌈AT /2⌉, we have that N ≤T⌈AT /2⌉.
For the converse, fix A and T along with any RL algorithm. Consider a set of states indexed by
sequences of actions of length 0 to T:
S = {sa1:ℓ| ℓ∈0, . . . , T, a1:ℓ∈Aℓ}.
Then, define a transition function
f(sa1:ℓ, a) = sa1:ℓ,a.
Now consider AT different MDPs which share the state space S and transition function f, differing
only in their reward functions:
M = {Ma1:T | a1:T ∈AT }
where the MDP Ma1:T has R(sa1:ℓ, a) =
1
a1:ℓ, a = a1:T
0
otherwise.
That is, each MDP has a single optimal sequence of actions that gives reward 1 on the final timestep;
all other rewards are 0.
Let the RL algorithm in question take in some source of randomness z and output a policy πn
t (a |
s; z, M) after n timesteps in MDP M. Now suppose by way of contradiction the sample complexity
of the algorithm is less than T(⌈AT /2⌉−1) in all MDPs in M. By our definition of sample complexity,
this means that
∀M ∈M
Pz

J

πT (⌈AT /2⌉−2)
t
(·; z, M)

= 1

≥1/2.
(1)
Clearly a policy can only be optimal in these MDPs if it is deterministic, so let τ(z, M) be the
sequence of actions that πT (⌈AT /2⌉−2)
t
(a | s; z, M) takes if it is optimal, and let it be any suboptimal
sequence of actions if the policy is suboptimal. We can rewrite (1) as
∀Ma1:T ∈M
Pz (τ(z, Ma1:T ) = a1:T ) ≥1/2.
Letting Unif(AT ) define a uniform distribution over action sequences of length T, this implies
Pa1:T ∼Unif(AT ),z

τ(z, Ma1:T ) = a1:T

≥1/2
which means that there must be some particular z such that
Pa1:T ∼Unif(AT )

τ(z, Ma1:T ) = a1:T

≥1/2

a1:T ∈AT | τ(z, Ma1:T ) = a1:T
	 ≥AT /2
(2)
16

Given that the RL algorithm is now deterministic due to the fixed z, we will prove that the LHS of (2)
must be less than or equal to ⌈AT /2⌉−1. This can be shown via induction. For the first episode,
the algorithm must take the same actions in every MDP since all MDPs give zero reward until the
final action (so there is no way to distinguish them). After the first episode, only one MDP can be
distinguished from the others: the one corresponding to the action sequence taken in the first episode,
which has reward 1 instead of 0. Thus in the remaining AT −1 MDPs the algorithm must take the
same actions in the second episode. Continuing this argument shows that after episode ⌈AT /2⌉−2,
the algorithm must still be unable to distinguish between AT −
 ⌈AT /2⌉−2

= ⌊AT /2⌋+ 2 of
the MDPs, and so τ(z, M) must be the same for all MDPs in this set. Since all of these MDPs
have different optimal action sequences, τ(z, M) can only be optimal in one of them. Thus τ(z, M)
must be suboptimal in at least ⌊AT /2⌋+ 1 MDPs, which means the LHS of (2) must be at most
AT −(⌊AT /2⌋+ 1) = ⌈AT /2⌉−1.
Combining this with (2) gives ⌈AT /2⌉−1 ≥AT /2, which is a contradiction. Thus, the sample
complexity of the RL algorithm must be at least T(⌈AT /2⌉−1).
■
A.2
Proof that k = T in the worst case
Lemma A.1. Let Q1 = Qπrand, Qi+1 = QVI(Qi) for i = 2, . . . , T, and Π(Qi) be defined as in
Section 4. Then for any horizon T and number of actions A ≥2 there is an MDP such that no policy
in Π(Qi) is optimal for i < T.
Proof. As in the proof of Theorem 2.1, define an MDP where every action sequence leads to a
different state. Pick an arbitrary action sequence a1:T , and let the reward of taking the final action in
that sequence be 1:
R(sa1:T −1, aT ) = 1.
Now take some action a′
1 ̸= a1, and let the reward for taking that action at the beginning of an
episode be 3/4:
R(s1, a′
1) = 3/4.
Let the rewards for all other state-action pairs be 0. We will show by induction that
Qi
t(sa1:t−1, at) =
1
Amax{T −i−t+1,0}
and
Qi
1(s1, a′
1) = 3/4.
(3)
The left half of (3) is clearly true for i = 1, since the random policy will take all action sequences
following the t −1-th optimal action with probability 1/AT −t, and exactly one of those gives reward
1. The right half is also clear since following a′
1 gives immediate reward of 3/4 and then no reward
afterwards.
For the inductive step, we begin with the left half of (3); assume it holds for some i. By the definition
of Q-value iteration, we have for t < T
Qi+1
t
(sa1:t−1, at)
(i)= R(sa1:t−1, at) + max
a
Qi
t+1(sa1:t, a)
= 0 +
1
Amax{T −i−(t+1)+1,0}
=
1
Amax{T −(i+1)−t+1,0}
and for t = T,
Qi+1
T
(sa1:T −1, aT ) = R(sa1:T −1, aT ) = 1 = 1
A0 =
1
Amax{T −(i+1)−t+1,0} .
(i) is because only one action at timestep t + 1 can have a Q-value greater than 0, since only one
action sequence leads to reward after taking a1. For the right half of (3), we have
Qi+1
1
(s1, a′
1) = R(s1, a′
1) + max
a
Qi
2(sa′
1, a) = 3/4 + 0 = 3/4.
which is due to no reward being possible after taking a′
1 at the first timestep.
Given that (3) holds for i = 1, . . . , T, it is easy to see that for i < T, any π ∈Π(Qi) will take a′
1 on
the first timestep, since
Qi
1(s1, a1) =
1
Amax{T −i,0} =
1
AT −i ≤1
A ≤1
2 ≤3
4 = Qi
1(s1, a′
1).
This means that π will be suboptimal, since a1 is the only optimal action at t = 1.
■
17

A.3
Proof of Lemma 5.3
Lemma 5.3. The sample complexity of GORP with optimal choices of k and m is T 2AH.
Proof. Recall the definition of effective horizon:
Definition 5.2 (Effective horizon). Given k ∈[T], let Hk = k + logA mk, where mk is the minimum
value of m needed for Algorithm 1 with parameter k to return the optimal policy with probability at
least 1/2, or ∞if no value of m suffices. The effective horizon is H = mink Hk.
Let k ∈arg mink Hk. Then by Definition 5.2, GORP (Algorithm 1) with parameters k, mk will
converge to an optimal policy with probability at least 1/2. Clearly, Algorithm 1 interacts with the
environment for T iterations, each of which require evaluating Ak action sequences with mk episodes
of T timesteps each, for a total of
T 2Akmk = T 2AHk = T 2AH
timesteps. Thus the sample complexity of GORP satisfies NGORP ≤T 2AH.
Now, suppose by way of contradiction that NGORP < T 2AH. Then this must mean that there are
parameters k′, m′ such running GORP with these parameters converges to an optimal policy with
probability at least 1/2, and
k′ + logA m′ < k + logA mk.
By Definition 5.2, this means that mk′ ≤m′, and thus
H = min
k H ≤k′ + logA mk′ ≤k′ + logA m′ < k + logA mk = H
which is clearly a contradiction. Thus, it must be that NGORP = T 2AH.
■
A.4
Proof of Theorem 5.4
Theorem 5.4. Suppose that an MDP is k-QVI-solvable and that all rewards are nonnegative, i.e.
R(s, a) ≥0 for all s, a. Let ∆k denote the gap of the Q-function Qk as defined in Definition 5.1.
Then
Hk ≤k +
max
t∈[T ],s∈Sopt
i ,a∈A logA
Qk
t (s, a)V ∗
t (s)
∆k
t (s)2

+ logA 6 log
 2TAk
,
where Sopt
i
is the set of states visited by some optimal policy at timestep i and V ∗
t (s) = maxπ V π
t (s)
is the optimal value function.
Proof. Let
m = log
 2TAk
max
t∈[T ],s∈Sopt
t ,a∈A
6Qk
t (s, a)V ∗
t (s)
∆k
t (s)2
.
We will show that GORP (Algorithm 1) converges to the optimal policy with probability at least
1/2 given parameters k and m. By Definition 5.2, this means the effective horizon must be at
most k + logA m, which gives the bound in the theorem. More precisely, we will show that GORP
converges to a policy in Π(Qk) with probability at least 1/2, which must be optimal because of the
assumption that the MDP is k-QVI-solvable.
First, we will show the following relationship between the k-action Q1 values and Qk:
Qk
i (si, ai) =
max
ai+1:i+k−1∈Ak−1 Q1
i (si, ai, ai+1:i+k−1).
(4)
We prove that (4) holds inductively. For k = 1, (4) is obviously true. Supposing it holds for k, then
Qk+1
i
(si, ai) = QVI(Qk
i )(si, ai)
= R(si, ai) + max
ai+1∈A Qk
i+1(f(si, ai), ai+1)
(i)= R(si, ai) +
max
ai+1:i+k∈Ak Q1
i+1(f(si, ai), ai+1:i+k)
=
max
ai+1:i+k∈Ak Q1
i (si, ai, ai+1:i+k),
18

which shows (4) holds for k + 1. (i) holds due to the inductive hypothesis.
Recall that in Algorithm 1, we use ˆQi(si, ai:i+k−1) to denote the estimated Q-value of the k-action
sequence ai:i+k−1. Analogously to (4), define
ˆQk
i (si, ai) =
max
ai+1:i+k−1∈Ak−1 ˆQi(si, ai, ai+1:i+k−1)
to be the maximum estimated Q-value of any action sequence that starts with ai. We can rewrite line
7 of Algorithm 1 as
πi(si) ←arg max
ai∈A
ˆQk
i (si, ai).
That is, the action that GORP selects for timestep i is chosen from those with the highest values of
ˆQk
i (si, ai). Suppose we can show that
P

arg max
ai∈A
ˆQk
i (si, ai) ⊆arg max
ai∈A Qk
i (si, ai)

≥1 −1
2T
(5)
holds for each i ∈[T]. Then by a union bound,
P

∀i ∈[T]
πi(si) ∈arg max
ai∈A Qk
i (si, ai)

≥1
2.
(6)
This implies that π ∈Π(Qk), which is the desired result.
It remains to show that (5) holds. We will actually prove the bound assuming that si ∈Sopt
i . This is
still sufficient to imply (6) since one can inductively assume that previous actions are optimal. We
can write (5) equivalently as
P

∃ai ∈arg max
ai∈A
ˆQk
i (si, ai) : ai /∈arg max
ai∈A Qk
i (si, ai)

≤1
2T .
(7)
Let a∗
i:i+k−1 ∈arg maxai:i+k−1∈Ak Q1
i (si, ai:i+k−1) be chosen arbitrarily. By (4), this implies that
a∗
i ∈arg maxai∈A Qk
i (si, ai). Then
P

∃ai ∈arg max
ai∈A
ˆQk
i (si, ai) : ai /∈arg max
ai∈A Qk
i (si, ai)

≤P

∃ai /∈arg max
ai∈A Qk
i (si, ai) : ˆQk
i (si, ai) ≥ˆQk
i (si, a∗
i )

≤P

∃ai /∈arg max
ai∈A Qk
i (si, ai), ai+1:i+k−1 ∈Ak−1 : ˆQi(si, ai, ai+1:k−1) ≥ˆQi(si, a∗
i:i+k−1)

≤
X
ai /∈arg maxai∈A Qk
i (si,ai),ai+1:i+k−1∈Ak−1
P

ˆQi(si, ai, ai+1:k−1) ≥ˆQi(si, a∗
i:i+k−1)

.
(8)
Consider a single term of the sum in (8). By the definition of the gap and the fact that ai /∈
arg maxai∈A Qk
i (si, ai), we know that
Qk
i (si, ai) ≤Qk
i (si, a∗
i ) −∆k
i (si).
Combining this with (4) implies that
Q1
i (si, ai:i+k−1) ≤Q1
i (si, a∗
i:i+k−1) −∆k
i (si)
Q1
i (si, a∗
i:i+k−1) −Q1
i (si, ai:i+k−1) ≥∆k
i (si).
(9)
Now, consider the random variables ˆQi(si, ai, ai+1:k−1) and ˆQi(si, a∗
i:i+k−1).
Let Xj
=
PT
t=i γt−iR(sj
t, aj
t) be the discounted reward from timestep i in the jth episode used to estimate
ˆQi(si, ai, ai+1:i+k−1); let Yj be the equivalent for estimating ˆQi(sia∗
i:i+k−1). Then we can write
ˆQi(si, ai, ai+1:i+k−1) = 1
m
m
X
j=1
Xj
ˆQi(si, a∗
i:i+k−1) = 1
m
m
X
j=1
Yj.
By the definition of the optimal value function V ∗and the assumption that all rewards are nonnegative,
we have that Xj, Yj ∈[0, V ∗
i (si)]. We also know that the expectations of the Xj and Yj are bounded:
E[Xj] = Q1
i (si, ai:i+k−1) ≤Qk
i (si, ai) ≤max
ai Qk
i (si, ai).
E[Yj] = Q1
i (si, a∗
i:i+k−1) = Qk
i (si, a∗
i ) ≤max
ai Qk
i (si, ai).
19

The variance of a random variable with mean µ bounded on an interval [α, β] is at most (β−µ)(µ−α).
This means we can bound the variance of Xj and Yj as well:
Var(Xj) ≤max
ai Qk
i (si, ai)V ∗
i (si)
Var(Yj) ≤max
ai Qk
i (si, ai)V ∗
i (si).
Now define
Z = ˆQ1
i (si, a∗
i:i+k−1) −ˆQ1
i (si, ai, ai+1:i+k−1) = 1
m
m
X
j=1
Yj −Xj = 1
m
m
X
j=1
Zj,
where Zj = Yj −Xj. Since Xj and Yj are independent,
Var(Zj) = Var(Xj) + Var(Yj) ≤2 max
ai Qk
i (si, ai)V ∗
i (si).
Also define centered versions ¯Zj = Zj −E[Zj] and ¯Z = Z −E[Z]. By (9), E[Z] ≥∆k
i (si).
Furthermore, since Xj, Yj ∈[0, V ∗
i (si)], we also know that |Zj| ≤V ∗
i (si) and | ¯Zj| ≤V ∗
i (si) +
E[Z] ≤2V ∗
i (si).
We can now finally apply Bernstein’s inequality to bound the probability of one term in (8):
P

ˆQi(si, ai, ai+1:k−1) ≥ˆQi(si, a∗
i:i+k−1)

= P (Z ≤0)
= P
  ¯Z ≤−E[Z]

≤P
  ¯Z ≤−∆k
i (si)

(i)
≤exp
(
−1
2m∆k
i (si)2
Var(Zj) + 2
3V ∗
i (si)∆k
i (si)
)
(ii)
≤exp
(
−1
2m∆k
i (si)2
2 maxai Qk
i (si, ai)V ∗
i (si) + 2
3V ∗
i (si)∆k
i (si)
)
(iii)
≤exp

−m∆k
i (si)2
6 maxai Qk
i (si, ai)V ∗
i (si)

(iv)
≤exp
 −log(2TAk)

=
1
2TAk .
(10)
Here, (i) is a direct application of Bernstein’s inequality to the sum ¯Z =
1
m
Pm
j=1 ¯Zj. (ii) uses the
bound on Var(Zj) = Var( ¯Zj) and (iii) uses the fact that ∆k
i (si) ≤maxai Qk
i (si, ai) by definition
of the gap ∆k
i . Finally, (iv) uses the definition of m; since si ∈Sopt
i
by assumption,
m ≥log
 2TAk
max
ai∈A
6Qk
t (si, ai)V ∗
t (si)
∆k
t (si)2
.
Applying (10) to (8) gives
P

∃ai ∈arg max
ai∈A
ˆQk
i (si, ai) : ai /∈arg max
ai∈A Qk
i (si, ai)

≤
X
ai /∈arg maxai∈A Qk
i (si,ai),ai+1:i+k−1∈Ak−1
1
2TAk
≤1
2T ,
which is the only thing left that is needed to complete the proof.
■
B
Additional theoretical results concerning the effective horizon
In this appendix, we present some additional theoretical results concerning the effective horizon. First,
we explore two algorithms—one in the style of policy gradient and one similar to fitted Q-iteration—
whose sample complexities can also be bounded by a quantity related to the effective horizon. Then
we show conditions under which the effective horizon is small, as well as information-theoretic lower
bounds for the sample complexity of RL in terms of the effective horizon.
20

B.1
PG-GORP and FQI-GORP
The two algorithms we introduce in this section, PG-GORP and FQI-GORP, can be viewed as a
bridge between GORP, which we use to define the effective horizon, and PPO and DQN, the deep
RL algorithms whose performance we predict using the effective horizon in Section 6. They help to
explain why the effective horizon is not only useful for understanding the performance of GORP, but
also other RL algorithms.
We will actually give sample complexity bounds on PG-GORP and FQI-GORP in terms of the bound
in Theorem 5.4, rather than the effective horizon itself. Supposing that the MDP is k-QVI-solvable,
define
¯Hk = k +
max
t∈[T ],s∈Sopt
i ,a∈A logA
Qk
t (s, a)V ∗
t (s)
∆k
t (s)2

+ logA 6 log
 2TAk
.
Our bounds will also depend on a quantity measuring how far the exploration policy πexpl is from the
uniformly random policy πrand:

πrand
πexpl

∞
=
max
(t,s,a)∈[T ]×S×A
1
A πexpl
t
(a | s)
.
 πrand
πexpl

∞= 1 in the case when πexpl = πrand, and increases as the smallest probabilities πexpl assigns
to actions becomes smaller.
We now introduce the first algorithm, PG-GORP.
Algorithm 2 The PG-GORP algorithm.
1: procedure PG-GORP(m)
2:
π ←πexpl.
3:
for i = 1, . . . , T do
4:
Sample m episodes following π.
5:
for a ∈A do
6:
ˆ∇i(a | si) ←
1
m
Pm
j=1
1aj
i =a
πi(a|si)
PT
t=i γt−iR(sj
t, aj
t).
7:
end for
8:
πi(si) ←arg maxa∈A ˆ∇i(a | si).
9:
end for
10:
return π
11: end procedure
PG-GORP resembles the REINFORCE algorithm [48], which gave rise to other policy gradient-based
algorithms like PPO. At each iteration, Algorithm 2 first samples a number of episodes following its
current policy (line 4). Then, it computes a an approximate gradient over the policy parameters—in
this case, just the action probabilities πi(a | si)—via the so-called “policy gradient theorem,” which
states
∇πi(·|si)J(π) ≈1
m
m
X
j=1
∇πi(·|si) log πi(aj
i | si)
T
X
t=i
γt−iR(sj
t, aj
t)
= 1
m
m
X
j=1
∇πi(·|si)πi(aj
i | si)
πi(aj
i | si)
T
X
t=i
γt−iR(sj
t, aj
t)
= ˆ∇i(· | si).
Then, in line 8, Algorithm 2 applies optimization to π based on its estimate of the gradient. In this
case, it optimizes until πi assigns all probability to only one action πi(si) at si.
Theorem B.1 (Sample complexity of PG-GORP). Suppose that an MDP is 1-QVI-solvable and that
all rewards are nonnegative. Then the sample complexity of PG-GORP is at most
2T 2A
¯
H1

πrand
πexpl

∞
.
21

Proof. Let
m = A
¯
H1

πrand
πexpl

∞
= 12A log (2TA)
max
t∈[T ],s∈Sopt
i ,a∈A
Qk
t (s, a)V ∗
t (s)
∆1
t(s)2
 
πrand
πexpl

∞
.
We will show that Algorithm 2 converges with probability at least 1/2 with this choice of parameter,
giving the sample complexity bound in the theorem since the algorithm clearly samples T 2m total
timesteps from the environment.
Similarly to the proof of Theorem 5.4, we will prove this by showing that with probability at least
1 −1/(2T), at each iteration πi(si) ∈arg maxa∈A Qπexpl
i
(si, a). This gives π ∈Π(Q1), which by
1-QVI-solvability means π must be optimal.
Consider the ith iteration of Algorithm 2. Define for each a ∈A and j ∈[m] the random variable
Xj(a) =
1aj
i =a
πi(a | si)
T
X
t=i
γt−iR(sj
t, aj
t).
First, can see that
0 ≤Xj(a) ≤
V ∗
i (si)
πi(a | si) =
V ∗
i (si)
πexpl
i
(a | si)
≤A V ∗
i (si)

πrand
πexpl

∞
,
since πi = πexpl
i
until line 8.
Second, we have
E [Xj(a)] =
1
πi(a | si)P

aj
i = a

E
" T
X
t=i
γt−iR(sj
t, aj
t) | aj
i = a
#
= Qπexpl
i
(si, a).
Finally, using same the reasoning as in the proof of Theorem 2, we can bound the variance of Xj(a):
Var (Xj) ≤A V ∗
i (si)

πrand
πexpl

∞
E [Xj] = A Qπexpl
i
(si, a)V ∗
i (si)

πrand
πexpl

∞
.
We now apply Bernstein’s inequality to
ˆ∇i(a | si) = 1
m
m
X
j=1
Xj(a)
for each a ∈A. If a ∈arg maxa∈A Qπexpl
i
(si, a), we apply a lower tail bound:
P

ˆ∇i(a | si) ≤Qπexpl
i
(si, a) −1
2∆1
i (si)

≤exp


−
m
 ∆1
i (si)
2 /8
 Qπexpl
i
(si, a) + 1
3∆1
i (si)

V ∗
i (si)
 πrand
πexpl

∞



≤exp


−
m
 ∆1
i (si)
2
12Qπexpl
i
(si, a)V ∗
i (si)
 πrand
πexpl

∞



≤
1
2TA.
If a /∈arg maxa∈A Qπexpl
i
(si, a), we apply an identical upper tail bound:
P

ˆ∇i(a | si) ≥Qπexpl
i
(si, a) + 1
2∆1
i (si)

≤
1
2TA.
These tail bounds hold simultaneously for all actions with probability at least 1 −
1
2T . Furthermore,
assuming they hold and using the definition of the gap ∆1
i (si), it must be that
arg max
a∈A
ˆ∇i(a | si) ⊆arg max
a∈A Qπexpl
i
(si, a),
which is enough to show that πi(si) ∈arg maxa∈A Qπexpl
i
(si, a) with probability at least 1 −
1
2T and
thus prove the theorem.
■
22

Now that we have seen that PG-GORP enjoys similar sample complexity bounds to GORP in the
common case that an MDP is 1-QVI-solvable, we introduce FQI-GORP. FQI-GORP derives its name
from fitted Q-iteration (FQI), which was originally proposed by Ernst et al. [49]. DQN was inspired
by neural FQI [50], so FQI-GORP provides a natural connection to DQN.
Algorithm 3 The FQI-GORP algorithm.
1: procedure FQI-GORP(k, m)
2:
for i = 1, . . . , T do
3:
Sample Akm episodes, following π for timesteps 1 to i −1 and then πexpl.
4:
ˆQ1
i+k−1 ←arg min ˆ
Q1
i+k−1
1
Akm
PAkm
j=1

ˆQ1
i+k−1(sj
i+k−1, aj
i+k−1) −PT
t=i+k−1 R(sj
t, aj
t)
2
.
5:
for t = i + k −2, . . . , i do
6:
ˆQi+k−t
t
←arg min ˆ
Qi+k−t
t
1
Akm
PAkm
j=1

ˆQi+k−t
t
(sj
t, aj
t) −R(sj
t, aj
t) −maxa′∈A ˆQi+k−t−1
t+1
(sj
t+1, a′)
2
.
7:
end for
8:
πi(si) ←arg maxa∈A ˆQk
i (si, a).
9:
end for
10:
return π
11: end procedure
FQI-GORP iteratively constructs a series of Q-functions at each iteration by minimizing a mean-
squared temporal difference error loss, similar to FQI and DQN.
Theorem B.2 (Sample complexity of FQI-GORP). Suppose that an MDP is k-QVI-solvable and that
all rewards are nonnegative. Then the sample complexity of FQI-GORP is at most
T 2

πrand
πexpl

k
∞
max
n
4A
¯
Hk, 10 log(4TAk)
o
.
Proof. Let
m =

πrand
πexpl

k
∞
max
(
24 log (2TA)
max
t∈[T ],s∈Sopt
i ,a∈A
Qk
t (s, a)V ∗
t (s)
∆1
t(s)2

, 10 log(4TAk)
Ak
)
.
We will show that FQI-GORP with parameters k and m will return an optimal policy with probability
at least 1/2. Since FQI-GORP samples a number of timesteps from the environment equal to T 2Akm,
this will prove the bound in the theorem.
Consider the ith iteration of Algorithm 2. We will show that with probability at least 1 −1/(2T), for
every si+k−1 ∈S reachable at timestep i + k −1 starting from si, and for every action ai+k−1
 ˆQ1
i+k−1(sj
i+k−1, aj
i+k−1) −Q1
i+k−1(sj
i+k−1, aj
i+k−1)
 < ∆1
i (si)
2
.
(11)
To prove (11), it is first helpful to write an explicit formula fitted Q-value, assuming that the loss in
line 4 of Algorithm 3 is minimized:
ˆQ1
i+k−1(s, a) =
PAkm
j=1 1sj
i+k−1=s∧aj
i+k−1=a
PT
t=i+k−1 R(sj
t, aj
t)
PAkm
j=1 1sj
i+k−1=s∧aj
i+k−1=a
.
That is, the Q-value is a simple average of several reward-to-go values, each of which has expectation
Q1
i+k−1(s, a). The probability of reaching some reachable state-action pair (s, a) at timestep i+k−1
must be at least A−k  πrand
πexpl

−k
∞. Thus, we can bound the sample size below via a concentration
inequality for binomial random variables:
P


Akm
X
j=1
1sj
i+k−1=s∧aj
i+k−1=a < 12 log (2TA) max
ai∈A
Qk
i (si, ai)V ∗
i (si)
∆1
i (si)2

≤exp{−3(10 log(4TAk))
28
} ≤
1
4TAk .
23

If the sample size is at least 12 log (2TA) maxai∈A
Qk
i (si,ai)V ∗
i (si)
∆1
i (si)2
, then Bernstein’s inequality (as
applied in the proofs of Theorems 5.4 and B.1) gives
P
 ˆQ1
i+k−1(sj
i+k−1, aj
i+k−1) −Q1
i+k−1(sj
i+k−1, aj
i+k−1)
 ≥∆1
i (si)
2

≤
1
4TAk .
Thus, taking a union bound, we have that the probability (11) does not hold for some reachable
state-action pair at timestep i + k −1 must be at most 1/(2T), since there can be at most Ak such
pairs.
We will now show by induction that given that (11) holds for all reachable state-action pairs,
 ˆQi+k−t
t
(sj
i+k−1, aj
i+k−1) −ˆQi+k−t
t
i + k −1(sj
i+k−1, aj
i+k−1)
 < ∆1
i (si)
2
.
(12)
holds for t = i + k −1, . . . , i for all reachable state-action pairs at t. The base case of t = i + k −1
is already taken care of, so we only need to show the inductive step.
Assume (12) holds for all reachable state-action pairs at t + 1. We can write an explicit formula for
the fitted Q-values at timestep t, given that the loss function on line 6 of Algorithm 3 is minimized:
ˆQi+k−t
t
(s, a) =
PAkm
j=1 1sj
t=s∧aj
t=a

R(sj
t, aj
t) + maxa′∈A ˆQi+k−t−1
t+1
(sj
t+1, a′)

PAkm
j=1 1sj
t=s∧aj
t=a
= R(s, a) + max
a′∈A
ˆQi+k−t−1
t+1
(f(s, a), a′).
Given that (12) holds for each pair (f(s, a), a′), it is now easy to see that (12) must hold for t as well.
By induction (12) must hold for t = i with probability at least 1 −1/(2T). Given that it holds, and
by definition of the gap, this implies that πi(si) ∈arg maxa∈A Qk
i (si, a). Thus with probability at
least 1/2, π ∈Π(Qk). By the assumption that the MDP is k-QVI solvable, π must be optimal with
probability at least 1/2.
■
B.2
Goal MDPs
Now, we will prove bounds on the effective horizon in one particular class of MDPs: goal MDPs.
Definition B.3 (Goal MDP). An MDP is considered a goal MDP if there is some set of goal states
Sgoal which are absorbing, i.e., f(s, a) = s for every s ∈Sgoal, and furthermore the reward function
is of the form
R(s, a) =
1
s /∈Sgoal ∧f(s, a) ∈Sgoal
0
otherwise.
That is, in a goal MDP reward is only received for reaching some set of goal states; the total episode
reward is 1 if a goal state is reached and 0 otherwise. As an example, all of the Minigrid environments
in BRIDGE are goal MDPs. We can show the following bound on the effective horizon in goal MDPs.
Theorem B.4 (The effective horizon in goal MDPs). Suppose that πexpl(a | s) > 0 for all s ∈S and
a ∈A. Then any goal MDP is 1-QVI-solvable. Furthermore, suppose that there is some p > 0 such
that, for all timesteps t ∈[T] and all state-action pairs st, at at that timestep from which a goal state
can be reached,
Pπexpl (sT ∈Sgoal | st, at) ≥p.
Then the effective horizon can be bounded as
H ≤1 + logA
log(2T)
p
.
(13)
Before we see the proof, note that Theorem B.4 agrees with our intuition that it should be harder to
find an optimal policy for a goal MDP when it is less likely that the exploration policy reaches the
goal, i.e., when p is smaller.
For instance, consider the MDP in Figure 2a. In this MDP, the minimum probability of reaching the
goal with the random exploration policy after taking some action is exponentially small: p = 1/AT −1.
Applying Theorem B.4 gives a bound of H ≤T + logA log(2T).
24

Figure 4: Empty-5x5, one of the Minigrid MDPs from BRIDGE and an example of a goal MDP (Definition B.3).
The agent (red triangle) can turn left, turn right, or go forward, and its goal is to reach the green square, which
gives a reward of 1.
In contrast, consider the Minigrid gridworld in Figure 4 from BRIDGE. Here we can bound p ≈
0.00137, which gives H ≤1 + logA(1/p) + logA log(2T) = 1 + log3 729 + log3 log(200) ≤
7 + 1.52 = 8.52 ≪T = 100. The techniques in Appendix C give a much tighter bound of
H ≤1.64.
Proof. We can assume that V ∗
1 (s1) = 1, since otherwise every trajectory in the MDP gives reward 0
and thus the effective horizon is trivially bounded. First, we will show that
Qπexpl
t
(s, a) > 0
⇔
Q∗
t (s, a) = 1.
(14)
This is enough to imply the MDP is 1-QVI-solvable, since any policy in Π(Qπexpl) must take only
actions with Qπexpl
t
(s, a) > 0, which must be optimal according to (14).
To show that (14) holds, first consider the ⇒implication. Assume Qπexpl
t
(s, a) > 0 and by way of
contradiction suppose that Q∗
t (s, a) ̸= 1, which means that Q∗
t (s, a) = 0. Clearly this cannot happen
since this would imply Qπexpl
t
(s, a) ≤0. Now, consider the ⇐direction. If Q∗
t (s, a) = 1, then there
must be some sequence of actions starting with a which leads from s to a goal state. By assumption,
πexpl assigns positive probability to each action in this sequence. Thus Qπexpl
t
(s, a) = Pπexpl(sT ∈
Sgoal | s, a) > 0.
Next, we will prove the bound on H from (13). From (14), we can see that Algorithm 1 (GORP)
will return an optimal policy for the MDP as long as at each iteration i it picks some action ai
with Qπexpl
i
(si, ai) > 0. In turn, this will happen as long as ˆQ1
i (si, ai) > 0 for some such ai, since
ˆQ1
i (si, a) must be 0 for any suboptimal a.
Thus, we need only show that
P

∃ai ∈A
ˆQ1
i (si, ai) > 0

≥1 −1
2T
(15)
holds for each i when m ≥log(2T)/p. This will allow us to conclude via a union bound that
Algorithm 1 will find an optimal policy with probability at least 1/2 in this case, which gives the
desired bound on the effective horizon.
To show (15), consider iteration i of Algorithm 1 and let ai ∈A such that Qπrand
i
(si, ai) > 0. We
can assume that such an action exists as long as Algorithm 1 has succeeded in iterations previous
to i. Let Xj = PT
t=i γt−iR(sj
t, aj
t) be the reward-to-go from the jth episode sampled to evaluate
ˆQ1
i (si, ai). By the definition of a goal MDP, each Xj(ai) ∈{0, 1}. Furthermore, since ˆQ1
i (si, ai) =
25

1
m
Pm
j=1 Xj, then ˆQ1
i (si, a) > 0 as long as some Xj = 1. This implies
P

∃ai ∈A
ˆQ1
i (si, ai) > 0

≥P

ˆQ1
i (si, ai) > 0

= P

∃j ∈[m]
Xj = 1

= 1 −P

∀j ∈[m]
Xj = 0

= 1 −P

Xj = 0
m
(i)
≥1 −(1 −p)m
≥1 −exp(−mp)
≥1 −1
2T ,
the bound previously proposed in (15) which we argued gives the desired bound. (i) uses the
assumption that Qπexpl
i
(si, ai) = P(Xj = 1) ≥p.
■
B.3
Lower bounds
Next, we will show that there are MDPs in which exponential dependence on the effective horizon
is unavoidable. That is, in some cases there is an information-theoretic lower bound on the sample
complexity of RL proportional to AH.
Theorem B.5. Fix T ≥1, A ≥2, and H ∈[T]. Then for any RL algorithm, there is an MDP with A
actions, horizon T, and effective horizon at most H for which the algorithm’s sample complexity is at
least
T⌊T/H⌋
 ⌈AH/2⌉−1

= Ω(T 2AH/H).
Note that this matches the upper bound on the sample complexity of GORP given in Lemma 5.3 up to
a factor of roughly 2H. When H = T, it exactly agrees with the lower bound given in Theorem 2.1.
Proof. The proof uses MDPs with the same state space and transition function as in Theorem 2.1.
Define AT such MDPs which differ only in their reward functions:
M = {Ma1:T | a1:T ∈AT }
where the MDP Ma1:T has R(sa′
1:ℓ, a′) =
1
a′
1:ℓ, a′ = a1:ℓ+1 and ℓ≡0
(mod H)
0
otherwise.
That is, each MDP has a single optimal sequence of actions that gives reward 1 every H timesteps.
By the same argument as in the proof of Theorem 2.1, for any RL algorithm there must be some
MDP in M such that after interacting with the environment for less than ⌈AH/2⌉−1 episodes, the
algorithm cannot with probability at least 1/2 identify the optimal actions for timesteps t = 1 to
t = H. We can repeat this line of reasoning for timesteps t = H + 1 to t = 2H, and so on for
a total of ⌊T/H⌋steps, to show that with less than ⌊T/H⌋
 ⌈AH/2⌉−1

episodes, there must be
some MDP in M whose optimal action sequence cannot be identified with probability greater than
(1/2)⌊T/H⌋≤1/2. Thus, the sample complexity of the RL algorithm on this MDP must be at least
T⌊T/H⌋
 ⌈AH/2⌉−1

,
which is the desired bound.
It only remains to be shown that the effective horizon of the MDPs in M is actually H. To see why,
consider running Algorithm 1 with k = H and m = 1. That is, at each iteration i, GORP will try
all H-length action sequences followed by actions from πexpl. Then, it will pick the action sequence
with the highest empirical reward-to-go. From the definition of the MDPs in M, all action sequences
starting with a suboptimal action must have empirical reward-to-go of 0. Furthermore, at least one
H-length action sequence starting with an optimal action must get reward-to-go of at least 1. Thus,
GORP will with probability 1 choose an optimal action at each timestep. This means that the effective
horizon must be at most H + logA 1 = H.
■
26

C
Tighter bounds on the effective horizon
In Theorem 5.4, we obtained bounds on the effective horizon and thus on the sample complexity of
GORP. However, we find that the bounds given by Theorem 5.4 are often very loose compared to
the empirical performance of GORP due to two factors. First, Theorem 5.4 requires considering the
worst case of Qk
t (s, a)V ∗
t (s)/∆k
t (s)2 over all optimal states. However, in many MDPs in our dataset,
there are optimal states with extremely small gaps that in practice are almost never reached by GORP.
When Theorem 5.4 is applied, these states make the sample complexity bounds very large despite
GORP working well empirically. Second, Theorem 5.4 uses asymptotically tight techniques for
bounding the sample complexity that can be quite loose for small sample sizes. Below, we describe
the algorithm we use to provably bound the sample complexity of GORP (and thus the effective
horizon) that gives much tighter results.
Consider the GORP algorithm as given in Algorithm 1. Let at denote the random variable correspond-
ing to the action ultimately chosen by the algorithm for timestep t. Let st denote the state reached by
actions a1, . . . , at−1. Denote by Pm the probability measure given by running the algorithm with
parameter m. We would like to bound the probability that the algorithm does not achieve the optimal
return in the MDP. Let this event by denoted as
E :=
T
X
t=1
Rt(st, at) < V ∗
1 (s1)
⇔∃t
at /∈A∗
t (st),
where A∗
t (s) denotes the set of optimal actions in state s at timestep t, i.e.
A∗
t (s) = arg max
a∈A Q∗
t (s).
It is straightforward to see from Algorithm 1 that it requires T 2Akm timesteps of interaction the
environment. Thus the sample complexity of the algorithm is
T 2Akm
where
m = min{m ∈N | Pm(E) < 1/2}.
Clearly, we can upper bound the sample complexity using any m such that the probability of failure
is bounded as Pm(E) < 1/2. To do so, for each value of k, we perform a binary search over values
of m from 1 to 10100. For each possible m, we calculate an upper bound on Pm(E). If the upper
bound is below 1/2, we then search below m; if it is greater, we search above m. When the search
has converged to a relative precision of 1/100, we output T 2Akm as the sample complexity and
Hk = k + logA m as the effective horizon for that particular value of k.
C.1
Upper bounding Pm(E)
We upper bound the failure probability Pm(E) recursively. Let Ot denote the event that all actions
taken before t have been optimal, i.e.,
Ot := ∀t′ < t
at ∈A∗
t (st).
To begin the recursion, note that at the final timestep,
P(E | OT , sT , aT ) = 1{aT /∈A∗
T (sT )}.
We will use two recursion rules: one from next states to state-action pairs and one from state-action
pairs to states. The first rule is
P(E | Ot, st) =
X
at∈A
P(E | Ot, st, at)P(at | st)
(16)
and the second (for t < T) is
P(E | Ot, st, at) =
1
at /∈A∗
t (st)
P(E | Ot+1, st+1)
at ∈A∗
t (st).
(17)
We apply these results recursively from t = T, . . . , 1 to finally obtain P(E | O1, s1) = P(E).
The remaining difficulty is calculating P(at | st). Recall from Algorithm 1 that at is chosen as the
first action of a k-action sequence
at:t+k−1 ∈arg
max
at:t+k−1∈Ak ˆQt(st, at:t+k−1),
27

where ˆQt(st, at:t+k−1) is the empirical mean return-to-go from m episodes starting in state st and
taking actions at, . . . , at+k−1 followed by actions sampled from the exploration policy. To simplify
notation, let ⃗at denote at:t+k−1. We use various inequalities, described in detail below, to bound the
probability that a particular k-action sequence is chosen:
p(⃗at) ≤Pm

ˆQt(st,⃗at) >
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≤Pm

ˆQt(st,⃗at) ≥
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≤p(⃗at).
Letting p(⃗at) denote the actual probability an action sequence is chosen, we can rewrite (16) to
P(E | Ot, st) =
X
at∈A
P(E | Ot, st, at)
X
at+1:t+k−1∈Ak−1
p(⃗at).
(18)
Given the bounds on p(⃗at) (i.e., p and p), we formulate a linear program with the bounds as constraints,
plus the constraint that P
⃗at∈Ak p(⃗at) = 1, with the objective of maximizing (18). Solving this gives
an upper bound on P(E | Ot, st). We can then propagate this bound recursively using (17) to bound
P(E).
Calculating p and p
We use up to four methods to bound p(⃗at), and pick the one which gives the lowest conditional failure
probability after solving the linear program described above. As previously, let Dt(st,⃗at) denote the
distribution of the reward-to-go starting in st and taking actions at, . . . , at+k−1 followed by actions
sampled from the exploration policy. Thus we can write
ˆQt(st,⃗at) = 1
m
m
X
i=1
Xi
where
X1, . . . , Xm
i.i.d.
∼Dt(st,⃗at).
Most of the following methods use the following decomposition:
Pm

ˆQt(st,⃗at) >
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

(i)=
Z
Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) < ˆQt(st,⃗at)

dPm

ˆQt(st,⃗at)

(ii)
≥
N
X
i=1
Pm

qi−1 < ˆQt(st,⃗at) ≤qi

Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) ≤qi−1

(19)
for some sequence −∞= q0 ≤q1 ≤. . . ≤qN = ∞. Here, (i) uses the fact that the random
variables ˆQt(st,⃗at) across all action sequences ⃗at ∈Ak are independent. (ii) is a lower bound on
the integral via a Riemann sum.
Alternatively, suppose we know that the CDF of ˆQt(st,⃗at) is bounded by
Pm

ˆQt(st,⃗at) ≤x

≥FZ(x)
where FZ(x) is the continuous CDF of some random variable Z. Then
Pm

ˆQt(st,⃗at) >
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≥Pm

Z >
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

=
Z
Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) < ˆQt(st,⃗at)

dPm (Z)
≥1
N
N
X
i=1
Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) ≤F −1
Z
i −1
N

(20)
28

by a similar argument. We also have equivalent bounds in the other direction:
Pm

ˆQt(st,⃗at) ≥
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≤
N
X
i=1
Pm

qi−1 < ˆQt(st,⃗at) ≤qi

Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) ≤qi

(21)
Pm

ˆQt(st,⃗at) ≥
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≤1
N
N
X
i=1
Y
⃗a′
t∈Ak\{⃗at}
Pm

ˆQ(st,⃗a′
t) ≤F −1
Z
 i
N

(22)
where the CDF of Z is greater than or equal to that of ˆQt(st,⃗at). We use N = 100 when using these
bounds.
Binomial bounds
In the case where for all ⃗at ∈Ak, the distribution Dt(st,⃗at) has mass on only
0 and some other value C, we have
ˆQt(st,⃗at) ∼C
m Binom

m, 1
C Qt(st,⃗at)

.
This case occurs in many environments that are goal-based, i.e. where the agent gets reward only
for reaching some goal and then the episode ends. We find that it significantly improves the sample
complexity bounds in those environments. Without loss of generality, we may assume C = 1. We
then apply (19) and (21) to obtain p(⃗at) and p(⃗at). We let q0, . . . , qN be set such that
Pm

qi−1 < ˆQt(st,⃗at) ≤qi

≈1
N
using either the exact inverse CDF of the binomial distribution for small m or a normal approximation
for large m. Then, we can calculate all terms in the bounds (19) and (21) using the binomial CDF.
Since the CDF is more expensive to calculate for larger m, we only use the binomial-based bounds
when m ≤106 and k = 1.
Berry-Esseen bounds
For this type of bound, we calculate the variance σ2 = Var(Dt(st,⃗at)) and
third absolute moment
ρ = EX∼Dt(st,⃗at)[|X|3]
for each ⃗at ∈Ak. Then by the Berry-Esseen theorem [51], we have that
Pm( ˆQt(st,⃗at) ≤u) −PX∼N(Qt(st,⃗at),σ2/m)(X ≤u)
 ≤min{0.3328(ρ/σ3 + 0.429), 0.33554(ρ/σ3 + 0.415)}
√m
.
The resulting upper and lower bounds on the CDFs of ˆQt(st,⃗at) for all ⃗at ∈Ak can be used in (20)
and (22) to calculate p(⃗at) and p(⃗at). Since this bound requires order N evaluations of the normal
CDF and inverse CDF, which is somewhat expensive, we only use it when Ak ≤100.
Bernstein bounds
Similarly to the Berry-Esseen bounds, Bernstein’s inequality can be used to
bound the CDF of ˆQt(st,⃗at), and is sometimes superior to Berry-Esseen for large m due to giving
tail bounds that decay exponentially rather than quadratically. In particular, suppose Dt(st,⃗at) is
supported on the interval [α, β]; we can compute these bounds via value iteration. Then
Var(Dt(st,⃗at)) ≤V = (β −Qt(st,⃗at)) (Qt(st,⃗at) −α) .
Bernstein’s inequality gives the following bounds on the CDF of ˆQt(st,⃗at):
Pm

ˆQt(st,⃗at) ≤Qt(st,⃗at) + u

≥
(
1
u ≤0
1 −exp
n
−
mu2/2
V +(β−α)u/3
o
otherwise
Pm

ˆQt(st,⃗at) ≤Qt(st,⃗at) + u

≤
(
1
u ≥0
exp
n
−
mu2/2
V +(β−α)u/3
o
otherwise.
Similarly to the Berry-Esseen bounds, we use these in (20) and (22) to calculate p(⃗at) and p(⃗at)
when Ak ≤100.
Bennett bounds
The final method we use to calculate p(⃗at) and p(⃗at) is computationally cheaper
than the others, so we can use it no matter the size of Ak. As in the Bernstein bounds, we calculate
29

the interval support and bound the variance of each Dt(st,⃗at). We then let u be the arithematic mean
of the highest action sequence Q value and the second-highest, i.e.
u = 1
2
 
max
⃗at∈Ak Qt(st,⃗at) +
max
⃗a′
t /∈arg max⃗at∈Ak Qt(st,⃗at) Qt(st,⃗a′
t)
!
.
Then, we for each action sequence with less-than-highest Q-values, i.e.
for each ⃗at
/∈
arg max⃗at∈Ak Qt(st,⃗at), we calculate the upper bound
Pm

ˆQt(st,⃗at) ≥
max
⃗a′
t∈Ak\{⃗at}
ˆQ(st,⃗a′
t)

≤Pm
 
ˆQt(st,⃗at) ≥
max
⃗a′
t∈arg max⃗at∈Ak Qt(st,⃗at)
ˆQ(st,⃗a′
t)
!
= 1 −Pm

∃⃗a′
t ∈arg max
⃗a′
t∈Ak Qt(st,⃗a′
t)
ˆQt(st,⃗at) < ˆQ(st,⃗a′
t)

≤1 −Pm

ˆQt(st,⃗at) < u
∧
∀⃗a′
t ∈arg max
⃗a′
t∈Ak Qt(st,⃗a′
t)
ˆQ(st,⃗a′
t) > u

= 1 −Pm

ˆQt(st,⃗at) < u

Y
⃗a′
t∈arg max⃗a′
t∈Ak ˆ
Q(st,⃗a′
t)
Pm

ˆQ(st,⃗a′
t) > u

= 1 −

1 −Pm

ˆQt(st,⃗at) ≥u

Y
⃗a′
t∈arg max⃗a′
t∈Ak ˆ
Q(st,⃗a′
t)

1 −Pm

ˆQ(st,⃗a′
t) ≤u

.
(23)
Each of the tail bounds in (23) can be upper bounded using Bennett’s inequality to obtain p(⃗at). We
let p(⃗at) = 1 if ⃗at ∈arg max⃗at∈Ak Qt(st,⃗at) and we set p(⃗at) = 0 for all ⃗at ∈Ak.
D
Previously proposed sample complexity bounds
In this appendix, we give proofs of sample complexity bounds based on properties previously
proposed in the RL theory literature. We also compare these bounds to our effective horizon-based
bounds in examples that showcase their failure modes.
D.1
Upper confidence bounds (UCB) and strategic exploration
A central problem in RL is exploration: how to efficiently reach enough states in an MDP in order to
identify the optimal policy. One common way of approaching exploration is with upper-confidence
bounds (UCB), which originated in the bandit literature. Algorithms using UCBs generally choose
actions based on the current best estimate of that action’s value plus an exploration “bonus” that
incentivizes exploration of little-seen states. Examples in the RL literature include Kakade [18], Azar
et al. [4], Jiang et al. [19], Jin et al. [5, 20], Du et al. [21], Jin et al. [22]. Generally, these UCB
algorithms achieve minimax sample complexity in terms of some measure of the “size” of the state
space—either the number of states S [4], or quantities like the Bellman-Eluder dimension [22].
A very simple UCB-type algorithm, R-MAX [23, 18], achieves a sample complexity bounded by
SAT in deterministic, tabular MDPs. It depends on knowing the maximum reward at any state-action
pair in the MDP, Rmax = max(s,a)∈S×A R(s, a). It also requires access to a computational oracle
that can calculate an optimal policy for any transition function f and reward function R, for instance
via value iteration.
1: procedure R-MAX
2:
initialize ˆf(s, a) ←s for all (s, a) ∈S × A
3:
initialize ˆR(s, a) ←Rmax for all (s, a) ∈S × A
4:
for j = 1, . . . , SA do
5:
for t = 1, . . . , T do
6:
take an action a in the current state s according to the optimal policy for ˆf and ˆR
7:
ˆR(s, a) ←the observed reward
8:
ˆf(s, a) ←the observed next state s′
30

9:
end for
10:
end for
11:
return an optimal policy for ˆf and ˆR
12: end procedure
This is the version of R-MAX for deterministic MDPs; there is a more complex version for stochastic
MDPs. The exploration bonuses in R-MAX are simply the initialization of ˆR to the maximum possible
reward. This ensures that an optimal value function computed from ˆf and ˆR is always an upper
bound on the true optimal value function. The following result shows that R-MAX finds the optimal
policy, and thus proves that its sample complexity is at most SAT.
Theorem D.1. R-MAX returns an optimal policy.
Proof. Let ˆV ∗and ˆQ∗be the optimal value function and Q-function under ˆf and ˆR. We will begin
by showing that at any point in the algorithm, V ∗
t (s) ≤ˆV ∗
t (s) and Q∗
t (s, a) ≤ˆQ∗
t (s, a) for all
(t, s, a) ∈[T] × S × A.
The proof is via induction on t from T to 1. To begin, clearly ˆQ∗
T (s, a) ∈{Q∗
T (s, a), Rmax}, so the
bound holds for ˆQ∗
T . Now, suppose that at some t ∈[T] and all (s, a) ∈S ×A, Q∗
t (s, a) ≤ˆQ∗
t (s, a).
Then
ˆV ∗
t (s) = max
a∈A
ˆQ∗
t (s, a) ≥max
a∈A Q∗
t (s, a) = V ∗
t (s),
so the bound holds for ˆV ∗
t as well. Finally, say that at some t ∈[T −1] and for all s ∈S,
V ∗
t+1(s) ≤ˆV ∗
t+1(s). To show this implies Q∗
t (s, a) ≤ˆQ∗
t (s, a) for any (s, a) ∈S × A, consider two
cases. First, if (s, a) has been seen by the algorithm, then
ˆQ∗
t (s, a) = ˆR(s, a)+ ˆV ∗
t+1( ˆf(s, a) = R(s, a)+ ˆV ∗
t+1(f(s, a) ≥R(s, a)+V ∗
t+1(f(s, a) = Q∗
t (s, a).
Otherwise, if (s, a) has not been seen, then ˆR(s, a) = Rmax and ˆf(s, a) = s. In this case,
ˆQ∗
t (s, a) = (T −t + 1)Rmax ≥Q∗
t (s, a).
By induction we see that V ∗
t (s) ≤ˆV ∗
t (s) and Q∗
t (s, a) ≤ˆQ∗
t (s, a) for all (t, s, a) ∈[T] × S × A.
Next, we will prove that any optimal policy π for ˆf and ˆR must either (a) be optimal for f and R or
(b) reach a previously unseen state-action pair. In particular, we will show that if V π
1 (s1) < ˆV ∗
1 (s1),
then π must reach a previously unseen state-action pair. Otherwise, V π
1 (s1) ≥ˆV ∗
1 (s1) ≥V ∗
1 (s1),
showing that π is optimal for f and R.
We will again work inductively starting from the last timestep. First, suppose that Qπ
T (s, a) <
ˆQ∗
T (s, a) for some (s, a) ∈S × A. This is equivalent to R(s, a) < ˆR(s, a), which clearly means
that (s, a) cannot have been explored.
Now, suppose that at some timestep t ∈[T], we know that for any (s, a) ∈S × A, Qπ
t (s, a) <
ˆQ∗
t (s, a) implies that π must explore some new state-action pair at or after timestep t starting in
(s, a). If V π
t (s) < ˆV ∗
t (s) for some s ∈S, then
Qπ
t (s, πt(s)) < max
a∈A
ˆQ∗
t (s, a) = ˆQ∗
t (s, πt(s)).
By assumption this means π must explore some new state-action pair at or after timestep t, since it
takes an action a satisfying Qπ
t (s, a) < ˆQ∗
t (s, a).
Finally, suppose that for some t ∈[T], we know that for any s ∈S, V π
t+1(s) < ˆV ∗
t+1(s) implies that
π must explore some new state-action pair at or after timestep t + 1 starting in s. Suppose for some
(s, a) ∈S × A that Qπ
t (s, a) < ˆQ∗
t (s, a). Then
R(s, a) + V π
t+1(f(s, a)) < ˆR(s, a) + V π
t+1( ˆf(s, a))
which implies that either R(s, a) < ˆR(s, a) or V π
t+1(f(s, a)) < V π
t+1( ˆf(s, a)). In the first case,
(s, a) must be unexplored. In the second case, either (s, a) is unexplored, or
V π
t+1(f(s, a)) < V π
t+1(f(s, a)).
In any of these cases, π must explore a new state-action pair either in this timestep or in the future
starting from (s, a).
31

Inductively, this shows that V π
1 (s1) < ˆV ∗
1 (s1) implies that π must reach a previously unseen
state-action pair. We will show that this property implies that R-MAX must return an optimal policy.
In particular, note that after the jth loop iteration in R-MAX, it must either have an optimal policy
or have explored at least j of the state-action pairs in the MDP. This is a simple consequence of the
above property: at each iteration, either the policy used by R-MAX must be optimal or it must explore
at least one additional state-action pair. This means that after all the SA loop iterations, R-MAX will
either have an optimal policy or have explored all the state-action pairs, in which case it will also
have an optimal policy.
■
D.2
Covering length
The covering length of an MDP was originally proposed by Even-Dar and Mansour [52] and later used
by Liu and Brunskill [7] to prove sample complexity bounds on RL algorithms which use random
exploration. Liu and Brunskill [7] show various bounds on the covering length using graph-theoretic
notions. While they focus on discounted infinite-horizon MDPs, we use a version of covering length
adapted to finite-horizon MDPs, similar to that used by Dann et al. [6].
Definition D.2 (Covering length). The covering length L of an MDP under an exploration policy
πexpl is the number of episodes needed until all state-action pairs have been visited with probability
at least 1/2.
One can easily show sample complexity bounds based on the covering length.
Theorem D.3 (Covering length sample complexity bound). There is an RL algorithm which can
solve any MDP with sample complexity TL given an exploration policy πexpl, where L is the covering
length of πexpl.
Proof. Consider the following RL algorithm:
1: procedure COVERINGLENGTHRL(πexpl, L)
2:
collect a dataset of L episodes, sampling actions according to πexpl
3:
record ˆR(s, a) and ˆf(s, a) for all state-action pairs seen in the dataset
4:
define ˆR(s, a) and ˆf(s, a) arbitrarily for state-action pairs not seen in the dataset
5:
run value iteration using ˆR and ˆf to obtain a policy π
6:
return π
7: end procedure
By the definition of covering length, with probability at least 1/2 the algorithm should produce
ˆR(s, a) = R(s, a) and ˆf(s, a) = f(s, a) for all (s, a) ∈S × A. In this case, π will be an
optimal policy. Thus, COVERINGLENGTHRL returns an optimal policy with probability at least
1/2 while interacting with the environment for TL timesteps. This means the sample complexity of
COVERINGLENGTHRL is at most TL.
■
To bound the covering length for MDPs in the BRIDGE dataset, we make use of the following result.
Lemma D.4 (Bounds on the covering length). Define the occupancy measure µ of πexpl as
µt(s, a) = Pπexpl(st = s ∧at = a).
Suppose that for every state-action pair (s, a), there is some timestep t when µt(s, a) > 0. Then
log(2)
2 min(s,a)∈S×A
P
t∈[T ] µt(s, a) ≤L ≤

log(2SA)
min(s,a)∈S×A maxt∈[T ] µt(s, a)

.
We calculate µmin = min(s,a)∈S×A maxt∈[T ] µt(s, a) for each MDP in BRIDGE and use the
upper bound from Lemma D.4 to obtain a sample complexity bound of T log(2SAT)/µmin. Since
P
t∈[T ] µt(s, a) ≤T maxt∈[T ] µt(s, a) the upper and lower bounds in Lemma D.4 agree up to a
factor of T log(2SA)/ log(2), so this is reasonably tight. In fact, in 122 of the 155 MDPs in BRIDGE,
min(s,a)∈S×A
P
t∈[T ] µt(s, a) = min(s,a)∈S×A maxt∈[T ] µt(s, a), making the upper and lower
bounds tight up to only logarithmic factors.
Proof. Let Pm be a probability measure corresponding to sampling m episodes following πexpl. Let
32

Ej
t (s, a) denote the event that the jth episode has (st, at) = (s, a). Let Et(s, a) be the event that
Ej
t (s, a) occurs in at least one one of those episodes, i.e.
Et(s, a) :=
m
_
j=1
Ej
t (s, a).
Finally, let C be the event defined by
C :=
^
(s,a)∈S×A
_
t∈[T ]
Et(s, a).
That is, C is when every state-action pair has been seen at some timestep in at least one episode. We
can thus equivalently define L = min{m | Pm(C) ≥1/2}.
We will now start by showing the upper bound of L ≤⌈log(2SA)/p⌉. Let m = ⌈log(2SA)/p⌉. We
can write
Pm(¬C) = Pm

∃(s, a) ∈S × A : ∀t ∈[T], j ∈[m]
¬Ej
t (s, a)

≤
X
(s,a)∈S×A
Pm

∀t ∈[T], j ∈[m]
¬Ej
t (s, a)

≤
X
(s,a)∈S×A
min
t∈[T ] Pm

∀j ∈[m]
¬Ej
t (s, a)

=
X
(s,a)∈S×A

1 −max
t∈[T ] Pm(E1
t (s, a))
m
≤
X
(s,a)∈S×A

1 −
min
(s′,a′)∈S×A max
t∈[T ] Pm(E1
t (s′, a′))
m
≤
X
(s,a)∈S×A

1 −
min
(s′,a′)∈S×A max
t∈[T ] µt(s′, a′)
m
≤
X
(s,a)∈S×A
exp

−m
min
(s′,a′)∈S×A max
t∈[T ] µt(s′, a′)

≤
X
(s,a)∈S×A
1
2SA
= 1
2,
which proves the upper bound.
To show the lower bound, take any (s, a) ∈arg min(s,a)∈S×A
P
t∈[T ] µt(s, a). First, suppose that
P
t∈[T ] µt(s, a) ≥log(2). Then the lower bound in the lemma must be at most 1/2, which is clearly
true. Thus, we may subsequently assume P
t∈[T ] µt(s, a) < log(2).
33

Suppose m < C/ P
t∈[T ] .µt(s, a) Then
Pm(¬C) = Pm

∃(s′, a′) ∈S × A : ∀t ∈[T], j ∈[m]
¬Ej
t (s′, a′)

≥Pm

∀t ∈[T], j ∈[m]
¬Ej
t (s, a)

=

1 −Pm
 ∃t ∈[T]
E1
t (s, a)
 m
≥

1 −
X
t∈[T ]
Pm
 E1
t (s, a)



m
=

1 −
X
t∈[T ]
µt(s, a)


m
(i)
≥exp

−2m
X
t∈[T ]
µt(s, a)


> 1
2,
where (i) uses the fact that 1 −x ≥exp(−2x) for x ∈[0, log(2)]. This shows that with probability
greater than 1/2, not all state-action pairs will be seen in m episodes, and thus establishes that
L > m, which is the desired bound.
■
D.3
Effective planning window (EPW)
Perhaps the closest existing concepts to our effective horizon are various notions of “effective
planning window.” This generally refers to tree-based planning algorithms which only consider action
sequences of some length W from the current state, rather than considering action sequences all the
way until the end of the MDP. For instance, Kearns et al. [24] show that in discounted MDPs, one
need only plan to some ϵ-horizon to obtain an ϵ-optimal policy. Jiang et al. [53] build on this and
show that one may want to use a different discount factor for planning than the one that is used for
evaluation. Malik et al. [8] also introduce a notion of effective planning window based on the number
of timesteps one must look ahead in an MDP to avoid terminal states.
We do not directly apply any of these previous results to our setting. Since we are concerned primarily
with finite-horizon undiscounted MDPs, it does not make much sense to apply a discount factor as
in Kearns et al. [24] and Jiang et al. [53]. We find that the assumptions in Malik et al. [8] are quite
unusual and do not really hold in any of the environments in BRIDGE. In particular, the analysis in
Malik et al. [8] requires that a trajectory through an MDP is either optimal or ends early in a terminal
state.
Instead of directly using these results, we define a notion of effective planning window based on
the length of action sequences one must consider in an MDP while ignoring any rewards after the
sequence.
Definition D.5 (Effective planning window). Define Q1
t(s, a) = R(s, a) for all (t, s, a) ∈[T]×S×A
and let Qi = QVI(Qi−1) for i = 2, . . . , T. The effective planning window of an MDP is the minimum
W ∈[T] such that all policies in Π(QW ) are optimal.
Note that the effective planning window bears significant similarity to the k-QVI-solvability property
from Definition 5.1. However, Q1 is defined as equal to the reward function for the EPW, while in
Definition 5.1 it is equal to Qπexpl.
The EPW also results in sample complexity bounds of T 2AW very similar to those of T 2AH for
the effective horizon. However, we find empirically that H < W in 82% of the MDPs in BRIDGE,
making the effective horizon-based bounds generally tighter.
Theorem D.6. For any MDP with effective planning window W, there is an RL algorithm whose
sample complexity is at most T 2AW .
Proof. We will use the following algorithm:
34

1: procedure PLANOVERWINDOW(W)
2:
for i = 1, . . . , T do
3:
for ai:i+W −1 ∈Ak do
4:
Sample an episode following π1, . . . , πi−1, then actions ai:i+W −1, and then arbitrary
actions.
5:
ˆRi(si, ai:i+W −1) ←Pi+W −1
t=i
R(st, at).
6:
end for
7:
πi(si) ←arg maxai∈A maxai+1:i+W −1∈Ak−1 ˆRi(si, ai:i+W −1).
8:
end for
9:
return π
10: end procedure
Again, this algorithm is quite similar to GORP (Algorithm 1) except that it only samples a single
episode per action sequence and it ignores rewards beyond the planning window. Clearly, PLANOVER-
WINDOW will take T 2AW steps in the environment. Thus, to bound the sample complexity, we only
need to show it returns an optimal policy with probability at least 1/2.
To prove this, we will show that
max
ai+1:i+W −1∈Ak−1 ˆRi(si, ai:i+W −1) = QW
i (si, ai).
(24)
Based on line 7 of the algorithm, this is enough to show that π ∈Π(QW ), and thus that π must be
optimal by Definition D.5.
To
prove
(24),
we
will
first
show
by
induction
that
Qj
t(st, at)
=
maxat+1:t+j−1∈Aj−1 Pt+j−1
t′=t
R(st′, at′), where st′+1 = f(st′, at′) for t′ = i, . . . , i + W −2. The
base case when j = 1 is by definition: Q1
t(st, at) = R(st, at). For the inductive step, assume the
formula holds for j and note that
Qj+1
t
(st, at) = QVI(Qj
t)(st, at)
= R(st, at) + max
at+1∈A Qj
t+1(f(st, at), at+1)
= R(st, at) + max
at+1∈A
max
at+2:t+j∈Aj−1
t+j
X
t′=t+1
R(st′, at′)
=
max
at+1:t+j∈Aj
t+j
X
t′=t
R(st′, at′).
Next, note that by the way ˆR is constructed, ˆR(si, ai:i+W −1) = Pi+W −1
t=i
R(st, at), where st+1 =
f(st, at) for t = i, . . . , i + W −2. Thus combining this with the formula proved by induction, (24)
clearly holds and the proof is complete.
■
D.4
Other bounds
One other work that derives sample complexity bounds for RL with random exploration is Dann et al.
[6]. They define an algorithm which maintains at all times a current best policy π, and acts according
to this policy but with some exploration noise, e.g., via an ϵ-greedy policy explϵ(π). They introduce
the notion of a “myopic exploration gap,” which is defined as
α = sup
π′,c≥1
1
√c (J(π′) −J(π))
such that for all (t, s, a) ∈[T] × S × A
µπ′
t ≤cµexplϵ(π)
t
(s, a)
µπ
t ≤cµexplϵ(π)
t
(s, a).
This gap is shown to generalize the notion of covering length as well as various others from the
literature. However, we find that it is not so useful in many environments in BRIDGE.
The problem we find is illustrated in the MDP below:
35

s1
s2
R = 1
. . .0
s2
0
. . .
R = 1
. . .
0
. . .
0
. . .
0
sT
R = 1
sT
0
sT
0
sT
T −1
In this MDP, one need simply follow the actions which give rewards of 1 to achieve the optimal return
of T. One can show H = W = 1 in this MDP, which give identical sample complexities of T 2A.
However, the difficulty with the myopic exploration gap is that the analysis in Dann et al. [6] cannot
rule out the policy π which takes all actions to the right (achieving return T −1) from being chosen
at some point while running their RL algorithm. If this happens, then the only way to find a better
policy is to completely switch to the policy π′ which takes all left actions (achieving return T). This
implies that α is maximized when c = AT , leading to α = (1/A)T/2. Since the sample complexity
bounds in Dann et al. [6] are O(1/α2), this gives a bound proportional to AT , which is no better than
the worst case.
Thus, whenever there are “distracting” rewards, no matter how distant, as in this case, the theory from
Dann et al. [6] cannot give good sample complexity bounds. There is also no easy way calculate
α directly for an arbitrary environment. For these reasons, we do not include their bounds in our
experiments.
E
Dataset details
67 Atari
games
55 Procgen
levels
33 Minigrid
gridworlds
The BRIDGE dataset
102105108
Num. states S
0
25
Num. MDPs
5 1015
Num. actions A
0
50
10 100 200
Horizon T
0
50
Figure 5: Our BRIDGE dataset consists of 155 de-
terministic MDPs with full tabular representations.
We include MDPs from three popular RL bench-
marks which cover a range of state space sizes,
action state sizes, and horizons.
In this appendix, we give a detailed explanation of
how we chose the MDPs in BRIDGE and how we
constructed their tabular representations. See Figure
5 for an overview of BRIDGE.
E.1
Environments
We limited the horizon of environments for BRIDGE
to some T ∈{10, 15, 20, 30, 50, 70, 100, 200}, de-
pending on the environment, in order to avoid the
state space becoming intractably large. We use sub-
scripts to denote the horizon to which an environment
is limited. For instance, PONG50 refers to the Atari
game Pong limited to 50 timesteps.
Frameskip
We carefully used frameskip for each
environment. Frameskip is a standard practice in
Atari [54] in which each action taken in the envi-
ronment is played for a certain number of frames;
the agent only receives the next state after all these
frames have completed.
We use unusually high
frameskips in order to capture episodes with longer
wall-clock times in a small number of environment
timesteps. The frameskip values we use are listed in
Table 4. For most Atari games, we use a frameskip of 30, corresponding to taking 2 actions per
second. The frameskips for Procgen environments vary; we chose ones that tended to align with how
long it took the agent to perform various low-level tasks in the environment like moving one space.
We did not use frameskip for Minigrid.
36

Environment
Frameskip
MONTEZUMAREVENGE
24
All other Atari games
30
BIGFISH
8
CHASER
2
CLIMBER
6
COINRUN
8
DODGEBALL
8
FRUITBOT
8
HEIST
2
JUMPER
8
LEAPER
6
MAZE
1
MINER
1
NINJA
8
PLUNDER
8
STARPILOT
8
All Minigrid gridworlds
1
Table 4: Frameskip values used for the MDPs in BRIDGE.
Atari games
For each of the 57 Atari games in the Arcade Learning Environment (ALE)
benchmark [43], we attempted to construct tabular representations for each horizon T
∈
{10, 20, 30, 50, 70, 100, 200}. However, we excluded environments once the state space exceeded
100 million states. We kept multiple horizon-limited versions of games, i.e., BRIDGE contains
PONG10, PONG20, PONG30, etc. For some games, even T = 10 produced too many states, so we did
not include these at all. We use the minimal action sets for each MDP rather than all 18 possible
Atari actions.
We made one exception to these procedures for Montezuma’s Revenge, as it is an environment
well-known for being difficult to explore, so we wanted to make sure to include it in BRIDGE. We
found that with T = 10, there was not enough time to get any reward, and with T = 20 there were
too many states. We found that using T = 15 and a frameskip of 24 did allow an agent to receive
reward, so we used this version in BRIDGE.
We made a couple other modifications to the standard Atari setup. First, we limited agents to one life:
as soon as a life is lost, the episode ends. Second, in SKIING10, we added an additional 200 frames
of NOOP actions after the 10 timesteps (= 300 frames) in each episode. This is necessary to correctly
reflect the reward incentives in SKIING with longer horizons.
Finally, we scaled the rewards for many Atari games to make the reward scale more uniform across
different games. Often, when deep RL is applied to Atari, rewards are clipped to [−1, 1] to avoid
instability. However, the MDP with clipped rewards may have a different optimal policy than the
unclipped MDP. Thus, instead of clipping, we use scaling. We generally choose the scale factor
based on the multiples of points received in the game: for instance, in ATLANTIS, rewards are always
received in multiples of 100 so we scale by 1/100. Table 5 lists the reward scaling factors for all
games where we apply scaling.
37

Game
Reward scaling factor
ALIEN
1/10
AMIDAR
1/10
ASSAULT
1/21
ASTERIX
1/50
ASTEROIDS
1/10
ATLANTIS
1/100
BANKHEIST
1/10
BATTLEZONE
1/1000
BEAMRIDER
1/44
CENTIPEDE
1/100
CHOPPERCOMMAND
1/100
CRAZYCLIMBER
1/100
DEMONATTACK
1/10
FROSTBITE
1/10
GOPHER
1/20
HERO
1/25
KANGAROO
1/100
MONTEZUMAREVENGE
1/100
MSPACMAN
1/10
NAMETHISGAME
1/10
PHOENIX
1/20
PRIVATEEYE
1/100
QBERT
1/25
ROADRUNNER
1/100
QEAQUEST
1/20
SKIING
1/100
SPACEINVADERS
1/5
TIMEPILOT
1/100
VIDEOPINBALL
1/100
WIZARDOFWOR
1/100
Table 5: Factors by which Atari games rewards are scaled by in BRIDGE, for those where we apply reward
scaling.
Procgen levels
The Procgen benchmark [44] consists of 16 games. For each game, one can
generate an arbitrary number of random levels, each of which is identified by a seed. Furthermore,
each game has an “easy” and “hard” difficulty, each with different levels, and some have an additional
“exploration” level which presents a particularly difficult exploration challenge.
While the benchmark is designed to measure generalization of RL agents trained on some number
of levels to unseen levels, we use each level as a separate MDP. For each game, we attempted to
construct an MDP for the easy levels with seeds 0, 1, and 2, the hard level with seed 0, and the
exploration level if it exists for that environment. We denote MAZEE1
30 to be the easy level with seed 1
for the MAZE game limited to T = 30 timesteps; MAZEH0
30 is the analogous hard level with seed 0
and MAZEEX
30 is the exploration level.
We
generally
increased
the
horizon
for
each
game
to
the
highest
value
in
{10, 20, 30, 40, 50, 70, 100, 200} before the number of states was greater than 100 million.
The horizon values we ultimately chose can be seen in the table in Appendix G.3.
Minigrid gridworlds
Minigrid [14] is an extensible framework for building gridworlds. We
considered all the pre-built gridworlds included in Minigrid for inclusion in BRIDGE except for those
requiring natural language observations for specifying the task to be completed. We also excluded
gridworlds with more than 1 million states, since for technical reasons we were unable to parallelize
the construction of tabular MDPs for Minigrid. For gridworlds with randomized start states, we chose
the start state with seed 0. We use T = 100 for all the gridworlds.
38

E.2
Constructing tabular representations
For each of the environments described above, we wrote a program to compute a full tabular
representation of the transition function f and reward function R. Our program uses a search
procedure to iteratively explore every state-action pair. We keep a queue of states that need to be
explored, which at first is just the initial state. In parallel, a number of worker threads take states from
this queue. After popping a state, a worker thread sets the environment to that state and then takes a
previously unexplored action, storing the resulting next state and reward. If there are still unexplored
actions in the current state, it adds it back to the queue. If the next state is not terminal and has not
had all its actions, it can continue this process.
While the search procedure for exhaustively enumerating states is conceptually simple, we expe-
rienced difficulties implementing it efficiently due to the massive scale of some of the MDPs in
BRIDGE. For instance, the full state of the Atari simulator used in the ALE is about 10-12 KB of
data. Storing 100 million states by themselves would thus require over one terabye of memory! We
avoided this problem by aggressively compressing state data using dictionary compression. Other
challenges included efficiently parallelizing the data structures we used to store the queue of states,
the transition function, and the reward function. Our final implementation is able to explore more
than 20,000 state-action pairs per second in PONG while running on 64 cores.
Once we have enumerated all states and actions that can be reached in the given horizon, we also
apply a consolidation step to reduce the number of states. Often, the internal representation of states
in the Atari and Procgen environments includes extra or superfluous data, which leads to duplicate
states in our tabular representation. We repeatedly consolidate states that (a) have the same screen,
(b) have the same rewards for each action, and (c) lead to the same next states for each action. When
no more states can be consolidated, we store the resulting transition and reward functions.
We excluded any MDPs for which every sequence of actions results in the same total reward, since
these are uninteresting from an RL perspective.
E.3
Reward shaping
For each Minigrid environment, we constructed one or more versions with shaped rewards for our
experiments on the effects of reward shaping. We used three potential functions for shaping:
1. Φdist(s): the negative distance from the state to the nearest goal. Distance is measured as the
minimum number of moves needed to reach the goal, assuming there are no obstacles in the
way.
2. Φdoors(s): the number of doors that are open.
3. Φpickup(s): the number of objects that have been picked up at least once.
For one or more potential functions Φ, we augment each reward R(s, a) with Φ(f(s, a))−Φ(s). The
potential functions are chosen to incentivize useful behavior in the environments: moving towards
goals, picking up objects like keys that could be helpful, and opening doors to reach more parts of the
gridworld.
For each Minigrid MDP, we use all potential functions that apply to that MDP. For instance, if an
MDP does not have any doors, we do not use Φdoors. We also apply the combination of Φdoors and
Φpickup if both are applicable to an MDP.
When analysing the reward shaping results, we only include MDPs for which PPO/DQN converged
on both the unshaped and shaped versions.
E.4
Datasheet for BRIDGE
We provide a datasheet, as proposed by Gebru et al. [55], for the BRIDGE dataset.
E.4.1
Motivation
For what purpose was the dataset created?
We have described the purpose extensively in
the paper: we aim to bridge the theory-practice gap in RL. BRIDGE allows this by providing
tabular representations of popular deep RL benchmarks such that instance-dependent bounds can be
39

calculated and compared to empirical RL performance.
Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,
company, institution, organization)?
Not specified for the double-blind reviewing process.
Who funded the creation of the dataset?
Also not specified for the double-blind reviewing
process.
Any other comments?
No.
E.4.2
Composition
What do the instances that comprise the dataset represent (e.g., documents, photos, people,
countries)?
The instances are Markov Decision Processes (MDPs).
How many instances are there in total (of each type, if appropriate)?
There are 155 MDPs in
BRIDGE. They include 67 MDPs based on Atari games from the Arcade Learning Environment [43],
55 MDPs based on Procgen games [44], and 33 MDPs based on MiniGrid gridworlds [14].
Does the dataset contain all possible instances or is it a sample (not necessarily random) of
instances from a larger set?
The MDPs in BRIDGE are based on a small subset of the many
environments that are used for empirically evaluating RL algorithms. We aimed to cover a range of
the most popular environments. To make our analysis possible, we excluded environments that were
not deterministic or did not have discrete action spaces. We also reduced the horizon of many of the
environments to make it tractable to compute their tabular representations.
What data does each instance consist of?
For each MDP, we provide the following data:
• A transition function and a reward function, which are represented as a matrix with an entry
for each state-action pair in the MDP.
• A corresponding gym environment [56] that can be used to train policies for the MDP with
various RL algorithms.
• Properties of the MDP that are calculated from its tabular representation, including the
effective planning window, bounds on the effective horizon, bounds on the covering length,
etc.
• Results of running RL algorithms (PPO, DQN, and GORP) on the MDP. This includes the
empirical sample complexity as well as various metrics logged during training.
• For MiniGrid MDPs, there are additional versions of the MDP with shaped reward functions
(see Appendix E.3) which also include all of the above data.
• For Atari and Procgen MDPs, there is additionally a non-uniform exploration policy (see
Appendix F.2). For Atari games, this is trained via behavior cloning from the Atari-HEAD
[57] dataset; for Procgen games, it is trained on other Procgen levels. We include the above
data recalculated using the non-uniform exploration policy in place of the uniformly random
exploration policy.
Is there a label or target associated with each instance?
In this paper, we aim to bound and/or
estimate the empirical sample complexity of RL algorithms, so these could be considered targets for
each instance.
Is any information missing from individual instances?
There is no information missing.
Are relationships between individual instances made explicit (e.g., users’ movie ratings, social
network links)?
No.
Are there recommended data splits (e.g., training, development/validation, testing)?
No.
Are there any errors, sources of noise, or redundancies in the dataset?
We do not believe
there are errors or sources of noise in the dataset. The tabular representations of the MDPs have
been carefully tested for correspondence with the environments they are based on. There is some
redundancy, as many Atari games are represented more than once with varying horizons.
Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,
websites, tweets, other datasets)?
The dataset is mostly self-contained, except that the gym
40

environments rely on external libraries. There are archival versions of these available through
package managers like PyPI.
Does the dataset contain data that might be considered confidential (e.g., data that is pro-
tected by legal privilege or by doctor– patient confidentiality, data that includes the content of
individuals’ non-public communications)?
No.
Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,
or might otherwise cause anxiety?
No.
E.4.3
Collection process
How was the data associated with each instance acquired?
The data was collected using
open-source implementations of each environment.
What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses
or sensors, manual human curation, software programs, software APIs)?
As described in
Appendix E.2, we developed a software tool to construct the tabular representations of the MDPs in
BRIDGE. We validated the correctness of the tabular MDPs through extensive testing to ensure they
corresponded exactly with the gym implementations of the environments.
If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,
probabilistic with specific sampling probabilities)?
The MDPs in BRIDGE were selected from
three collections of commonly used RL environments: the Arcade Learning Environment, ProcGen,
and MiniGrid. We chose these three collections to represent a broad set of deterministic environments
with discrete action spaces. Within each collection, the environments were further filtered based on
the criteria described in Appendix E.1.
Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and
how were they compensated (e.g., how much were crowdworkers paid)?
Only the authors were
involved in the data collection process.
Over what timeframe was the data collected?
The dataset was assembled between February
2022 and January 2023. The RL environments from which the MDPs in BRIDGE were constructed
were created prior to this; see the cited works for each collection of environments for more details.
Were any ethical review processes conducted (e.g., by an institutional review board)?
No.
E.4.4
Preprocessing/cleaning/labeling
Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,
tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing
of missing values)?
Yes, various preprocessing and analysis was done. See Appendix E.2 for
details.
Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support
unanticipated future uses)?
Yes, this is included with the dataset.
Is the software that was used to preprocess/clean/label the data available?
Yes, this is available
with the rest of our code.
Any other comments?
No.
E.4.5
Uses
Has the dataset been used for any tasks already?
The dataset has thus far only been used to
validate our theory of the effective horizon in this paper.
Is there a repository that links to any or all papers or systems that use the dataset?
There is
not. However, we will require that any uses of the dataset cite this paper, allowing one to use tools
like Semantic Scholar or Google Scholar to find other papers which use the BRIDGE dataset.
What (other) tasks could the dataset be used for?
We hope that the BRIDGE dataset is used for
further efforts to bridge the theory-practice gap in RL. The dataset could be used to identify other
properties or assumptions that hold in common environments, or to calculate instance-dependent
sample complexity bounds and compare them to the empirical sample complexity of RL algorithms.
41

Is there anything about the composition of the dataset or the way it was collected and pre-
processed/cleaned/labeled that might impact future uses?
As we have already mentioned,
BRIDGE is restricted to deterministic MDPs with discrete action spaces and relatively short horizons.
This could mean that analyses of the dataset like ours do not generalize to the broader space of RL
environments that may have continuous action spaces, stochastic transitions, and/or long horizons.
We have included some experiments, like those in Appendix F.1, to show that our theory of the
effective horizon generalizes beyond the MDPs in BRIDGE. We encourage others to do the same and
we hope to address some of these limitations in the future with extensions to BRIDGE.
Are there tasks for which the dataset should not be used?
We do not foresee any particular
tasks for which the dataset should not be used.
Any other comments?
No.
E.4.6
Distribution
Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,
organization) on behalf of which the dataset was created?
Yes, we will distribute the dataset
publicly.
How will the dataset will be distributed (e.g., tarball on website, API, GitHub)?
We are still
finalizing the method through which the dataset will be distributed.
When will the dataset be distributed?
We plan to make the dataset public in May or June 2023.
Will the dataset be distributed under a copyright or other intellectual property (IP) license,
and/or under applicable terms of use (ToU)?
It will be distributed under CC-BY-4.0.
Have any third parties imposed IP-based or other restrictions on the data associated with the
instances?
The Atari ROMs used to construct the Atari MDPs in BRIDGE are copyrighted by the
original creators of the games. However, they are widely used throughout the reinforcement learning
literature and to our knowledge the copyright holders have not complained about this. Since we are
not legal experts, we do not know if releasing our dataset violates their copyright, but we do not
believe that we are harming them since the tabular representations in BRIDGE are only useful for
research purposes and cannot be used to play the games in any meaningful way.
Do any export controls or other regulatory restrictions apply to the dataset or to individual
instances?
No.
Any other comments?
No.
E.4.7
Maintenance
Who will be supporting/hosting/maintaining the dataset?
We (the authors) will support and
maintain the dataset.
How can the owner/curator/manager of the dataset be contacted (e.g., email address)?
Redacted for double-blind review.
Is there an erratum?
We will record reports of any errors in the dataset and release new versions
with descriptions of what was fixed as necessary.
Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?
We will release new versions of the dataset to correct any reported errors as described above. We may
also expand the dataset in the future with more MDPs or new kinds of MDPs, such as stochastic or
continuous-action-space MDPs. Any updates will be communicated through the service we use to
host the dataset (TBD).
If the dataset relates to people, are there applicable limits on the retention of the data associated
with the instances (e.g., were the individuals in question told that their data would be retained
for a fixed period of time and then deleted)?
No.
Will older versions of the dataset continue to be supported/hosted/maintained? If so, please
describe how. If not, please describe how its obsolescence will be communicated to dataset
consumers.
We hope to find a host for the dataset that will retain older versions of the dataset.
We only plan to maintain the latest version of the dataset, however. We will note this policy in the
42

dataset’s description.
If others want to extend/augment/build on/contribute to the dataset, is there a mechanism
for them to do so?
There is no predefined mechanism to contribute to the dataset, but we will
consider external contributions on a case-by-case basis. We encourage others to extend and build on
the dataset.
Any other comments?
No.
F
Experiment details
In this appendix, we describe details of the experiments from Section 6. In particular, we describe
how we calculate the empirical sample complexity of PPO and DQN.
We use the implementations of PPO and DQN from Stable-Baselines3 (SB3) [58]. For the network
archictures, we use convolutional neural nets (CNNs) for Atari and Procgen and a fully-connected
network for Minigrid. The network architectures are the default CNN and fully-connected architec-
tures chosen by SB3. We used the below hyperparameters for PPO and DQN, which are mainly taken
from the tuned Atari hyperparameters in the RL Baselines3 Zoo [59] repository.
We used a discount rate of γ = 1 for all environments and algorithms except for PPO and DQN in
MiniGrid environments, where we used γ = 0.99. We found that the performance of PPO and DQN
significantly degraded when we used γ = 1 for MiniGrid.
PPO
We use the following hyperparameters for PPO:
Hyperparameter
Value
Training timesteps
5,000,000
Number of environments
8
Number of steps per rollout
{128, 1280}
Clipping parameter (ϵ)
0.1
Value function coefficient
0.5
Entropy coefficient
0.01
Optimizer
Adam
Learning rate
2.5 × 10−4
Number of epochs per training batch
4
Minibatch size
256
GAE coefficient (λ)
0.95
Advantage normalization
Yes
Gradient clipping
0.5
Table 6: Hyperparameters we use for PPO.
For each environment, we try rollout lengths of 128 and 1,280, as we find this is the most sensitive
hyperparameter to tune.
DQN
We use the following hyperparameters for DQN:
43

Hyperparameter
Value
Training timesteps
5,000,000
Timesteps before learning starts
0
Replay buffer size
100,000
Target network update interval
1,000
Training frequency
4
Gradient steps per training step
1
Optimizer
Adam
Learning rate
10−4
Exploration fraction
{0.1, 1}
Final ϵ
0.01
Learning rate
10−3
Gradient clipping
10
Table 7: Hyperparameters we use for DQN.
We try decaying the ϵ value for ϵ-greedy over the course of either 500 thousand or 5 million timesteps,
as we found this was the most sensitive hyperparameter to tune for DQN.
Calculating the empirical sample complexity
In order to compute the empirical sample com-
plexities of PPO and DQN, throughout training we run evaluation episodes and see if the algorithms
have discovered an optimal policy yet. During the evaluation, PPO policies take actions according to
the argmax over the probabilities they assign to each action, rather than sampling as during training
episodes. DQN takes actions greedily with respect to its current Q-function (i.e., with ϵ = 0). If the
total episode reward during the evaluation is the optimal return, then we terminate the training run
and record the total number of timesteps interacted with the environment as the empirical sample
complexity. We take the median sample complexity over 5 random seeds and then the minimum over
all hyperparameter settings to get the final empirical sample complexity.
We found that in some environments PPO achieved optimal reward during almost all the training
episodes but none of the evaluation episodes. This can happen if a policy does not assign the highest
probability to an optimal action in some states but can make up for this by being very likely overall
to obtain the highest possible total reward. Thus, if more than half of the training episodes during
an iteration achieve the optimal return, we also count this as converging to an optimal policy for the
purposes of calculating the empirical sample complexity.
F.1
GORP vs. deep RL algorithms over longer horizons
To show that GORP is not just effective over short horizons, we ran additional experiments comparing
GORP, PPO, and DQN in more typical Atari benchmark environments with frameskip 4 and a
horizon of T = 27, 000 (corresponding to a maximum of 30 minutes of gameplay). Similarly to
the environments in BRIDGE, we limit agents to a single life and make the environments entirely
deterministic. The hyperparameters for PPO and DQN are identical to those given in Tables 6 and
7 except for the following changes: we train for 50 million timesteps; we use a discount rate of
γ = 0.99; and, we set an entropy coefficient of 0.01 for PPO.
We use a training batch size of 104 for PPO and decay ϵ for DQN over the course of the first 5 million
timesteps. For GORP, we use k = 1 and tune m for each environment.
F.2
Exploration policies
For the experiments in Section 6 we needed pre-trained policies to initialize PPO with; here, we
describe the details of how we trained them. We used two different training methods: one for Atari
environments and one for Procgen environments.
In the Atari environments, we trained policies via behavior cloning (BC), i.e., supervised learning,
from human data in the Atari-HEAD dataset [57]. We resampled the actions and processed the
screen images from the dataset to align with the frameskip and observation preprocessing of our Atari
environments. We trained a BC policy on each environment for 400 batches of 500 timesteps each.
We used Adam with a learning rate of 10−3. We also added an entropy bonus to the loss function with
44

a weight of 0.1 to avoid the BC policy assigning very little weight to some actions. Our theoretical
results in Section B.1 suggest that this should improve the sample complexity. Since not all Atari
games are included in Atari-HEAD, we only used a subset for the experiments in Section 6.
In the Procgen environments, we pre-trained policies on a set of levels not included in BRIDGE. In
particular, we trained a policy with PPO on 500 easy levels for 25 million timesteps, which in very
similar to the methodology in Cobbe et al. [44]. We also use an entropy bonus in PPO with weight
0.1 for the same reason as above.
Once we have the pre-trained policies, we compute tabular representations of them on the correspond-
ing MDPs in BRIDGE. To do so, we feed the observations for every state in the MDP through the
pre-trained policy network and record the resulting action distribution. This allows us to compute
Qπexpl and thus obtain bounds on the effective horizon when using the pre-trained exploration policy.
F.3
Computational resources
For deep RL experiments, we used a mix of A100, A4000, and A6000 GPUs from Nvidia. We ran the
algorithms either on separate GPUs or sometimes we ran multiple random seeds simultaneously on
the same hardware. We used 1-8 CPU threads to run the RL environments. Using this setup, PPO and
DQN generally took 2-8 hours to complete 5 million timesteps of training. We used early stopping
when the algorithms found an optimal policy before 5 million timesteps, so the amount of compute
per experiment was often less than this.
For constructing and analyzing the tabular MDPs in BRIDGE, we used up to 128 CPU threads and
500 GB of memory. The amount of time necessary to construct and analyze the MDPs ranged from
less than a minute to around 5 days.
G
Additional experiment results
Here, we present additional results from the experiments in Section 6.
G.1
Example of the effective horizon failing to predict generalization
As we described in the discussion, the effective horizon cannot model generalization across different
states. For instance, in PONG-30 (Pong limited to 30 timesteps/15 seconds), the effective horizon
gives a sample complexity of roughly 5 billion timesteps and empirically GORP takes over 80 million
timesteps to converge to an optimal policy (Appendix G.3). However, both PPO and DQN converge
in under 500,000 environment steps. We hypothesize this is because they are able to generalize the
skill of hitting the ball across the multiple rounds of the Pong game, which the effective horizon
cannot capture because it considers learning separately at every timestep.
G.2
Additional plots
1
2
3
4
5
6
7
8
9
≥10
Minimum value of k for which MDP is k-QVI-solvable
0
25
50
75
100
Number of MDPs
PPO sample complexity > 5M
PPO sample complexity ≤5M
Figure 6: The distribution of the minimum values of k for which the MDPs in BRIDGE are k-QVI solvable.
About two thirds are 1-QVI-solvable, meaning they can be solved by simply acting greedily with respect to the
Q-function of the random policy. The MDPs are split into those which PPO can and cannot solve in 5 million
steps; among those that can be solved efficiently, the values of k are even lower.
45

10251050≥1075
103
104
105
106
> 5M
PPO
108 1016≥1024
108 1016≥1024 104
107 ≥1010 104
107≥1010
10251050≥1075
Worst-case
(T⌈AT /2⌉)
104
105
106
> 5M
DQN
108 1016≥1024
Covering length
(TL)
108 1016≥1024
EPW
(T 2AW )
104
107 ≥1010
UCB
(SAT)
104
107≥1010
Effective horizon
(T 2AH)
Sample complexity bounds
Empirical sample complexity
Atari
Procgen
Minigrid
Figure 7: A comparison of sample complexity bounds and the empirical sample complexities of PPO and
DQN across the MDPs in BRIDGE. In each plot, every dot represents one MDP and its color indicates which
benchmark it comes from. Our effective horizon-based bound most closely correlates with empirical sample
complexity. See Table 2 for a quantitative comparison of the bounds.
≤10−8 10−2
≥104
≤10−2
10−1
100
PPO
≤10−1
100
≥101
≤10−8 10−2
≥104
EPW
10−1
100
101
DQN
≤10−1
100
≥101
Effective horizon
Change in sample complexity bounds (ratio)
Change in empirical samp. complex. (ratio)
Minigrid
Figure 8: A comparison between the empirical change in sample complexity and the change predicted by sample
complexity bounds due to reward shaping. See Table 3a for a quantitative comparison.
46

≤100
106
≥1012
Covering length
10−2
100
102
PPO
≤10−2
100
≥102
Effective horizon
Change in sample complexity bounds (ratio)
Change in empirical
samp. complex. (ratio)
Atari
Procgen
Figure 9: A comparison between the empirical change in the sample complexity of PPO and the change predicted
by sample complexity bounds due to initializing with a pre-trained policy. The initial policies for Atari are
trained from human data and those for Procgen are trained on other procedurally generated levels. See Table 3b
for a quantitative comparison.
G.3
List of MDPs in BRIDGE with statistics
The following table lists all the MDPs in BRIDGE, along with various properties: the number of states
S, number of actions A, horizon T, minimum k for which the MDP is k-QVI-solvable, a bound on
the effective horizon using the techniques in Appendix C, a bound on the covering length L using
Lemma D.4, and the effective planning window W.
MDP
S
A
T
Min k
Bound on H
Bound on L
W
ALIEN10
7.97 × 105
18
10
1
3.5
2.27 × 1012
6
AMIDAR20
1.00 × 105
10
20
4
12
4.03 × 1011
12
ASSAULT10
7.21 × 105
7
10
3
9.8
4.56 × 109
8
ASTERIX10
9.03 × 104
9
10
1
3.5
4.99 × 1010
3
ASTEROIDS10
5.11 × 106
14
10
7
10
2.72 × 1012
9
ATLANTIS10
49
4
10
1
1.5
6.12 × 103
3
ATLANTIS20
471
4
20
1
4.7
5.53 × 108
3
ATLANTIS30
4.56 × 103
4
30
19
23
2.89 × 1012
23
ATLANTIS40
2.06 × 104
4
40
5
27.7
9.62 × 1014
9
ATLANTIS50
5.63 × 104
4
50
12
43
4.27 × 1018
38
ATLANTIS70
1.41 × 105
4
70
47
62
5.11 × 1024
50
BANKHEIST10
4.24 × 106
18
10
1
3.5
6.73 × 1013
9
BATTLEZONE10
7.01 × 104
18
10
1
2.3
5.01 × 108
4
BEAMRIDER20
2.39 × 104
9
20
1
3.3
1.10 × 1013
10
BOWLING30
2.50 × 105
6
30
2
18.5
1.73 × 1016
28
BREAKOUT10
238
4
10
1
4.1
7.92 × 106
5
BREAKOUT20
1.27 × 103
4
20
1
5.4
1.01 × 1013
13
BREAKOUT30
2.80 × 103
4
30
4
13
1.15 × 1019
29
BREAKOUT40
6.12 × 103
4
40
4
15.7
1.31 × 1025
29
BREAKOUT50
1.62 × 104
4
50
3
17.3
1.49 × 1031
20
BREAKOUT70
7.83 × 104
4
70
40
48.5
1.86 × 1043
61
BREAKOUT100
1.31 × 105
4
100
74
87.0
2.23 × 1061
75
BREAKOUT200
1.39 × 105
4
200
105
114.9
3.60 × 10121
108
CENTIPEDE10
1.32 × 107
18
10
7
10
7.13 × 1013
7
CHOPPERCOMMAND10
1.39 × 106
18
10
1
3.9
1.95 × 1011
7
CRAZYCLIMBER20
1.77 × 103
9
20
1
3.1
4.02 × 109
8
CRAZYCLIMBER30
4.63 × 105
9
30
1
3.9
2.15 × 1019
18
DEMONATTACK10
6.32 × 104
6
10
5
9
8.19 × 108
9
ENDURO10
2.54 × 107
9
10
7
10
6.95 × 1010
9
FISHINGDERBY10
2.80 × 105
18
10
6
9
3.60 × 1012
9
FREEWAY10
198
3
10
1
4.3
1.39 × 105
6
FREEWAY20
3.16 × 103
3
20
1
7.4
1.14 × 1010
13
FREEWAY30
1.02 × 104
3
30
1
7.8
1.26 × 1014
26
47

FREEWAY40
2.08 × 104
3
40
1
7.9
2.64 × 1017
35
FREEWAY50
3.40 × 104
3
50
1
8.1
5.95 × 1021
44
FREEWAY70
7.02 × 104
3
70
1
9.1
3.89 × 1029
66
FREEWAY100
1.51 × 105
3
100
39
55.4
2.82 × 1041
67
FREEWAY200
6.33 × 105
3
200
3
30.3
1.26 × 1078
150
FROSTBITE10
5.73 × 104
18
10
1
2.9
8.01 × 1010
3
GOPHER30
778
8
30
1
1.9
7.62 × 1010
5
GOPHER40
8.18 × 103
8
40
5
9.7
4.86 × 1012
13
HERO10
4.89 × 103
18
10
1
1
1.85 × 109
3
ICEHOCKEY10
2.53 × 106
18
10
1
3.2
1.01 × 1011
5
KANGAROO20
1.30 × 105
18
20
1
3.7
2.40 × 1017
11
KANGAROO30
5.84 × 106
18
30
21
24
1.07 × 1030
23
MONTEZUMAREVENGE15
8.47 × 103
18
15
1
8.2
1.08 × 1015
15
MSPACMAN20
1.85 × 106
9
20
11
11
8.15 × 1011
11
NAMETHISGAME20
6.04 × 103
6
20
2
8
9.40 × 106
5
PHOENIX10
4.64 × 104
8
10
5
10
1.45 × 1010
8
PONG20
255
6
20
1
3.2
5.60 × 1010
5
PONG30
2.01 × 103
6
30
6
14.9
4.15 × 1015
18
PONG40
1.60 × 104
6
40
6
14.3
2.96 × 1020
23
PONG50
1.25 × 105
6
50
13
19.6
2.04 × 1025
25
PONG70
2.89 × 106
6
70
20
28.1
8.69 × 1034
25
PONG100
3.46 × 107
6
100
42
70.1
2.05 × 1049
25
PRIVATEEYE10
1.29 × 104
18
10
1
4.1
8.64 × 108
8
QBERT10
289
6
10
5
5
3.80 × 105
5
QBERT20
3.75 × 106
6
20
5
14.6
1.18 × 1014
9
ROADRUNNER10
2.37 × 107
18
10
3
9.9
7.34 × 1013
9
SEAQUEST10
5.46 × 103
18
10
1
1.2
4.15 × 108
5
SKIING10
1.75 × 104
3
10
8
10
6.83 × 105
10
SPACEINVADERS10
994
6
10
1
3.0
4.38 × 105
3
TENNIS10
3.79 × 105
18
10
3
8.2
2.29 × 1011
6
TIMEPILOT10
5.03 × 103
10
10
1
3.9
5.76 × 106
6
TUTANKHAM10
1.66 × 104
8
10
3
7.7
1.68 × 109
9
VIDEOPINBALL10
1.25 × 105
9
10
2
9.5
5.10 × 1010
8
WIZARDOFWOR20
8.92 × 103
10
20
7
13
1.89 × 1013
13
BIGFISHE0
10
2.31 × 104
9
10
5
9
2.26 × 1010
8
BIGFISHE1
10
2.74 × 104
9
10
1
2.8
4.57 × 1010
4
BIGFISHE2
10
1.96 × 105
9
10
9
10
5.26 × 1010
10
BIGFISHH0
10
9.26 × 103
9
10
1
4.3
4.19 × 1010
7
CHASERE0
20
8.13 × 105
9
20
2
12.1
1.16 × 1017
15
CHASERE1
20
3.98 × 105
9
20
1
4.4
1.62 × 1017
9
CHASERE2
20
5.03 × 105
9
20
5
15.2
1.34 × 1017
10
CHASERH0
20
8.75 × 105
9
20
18
20
1.04 × 1017
19
CLIMBERE0
10
2.42 × 105
9
10
1
3.4
2.67 × 1010
7
CLIMBERE1
10
1.18 × 105
9
10
1
4.0
8.46 × 109
9
CLIMBERE2
10
1.12 × 105
9
10
1
2.8
8.43 × 109
6
CLIMBERH0
10
2.33 × 105
9
10
1
6.6
2.66 × 1010
10
COINRUNE0
10
2.23 × 105
9
10
1
4.2
1.77 × 1010
8
COINRUNE1
10
6.23 × 104
9
10
1
3.6
4.05 × 109
7
COINRUNE2
10
1.74 × 105
9
10
1
5.0
1.30 × 1010
9
COINRUNH0
10
2.72 × 105
9
10
1
2.3
1.34 × 1010
7
DODGEBALLE0
10
1.19 × 105
10
10
1
4.2
1.47 × 1011
7
DODGEBALLE1
10
1.95 × 104
10
10
2
9.1
1.29 × 1011
10
DODGEBALLE2
10
3.42 × 104
10
10
1
3.7
1.34 × 1011
6
DODGEBALLH0
10
7.24 × 104
10
10
7
10
1.42 × 1011
8
FRUITBOTE0
40
230
9
40
13
18.6
2.32 × 107
10
FRUITBOTE1
40
379
9
40
18
24.1
6.00 × 1011
18
FRUITBOTE2
40
161
9
40
1
3.1
1.12 × 105
2
FRUITBOTH0
40
620
9
40
6
18.7
6.56 × 1013
24
HEISTE1
10
8.28 × 104
9
10
1
6.3
4.96 × 1010
10
48

JUMPERH0
10
1.30 × 105
9
10
1
8.5
5.11 × 1010
10
JUMPERE0
20
1.20 × 105
9
20
1
1
8.86 × 1019
1
JUMPERE1
20
8.29 × 105
9
20
1
1.3
2.01 × 1020
2
JUMPERE2
20
1.38 × 106
9
20
1
2.4
1.29 × 1019
5
JUMPEREX
20
3.40 × 106
9
20
1
13.1
1.09 × 1020
17
LEAPERE1
20
1.05 × 104
9
20
1
11.2
1.48 × 1020
16
LEAPERE2
20
2.20 × 105
9
20
1
4.3
1.85 × 1020
8
LEAPERH0
20
1.71 × 104
9
20
1
13.1
1.54 × 1020
15
LEAPEREX
20
3.31 × 104
9
20
1
12.8
1.62 × 1020
16
MAZEE0
30
244
9
30
1
1.3
1.65 × 105
1
MAZEE1
30
1.46 × 103
9
30
1
14.2
9.02 × 1022
23
MAZEE2
30
655
9
30
1
1.3
4.48 × 107
3
MAZEH0
30
1.75 × 103
9
30
1
7.9
8.26 × 1023
15
MAZEEX
100
3.72 × 104
9
100
1
40.3
4.27 × 1063
76
MINERE0
10
5.96 × 104
9
10
1
3.8
4.84 × 1010
8
MINERE1
10
6.86 × 104
9
10
1
5.2
4.89 × 1010
9
MINERE2
10
5.56 × 104
9
10
1
1.3
4.82 × 1010
3
MINERH0
10
1.04 × 105
9
10
1
3.4
5.04 × 1010
7
NINJAE0
10
2.02 × 105
13
10
1
4.4
2.67 × 1011
9
NINJAE1
10
4.87 × 105
13
10
1
3.9
2.82 × 1011
8
NINJAE2
10
2.11 × 105
13
10
1
7.0
2.67 × 1011
9
NINJAH0
10
6.61 × 104
13
10
1
6.8
2.47 × 1011
10
PLUNDERE0
10
1.55 × 104
10
10
1
1.3
1.26 × 1011
5
PLUNDERE1
10
2.06 × 104
10
10
1
1
1.29 × 1011
3
PLUNDERE2
10
9.99 × 103
10
10
1
3.3
1.22 × 1011
7
PLUNDERH0
10
8.28 × 103
10
10
1
1.3
1.20 × 1011
5
STARPILOTE0
10
3.24 × 105
11
10
2
8.0
4.09 × 1011
9
STARPILOTE1
10
1.76 × 105
11
10
1
3.7
3.93 × 1011
8
STARPILOTE2
10
9.50 × 104
11
10
7
9
3.77 × 1011
7
STARPILOTH0
10
2.46 × 105
11
10
4
9.2
4.02 × 1011
8
EMPTY-5X5
37
3
100
1
1.6
1.76 × 103
5
EMPTY-6X6
65
3
100
1
2.3
3.47 × 103
7
EMPTY-8X8
145
3
100
1
3.0
9.28 × 103
11
EMPTY-16X16
785
3
100
1
7.9
1.22 × 106
27
DOORKEY-5X5
265
6
100
1
3.6
3.85 × 107
11
DOORKEY-6X6
1.30 × 103
6
100
1
5.0
5.00 × 1010
14
DOORKEY-8X8
9.24 × 103
6
100
1
6.6
2.27 × 1015
17
DOORKEY-16X16
3.58 × 105
6
100
1
14.1
1.04 × 1042
29
MULTIROOM-N2-S4
61
6
100
1
2.4
3.27 × 105
7
MULTIROOM-N4-S5
5.70 × 103
6
100
1
17.2
2.29 × 1043
36
MULTIROOM-N6
1.12 × 104
6
100
1
24.8
3.81 × 1072
46
KEYCORRIDORS3R1
169
6
100
1
4.1
2.10 × 107
15
KEYCORRIDORS3R2
4.90 × 103
6
100
1
4.7
2.48 × 1013
15
KEYCORRIDORS3R3
1.08 × 105
6
100
1
5.5
6.27 × 1025
17
KEYCORRIDORS4R3
6.03 × 105
6
100
1
9.1
1.66 × 1042
20
UNLOCK
1.08 × 103
6
100
1
4.6
1.36 × 107
15
UNLOCKPICKUP
1.72 × 104
6
100
1
5.3
6.26 × 1017
16
BLOCKEDUNLOCKPICKUP
5.10 × 105
6
100
1
10.4
5.85 × 1035
24
OBSTRUCTEDMAZE-1DL
3.20 × 103
6
100
1
4.9
1.92 × 1010
14
OBSTRUCTEDMAZE-1DLH
4.35 × 103
6
100
1
5.5
7.25 × 1010
15
OBSTRUCTEDMAZE-1DLHB
6.73 × 104
6
100
1
9.1
6.05 × 1016
21
FOURROOMS
1.04 × 103
6
100
1
5.1
7.25 × 1011
15
LAVACROSSINGS9N1
173
6
100
1
3.7
3.19 × 105
14
LAVACROSSINGS9N2
133
6
100
1
4.1
3.65 × 105
14
LAVACROSSINGS9N3
53
6
100
1
4.5
9.99 × 105
13
LAVACROSSINGS11N5
85
6
100
1
10.4
1.74 × 109
21
SIMPLECROSSINGS9N1
173
6
100
1
3.6
1.84 × 105
14
SIMPLECROSSINGS9N2
133
6
100
1
3.9
1.41 × 105
14
SIMPLECROSSINGS9N3
53
6
100
1
3.9
1.04 × 105
13
49

SIMPLECROSSINGS11N5
85
6
100
1
6.9
2.71 × 107
21
LAVAGAPS5
21
6
100
1
1.8
1.19 × 104
6
LAVAGAPS6
53
6
100
1
3.0
1.15 × 105
9
LAVAGAPS7
85
6
100
1
3.6
4.03 × 105
11
G.4
Table of bounds and empirical sample complexities
This table lists all the sample complexity bounds we calculate for each MDP in BRIDGE along with
the empirical sample complexities of PPO, DQN, and GORP.
Sample complexity bounds
Empirical sample complexities
MDP
Worst-case
Covering length
EPW
UCB
Effective horizon
(T ⌈AT /2⌉)
(T L)
(T 2AW )
(SAT )
(T 2AH )
PPO
DQN
GORP
ALIEN10
1.79 × 1013
2.27 × 1013
3.40 × 109
1.43 × 108
2.78 × 106
> 5 × 106
6.30 × 104
2.70 × 104
AMIDAR20
1.00 × 1021
8.06 × 1012
4.00 × 1014
2.01 × 107
4.00 × 1014
> 5 × 106
2.21 × 106
> 108
ASSAULT10
1.41 × 109
4.56 × 1010
5.76 × 108
5.04 × 107
1.95 × 1010
2.68 × 105
3.26 × 105
3.35 × 106
ASTERIX10
1.74 × 1010
4.99 × 1011
7.29 × 104
8.13 × 106
2.20 × 105
5.20 × 104
7.00 × 104
4.50 × 103
ASTEROIDS10
1.45 × 1012
2.72 × 1013
2.07 × 1012
7.16 × 108
2.89 × 1013
> 5 × 106
> 5 × 106
> 108
ATLANTIS10
5.24 × 106
6.11 × 104
6.40 × 103
1.96 × 103
800
1.10 × 104
1.40 × 104
800
ATLANTIS20
1.10 × 1013
1.11 × 1010
2.56 × 104
3.77 × 104
2.80 × 105
1.65 × 105
2.95 × 105
1.90 × 104
ATLANTIS30
1.73 × 1019
8.66 × 1013
6.33 × 1016
5.48 × 105
6.33 × 1016
> 5 × 106
> 5 × 106
> 108
ATLANTIS40
2.42 × 1025
3.85 × 1016
4.19 × 108
3.29 × 106
3.14 × 1011
> 5 × 106
9.39 × 105
1.56 × 107
ATLANTIS50
3.17 × 1031
2.13 × 1020
1.89 × 1026
1.13 × 107
1.93 × 1029
> 5 × 106
> 5 × 106
> 108
ATLANTIS70
4.88 × 1043
3.58 × 1026
6.21 × 1033
3.96 × 107
1.04 × 1041
> 5 × 106
> 5 × 106
> 108
BANKHEIST10
1.79 × 1013
6.73 × 1014
1.98 × 1013
7.63 × 108
2.16 × 106
9.90 × 104
2.06 × 106
3.78 × 104
BATTLEZONE10
1.79 × 1013
5.01 × 109
1.05 × 107
1.26 × 107
6.84 × 104
5.70 × 104
8.70 × 104
3.60 × 103
BEAMRIDER20
1.22 × 1020
2.20 × 1014
1.39 × 1012
4.30 × 106
5.22 × 105
3.20 × 104
9.50 × 104
2.52 × 104
BOWLING30
3.32 × 1024
5.18 × 1017
5.53 × 1024
4.51 × 107
9.72 × 106
1.26 × 105
1.39 × 106
6.48 × 104
BREAKOUT10
5.24 × 106
7.92 × 107
1.02 × 105
9.52 × 103
2.24 × 104
6.80 × 104
3.00 × 104
2.62 × 103
BREAKOUT20
1.10 × 1013
2.03 × 1014
2.68 × 1010
1.02 × 105
6.83 × 105
4.80 × 104
1.23 × 105
9.07 × 103
BREAKOUT30
1.73 × 1019
3.46 × 1020
2.59 × 1020
3.36 × 105
6.04 × 1010
> 5 × 106
> 5 × 106
2.50 × 106
BREAKOUT40
2.42 × 1025
5.22 × 1026
4.61 × 1020
9.80 × 105
4.35 × 1012
> 5 × 106
> 5 × 106
5.22 × 106
BREAKOUT50
3.17 × 1031
7.46 × 1032
2.75 × 1015
3.24 × 106
1.58 × 1011
> 5 × 106
> 5 × 106
5.68 × 106
BREAKOUT70
4.88 × 1043
1.30 × 1045
2.61 × 1040
2.19 × 107
7.87 × 1032
> 5 × 106
> 5 × 106
> 108
BREAKOUT100
8.03 × 1061
2.23 × 1063
1.43 × 1049
5.25 × 107
2.33 × 1056
> 5 × 106
> 5 × 106
> 108
BREAKOUT200
2.58 × 10122
7.19 × 10123
4.21 × 1069
1.12 × 108
6.26 × 1073
> 5 × 106
> 5 × 106
> 108
CENTIPEDE10
1.79 × 1013
7.13 × 1014
6.12 × 1010
2.38 × 109
3.57 × 1014
> 5 × 106
> 5 × 106
> 108
CHOPPERCOMMAND10
1.79 × 1013
1.95 × 1012
6.12 × 1010
2.49 × 108
8.87 × 106
> 5 × 106
1.85 × 105
6.45 × 104
CRAZYCLIMBER20
1.22 × 1020
8.03 × 1010
1.72 × 1010
3.19 × 105
3.35 × 105
2.18 × 104
3.00 × 103
1.08 × 104
CRAZYCLIMBER30
6.36 × 1029
6.46 × 1020
1.35 × 1020
1.25 × 108
5.11 × 106
6.50 × 104
5.00 × 103
1.04 × 105
DEMONATTACK10
3.02 × 108
8.19 × 109
1.01 × 109
3.79 × 106
1.01 × 109
2.88 × 105
3.05 × 105
1.53 × 106
ENDURO10
1.74 × 1010
6.95 × 1011
3.87 × 1010
2.29 × 109
3.49 × 1011
3.49 × 105
4.57 × 105
9.57 × 108
FISHINGDERBY10
1.79 × 1013
3.60 × 1013
1.98 × 1013
5.04 × 107
1.98 × 1013
> 5 × 106
1.32 × 106
2.10 × 107
FREEWAY10
2.95 × 105
1.39 × 106
7.29 × 104
5.94 × 103
1.11 × 104
2.00 × 103
4.00 × 103
8.70 × 103
FREEWAY20
3.49 × 1010
2.29 × 1011
6.38 × 108
1.89 × 105
1.40 × 106
2.00 × 103
3.00 × 103
1.87 × 105
FREEWAY30
3.09 × 1015
3.78 × 1015
2.29 × 1015
9.21 × 105
4.87 × 106
1.50 × 105
5.60 × 104
1.09 × 106
FREEWAY40
2.43 × 1020
1.06 × 1019
8.01 × 1019
2.50 × 106
9.36 × 106
2.83 × 105
8.20 × 104
2.84 × 106
FREEWAY50
1.79 × 1025
2.97 × 1023
2.46 × 1024
5.10 × 106
1.81 × 107
3.29 × 105
4.00 × 104
5.35 × 106
FREEWAY70
8.76 × 1034
2.73 × 1031
1.51 × 1035
1.47 × 107
1.11 × 108
1.14 × 106
1.85 × 105
4.53 × 107
FREEWAY100
2.58 × 1049
2.82 × 1043
9.27 × 1035
4.54 × 107
2.71 × 1030
> 5 × 106
> 5 × 106
> 108
FREEWAY200
2.66 × 1097
2.52 × 1080
1.48 × 1076
3.80 × 108
2.00 × 1012
> 5 × 106
2.88 × 106
> 108
FROSTBITE10
1.79 × 1013
8.01 × 1011
5.83 × 105
1.03 × 107
3.89 × 105
4.50 × 104
2.17 × 105
5.40 × 103
GOPHER30
1.86 × 1028
2.29 × 1012
2.95 × 107
1.87 × 105
4.32 × 104
2.30 × 104
3.20 × 104
1.44 × 104
GOPHER40
2.66 × 1037
1.94 × 1014
8.80 × 1014
2.62 × 106
8.30 × 1011
> 5 × 106
> 5 × 106
1.31 × 107
HERO10
1.79 × 1013
1.85 × 1010
5.83 × 105
8.80 × 105
1.80 × 103
1.20 × 104
7.00 × 103
3.60 × 103
ICEHOCKEY10
1.79 × 1013
1.01 × 1012
1.89 × 108
4.56 × 108
1.18 × 106
9.00 × 103
2.00 × 104
7.20 × 103
KANGAROO20
1.27 × 1026
4.80 × 1018
2.57 × 1016
4.69 × 107
1.57 × 107
2.62 × 105
8.00 × 104
3.60 × 104
KANGAROO30
6.83 × 1038
3.21 × 1031
6.69 × 1031
3.16 × 109
1.20 × 1033
> 5 × 106
> 5 × 106
> 108
MONTEZUMAREVENGE15
5.06 × 1019
1.62 × 1016
1.52 × 1021
2.29 × 106
4.22 × 1012
> 5 × 106
> 5 × 106
> 108
MSPACMAN20
1.22 × 1020
1.63 × 1013
1.26 × 1013
3.32 × 108
1.26 × 1013
> 5 × 106
4.43 × 106
> 108
NAMETHISGAME20
3.66 × 1016
1.88 × 108
3.11 × 106
7.25 × 105
3.80 × 107
5.08 × 105
9.60 × 104
2.30 × 105
PHOENIX10
5.37 × 109
1.45 × 1011
1.68 × 109
3.72 × 106
1.07 × 1011
> 5 × 106
3.37 × 105
6.55 × 106
PONG20
3.66 × 1016
1.12 × 1012
3.11 × 106
3.06 × 104
8.64 × 104
5.50 × 104
1.90 × 104
4.80 × 103
PONG30
3.32 × 1024
1.25 × 1017
9.14 × 1016
3.61 × 105
4.66 × 109
1.41 × 105
9.70 × 104
8.40 × 107
PONG40
2.67 × 1032
1.18 × 1022
1.26 × 1021
3.85 × 106
2.35 × 1014
4.93 × 105
3.38 × 105
2.49 × 107
PONG50
2.02 × 1040
1.02 × 1027
7.11 × 1022
3.76 × 107
4.71 × 1018
> 5 × 106
1.03 × 106
> 108
PONG70
1.03 × 1056
6.08 × 1036
1.39 × 1023
1.21 × 109
3.66 × 1025
> 5 × 106
> 5 × 106
> 108
PONG100
3.27 × 1079
2.05 × 1051
2.84 × 1023
2.08 × 1010
3.60 × 1058
> 5 × 106
> 5 × 106
> 108
PRIVATEEYE10
1.79 × 1013
8.64 × 109
1.10 × 1012
2.32 × 106
1.39 × 107
4.20 × 104
2.50 × 104
7.74 × 104
QBERT10
3.02 × 108
3.80 × 106
7.78 × 105
1.73 × 104
7.78 × 105
3.39 × 105
1.30 × 104
2.41 × 105
QBERT20
3.66 × 1016
2.36 × 1015
4.03 × 109
4.51 × 108
4.99 × 1013
5.75 × 105
2.95 × 105
1.71 × 107
ROADRUNNER10
1.79 × 1013
7.34 × 1014
1.98 × 1013
4.26 × 109
2.52 × 1014
8.10 × 104
1.32 × 105
2.24 × 106
SEAQUEST10
1.79 × 1013
4.15 × 109
1.89 × 108
9.84 × 105
3.60 × 103
4.00 × 103
1000
3.58 × 103
SKIING10
2.95 × 105
6.83 × 106
5.90 × 106
5.25 × 105
5.90 × 106
> 5 × 106
3.58 × 106
8.53 × 106
SPACEINVADERS10
3.02 × 108
4.38 × 106
2.16 × 104
5.96 × 104
2.28 × 104
4.40 × 104
1.30 × 104
1.20 × 103
TENNIS10
1.79 × 1013
2.29 × 1012
3.40 × 109
6.82 × 107
1.01 × 108
> 5 × 106
5.94 × 105
1.17 × 106
TIMEPILOT10
5.00 × 1010
5.76 × 107
1.00 × 108
5.03 × 105
8.83 × 105
3.30 × 104
2.06 × 105
7.00 × 103
50

TUTANKHAM10
5.37 × 109
1.68 × 1010
1.34 × 1010
1.33 × 106
8.45 × 108
3.19 × 105
4.98 × 105
2.04 × 105
VIDEOPINBALL10
1.74 × 1010
5.10 × 1011
4.30 × 109
1.13 × 107
1.38 × 108
> 5 × 106
1.58 × 106
5.64 × 104
WIZARDOFWOR20
1.00 × 1021
3.78 × 1014
4.00 × 1015
1.78 × 106
4.00 × 1015
> 5 × 106
> 5 × 106
> 108
BIGFISHE0
10
1.74 × 1010
2.26 × 1011
4.30 × 109
2.08 × 106
3.87 × 1010
> 5 × 106
1.21 × 105
1.14 × 107
BIGFISHE1
10
1.74 × 1010
4.57 × 1011
6.56 × 105
2.47 × 106
4.68 × 104
5.30 × 104
1.10 × 104
1.79 × 103
BIGFISHE2
10
1.74 × 1010
5.26 × 1011
3.49 × 1011
1.76 × 107
3.49 × 1011
> 5 × 106
1.58 × 106
> 108
BIGFISHH0
10
1.74 × 1010
4.19 × 1011
4.78 × 108
8.33 × 105
1.30 × 106
> 5 × 106
3.10 × 105
3.99 × 104
CHASERE0
20
1.22 × 1020
2.32 × 1018
8.24 × 1016
1.46 × 108
8.89 × 109
4.42 × 105
2.03 × 106
2.38 × 106
CHASERE1
20
1.22 × 1020
3.23 × 1018
1.55 × 1011
7.16 × 107
6.27 × 106
> 5 × 106
3.33 × 106
2.30 × 105
CHASERE2
20
1.22 × 1020
2.67 × 1018
1.39 × 1012
9.05 × 107
4.95 × 1014
2.42 × 105
8.17 × 105
7.87 × 106
CHASERH0
20
1.22 × 1020
2.07 × 1018
5.40 × 1020
1.58 × 108
4.86 × 1021
> 5 × 106
> 5 × 106
> 108
CLIMBERE0
10
1.74 × 1010
2.67 × 1011
4.78 × 108
2.18 × 107
1.59 × 105
2.50 × 104
1.43 × 105
4.50 × 103
CLIMBERE1
10
1.74 × 1010
8.46 × 1010
3.87 × 1010
1.06 × 107
5.89 × 105
7.30 × 104
3.03 × 105
3.21 × 104
CLIMBERE2
10
1.74 × 1010
8.43 × 1010
5.31 × 107
1.01 × 107
5.04 × 104
8.70 × 104
9.00 × 104
5.67 × 104
CLIMBERH0
10
1.74 × 1010
2.66 × 1011
3.49 × 1011
2.09 × 107
2.09 × 108
> 5 × 106
> 5 × 106
1.94 × 107
COINRUNE0
10
1.74 × 1010
1.77 × 1011
4.30 × 109
2.01 × 107
9.33 × 105
1.14 × 105
2.03 × 105
8.35 × 105
COINRUNE1
10
1.74 × 1010
4.05 × 1010
4.78 × 108
5.61 × 106
2.88 × 105
5.40 × 104
8.60 × 104
1.72 × 105
COINRUNE2
10
1.74 × 1010
1.30 × 1011
3.87 × 1010
1.57 × 107
5.88 × 106
9.32 × 105
4.91 × 105
5.08 × 106
COINRUNH0
10
1.74 × 1010
1.34 × 1011
4.78 × 108
2.45 × 107
1.44 × 104
4.00 × 103
6.00 × 103
1.24 × 104
DODGEBALLE0
10
5.00 × 1010
1.47 × 1012
1.00 × 109
1.19 × 107
1.51 × 106
1.31 × 105
9.80 × 104
1.76 × 104
DODGEBALLE1
10
5.00 × 1010
1.29 × 1012
1.00 × 1012
1.95 × 106
2.99 × 107
3.29 × 105
4.46 × 105
5.08 × 106
DODGEBALLE2
10
5.00 × 1010
1.34 × 1012
1.00 × 108
3.42 × 106
4.98 × 105
1.74 × 105
1.26 × 105
1.59 × 104
DODGEBALLH0
10
5.00 × 1010
1.42 × 1012
1.00 × 1010
7.24 × 106
1.00 × 1012
> 5 × 106
4.88 × 106
8.16 × 106
FRUITBOTE0
40
2.96 × 1039
9.28 × 108
5.58 × 1012
8.28 × 104
9.80 × 1020
> 5 × 106
1.44 × 105
4.00 × 107
FRUITBOTE1
40
2.96 × 1039
2.40 × 1013
2.40 × 1020
1.36 × 105
1.74 × 1026
> 5 × 106
3.61 × 105
> 108
FRUITBOTE2
40
2.96 × 1039
4.47 × 106
1.30 × 105
5.80 × 104
1.43 × 106
2.50 × 104
2.00 × 103
1.01 × 104
FRUITBOTH0
40
2.96 × 1039
2.62 × 1015
1.28 × 1026
2.23 × 105
1.17 × 1021
> 5 × 106
> 5 × 106
> 108
HEISTE1
10
1.74 × 1010
4.96 × 1011
3.49 × 1011
7.45 × 106
1.06 × 108
> 5 × 106
6.86 × 105
7.09 × 107
JUMPERH0
10
1.74 × 1010
5.11 × 1011
3.49 × 1011
1.17 × 107
1.41 × 1010
> 5 × 106
> 5 × 106
> 108
JUMPERE0
20
1.22 × 1020
1.77 × 1021
3.60 × 103
2.15 × 107
3.60 × 103
224
20
374
JUMPERE1
20
1.22 × 1020
4.02 × 1021
3.24 × 104
1.49 × 108
7.20 × 103
888
46
4.96 × 103
JUMPERE2
20
1.22 × 1020
2.59 × 1020
2.36 × 107
2.48 × 108
8.64 × 104
2.56 × 104
7.00 × 103
3.42 × 104
JUMPEREX
20
1.22 × 1020
2.18 × 1021
6.67 × 1018
6.13 × 108
1.24 × 1015
> 5 × 106
> 5 × 106
> 108
LEAPERE1
20
1.22 × 1020
2.96 × 1021
7.41 × 1017
1.90 × 106
2.16 × 1013
> 5 × 106
> 5 × 106
> 108
LEAPERE2
20
1.22 × 1020
3.69 × 1021
1.72 × 1010
3.97 × 107
4.90 × 106
1.27 × 105
2.34 × 105
2.94 × 106
LEAPERH0
20
1.22 × 1020
3.07 × 1021
8.24 × 1016
3.08 × 106
1.27 × 1015
> 5 × 106
> 5 × 106
> 108
LEAPEREX
20
1.22 × 1020
3.23 × 1021
7.41 × 1017
5.96 × 106
7.04 × 1014
> 5 × 106
2.61 × 106
> 108
MAZEE0
30
6.36 × 1029
4.95 × 106
8.10 × 103
6.59 × 104
1.62 × 104
1000
66
4.57 × 103
MAZEE1
30
6.36 × 1029
2.71 × 1024
7.98 × 1024
3.95 × 105
3.44 × 1016
> 5 × 106
> 5 × 106
> 108
MAZEE2
30
6.36 × 1029
1.35 × 109
6.56 × 105
1.77 × 105
1.62 × 104
1000
411
1.36 × 104
MAZEH0
30
6.36 × 1029
2.48 × 1025
1.85 × 1017
4.73 × 105
3.01 × 1010
> 5 × 106
> 5 × 106
> 108
MAZEEX
100
1.33 × 1097
4.27 × 1065
3.33 × 1076
3.34 × 107
2.67 × 1042
> 5 × 106
> 5 × 106
> 108
MINERE0
10
1.74 × 1010
4.84 × 1011
4.30 × 109
5.37 × 106
4.58 × 105
9.40 × 104
7.40 × 104
4.69 × 105
MINERE1
10
1.74 × 1010
4.89 × 1011
3.87 × 1010
6.17 × 106
9.34 × 106
3.00 × 105
3.77 × 105
1.74 × 106
MINERE2
10
1.74 × 1010
4.82 × 1011
7.29 × 104
5.00 × 106
1.80 × 103
1.02 × 104
2.00 × 103
1.79 × 103
MINERH0
10
1.74 × 1010
5.04 × 1011
4.78 × 108
9.35 × 106
1.92 × 105
4.90 × 104
4.03 × 105
4.50 × 103
NINJAE0
10
6.89 × 1011
2.67 × 1012
1.06 × 1012
2.62 × 107
8.49 × 106
2.64 × 105
> 5 × 106
5.32 × 106
NINJAE1
10
6.89 × 1011
2.82 × 1012
8.16 × 1010
6.33 × 107
2.43 × 106
4.45 × 105
4.23 × 105
1.85 × 106
NINJAE2
10
6.89 × 1011
2.67 × 1012
1.06 × 1012
2.74 × 107
5.97 × 109
> 5 × 106
> 5 × 106
> 108
NINJAH0
10
6.89 × 1011
2.47 × 1012
1.38 × 1013
8.60 × 106
3.57 × 109
> 5 × 106
> 5 × 106
> 108
PLUNDERE0
10
5.00 × 1010
1.26 × 1012
1.00 × 107
1.55 × 106
2.00 × 103
1.15 × 104
1.00 × 104
5.00 × 103
PLUNDERE1
10
5.00 × 1010
1.29 × 1012
1.00 × 105
2.06 × 106
1000
2.00 × 103
3.00 × 103
2.00 × 103
PLUNDERE2
10
5.00 × 1010
1.22 × 1012
1.00 × 109
9.99 × 105
1.96 × 105
6.40 × 104
1.13 × 105
2.10 × 104
PLUNDERH0
10
5.00 × 1010
1.20 × 1012
1.00 × 107
8.28 × 105
2.00 × 103
3.00 × 103
6.00 × 103
4.00 × 103
STARPILOTE0
10
1.30 × 1011
4.09 × 1012
2.36 × 1011
3.56 × 107
1.99 × 109
> 5 × 106
6.36 × 105
7.02 × 105
STARPILOTE1
10
1.30 × 1011
3.93 × 1012
2.14 × 1010
1.94 × 107
6.90 × 105
5.20 × 104
1.37 × 105
6.24 × 103
STARPILOTE2
10
1.30 × 1011
3.77 × 1012
1.95 × 109
1.05 × 107
2.36 × 1011
9.90 × 104
2.67 × 105
2.61 × 105
STARPILOTH0
10
1.30 × 1011
4.02 × 1012
2.14 × 1010
2.71 × 107
1.04 × 1010
> 5 × 106
5.12 × 105
2.15 × 107
EMPTY-5X5
2.58 × 1049
1.76 × 105
2.43 × 106
1.11 × 104
6.00 × 104
1.02 × 103
232
3.86 × 104
EMPTY-6X6
2.58 × 1049
3.47 × 105
2.19 × 107
1.95 × 104
1.20 × 105
2.05 × 103
543
4.50 × 104
EMPTY-8X8
2.58 × 1049
9.28 × 105
1.77 × 109
4.35 × 104
2.70 × 105
1.57 × 104
7.90 × 104
8.49 × 104
EMPTY-16X16
2.58 × 1049
1.22 × 108
7.63 × 1016
2.35 × 105
5.93 × 107
1.78 × 105
3.81 × 105
3.02 × 107
DOORKEY-5X5
3.27 × 1079
3.85 × 109
3.63 × 1012
1.59 × 105
6.30 × 106
7.01 × 104
1.45 × 105
8.17 × 105
DOORKEY-6X6
3.27 × 1079
5.00 × 1012
7.84 × 1014
7.81 × 105
7.51 × 107
7.22 × 105
2.85 × 105
6.79 × 106
DOORKEY-8X8
3.27 × 1079
2.27 × 1017
1.69 × 1017
5.55 × 106
1.34 × 109
8.35 × 105
7.23 × 105
> 108
DOORKEY-16X16
3.27 × 1079
1.04 × 1044
3.68 × 1026
2.15 × 108
1.01 × 1015
> 5 × 106
> 5 × 106
> 108
MULTIROOM-N2-S4
3.27 × 1079
3.27 × 107
2.80 × 109
3.66 × 104
7.80 × 105
3.63 × 104
1.23 × 105
9.98 × 104
MULTIROOM-N4-S5
3.27 × 1079
2.29 × 1045
1.03 × 1032
3.42 × 106
2.33 × 1017
> 5 × 106
> 5 × 106
> 108
MULTIROOM-N6
3.27 × 1079
3.81 × 1074
6.24 × 1039
6.72 × 106
1.84 × 1023
> 5 × 106
> 5 × 106
> 108
KEYCORRIDORS3R1
3.27 × 1079
2.10 × 109
4.70 × 1015
1.01 × 105
1.46 × 107
3.31 × 105
1.72 × 105
2.82 × 106
KEYCORRIDORS3R2
3.27 × 1079
2.48 × 1015
4.70 × 1015
2.94 × 106
4.21 × 107
6.32 × 105
3.40 × 105
6.59 × 106
KEYCORRIDORS3R3
3.27 × 1079
6.27 × 1027
1.69 × 1017
6.48 × 107
1.80 × 108
1.01 × 106
1.33 × 106
3.01 × 107
KEYCORRIDORS4R3
3.27 × 1079
1.66 × 1044
3.66 × 1019
3.62 × 108
1.17 × 1011
> 5 × 106
> 5 × 106
> 108
UNLOCK
3.27 × 1079
1.36 × 109
4.70 × 1015
6.51 × 105
3.76 × 107
1.79 × 105
2.70 × 105
5.36 × 106
UNLOCKPICKUP
3.27 × 1079
6.26 × 1019
2.82 × 1016
1.03 × 107
1.31 × 108
6.00 × 105
3.58 × 105
1.37 × 107
BLOCKEDUNLOCKPICKUP
3.27 × 1079
5.85 × 1037
4.74 × 1022
3.06 × 108
1.25 × 1012
1.81 × 106
> 5 × 106
> 108
OBSTRUCTEDMAZE-1DL
3.27 × 1079
1.92 × 1012
7.84 × 1014
1.92 × 106
6.09 × 107
4.31 × 105
4.05 × 105
6.09 × 106
OBSTRUCTEDMAZE-1DLH
3.27 × 1079
7.25 × 1012
4.70 × 1015
2.61 × 106
1.77 × 108
7.32 × 105
1.86 × 106
2.12 × 107
OBSTRUCTEDMAZE-1DLHB
3.27 × 1079
6.05 × 1018
2.19 × 1020
4.04 × 107
1.28 × 1011
> 5 × 106
> 5 × 106
> 108
FOURROOMS
3.27 × 1079
7.25 × 1013
4.70 × 1015
6.22 × 105
9.54 × 107
> 5 × 106
3.26 × 105
1.00 × 107
51

LAVACROSSINGS9N1
3.27 × 1079
3.19 × 107
7.84 × 1014
1.04 × 105
7.80 × 106
1.26 × 105
1.80 × 106
9.86 × 105
LAVACROSSINGS9N2
3.27 × 1079
3.65 × 107
7.84 × 1014
7.98 × 104
1.63 × 107
2.93 × 105
1.55 × 106
2.09 × 106
LAVACROSSINGS9N3
3.27 × 1079
9.99 × 107
1.31 × 1014
3.18 × 104
3.32 × 107
9.70 × 104
1.56 × 106
5.24 × 106
LAVACROSSINGS11N5
3.27 × 1079
1.74 × 1011
2.19 × 1020
5.10 × 104
1.19 × 1012
> 5 × 106
> 5 × 106
> 108
SIMPLECROSSINGS9N1
3.27 × 1079
1.84 × 107
7.84 × 1014
1.04 × 105
6.30 × 106
7.65 × 104
2.34 × 105
1.02 × 106
SIMPLECROSSINGS9N2
3.27 × 1079
1.41 × 107
7.84 × 1014
7.98 × 104
1.01 × 107
9.63 × 104
2.15 × 105
1.55 × 106
SIMPLECROSSINGS9N3
3.27 × 1079
1.04 × 107
1.31 × 1014
3.18 × 104
1.16 × 107
3.91 × 104
1.59 × 105
2.64 × 106
SIMPLECROSSINGS11N5
3.27 × 1079
2.71 × 109
2.19 × 1020
5.10 × 104
2.49 × 109
5.42 × 105
1.26 × 106
> 108
LAVAGAPS5
3.27 × 1079
1.19 × 106
4.67 × 108
1.26 × 104
2.40 × 105
3.69 × 104
1.19 × 105
4.33 × 104
LAVAGAPS6
3.27 × 1079
1.15 × 107
1.01 × 1011
3.18 × 104
2.28 × 106
1.49 × 105
2.35 × 105
1.96 × 105
LAVAGAPS7
3.27 × 1079
4.03 × 107
3.63 × 1012
5.10 × 104
6.78 × 106
2.41 × 105
2.38 × 105
8.20 × 105
G.5
Table of returns
This table lists the returns for the random and optimal policies in each MDP as well as the final
returns achieved by PPO, DQN, and GORP. We ran PPO and DQN for 5 million timesteps in each
environment, and GORP for both 5 million and 100 million timesteps.
Returns
MDP
Random policy
Optimal policy
PPO
DQN
GORP (5M)
(100M)
ALIEN10
74.74
160
160
160
160
160
AMIDAR20
6.77
109
68
68
65
68
ASSAULT10
8.66
126
126
126
126
126
ASTERIX10
99.24
400
400
400
400
400
ASTEROIDS10
15.08
320
170
190
220
220
ATLANTIS10
24.22
200
200
200
200
200
ATLANTIS20
124.25
1, 200
1, 200
1, 200
1, 200
1, 200
ATLANTIS30
145.46
4, 300
1, 600
1, 600
1, 700
4, 000
ATLANTIS40
146.68
5, 500
5, 500
5, 400
5, 400
5, 500
ATLANTIS50
146.84
7, 800
7, 600
5, 500
5, 600
7, 700
ATLANTIS70
146.86
11, 100
8, 900
8, 900
8, 200
10, 900
BANKHEIST10
0.36
30
30
30
30
30
BATTLEZONE10
115.32
2, 000
2, 000
2, 000
2, 000
2, 000
BEAMRIDER20
23.77
132
132
132
132
132
BOWLING30
1.82
9
9
9
9
9
BREAKOUT10
0.13
2
2
2
2
2
BREAKOUT20
0.17
4
4
4
4
4
BREAKOUT30
0.18
9
4
7
9
9
BREAKOUT40
0.18
11
9
9
9
11
BREAKOUT50
0.18
13
10
10
10
13
BREAKOUT70
0.18
22
13
14
4
17
BREAKOUT100
0.18
44
14
14
4
19
BREAKOUT200
0.18
60
14
14
4
17
CENTIPEDE10
141.37
1, 923
922
1, 632
1, 511
1, 621
CHOPPERCOMMAND10
117.25
600
600
600
600
600
CRAZYCLIMBER20
125
400
400
400
400
400
CRAZYCLIMBER30
248.16
900
900
900
900
900
DEMONATTACK10
8.31
50
50
50
50
50
ENDURO10
0.04
6
6
6
4
6
FISHINGDERBY10
0.16
8
6
8
6
8
FREEWAY10
0.01
1
1
1
1
1
FREEWAY20
0.02
2
2
2
2
2
FREEWAY30
0.07
4
4
4
4
4
FREEWAY40
0.1
5
5
5
5
5
FREEWAY50
0.14
6
6
6
5
6
FREEWAY70
0.23
9
9
9
7
9
FREEWAY100
0.33
13
12
12
9
12
FREEWAY200
0.72
25
24
25
12
20
FROSTBITE10
9.13
70
70
70
70
70
GOPHER30
1.43
20
20
20
20
20
GOPHER40
10.05
180
100
100
100
180
HERO10
8.18
75
75
75
75
75
ICEHOCKEY10
0.01
1
1
1
1
1
KANGAROO20
1.02
200
200
200
200
200
52

KANGAROO30
3.84
500
200
200
400
400
MONTEZUMAREVENGE15
0
100
0
0
0
0
MSPACMAN20
148.91
480
460
470
470
470
NAMETHISGAME20
11.22
180
180
180
180
180
PHOENIX10
23.43
260
260
260
260
260
PONG20
−2.71
−1
−1
−1
−1
−1
PONG30
−4.25
−1
−1
−1
−1
−1
PONG40
−5.99
0
−1
0
0
0
PONG50
−7.85
1
−1
1
0
0
PONG70
−11.47
2
−1
0
−1
0
PONG100
−16.91
4
−1
−1
−1
1
PRIVATEEYE10
0.11
100
100
100
100
100
QBERT10
50.95
425
425
425
425
425
QBERT20
87.21
675
675
675
650
675
ROADRUNNER10
0.05
600
600
600
600
600
SEAQUEST10
0.78
20
20
20
20
20
SKIING10
−12, 652.16
−7, 066
−8, 149
−7, 078
−7, 069
−7, 066
SPACEINVADERS10
3.46
40
40
40
40
40
TENNIS10
−0.75
1
0
1
1
1
TIMEPILOT10
8.17
200
200
200
200
200
TUTANKHAM10
0.07
23
23
23
23
23
VIDEOPINBALL10
108.99
4, 100
3, 000
4, 100
4, 100
4, 100
WIZARDOFWOR20
11.5
500
100
100
300
400
BIGFISHE0
10
0.37
4
4
4
4
4
BIGFISHE1
10
0.73
3
3
3
3
3
BIGFISHE2
10
2.98
8
7
8
7
7
BIGFISHH0
10
0.03
3
3
3
3
3
CHASERE0
20
0.38
0.88
0.88
0.88
1
1
CHASERE1
20
0.33
0.88
0.88
0.88
1
1
CHASERE2
20
0.37
0.88
0.88
0.88
1
1
CHASERH0
20
0.39
0.88
0.88
0.84
1
1
CLIMBERE0
10
0.14
2
2
2
2
2
CLIMBERE1
10
0.02
2
2
2
2
2
CLIMBERE2
10
0.03
11
11
11
11
11
CLIMBERH0
10
0.31
2
1
1
1
2
COINRUNE0
10
0
10
10
10
10
10
COINRUNE1
10
0
10
10
0
10
10
COINRUNE2
10
0
10
10
0
10
10
COINRUNH0
10
0.08
10
10
10
10
10
DODGEBALLE0
10
0.08
16
16
16
16
16
DODGEBALLE1
10
0.24
8
8
8
6
8
DODGEBALLE2
10
0.1
8
8
8
8
8
DODGEBALLH0
10
0.04
8
6
6
6
8
FRUITBOTE0
40
−4.04
5
2
5
2
5
FRUITBOTE1
40
−2.09
7
4
7
4
5
FRUITBOTE2
40
−5.84
1
1
1
1
1
FRUITBOTH0
40
−0.42
10
5
10
0
9
HEISTE1
10
0
10
0
0
0
10
JUMPERH0
10
0
10
0
0
0
0
JUMPERE0
20
4.99
10
10
10
10
10
JUMPERE1
20
4.96
10
10
10
10
10
JUMPERE2
20
0.06
10
10
10
10
10
JUMPEREX
20
0
10
0
0
0
0
LEAPERE1
20
0
10
0
0
0
0
LEAPERE2
20
0
10
10
10
10
10
LEAPERH0
20
0
10
0
0
0
0
LEAPEREX
20
0
10
0
0
0
0
MAZEE0
30
6.1
10
10
10
10
10
MAZEE1
30
0
10
0
0
0
0
MAZEE2
30
5.65
10
10
10
10
10
MAZEH0
30
0
10
0
0
0
0
53

MAZEEX
100
0
10
0
0
0
0
MINERE0
10
0
1
1
1
1
1
MINERE1
10
0.05
2
2
2
2
2
MINERE2
10
0.11
1
1
1
1
1
MINERH0
10
0.04
3
3
3
3
3
NINJAE0
10
0
10
10
0
10
10
NINJAE1
10
0
10
10
0
10
10
NINJAE2
10
0
10
0
0
0
0
NINJAH0
10
0
10
0
0
0
0
PLUNDERE0
10
0.06
1
1
1
1
1
PLUNDERE1
10
0.17
1
1
1
1
1
PLUNDERE2
10
0
1
1
1
1
1
PLUNDERH0
10
0.07
1
1
1
1
1
STARPILOTE0
10
0.23
10
9
10
10
10
STARPILOTE1
10
0.14
5
5
5
5
5
STARPILOTE2
10
0.08
5
5
5
5
5
STARPILOTH0
10
0.17
5
4
5
4
5
EMPTY-5X5
0.76
1
1
1
1
1
EMPTY-6X6
0.5
1
1
1
1
1
EMPTY-8X8
0.18
1
1
1
1
1
EMPTY-16X16
0
1
1
0
0
1
DOORKEY-5X5
0.01
1
1
1
1
1
DOORKEY-6X6
0
1
1
1
0
1
DOORKEY-8X8
0
1
0
0
0
0
DOORKEY-16X16
0
1
0
0
0
0
MULTIROOM-N2-S4
0.09
1
1
1
1
1
MULTIROOM-N4-S5
0
1
0
0
0
0
MULTIROOM-N6
0
1
0
0
0
0
KEYCORRIDORS3R1
0
1
1
1
1
1
KEYCORRIDORS3R2
0
1
1
1
0
1
KEYCORRIDORS3R3
0
1
0
0
0
1
KEYCORRIDORS4R3
0
1
0
0
0
0
UNLOCK
0
1
1
1
1
1
UNLOCKPICKUP
0
1
1
0
0
1
BLOCKEDUNLOCKPICKUP
0
1
0
0
0
0
OBSTRUCTEDMAZE-1DL
0
1
1
0
0
1
OBSTRUCTEDMAZE-1DLH
0
1
0
0
0
1
OBSTRUCTEDMAZE-1DLHB
0
1
0
0
0
0
FOURROOMS
0
1
1
0
0
1
LAVACROSSINGS9N1
0
1
1
1
1
1
LAVACROSSINGS9N2
0
1
1
1
1
1
LAVACROSSINGS9N3
0
1
1
0
0
1
LAVACROSSINGS11N5
0
1
0
0
0
0
SIMPLECROSSINGS9N1
0.01
1
1
1
1
1
SIMPLECROSSINGS9N2
0
1
1
1
1
1
SIMPLECROSSINGS9N3
0
1
1
1
1
1
SIMPLECROSSINGS11N5
0
1
0
0
0
0
LAVAGAPS5
0.07
1
1
1
1
1
LAVAGAPS6
0.02
1
1
1
1
1
LAVAGAPS7
0.01
1
1
1
1
1
54

G.6
Learning curves for BRIDGE MDPs
0
5M
100
150
ALIEN10
0
5M
50
100
AMIDAR20
0
5M
50
100
ASSAULT10
0
5M
200
400
ASTERIX10
0
5M
0
250
ASTEROIDS10
0
50k
100
200
ATLANTIS10
0
500k
500
1000
ATLANTIS20
0
5M
0
2500
ATLANTIS30
0
5M
0
5000
ATLANTIS40
0
5M
0
5000
ATLANTIS50
0
5M
0
10000
ATLANTIS70
0
2.5M
0
25
BANKHEIST10
0
1M
0
2000
BATTLEZONE10
0
5M
50
100
BEAMRIDER20
0
2.5M
0
5
BOWLING30
0
2.5M
0
2
BREAKOUT10
0
5M
0.0
2.5
BREAKOUT20
0
5M
0
5
BREAKOUT30
0
5M
0
10
BREAKOUT40
0
5M
0
10
BREAKOUT50
0
5M
0
20
BREAKOUT70
0
5M
0
25
BREAKOUT100
0
5M
0
50
BREAKOUT200
0
5M
1000
2000
CENTIPEDE10
0
5M
250
500
CHOPPERCOMMAND10
0
20k
200
400
CRAZYCLIMBER20
0
2.5M
250
500
750
CRAZYCLIMBER30
0
500k
25
50
DEMONATTACK10
0
5M
0
5
ENDURO10
0
5M
0
5
FISHINGDERBY10
0
100k
0
1
FREEWAY10
0
2.5M
0
2
FREEWAY20
0
5M
0.0
2.5
FREEWAY30
0
2.5M
0
5
FREEWAY40
0
5M
0
5
FREEWAY50
0
5M
0
5
FREEWAY70
0
5M
0
10
FREEWAY100
0
5M
0
25
FREEWAY200
0
250k
25
50
FROSTBITE10
0
500k
0
20
GOPHER30
0
5M
50
100
150
GOPHER40
0
25k
0
50
HERO10
0
100k
0
1
ICEHOCKEY10
0
2.5M
0
200
KANGAROO20
0
5M
0
500
KANGAROO30
0
5M
0
100
MONTEZUMAREVENGE15
0
5M
200
400
MSPACMAN20
0
5M
50
100
150
NAMETHISGAME20
0
5M
100
200
PHOENIX10
0
50k
−2
−1
PONG20
0
5M
−4
−2
PONG30
0
5M
−5
0
PONG40
0
5M
−5
0
PONG50
0
5M
−10
0
PONG70
0
5M
−10
0
PONG100
0
50k
0
100
PRIVATEEYE10
0
5M
200
400
QBERT10
0
2.5M
250
500
QBERT20
0
5M
0
500
ROADRUNNER10
0
1M
0
20
SEAQUEST10
0
5M
−30000
−20000
−10000
SKIING10
0
50k
20
40
SPACEINVADERS10
0
5M
0
1
TENNIS10
0
250k
0
200
TIMEPILOT10
0
500k
0
20
TUTANKHAM10
0
5M
0
2500
VIDEOPINBALL10
0
5M
0
500
WIZARDOFWOR20
0
5M
2
4
BIGFISHE0
10
0
1M
1
2
3
BIGFISHE1
10
0
5M
2.5
5.0
7.5
BIGFISHE2
10
0
5M
0.0
2.5
BIGFISHH0
10
0
5M
0.25
0.50
0.75
CHASERE0
20
0
5M
0.25
0.50
0.75
CHASERE1
20
0
5M
0.25
0.50
0.75
CHASERE2
20
0
5M
0.50
0.75
CHASERH0
20
0
100k
0
2
CLIMBERE0
10
0
500k
0
2
CLIMBERE1
10
0
5M
0
10
CLIMBERE2
10
0
5M
0
2
CLIMBERH0
10
0
1M
0
10
COINRUNE0
10
0
1M
0
10
COINRUNE1
10
0
2.5M
0
10
COINRUNE2
10
0
1M
0
10
COINRUNH0
10
0
200k
0
10
DODGEBALLE0
10
0
5M
0
5
DODGEBALLE1
10
0
250k
0
5
DODGEBALLE2
10
0
5M
0
5
DODGEBALLH0
10
0
5M
0
5
FRUITBOTE0
40
0
5M
0
5
FRUITBOTE1
40
0
25k
−5
0
FRUITBOTE2
40
0
5M
0
10
FRUITBOTH0
40
0
5M
0
10
HEISTE1
10
0
5M
0
10
JUMPERH0
10
0
5k
0
10
JUMPERE0
20
0
2.5M
5
10
JUMPERE1
20
0
500k
0
10
JUMPERE2
20
0
5M
0
10
JUMPEREX
20
0
5M
0
10
LEAPERE1
20
0
2.5M
0
10
LEAPERE2
20
0
5M
0
10
LEAPERH0
20
0
5M
0
10
LEAPEREX
20
0
100k
5
10
MAZEE0
30
0
5M
0
10
MAZEE1
30
0
1M
0
10
MAZEE2
30
0
5M
0
10
MAZEH0
30
0
5M
0
10
MAZEEX
100
0
1M
0
1
MINERE0
10
0
5M
0
2
MINERE1
10
0
10k
0
1
MINERE2
10
0
1M
0.0
2.5
MINERH0
10
0
5M
0
10
NINJAE0
10
0
2M
0
10
NINJAE1
10
0
5M
0
10
NINJAE2
10
0
5M
0
10
NINJAH0
10
0
2M
0
1
PLUNDERE0
10
0
20k
0
1
PLUNDERE1
10
0
200k
0
1
PLUNDERE2
10
0
25k
0
1
PLUNDERH0
10
0
5M
0
10
STARPILOTE0
10
0
250k
0
5
STARPILOTE1
10
0
5M
0
5
STARPILOTE2
10
0
5M
0
5
STARPILOTH0
10
0
50k
0.5
1.0
EMPTY-5X5
0
500k
0.5
1.0
EMPTY-6X6
0
5M
0
1
EMPTY-8X8
0
5M
0
1
EMPTY-16X16
0
2.5M
0
1
DOORKEY-5X5
0
5M
0
1
DOORKEY-6X6
0
5M
0
1
DOORKEY-8X8
0
5M
0
1
DOORKEY-16X16
0
100k
0
1
MULTIROOM-N2-S4
0
5M
0
1
MULTIROOM-N4-S5
0
5M
0
1
MULTIROOM-N6
0
2.5M
0
1
KEYCORRIDORS3R1
0
2.5M
0
1
KEYCORRIDORS3R2
0
5M
0
1
KEYCORRIDORS3R3
0
5M
0
1
KEYCORRIDORS4R3
0
5M
0
1
UNLOCK
0
5M
0
1
UNLOCKPICKUP
0
5M
0
1
BLOCKEDUNLOCKPICKU
0
5M
0
1
OBSTRUCTEDMAZE-1DL
0
5M
0
1
OBSTRUCTEDMAZE-1DLH
0
5M
0
1
OBSTRUCTEDMAZE-1DLHB
0
5M
0
1
FOURROOMS
0
5M
0
1
LAVACROSSINGS9N1
0
5M
0
1
LAVACROSSINGS9N2
0
2.5M
0
1
LAVACROSSINGS9N3
0
5M
0
1
LAVACROSSINGS11N5
0
2M
0
1
SIMPLECROSSINGS9N1
0
2.5M
0
1
SIMPLECROSSINGS9N2
0
2.5M
0
1
SIMPLECROSSINGS9N3
0
5M
0
1
SIMPLECROSSINGS11N5
0
100k
0
1
LAVAGAPS5
0
5M
0
1
LAVAGAPS6
0
500k
0
1
LAVAGAPS7
GORP
PPO
DQN
Optimal return
Figure 10: Learning curves for PPO, DQN, and GORP on all the MDPs in BRIDGE. Solid lines show the median
return (over multiple random seeds) of the policies learned by each algorithm throughout training. We use 5
random seeds for PPO and DQN and 101 random seeds for GORP. The shaded region shows the range of returns
over all random seeds for PPO and DQN, and shows the range from the 10th to 90th percentile over random
seeds for GORP. The optimal return in each environment is shown as the dashed black line.
55

