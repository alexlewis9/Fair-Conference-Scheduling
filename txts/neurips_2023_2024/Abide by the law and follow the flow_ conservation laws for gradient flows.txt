Abide by the Law and Follow the Flow:
Conservation Laws for Gradient Flows
Sibylle Marcotte
ENS - PSL Univ.
sibylle.marcotte@ens.fr
Rémi Gribonval
Univ Lyon, EnsL, UCBL,
CNRS, Inria, LIP,
remi.gribonval@inria.fr
Gabriel Peyré
CNRS, ENS - PSL Univ.
gabriel.peyre@ens.fr
Abstract
Understanding the geometric properties of gradient descent dynamics is a key
ingredient in deciphering the recent success of very large machine learning mod-
els. A striking observation is that trained over-parameterized models retain some
properties of the optimization initialization. This “implicit bias” is believed to be
responsible for some favorable properties of the trained models and could explain
their good generalization properties. The purpose of this article is threefold. First,
we rigorously expose the deﬁnition and basic properties of “conservation laws”,
that deﬁne quantities conserved during gradient ﬂows of a given model (e.g. of a
ReLU network with a given architecture) with any training data and any loss. Then
we explain how to ﬁnd the maximal number of independent conservation laws by
performing ﬁnite-dimensional algebraic manipulations on the Lie algebra generated
by the Jacobian of the model. Finally, we provide algorithms to: a) compute a
family of polynomial laws; b) compute the maximal number of (not necessarily
polynomial) independent conservation laws. We provide showcase examples that
we fully work out theoretically. Besides, applying the two algorithms conﬁrms for
a number of ReLU network architectures that all known laws are recovered by the
algorithm, and that there are no other independent laws. Such computational tools
pave the way to understanding desirable properties of optimization initialization in
large machine learning models.
1
Introduction
State-of-the-art approaches in machine learning rely on the conjunction of gradient-based optimization
with vastly “over-parameterized” architectures. A large body of empirical [30] and theoretical [5]
works suggest that, despite the ability of these models to almost interpolate the input data, they are
still able to generalize well. Analyzing the training dynamics of these models is thus crucial to gain a
better understanding of this phenomenon. Of particular interest is to understand what properties of
the initialization are preserved during the dynamics, which is often loosely referred to as being an
“implicit bias” of the training algorithm. The goal of this article is to make this statement precise, by
properly deﬁning maximal sets of such “conservation laws”, by linking these quantities to algebraic
computations (namely a Lie algebra) associated with the model parameterization (in our framework,
this parameterization is embodied by a re-parameterization mapping φ), and ﬁnally by exhibiting
algorithms to implement these computations in SageMath [29].
Over-parameterized model
Modern machine learning practitioners and researchers have found
that over-parameterized neural networks (with more parameters than training data points), which
are often trained until perfect interpolation, have impressive generalization properties [30, 5]. This
performance seemingly contradicts classical learning theory [25], and a large part of the theoretical
deep learning literature aims at explaining this puzzle. The choice of the optimization algorithm is
crucial to the model generalization performance [10, 21, 14], thus inducing an implicit bias.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Implicit bias
The terminology “implicit bias” informally refers to properties of trained models
which are induced by the optimization procedure, typically some form of regularization [22]. For
gradient descent, in simple cases such as scalar linear neural networks or two-layer networks with a
single neuron, it is actually possible to compute in closed form the implicit bias, which induces some
approximate or exact sparsity regularization [10]. Another interesting case is logistic classiﬁcation on
separable data, where the implicit bias selects the max-margin classiﬁer both for linear models [26]
and for two-layer neural networks in the mean-ﬁeld limit [8]. A key hypothesis to explicit the implicit
bias is often that the Riemannian metric associated to the over-parameterization is either of Hessian
type [10, 3], or can be somehow converted to be of Hessian type [3], which is seemingly always a very
strong constraint. For example, even for simple two-layer linear models (i.e., matrix factorization)
with more than a single hidden neuron, the Hessian type assumption does not hold, and no closed form
is known for the implicit bias [11]. The work of [17] gives conditions on the over-parameterization
for this to be possible (for instance certain Lie brackets should vanish). These conditions are (as
could be expected) stronger than those required to apply Frobenius theory, as we do in the present
work to retrieve conservation laws.
Conservation laws
Finding functions conserved during gradient ﬂow optimization of neural
networks (a continuous limit of gradient descent often used to model the optimization dynamics)
is particularly useful to better understand the ﬂow behavior. One can see conservation laws as a
“weak” form of implicit bias: to explain, among a possibly inﬁnite set of minimizers, which properties
(e.g. in terms of sparsity, low-rank, etc.) are being favored by the dynamic. If there are enough
conservation laws, one has an exact description of the dynamic (see Section 3.4), and in some cases,
one can even determine explicitly the implicit bias. Otherwise, one can still predict what properties
of the initialization are retained at convergence, and possibly leverage this knowledge. For example,
in the case of linear neural networks, certain balancedness properties are satisﬁed and provide a
class of conserved functions [24, 9, 1, 2, 15, 28, 19]. These conservation laws enable for instance
to prove the global convergence of the gradient ﬂow [4] under some assumptions. We detail these
laws in Proposition 4.1. A subset of these “balancedness” laws still holds in the case of a ReLU
activation [9], which reﬂects the rescaling invariance of these networks (see Section 4 for more
details). More generally such conservation laws bear connections with the invariances of the model
[16]: to each 1-parameter group of transformation preserving the loss, one can associate a conserved
quantity, which is in some sense analogous to Noether’s theorem [23, 12]. Similar reasoning is used
by [31] to show the inﬂuence of initialization on convergence and generalization performance of the
neural network. Our work is somehow complementary to this line of research: instead of assuming a
priori known symmetries, we directly analyze the model and give access to conservation laws using
algebraic computations. For matrix factorization as well as for certain ReLU network architectures,
this allows us to show that the conservation laws reported in the literature are complete (there are no
other independent quantities that would be preserved by all gradient ﬂows).
Contributions
We formalize the notion of a conservation law, a quantity preserved through all gradient ﬂows given a
model architecture (e.g. a ReLU neural network with prescribed layers). Our main contributions are:
• to show that for several classical losses, characterizing conservation laws for deep linear (resp.
shallow ReLU) networks boils down to analyzing a ﬁnite dimensional space of vector ﬁelds;
• to propose an algorithm (coded in SageMath) identifying polynomial conservation laws on linear
/ ReLU network architectures; it identiﬁes all known laws on selected examples;
• to formally deﬁne the maximum number of (not necessarily polynomial) independent conservation
laws and characterize it a) theoretically via Lie algebra computations; and b) practically via an
algorithm (coded in SageMath) computing this number on worked examples;
• to illustrate that in certain settings these ﬁndings allow to rewrite an over-parameterized ﬂow as
an “intrinsic” low-dimensional ﬂow;
• to highlight that the cost function associated to the training of linear and ReLU networks, shallow
or deep, with various losses (quadratic and more) fully ﬁts the proposed framework.
A consequence of our results is to show for the ﬁrst time that conservation laws commonly reported
in the literature are maximal: there is no other independent preserved quantity (see Propositions 4.3,
4.2, Corollary 4.4) and Section 4.2).
2

2
Conservation Laws for Gradient Flows
After some reminders on gradient ﬂows, we formalize the notion of conservation laws.
2.1
Gradient dynamics
We consider learning problems, where we denote xi 2 Rm the features and yi 2 Y the targets (for
regression, typically with Y = Rn) or labels (for classiﬁcation) in the case of supervised learning,
while yi can be considered constant for unsupervised/self-supervised learning. We denote X := (xi)i
and Y := (yi)i. Prediction is performed by a parametric mapping g(✓, ·) : Rm ! Rn (for instance a
neural network) which is trained by empirically minimizing over parameters ✓2 ⇥✓RD a cost
EX,Y (✓) :=
X
i
`(g(✓, xi), yi),
(1)
where ` is the loss function. In practical examples with linear or ReLU networks, ⇥is either RD or
an open set of “non-degenerate” parameters. The goal of this paper is to analyze what functions h(✓)
are preserved during the gradient ﬂow (the continuous time limit of gradient descent) of EX,Y :
.
✓(t) = −rEX,Y (✓(t)), with ✓(0) = ✓init.
(2)
A priori, one can consider different “levels” of conservation, depending whether h is conserved:
during the optimization of EX,Y for a given loss ` and a given data set (xi, yi)i; or given a loss `,
during the optimization of EX,Y for any data set (xi, yi)i. Note that using stochastic optimization
methods and discrete gradients would break the exact preservation of the conservation laws, and only
approximate conservation would hold, as remarked in [16].
2.2
Conserved functions
As they are based on gradient ﬂows, conserved functions are ﬁrst deﬁned locally.
Deﬁnition 2.1 (Conservation through a ﬂow). Consider an open subset ⌦✓⇥and a vector ﬁeld
χ 2 C1(⌦, RD). By the Cauchy-Lipschitz theorem, for each initial condition ✓init, there exists a unique
maximal solution t 2 [0, T✓init) 7! ✓(t, ✓init) of the ODE ˙✓(t) = χ(✓(t)) with ✓(0) = ✓init. A function
h : ⌦✓RD ! R is conserved on ⌦through the vector ﬁeld χ if h(✓(t, ✓init)) = h(✓init) for each
choice of ✓init 2 ⌦and every t 2 [0, T✓init). It is conserved on ⌦through a subset W ⇢C1(⌦, RD) if
h is conserved on ⌦during all ﬂows induced by all χ 2 W.
In particular, one can adapt this deﬁnition for the ﬂow induced by the cost (1).
Deﬁnition 2.2 (Conservation during the ﬂow (2) with a given dataset). Consider an open subset
⌦✓⇥and a dataset (X, Y ) such that EX,Y 2 C2(⌦, R). A function h : ⌦✓RD ! R is conserved
on ⌦during the ﬂow (2) if it is conserved through the vector ﬁeld χ(·) := rEX,Y (·).
Our goal is to study which functions are conserved during “all” ﬂows deﬁned by the ODE (2). This
in turn leads to the following deﬁnition.
Deﬁnition 2.3 (Conservation during the ﬂow (2) with “any” dataset). Consider an open subset ⌦⇢⇥
and a loss `(z, y) such that `(·, y) is C2-differentiable for all y 2 Y. A function h : ⌦✓RD ! R is
conserved on ⌦for any data set if, for each data set (X, Y ) such that g(·, xi) 2 C2(⌦, R) for each i,
the function h is conserved on ⌦during the ﬂow (2). This leads us to introduce the family of vector
ﬁelds:
W g
⌦:=
"
χ(·) : 9X, Y, 8i g(·, xi) 2 C2(⌦, R), χ = rEX,Y
 
✓C1(⌦, RD)
(3)
so that being conserved on ⌦for any dataset is the same as being conserved on ⌦through W g
⌦.
The above deﬁnitions are local and conditioned on a choice of open set of parameters ⌦⇢⇥. We are
rather interested in functions deﬁned on the whole parameter space ⇥, hence the following deﬁnition.
Deﬁnition 2.4. A function h : ⇥7! R is locally conserved on ⇥for any data set if for each open
subset ⌦✓⇥, h is conserved on ⌦for any data set.
A basic property of C1 conserved functions (which proof can be found in Appendix A) corresponds
to an “orthogonality” between their gradient and the considered vector ﬁelds.
3

Proposition 2.5. Given a subset W ⇢C1(⌦, RD), its trace at ✓2 ⌦is deﬁned as the linear space
W(✓) := span{χ(✓) : χ 2 W} ✓RD.
(4)
A function h 2 C1(⌦, R) is conserved on ⌦through W if, and only if rh(✓) ? W(✓), 8✓2 ⌦.
Therefore, combining Proposition 2.5 and Deﬁnition 2.4, the object of interest to study locally
conserved functions is the union of the traces
W g
✓:=
[ n
W g
⌦(✓) : ⌦✓⇥with ⌦a neighborhood of ✓
o
.
(5)
Corollary 2.6. A function h : ⇥7! R is locally conserved on ⇥for any data set if and only if
rh(✓) ? W g
✓for all ✓2 ⇥.
It will soon be shown (cf Theorem 2.14) that W g
✓can be rewritten as the trace W(✓) of a simple ﬁnite-
dimensional functional space W. Meanwhile, we keep the speciﬁc notation. For the moment, this set
is explicitly characterized via the following proposition (which proof can be found in Appendix B).
Proposition 2.7. Assume that for each y 2 Y the loss `(z, y) is C2-differentiable with respect to
z 2 Rn. For each ✓2 ⇥we have:
W g
✓=
span
(x,y)2X✓⇥Y
{[@✓g(✓, x)]>rz`(g(✓, x), y)}
where X✓is the set of data points x such that g(·, x) is C2-differentiable in the neighborhood of ✓.
Example 2.8. As a ﬁrst simple example, consider a two-layer linear neural network in dimension
1 (both for the input and output), with a single neuron. For such – admittedly trivial – architecture,
the parameter is ✓= (u, v) ✓R2 and the model writes g(✓, x) = uvx. One can directly check that
the function: h(u, v) = u2 −v2 is locally conserved on R2 for any data set. Indeed in that case
rh(u, v) = (2u, −2v)> ? W g
✓=
span
(x,y)2R⇥Y
{(vx, ux)>rz`(g(✓, x), y)} = R ⇥(v, u)> given that
the gradient rz`(g(✓, x), y) is an arbitrary scalar.
In this example we obtain a simple expression of W g
✓, however in general cases it is not possible to
obtain such a simple expression from Proposition 2.7. We will show that in some cases, it is possible
to express W g
✓as the trace W(✓) of a simple ﬁnite-dimensional space W (cf. Theorem 2.14).
2.3
Reparametrization
To make the mathematical analysis tractable and provide an algorithmic procedure to determine
these functions, our fundamental hypothesis is that the model g(✓, x) can be (locally) factored via a
reparametrization φ as f(φ(✓), x). We require that the model g(✓, x) satisﬁes the following central
assumption.
Assumption 2.9 (Local reparameterization). There exists d and φ 2 C2(⇥, Rd) such that: for each
parameter ✓0 in the open set ⇥✓RD, for each x 2 X such that ✓7! g(✓, x) is C2 in a neighborhood
of ✓01, there is a neighborhood ⌦of ✓0 and f(·, x) 2 C2(φ(⌦), Rn) such that
8✓2 ⌦,
g(✓, x) = f(φ(✓), x).
(6)
Note that if the model g(·, x) is smooth on ⌦then (6) is always satisﬁed with φ := id and f(·, x) :=
g(·, x), yet this trivial factorization fails to capture the existence and number of conservation laws as
studied in this paper. This suggests that, among all factorizations shaped as (6), there may be a notion
of an optimal one.
Example 2.10. (Factorization for linear neural networks) In the two-layer case, with r neurons,
denoting ✓= (U, V ) 2 Rn⇥r ⇥Rm⇥r (so that D = (n + m)r), we can factorize g(✓, x) := UV >x
by the reparametrization φ(✓) := UV > 2 Rn⇥m using f(φ, x) = φ · x. More generally for q layers,
with ✓= (U1, · · · , Uq), we can still factorize g(✓, x) := U1 · · · Uqx using φ(✓) := U1 · · · Uq and the
same f. This factorization is globally valid on ⌦= ⇥= RD since f(·, x) does not depend on ✓0.
The notion of locality of the factorization (6) is illustrated by the next example.
1i.e., x belongs to the set X✓0, as deﬁned in Proposition 2.7.
4

Example
2.11
(Factorization
for
two-layer
ReLU
networks).
Consider
g(✓, x)
=
' Pr
j=1 uk,jσ(hvj, xi + bj) + ck
)n
k=1, with σ(t)
:=
max(t, 0) the ReLU activation func-
tion and vj
2
Rm, uk,j
2
R, bj, ck
2
R.
Then, denoting ✓
=
(U, V, b, c) with
U = (uk,j)k,j =: (u1, · · · , ur) 2 Rn⇥r, V = (v1, · · · , vr) 2 Rm⇥r, b = (b1, · · · , br)> 2 Rr
and c = (c1, · · · , cn) 2 Rn (so that D = (n + m + 1)r + n), we rewrite g(✓, x) =
Pr
j=1 uj"j,x
'
v>
j x + bj
)
+ c where, given x, "j,x = 1(v>
j x + bj > 0) is piecewise constant with re-
spect to ✓. Consider ✓0 = (U 0, V 0, b0, c0) 2 RD where V 0 = (v0
1, · · · , v0
r) and b0 = (b0
1, · · · , b0
r)>.
Then the set X✓0 introduced in Proposition 2.7 is X✓0 = Rm−[j{v0
j
>x+b0
j = 0}. Let x 2 X✓0. Then
on any domain ⌦⇢RD such that ✓0 2 ⌦and "j,x(✓) := 1(v>
j x+bj > 0) is constant over ✓2 ⌦, the
model g✓(x) can be factorized by the reparametrization φ(✓) = ((ujv>
j , ujbj)r
j=1, c). In particular,
in the case without bias ((b, c) = (0, 0)), the reparametrization is deﬁned by φ(✓) = (φj)r
j=1 where
φj = φj(✓) := ujv>
j 2 Rn⇥m (here d = rmn) using f(φ, x) = P
j "j,xφjx: the reparametrization
φ(✓) contains r matrices of size m ⇥n (each of rank at most one) associated to a “local” f(·, x)
valid in a neighborhood of ✓. A similar factorization is possible for deeper ReLU networks [27], as
further discussed in the proof of Theorem 2.14 in Appendix C.
Combining Proposition 2.7 and using chain rules, we get a new characterization of W g
✓:
Proposition 2.12. Assume that the loss `(z, y) is C2-differentiable with respect to z. We recall (cf
(5)) that W g
✓:= [⌦✓⇥:⌦is open and ⌦3✓W g
⌦(✓). Under Assumption 2.9, for all ✓2 ⇥:
W g
✓= @φ(✓)>W f
φ(✓)
(7)
with @φ(✓) 2 Rd⇥D the Jacobian of φ and W f
φ(✓) :=
span
(x,y)2X✓⇥Y
{@f x(φ(✓))>rz`(g(✓, x), y)},
where f x(·) := f(·, x).
We show in Section 2.4 that, under mild assumptions on the loss `, W f
φ(✓) = Rd, so that Proposi-
tion 2.12 yields W g
✓= range(@φ(✓)>). Then by Corollary 2.6, a function h that is locally conserved
on ⇥for any data set is entirely characterized via the kernel of @φ(✓)>: @φ(✓)>rh(✓) = 0 for all
✓2 ⇥. The core of our analysis is then to analyze the (Lie algebraic) structure of range(@φ(·)>).
2.4
From conserved functions to conservation laws
For linear and ReLU networks we show in Theorem 2.14 and Proposition 2.16 that under (mild) as-
sumptions on the loss `(·, ·), being locally conserved on ⇥for any data set (according to Deﬁnition 2.4)
is the same as being conserved (according to Deﬁnition 2.1) on ⇥through the ﬁnite-dimensional
subspace
Wφ := span{rφ1(·), · · · , rφd(·)} =
n
✓7!
X
i
airφi(✓) : (a1, . . . , ad) 2 Rdo
(8)
where we write @φ(✓)> = (rφ1(✓), · · · , rφd(✓)) 2 RD⇥d, with rφi 2 C1(⇥, RD).
The following results (which proofs can be found in Appendix C) establish that in some cases, the
functions locally conserved for any data set are exactly the functions conserved through Wφ.
Lemma 2.13. Assume that the loss (z, y) 7! `(z, y) is C2-differentiable with respect to z 2 Rn and
satisﬁes the condition:
span
y2Y
{rz`(z, y)} = Rn, 8z 2 Rn.
(9)
Then for linear neural networks (resp. for two-layer ReLU networks) and all ✓2 ⇥we have
W f
φ(✓) = Rd, with the reparametrization φ from Example 2.10 and ⇥:= RD (resp. with φ from
Example 2.11 and ⇥consisting of all parameter ✓of the network such that hidden neurons are
associated to pairwise distinct “hyperplanes”, cf Appendix C for details).
Condition (9) holds for classical losses ` (e.g. quadratic/logistic losses), as shown in Lemma C.2 in
Appendix C. Note that the additional hypothesis of pairwise distinct hyperplanes for the two-layer
ReLU case is a generic hypothesis and is usual (see e.g. the notion of twin neurons in [27]). The
tools from Appendix C extend Theorem 2.14 beyond (deep) linear and shallow ReLU networks. An
open problem is whether the conclusions of Lemma 2.13 still hold for deep ReLU networks.
5

Theorem 2.14. Under the same assumptions as in Lemma 2.13, we have that for linear neural
networks, for all ✓2 ⇥:= RD:
W g
✓= Wφ(✓).
(10)
The same result holds for two-layer ReLU networks with φ from Example 2.11 and ⇥the (open) set
of all parameters ✓such that hidden neurons are associated to pairwise distinct “hyperplanes”.
This means as claimed that for linear and two-layer ReLU networks, being locally conserved on ⇥
for any data set exactly means being conserved on ⇥through the ﬁnite-dimensional functional space
Wφ ✓C1(⇥, RD). This motivates the following deﬁnition
Deﬁnition 2.15. A real-valued function h is a conservation law of φ if it is conserved through Wφ.
Proposition 2.5 yields the following intermediate result.
Proposition 2.16. h 2 C1(⌦, R) is a conservation law for φ if and only if
rh(✓) ? rφj(✓), 8 ✓2 ⌦, 8j 2 {1, . . . , d}.
Thanks to Theorem 2.14, the space Wφ deﬁned in (8) introduces a much simpler proxy to express W g
✓
as a trace of a subset of C1(⇥, RD). Moreover, when φ is C1, Wφ is a ﬁnite-dimensional space of
inﬁnitely smooth functions on ⇥, and this will be crucial in Section 4.1 to provide a tractable scheme
(i.e. operating in ﬁnite dimension) to compute the maximum number of independent conservation
laws, using the Lie algebra computations that will be described in Section 3.
Example 2.17. Revisiting Example 2.8, the function to minimize is factorized by the reparametrization
φ : (u 2 R, v 2 R) 7! uv 2 R with ✓:= (u, v). We saw that h((u, v)) := u2 −v2 is conserved: and
indeed hrh(u, v), rφ(u, v)i = 2uv −2vu = 0, 8(u, v).
In this simple example, the characterization of Proposition 2.16 gives a constructive way to ﬁnd
such a conserved function: we only need to ﬁnd a function h such that hrh(u, v), rφ(u, v)i =
hrh(u, v), (v, u)>i = 0. The situation becomes more complex in higher dimensions, since one
needs to understand the interplay between the different vector ﬁelds in Wφ.
2.5
Constructibility of some conservation laws
Observe that in Example 2.17 both the reparametrization φ and the conservation law h are polynomials,
a property that surprisingly systematically holds in all examples of interest in the paper, making it
possible to algorithmically construct some conservation laws as detailed now.
By Proposition 2.16, a function h is a conservation law if it is in the kernel of the linear operator
h 2 C1(⌦, R) 7! (✓2 ⌦7! (hrh(✓), rφi(✓)i)i=1,··· ,d). Thus, one could look for conservation laws
in a prescribed ﬁnite-dimensional space by projecting these equations in a basis (as in ﬁnite-element
methods for PDEs). Choosing the ﬁnite-dimensional subspace could be generally tricky, but for the
linear and ReLU cases all known conservation laws are actually polynomial “balancedness-type con-
ditions” [1, 2, 9], see Section 4. In these cases, the vector ﬁelds in Wφ are also polynomials (because
φ is polynomial, see Theorem C.4 and Theorem C.5 in Appendix C), hence ✓7! hrh(✓), rφi(✓)i is
a polynomial too. This allows us to compute a basis of independent polynomial conservation laws
of a given degree (to be freely chosen) for these cases, by simply focusing on the corresponding
subspace of polynomials. We coded the resulting equations in SageMath, and we found back on
selected examples (see Appendix J) all existing known conservation laws both for ReLU and linear
networks. Open-source code is available at [18].
2.6
Independent conserved functions
Having an algorithm to build conservation laws is nice, yet how can we know if we have built “all”
laws? This requires ﬁrst deﬁning a notion of a “maximal” set of functions, which would in some
sense be independent. This does not correspond to linear independence of the functions themselves
(for instance, if h is a conservation law, then so is hk for each k 2 N but this does not add any
other constraint), but rather to pointwise linear independence of their gradients. This notion of
independence is closely related to the notion of “functional independence” studied in [7, 20]. For
instance, it is shown in [20] that smooth functionally dependent functions are characterized by having
dependent gradients everywhere. This motivates the following deﬁnition.
Deﬁnition 2.18. A family of N functions (h1, · · · , hN) conserved through W ⇢C1(⌦, RD) is said
to be independent if the vectors (rh1(✓), · · · , rhN(✓)) are linearly independent for all ✓2 ⌦.
6

An immediate upper bound holds on the largest possible number N of functionally independent func-
tions h1, . . . , hN conserved through W: for ✓2 ⌦✓RD, the space span{rh1(✓), . . . , rhN(✓)} ✓
RD is of dimension N (by independence) and (by Proposition 2.5) orthogonal to W(✓). Thus, it is
necessary to have N D −dim W(✓). As we will now see, this bound can be tight under additional
assumptions on W related to Lie brackets (corresponding to the so-called Frobenius theorem). This
will in turn lead to a characterization of the maximum possible N.
3
Conservation Laws using Lie Algebra
The study of hyper-surfaces trapping the solution of ODEs is a recurring theme in control theory,
since the existence of such surfaces is the basic obstruction of controllability of such systems [6].
The basic result to study these surfaces is the so-called Frobenius theorem from differential calculus
(See Section 1.4 of [13] for a good reference for this theorem). It relates the existence of such
surfaces, and their dimensions, to some differential condition involving so-called “Lie brackets” [u, v]
between pairs of vector ﬁelds (see Section 3.1 below for a more detailed exposition of this operation).
However, in most cases of practical interest (such as for instance matrix factorization), the Frobenius
theorem is not suitable for a direct application to the space Wφ because its Lie bracket condition is
not satisﬁed. To identify the number of independent conservation laws, one needs to consider the
algebraic closure of Wφ under Lie brackets. The fundamental object of interest is thus the Lie algebra
generated by the Jacobian vector ﬁelds, that we recall next. While this is only deﬁned for vector
ﬁelds with stronger smoothness assumption, the only consequence is that φ is required to be inﬁnitely
smooth, unlike the loss `(·, y) and the model g(·, x) that can be less smooth. All concretes examples
of φ in this paper are polynomial hence indeed inﬁnitely smooth.
Notations
Given a vector subspace of inﬁnitely smooth vector ﬁelds W ✓X(⇥) := C1(⇥, RD),
where ⇥is an open subset of RD, we recall (cf Proposition 2.5) that its trace at some ✓is the subspace
W(✓) := span{χ(✓) : χ 2 W} ✓RD.
(11)
For each open subset ⌦✓⇥, we introduce the subspace of X(⌦): W|⌦:= {χ|⌦: χ 2 W}.
3.1
Background on Lie algebra
A Lie algebra A is a vector space endowed with a bilinear map [·, ·], called a Lie bracket, that veriﬁes
for all X, Y, Z 2 A: [X, X] = 0 and the Jacobi identity: [X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y ]] = 0.
For the purpose of this article, the Lie algebra of interest is the set of inﬁnitely smooth vector ﬁelds
X(⇥), endowed with the Lie bracket [·, ·] deﬁned by
[χ1, χ2] :
✓2 ⇥7! [χ1, χ2](✓) := @χ1(✓)χ2(✓) −@χ2(✓)χ1(✓),
(12)
with @χ(✓) 2 RD⇥D the jacobian of χ at ✓. The space Rn⇥n of matrices is also a Lie algebra
endowed with the Lie bracket [A, B] := AB −BA. This can be seen as a special case of (12) in the
case of linear vector ﬁelds, i.e. χ(✓) = A✓.
Generated Lie algebra
Let A be a Lie algebra and let W ⇢A be a vector subspace of A. There
exists a smallest Lie algebra that contains W. It is denoted Lie(W) and called the generated Lie
algebra of W. The following proposition [6, Deﬁnition 20] constructively characterizes Lie(W),
where for vector subspaces [W, W 0] := {[χ1, χ2] : χ1 2 W, χ2 2 W 0}, and W + W 0 = {χ1 + χ2 :
χ1 2 W, χ2 2 W 0}.
Proposition 3.1. Given any vector subspace W ✓A we have Lie(W) = S
k Wk where:
⇢
W0
:= W
Wk
:= Wk−1 + [W0, Wk−1] for k ≥1.
We will see in Section 3.2 that the number of conservation laws is characterized by the dimension of
the trace Lie(Wφ)(✓) deﬁned in (11). The following lemma (proved in Appendix D) gives a stopping
criterion to algorithmically determine this dimension (see Section 3.3 for the algorithm).
Lemma 3.2. Given ✓2 ⇥, if for a given i, dimWi+1(✓0) = dimWi(✓) for every ✓0 in a neighborhood
of ✓, then there exists a neighborhood ⌦of ✓such that Wk(✓0) = Wi(✓0) for all ✓0 2 ⌦and k ≥i,
where the Vi are deﬁned by Proposition 3.1. Thus Lie(W)(✓0) = Wi(✓0) for all ✓0 2 ⌦. In particular,
the dimension of the trace of Lie(W) is locally constant and equal to the dimension of Wi(✓).
7

3.2
Number of conservation laws
The following theorem uses the Lie algebra generated by Wφ to characterize the number of conserva-
tion laws. The proof of this result is based on two successive uses of the Frobenius theorem and can
be found in Appendix E (where we also recall Frobenius theorem for the sake of completeness).
Theorem 3.3. If dim(Lie(Wφ)(✓)) is locally constant then each ✓2 ⌦✓RD admits a neighborhood
⌦0 such that there are D−dim(Lie(Wφ)(✓)) (and no more) independent conserved functions through
Wφ|⌦0, i.e., there are D −dim(Lie(Wφ)(✓)) independent conservation laws of φ on ⌦0.
Remark 3.4. The proof of the Frobenius theorem (and therefore of our generalization Theorem 3.3)
is actually constructive. From a given φ, conservation laws are obtained in the proof by integrating in
time (i.e. solving an advection equation) the vector ﬁelds belonging to Wφ. Unfortunately, this cannot
be achieved in closed form in general, but in small dimensions, this could be carried out numerically
(to compute approximate discretized laws on a grid or approximate them using parametric functions
such as Fourier expansions or neural networks).
A fundamental aspect of Theorem 3.3 is to rely only on the dimension of the trace of the Lie algebra
associated with the ﬁnite-dimensional vector space Wφ. Yet, even if Wφ is ﬁnite-dimensional, it
might be the case that Lie(Wφ) itself remains inﬁnite-dimensional. Nevertheless, what matters is not
the dimension of Lie(Wφ), but that of its trace Lie(Wφ)(✓), which is always ﬁnite (and potentially
much smaller that dim Lie(Wφ) even when the latter is ﬁnite) and computationally tractable thanks to
Lemma 3.2 as detailed in Section 3.3. In section 4.1 we work out the example of matrix factorization,
a non-trivial case where the full Lie algebra Lie(Wφ) itself remains ﬁnite-dimensional.
Theorem 3.3 requires that the dimension of the trace at ✓of the Lie algebra is locally constant. This is
a technical assumption, which typically holds outside a set of pathological points. A good example is
once again matrix factorization, where we show in Section 4.1 that this condition holds generically.
3.3
Method and algorithm, with examples
Given a reparametrization φ for the architectures to train, to determine the number of independent con-
servation laws of φ, we leverage the characterization 3.1 to algorithmically compute dim(Lie(Wφ)(✓))
using an iterative construction of bases for the subspaces Wk starting from W0 := Wφ, and stopping
as soon as the dimension stagnates thanks to Lemma 3.2. Our open-sourced code is available at [18]
and uses SageMath. As we now show, this algorithmic principle allows to fully work out certain
settings where the stopping criterion of Lemma 3.2 is reached at the ﬁrst step (i = 0) or the second
one (i = 1). Section 4.2 also discusses its numerical use for an empirical investigation of broader
settings.
Example where the iterations of Lemma 3.2 stop at the ﬁrst step.
This corresponds to the case
where LieWφ(✓) = W1(✓) = W0(✓) := Wφ(✓) on ⌦. This is the case if and only if Wφ satisﬁes that
[χ1, χ2](✓) := @χ1(✓)χ2(✓) −@χ2(✓)χ1(✓) 2 Wφ(✓),
for all χ1, χ2 2 Wφ and all ✓2 ⌦. (13)
i.e., when Frobenius Theorem (see Theorem E.1 in Appendix E) applies directly. The ﬁrst example is
a follow-up to Example 2.11.
Example 3.5 (two-layer ReLU networks without bias). Consider ✓= (U, V ) with U 2 Rn⇥r, V 2
Rm⇥r, n, m, r ≥1 (so that D = (n + m)r), and the reparametrization φ(✓) := (uiv>
i )i=1,··· ,r 2
Rn⇥m⇥r, where U = (u1; · · · ; ur) and V = (v1; · · · ; vr). As detailed in Appendix F.1, since φ(✓)
is a collection of r rank-one n⇥m matrices, dim(Wφ(✓)) = rank@φ(✓) = (n+m−1)r is constant
on the domain ⌦such that ui, vj 6= 0, and Wφ satisﬁes (13), hence by Theorem 3.3 each ✓has a
neighborhood ⌦0 such that there exists r (and no more) independent conserved function through
Wφ|⌦0. The r known conserved functions [9] given by hi : (U, V ) 7! kuik2 −kvik2, i = 1, · · · , r,
are independent, hence they are complete.
Example where the iterations of Lemma 3.2 stop at the second step (but not the ﬁrst one).
Our
primary example is matrix factorization, as a follow-up to Example 2.10.
Example 3.6 (two-layer linear neural networks). With ✓= (U, V ), where (U 2 Rn⇥r, V 2 Rm⇥r)
the reparameterization φ(✓) := UV > 2 Rn⇥m (here d = nm) factorizes the functions minimized
during the training of linear two-layer neural networks (see Example 2.10). As shown in Appendix I,
condition (13) is not satisﬁed when r > 1 and max(n, m) > 1. Thus, the stopping criterion of
Lemma 3.2 is not satisﬁed at the ﬁrst step. However, as detailed in Proposition H.3 in Appendix H,
(Wφ)1 = (Wφ)2 = Lie(Wφ), hence the iterations of Lemma 3.2 stop at the second step.
8

We complete this example in the next section by showing (Corollary 4.4) that known conservation
laws are indeed complete. Whether known conservation laws remain valid and/or complete in this
settings and extended ones is further studied in Section 4 and Appendix F.
3.4
Application: recasting over-parameterized ﬂows as low-dimensional Riemannian ﬂows
As we now show, one striking application of Theorem 3.3 (in simple cases where dim(Wφ(✓)) =
dim(LieWφ(✓)) is constant on ⌦, i.e., rank(@φ(✓)) is constant on ⌦and Wφ satisﬁes (13)) is to fully
rewrite the high-dimensional ﬂow ✓(t) 2 RD as a low-dimensional ﬂow on z(t) := φ(✓(t)) 2 Rd,
where this ﬂow is associated with a Riemannian metric tensor M that is induced by φ and depends
on the initialization ✓init. We insist on the fact that this is only possible in very speciﬁc cases, but
this phenomenon is underlying many existing works that aim at writing in closed form the implicit
bias associated with some training dynamics (see Section 1 for some relevant literature. Our analysis
sheds some light on cases where this is possible, as shown in the next proposition.
Proposition 3.7. Assume that rank(@φ(✓)) is constant on ⌦and that Wφ satisﬁes (13). If ✓(t) 2 RD
satisﬁes the ODE (2) where ✓init 2 ⌦, then there is 0 < T ?
✓init T✓init such that z(t) := φ(✓(t)) 2 Rd
satisﬁes the ODE
.z(t) = −M(z(t), ✓init)rf(z(t))
for all t 2 [0, T ?
✓init), with z(0) = φ(✓init),
(14)
where M(z(t), ✓init) 2 Rd⇥d is a symmetric positive semi-deﬁnite matrix.
See Appendix G for a proof. Revisiting Example 3.5 leads to the following analytic example.
Example 3.8. Given the reparametrization φ : (u 2 R⇤, v 2 Rd) 7! uv 2 Rd, the variable
z := uv satisﬁes (14) with M(z, ✓init) = kzkδId + kzk−1
δ zz>, with kzkδ := δ +
p
δ2 + kzk2,
δ := 1/2(u2
init −kvinitk2).
Another analytic example is discussed in Appendix G. In light of these results, an interesting perspec-
tive is to better understand the dependance of the Riemannian metric with respect to initialization, to
possibly guide the choice of initialization for better convergence dynamics.
Note that the metric M(z, ✓init) can have a kernel. Indeed, in practice, while φ is a function from RD
to Rd, the dimensions often satisfy rank@φ(✓) < min(d, D), i.e., φ(✓) lives in a manifold of lower
dimension. The evolution (14) should then be understood as a ﬂow on this manifold. The kernel of
M(z, ✓init) is orthogonal to the tangent space at z of this manifold.
4
Conservation Laws for Linear and ReLU Neural Networks
To showcase the impact of our results, we show how they can be used to determine whether known
conservation laws for linear (resp. ReLU) neural networks are complete, and to recover these laws
algorithmically using reparametrizations φ adapted to these two settings. Concretely, we study
the conservation laws for neural networks with q layers, and either a linear or ReLU activation,
with an emphasis on q = 2. We write ✓= (U1, · · · , Uq) with Ui 2 Rni−1⇥ni the weight matrices
and we assume that ✓satisﬁes the gradient ﬂow (2). In the linear case the reparametrization is
φLin(✓) := U1 · · · Uq. For ReLU networks, we use the (polynomial) reparametrization φReLu of [27,
Deﬁnition 6], which is deﬁned for any (deep) feedforward ReLU network, with or without bias. In
the simpliﬁed setting of networks without biases it reads explicitly as:
φReLu(U1, · · · , Uq) :=
⇣
U1[:, j1]U2[j1, j2] · · · Uq−1[jq−2, jq−1]Uq[jq−1, :]
⌘
j1,··· ,jq−1
(15)
with U[i, j] the (i, j)-th entry of U. This covers φ(✓) := (ujv>
j )r
j=1 2 Rn⇥m⇥r from Example 2.11.
Some conservation laws are known for the linear case φLin [1, 2] and for the ReLu case φReLu [9].
Proposition 4.1 ( [1, 2, 9] ). If ✓:= (U1, · · · , Uq) satisﬁes the gradient ﬂow (2), then for each i =
1, · · · , q−1 the function ✓7! U >
i Ui−Ui+1U >
i+1 (resp. the function ✓7! diag
'
U >
i Ui −Ui+1U >
i+1
)
)
deﬁnes ni ⇥(ni + 1)/2 conservation laws for φLin (resp. ni conservation laws for φReLu).
Proposition 4.1 deﬁnes Pq−1
i=1 ni ⇥(ni + 1)/2 conserved functions for the linear case. In general
they are not independent, and we give below in Proposition 4.2, for the case of q = 2, the exact
9

number of independent conservation laws among these particular laws. Establishing whether there are
other (previously unknown) conservation laws is an open problem for q > 2. We already answered
negatively to this question in the two-layer ReLu case without bias (See Example 3.5). In the following
Section (Corollary 4.4), we show the same result in the linear case q = 2. Numerical computations
suggest this is still the case for deeper linear and ReLU networks as detailed in Section 4.2.
4.1
The matrix factorization case (q = 2)
To simplify the analysis when q = 2, we rewrite ✓= (U, V ) as a vertical matrix concatenation
denoted (U; V ) 2 R(n+m)⇥r, and φ(✓) = φLin(✓) = UV > 2 Rn⇥m.
How many independent conserved functions are already known?
The following proposition
reﬁnes Proposition 4.1 for q = 2 by detailing how many independent conservation laws are already
known. See Appendix H.1 for a proof.
Proposition 4.2. Consider  : ✓= (U; V ) 7! U >U −V >V 2 Rr⇥r and assume that (U; V ) has
full rank noted rk. Then the function  gives rk · (2r + 1 −rk)/2 independent conserved functions.
There exist no more independent conserved functions.
We now come to the core of the analysis,
which consists in actually computing Lie(Wφ) as well as its traces Lie(Wφ)(✓) in the matrix factor-
ization case. The crux of the analysis, which enables us to fully work out theoretically the case q = 2,
is that Wφ is composed of linear vector ﬁelds (that are explicitly characterized in Proposition H.2 in
Appendix H), the Lie bracket between two linear ﬁelds being itself linear and explicitly characterized
with skew matrices, see Proposition H.3 in Appendix H. Eventually, what we need to compute is the
dimension of the trace Lie(Wφ)(U, V ) for any (U, V ). We prove the following in Appendix H.
Proposition 4.3. If (U; V ) 2 R(n+m)⇥r has full rank noted rk, then: dim(Lie(Wφ)(U; V )) =
(n + m)r −(2r + 1 −rk)/2.
With this explicit characterization of the trace of the generated Lie algebra and Proposition 4.2, we
conclude that Proposition 4.1 has indeed exhausted the list of independent conservation laws.
Corollary 4.4. If (U; V ) has full rank, then all conserved functions are given by  : (U, V ) 7!
U >U −V >V . In particular, there exist no more independent conserved functions.
4.2
Numerical guarantees in the general case
The expressions derived in the previous section are speciﬁc to the linear case q = 2. For deeper linear
networks and for ReLU networks, the vector ﬁelds in Wφ are non-linear polynomials, and computing
Lie brackets of such ﬁelds can increase the degree, which could potentially make the generated Lie
algebra inﬁnite-dimensional. One can however use Lemma 3.2 and stop as soon as dim ((Wφ)k(✓))
stagnates. Numerically comparing this dimension with the number N of independent conserved
functions known in the literature (predicted by Proposition 4.1) on a sample of depths/widths of
small size, we empirically conﬁrmed that there are no more conservation laws than the ones already
known for deeper linear networks and for ReLU networks too (see Appendix J for details). Our code
is open-sourced and is available at [18]. It is worth mentioning again that in all tested cases φ is
polynomial, and there is a maximum set of conservation laws that are also polynomial, which are
found algorithmically (as detailed in Section 2.5).
Conclusion
In this article, we proposed a constructive program for determining the number of conservation laws.
An important avenue for future work is the consideration of more general classes of architectures,
such as deep convolutional networks, normalization, and attention layers. Note that while we focus in
this article on gradient ﬂows, our theory can be applied to any space of displacements in place of
Wφ. This could be used to study conservation laws for ﬂows with higher order time derivatives, for
instance gradient descent with momentum, by lifting the ﬂow to a higher dimensional phase space. A
limitation that warrants further study is that our theory is restricted to continuous time gradient ﬂow.
Gradient descent with ﬁnite step size, as opposed to continuous ﬂows, disrupts exact conservation.
The study of approximate conservation presents an interesting avenue for future work.
10

Acknowledgement
The work of G. Peyré was supported by the European Research Council (ERC project NORIA)
and the French government under management of Agence Nationale de la Recherche as part of the
“Investissements d’avenir” program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). The
work of R. Gribonval was partially supported by the AllegroAssai ANR project ANR-19-CHIA-0009.
We thank Thomas Bouchet for introducing us to SageMath, as well as Léo Grinsztajn for helpful
feedbacks regarding the numerics. We thank Pierre Ablin and Raphaël Barboni for comments on a
draft of this paper. We also thank the anonymous reviewers for their fruitful feedback.
References
[1] S. ARORA, N. COHEN, N. GOLOWICH, AND W. HU, A convergence analysis of gradient
descent for deep linear neural networks, arXiv preprint arXiv:1810.02281, (2018).
[2] S. ARORA, N. COHEN, AND E. HAZAN, On the optimization of deep networks: Implicit
acceleration by overparameterization, in Int. Conf. on Machine Learning, PMLR, 2018, pp. 244–
253.
[3] S. AZULAY, E. MOROSHKO, M. S. NACSON, B. E. WOODWORTH, N. SREBRO, A. GLOBER-
SON, AND D. SOUDRY, On the implicit bias of initialization shape: Beyond inﬁnitesimal mirror
descent, in Proceedings of the 38th International Conference on Machine Learning, M. Meila
and T. Zhang, eds., vol. 139 of Proceedings of Machine Learning Research, PMLR, 18–24 Jul
2021, pp. 468–477.
[4] B. BAH, H. RAUHUT, U. TERSTIEGE, AND M. WESTDICKENBERG, Learning deep linear
neural networks: Riemannian gradient ﬂows and convergence to global minimizers, Information
and Inference: A Journal of the IMA, 11 (2022), pp. 307–353.
[5] M. BELKIN, D. HSU, S. MA, AND S. MANDAL, Reconciling modern machine-learning
practice and the classical bias–variance trade-off, Proc. of the National Academy of Sciences,
116 (2019), pp. 15849–15854.
[6] B. BONNARD, M. CHYBA, AND J. ROUOT, Geometric and Numerical Optimal Control - Appli-
cation to Swimming at Low Reynolds Number and Magnetic Resonance Imaging, SpringerBriefs
in Mathematics, Springer Int. Publishing, 2018.
[7] A. B. BROWN, Functional dependence, Transactions of the American Mathematical Society,
38 (1935), pp. 379–394.
[8] L. CHIZAT AND F. BACH, Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss, in Conf. on Learning Theory, PMLR, 2020, pp. 1305–1338.
[9] S. S. DU, W. HU, AND J. D. LEE, Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced, Adv. in Neural Inf. Proc. Systems, 31 (2018).
[10] S. GUNASEKAR, J. LEE, D. SOUDRY, AND N. SREBRO, Characterizing implicit bias in terms
of optimization geometry, in Int. Conf. on Machine Learning, PMLR, 2018, pp. 1832–1841.
[11] S. GUNASEKAR, B. E. WOODWORTH, S. BHOJANAPALLI, B. NEYSHABUR, AND N. SREBRO,
Implicit regularization in matrix factorization, Adv. in Neural Inf. Proc. Systems, 30 (2017).
[12] G. GŁUCH AND R. URBANKE, Noether: The more things change, the more stay the same,
2021.
[13] A. ISIDORI, Nonlinear system control, New York: Springer Verlag, 61 (1995), pp. 225–236.
[14] Z. JI, M. DUDÍK, R. E. SCHAPIRE, AND M. TELGARSKY, Gradient descent follows the
regularization path for general losses, in Conf. on Learning Theory, PMLR, 2020, pp. 2109–
2136.
[15] Z. JI AND M. TELGARSKY, Gradient descent aligns the layers of deep linear networks, arXiv
preprint arXiv:1810.02032, (2018).
11

[16] D. KUNIN, J. SAGASTUY-BRENA, S. GANGULI, D. L. YAMINS, AND H. TANAKA, Neural
mechanics: Symmetry and broken conservation laws in deep learning dynamics, arXiv preprint
arXiv:2012.04728, (2020).
[17] Z. LI, T. WANG, J. D. LEE, AND S. ARORA, Implicit bias of gradient descent on
reparametrized models: On equivalence to mirror descent, in Advances in Neural Information
Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
eds., vol. 35, Curran Associates, Inc., 2022, pp. 34626–34640.
[18] S. MARCOTTE, R. GRIBONVAL, AND G. PEYRÉ, Code for reproducible research. Abide by
the Law and Follow the Flow: Conservation Laws for Gradient Flows, Oct. 2023.
[19] H. MIN, S. TARMOUN, R. VIDAL, AND E. MALLADA, On the explicit role of initialization
on the convergence and implicit bias of overparametrized linear networks, in Int. Conf. on
Machine Learning, PMLR, 2021, pp. 7760–7768.
[20] W. F. NEWNS, Functional dependence, The American Mathematical Monthly, 74 (1967),
pp. 911–920.
[21] B. NEYSHABUR, Implicit regularization in deep learning, arXiv preprint arXiv:1709.01953,
(2017).
[22] B. NEYSHABUR, R. TOMIOKA, AND N. SREBRO, In search of the real inductive bias: On the
role of implicit regularization in deep learning, arXiv preprint arXiv:1412.6614, (2014).
[23] E. NOETHER, Invariante variationsprobleme, Nachrichten von der Gesellschaft der Wis-
senschaften zu Göttingen, Mathematisch-Physikalische Klasse, 1918 (1918), pp. 235–257.
[24] A. M. SAXE, J. L. MCCLELLAND, AND S. GANGULI, Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks, arXiv preprint arXiv:1312.6120, (2013).
[25] S. SHALEV-SHWARTZ AND S. BEN-DAVID, Understanding machine learning: From theory to
algorithms, Cambridge university press, 2014.
[26] D. SOUDRY, E. HOFFER, M. S. NACSON, S. GUNASEKAR, AND N. SREBRO, The implicit
bias of gradient descent on separable data, The Journal of Machine Learning Research, 19
(2018), pp. 2822–2878.
[27] P. STOCK AND R. GRIBONVAL, An Embedding of ReLU Networks and an Analysis of their
Identiﬁability, Constructive Approximation, (2022). Publisher: Springer Verlag.
[28] S. TARMOUN, G. FRANCA, B. D. HAEFFELE, AND R. VIDAL, Understanding the dynamics
of gradient ﬂow in overparameterized linear models, in Int. Conf. on Machine Learning, PMLR,
2021, pp. 10153–10161.
[29] THE SAGE DEVELOPERS, SageMath, the Sage Mathematics Software System (Version 9.7),
2022. https://www.sagemath.org.
[30] C. ZHANG, S. BENGIO, M. HARDT, B. RECHT, AND O. VINYALS, Understanding deep
learning requires rethinking generalization, in Int. Conf. on Learning Representations, 2017.
[31] B. ZHAO, I. GANEV, R. WALTERS, R. YU, AND N. DEHMAMY, Symmetries, ﬂat minima, and
the conserved quantities of gradient ﬂow, arXiv preprint arXiv:2210.17216, (2022).
12

