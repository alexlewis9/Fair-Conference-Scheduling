Reinforcement Learning under Latent Dynamics:
Toward Statistical and Algorithmic Modularity
Philip Amortila∗
philipa4@illinois.edu
Dylan J. Foster
dylanfoster@microsoft.com
Nan Jiang
nanjiang@illinois.edu
Akshay Krishnamurthy
akshaykr@microsoft.com
Zakaria Mhammedi
mhammedi@google.com
Abstract
Real-world applications of reinforcement learning often involve environments
where agents operate on complex, high-dimensional observations, but the
underlying (“latent”) dynamics are comparatively simple.
However, outside
of restrictive settings such as small latent spaces, the fundamental statistical
requirements and algorithmic principles for reinforcement learning under latent
dynamics are poorly understood.
This paper addresses the question of reinforcement learning under general latent
dynamics from a statistical and algorithmic perspective. On the statistical side,
our main negative result shows that most well-studied settings for reinforcement
learning with function approximation become intractable when composed
with rich observations; we complement this with a positive result, identifying
latent pushforward coverability as a general condition that enables statistical
tractability. Algorithmically, we develop provably efficient observable-to-latent
reductions—that is, reductions that transform an arbitrary algorithm for the latent
MDP into an algorithm that can operate on rich observations—in two settings:
one where the agent has access to hindsight observations of the latent dynamics
[LADZ23], and one where the agent can estimate self-predictive latent models
[SAGHCB20]. Together, our results serve as a first step toward a unified statistical
and algorithmic theory for reinforcement learning under latent dynamics.
1
Introduction
Many application domains for reinforcement learning (RL) require the agent to operate on rich,
high-dimensional observations of the environment, such as images or text [WSD15; LFDA16;
KFPM21; NRKFG22; Bak+22; Bro+22]. However, the environment itself can often be summarized
by latent dynamics for a low-dimensional or otherwise simple latent state space. The decoupling
of latent dynamics from the complex observation process naturally suggests a modular framework
for algorithm design: first learn a representation that decodes the latent state from observations, then
apply a reinforcement learning algorithm for the latent dynamics on top of the learned representation.
This paper investigates the algorithmic and statistical foundations of this framework. We ask: Can
we take existing algorithms and sample complexity guarantees for reinforcement learning in the
latent state space and lift them to the observation space in a modular fashion?
There is a growing body of theoretical and empirical work developing algorithms that combine
representation learning and reinforcement learning to develop scalable algorithms. On the empirical
side, a plethora of representation learning objectives have been deployed to varying degrees of
success [PAED17; Tan+17; ZMCGL21; LSA20; YFK21; Lam+24; Guo+22; HPBL23], but we lack
∗The full (author-recommended) version of this paper can be found at: https://arxiv.org/pdf/2410.17904.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).

a mathematical framework to systematically compare these objectives and understand when one
might be preferred to another. On the theoretical side, all existing approaches suffer from three
primary drawbacks: (a) they are tailored to restricted classes of latent dynamics models (tabular
MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], LQR [DR21; Mha+20], or factored
MDPs [MLJL21]), limiting generality; (b) the analyses, despite focusing on restrictive settings, are
unwieldy, limiting progress in algorithm development; and (c) they are not modular, in the sense
that the representation learning procedures are specialized to specific choices of latent reinforcement
learning algorithm, limiting ease of use.
1.1
Contributions
We address the aforementioned limitations by introducing a new framework, reinforcement learning
under general latent dynamics.
Reinforcement learning under general latent dynamics (Section 2).
In our framework, the agent
performs control based on high-dimensional observations, but the dynamics of the environment are
governed by an unobserved latent state space. Following prior work (particularly the so-called Block
MDP formulation [DKJADL19]), we assume that the latent states can be uniquely decoded from
observations, but that the true decoder is unknown and must be learned. To aid in the decoding
process, we supply the learner with a class of representations that is realizable in the sense that it is
powerful enough to represent the true decoder. Our point of departure from prior theoretical works is
that we do not assume specific structure (e.g., tabular or linear dynamics) on the Markov decision
process (MDP) that governs the latent dynamics. Instead, we make the minimal assumption that the
latent dynamics belong to a base MDP class which is statistically tractable, in the sense that when
the latent states are directly observed there exists some reinforcement learning algorithm with low
sample complexity that is capable of learning a near-optimal policy for every MDP in the class. We
take the first steps toward building a unified and modular theory for reinforcement learning in this
setting.
Contributions: Statistical modularity (Section 3).
A central consideration for reinforcement
learning under latent dynamics is that representation learning and exploration must be intertwined:
an accurate decoder is required to explore the latent state space, but exploration is required to learn
an accurate decoder. To develop provable sample complexity guarantees, one must prevent errors
from compounding during this interleaving process, a challenging statistical problem which prior
work addresses through strong structural assumptions on the base MDP [KAL16; DKJADL19;
MHKL20; ZSUWAS22; MFR23; DR21; Mha+20; MLJL21]. For the general latent-dynamics setting
we consider, it is unclear whether similar techniques can be applied, or whether the setting is even
statistically tractable, ignoring computational considerations. Thus, our first contribution considers
the question of statistical modularity:2
If a base MDP class is tractable when observed directly, is the corresponding latent-dynamics
problem tractable?
Statistical modularity adopts a minimax perspective by assuming that the base MDP lies in a given
class, and demands that the sample complexity of the latent-dynamics setting is controlled by a
natural bound on the sample complexity of the base MDP class. We show, perhaps surprisingly,
that most well-studied reinforcement learning settings involving function approximation [RVR13;
JKALS17; SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21;
FKQR21] do not admit statistical modularity (Theorem 3.1). In other words, statistical tractability
of an MDP class does not extend to the latent-dynamics setting. We complement these negative
findings with a positive result, identifying pushforward coverability as a general structural condition
on the latent dynamics that enables sample efficiency (Theorem 3.2).
Contributions: Algorithmic modularity (Section 4).
Beyond developing a modular understanding
of the statistical landscape, we investigate modular algorithm design principles for RL under general
latent dynamics. Specifically, we consider the question of observable-to-latent reductions, whereby
RL under latent dynamics can be reduced to the simpler problem of RL with latent states directly
observed:
Can we generically lift algorithms for a base MDP class to solve the corresponding latent-dynamics
problem?
2This question and associated definitions are restated formally in Section 3.1.
2

This property, which we refer to as algorithmic modularity, enables modular, greatly simplified
algorithm design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the
corresponding latent-dynamics problem. Algorithmic modularity is a stronger property than mere
statistical modularity, and thus is subject to our statistical lower bound. Accordingly, we consider two
settings that sidestep the lower bound through additional feedback and modeling assumptions. Our
first algorithmic result considers hindsight observability [LADZ23], where latent states are revealed
during training, but not at deployment (Theorem 4.1). Our second considers stronger function
approximation conditions that enable the estimation of self-predictive latent models [SAGHCB20]
through representation learning (Theorem A.1). Both results are fully modular: they transform
any sample-efficient algorithm for the base MDP class into a sample-efficient algorithm for the
latent-dynamics setting. Thus, they constitute the first general-purpose algorithms for RL under
latent dynamics.
Together, we believe our results can serve as a foundation for further development of practical,
general-purpose algorithms for RL under latent dynamics. To this end, we highlight a number of
fascinating and challenging open problems for future research (Section 5).
2
Reinforcement Learning under General Latent Dynamics
In this section we formally introduce our framework, reinforcement learning under general latent
dynamics.
MDP preliminaries.
We consider an episodic finite-horizon online reinforcement learn-
ing setting.
With H denoting the horizon, a Markov decision process (MDP) M ⋆
=

X, A, {P ⋆
h}H
h=0, {R⋆
h}H
h=1, H
	
consists of a state space X, an action space A, a reward distribution
R⋆
h : X × A →∆([0, 1]) (with expectation r⋆
h(x, a)), and a transition kernel P ⋆
h : X × A →∆(X)
(with the convention that P ⋆
0 (· | ∅) is the initial state distribution).3
At the beginning of the episode, the learner selects a randomized, non-stationary policy π =
(π1, . . . , πH), where πh : X →∆(A); we let Πrns denote the set of all such policies. The episode
evolves through the following process; beginning from x1 ∼P ⋆
0 (· | ∅), the MDP generates a trajec-
tory (x1, a1, r1), . . . , (xH, aH, rH) via ah ∼πh(xh), rh ∼R⋆
h(xh, ah), and xh+1 ∼P ⋆
h(· | xh, ah).
We let PM⋆,π denote the law under this process, and let E
M⋆,π denote the corresponding expectation,
and likewise let PM,π and E
M,π denote the analogous laws and expectations in another MDP M. We
assume that PH
h=1 rh ∈[0, 1] almost surely for any trajectory in M ⋆.
For a policy π and MDP M, the expected reward for π is given by J M(π) := EM,πPH
h=1 rh

,
and the value functions are given by
V
M,π
h
(x) := EM,πPH
h′=h rh′
| xh
= x

, and
Q
M,π
h
(x, a) := EM,πPH
h′=h rh′ | xh = x, ah = a

.
We let πM = {πM,h}H
h=1 denote an
optimal deterministic policy of M, which maximizes V M,π (over π) at all states (and in particular,
satisfies πM ∈arg maxπ∈Πrns J M(π)), and write QM,⋆:= QM,πM . For f : X × A →R, we
write πf(x) := arg maxa f(x, a) as well as Vf(x) = maxa f(x, a).
For MDP M, horizon
h ∈[H], and g : X
→R, we let T M
h
denote the Bellman (optimality) operator defined
via
[T M
h g](x, a) = E
M[rh + g(xh+1) | xh = x, ah = a], and we overload notation by letting
[T M
h f](x, a) = [T M
h Vf](x, a).
We also let T
M,π
h
denote the Bellman evaluation operator
defined via [T
M,π
h
f](x, a) = E
M
rh + Ea′∼πh+1(·|xh+1)[f(xh+1, a′)] | xh = x, ah = a

, for any
π ∈Πrns. We define the occupancy measures for layer h via
d
M,π
h
(x) = PM,π[xh = x] and
d
M,π
h
(x, a) = PM,π[xh = x, ah = a].
Online reinforcement learning.
In online reinforcement learning, the learning algorithm ALG
repeatedly interacts with an unknown MDP M ⋆by executing a policy and observing the resulting
trajectory. After T rounds of interaction, the algorithm outputs a final policy bπ, with the goal of
minimizing their risk, defined via
Risk(T, ALG, M ⋆) := J
M⋆(πM⋆) −J
M⋆(bπ).
(1)
3To simplify presentation, we assume that X and A are countable; our results extend to handle continuous
variables with an appropriate measure-theoretic treatment.
3

Framework: Reinforcement learning under general latent dynamics.
In reinforcement learning
under general latent dynamics, we consider MDPs M ⋆where the dynamics are governed by the evolu-
tion of an unobserved latent state sh, while the agent observes and acts on observations xh generated
from these latent states. Formally, a latent-dynamics MDP consists of two ingredients: a base MDP
Mlat = {S, A, {Plat,h}H
h=0, {Rlat,h}H
h=1, H} defined over a latent state space S, and a decodable
emission process ψ := {ψh : S →∆(X)}H
h=1, which maps each latent state to a distribution over
observations. The former is an arbitrary MDP defined over S, while the latter is defined as follows.
Definition 2.1 (Emission process). An emission process is any function ψ := {ψh : S →∆(X)}H
h=1,
and is said to be decodable if
∀h, ∀s′ ̸= s ∈S :
supp ψh(s) ∩supp ψh(s′) = ∅.
.
(2)
When ψ = {ψh}H
h=1 is decodable, we let ψ−1 := {ψ−1
h
: X →S}H
h=1 denote the associated decoder.
With this, we can formally introduce the notion of a latent-dynamics MDP.
Definition
2.2
(Latent-dynamics
MDP).
For
a
base
MDP
Mlat
=
{S, A, {Plat,h}H
h=0, {Rlat,h}H
h=1, H}, and a decodable emission process ψ, the latent-dynamics
MDP ⟪Mlat, ψ⟫:=

X, A, {Pobs,h}H
h=0, {Robs,h}H
h=1, H
	
is defined as the MDP where the latent
dynamics evolve based on the agent’s action ah ∈A via the process sh+1 ∼Plat,h(sh, ah) and
rh ∼Rlat,h(sh, ah). The latent state is not observed directly, and instead the agent observes
xh ∈X generated by the emission process xh ∼ψh+1(sh).4
Note that under these dynamics, the decoder ψ−1 associated with ψ ensures that ψ−1
h (xh) = sh
almost surely for all h ∈[H]. That is, the latent states can be uniquely decoded from the observations.
To emphasize the distinction between the latent-dynamics MDP ⟪Mlat, ψ⟫(which operates on the
observable state space X) and the MDP Mlat (which operates on the latent state space S), we refer
to the latter as a base MDP rather than, for example, a “latent MDP”, and apply a similar convention
to other latent objects whenever possible.5
Departing from prior work, we do not place any inherent restrictions on the base MDP, and in
particular do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand—in
a unified fashion—what structural assumptions on the base MDP Mlat are required to enable
learnability under latent dynamics. To this end, it will be useful to considers specific classes (i.e.,
subsets) of base MDPs Mlat and the classes of latent-dynamics MDPs they induce.
Definition 2.3 (Latent-dynamics MDP class). Given a set of base MDPs Mlat and a set of decoders
Φ ⊂{X →S}, we let
⟪Mlat, Φ⟫:= {⟪Mlat, ψ⟫: Mlat ∈Mlat, ψ is decodable, ψ−1 ∈Φ}
(3)
denote the class of induced latent-dynamics MDPs.
Stated another way, ⟪Mlat, Φ⟫is the set of all latent-dynamics MDPs ⟪Mlat, ψ⟫where (i) the
base MDP Mlat lies in Mlat, and (ii), the emission process ψ is decodable, with the corresponding
decoder belonging to Φ. The class Mlat represents our prior knowledge about the underlying
MDP Mlat; concrete classes considered in prior work include tabular MDPs [KAL16; DKJADL19;
MHKL20; ZSUWAS22; MFR23], linear dynamical systems [DMRY20; DR21; Mha+20], and
factored MDPs [MLJL21].
In particular, the class Mlat may itself warrant using function
approximation. At the same time, the class Φ represents our prior knowledge or inductive bias
about the emission process, enabling representation learning. In what follows, we investigate
what conditions on Mlat make the induced class ⟪Mlat, Φ⟫tractable, both statistically (statistical
modularity; Section 3) and via reduction (algorithmic modularity; Section 4).
3
Statistical Modularity: Positive and Negative Results
This section presents our main statistical results. We begin by formally defining the notion of
statistical modularity introduced in Section 1, present our main impossibility result (lower bound) and
its implications (Section 3.2), then give positive results for the general class of pushforward-coverable
MDPs (Section 3.3).
4Equivalently the dynamics can be described via Robs,h(xh, ah) = Rlat(ψ−1
h (xh), ah) and Pobs,h(xh+1 |
xh, ah) = Plat,h(ψ−1
h+1(xh+1) | ψ−1
h (xh), ah) · ψh+1(xh+1 | ψ−1
h+1(xh+1)).
5For example, in Section 4 we will be concerned with reductions from observation-space algorithms to “base
algorithms” that operate on the latent state space.
4

3.1
Statistical modularity: A formal definition
We first define the statistical complexity for a MDP class (or, model class) M.
Definition 3.1 (Statistical complexity). We say that an MDP class M can be learned up to ε-
optimality using comp(M, ε, δ) samples if there exists an algorithm ALG which, for every M ∈M,
attains
Risk(T, ALG, M) ≤ε
with probability at least 1 −δ, after T = comp(M, ε, δ) rounds of online interaction in M.
We say that a base MDP class Mlat admits statistically modularity if, for any decoder class Φ, the
induced latent-dynamics MDP class ⟪Mlat, Φ⟫can be learned with statistical complexity that is
polynomial in: (i) the statistical complexity for the base class, and (ii) the capacity of the decoder class.
Definition 3.2 (Statistical modularity). We say the MDP class Mlat is statistically modular under
complexity comp(Mlat, ε, δ) if, for every decoder class Φ, we have
comp(⟪Mlat, Φ⟫, ε, δ) = poly(comp(Mlat, ε, δ), log|Φ|).
(4)
We say that Mlat admits strong statistical modularity if Eq. (4) holds when comp(Mlat, ε, δ) is the
minimax sample complexity for Mlat.
In the sequel, we examine well-studied MDP classes Mlat (e.g., those which admit low Bellman
rank [JKALS17]) and choose comp(Mlat, ε, δ) based on natural upper bounds on their optimal
sample complexity; in this case we will simply say they are (or are not) statistical modular, leaving
the complexity upper bound comp implicit. Following prior work [KAL16; DKJADL19; MHKL20;
ZSUWAS22; MFR23; DR21; Mha+20; MLJL21], we use log|Φ| as a proxy for the statistical
complexity of supervised learning with the decoder class Φ.6
The two most notable examples of statistical modularity covered by prior work are: (i) taking
Mlat as the set of tabular MDPs admits strong statistical modularity [DKJADL19; MHKL20;
MFR23], and (ii) taking Mlat as the set of linear MDPs admits statistical modularity with complexity
poly(d, H, |A|, ε−1, log
 δ−1
) [AKKS20; UZS22; MCKJA24; MBFR23].7 Interestingly, the latter
does not admit strong statistical modularity, because the optimal rate for Mlat does not scale with
|A|, but the rate for ⟪Mlat, Φ⟫necessarily does [LS20; HLSW21]. The results of Mhammedi et al.;
Misra et al.; Song et al. [Mha+20; MLJL21; SWFK24] can also be viewed as instances of statistical
modularity for other base MDP classes.
3.2
Lower bounds: Impossibility of statistical modularity
Our main result in this section is to show that for most MDP classes Mlat considered in the
literature on sample-efficient reinforcement learning with function approximation [RVR13; JKALS17;
SJKAL19; MJTS20; AJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21; FKQR21],
statistical modularity (under the natural complexity upper bound for the class of interest) is impossible.
Our central technical result is the following lower bound, which shows that statistical modularity
can be impossible even when the base MDP is known to the learner a-priori. The lower bound is a
significant generalization of the result from Song et al. [SWFK24]; we first state the lower bound,
then discuss implications.
Theorem 3.1 (Impossibility of statistical modularity). For every N ≥4, there exists a decoder class
Φ with |Φ| = N and a family of base MDPs Mlat satisfying (i) |Mlat| = 1, (ii) H ≤O(log(N)),
(iii) |S| = |X| ≤N 2, (iv) |A| = 2, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0.
2. For an absolute constant c > 0, comp(⟪Mlat, Φ⟫, c, c) ≥Ω(N/ log(N)).
In other words, even when the base dynamics are fully known, strong statistical modularity
(in this case, poly(log|Φ|) complexity) is impossible; any algorithm will require at least
min{
√
S, 2Ω(H)/H, |Φ|/log|Φ|} episodes to learn a near-optimal policy for a latent-dynamics MDP
⟪Mlat, ψ⟫∈⟪Mlat, Φ⟫.
6Our main results easily extend to infinite classes through standard arguments.
7In the latter case, the latent-dynamics class ⟪Mlat, Φ⟫may be seen to be a set of low-rank MDPs (that
is, linear MDPs with unknown features), so that low-rank MDP algorithms may be applied directly on the
observations (Appendix E.2).
5

Base MDP class Mlat
Statistical
Modularity?
Tabular
✓
Contextual Bandits
✓
Low-Rank MDP
✓
Known Deterministic MDP (|Mlat| = 1)
✓
Low State Occupancy (∀π : S →∆(A))
✓
Model Class + Pushforward Coverability
✓
Linear CB/MDP
✗⋆
Model Class + Coverability (∀πM : M ∈M)
✗
Known Stochastic MDP (|Mlat| = 1)
✗
Bellman Rank (Q-type or V -type)
✗
Eluder Dimension + Bellman Completeness
✗
Q⋆-Irrelevant State Abstraction
✗
Linear Mixture MDP
✗
Linear Q⋆/V ⋆
✗
Low State/State-Action Occupancy (∀πM : M ∈M)
✗
Bisimulation
?
Low State-Action Occupancy (∀π : S →∆(A))
?⋆
Model Class + Coverability (∀π : S →∆(A))
?
Figure 1: Summary of statistical
modularity (SM) results.
✓: SM is possible for a nat-
ural
choice
of
comp(·)
(e.g.,
poly(|S|, |A|, H, ε−1, log
 δ−1
)
for tabular MDPs).
✗: SM is not possible with natural
choices of comp(·).
?: open.
⋆: SM is possible if willing to pay
for (suboptimal) |A| complexity.
See Appendix E.2 for precise
descriptions of each setting and
our choices for their complexities.
Intuition for lower bound.
The intuition behind the lower bound in Theorem 3.1 is as follows: the
unobserved latent state space consists of N = |Φ| binary trees (indexed from 1 to N), each with N
leaf nodes. The starting distribution is uniform over the roots of the N trees, and the agent receives a
reward of 1 if and only if they navigate to the leaf node that corresponds to the index of their current
tree. The observed state space is identical to the latent state space, but the emission process shifts the
index of the tree by an amount which is unknown to the agent. Despite the base MDP being known
and the decoder class satisfying realizability, the agent requires near-exhaustive search to identify the
value of the shift and recover a near-optimal policy.
A taxonomy of statistical modularity.
As a corollary, we prove that many (but not all) well-studied
function approximation settings do not admit statistical modularity by embedding them into the
lower bound construction of Theorem 3.1 (as well as a variant of the result, Theorem E.1). Our
results are summarized in Figure 1. Our impossibility results highlight the following phenomenon:
many MDP classes Mlat that place structural assumptions via the value functions (e.g., MDPs
with linear-Q⋆/V ⋆[Du+21] or MDPs with a Bellman complete value function class of bounded
eluder dimension [JLM21; WSY20]) become intractable under latent dynamics. Intuitively, this
is because it is not possible to take advantage of structure in value functions without learning a
good representation, and, simultaneously, these assumptions are too weak by themselves to enable
learning such a representation. Meanwhile, MDP classes Mlat that place structural assumptions on
the transition distribution (e.g., MDPs with low state occupancy complexity [Du+21] or low-rank
MDPs [AKKS20]) are sometimes (but not always) tractable under latent dynamics.8
We point to Appendix E.2 for background on all the settings in Figure 1 and proofs that they are (or
are not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP
classes of Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP
Mlat, and we expect that many other base MDP classes can similarly be shown to be intractable.
However, proving the positive results in Figure 1 requires establishing several new results showing
that certain base classes are tractable under latent dynamics; most notably, we next discuss the case
of pushforward coverability.
3.3
Upper bounds: Pushforward-coverable MDPs are statistically modular
Our main postive result concerning statistical modularity is to highlight pushforward coverability
[XJ21; AFK24; MFR24]—a strengthened version of the coverability parameter introduced in Xie
et al. [XFBJK23]—as a general structural parameter that enables sample-efficient reinforcement
learning under latent dynamics.
8If one is willing to pay for suboptimal |A| factors, then more (but not all) classes become statistically
tractable (e.g., linear MDPs [JYWJ20] and MDPs with low state-action occupancy [Du+21]).
6

Definition 3.3 (Pushforward coverability). The pushforward coverability coefficient Cpush for an
MDP Mlat with transition kernel Plat is defined by
Cpush(Mlat) = max
h∈[H]
inf
µ∈∆(S)
sup
(s,a,s′)∈S×A×S
Plat,h−1(s′ | s, a)
µ(s′)
.
(5)
Concrete examples [AFK24; MFR24] include: (i) tabular MDPs Mlat admit Cpush(Mlat) ≤|S|; and
(ii) Low-Rank MDPs Mlat (with or without known features) in dimension d admit Cpush(Mlat) ≤d.
Further examples include analytically sparse Low-Rank MDPs [GMR24] and Exogenous Block
MDPs with weakly correlated noise [MFR24]. Our main result is as follows.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let Mlat be a base MDP
class such that each Mlat ∈Mlat has pushforward coverability bounded by Cpush(Mlat) ≤Cpush.
Then, for any decoder class Φ, we have:
1. comp(Mlat, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, ε−1, log
 δ−1
), and
2. comp(⟪Mlat, Φ⟫, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, log|Φ|, ε−1, log
 δ−1
, log log|S|).
Theorem 3.2 shows that, modulo a term that is doubly-logarithmic in |S|, latent pushforward cover-
ability enables statistical modularity. That is, when the base (latent) dynamics satisfy pushforward
coverability, there exists an algorithm for the latent-dynamics setting which scales with the statistical
complexity of the base MDP class and log|Φ|. We suspect that the additional log log|S| factor is not
essential and can be removed with a more sophisticated analysis. We note that the complexity comp
chosen above is not the minimax complexity for Mlat, since every set of pushforward coverable
MDPs is also a set of coverable MDPs with a potentially smaller coverability parameter [AFK24].
Let us provide some intuition for this result. We firstly note that when M ⋆
lat has pushforward cover-
ability parameter Cpush, it holds that for any emission process ψ⋆, the observation-level MDP M ⋆
obs :=
⟪M ⋆
lat, ψ⋆⟫also satisfies pushforward coverability with the same parameter Cpush (Lemma D.5). Yet,
despite access to realizable base MDP class Mlat and decoder class Φ, it is unclear whether the latent-
dynamics MDP M ⋆
obs satisfies any of the observation-level function approximation conditions required
by existing approaches that provide sample complexity guarantees under pushforward coverability.
In particular, known algorithms for this setting either require a Bellman-complete value function
class [XFBJK23], a class realizing certain density ratios [AFJSX24; AFK24], or a realizable model
class [AFK24], and it is highly nontrivial to construct these for the latent-dynamics MDP M ⋆
obs =
⟪M ⋆
lat, ψ⋆⟫given only the base MDP class Mlat and the decoder class Φ. Intuitively, this is because
the former observation-level function approximation classes capture properties of the observation-
level dynamics which cannot be obtained without some knowledge of the emission process.
Our main technical contribution is to establish a new structural property for pushforward-coverable
MDPs (Lemma F.1): low-dimensional linear embeddings of their latent models can approximate the
Bellman updates for an arbitrary set of test functions (as long as the set is not too large). We use
this property to construct low-dimensional linear features that can approximate Bellman backups
in observation-space, allowing us to (approximately) satisfy the Bellman completeness assumption
required to apply GOLF [JLM21] to the latent-dynamics MDP. A fascinating open question is whether
a similar approach can be used to establish that standard (as opposed to pushforward) coverable
MDPs are statistically modular, which would encompass all other known positive cases of statistical
modularity (cf. Figure 1). We refer interested readers to a more detailed technical overview in
Appendix F.1, as well as the full proof in Appendix F.2.
4
Algorithmic Modularity
We now turn our attention to algorithmic modularity. Specifically, we aim for observable-to-latent
reductions, whereby—via representation learning—RL under latent dynamics can be efficiently
reduced to the simpler problem of RL with latent states directly observed. Since algorithmic
modularity is a stronger property than statistical modularity, we sidestep the previous lower bounds in
Section 3 through additional feedback and modeling assumptions. Our main result for this section is a
new meta-algorithm, O2L, which, under these assumptions (and when equipped with an appropriately
designed representation learning oracle), acts as a universal reduction in the sense that, whenever the
representation learning oracle has low risk, the reduction transforms any sample-efficient algorithm
for any base MDP class into a sample-efficient algorithm for the induced latent-dynamics MDP class.
7

Algorithm 1 O2L: Observable-to-Latent Reduction
1: input: Epochs T, episodes K, decoder set Φ, rep. learning oracle REPLEARN, base alg. ALGlat.
2: for t = 1, 2, · · · , T do
3:
REPLEARN chooses a representation bϕ(t) : X →S ∈Φ based on data collected so far.
4:
Initialize new instance of ALGlat.
5:
for k = 1, 2, · · · , K do
// ALGlat plays K rounds in the “bϕ(t)-compressed dynamics.”
6:
ALGlat chooses policy π
(t,k)
lat : S × [H] →∆(A).
7:
Deploy πlat ◦bϕ(t) to collect trajectory {x
(t,k)
h
, a
(t,k)
h
, r
(t,k)
h
}H
h=1.
8:
Update ALGlat with compressed trajectory {bϕ
(t)
h (x
(t,k)
h
), a
(t,k)
h
, r
(t,k)
h
}H
h=1.
9:
end for
10:
ALGlat returns final policy bπ(t) : S ×[H] →∆(A), deploy bπ(t)◦bϕ(t) to collect one trajectory.
11: end for
12: return bπ = Unif(bπ(1) ◦bϕ(1), . . . , bπ(T ) ◦bϕ(T )).
Setup and O2L meta-algorithm.
For the results in this section, we denote the (unknown) latent-
dynamics MDP of interest by M ⋆
obs := ⟪M ⋆
lat, ψ⋆⟫, and use ϕ⋆:= (ψ⋆)−1 to denote the true decoder.
The O2L meta-algorithm (Algorithm 1) learns a near-optimal policy for M ⋆
obs by alternating between
performing representation learning and executing a black-box “base” RL algorithm (designed for the
base MDP) on the learned representation; this approach is inspired by empirical methods that blend
representation learning and RL in the latent space (e.g., [GKBNB19; SAGHCB20; Ni+24]).
Concretely, the algorithm takes as input a representation learning oracle REPLEARN and a base RL
algorithm ALGlat that operates in the latent space. In each epoch t ∈[T], REPLEARN produces a new
representation bϕ(t) : X →S based on data observed so far (potentially using additional side informa-
tion, which we will elaborate on in the sequel). Then, the reduction invokes ALGlat, using bϕ(t) to sim-
ulate access to the true latent states. In particular, ALGlat runs for K episodes, where at each episode
k: (i) ALGlat produces a latent policy πlat
(t,k) : S×[H] →∆(A), (ii) the latent policy is transformed
into an observation-level policy via composition with bϕ(t), i.e. πlat
(t,k) ◦bϕ(t), which is then deployed
to produce a trajectory {x
(t,k)
h
, a
(t,k)
h
, r
(t,k)
h
}H
h=1, and (iii) the trajectory is compressed through bϕ(t)
and used to update ALGlat via {bϕ
(t)
h (x
(t,k)
h
), a
(t,k)
h
, r
(t,k)
h
}H
h=1 (cf. Line 8 of Algorithm 1).9 After the
K rounds conclude, ALGlat produces a final latent policy bπ
(t)
lat : S × [H] →∆(A). The final policy
bπ chosen by the O2L algorithm is a uniform mixture of bπ
(t)
lat ◦bϕ(t) over all the epochs.
The central assumption behind O2L is that the base algorithm ALGlat can achieve low-risk in
the underlying base MDP M ⋆
lat if given access to the true latent states sh = ϕ⋆(xh). Beyond this
assumption, we require that the representation learning oracle REPLEARN can learn a sufficiently
high-quality representation. In our applications, this will be made possible by assuming access
to a realizable decoder class Φ and two distinct assumptions: hindsight observability (Section 4.1)
and conditions enabling self-predictive representation learning (Section 4.2). We will show that
under these conditions, we can instantiate a representation learning oracle such that O2L inherits
the sample complexity guarantee for ALGlat, thereby achieving algorithmic modularity.
4.1
Algorithmic modularity via hindsight observability
Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of hindsight
observability, which has garnered recent interest in the context of POMDPs [LADZ23; GCWXWB24;
SLS23; LXJZV24]. Here, we assume that at training time (but not during deployment), the algorithm
has access to additional feedback in the form of the true latent states, which are revealed at the end of
each episode.
Assumption 4.1 (Hindsight Observability [LADZ23]). The latent states (ϕ⋆
1(x1), . . . , ϕ⋆
H(xH)) are
revealed to the learner after each episode (x1, a1, r1, . . . , xH, aH, rH) concludes.
We emphasize that in the hindsight observability framework, the learner must still execute observation-
space policies πobs : X ×[H] →∆(A), as the latent states are only revealed at the end of each episode.
Under hindsight observability, we can instantiate the representation learning oracle in O2L so that the
9Note that, if bϕ is inaccurate, the compressed trajectory cannot necessarily be viewed as being generated by a
latent MDP, and must instead be viewed as coming from a Partially Observed MDP (Appendix I.1.1).
8

reduction achieves low risk for any choice of black-box base algorithm ALGlat. In particular, we make
use of online classification oracles, which use the revealed latent states to achieve low classification
loss with respect to ϕ⋆under adaptively generated data. We first state a guarantee based on generic
classification oracles, then instantiate it to give a concrete end-to-end sample complexity bound.
Formally, at each step t, the online classification oracle, denoted via REPclass, is given the
states and hindsight observations collected so far and produces a deterministic estimate bϕ(t) =
REPclass({x
(i)
h , ϕ⋆
h(x
(i)
h )}i<t,h≤H) for the true decoder ϕ⋆. We measure the regret of the oracle via
the 0/1 loss for classification:
Regclass(T) :=
T
X
t=1
H
X
h=1
Eπ(t)∼p(t) Eπ(t)h
I
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
	i
,
where p(t) represents a randomization distribution over the policy π(t). Our reduction succeeds under
the assumption that the oracle has low expected regret.
Assumption 4.2. For any (possibly adaptive) sequence π(t), with π(t) ∼p(t), the online classification
oracle REPclass has expected regret bounded by
E[Regclass(T)] ≤Estclass(T),
where Estclass(T) is a known upper bound.
We apply such an oracle within O2L as follows: at the end of each iteration t ∈[T] in
O2L, we sample k ∼[K] uniformly, and update the classification oracle with the trajectory
(x
(t,k)
1
, a
(t,k)
1
, r
(t,k)
1
), . . . , (x
(t,k)
H
a
(t,k)
H
, r
(t,k)
H
); see the proof of Theorem 4.1 for details.
We let
Riskobs(TK) denote the risk of the O2L reduction when run for T epochs of K episodes, and
we let Risk⋆(K) := E[Risk(K, ALGlat, M ⋆
lat)] denote the expected risk of ALGlat when executed
on M ⋆
lat with access to the true latent states sh = ϕ⋆(xh) for K episodes.
Theorem 4.1 (Risk bound for O2L under hindsight observability). Let ALGlat be a base algorithm
with base risk Risk⋆(K), and REPclass a representation learning oracle satisfying Assumption 4.2.
Then Algorithm 1, with inputs T, K, Φ, REPclass, and ALGlat, has expected risk
E[Riskobs(TK)] ≤Risk⋆(K) + 2K
T Estclass(T).
This result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base
algorithm achieves sublinear risk Risk⋆(K) given access to the true latent states, and (ii) the classifi-
cation oracle achieves sublinear regret Estclass(T). Notably, the result is fully modular, meaning we
require no explicit conditions on the latent dynamics or the base algorithm, and is computationally
efficient whenever the base algorithm and classification oracle are efficient.
To make Theorem 4.1 concrete, we next provide a representation learning oracle (EXPWEIGHTS.DR;
Algorithm 3 in Appendix G.1) based on a derandomization of the classical exponential weights
mechanism, which satisfies Assumption 4.2 with Estclass ≲H log |Φ| whenever it has access to a
class Φ that satisfies decoder realizability.
Lemma 4.1 (Online classification via EXPWEIGHTS.DR). Under decoder realizability (ϕ⋆∈Φ),
EXPWEIGHTS.DR (Algorithm 3) satisfies Assumption 4.2 with10
Estclass(T) = e
O(H log|Φ|).
Instantiating Theorem 4.1 with the above representation learning oracle, we obtain the following
algorithmic modularity result.
Corollary 4.1 (Algorithmic modularity under hindsight observability). For any base algorithm
ALGlat, under decoder realizability (ϕ⋆∈Φ), O2L with inputs T, K, Φ, EXPWEIGHTS.DR, and
ALGlat achieves
E[Riskobs(TK)] ≲Risk⋆(K) + HK log|Φ|
T
.
Consequently, for any ALGlat, setting T ≈KH log|Φ|/Risk⋆(K) achieves E[Riskobs(TK)] ≲
Risk⋆(K) with a number of trajectories TK = e
O(K2H log|Φ|/Risk⋆(K)).
10In this section, the notations e
O, ≈, and ≲ignore only constants and logarithmic factors of H.
9

Beyond achieving algorithmic modularity, this result shows that under hindsight observability, we
can achieve strong statistical modularity (modulo possible H factors) for every base MDP class
Mlat, an important result in its own right.11 As an example, suppose that Risk⋆(K) = O(K−1/2),
which is satisfied by many standard algorithms of interest [JKALS17; JYWJ20; JLM21; FKQR21].
Then, setting T according to Corollary 4.1 obtains an expected risk bound of ε using O(H log|Φ|/ε5)
trajectories.
Remark 4.1 (Online versus offline oracles). Theorem 4.1 critically uses that assumption that REPclass
satisfies an online classification error bound to handle the fact that data is generated adaptively based
on the estimators bϕ(1), . . . , bϕ(T ) it produces, which is by now a relatively standard technique in the
design of interactive decision making algorithms [FR20; FKQR21; FR23]. We note that under
coverability and other exploration conditions, online oracles for classification can be directly obtained
from offline (i.e. supervised) classification oracles [XFBJK23; BRS24; FHQR24].
4.2
Algorithmic modularity via self-predictive estimation
We complement the above results by studying the general online RL setting without hindsight
observations. To address this more challenging setting, we design an optimistic self-predictive
estimation objective (Eq. (7)), which learns a representation by jointly fitting a decoder together
with a latent model. We prove that any representation learning oracle that attains low regret with
respect to this objective can be used in O2L to obtain observable-to-latent reductions for any low-risk
base algorithm ALGlat (for a formal statement, see Theorem A.1). We provide a (computationally
inefficient) estimator (SELFPREDICT.OPT; Algorithm 4 in Appendix H.1) which we show attains
low optimistic self-regret under certain statistical conditions (namely, coverability of the base MDP
and a function approximation condition enabling us to express the self-prediction target as a latent
model, see Lemma A.1 for a formal statement), thereby obtaining an end-to-end reduction for the
general online RL setting. For lack of space, these results are deferred to Appendix A.
5
Discussion
Our work initiates the study of statistical and algorithmic modularity for reinforcement learning under
general latent dynamics. Our positive and negative results serve as a first step toward a unified theory
for reinforcement learning in the presence of high-dimensional observations. To this end, we close
with some important future directions and open problems.
Statistical modularity.
Can we obtain a unified characterization for the statistical complexity of RL
under latent dynamics with a given class of base MDPs Mlat? Our results in Section 3 suggest that
this will require new tools that go beyond existing notions of statistical complexity. Toward resolving
this problem, concrete questions that are not yet understood include: (i) Is coverability [XFBJK23]
(as opposed to pushforward coverability) sufficient for learnability under latent dynamics? (ii) Is the
Exogenous Block MDP problem [EMKAL22; MFR24]—a special case of our general framework—
statistically tractable? Lastly, are there additional types of feedback that are weaker than hindsight
observability, yet suffice to bypass the hardness results in Section 3?
Algorithmic modularity. Can we derive a unified representation learning objective that enables
algorithmic modularity whenever statistical modularity is possible? Ideally, such an objective would
be computationally tractable. Alternatively, can we show that algorithmic modularity fundamentally
requires stronger modeling assumptions than statistical modularity? Toward addressing the problems
above, a first step might be to understand: (i) What are the minimal statistical assumptions under
which we can minimize the self-predictive objective in Section 4.2? (ii) How can we encourage
finding good representations via self-prediction beyond the use of optimism over the base (latent)
models; and (iii) when can we minimize self-prediction in a computationally efficient fashion?
Acknowledgements
Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781,
Google Scholar Award, and Sloan Fellowship.
11Formally, while we have defined the statistical modularity condition in terms of high-probability risk bounds,
it is straightforward to extend it to instead consider expected risk bounds.
10

References
[ACK24]
Philip Amortila, Tongyi Cao, and Akshay Krishnamurthy. “Mitigating Covariate
Shift in Misspecified Regression With Applications to Reinforcement Learning”.
In: Conference on Learning Theory. 2024.
[AFJSX24]
Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie.
“Harnessing Density Ratios for Online Reinforcement Learning”. In: International
Conference on Learning Representations. 2024.
[AFK24]
Philip Amortila, Dylan J Foster, and Akshay Krishnamurthy. “Scalable Online
Exploration via Coverability”. In: International Conference on Machine Learning.
2024.
[AHKLLS14]
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
Schapire. “Taming the monster: A fast and simple algorithm for contextual bandits”.
In: International Conference on Machine Learning. 2014.
[AJKS22]
Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learn-
ing: Theory and algorithms. https://rltheorybook.github.io/. Version:
January 31, 2022. 2022.
[AJSWY20]
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. “Model-
Based Reinforcement Learning with Value-Targeted Regression”. In: International
Conference on Machine Learning. 2020.
[AKKS20]
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. “FLAMBE:
Structural Complexity and Representation Learning of Low Rank MDPs”. In:
Neural Information Processing Systems. 2020.
[AOM17]
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. “Minimax Regret
Bounds for Reinforcement Learning”. In: International Conference on Machine
Learning. 2017.
[AZ22]
Alekh Agarwal and Tong Zhang. “Model-based RL with Optimistic Posterior
Sampling: Structural Conditions and Sample Complexity”. In: Neural Information
Processing Systems. 2022.
[Bak+22]
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet,
Brandon Houghton, Raul Sampedro, and Jeff Clune. “Video PreTraining (VPT):
Learning to Act by Watching Unlabeled Online Videos”. In: Neural Information
Processing Systems. 2022.
[BLM13]
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequali-
ties: A nonasymptotic theory of independence. Oxford university press, 2013.
[Bro+22]
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine
Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil
J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei
Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch,
Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell
Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin
Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran,
Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun
Xu, Tianhe Yu, and Brianna Zitkovich. “RT-1: Robotics Transformer for Real-
World Control at Scale”. In: arXiv:2212.06817. 2022.
[BRS24]
Adam Block, Alexander Rakhlin, and Abhishek Shetty. “On the Performance of
Empirical Risk Minimization with Smoothed Data”. In: Conference on Learning
Theory. 2024.
[CBL06]
Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games. Cam-
bridge university press, 2006.
[CJ19]
Jinglin Chen and Nan Jiang. “Information-Theoretic Considerations in Batch
Reinforcement Learning”. In: International Conference on Machine Learning.
2019.
[DKJADL19]
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik,
and John Langford. “Provably Efficient RL With Rich Observations via Latent
State Decoding”. In: International Conference on Machine Learning. 2019.
11

[DMKV21]
Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko.
“Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Re-
visited”. In: Algorithmic Learning Theory. 2021.
[DMRY20]
Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. “Robust Guarantees
for Perception-Based Control”. In: Learning for Dynamics and Control. 2020.
[DR21]
Sarah Dean and Benjamin Recht. “Certainty Equivalent Perception-Based Control”.
In: Learning for Dynamics and Control. 2021.
[Du+21]
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen
Sun, and Ruosong Wang. “Bilinear Classes: A Structural Framework for Provable
Generalization in RL”. In: International Conference on Machine Learning. 2021.
[DVRZ19]
Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. “Provably Efficient Rein-
forcement Learning With Aggregated States”. In: arXiv:1912.06366. 2019.
[EFMKL22]
Yonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and
John Langford. “Sample-Efficient Reinforcement Learning in the Presence of
Exogenous Information”. In: Conference on Learning Theory. 2022.
[EMKAL22]
Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and
John Langford. “Provable RL With Exogenous Distractors via Multistep Inverse
Dynamics”. In: International Conference on Learning Representations. 2022.
[FGH23]
Dylan J Foster, Noah Golowich, and Yanjun Han. “Tight Guarantees for Interactive
Decision Making with the Decision-Estimation Coefficient”. In: Conference on
Learning Theory. 2023.
[FGQRS23]
Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari.
“Model-Free Reinforcement Learning with the Decision-Estimation Coefficient”.
In: Neural Information Processing Systems. 2023.
[FHQR24]
Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. “Online Es-
timation via Offline Estimation: An Information-Theoretic Framework”. In:
arXiv:2404.10122. 2024.
[FKQR21]
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. “The Statisti-
cal Complexity of Interactive Decision Making”. In: arXiv:2112.13487. 2021.
[FR20]
Dylan J Foster and Alexander Rakhlin. “Beyond UCB: Optimal and Efficient
Contextual Bandits With Regression Oracles”. In: International Conference on
Machine Learning. 2020.
[FR23]
Dylan J Foster and Alexander Rakhlin. “Foundations of Reinforcement Learning
and Interactive Decision Making”. In: arXiv:2312.16730. 2023.
[FWYDY20]
Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. “Provably
Efficient Exploration for Reinforcement Learning Using Unsupervised Learning”.
In: Neural Information Processing Systems. 2020.
[GCWXWB24]
Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, and
Yu Bai. “Sample-Efficient Learning of POMDPs with Multiple Observations In
Hindsight”. In: International Conference on Learning Representations. 2024.
[Gee00]
S. A. van de Geer. Empirical Processes in M-Estimation. Cambridge University
Press, 2000.
[GKBNB19]
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Belle-
mare. “DeepMDP: Learning Continuous Latent Space Models for Representation
Learning”. In: International Conference on Machine Learning. 2019.
[GMR24]
Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. “Exploring and Learning in
Sparse Linear MDPs Without Computationally Intractable Oracles”. In: Symposium
on Theory of Computing. 2024.
[Guo+22]
Zhaohan Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent
Altché, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill,
Yunhao Tang, Michal Valko, Rémi Munos, Mohammad Gheshlaghi Azar, and
Bilal Piot. “BYOL-Explore: Exploration by Bootstrapped Prediction”. In: Neural
Information Processing Systems. 2022.
[Haf+19]
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak
Lee, and James Davidson. “Learning Latent Dynamics for Planning From Pixels”.
In: International Conference on Machine Learning. 2019.
12

[HLBN19]
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. “Dream to
Control: Learning Behaviors by Latent Imagination”. In: International Conference
on Learning Representations. 2019.
[HLNB21]
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. “Master-
ing Atari With Discrete World Models”. In: International Conference on Learning
Representations. 2021.
[HLSW21]
Botao Hao, Tor Lattimore, Csaba Szepesvári, and Mengdi Wang. “Online Sparse
Reinforcement Learning”. In: International Conference on Artificial Intelligence
and Statistics. 2021.
[HPBL23]
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. “Mastering
Diverse Domains Through World Models”. In: arXiv:2301.04104. 2023.
[Jia24]
Nan Jiang. “A Note on Loss Functions and Error Compounding in Model-based
Reinforcement Learning”. In: arXiv:2404.09946. 2024.
[JKALS17]
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire. “Contextual Decision Processes With Low Bellman Rank Are PAC-
Learnable”. In: International Conference on Machine Learning. 2017.
[JLM21]
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. “Bellman Eluder Dimension: New
Rich Classes of RL Problems, and Sample-Efficient Algorithms”. In: Neural
Information Processing Systems. 2021.
[JYWJ20]
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Provably Efficient
Reinforcement Learning With Linear Function Approximation”. In: Conference
on Learning Theory. 2020.
[KAL16]
Akshay Krishnamurthy, Alekh Agarwal, and John Langford. “PAC Reinforcement
Learning With Rich Observations”. In: Neural Information Processing Systems.
2016.
[KFPM21]
Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. “RMA: Rapid
Motor Adaptation for Legged Robots”. In: Robotics: Science and Systems. 2021.
[LADZ23]
Jonathan Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. “Learning in
POMPDs Is Sample-Efficient With Hindsight Observability”. In: International
Conference on Machine Learning. 2023.
[Lam+24]
Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Rajiv Didolkar, Dipendra
Misra, Dylan J Foster, Lekan P Molu, Rajan Chari, Akshay Krishnamurthy, and
John Langford. “Guaranteed Discovery of Control-Endogenous Latent States with
Multi-Step Inverse Models”. In: Transactions on Machine Learning Research.
2024.
[LFDA16]
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. “End-To-End
Training of Deep Visuomotor Policies”. In: The Journal of Machine Learning
Research. 2016.
[Li09]
Lihong Li. A Unifying Framework for Computational Reinforcement Learning
Theory. Rutgers, The State University of New Jersey, 2009.
[LS20]
Tor Lattimore and Csaba Szepesvári. Bandit Algorithms. Cambridge University
Press, 2020.
[LSA20]
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. “CURL: Contrastive Unsu-
pervised Representations for Reinforcement Learning”. In: International Confer-
ence on Machine Learning. 2020.
[LXJZV24]
Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, and Yevgeniy Vorobey-
chik. “Learning Interpretable Policies in Hindsight-Observable POMDPs through
Partially Supervised Reinforcement Learning”. In: arXiv:2402.09290. 2024.
[MBFR23]
Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. “Efficient
Model-Free Exploration in Low-Rank MDPs”. In: Neural Information Processing
Systems. 2023.
[MCKJA24]
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
“Model-Free Representation Learning and Exploration in Low-Rank Mdps”. In:
Journal of Machine Learning Research. 2024.
13

[MFR23]
Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. “Representation
Learning With Multi-Step Inverse Kinematics: An Efficient and Optimal Approach
to Rich-Observation RL”. In: International Conference on Machine Learning.
2023.
[MFR24]
Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. “The Power of Resets
in Online Reinforcement Learning”. In: arXiv:2404.15417. 2024.
[Mha+20]
Zakaria Mhammedi, Dylan J Foster, Max Simchowitz, Dipendra Misra, Wen Sun,
Akshay Krishnamurthy, Alexander Rakhlin, and John Langford. “Learning the
Linear Quadratic Regulator From Nonlinear Observations”. In: Neural Information
Processing Systems. 2020.
[MHKL20]
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. “Kine-
matic State Abstraction and Provably Efficient Rich-Observation Reinforcement
Learning”. In: International Conference on Machine Learning. 2020.
[MJTS20]
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. “Sample Complexity
of Reinforcement Learning Using Linearly Combined Model Ensembles”. In:
International Conference on Artificial Intelligence and Statistics. 2020.
[MLJL21]
Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford. “Provable Rich Observa-
tion Reinforcement Learning With Combinatorial Latent States”. In: International
Conference on Learning Representations. 2021.
[Ni+24]
Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement
Gehring, Aditya Mahajan, and Pierre-Luc Bacon. “Bridging State and History
Representations: Understanding Self-Predictive RL”. In: arXiv:2401.08898. 2024.
[NRKFG22]
Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav
Gupta. “R3M: A Universal Visual Representation for Robot Manipulation”. In:
arXiv:2203.12601. 2022.
[OVR16]
Ian Osband and Benjamin Van Roy. “On Lower Bounds for Regret in Reinforce-
ment Learning”. In: arXiv:1608.02732. 2016.
[PAED17]
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. “Curiosity-
Driven Exploration by Self-Supervised Prediction”. In: International Conference
on Machine Learning. 2017, pp. 2778–2787.
[RH23]
Philippe Rigollet and Jan-Christian Hütter. “High-dimensional statistics”. In:
arXiv:2310.19244. 2023.
[RVR13]
Daniel Russo and Benjamin Van Roy. “Eluder Dimension and the Sample Com-
plexity of Optimistic Exploration”. In: Neural Information Processing Systems.
2013.
[SAGHCB20]
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville,
and Philip Bachman. “Data-Efficient Reinforcement Learning with Self-Predictive
Representations”. In: International Conference on Learning Representations. 2020.
[Sch+20]
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Lau-
rent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore
Graepel, Timothy Lillicrap, and David Silver. “Mastering Atari, Go, Chess and
Shogi by Planning With a Learned Model”. In: Nature. 2020.
[SJKAL19]
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
“Model-Based RL in Contextual Decision Processes: PAC Bounds and Exponential
Improvements Over Model-Free Approaches”. In: Conference on Learning Theory.
2019.
[SLS23]
Ming Shi, Yingbin Liang, and Ness Shroff. “Theoretical Hardness and Tractability
of POMDPs in RL with Partial Hindsight State Information”. In: arXiv:2306.08762.
2023.
[SWFK24]
Yuda Song, Lili Wu, Dylan J Foster, and Akshay Krishnamurthy. “Rich-
Observation Reinforcement Learning with Continuous Latent Dynamics”. In:
International Conference on Machine Learning. 2024.
[SZSBKS23]
Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy,
and Wen Sun. “Hybrid RL: Using Both Offline and Online Data Can Make RL
Efficient”. In: International Conference on Learning Representations. 2023.
14

[Tan+17]
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen,
Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. “# Exploration: A
Study of Count-Based Exploration for Deep Reinforcement Learning”. In: Neural
Information Processing Systems. 2017.
[Tan+23]
Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila
Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar,
Charline Le Lan, Clare Lyle, András György, Shantanu Thakoor, Will Dabney,
Bilal Piot, Daniele Calandriello, and Michal Valko. “Understanding Self-Predictive
Learning for Reinforcement Learning”. In: International Conference on Machine
Learning. 2023.
[UZS22]
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. “Representation Learning for
Online and Offline RL in Low-rank MDPs”. In: The Tenth International Conference
on Learning Representations. 2022.
[WSD15]
Niklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. “From Pixels
to Torques: Policy Learning With Deep Dynamical Models”. In: International
Conference on Machine Learning. 2015.
[WSY20]
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. “Reinforcement Learning
with General Value Function Approximation: Provably Efficient Approach via
Bounded Eluder Dimension”. In: Neural Information Processing Systems. 2020.
[WYDW21]
Tianhao Wu, Yunchang Yang, Simon Du, and Liwei Wang. “On Reinforcement
Learning With Adversarial Corruption and Its Application to Block MDP”. In:
International Conference on Machine Learning. 2021.
[XFBJK23]
Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. “The Role
of Coverage in Online Reinforcement Learning”. In: International Conference on
Learning Representations. 2023.
[XJ21]
Tengyang Xie and Nan Jiang. “Batch Value-Function Approximation With Only
Realizability”. In: International Conference on Machine Learning. 2021.
[YFK21]
Denis Yarats, Rob Fergus, and Ilya Kostrikov. “Image Augmentation Is All You
Need: Regularizing Deep Reinforcement Learning From Pixels”. In: International
Conference on Learning Representations. 2021.
[ZGS21]
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. “Nearly Minimax Optimal
Reinforcement Learning for Linear Mixture Markov Decision Processes”. In:
Conference on Learning Theory. 2021.
[Zha06]
Tong Zhang. “From ϵ-entropy to KL-entropy: Analysis of minimum information
complexity density estimation”. In: The Annals of Statistics. Vol. 34. 5. Institute of
Mathematical Statistics, 2006, pp. 2180–2210.
[Zha22]
Tong Zhang. “Feel-Good Thompson Sampling for Contextual Bandits and Rein-
forcement Learning”. In: SIAM Journal on Mathematics of Data Science. 2022.
[ZMCGL21]
Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine.
“Learning Invariant Representations for Reinforcement Learning Without Recon-
struction”. In: International Conference on Learning Representations. 2021.
[ZSUWAS22]
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal,
and Wen Sun. “Efficient Reinforcement Learning in Block MDPs: A Model-Free
Representation Learning Approach”. In: International Conference on Machine
Learning. 2022.
15

Contents
A Omitted Results from Section 4: Algorithmic Modularity via Self-predictive Estimation 17
A.1
Self-predictive estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.2
Main result
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
A.3
Instantiating the self-predictive estimation oracle
. . . . . . . . . . . . . . . . . .
19
B Additional Discussion of Related Work
22
C Technical Tools
23
D Structural Properties of Coverability and Mismatch Functions
24
E
Proofs and Additional Results for Section 3.2: Impossibility Results
28
E.1
Additional Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
E.2
Details for Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
E.3
Proofs for Lower Bounds (Theorems 3.1 and E.1) . . . . . . . . . . . . . . . . . .
34
F
Proofs for Section 3.3: Positive Results
40
F.1
Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs. 40
F.2
Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2) . . . . .
41
G Proofs and Additional Information for Section 4.1: Hindsight RL
52
G.1
Pseudocode and Proofs for EXPWEIGHTS.DR (Lemma 4.1)
. . . . . . . . . . . .
52
G.2
Proofs for O2L Under Hindsight Observability (Theorem 4.1)
. . . . . . . . . . .
53
H Proofs for Appendix A: Self-Predictive Estimation
56
H.1
Pseudocode and Proofs for SELFPREDICT.OPT (Lemma A.1) . . . . . . . . . . . .
56
H.2
Proofs for Main Risk Bound (Theorem A.1) . . . . . . . . . . . . . . . . . . . . .
60
I
Additional Results for Appendix A: Self-Predictive Estimation
63
I.1
O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms . .
63
I.2
Proofs for Appendix I.1.2: Properties of ϕ-compressed POMDPs . . . . . . . . . .
66
I.3
Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1) 71
I.4
Proofs for Appendix I.1.4: Examples of CorruptionRobust Algorithms . . . . . .
73
16

A
Omitted Results from Section 4: Algorithmic Modularity via
Self-predictive Estimation
In this section, we remove the assumption of hindsight observability used in Section 4.1 and instantiate
O2L in the general online RL setting. Rather than assume access to additional side-information,
we adopt a model-based representation learning approach, and augment our ability to perform
representation learning by equipping the representation learning algorithm with a set of base MDPs
Mlat in addition to the decoder class Φ. We will learn a representation by jointly fitting a decoder
and the base (latent) dynamics, which is a common approach in practice [GKBNB19; HLBN19;
Haf+19; HLNB21; Sch+20; SAGHCB20; Guo+22]. We firstly present in Appendix A.1 a new notion
of optimistic self-predictive regret which combines self-predictive representation learning with a form
of optimism over a learned latent model. We then show in Appendix A.2 that any representation
learning oracle that attains low regret, when used within O2L (Algorithm 1), leads to observable-to-
latent reductions that ensure low risk for any base algorithm ALGlat, thereby achieving algorithmic
modularity. Lastly, in Appendix A.3, we instantiate this oracle under natural structural and function
approximation conditions, yielding end-to-end modularity and sample complexity guarantees.
A.1
Self-predictive estimation
Our self-predictive representation learning oracles learn to fit a representation ϕ such that the induced
latent transitions (ϕh(xh) to ϕh+1(xh+1)) can be accurately modeled by some base (latent) MDP
Mlat ∈Mlat. To describe the objective, let us first introduce some notation. For a given MDP M
over either S (resp. X), we write Mh(rh, sh+1 | sh, ah) (resp. Mh(rh, xh+1 | xh, ah)) for the joint
conditional distribution over rewards and next states. Next, for any ϕ ∈Φ, we define the pushforward
model for M ⋆
obs,h induced by ϕ via:

ϕh+1♯M ⋆
obs,h

(r, s′ | x, a) :=
X
x′:ϕh+1(x′)=s′
M ⋆
obs,h(r, x′ | x, a).
(6)
The pushforward model for ϕ captures the forward probability of the estimated latent state ϕ(x′)
given a current observation x. To measure distance between models, we will use squared Hellinger
distance (e.g, Foster et al. [FKQR21]), defined via D2
H(P, Q) =
R  q
dP
dν −
q
dQ
dν
2dν for a common
dominating measure ν. Then, for a base model Mlat and a decoder ϕ, the self-predictive error of
(Mlat, ϕ), at state-action pair xh, ah, is given by
[∆h(Mlat, ϕ)](xh, ah) := D2
H
 Mlat,h(ϕh(xh), ah),

ϕh+1♯M ⋆
obs,h

(xh, ah)

.
This term captures the ability of Mlat,h(ϕh(xh), ah) to predict the next latent state ϕh+1(xh+1)
which is obtained by the pushforward model

ϕh+1♯M ⋆
obs,h

(xh, ah). Formally, in our model-based
representation learning setup, we consider oracles which, for each iteration t within O2L, take as
input the trajectories collected so far and produce an estimate (c
M
(t)
lat, bϕ(t)) for the decoder and base
model. The representation learning oracle’s self-predictive regret, for the sequence (c
M
(t)
lat, bϕ(t)), is
then defined as
Regself(T) =
T
X
t=1
H
X
h=0
Eπ(t)∼p(t) Eπ(t)h
[∆h(c
M
(t)
lat, bϕ
(t))](xh, ah)
i
,
where p(t) represents a randomization distribution over the policy π(t).
On its own, minimizing this regret may lead to degenerate solutions, a widely observed phenomenon
in practice [Tan+23]. For example, in a standard combination lock MDP (e.g., Agarwal et al.; Misra
et al. [AJKS22; MHKL20]), a degenerate decoder-model pair that maps all observations to a single
latent state will have zero self-predictive loss until we reach the goal, which can take exponentially
long.12 We address this via the notion of optimistic estimation used in Zhang; Foster et al. [Zha22;
FGQRS23], which biases the objective towards latent models with high return. This leads to the
12This is similar to the observation that naive value function approximation methods, such as Fitted Q-Iteration,
can fail to explore in online RL without optimism. We expect that given access to additional exploratory data
(e.g., in the Hybrid RL setting of Song et al. [SZSBKS23]), the latent optimism term can be removed.
17

following optimistic self-predictive regret, defined for a parameter γ > 0, via
Regself;opt(T, γ) =
T
X
t=1
H
X
h=0
Eπ(t)∼p(t) Eπ(t)h
[∆h(c
M
(t)
lat, bϕ
(t))](xh, ah)
i
+ γ−1(JM ⋆
lat(πM ⋆
lat) −J
c
M (t)
lat (πc
M (t)
lat )).
(7)
We assume going forward that REPself;opt obtains low optimistic self-predictive regret; in Ap-
pendix A.3 we provide a maximum-likelihood-type estimator and conditions under which this holds.
Assumption A.1. For a parameter γ > 0 and any (possibly adaptive) sequence π(t), with π(t) ∼p(t),
the online representation learning oracle REPself;opt is proper (i.e. outputs c
M
(t)
lat ∈Mlat for all
t ∈[T]) and satisfies
E

Regself;opt(T, γ)

≤Estself;opt(T, γ),
where Estself;opt(T, γ) is a known upper bound.
We note that only the decoder bϕ(t) is used within O2L; the model c
M
(t)
lat is only used for analysis (and
possibly within the representation learner REPself;opt).
A.2
Main result
We now state the main guarantee for O2L with self-predictive representation learning. Recall
that Riskobs(TK) denotes the risk of the O2L reduction. Compared to the hindsight-observable
setting, we require a slightly stronger performance guarantee from the base algorithm ALGlat: our
result scales with the worst-case expected risk for ALGlat over all Mlat ∈Mlat, defined via
Riskbase(K) := supMlat∈Mlat E[Risk(K, ALGlat, Mlat))].
Theorem A.1 (Risk bound for O2L under self-predictive estimation). Suppose REPself;opt satisfies
Assumption A.1 with parameter γ > 0. Then Algorithm 1, with inputs T, K, Φ, REPself;opt, and
ALGlat has expected risk
E[Riskobs(TK)] ≤c1 · Riskbase(K) + c2γ · K
T Estself;opt(T, γ) + c3γ−1 · KH,
for absolute constants c1, c2, c3 > 0.
Theorem A.1 achieves sublinear risk as long as (i) the latent algorithm achieves sublinear risk
Riskbase(K) given access to the true states, and (ii) the self-predictive representation learning oracle
achieves sublinear regret Estself;opt(T, γ) for an appropriate choice of γ.13 Intuitively, our result
scales with Riskbase(K) instead of Risk⋆(K) due to potential symmetries in the self-predictive
objective. For example, there might be a representation-model pair (c
Mlat, bϕ) that is identical
to (M ⋆
lat, ϕ⋆) up to permutations of the latent state space; these cannot be distinguished by a
representation learning oracle that does not observe the latent states directly, and thus the base
algorithm may be tasked with solving either of these base MDPs. As with Theorem 4.1, this
result achieves algorithmic modularity (since O2L inherits the risk of the base algorithm), and is
computationally efficient whenever the base algorithm and self-predictive representation learning
oracle are efficient.
Let us provide some intuition behind the proof of Theorem A.1. Recall that, within the inner loop
of O2L, the latent algorithm ALGlat interacts with the bϕ(t)-compressed dynamics generated by
compressing the observations xh, ah through the current decoder bϕ
(t)
h (Line 8). The crux of the
analysis is the following observation: by the self-predictive representation learning guarantee, these
dynamics, despite being possibly non-Markovian and generated from a POMDP (Definition I.1), are
well approximated in squared Hellinger distance by the base model c
M
(t)
lat estimated by REPself;opt (cf.
Lemma I.2). We can then show that ALGlat, when given data from the bϕ(t)-compressed dynamics,
has risk (for solving c
M
(t)
lat) that is proportional to: i) its base risk if it were to observe states from
c
M
(t)
lat, and ii) the Hellinger distance between c
M
(t)
lat and the process induced by its bϕ(t)-compressed
13For example, in our estimator of Appendix A.3, we can first set γ ≈KH/Riskbase(K) so that the third
term matches Riskbase(K), and then set T so that the second term does.
18

dynamics. The last ingredient is the use of latent optimism in Eq. (7), through which the risk on M ⋆
lat
is upper bounded by the risk on c
M
(t)
lat.
In the above, showing that ALGlat obtains low risk for c
M
(t)
lat (despite given data from a different
process) is done by establishing a certain form of corruption robustness (Definition I.2). Indeed,
Theorem A.1 is a special case of a more general theorem (Theorem H.1), which provides a bound
that adapts to ALGlat’s level of robustness. We obtain Theorem A.1 by showing that any algorithm
satisfies the property we require (for a suitably slow rate), but we further show that tighter rates can
be achieved by analyzing the specifics of various algorithms of interest (Appendix I.1.4).
A.3
Instantiating the self-predictive estimation oracle
We now present an algorithm, SELFPREDICT.OPT (Algorithm 4 in Appendix H.1), which satisfies
Assumption A.1 under additional technical conditions, allowing us to instantiate Theorem A.1 to give
end-to-end guarantees. Before stating the main guarantee, we highlight a few technical difficulties
regarding obtaining finite-sample guarantees for (online) self-predictive estimation, and use them to
motivate our statistical assumptions and algorithm design.
The statistics of (online) self-predictive estimation.
The first challenge is a realizability issue:
when ϕ ̸= ϕ⋆, we may not even be able to represent the objective ϕ♯M ⋆
obs as a latent model using
only decoder and latent model realizability. Since we can never guarantee that ϕ = ϕ⋆exactly in the
presence of statistical errors, we must introduce a modelling assumption which lets us capture the
pushforward models ϕ♯M ⋆
obs. To this end, we introduce the mismatch functions, which are defined
as follows.
Definition A.1 (Mismatch functions). For a decodable emission process ψ⋆and decoder ϕ ∈Φ,
the mismatch function for ϕ, Γϕ = {Γϕ,h : S →∆(S)}H
h=1, is defined, for every h ∈[H], as the
probability kernel
Γϕ,h(s′
h | sh) := Pxh∼ψ⋆
h(sh)(ϕh(xh) = s′
h).
In the context of self-prediction, we show that the following mismatch completeness assumption
suffices to capture the pushforward models ϕ♯M ⋆
obs.
Assumption A.2 (Mismatch completeness). We have a model class L such that, for each ϕ ∈Φ, and
Mlat ∈Mlat, we have Γϕ ◦Mlat ∈L, where
[Γϕ ◦Mlat]h(rh, sh+1 | sh, ah) :=
X
s′
h+1∈S
Mlat,h(rh, s′
h+1 | sh, ah)Γϕ,h+1(sh+1 | s′
h+1).
In particular, Lemma D.8 establishes that
[ϕh+1♯M ⋆
obs,h](· | x, a) = [Γϕ ◦M ⋆
lat]h(· | ϕ⋆
h(x), a).
Accordingly, we view this assumption as a minimal way to realize the pushforward models ϕ♯M ⋆
obs.
The second challenge is a double-sampling issue, which appears because the decoders in Eq. (7)
are coupled at different horizons. We address this with a novel “debiased” maximum likelihood
procedure that subtracts a form of excess risk (cf. Eq. (60)) to recover an unbiased estimator [Jia24].
Our debiased estimator and the mismatch completeness assumption can be viewed as analogous
to the techniques and assumptions that are required for squared Bellman error minimization in the
context of value function approximation [CJ19; JLM21].
The last issue stems from seeking an online estimation guarantee: the policies chosen by the latent
algorithm are a function of the estimated decoders, which precludes the use of randomized estimators
(e.g. exponential weights). We bypass this issue by appealing to the structural condition of coverability
[XFBJK23], which allows us to restrict our attention to estimators that achieve low offline estimation
error (via Lemma C.7).14
Definition A.2 (State Coverability). The state coverability coefficient for an MDP M and a policy
class Π defined over a state space Z, Ccov,st(M, Π), is given by
Ccov,st(M, Π) := max
h∈[H]
min
µ∈∆(Z) max
π∈Π max
z∈Z
d
M,π
h
(z)
µ(z)

.
(8)
14More generally, we expect that our results can be extended to any “decoupling coefficient” [Zha22; AZ22].
19

We require coverability in M ⋆
obs over the set of (observation-space) policies played by the O2L
reduction (cf. Line 7). Again appealing to the mismatch functions, we can express this as an
assumption about the base dynamics M ⋆
lat; we show (Lemma D.1) that the latter is equivalent to
assuming coverability in M ⋆
lat over the set of stochastic policies
ΓΦ ◦Πlat :=
(
[Γϕ ◦πlat]h(a | s) =
X
s′∈S
Γϕ,h(s′ | s)πlat,h(a | s′) | ϕ ∈Φ, πlat ∈Πlat
)
,
(9)
where Πlat denotes the set of policies that ALGlat may execute. While this set may appear compli-
cated, it is sufficient to assume coverability over the set of all deterministic non-stationary policies on
M ⋆
lat.15
Guarantee for our self-predictive estimation oracle.
With these prerequisites, the main guarantee
for our estimator, SELFPREDICT.OPT (Algorithm 4), is as follows.
Lemma A.1 (Optimistic self-predictive estimation via SELFPREDICT.OPT). Let Πlat denote the
set of policies played by ALGlat, and Ccov,st = Ccov,st(M ⋆
lat, ΓΦ ◦Πlat) be the state coverability
parameter on M ⋆
lat over the set of stochastic policies ΓΦ ◦Πlat (Eq. (9)). Then, for any γ > 0,
under decoder realizability (ϕ⋆∈Φ), base model realizability (M ⋆
lat ∈Mlat), and mismatch
function completeness with class Llat (Assumption A.2), the estimator in Algorithm 4 with inputs
Φ, Mlat, Llat, and γ satisfies Assumption A.1 with16
Estself;opt(T, γ) = e
O
q
HCcov,st|A|T log(|Mlat||Llat||Φ|)

.
Instantiating Theorem A.1 with the above representation learning oracle, we obtain the following
algorithmic modularity result.
Corollary A.1 (Algorithmic modularity via SELFPREDICT.OPT). Under the same conditions as in
Lemma A.1, and for any base algorithm ALGlat, O2L with inputs T, K, Φ, SELFPREDICT.OPT, and
ALGlat achieves
E[Riskobs(TK)] ≲c1·Riskbase(K)+c2γ· K
√
T
q
HCcov,st|A| log(|Mlat||Llat||Φ|)+c3γ−1·KH,
for absolute constants c1, c2, c3. Consequently, for any ALGlat with base risk Riskbase(K), setting
γ and T appropriately gives
E[Riskobs(TK)] ≲Riskbase(K),
with a number of trajectories TK = e
O(K5H3Ccov,st|A| log2(|Mlat||Llat||Φ|)/(Riskbase(K))4).
For example, if ALGlat is a base algorithm with Riskbase(K)
=
O(K−1/2), setting γ
and T appropriately gives an expected risk of ε with a number of trajectories TK
=
e
O
 H3Ccov,st|A|(log(|Mlat||Llat||Φ|))2/ε14
. This result shows that statistical modularity can be achieved
up to log(|Llat|) factors for every base MDP class Mlat which is subsumed by coverability, in-
cluding tabular MDPs and low-rank MDPs.17 Compared to our positive result for the case of
pushforward coverability (Section 3.3), this imposes less dynamics assumptions (since coverability
is implied by pushforward coverability) but requires more representational assumptions (namely,
access to the mismatch-complete class Llat). We further remark that the mismatch completeness
assumption always holds for i) the Block MDP setting, since we can always construct Llat such that
log(|Llat|) = O(HS2), and ii) every MDP class Mlat whenever we also have a realizable set of
emission processes (ψ⋆∈Ψ), since we can construct Llat such that log(|Llat|) = log(|Φ||Mlat||Ψ|).
However, the mismatch completeness assumption may be more general than either of these settings.
15This follows from Lemma D.3 by noting that each maximum on the right hand side of Eq. (13) is attained
by a deterministic non-stationary policy.
16In this section, the notations e
O and ≲ignores constants and logarithmic factors of: H, Ccov,st, |A|, T, and
log(|Mlat||Llat||Φ|).
17This provides a partial answer to the “Model Class + Coverability” open question of Figure 1.
20

Our results can be viewed as providing a theoretical justification for self-predictive representation
learning, which has been widely used in empirical works [GKBNB19; SAGHCB20]. We consider
self-prediction’s ability to obtain universal observable-to-latent reductions as a strong indicator that it
merits further theoretical study. In particular, many empirical works propose heuristics to alleviate the
degeneracy/non-uniqueness issues inherent with self-prediction [GKBNB19; SAGHCB20; HPBL23;
Tan+23]. Our methods provide a principled way to address these, and it would be interesting to
investigate whether this is also empirically effective. In general, however, it is unclear whether our
loss admits a computationally efficient implementation, due to the presence of optimism. Towards
this, a fascinating direction for future work is understanding how self-predictive estimation can be
used to obtain algorithmic modularity without the addition of optimism over the base (latent) models.
21

B
Additional Discussion of Related Work
In this section, we discuss aspects of related work not already covered in greater detail.
Reinforcement learning under latent dynamics (or, with rich observations).
Reinforcement
learning under latent dynamics (or, with rich observations) has received extensive investigation in
recent years, however most works have been focused on the Block MDP model in which the latent
state space is tabular/finite [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23] (see also the
the closely related framework of Low-Rank MDPs [AKKS20; MCKJA24; ZSUWAS22; UZS22;
MBFR23]). Beyond tabular spaces, Dean et al.; Dean et al.; Mhammedi et al. [DMRY20; DR21;
Mha+20] consider continuous linear dynamics, Misra et al. [MLJL21] considers factored (but discrete)
latent dynamics, Efroni et al.; Efroni et al.; Mhammedi et al. [EMKAL22; EFMKL22; MFR24]
consider the Exogenous Block MDP problem in which a tabular latent state space is augmented with
a non-controllable (“exogenous”) factor, and Song et al. [SWFK24] consider Lipshitz continuous
dynamics. To our knowledge, our work is the first to: i) explore reinforcement learning under general
latent dynamics, in particular in settings where the latent space itself admits function approximation,
and ii) take a more modular approach (cf. the taxonomy of Section 3).
On the algorithmic side, the works of Uehara et al. [UZS22] and Zhang et al. [ZSUWAS22], which
consider Low-Rank MDPs and Block MDPs respectively, can be viewed as interleaving representation
learning with “latent” reinforcement learning algorithms that assume access to a good representation,
and were an inspiration for this work. However, the algorithmic details and analyses are highly
specialized to Block/Low-Rank MDPs, and unlikely to be directly applicable to reinforcement
learning under general latent dynamics. Other works with a modular flavor include:
• Feng et al. [FWYDY20] solve tabular Block MDPs by combining a black-box latent algorithm
with an “unsupervised learning oracle” for representation learning. This approach only leads to
guarantees for tabular Block MDPs, and it is unclear whether the unsupervised learning oracle their
approach requires can be constructed in natural settings.
• Wu et al. [WYDW21] solve tabular block MDPs by combining a corruption-robust latent algorithm
with a representation learning procedure based on clustering. Again, this work is restricted to the
tabular setting, and requires a separation condition which may not be satisfied in general.
General complexity measures for reinforcement learning.
Another line of research provides
general complexity measures that enable sample-efficient reinforcement learning, including Bellman
rank [JKALS17; SJKAL19; Du+21; JLM21], eluder dimension [RVR13], coverability [XFBJK23],
and the Decision-Estimation Coefficient (DEC) [FKQR21; FGH23; FGQRS23]. Bellman rank and
other complexity measures based on average Bellman error [JKALS17; SJKAL19; Du+21; JLM21]
are insufficient to characterize learnability under general latent dynamics, as there are classes Mlat
that are known to be learnable, yet do not have bounded Bellman rank or Bellman-Eluder dimension
[EMKAL22; XFBJK23]. Meanwhile, variants of Bellman rank based on squared Bellman error
or related notions of error can [XFBJK23; AFJSX24] address this problem for some settings, but
satisfying the modeling/realizability assumptions (e.g., Bellman completeness) required by these
methods in the latent-dynamics setting is non-trivial. For example, the crux of our sample complexity
bounds under latent pushforward coverability in Section 3 (Theorem 3.2) is to prove a rather involved
structural result which shows that Bellman completeness can indeed be satisfied under this assumption,
but it is unclear whether these techniques can be applied to more general latent dynamics classes. We
expect that it is possible to bound the Decision-Estimation Coefficient [FKQR21; FGH23; FGQRS23]
for the framework, but deriving efficient algorithms using this framework is non-trivial.
22

C
Technical Tools
Lemma C.1. For any sequence of real-valued random variables (Xt)t≤T adapted to a filtration
(Ft)t≤T , it holds that with probability at least 1 −δ,
T
X
t=1
Xt ≤
T
X
t=1
log
 Et−1

eXt
+ log
 δ−1
.
Lemma C.2 (Freedman’s inequality (e.g., Agarwal et al. [AHKLLS14])). Let (Xt)t≤T be a real-
valued martingale difference sequence adapted to a filtration (Ft)t≤T . If |Xt| ≤R almost surely,
then for any η ∈(0, 1/R), with probability at least 1 −δ,
T
X
t=1
Xt ≤η
T
X
t=1
Et−1

X2
t

+ log
 δ−1
η
.
Lemma C.3 (Corollary of Lemma C.2). Let (Xt)t≤T be a sequence of random variables adapted to
a filtration (Ft)t≤T . If 0 ≤Xt ≤R almost surely, then with probability at least 1 −δ,
T
X
t=1
Xt ≤3
2
T
X
t=1
Et−1[Xt] + 4R log
 2δ−1
,
and
T
X
t=1
Et−1[Xt] ≤2
T
X
t=1
Xt + 8R log
 2δ−1
.
Lemma C.4 (Lemma D.2 of Foster et al. [FHQR24]). Let (X1, F1), . . . , (Xn, Fn) be a sequence of
measurable spaces, and let X (i) = Qi
t=1 Xt and F(i) = ⊗i
t=1Ft. For each i, let P (i) and Q(i) be
probability kernels from (X (i−1), F(i−1)) to (Xi, Fi). Let P and Q be the laws of X1, . . . , Xn under
Xi ∼P (i)(· | X1:i−1) and Xi ∼Q(i)(· | X1:i−1), respectively. Then it holds that
D2
H(P, Q) ≤7 EP
" n
X
i=1
D2
H(P
(i)(· | X1:i−1), Q
(i)(· | X1:i−1))
#
Lemma C.5 (Lemma A.11 of Foster et al. [FKQR21]). Let P and Q be probability measures on
(X, F). For all h : X →R with 0 ≤h(X) ≤R almost surely under P and Q, we have
EP[h(X)] ≤3 EQ[h(X)] + 4RD2
H(P, Q).
Lemma C.6 (Lemma 1 of Jiang et al. [JKALS17]). For any f : X ×A →[0, 1], π : S×[H] →∆(A),
we have
Ex1[f(x1, π(x1))] −J(π) =
H
X
h=1
Eπ[f(xh, ah) −T πf(xh, ah)].
Lemma C.7 (Offline-to-online conversion under coverability [XFBJK23; FHQR24]). Let M be an
MDP over state space Z, Π be a policy set, and Ccov = Ccov(M, Π) be the (state-action) coverability
coefficient for M and Π (Definition D.3). Let p(t) ∈∆(Π) be a sequence of distributions over Π, and
g
(t)
h : Z × A →[0, 1] be a sequence of functions. Then we have that
T
X
t=1
H
X
h=1
Eπ(t)∼p(t) Eπ(t)
g
(t)
h (xh, ah)

≤O


v
u
u
tHCcov log(T)
T
X
t=1
H
X
h=1
t−1
X
i=1
Eπ(i)∼p(i) Eπ(i)
g
(t)
h (xh, ah)

+ HCcov

.
23

D
Structural Properties of Coverability and Mismatch Functions
This appendix contains structural results regarding coverability and the mismatch functions. We
firstly recall the definition of the mismatch functions.
Definition D.1 (Mismatch functions). For decodable emission process ψ⋆, decoder ϕ ∈Φ and
h ∈[H], we define the mismatch function for ϕ, Γϕ,h : S →∆(S), as the probability kernel
Γϕ,h(s′
h | sh) := Pxh∼ψ⋆
h(sh)(ϕh(xh) = s′
h).
We also recall the definition of state coverability.
Definition D.2 (State Coverability). The coverability coefficient for an MDP M and a policy class Π
defined over a state space Z, Ccov,st(M, Π), is given by
Ccov,st(M, Π) := max
h∈[H]
min
µ∈∆(Z) max
π∈Π max
z∈Z
d
M,π
h
(z)
µ(z)

.
(10)
We also define the related notion of state-action coverability.
Definition D.3 (State-Action Coverability). The coverability coefficient for an MDP M and a policy
class Π defined over a state space Z and action space A, Ccov(M, Π), is given by
Ccov(M, Π) := max
h∈[H]
min
µ∈∆(Z×A) max
π∈Π
max
z,a∈Z×A
d
M,π
h
(z, a)
µ(z, a)

.
(11)
In the remainder of the section, we let Πlat ⊆{S × [H] →∆(A)} denote an arbitrary set of latent
policies, and
ΓΦ ◦Πlat =
(
[Γϕ ◦πlat]h(a | s) :=
X
s′∈S
Γϕ,h(s′ | s)πlat,h(a | s′) | ϕ ∈Φ, πlat ∈Πlat
)
. (12)
Lemma D.1 (State coverability is invariant to rich observations). Let M ⋆
obs = ⟪M ⋆
lat, ψ⋆⟫. Then, we
have
Ccov,st(M ⋆
obs, Πlat ◦Φ) = Ccov,st(M ⋆
lat, ΓΦ ◦Πlat).
Furthermore, letting {µlat,h ∈∆(S)}h∈[H] denote the distribution which witnesses the right-hand-
side, the left-hand-side is witnessed by the distribution
µobs,h(x) = ψ⋆
h(x | ϕ⋆
h(x))µlat,h(ϕ⋆
h(x)).
The lemma follows from the following two observations.
Lemma D.2. Let {Γϕ}ϕ∈Φ denote the mismatch functions for emission ψ⋆, and let Mobs =
⟪Mlat, ψ⋆⟫. Then, for any πlat ∈Πlat, ϕ ∈Φ, h ∈[H], x ∈X, we have
dMobs,πlat◦ϕ
h
(x) = ψ⋆
h(x | ϕ⋆
h(x))dMlat,Γϕ◦πlat
h
(ϕ⋆
h(x)).
Proof of Lemma D.2. Below, we write sh = ϕ⋆(xh). We proceed by induction, simply writing
dobs,h := dMobs,πlat◦ϕ
h
and dlat,h := dMlat,Γϕ◦πlat
h
. The base case (h = 1) is obtained by noting that
dlat,1(s) = Plat,1(s | ∅) while dobs,1(x) = Pobs,1(x | ∅) = ψ⋆
1(x | s)Plat,1(s | ∅). For the general
case, via the Bellman flow equations, we have
dobs,h(xh) =
X
xh−1,ah−1∈X×A
Pobs,h(xh | xh−1, ah−1)dobs,h−1(xh−1)πlat(ah−1 | ϕ(xh−1))
= ψ(xh | sh)
X
xh−1,ah−1∈X×A
Plat,h(sh | sh−1, ah−1)dlat,h−1(sh−1)ψ(xh−1 | sh−1)
× πlat(ah−1 | ϕ(xh−1))
= ψ(xh | sh)
X
sh−1,ah−1∈S×A
Plat,h(sh | sh−1, ah−1)dlat,h−1(sh−1)
×
X
xh−1:ϕ⋆(xh−1)=sh−1
ψ(xh−1 | sh−1)πlat(ah−1 | ϕ(xh−1)).
24

The result is obtained by noting that
Γϕ ◦πlat(ah−1 | sh−1) =
X
s′∈S
Γϕ(s′ | sh−1)πlat(ah−1 | s′)
=
X
s′∈S
X
xh−1:ϕ⋆(xh−1)=sh−1
ψ(xh−1 | sh−1)I{ϕ(xh−1) = s′}πlat(ah | s′)
=
X
xh−1:ϕ⋆(xh−1)=sh−1
ψ(xh−1 | sh−1)πlat(ah−1 | ϕ(xh−1)),
where the second line follows from the definition of the mismatch functions.
Lemma D.3 (Equivalence of state coverability and cumulative state reachability). Let M be an MDP
defined over a state space Z. The following definition is equivalent to Definition D.2:
Ccov,st(M, Π) := max
h∈[H]
X
z∈Z
max
π∈Π dM,π
h
(z).
(13)
Proof of Lemma D.3.
Straightforward adaptation of the proof of Lemma 3 from Xie et al.
[XFBJK23].
Proof of Lemma D.1. Using Lemma D.2 and Lemma D.3, we have
Ccov,st(Mobs, Πlat ◦Φ) = max
h∈[H]
X
x∈X
max
πlat,ϕ dπlat◦ϕ
obs
(x)
= max
h∈[H]
X
x∈X
max
πlat,ϕ ψ⋆(x | ϕ⋆(x))dΓϕ◦πlat
lat
(ϕ⋆(x))
= max
h∈[H]
X
s∈S
X
x:ϕ⋆(x)=s
max
πlat,ϕ ψ⋆(x | s)dΓϕ◦πlat
lat
(s)
= max
h∈[H]
X
s∈S
max
πlat,ϕ dΓϕ◦πlat
lat
(s)
X
x:ϕ⋆(x)=s
ψ⋆(x | s)
= Ccov,st(Mlat, ΓΦ ◦Πlat).
Lastly, we show that state-action coverability is bounded by state coverability times the size of the
action set.
Lemma D.4 (State-action coverability bound). For any MDP M and policy set Π, we have
Ccov(M, Π) ≤Ccov,st(M, Π)|A|.
Proof of Lemma D.4. Let µs ∈∆(Z) witness Ccov,st(M, Π). Fix h ∈[H], which we omit below
for cleanliness. Then, we have
min
µs,a∈∆(Z×A) max
π∈Π
max
z,a∈Z×A
dM,π(z, a)
µs,a(z, a)

≤max
π∈Π
max
z,a∈Z×A
dM,π(z)π(a | z)
µs(z)1/|A|

≤|A| max
π∈Π max
z∈Z
dM,π(z)
µs(z)

= Ccov,st(M, Π)|A|.
Lemma D.5 (Pushforward coverability is invariant to rich observations). Let Cpush(M) denote the
pushforward coverability parameter for an MDP M (Definition 3.3), and M ⋆
obs := ⟪M ⋆
lat, ψ⋆⟫. Then,
we have
Cpush(M ⋆
obs) = Cpush(M ⋆
lat).
25

Furthermore, letting {µlat,h ∈∆(S)}h∈[H] denote the distribution which witnesses the right-hand-
side, the left-hand-side is witnessed by the distribution
µobs,h(x) = ψ⋆
h(x | ϕ⋆
h(x))µlat,h(ϕ⋆
h(x)).
This follows from an analogous equivalence of pushforward coverability and cumulative conditional
reachability.
Lemma D.6 (Equivalence of pushforward coverability and cumulative conditional reachability). Let
M be an MDP defined over a state space Z with transition kernel P. The following definition is
equivalent to pushforward coverability (Definition 3.3):
Cpush(M) := max
h∈[H]
X
z′∈Z
max
z,a∈Z×A Ph(z′ | z, a).
Proof of Lemma D.6. Fix h ∈[H], whose dependence we omit below. For the first direction, letting
µ denote the pushforward coverability distribution, we have:
X
z′∈Z
max
z,a∈Z×A P(z′ | z, a) =
X
z′∈Z
max
z,a∈Z×A
P(z′ | z, a)
µ(z′)
µ(z′) ≤Cpush
X
z′∈Z
µ(z′) = Cpush.
For the second direction, taking µ(z′) ∝maxz,a P(z′ | z, a), we have
min
µ∈∆(Z)
max
z,a,z′∈Z×A×Z
P(z′ | z, a)
µ(z′)
≤
max
z,a,z′∈Z×A×Z
P(z′ | z, a)
max˜z,˜a P(z′ | ˜z, ˜a)
X
˜z′
max
˜z,˜a P(˜z′ | ˜z, ˜a)
≤
X
z′
max
z,a P(z′ | z, a).
Proof of Lemma D.5. This result follows by Lemma D.6 since,
Cpush(Mobs) =
X
x′∈X
max
x,a Pobs(x′ | x, a)
=
X
s′∈S
X
x′:ϕ⋆(x′)=s′
max
x,a ψ⋆(x′ | s′)Plat(s′ | ϕ⋆(x), a)
=
X
s′∈S
max
x,a Plat(s′ | ϕ⋆(x), a)
X
x′:ϕ⋆(x′)=s′
ψ⋆(x′ | s′)
=
X
s′∈S
max
s,a Plat(s′ | s, a) = Cpush(Mlat).
We next show that the mismatch functions can be used to express the observation-level backups for
any function of the decoders. For any g : S →R, h ∈[H], we define the function [Γϕ,h ◦g] : S →R
[Γϕ,h ◦g](s) :=
X
s′∈S
Γϕ,h(s′ | s)g(s′).
We further overload the Bellman operator notation and define, for any g : S →R and Mlat =
(rlat, Plat),
[T Mlat
h
g](s, a) = rlat(s, a) + Es′∼Plat(s,a)[g(s′)].
Lemma D.7. Let Mobs = ⟪Mlat, ψ⋆⟫, ϕ⋆:= (ψ⋆)−1, ϕ ∈Φ, and Γϕ be the mismatch function for
emission ψ⋆(Definition D.1). Then, for any flat : S × A →R, h ∈[H], and (x, a) ∈X × A, we
have
h
T Mobs
h
(flat ◦ϕh+1)
i
(x, a) =
h
T Mlat
h
(Γϕ,h+1 ◦Vflat)
i
(ϕ⋆
h(x), a).
26

Proof of Lemma D.7. Let f := flat, h ∈[H], and (x, a) ∈X × A be given. Then, we have:
h
T Mobs
h
(f ◦ϕh+1)
i
(x, a)
= rlat,h(ϕ⋆
h(x), a) + Esh+1∼Plat,h(ϕ⋆
h(x),a) Exh+1∼ψ⋆
h+1(sh+1)[Vf(ϕ(xh+1))]
= rlat,h(ϕ⋆
h(x), a) + Esh+1∼Plat,h(ϕ⋆
h(x),a)

X
xh+1∈X
ψ⋆(xh+1 | sh+1)Vf(ϕ(xh+1))


= rlat,h(ϕ⋆
h(x), a) + Esh+1∼Plat,h(ϕ⋆
h(x),a)
"X
s′∈S
Γϕ(s′ | sh+1)Vf(s′)
#
= rlat,h(ϕ⋆
h(x), a) + Esh+1∼Plat,h(ϕ⋆
h(x),a)[Γϕ ◦Vf(sh+1)]
=
h
T Mlat
h
(Γϕ ◦Vf)
i
(ϕ⋆
h(x), a),
where the third line follows from the definition of the mismatch function Γϕ.
We next show that the mismatch functions can be used to realize the pushforward dynamics ϕ♯M ⋆
obs,
which we recall are defined as:

ϕ♯M ⋆
obs,h

(r, s′ | x, a) =
X
x′:ϕ(x′)=s′
M ⋆
obs,h(r, x′ | x, a).
(14)
We also recall the notation [Γϕ,h+1 ◦Mlat]h, defined via:
[Γϕ ◦Mlat]h(rh, sh+1 | sh, ah) :=
X
s′
h+1∈S
Mlat,h(rh, s′
h+1 | sh, ah)Γϕ,h+1(sh+1 | s′
h+1).
Lemma D.8 (Pushforward model realizability via mismatch functions). For all ϕ ∈Φ, h ∈[H], we
have:
[ϕh+1♯M ⋆
obs,h](· | x, a) =

[Γϕ ◦M ⋆
lat]h ◦ϕ⋆
h

(· | x, a)
(15)
Proof of Lemma D.8. Note that Γϕ can alternatively be written as:
Γϕ,h(s′
h | sh) =
X
xh:ϕ(xh)=s′
h
ψ⋆
h(xh | sh).
We have
ϕh+1♯M ⋆
obs,h(rh+1, sh+1 | xh, ah)
=
X
xh+1:ϕh+1(xh+1)=sh+1
M ⋆
obs,h(rh+1, xh+1 | xh, ah)
=
X
xh+1:ϕh+1(xh+1)=sh+1


X
r,s′∈R×S
M ⋆
lat,h(r, s′ | ϕ⋆
h(xh), ah)ψ⋆
h+1(xh+1 | s′)


=
X
r,s′∈R×S
M ⋆
lat,h(r, s′ | ϕ⋆
h(xh), ah)
X
xh+1:ϕh+1(xh+1)=sh+1
ψ⋆
h+1(xh+1 | s′)
=
X
r,s′∈R×S
M ⋆
lat,h(r, s′ | ϕ⋆
h(xh), ah)Γϕ,h+1(s′ | sh+1)
= [Γϕ ◦M ⋆
lat]h(r, sh+1 | ϕ⋆
h(xh), ah),
as desired.
27

E
Proofs and Additional Results for Section 3.2: Impossibility Results
This section contains additional information and proofs related to our impossibility results regarding
statistical modularity (Section 3.2), and is organized as follows:
• Appendix E.1 contains the statement for an additional lower bound that is useful for establishing
the impossibility results of Figure 1.
• Appendix E.2 contains details for each entry of Figure 1.
• Appendix E.3 contains for proofs for our main lower bound (Theorem 3.1) and the additional lower
bound (Theorem E.1).
E.1
Additional Lower Bound
Theorem E.1 (Alternative lower bound). For every N ≥4, there exists an emission class Ψ and a
decoder class Φ with |Ψ| = |Φ| = N and a family of latent MDPs Mlat satisfying (i) |Mlat| = 1,
(ii) H = 1, (iii) |S| = |X| = N, (iv) |A| = N, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0.
2. For an absolute constant c > 0, comp(⟪Mlat, Φ⟫, c, c) ≥Ω(N/ log(N)).
Proof of Theorem E.1. See Appendix E.3.2.
E.2
Details for Figure 1
Below, we provide details on each entry in Figure 1. More precisely, for each latent class Mlat, we
will give a (brief) description of the MDP class Mlat, give our choice of latent complexity comp for
Mlat, and prove that the class is or is not statistically modular for that choice of latent complexity.
We view our choices of latent complexities as natural complexities for the respective classes.
Tabular MDPs (✓).
• Latent class Mlat: Tabular MDPs Mlat = (S, A, Plat, Rlat, H). [AOM17]
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(|S|, |A|, H, ε−1, log δ−1), which is
attainable, for example, via the UCB-VI algorithm of Azar et al. [AOM17]
• Statistical modularity (✓): Known Block MDP algorithms (e.g.
MUSIK [MFR23], BRIEE
[ZSUWAS22]) have sample complexities of poly(|S|, |A|, H, ε−1, log δ−1, log |Φ|).
Contextual Bandits (✓).
• Latent class Mlat: Contextual bandits with context space S, action space A, reward function
r⋆
lat : S × A →[0, 1] and a finite realizable function class satisfying r⋆∈Flat.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(|A|, log|Flat|, ε−1, log δ−1), attain-
able via, e.g., the SQUARE-CB algorithm [FR20].
• Statistical modularity (✓): We note that Flat ◦Φ = {[f ◦ϕ] | f ∈F, ϕ ∈Φ} is a realizable
function class for the observation-level reward function r⋆
obs, since r⋆
obs = [r⋆
lat ◦ϕ⋆] ∈Flat ◦Φ.
Thus, applying the SQUARE-CB algorithm directly on the observations x(t), a(t), r(t) will give
complexity poly(|A| log(|Flat||Φ|), ε−1, log δ−1) = poly(|A|, log |Flat|, log |Φ|, ε−1, log δ−1).
Low-rank MDP (✓).
• Latent class Mlat: MDPs Mlat = (S, A, H, Plat, rlat) such that there exists µ⋆
lat,h ∈Rd,
θ⋆
lat,h ∈Rd, and a known set of features Ξlat =
n
ξlat =

ξlat,h : S × A →Rd	H
h=1
o
such that
for all h ∈[H] we have rlat(sh, ah) = ⟨ξ⋆
lat,h(sh, ah), θ⋆
lat,h⟩as well as
Plat,h(sh+1 | sh, ah) = ⟨ξ⋆
lat,h(sh, ah), µ⋆
lat,h+1(sh+1)⟩
(16)
for some ξ⋆
lat ∈Ξlat.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, |A|, H, log |Ξlat|, ε−1, log δ−1),
which is attainable via the VOX algorithm of Mhammedi et al. [MBFR23].
28

• Statistical modularity (✓): This is obtained by noting that the observation-level dynamics also
satisfy the low-rank property with the same dimension. Formally, letting Pobs be the transition
kernel for ⟪Mlat, ψ⋆⟫and ϕ⋆= (ψ⋆)−1, we have
Pobs,h(xh+1 | xh, ah) =
X
sh+1∈S
Plat,h(sh+1 | ϕ⋆
h(xh), ah)ψ⋆
h+1(xh+1 | sh+1)
=
X
sh+1∈S

ξ⋆
lat,h(ϕ⋆
h(x), a), µ⋆
lat,h+1(sh+1)

ψ⋆
h+1(xh+1 | sh+1)
=
*
ξ⋆
lat,h(ϕ⋆
h(x), a),
X
sh+1∈S
µ⋆
lat,h+1(sh+1)ψ⋆
h+1(xh+1 | sh+1)
+
.
Thus,
the
transition
kernel
Pobs
is
a
low-rank
MDP
with
µobs,h+1(xh+1)
:=
P
sh+1 µ⋆
lat,h+1(sh+1)ψ⋆
h+1(xh+1 | sh+1) and feature class
Ξlat ◦Φ =
n
ξlat ◦ϕ = {ξh ◦ϕh : x, a 7→ξh(ϕh(x), a)}H
h=1 | ξlat ∈Ξlat, ϕ ∈Φ
o
.
Lastly, since robs = [rlat ◦ϕ⋆], the reward function is also linear with the same unknown feature
class. Thus we can apply VOX directly on top of the observations, with the feature class Ξlat ◦Φ,
which will achieve a complexity poly(d, |A|, H, log|Ξlat|, log|Φ|, ε−1, log
 δ−1
).
Known Deterministic MDP (|Mlat| = 1) (✓).
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs of size 1 with both
deterministic rewards and deterministic transitions.
• Latent complexity comp: We take comp(Mlat, ε, δ) = 0, which is attainable as Mlat is known and
we can simply deploy its optimal policy.
• Statistical modularity (✓): We note that, due to determinism, the latent optimal policy can be
chosen to be open-loop without loss of generality, and thus will always experience the same
trajectory (s⋆
1, a⋆
1, . . . , s⋆
H, a⋆
H). We can define the observation-level policy which commits to this
same sequence of actions, i.e. πobs,h(xh) = a⋆
h for all xh. This will be an optimal policy for any
Mobs = ⟪Mlat, ψ⟫, and can also be learned in 0 samples.
Low State Occupancy (∀π : S →∆(A)) (✓).
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs for which
we have a realizable value function class, and such that there exists a feature map ζlat =

ζlat,h : S →Rd	H
h=1 such that for all π : S →∆(A) and for all Mlat ∈Mlat, we have
∀h ∈[H] ∃θMlat,π
h
:
dMlat,π
h
(s) =
D
ζlat,h(s), θMlat,π
h
E
.
Note that the feature map does not need to be known.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, |A|, H, log|Flat|, ε−1, log
 δ−1
),
which is attainable by the BILIN-UCB algorithm of Du et al., since i) MDPs with this property have
Bilinear rank bounded by d|A| (see Definition 4.3 and Lemma 4.6 of Du et al. [Du+21]), and ii)
one can construct the value function class Flat = {QMlat,⋆| Mlat ∈Mlat}, which is realizable
and has size log|Flat| = log|Mlat|.
• Statistical modularity (✓): We firstly note that one can construct a realizable value function class
for the set ⟪Mlat, Φ⟫, via the set Fobs =

QMlat,⋆◦ϕ | Mlat ∈Mlat, ϕ ∈Φ
	
. This is realizable
since, for any Mobs := ⟪Mlat, ψ⟫, letting ϕ⋆= ψ−1, we have QMobs,⋆= QMlat,⋆◦ϕ⋆, and that this
class has size log|Mlat||Φ|. We can then show that the occupancies dMobs,πfobs, for fobs ∈Fobs,
can also be expressed as d-dimensional linear function for an appropriate choice of features,
which will imply that the BILIN-UCB algorithm run directly on Mobs will attain a complexity of
poly(d, |A|, H, log Mlat, log Φ, ε−1, log
 δ−1
). To obtain this, we recall the following lemma:
Lemma D.2. Let {Γϕ}ϕ∈Φ denote the mismatch functions for emission ψ⋆, and let Mobs =
⟪Mlat, ψ⋆⟫. Then, for any πlat ∈Πlat, ϕ ∈Φ, h ∈[H], x ∈X, we have
dMobs,πlat◦ϕ
h
(x) = ψ⋆
h(x | ϕ⋆
h(x))dMlat,Γϕ◦πlat
h
(ϕ⋆
h(x)).
29

Thanks to the above lemma, we have
dπf◦ϕ
obs (xh) = ψ(xh | ϕ⋆(xh))dΓϕ◦πf
lat
(ϕ⋆(xh))
= ψ(xh | ϕ⋆(xh))
D
[ζlat,h ◦ϕ⋆
h](xh), θMlat,Γϕ◦πf
h
E
=
D
ψ(xh | ϕ⋆(xh))[ζlat,h ◦ϕ⋆
h](xh), θMlat,Γϕ◦πf
h
E
and so dπf◦ϕ
obs
is linear with feature mapping ψ(xh | ϕ⋆(xh))[ζlat,h ◦ϕ⋆
h] and parameter θMlat,Γϕ◦πf .
Recall that the feature map need not be known, so that BILIN-UCB can still be applied despite not
knowing ψ and ϕ⋆.
Model class + Pushforward Coverability (✓).
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs that all satisfy
pushforward coverability Cpush(Mlat) ≤Cpush (cf. Eq. (28) for the definition).
• Latent
complexity
comp:
We
take
comp(Mlat, ε, δ)
=
poly(Cpush, |A|, H, log|Mlat|, ε−1, log
 δ−1
), which is attainable by the GOLF algorithm
via the results of Xie et al. [XFBJK23] (see also Lemma F.3). We obtain this by noting that i)
Ccov ≤Cpush|A|, where Ccov is defined in Definition 2 of Xie et al. [XFBJK23], and ii) a realizable
model class can be used to construct a realizable value function class F and a Bellman-complete
value function helper class G with sizes log|F| = log|M| and log|G| = O(log|M|).
• Statistical modularity (✓): This is obtained via Theorem 3.2.
Linear CB/MDP (✗⋆).
• Latent class Mlat: MDPs Mlat = (S, A, Plat, Rlat, H) that are linear with respect to a known
feature map ξ⋆
lat : S × A →Rd (i.e. such that Eq. (16) holds for ξ⋆
lat).
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, H, ε−1, log
 δ−1
), which is attain-
able via the LSVI-UCB algorithm of Jin et al. [JYWJ20]. Note that this guarantee does not depend
on the number of actions.
• Statistical intractability (✗): The latent model used in the construction of Theorem E.1 is a set (of
size 1) of linear MDPs with d = 1. In particular, that construction was a contextual bandit so we
only have to realize a reward function, and since there is only one latent model so we can trivially
embed this with d = 1 via ξ⋆
lat(s, a) = rlat(s, a), where rlat is the reward function of the MDP
used in Theorem E.1.
• Statistical modularity with additional |A|-dependence: As in the Low-rank MDP case above,
⟪Mlat, ψ⟫is low-rank with unknown feature set Φ′ = {ξ⋆
lat ◦ϕ | ϕ ∈Φ}. Thus, by the same
conclusion, a the VOX algorithm will have complexity poly(d, |A|, H, log |Φ|), which is of the
desired form if we allow suboptimal dependence on |A|.
Model class + Coverability (∀πM : M ∈M) (✗).
• Latent assumption: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs that all satisfy
coverability with respect to the policy class ΠM = {πM | M ∈M}, i.e. we have
∀Mlat ∈Mlat :
Ccov(Mlat) =
inf
µh∈∆(S×A) sup
h∈[H]
sup
π∈ΠM

d
Mlat,π
h
µh

∞
< ∞
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(Ccov, H, log|Mlat|, ε−1, log
 δ−1
),
which is attainable by the GOLF algorithm via the results of Xie et al. [XFBJK23] (see also
Lemma F.3). We obtain this by noting that a realizable model class can be used to construct a
realizable value function class F and a complete value function class G of sizes log|F| = log|M|
and log|G| = O(log|M|).
• Statistical intractability (✗): The latent models used in the construction of Theorem 3.1 are a set of
coverable MDPs – in particular, these are trivially coverable with Ccov = 1 since there is a single
latent model and we can take µ = dM ⋆
lat,πM⋆
lat. We remark that it is an interesting open question
whether this impossibility result continues to hold if we require coverability with respect to the
class Π of all possible latent policies.
30

Known Stochastic MDP (|Mlat| = 1) (✗).
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs of size 1.
• Latent complexity comp: We take comp(Mlat, ε, δ) = 0, which is attainable as Mlat is known and
we can simply deploy its optimal policy.
• Statistical intractability (✗): This is precisely the setting of Theorem 3.1, which shows that at least
Ω(N/ log(N)) samples will be needed, where N = |Φ|.
Bellman rank (Q-type or V -type) (✗)
• Latent assumption: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of latent models such that
each Mlat ∈Mlat has Q-type Bellman rank d or V -type Bellman rank d [JLM21]. Letting F be a
realizable value function class for Mlat, in the Q-type case, this means that the |ΠF| × |F| matrix
EQ
h (π, f) = Eπh
fh(sh, ah) −rh −max
a′ fh+1(sh+1, a′)
i
,
admits a rank d factorization. In the V -type case, the matrix
EV
h (π, f) = Esh∼dπ
h,ah∼πf
h
fh(sh, ah) −rh −max
a′ fh+1(sh+1, a′)
i
admits a rank-d matrix factorization.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, H, |A| log|F|, ε−1, log
 δ−1
)
for the V -type Bellman rank case, which is achievable by the OLIVE algorithm of Jiang et al.
[JKALS17], and comp(Mlat, ε, δ) = poly(d, H, log|F|, ε−1, log
 δ−1
) for Q-type Bellman rank,
which is achievable by the BILIN-UCB algorithm of Du et al. [Du+21].
• Statistical intractability (✗): We note that the construction in Theorem 3.1 has |Mlat| = 1,
which trivially has Bellman rank equal to 1, so Theorem 3.1 precludes statistical modularity with
complexity comp.
Eluder dimension + Bellman Completeness (✗)
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs such that there is a
function class Flat satisfying
∀flat ∈Flat, Mlat ∈Mlat :
T Mlatflat ∈Flat.
Furthermore, each Mlat ∈Mlat has Bellman-Eluder dimension bounded by d (see Definition 8 of
[JLM21]).
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, H, log|F|, ε−1, log
 δ−1
), which
is attainable by the GOLF algorithm of Jin et al. [JLM21].
• Statistical intractability (✗): As in the Bellman rank case, the construction in Theorem 3.1 has
|Mlat| = 1, so we can take Flat = {QMlat,⋆| Mlat ∈Mlat} which is evidently complete
for T Mlat, and has Eluder dimension 1, so Theorem 3.1 precludes statistical modularity with
complexity comp.
Q⋆-irrelevant State Abstraction (✗)
• Latent class Mlat: Mlat = (S, A, Plat, Rlat, H) such that there is a known state abstraction
function ζlat : S →Z such that ζlat(s) = ζlat(s′) implies that QMlat,⋆(s, a) = QMlat,⋆(s′, a) for
all a ∈A.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(|Z|, |A|, H, ε−1, log
 δ−1
) which is
attainable by the OLIVE algorithm of Jiang et al. [JKALS17].
• Statistical intractability (✗): We take Mlat = {Mlat} as the MDP class from the construction of
Theorem 3.1. Let Q⋆
lat := QMlat,⋆. Note that we have Q⋆
lat(s, a) ∈{0, 1} for all s, a, so we can take
a latent abstract state space Z = {(0, 0), (0, 1), (1, 0), (1, 1)} and a state abstraction function ζlat
such that ζlat(s) = (i, j) if Q⋆
lat(s, 0) = i and Q⋆
lat(s, 1) = j. This satisfies the property of a Q⋆-
irrelevant abstraction, since ζlat(s) = ζlat(s′) = (i, j) implies that Q⋆
lat(s, 0) = Q⋆
lat(s′, 0) = i
and Q⋆
lat(s, 1) = Q⋆
lat(s′, 1) = j. This has a constant-sized abstract space (|Z| = 4) and |A| = 2,
so Theorem 3.1 precludes statistical modularity with complexity comp.
31

Linear Mixture MDP (✗).
• Latent class Mlat: MDPs Mlat = (S, A, Plat, Rlat, H) such that there is a known feature map
ζlat = {ζlat,h : s′, s, a 7→Rd}H
h=1 such that
∀h ∈[H], ∃θh ∈Rd :
Plat,h(s′ | s, a) = ⟨ζlat,h(s′ | s, a), θh⟩
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, H, ε−1, log
 δ−1
), which is attain-
able by the UCRL-VTR+ algorithm of Zhou et al. [ZGS21]
• Statistical intractability (✗): We take Mlat = {Mlat} to be the construction of Theorem 3.1. Here,
there is a single latent model, so this is trivially embeddable with ζlat,h(s′ | s, a) = P ⋆
lat,h(s′ |
s, a) ∈R1. This has dimension d = 1, so Theorem 3.1 precludes statistical modularity with
complexity comp.
Linear Q⋆/V ⋆(✗).
• Latent class Mlat: MDPs Mlat = (S, A, Plat, Rlat, H) such that there are known features
maps αlat : S × A →Rd and βlat : S →Rd such that for all Mlat ∈Mlat, there exists
unknown parameters θQ, θV ∈Rd such that QMlat,⋆(s, a) = ⟨αlat(s, a), θQ⟩and V Mlat,⋆(s) =
⟨βlat(s), θV ⟩.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, H, ε−1, log
 δ−1
), which is attain-
able by the BILIN-UCB algorithm of Du et al. [Du+21].
• Statistical intractability (✗): We can take Mlat to be the latent MDP class from the construction
of Theorem 3.1. Since there is a single latent model, this is trivially embeddable with dimension
1, i.e. we can take ζlat(s, a) = Q⋆
lat(s, a) and βlat(s) = V ⋆
lat(s). This has dimension d = 1, so
Theorem 3.1 precludes statistical modularity with complexity comp.
Low State or State-Action Occupancy (∀πM : M ∈M) (✗).
• Latent class Mlat: In the Low State Occupancy model, Mlat = {Mlat = (S, A, Plat, Rlat, H)}
is a set of MDPs such that there exists a feature map ζV
lat =

ζlat,h : S →Rd	H
h=1 such that for
all π ∈{πMlat | Mlat ∈Mlat} and for all Mlat ∈Mlat, we have
∀h ∈[H] ∃θMlat,π
h
:
dMlat,π
h
(s) =
D
ζV
lat,h(s), θMlat,π
h
E
.
For the State-Action Occupancy model, we have that there exists a feature map ζQ
lat
=

ζlat,h : S × A →Rd	H
h=1 such that for all π ∈{πMlat | Mlat ∈Mlat} and for all Mlat ∈
Mlat, we have
∀h ∈[H] ∃θMlat,π
h
:
dMlat,π
h
(s, a) =
D
ζQ
lat,h(s, a), θMlat,π
h
E
.
Note that the feature map does not need to be known in either case.
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(d, |A|, H, log|Flat|, ε−1, log
 δ−1
)
for the state occupancy case and comp(Mlat, ε, δ) = poly(d, H, log|Mlat|, ε−1, log
 δ−1
). Both
are attainable by the BILIN-UCB algorithm of Du et al., since i) MDPs with this property have
Bilinear rank bounded by d|A| and d respectively (see Definition 4.3 and Lemma 4.6 of [Du+21]),
and ii) one can construct the value function class Flat = {QMlat,⋆| Mlat ∈Mlat} which is
realizable and has size log|Flat| = log|Mlat|.
• Intractability: We can take the construction of Theorem 3.1, which has |Mlat| = 1 and thus is
trivially embeddable with dimension 1, i.e. we can take ζV
lat(s) = dMlat,πMlat(s) and ζQ
lat(s, a) =
dMlat,πMlat (s, a).
Bisimulation (?)
• Latent class Mlat: MDPs Mlat = (S, A, Plat, Rlat, H) such that there is a known state abstraction
function ζlat : S →Z such that ζlat(s) = ζlat(es) implies that Rlat(s, a) = Rlat(es, a) for all
a ∈A as well as P
s′:ζlat(s′)=z′ Plat(s′ | s, a) = P
s′:ζlat(s′)=z′ Plat(s′ | es, a) for all z′.
32

• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(|Z|, |A|, H, ε−1, log
 δ−1
) which is
attainable by the OLIVE algorithm of [JKALS17].
• Openness (?): A negative result does not follow from existing constructions, since the dynamics
from the tree-based construction of Theorem 3.1 are not bisimilar unless |Z| = |S|, which allows for
the application of tabular methods. At the same time, a positive result does not follow from existing
methods, since it is non-trivial to extend existing Block MDP methods to use the bisimulation state
abstraction in a way that only pays for |Z|.
Low State-Action Occupancy (∀π : S →∆(A)) (?⋆)
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs such that there
exists a feature map ζQ
lat =

ζlat,h : S × A →Rd	H
h=1 such that for all π : S →∆(A) and for
all Mlat ∈Mlat, we have
∀h ∈[H] ∃θMlat,π
h
:
dMlat,π
h
(s, a) =
D
ζQ
lat,h(s, a), θMlat,π
h
E
.
Note that the feature map does not need to be known.
• We take comp(Mlat, ε, δ) = poly(d, H, log|Mlat|, ε−1, log
 δ−1
), which is attainable by the
BILIN-UCB algorithm of Du et al., since i) MDPs with this property have Bilinear rank bounded
by d (see Definition 4.3 and Lemma 4.6 of [Du+21]), and ii) one can construct a realizable value
function class of size log|F| = log|M|.
• Openness (?): A negative result does not follow from existing constructions, since the dynamics
from the tree-based construction of Theorem 3.1 do not have linear occupancies for all π : S →
∆(A) unless d = |S|, which allows for the application of tabular methods, and the dynamics from
the bandit-based construction Theorem E.1 do not have linear occupancies for all π : S →∆(A)
unless d = |A|. At the same time, unlike the low state occupancy case, a positive result does not
follow as it is unclear if we can express the observation-space occupancies linearly.
• Statistical tractability with additional (suboptimal) |A|-dependence (✓): Note that we can reduce
to the Low State Occupancy case (✓), since
dπ(s) =
X
a∈A
dπ(s, a) =
*
θπ,
X
a∈A
ζQ
lat(s, a)
+
:=

θπ, ζV
lat(s)

.
However, this blows up the feature norm bound of the feature map ζV
lat(s) by a factor of |A|, which
will appear logarithmically in the bound obtained by BILIN-UCB.
Model class + Coverability (∀π : S →∆(A)) (?).
• Latent class Mlat: Mlat = {Mlat = (S, A, Plat, Rlat, H)} is a set of MDPs that all satisfy
coverability with respect to all policies πlat : S →∆(A), i.e. we have
∀Mlat ∈Mlat :
Ccov(Mlat) =
inf
µh∈∆(S×A) sup
h∈[H]
sup
π:S→∆(A)

d
Mlat,π
h
µh

∞
< ∞
• Latent complexity comp: We take comp(Mlat, ε, δ) = poly(Ccov, H, log|Mlat|, ε−1, log
 δ−1
),
which is attainable by the GOLF algorithm via the results of Xie, Foster, Bai, Jiang, and Kakade (see
also Lemma F.3). We obtain this by noting that a realizable model class can be used to construct a
realizable value function class F and a complete value function class G of sizes log|F| = log|M|
and log|G| = O(log|M|).
• Openness (?): A negative result does not follow from the existing constructions. The tree-based
construction of Theorem 3.1 satisfies coverability with Ccov = exp(Ω(H)) and the bandit-based
construction of Theorem E.1 satisfies coverability with Ccov = |A|. In both cases, the lower bounds
cannot be used to rule out statistical modularity with the above latent complexity. Similarly, it
unclear how to obtain a positive result for the latent-dynamics class ⟪Mlat, Φ⟫.
33

E.3
Proofs for Lower Bounds (Theorems 3.1 and E.1)
E.3.1
Main lower bound (Theorem 3.1)
We will prove the following result.
Theorem 3.1 (Impossibility of statistical modularity). For every N ≥4, there exists a decoder class
Φ with |Φ| = N and a family of base MDPs Mlat satisfying (i) |Mlat| = 1, (ii) H ≤O(log(N)),
(iii) |S| = |X| ≤N 2, (iv) |A| = 2, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0.
2. For an absolute constant c > 0, comp(⟪Mlat, Φ⟫, c, c) ≥Ω(N/ log(N)).
Proof.
Let N be given and assume without loss of generality that it is a power of 2. We first
construct the class of latent-dynamics MDPs, following Song et al. [SWFK24].
Latent MDP.
The construction has a single “known” latent MDP Mlat, so that the only uncertainty
in the family of latent-dynamics MDPs we construct arises from the emission processes. We set
Mlat = {Mlat}. Set H = log2(N) + 1 and A = {0, 1}. We define the state space and latent
transition dynamics as follows.
• The state space can be partitioned as S = S1, . . . , SN.
• Each block Si corresponds to a standard depth-H binary tree MDP with deterministic dynamics
(e.g., Osband et al.; Domingues et al. [OVR16; DMKV21]). There is a single “root” node at layer
h = 1, which we denote by si
root, and N “leaf” nodes at layer H, which we denote by

si,j
leaf
	
j∈[N].
For each h = 1, . . . , H −1, choosing action 0 leads to the left successor of the current state
deterministically, and choosing action 1 leads to the right sucessor; this process continues until
we reach a leaf node at layer H.
• The initial state distribution is Plat,1(∅) = Unif(s1
root, . . . , sN
root).
• There are no rewards for layers 1, . . . , H −1. For layer H, the reward is
RH(si,j
leaf, ·) = I{j = i}.
(17)
This construction can summarized as follows. At layer 1, we draw the index of one of N binary trees
uniformly at random, and initialize into the root of the tree. From here, we receive a reward of 1 if
we successfully navigate to the leaf node whose index agrees with the index of the tree itself, and
receive a reward of 0 otherwise.
Note that the total number of latent states in this construction is |S| = N · |S1| = N(2N −1)
Observation space and decoder class.
Let us introduce some additional notation. For each
block Si, let Si
h := {si,j
h }j∈[2h−1] denote the states in block i that are reachable at layer h, so that
Si
1 =

si
root
	
and Si
H = {si,j
leaf}j∈[N]. We define X = S so that |X| ≤4N 2, and consider a class of
emission processes corresponding to deterministic maps. Let Σ denote the set of cyclic permutations
on N elements, excluding the identity permutation. That is, each σi ∈Σ takes the form
σi : k 7→k + i
mod N
for i ∈{1, . . . , N}.
For each σ ∈Σ, we consider the emission process
ψσ
h(· | s
(i,j)
h
) = Is(σ(i),j)
h
.
That is, ψσ shifts the index of the binary tree containing s
(i,j)
h
according to σ. Let Ψ = {ψσ | σ ∈Σ}.
Consider the decoder class
Φ = Ψ−1 :=

si 7→sψ−1(i) | ψ ∈Ψ
	
,
which has |Φ| = N. We consider the class of rich-observation MDPs given by
⟪Mlat, Φ⟫:=

M i := ⟪Mlat, ψσi⟫| σi ∈Σ
	
.
(18)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions
Ψ.
34

Sample complexity lower bound.
To lower bound the sample complexity, we prove a lower bound
on the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP
M (defined over the space X) and ε ∈[0, 21/2], define18
decε(M, M) =
inf
p,q∈∆(Π) sup
M∈M

Eπ∼p[J
M(πM) −J
M(π)] | Eπ∼q

D2
H
 M(π), M(π)

≤ε2	
,
where M(π) denotes the law over trajectories (x1, a1, r1), . . . , (xH, aH, rH) induced by executing
the policy π in the MDP M, J M(π) denotes the expected reward for policy π under M, and πM
denotes the optimal policy for M. We further define
decε(M) = sup
M
decε(M, M),
where the supremum ranges over all MDPs defined over X and A. We now appeal to the following
technical lemma.
Lemma E.1. For all ε2 ≥4/N, we have that decε(⟪Mlat, Φ⟫) ≥1
2.
In light of Lemma E.1, it follows from Theorem 2.1 in Foster et al. [FGH23]19 that any PAC RL
algorithm that uses T episodes of interaction for T log(T) ≤c·N must have E[J M(πM) −J M(bπ)] ≥
c′ for a worst-case MDP in M, where c, c′ > 0 are absolute constants. This implies that any PAC RL
which has E[J M(πM) −J M(bπ)] ≤c′ must have T log(T) ≥c · N and thus T ≥c · N/ log(N).
Proof of Lemma E.1. Define M lat as the latent-space MDP that has identical dynamics to Mlat but,
has zero reward in every state, and define M := ⟪M lat, id⟫as the rich-observation MDP obtained
by composing M lat with the “identity” emission process id that sets xh = sh. Observe that M and
M i, induce identical dynamics in observation space if rewards are ignored: For all policies π,
P
M,π[(x1, a1), . . . , (xH, aH) = ·] = P
Mi,π[(x1, a1), . . . , (xH, aH) = ·].
(19)
It follows that for each i, for all policies π, we have
D2
H
 M i(π), M(π)

= D2
H
 (⟪Mlat, ψi⟫)(π), (⟪M lat, id⟫)(π)

=
N
X
j=1
P
M,π
xH = s
(ψi(j),j)
leaf

· D2
H(I1, I0)
= 2
N
X
j=1
P
M,π
xH = s
(ψi(j),j)
leaf

= 2
N
N
X
j=1
P
M,π
xH = s
(ψi(j),j)
leaf
| x1 = s
(ψi(j))
root

,
(20)
= 2
N
N
X
j=1
P
M,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i
,
(21)
since the learner receives identical feedback in the MDPs M i and M unless they reach the observation
xH = s
(ψi(j),j)
leaf
for some j (corresponding to latent state s
(j,j)
leaf in M i), in which case they receiver
reward 1 in M i but reward 0 in M. We now claim that for any q ∈∆(Π), there exists a set of at least
N/2 indices Iq ⊂[N] such that
Eπ∼q

D2
H
 M i(π), M(π)

≤4
N
(22)
18For measures P and Q, we define squared Hellinger distance by D2
H(P, Q) =
R
(
√
dP −√dQ)2.
19Theorem 2.1 in Foster et al. [FGH23] is stated with respect to supM∈conv(M) decε(M, M), but the actual
proof (Section 2.2) gives a stronger result that scales with supM decε(M, M).
35

for all i ∈Iq. To see this, note that by Eq. (21), we have
Ei∼Unif([N]) Eπ∼q

D2
H
 M i(π), M(π)

≤Eπ∼q

2
N
N
X
j=1
1
N
N
X
i=1
P
M,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i


≤Eπ∼q

2
N
N
X
j=1
1
N

= 2
N ,
where the second inequality uses that PN
i=1 PM,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i
≤1, as the events
in the sum are mutually exclusive (and the event we condition on does not depend on i). We conclude
by Markov’s inequality that Pi∼Unif([N])

Eπ∼q

D2
H
 M i(π), M(π)

≥4/N

≤1/2, giving Iq ≥
N/2.
From Eq. (26), we conclude that for all ε2 ≥4/N,
decε(M, M) ≥
inf
q∈∆(Π)
inf
p∈∆(Π) sup
i∈Iq
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
.
To lower bound this quantity, observe that for any index i and any policy π, we have
J
Mi(πMi) −J
Mi(π) = 1
N
N
X
j=1
P
M(i),π
xH ̸= s
(ψi(j),j)
leaf
| x1 = s
(ψi(j))
root

= 1 −1
N
N
X
j=1
P
M(i),π
xH = s
(ψi(j),j)
leaf
| x1 = s
(ψi(j))
root

= 1 −1
N
N
X
j=1
P
M,π
xH = s
(ψi(j),j)
leaf
| x1 = s
(ψi(j))
root

= 1 −1
N
N
X
j=1
P
M,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i
,
where the third inequality uses Eq. (19). We conclude that for any distribution p, q ∈∆(Π),
sup
i∈Iq
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
≥Ei∼Unif(Iq)
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
≥1 −1
N
N
X
j=1
Ei∼Unif(Iq) P
M,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i
= 1 −1
N
N
X
j=1
1
|Iq|
X
i∈Iq
P
M,πh
xH = s
(j,ψ−1
i
(j))
leaf
| x1 = s
(j)
root
i
≥1 −
1
|Iq| ≥1
2
as long as N ≥4, where the second-to-last inequality uses that for all j, the events

xH = s
(j,ψ−1
i
(j))
leaf
|
x1 = s
(j)
root
	
are disjoint for all i. Since this lower bound holds uniformly for all q, p ∈∆(Π), we
conclude that
decε(⟪Mlat, Φ⟫, M) ≥1
2.
E.3.2
Proof of alternative lower bound (Theorem E.1)
We will prove the following result.
36

Theorem E.1 (Alternative lower bound). For every N ≥4, there exists an emission class Ψ and a
decoder class Φ with |Ψ| = |Φ| = N and a family of latent MDPs Mlat satisfying (i) |Mlat| = 1,
(ii) H = 1, (iii) |S| = |X| = N, (iv) |A| = N, and such that
1. For all ε, δ > 0, we have comp(Mlat, ε, δ) = 0.
2. For an absolute constant c > 0, comp(⟪Mlat, Φ⟫, c, c) ≥Ω(N/ log(N)).
Proof of Theorem E.1. We repeat more or less repeat the same proof as Theorem 3.1, but with the
appropriate modifications to translate from the contextual tree-based construction in Theorem 3.1
to the contextual bandit-based construction in the theorem statement. Let N be given and assume
without loss of generality that it is a power of 2.
Latent MDP.
Our construction has a single “known” latent MDP Mlat; that is, the only uncertainty
in the family of rich-observation MDPs we construct arises from the emission processes. Set
Mlat = {Mlat}. Set H = 1 and A = [N]. We define the state space and latent transition dynamics
as follows.
• The state space can be partitioned as S = S1, . . . , SN.
• Each block Si corresponds to a single state si with N actions denoted by ai, i ∈[N].
• The initial state distribution is Plat,1(∅) = Unif(s1, . . . , sN).
• The reward function is
R1(si, aj) = I{j = i}.
(23)
Informally, this construction can summarized as a contextual bandit (with uniform context distribu-
tion), with a reward of 1 if and only if we play the action corresponding to the index of the context
drawn.
Note that the total number of latent states in this construction is |S| = N and the number of actions
is |A| = N.
Observation space and decoder class.
We define X = S so that |X| = |S|, and consider a class of
emission processes corresponding to deterministic maps. Let Σ denote the set of cyclic permutations
on N elements, excluding the identity permutation. That is, each σi ∈Σ takes the form
σi : k 7→k + i
mod N,
for i ∈{1, . . . , N}.
For each σ ∈Σ, we consider the emission process
ψσ(· | si) = Isσ(i)(·)
That is, ψσ shifts the context si according to σ. Let Ψ = {ψσ | σ ∈Σ}. Consider the decoder class
Φ = Ψ−1 :=

si 7→sψ−1(i) | ψ ∈Ψ
	
,
which has |Φ| = N. We consider the class of rich-observation MDPs given by
⟪Mlat, Φ⟫:=

M i := ⟪Mlat, ψσi⟫| σi ∈Σ
	
.
(24)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions
Ψ.
Sample complexity lower bound.
To lower bound the sample complexity, we prove a lower bound
on the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP
M (defined over the space X) and ε ∈[0, 21/2], define20
decε(M, M) =
inf
p,q∈∆(Π) sup
M∈M

Eπ∼p[J
M(πM) −J
M(π)] | Eπ∼q

D2
H
 M(π), M(π)

≤ε2	
,
where M(π) denotes the law over observations (x1, a1, r1) induced by executing the policy π in the
MDP M, J M(π) denotes the expected reward for policy π under M, and πM denotes the optimal
policy for M. We further define
decε(M) = sup
M
decε(M, M),
where the supremum ranges over all MDPs defined over X and A. We now appeal to the following
technical lemma.
20For measures P and Q, we define squared Hellinger distance by D2
H(P, Q) =
R
(
√
dP −√dQ)2.
37

Lemma E.2. For all ε2 ≥4/N, we have that supM decε(M, M) ≥1
2.
In light of Lemma E.2, it follows from Theorem 2.1 in Foster et al. [FGH23]21 that any PAC RL algo-
rithm that uses T episodes of interaction for T log(T) ≤c · N must have E[J M(πM) −J M(bπ)] ≥c′
for a worst-case MDP in M, where c, c′ > 0 are absolute constants. This implies that any PAC RL
which has E[J M(πM) −J M(bπ)] ≤c′ must have T log(T) ≥c · N and thus T ≥c · N/ log(N).
Proof of Lemma E.2. Define M lat as the latent-space MDP that has identical dynamics to Mlat
but, has zero reward for every state-action pair, and define M := ⟪M lat, id⟫as the rich-observation
MDP obtained by composing M lat with the identity emission process that sets xh = sh. In the rest
of the proof, we use the shorthand ψi := ψσi. Observe that M and M i, induce identical dynamics in
observation space if rewards are ignored, i.e. for all policies π : X →∆(A),
P
M,π[(x1, a1) = ·] = P
Mi,π[(x1, a1) = ·].
(25)
It follows that for each i, for all policies π, we have
D2
H
 M i(π), M(π)

= D2
H
 (⟪Mlat, ψi⟫)(π), (⟪M lat, id⟫)(π)

=
N
X
j=1
P
M,πh
x1 = sψi(j), a1 = aji
· D2
H(I1, I0)
= 2
N
X
j=1
P
M,πh
x1 = sψi(j), a1 = aji
= 2
N
N
X
j=1
P
M,πh
a1 = aj | x1 = sψi(j)i
= 2
N
N
X
j=1
P
M,πh
a1 = aψ−1
i
(j) | x1 = sji
since the learner receives identical feedback in the MDPs M i and M unless they play the action
a1 = aj given observation x1 = sψi(j) (corresponding to latent state si in M i), in which case they
receiver reward 1 in M i but reward 0 in M. We now claim that for any q ∈∆(Π), there exists a set
of at least N/2 indices Iq ⊂[N] such that
Eπ∼q

D2
H
 M i(π), M(π)

≤4
N
(26)
for all i ∈Iq. To see this, note that by Eq. (21), we have
Ei∼Unif([N]) Eπ∼q

D2
H
 M i(π), M(π)

≤Eπ∼q

2
N
N
X
j=1
1
N
N
X
i=1
P
M,πh
a1 = aψ−1
i
(j) | x1 = j
i


≤Eπ∼q

2
N
N
X
j=1
1
N

= 2
N .
We conclude by Markov’s inequality that Pi∼Unif([N])

Eπ∼q

D2
H
 M i(π), M(π)

≥4/N

≤1/2,
giving Iq ≥N/2.
From Eq. (26), we conclude that for all ε2 ≥4/N,
decε(⟪Mlat, Φ⟫, M) ≥
inf
q∈∆(Π)
inf
p∈∆(Π) sup
i∈Iq
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
.
21Theorem 2.1 in Foster et al. [FGH23] is stated with respect to supM∈conv(M) decε(M, M), but the actual
proof (Section 2.2) gives a stronger result that scales with supM decε(M, M).
38

To lower bound this quantity, observe that for any index i and any policy π, we have
J
Mi(πMi) −J
Mi(π) = 1 −1
N
N
X
j=1
P
M(i),π[a1 = a
(j) | x1 = s
(ψi(j))]
= 1 −1
N
N
X
j=1
P
M,π[a1 = a
(j) | x1 = s
(ψi(j))]
= 1 −1
N
N
X
j=1
P
M,πh
a1 = a
(ψ−1
i
(j)) | x1 = s
(j)i
,
where the third inequality uses Eq. (25). We conclude that for any distribution p, q ∈∆(Π),
sup
i∈Iq
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
≥Ei∼Unif(Iq)
n
Eπ∼p
h
J
Mi(πMi) −J
Mi(π)
io
≥1 −1
N
N
X
j=1
Ei∼Unif(Iq) P
M,πh
a1 = a
(ψ−1
i
(j)) | x1 = s
(j)i
= 1 −1
N
N
X
j=1
1
|Iq|
X
i∈Iq
P
M,πh
a1 = a
(ψ−1
i
(j)) | x1 = s
(j)i
≥1 −
1
|Iq| ≥1
2
as long as N ≥4. Since this lower bound holds uniformly for all q, p ∈∆(Π), we conclude that
decε(⟪Mlat, Φ⟫, M) ≥1
2.
39

F
Proofs for Section 3.3: Positive Results
This section is dedicated to our upper bound establishing that pushforward-coverable MDPs are
statistically modular (Theorem 3.2). We provide a technical overview in Appendix F.1, and provide a
full proof in Appendix F.2.
F.1
Technical Overview: Low-dimensional embeddings for pushforward-coverable MDPs.
The idea behind our positive result is to show that under the conditions of Theorem 3.2, it is possible
to construct an (approximately) Bellman-complete value function class for the latent-dynamics MDP
M ⋆
obs, at which point we can apply the GOLF algorithm of Jin et al. [JLM21]. We achieve this via two
technical contributions. The first is the introduction of the mismatch functions Γϕ, formally defined
as follows.
Definition F.1 (Mismatch functions). For a decodable emission process ψ⋆and decoder ϕ ∈Φ,
the mismatch function for ϕ, Γϕ = {Γϕ,h : S →∆(S)}H
h=1, is defined, for every h ∈[H], as the
probability kernel
Γϕ,h(s′
h | sh) := Pxh∼ψ⋆
h(sh)(ϕh(xh) = s′
h).
The mismatch functions allow us to express functions of the decoders as latent objects, and we revisit
them in the context of self-predictive estimation (Appendix A). For the present result, we show
(Lemma D.7) that the mismatch functions can capture the observation-level Bellman backups for
any function of the decoders. That is, for any xh, ah, letting sh = (ψ⋆)−1(xh) denote the true latent
state, we have that for any flat : S × A →R and ϕ ∈Φ:
[T M ⋆
obs
h
(flat ◦ϕh+1)](xh, ah) = [T M ⋆
lat
h
(Γϕ,h+1 ◦Vflat)](sh, ah).
(27)
That is, the Bellman update of flat ◦ϕh+1 in the latent-dynamics MDP M ⋆
obs can be expressed as
a Bellman update in the base MDP M ⋆
lat for a different (latent) function Γϕ,h+1 ◦Vflat(sh+1) :=
P
s′
h+1 Γϕ,h+1(s′
h+1 | sh+1) maxa′ flat(s′
h+1, a′).
However, the mismatch functions Γϕ embed some knowledge of the emission process, and (with only
decoder and base model realizability) are unknown to the learner. Our second technical contribution
bypasses this by establishing a new structural property for pushforward-coverable MDPs (Lemma F.1):
there exist low-dimensional linear embeddings of their transition kernels which can approximate
Bellman backups for an arbitrary and potentially unknown set of functions, as long as the set is not
too large.
Lemma F.1 (Pushforward-coverable MDPs admit low-dimensional embeddings). Let M be a
known MDP with reward function r, transition kernel P, and pushforward coverability parameter
Cpush. Let µ = {µh}h∈[H] denote its pushforward coverability distribution (i.e. the minimizer of
Definition 3.3) and F ⊆(S × [H] →[0, 1]) be an arbitrary class of functions. Suppose that we
sample W ∈{±1}d×S as a matrix of independent Rademacher random variables, and define
ψh(s, a) = rh(s, a) ⊕1
√
d
W

Ph(· | s, a)/µ1/2
h (·)

·∈S ∈Rd+1.
and
wf,h = 1 ⊕1
√
d
W

µ1/2
h (·)fh+1(·)

·∈S ∈Rd+1.
Then for any εapx ∈(0, 1), as long as we set
d ≥29 Cpush log
 16|F|Hδ−1/εapx

εapx
,
we have that for all f ∈F and h ∈[H], with probability at least 1 −δ:
Eµh⊗Unif(A)
h clip[0,2][⟨wf,h, ψh(s, a)⟩] −Thfh+1(s, a)
2i
≤εapx,
as well as maxs,a,h∥ψh(s, a)∥2
2
≤
Cpush(16 log(|S||A|H) + 11) and maxf,h∥wf,h∥2
2
≤
16 log(|F|H) + 11. We emphasize that the feature map ψ = {ψh}H
h=1 is oblivious to F, in the
sense that it can be computed directly from M without any knowledge of F.
We use this property, in conjunction with latent model realizability, to construct linear features that
can approximate the right-hand-side of Eq. (27), thus yielding an (approximately) Bellman-complete
value function class for the latent-dynamics MDP M ⋆
obs.
40

F.2
Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2)
In this section, we establish positive results under latent MDP classes which satisfy pushforward
coverability. We assume that every model in Mlat satisfies pushforward coverability, defined as
follows:
Definition F.2 (Pushforward coverability). The pushforward coverability coefficient Cpush for an
MDP M with transition kernel P is defined by
Cpush(M) = max
h∈[H]
inf
µ∈∆(S)
sup
(s,a,s′)∈S×A×S
Ph−1(s′ | s, a)
µ(s′)
.
(28)
The pushforward coverability coefficient for an MDP class M is defined by
Cpush(M) = max
M∈M Cpush(M).
Note that for any MDP M we always have
Ccov(M, Πrns) ≤Cpush(M)|A|,
(29)
where Ccov is the state-action coverability coefficient (Definition D.3). Thus, an MDP with low
pushforward coverability is also an MDP with low state-action coverability for all policies (upto a
dependence on |A|).
We will show the show the following result.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let Mlat be a base MDP
class such that each Mlat ∈Mlat has pushforward coverability bounded by Cpush(Mlat) ≤Cpush.
Then, for any decoder class Φ, we have:
1. comp(Mlat, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, ε−1, log
 δ−1
), and
2. comp(⟪Mlat, Φ⟫, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, log|Φ|, ε−1, log
 δ−1
, log log|S|).
The proof comes in three parts. We will firstly show that MDP that satisfies pushforward coverabil-
ity admit low-dimensional feature maps that can approximate Bellman backups (Appendix F.2.1),
then establish that a regret bound for the GOLF algorithm [XFBJK23] under misspecification (Ap-
pendix F.2.2), and then combine these ingredients (Appendix F.2.3).
F.2.1
A structural result: Pushforward-coverable MDPs are approximately low-rank
Our central technical result for this section is Lemma F.1, which is based on a variant of the Johnson-
Lindenstrauss lemma and establishes that under pushforward coverability, we can define a linear
feature class which satisfies an approximate form of Bellman completeness. We define the clipping
operator via
clip[0,2](x) := max{min{x, 2}, 0}.
We prove the following lemma.
Lemma F.1 (Pushforward-coverable MDPs admit low-dimensional embeddings). Let M be a
known MDP with reward function r, transition kernel P, and pushforward coverability parameter
Cpush. Let µ = {µh}h∈[H] denote its pushforward coverability distribution (i.e. the minimizer of
Definition 3.3) and F ⊆(S × [H] →[0, 1]) be an arbitrary class of functions. Suppose that we
sample W ∈{±1}d×S as a matrix of independent Rademacher random variables, and define
ψh(s, a) = rh(s, a) ⊕1
√
d
W

Ph(· | s, a)/µ1/2
h (·)

·∈S ∈Rd+1.
and
wf,h = 1 ⊕1
√
d
W

µ1/2
h (·)fh+1(·)

·∈S ∈Rd+1.
Then for any εapx ∈(0, 1), as long as we set
d ≥29 Cpush log
 16|F|Hδ−1/εapx

εapx
,
41

we have that for all f ∈F and h ∈[H], with probability at least 1 −δ:
Eµh⊗Unif(A)
h clip[0,2][⟨wf,h, ψh(s, a)⟩] −Thfh+1(s, a)
2i
≤εapx,
as well as maxs,a,h∥ψh(s, a)∥2
2
≤
Cpush(16 log(|S||A|H) + 11) and maxf,h∥wf,h∥2
2
≤
16 log(|F|H) + 11. We emphasize that the feature map ψ = {ψh}H
h=1 is oblivious to F, in the
sense that it can be computed directly from M without any knowledge of F.
Proof of Lemma F.1. Fix h ∈[H], whose dependence we omit for cleanliness. We begin by verifying
that, in expectation, ⟨wf, ψ(s, a)⟩is equal to T f(s, a). For this, note that
⟨wf, ψ(s, a)⟩
= r(s, a) + 1
d
d
X
i=1
 X
s′∈S
Wi,s′ P(s′ | s, a)
µ1/2(s′)
! X
s′′∈S
Wi,s′′µ1/2(s′′)f(s′′)
!
= r(s, a) +
X
s′∈S
P(s′ | s, a)f(s′) + 1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′ P(s′ | s, a)
µ1/2(s′) Wi,s′′µ1/2(s′′)f(s′′).
Consequently, we have
|T f(s, a) −⟨wf, ψ(s, a)⟩| =

1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′ P(s′ | s, a)
µ1/2(s′) Wi,s′′µ1/2(s′′)f(s′′)

.
(30)
Note that this remaining noise term is zero-mean – we will show in the sequel that it can be made
small by picking d appropriately. We next examine the norms of the vectors ψ(s, a) and wf. Note
that we have
∥ψ(s, a)∥2
2 = 1
d
d
X
i=1
 X
s′∈S
Wi,s′ P(s′ | s, a)
µ1/2(s′)
!2
=
X
s′∈S
P 2(s′ | s, a)
µ(s′)
+ 1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′Wi,s′′ P(s′ | s, a)
µ1/2(s′)
P(s′′ | s, a)
µ1/2(s′′)
≤Cpush + 1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′Wi,s′′ P(s′ | s, a)
µ1/2(s′)
P(s′′ | s, a)
µ1/2(s′′) ,
(31)
where we have used that
X
s′∈S
P 2(s′ | s, a)
µ(s′)
≤Cpush
X
s′∈S
P(s′ | s, a) = Cpush
by definition of pushforward coverability. Further note that we have
∥wf∥2
2 = 1
d
d
X
i=1
 X
s′∈S
Wi,s′µ1/2(s′)f(s′)
!2
= Es′∼µ[f(s′)] + 1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′Wi,s′′µ1/2(s′)f(s′) · µ1/2(s′′)f(s′′)
≤1 + 1
d
d
X
i=1
X
s′∈S
X
s′′∈S
s′′̸=s′
Wi,s′Wi,s′′µ1/2(s′)f(s′) · µ1/2(s′′)f(s′′).
(32)
We will now appeal to the following technical lemma to upper bound Eq. (30), Eq. (31), and Eq. (32)
by establishing that the Rademacher noise terms concentrate to their expectations. The proof of the
lemma will be given in the sequel.
42

Lemma F.2. Let u, v ∈Rn, and let W ∈{±1}d×n have independent Rademacher entries. Then
with probability at least 1 −δ,

1
d
X
i∈[d]
X
j∈[n]
X
k∈[n]
k̸=j
Wi,jWi,kujvk

≤∥u∥2∥v∥2 ·
r
32 log(2δ−1)
d
+∥u∥2
2∥v∥2
2 · 64 log
 2δ−1
d
. (33)
Furthermore, for any set of vectors V ⊂Rn, we also have
1
d max
v∈V
X
i∈[d]
X
j∈[n]
X
k∈[n]
k̸=j
Wi,jWi,kvjvk
≤max
v∈V ∥v∥2
2(16 log|V| + 9) + max
v∈V ∥v∥2
2 ·
r
32 log(2δ−1)
d
+ max
v∈V ∥v∥4
2 · 64 log
 2δ−1
d
.
Let (s, a) ∈S ×A and f ∈F. To bound |⟨ψ(s, a), wf⟩−T f(s, a)| (cf. Eq. (30)), we apply the first
bound of Lemma F.2 with u =
 P(s′ | s, a)/µ1/2(s′)

s′∈S and v =
 µ1/2(s′)f(s′)

s′∈S, which
gives
|⟨ψ(s, a), wf⟩−T f(s, a)| ≤
r
32Cpush log(2δ−1)
d
+ 64Cpush
log
 2δ−1
d
:= ε(δ−1),
(34)
where we have again used that ∥u∥2
2 = P
s′∈S
P 2(s′|s,a)
µ(s′)
≤Cpush and also that ∥v∥2
2 = 1 since
∥f∥∞≤1 for all f ∈F. To bound Eq. (31), we apply the second bound of Lemma F.2 with
V =

Ph−1(s′|s,a)
µ1/2
h
(s′)

s′∈S

s,a∈S×A
h×[H]
, which gives
max
s,a∈S×A,h∈[H]∥ψh(s, a)∥2
2 ≤Cpush(16 log|S||A|H + 9) + Cpush
r
32 log(2δ−1)
d
+ C2
push
64 log
 2δ−1
d
.
Lastly, to bound Eq. (32), we take V =

µ1/2
h (s′)fh(s′)

s′∈S

f∈F
h∈[H]
in Lemma F.2, which
establishes that
max
f∈F,h∈[H]∥wf,h∥2
2 ≤9 + 16 log|F|H +
r
32 log(2δ−1)
d
+ 64 log
 2δ−1
d
.
Note that Eq.
(34) establishes that the Bellman backup T f(s, a) is well-approximated by
⟨ψ(s, a), wf⟩only at a single state-action pair (s, a). We can obtain an L∞-approximation guar-
antee by taking a union bound over S and A, which would incur a dependence on log|S| in
the final sample complexity. Here, we bypass this by instead requiring only an approximation
guarantee under the L2(µ ⊗Unif(A)) norm. Via (pushforward) coverability, this will ensure
that Eπh
(⟨wf, ψ(s, a)⟩−T f(s, a))2i
is well-controlled for all policies π, which will be suffi-
cient for our downstream sample-complexity analysis of GOLF. However, directly establishing
an L2(µ ⊗Unif(A)) approximation guarantee is technically challenging since it would require
establishing a fourth-order (rather than second-order) equivalent of Eq. (33). The remainder of the
proof will obtain an L2(µ ⊗Unif(A)) approximation guarantee by instead sampling a dataset of size
n from µ ⊗Unif(A) and taking a union bound over that dataset to ensure a uniform bound on all
state-action pairs in that dataset. Via an additional concentration bound, this will ensure that the error
is well-behaved under the L2(µ ⊗Unif(A)) norm.
For each h ∈[H], sample a dataset D = {(s
(i)
h , a
(i)
h )}n
i=1 i.i.d. from µh ⊗Unif(A). By a union
bound over n, F, and H, we have that
∀i ∈[n], f ∈F, h ∈[H] :

ψh(s
(i)
h , a
(i)
h ), wf,h

−Thfh+1(s
(i)
h , a
(i)
h )
 ≤ε(n|F|Hδ−1),
(35)
where we recall the definition of ε(·) from Eq. (34). Now, let
Xf,h(s, a) :=
 clip[0,2][⟨ψh(s, a), wf,h⟩] −Thfh+1(s, a)
2.
43

Note that |Xf,h(s, a)| ≤4 and
Xf,h(s, a) ≤(⟨ψh(s, a), wf,h⟩−Thfh+1(s, a))2,
since Thfh+1(s, a) ∈[0, 2] and the clipping operator is 1-Lipshitz. Note that
E(s,a)∼µh⊗Unif(A)[Xf,h(s, a)] := Eµh⊗Unif(A)
h clip[0,2][⟨ψh(s, a), wf⟩] −Thfh+1(s, a)
2i
,
where this expectation is only over the sampling of the data point (s, a) (and not the Rademacher
matrix W). Let
Xi,f,h := Xf,h(s
(i)
h , a
(i)
h ).
By boundedness of Xf,h(s, a) and Hoeffding’s inequality, we have that with probability at least
1 −δ:

1
n
n
X
i=1
Xi,f,h −Eµ⊗Unif(A)[Xf,h(s, a)]
 ≤4
r
log(2δ−1)
n
.
Taking another union bound over F and H as well as the event in Eq. (35) gives that
∀f ∈F, h ∈[H] :

1
n
n
X
i=1
Xi,f,h −Eµ⊗Unif(A)[Xf,h(s, a)]
 ≤4
r
log(2|F|Hδ−1)
n
,
(36)
and ∀i ∈[n], f ∈F, h ∈[H] :
Xi,f,h ≤ε2(n|F|Hδ−1),
(37)
recalling the definition of ε(·) from Eq. (34).
Then, re-arranging Eq. (36) gives us that
Eµ⊗Unif(A)
h clip[0,2][⟨ψh(sh, ah), wf⟩] −Thfh+1(sh, ah)
2i
≤1
n
n
X
i=1
Xi,f,h + 4
r
log(2|F|Hδ−1)
n
≤ε2(n|F|Hδ−1) + 4
r
log(2|F|Hδ−1)
n
,
(38)
We now conclude the proof by picking n and d appropriately to ensure that the right-hand-side is
bounded by εapx, which will ensure the desired claim that
Eµ⊗Unif(A)
h clip[0,2][⟨ψh(sh, ah), wf⟩] −Thfh+1(sh, ah)
2i
≤εapx.
For convenience, we introduce absolute constants c and c′ whose precise values may change from
line to line. We pick n = 64 log
 2|F|Hδ−1
/ε2
apx. Plugging this into (38) gives
Eµ⊗Unif(A)
h clip[0,2][⟨ψh(sh, ah), wf⟩] −Thfh+1(sh, ah)
2i
≤ε2(n|F|Hδ−1) + c · ε
(39)
Noting that n ≤128 |F|Hδ−1
ε2apx
and plugging this into ε (Eq. (34)) gives
ε(n|F|Hδ−1) ≤C1/2
push
r
64 log(16|F|Hδ−1/εapx)
d
+ Cpush
128 log
 16|F|Hδ−1/εapx

d
.
(40)
Setting
d ≥29 Cpush log
 16|F|Hδ−1/εapx

εapx
ensures that
ε2(n|F|Hδ−1) ≤ε(n|F|Hδ−1) ≤εapx
2
(41)
by Eq. (40). Combining Eq. (38) and Eq. (41), we get
Eµ⊗Unif(A)
h clip[0,2][⟨ψh(sh, ah), wf⟩] −Thfh+1(sh, ah)
2i
≤εapx,
(42)
44

as desired. It only remains to establish the concentration results of Lemma F.2.
Proof of Lemma F.2. We establish the first claim. Let i ∈[d] be fixed, and consider the random
variable
Zi :=
X
j∈[n]
X
k∈[n]
k̸=j
Wi,jWi,kvjuk.
Note that E[Zi] = 0 by independence of Wi,j and Wi,k for every j ̸= k. By Exercise 6.9 of
Boucheron et al. [BLM13], we have that
log E[exp(λZi)] ≤
16λ2
2(1 −64∥u∥2
2∥v∥2
2λ)∥u∥2
2∥v∥2
2.
Since Zi are independent, it follows that
log E
"
exp
 
λ
d
X
i=1
Zi
!#
≤
16λ2
2(1 −64∥u∥2
2∥v∥2
2λ)∥u∥2
2∥v∥2
2d.
Hence, Pd
i=1 Zi is a sub-Gamma random variable with parameters ν = 16∥u∥2
2∥v∥2
2d and c =
64∥u∥2
2∥v∥2
2, and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13] that for all
ε > 0,
P
 d
X
i=1
Zi ≥∥u∥2∥v∥2
√
32dε + 64∥u∥2
2∥v∥2
2ε
!
≤e−ε.
Taking a union bound, and using that the random variable is symmetric, we obtain the desired claim.
We now establish the second claim. Let V ⊂Rn be a subset of vectors. Let i ∈[d] be fixed, and
re-consider the random variable
Zi := max
v∈V
X
j∈[n]
X
k∈[n]
k̸=j
Wi,jWi,kvjvk.
Again appealing to Exercise 6.9 of Boucheron et al. [BLM13], we have that
log E[exp(λ(Zi −E[Zi]))] ≤
16λ2
2(1 −64Bλ) E

max
v∈V
X
j∈[n]
X
k∈[n]
k̸=j
Wi,jWi,kv2
j v2
k


≤
16λ2
2(1 −64Bλ) E

max
v∈V
n
X
j,k=1
v2
j v2
k


=
16λ2
2(1 −64Bλ) max
v∈V ∥v∥4
2
where B := maxv∈V∥v∥4
2. Since Zi are independent, it follows that
log E
"
exp
 
λ
d
X
i=1
(Zi −E[Zi])
!#
≤
16λ2
2(1 −64Bλ) max
v∈V ∥v∥4
2d.
Hence, Pd
i=1 Zi is a sub-Gamma random variable with parameters ν = 16 maxv∈V∥v∥4
2d and
c = 64 maxv∈V∥v∥4
2, and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13]
that for all ε > 0,
P
 
1
d
d
X
i=1
Zi ≥E[Zi] + max
v∈V ∥v∥2
4
r
32ε
d
+ 64 max
v∈V ∥v∥4
2
ε
d
!
≤e−ε.
45

To conclude, it remains only to show the bound E[Zi] ≤maxv∥v∥2
2(16 log|V| + 9). This follows by
a standard log-sum-exp approach. Below, we abbreviate ρj := Wi,j. We can observe that for any
λ > 0:
E[Zi] = E

max
v∈V
X
j∈[n]
X
k∈[n]
k̸=j
ρjρkvjvk


≤1
λ log




X
v∈V
E

exp



λ
X
j∈[n]
X
k∈[n]
k̸=j
ρjρkvjvk










≤1
λ log



X
v∈V
E

exp


λ


n
X
j=1
ρjvj


2







(43)
Note that X := P
j ρjvj is subGaussian with parameter ∥v∥2
2, since:
E

exp

λ
n
X
j=1
ρjvj



=
n
Y
j=1
E[exp(λρjvj)] ≤
n
Y
j=1
exp
 
λ2v2
j
2
!
= exp
λ2
2 ∥v∥2
2

.
Then, it follows (e.g. Lemma 1.12 of Rigollet et al. [RH23]) that X2 −E[X2] satisfies a sub-
exponential MGF bound with parameter 16∥v∥2
2, i.e.
E[exp
 λ(X2 −E[X2])

] ≤exp
256
2 λ2∥v∥4
2

∀|λ| ≤
1
16∥v∥2
2
.
We also note that
E[X2] =
n
X
i,j=1
vivj E[εiεj] = ∥v∥2
2.
Adding and subtracting E[X2] in Eq. (43) gives
≤1
λ log
 X
v∈V
E

exp
 λ
 X2 −∥v∥2
2

+ λ∥v∥2
2

!
= 1
λ log
 X
v∈V
E

exp
 λ
 X2 −∥v∥2
2

exp
 λ∥v∥2
2

!
≤1
λ log
 X
v∈V
exp
 128λ2∥v∥4
2 + λ∥v∥2
2

!
∀|λ| ≤
1
16 maxv∥v∥2
2
≤1
λ log|V| + max
v
128λ∥v∥4
2 + max
v ∥v∥2
2
∀|λ| ≤
1
16 maxv∥v∥2
2
Picking λ =
1
16 maxv∥v∥2
2 concludes the proof.
F.2.2
GOLF with on-policy misspecification
Consider the version of GOLF [JLM21] in Algorithm 2. We have the following guarantee for the
regret of GOLF, which extends Jin et al. [JLM21] to allow for on-policy misspecification.
Lemma F.3. Suppose that QM ⋆
obs,⋆∈F and G satisfies εapx-completeness in the sense that for
all h ∈[H] and f ∈Fh+1, there exists g ∈Gh such that Eπ
g −T M ⋆
obs
h
f
2
≤ε2
apx for all
π ∈ΠF := {πf : f ∈F}. Let Ccov := Ccov(M ⋆
obs, ΠF) (Definition D.3). Then for an appropriate
choice of β, Algorithm 2 ensures that
Reg ≤H
p
CcovT log(|F||G|HT/δ) + HT
p
Ccov log(T)εapx.
46

Algorithm 2 GOLF [JLM21]
input: Function classes F and G, confidence width β > 0.
initialize: F (0) ←F, D
(0)
h ←∅∀h ∈[H].
1: for episode t = 1, 2, . . . , T do
2:
Select policy π(t) ←πf (t), where f (t) := arg maxf∈F(t−1) f(x1, πf,1(x1)).
3:
Execute π(t) for one episode and obtain trajectory (x
(t)
1 , a
(t)
1 , r
(t)
1 ), . . . , (x
(t)
H , a
(t)
H , r
(t)
H ).
4:
Update dataset: D
(t)
h ←D
(t−1)
h
∪
 x
(t)
h , a
(t)
h , x
(t)
h+1
	
∀h ∈[H].
5:
Compute confidence set:
F
(t) ←

f ∈F : L
(t)
h (fh, fh+1) −min
gh∈Gh L
(t)
h (gh, fh+1) ≤β ∀h ∈[H]

,
where
L
(t)
h (f, f ′) :=
X
(x,a,r,x′)∈D(t)
h

f(x, a) −r −max
a′∈A f ′(x′, a′)
2
, ∀f, f ′ ∈F.
6: end for
7: Output bπ = Unif(π(1:T )).
Proof
of
Lemma
F.3.
For
each
fh+1
∈
Fh+1,
let
apx[fh]
=
arg mingh∈Gh supπ∈Π Eπh
(gh −Thfh+1)2i
. Let
δ
(t)
h (·, ·) := f
(t)
h (·, ·) −Tff
(t)
h+1(·, ·)
&
eδ
(t)
h (·, ·) := f
(t)
h (·, ·) −apx

f
(t)
h+1

(·, ·),
and note that by Jensen’s inequality we have that for all π, Eπ
δ
(t)
h (·, ·)

≤Eπh
eδ
(t)
h (·, ·)
i
+ εapx.
We further adopt the shorthand d
(t)
h (x, a) := dπ(t)
h
(x, a) and ˜d
(t)
h (x, a) := P
i<t d
(t)
h (x, a). As
a consequence of realizability (Q⋆
obs,h ∈Fh) and approximate Bellman completeness, standard
concentration arguments (proved in the sequel) lead to the following result.
Lemma F.4 (Optimism and small in-sample squared Bellman errors). With probability at least 1 −δ,
by taking β = c log(TH|F||G|/δ) + Tεapx, we have that for all t ∈[T],
(i) Q⋆
obs,h ∈F
(t),
and
(ii)
X
x,a
˜d
(t)
h (x, a)

eδ
(t)
h (x, a)
2
≤O(β).
The rest of the proof proceeds similarly to the analysis of Section 3.2 in Xie et al. [XFBJK23].
Namely, by optimism (Lemma F.4) and a standard Bellman error decomposition (Lemma C.6) we
have
Reg ≤
T
X
t=1
H
X
h=1
Ed(t)
h

δ
(t)
h (x, a)

≤TH · εapx +
T
X
t=1
H
X
h=1
Ed(t)
h
h
eδ
(t)
h (x, a)
i
.
Let us defining the burn-in time
τh(x, a) = min{t | ˜d
(t)
h (x, a) ≥Ccovµ⋆
h(x, a)},
where µ⋆
h is the coverability distribution for the set of policies ΠF (i.e., the distribution µ⋆
h that
achieves the minimum in the coverability definition). Using the same decomposition into “burn-in
phase” and “stable phase” in Xie et al. [XFBJK23], we have:
T
X
t=1
H
X
h=1
Ed(t)
h
h
eδ
(t)
h (x, a)
i
≤2HCcov +
T
X
t=1
H
X
h=1
Ed(t)
h
h
eδ
(t)
h (x, a)I{t ≥τh(x, a)}
i
.
47

Applying a change of measure argument on the second term then gives:
T
X
t=1
H
X
h=1
Ed(t)
h
h
eδ
(t)
h (x, a)I{t ≥τh(x, a)}
i
≤H
v
u
u
t
T
X
t=1
X
x,a
 I{t ≥τh(x, a)}d
(t)
h (x, a)
2
˜d
(t)
h (x, a)
|
{z
}
(A)
×
v
u
u
t
T
X
t=1
X
x,a
˜d
(t)
h (x, a)

eδ
(t)
h (x, a)
2
|
{z
}
(B)
By the same reasoning as in Xie et al. [XFBJK23], we have (A) ≤O(
p
Ccov log(T)), and by
Lemma F.4 we have (B) ≤O(√βT). Using that β = log(TH|F|/δ) + Tε2
apx gives the desired
result. It remains to establish the concentration results of Lemma F.4.
Proof of Lemma F.4. For any function f, define a random variable
Xt(h, f) =
 fh(s
(t)
h , a
(t)
h ) −r
(t)
h −fh+1(s
(t)
h+1)
2 −
 Thfh+1(s
(t)
h , a
(t)
h ) −r
(t)
h −fh+1(s
(t)
h+1)
2.
Let Ft,h = {s
(i)
1 , a
(i)
1 , r
(i)
1 , . . . , s
(i)
H , a
(i)
H , r
(i)
H }i<t. Note that
E

r
(t)
h + fh+1(s
(t)
h+1) | Ft,h

= Eπ(t)[Thf(sh, ah)].
(44)
and thus that
E[Xt(h, f) | Ft,h] = Eπ(t)h
(fh(sh, ah) −Thfh(sh, ah))2i
.
Next, note that
Var[Xt(h, f) | Ft,h] ≤E
h
(Xt(h, f))2 | Ft,h
i
≤E
h fh(s
(t)
h , a
(t)
h ) −Thfh(s
(t)
h , a
(t)
h )
2 fh(s
(t)
h , a
(t)
h ) + Thfh(s
(t)
h , a
(t)
h ) + 2
 r
(t)
h −fh+1(s
(t)
h+1)
2 | Ft,h
i
≤16 E
h fh(s
(t)
h , a
(t)
h ) −Thfh(s
(t)
h , a
(t)
h )
2 | Ft,h
i
= 16 E[Xt(h, f) | Ft,h].
By Freedman’s inequality (Lemma C.2, Lemma C.3), we have that with probability at least 1 −δ:

X
i<t
Xi(h, f) −
X
i<t
E[Xi(h, f) | Fi,h]
 ≤O


s
log(1/δ)
X
i<t
E[Xi(h, f) | Fi,h] + log(1/δ)


Taking a union bound over [T]×[H]×F, we have that for all t, h, f, with probability at least 1−δ:

X
i<t
Xi(h, f) −
X
i<t
Eπ(i)h
(fh(sh, ah) −Thfh(sh, ah))2i
(45)
≤O


s
ι
X
i<t
Eπ(i)h
(fh(sh, ah) −Thfh(sh, ah))2i
+ ι

,
(46)
where ι = log(|F|HT/δ). We now show that
X
i<t
Xi(h, f
(t)) ≤β + O
 Tε2
apx + ι

= O(β),
(47)
which will imply, from Eq. (46), that
X
i<t
Eπ(t)h
(fh(sh, ah) −Thfh(sh, ah))2i
≤O(ι + β) = O(β),
as desired. To see Eq. (47), let
∆t =
X
i<t
 apx

Thf
(t)
h+1

(s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2−
 Thf
(t)
h (s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2
48

and then note that:
X
i<t
Xi(h, f
(t)) =
X
i<t
 f
(t)
h (s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2 −
 Thf
(t)
h (s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2
=
X
i<t
 f
(t)
h (s
(i)
h , a
(i)
h ) −r
(i)
h −f
(i)
h+1(s
(i)
h+1)
2
−
X
i<t
 apx

Thf
(t)
h+1

(s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2 + ∆t
≤
X
i<t
 f
(t)
h (s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2
−inf
gh∈Gh
X
i<t
 g(s
(i)
h , a
(i)
h ) −r
(i)
h −f
(t)
h+1(s
(i)
h+1)
2 + ∆t
≤β + ∆t.
where the second-to-last line follows from apx

Thf
(t)
h+1

∈G and the last line follows from the
definition of the confidence set. It remains to show that ∆t ≤O(Tε2
apx + ι), which we do via a
similar concentration argument. Namely, let
Yt(h, f) =
 apx[Thfh+1](s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2−
 Thfh(s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2,
and note that, as before,
E[Yt(h, f) | Ft,h] = Eπ(t)h
(apx[Thfh+1](sh, ah) −Thfh(sh, ah))2i
,
and
Var[Yt(h, f) | Ft,h] ≤16 E[Yt(h, f) | Ft,h],
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that,
with probability at least 1 −δ,

X
i<t
Yt(h, f) −
X
i<t
Eπ(t)h
(apx[Thfh+1](sh, ah) −Thfh(sh, ah))2i
(48)
≤O


s
ι
X
i<t
Eπ(t)h
(apx[Thfh+1](sh, ah) −Thfh(sh, ah))2i
+ ι

,
(49)
where ι = log(|F|HT/δ). Recalling the misspecification assumption, this implies that
X
i<t
Yt(h, f) ≤O
 tε2
apx + ι

,
for all h, f, t, with high probability. This concludes the result for (ii). For (i), this follows identically
to the proof of Lemma 40 in Jin et al. [JLM21], since this only uses the property that Q⋆∈F.
F.2.3
Sample-efficient latent-dynamics RL under pushforward coverability
We conclude by combining the previous two results to obtain the main result for this section.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let Mlat be a base MDP
class such that each Mlat ∈Mlat has pushforward coverability bounded by Cpush(Mlat) ≤Cpush.
Then, for any decoder class Φ, we have:
1. comp(Mlat, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, ε−1, log
 δ−1
), and
2. comp(⟪Mlat, Φ⟫, ε, δ) ≤poly(Cpush, |A|, H, log|Mlat|, log|Φ|, ε−1, log
 δ−1
, log log|S|).
Proof of Theorem 3.2. Let M ⋆
obs := ⟪M ⋆
lat, ψ⋆⟫∈⟪Mlat, Φ⟫be the unknown latent-dynamics
MDP. Define observation-level value functions
F = {Q
Mlat,⋆◦ϕ | Mlat ∈Mlat, ϕ ∈Φ},
49

so that QM ⋆
obs,⋆= QM ⋆
lat,⋆◦ϕ⋆∈F via decoder and model realizability, and log|Fh| ≤log|Mlat||Φ|.
Consider any function class L ⊆{S →[0, 1]} and MDP Mlat = (rlat, Plat). For a given value
εapx > 0, setting d according to Lemma F.1 implies that there exists a d-dimensional feature map
φMlat,h(s, a) ∈Rd+1 such that for all ℓ∈L and h ∈[H], there exists wℓ,h ∈Rd+1 such that
EµMlat⊗Unif(A)

clip[0,2]

φMlat,h(s, a), wℓ,h

−T Mlat
h
ℓh+1(s, a)
2
≤εapx,
(50)
where µMlat is the pushforward coverability distribution for Mlat. Moreover, the map φh is explicitly
computed as a function of Mlat by a randomized algorithm with success probability 1 −δ, with no
knowledge of the class L required. We consider the class
L =
(
Γϕ ◦QMlat,⋆(s, a) :=
X
s′∈S
Γϕ(s′ | s)QMlat,⋆(s′, a) | ϕ ∈Φ, Mlat ∈Mlat
)
,
(51)
where Γϕ : S →∆(S) is the mismatch function for decoder ϕ and emission ψ⋆, defined in
Definition F.1. Note that L has size log|L| ≤log|Mlat||Φ|, and that we have
T M ⋆
obs
h
(QMlat,⋆
h
◦ϕh)(x, a) = T M ⋆
lat
h
(Γϕ,h+1 ◦V Mlat,⋆
h
)(ϕ⋆
h(x), a)
by Lemma D.7. By Lemma D.1 we have that µM ⋆
obs,h(x) = ψ⋆
h(x | ϕ⋆
h(x))µM ⋆
lat,h(ϕ⋆
h(x)) is the
coverability distribution for MDP M ⋆
obs, and
EµM⋆
lat⊗Unif(A)[f(s, a)] = EµM⋆
obs⊗Unif(A)[f(ϕ⋆(x), a)].
Now, define
GMlat,h =
n
(x, a) 7→clip[0,2][⟨φMlat,h(ϕ(x), a), w⟩] | ϕ ∈Φ, ∥w∥2
2 ≤11 + 16 log(|Mlat||Φ|H)
o
.
Recall the definition of wf (for f : S × A →[0, 1]) from Lemma F.1, and note that by the
norm bound maxℓ∈L∥wℓ∥2
2 ≤11 + 16 log(|Mlat||Φ|H) given by Lemma F.1, we have (x, a) 7→
⟨φMlat,h(ϕ(x), a), wℓ⟩∈Gh for every ℓ∈L. Next, note that by the norm bound maxs,a∥ψ(s, a)∥2
2 ≤
Cpush(11 + 16 log(|S||A|H)), given by Lemma F.1, we have every gh ∈GMlat,h satisfies ∥gh∥∞≤
cC1/2
push log(|Mlat||Φ||S||A|H) := B for some absolute constant c. Therefore, GMlat,h has size
log|GMlat,h| ≤eO(d · log(B) + log|Φ|) = eO(d log log(|S|) + log|Φ|), where the eO notation ignores
logarithmic factors of Cpush, |A|, log|Mlat|, and log|Φ|.22 Define Gh = ∪Mlat∈MlatGMlat,h, which
has size log|Gh| ≤log|Mlat|+( eO(d log log(|S|) + log|Φ|)). Together, these results with Lemma F.1
imply that for all fh+1 ∈Fh+1, there exists gh ∈Gh such that
EµM⋆
obs,h⊗Unif(A)

gh(xh, ah) −
h
T M ⋆
obs
h
fh+1
i
(xh, ah)
2
≤εapx.
This, in turn, implies that for all πobs ∈Πrns we have
Eπobs

gh(xh, ah) −
h
T
M⋆
obs
h
fh+1
i
(xh, ah)
2
≤Cpush|A|εapx,
since µM ⋆
obs,h ⊗Unif(A) satisfies coverability (Definition D.3) with parameter Ccov(M ⋆
obs, Πrns) ≤
Cpush|A| (Eq. (29)).
Then, it follows by Lemma F.3 that if we run Algorithm 2 with the classes F and G we will get
Reg ≤H
q
Cpush|A|T log(|Mlat||Φ|HT/δ)(d log log(|S|) + log|Φ|) + HT
q
C2
push|A|2 log(T)εapx
≤H
s
C5
push|A|T log(|Mlat||Φ|HT/δ)log
 C2
push|Mlat||Φ|2Hδ−1/εapx

log log(|S|)
εapx
+ HT
q
C2
push|A|2 log(T)εapx
22Formally, this requires a standard covering number argument; we omit the details.
50

Choosing εapx =
1
√
T to balance leads to
Reg ≲HT 3/4q
C5
push|A| log(|Mlat||Φ|HT/δ) log
 C2
push|Mlat||Φ|2Hδ−1T

log log(|S|)
+ HT 3/4q
C2
push|A|2 log(T)
≲HT 3/4q
C5
push|A|2 log(|Mlat||Φ|HT/δ) log
 TC2
push|Mlat||Φ|2H/δ

log log(|S|),
which gives a risk bound of
Risk ≲
1
T 1/4 H
q
C5
push|A|2 log(|Mlat||Φ|HT/δ) log
 TC2
push|Mlat||Φ|2H/δ

log log(|S|).
Equating this to ε gives a sample complexity of
T = poly(Cpush, A, H, log|Mlat|, log|Φ|, ε−1, log
 δ−1
, log log(|S|)),
as desired. Note that we have not made much effort to optimize the rate; in particular, a faster rate is
likely possible by using the GOLF.DBR algorithm of Amortila et al. [ACK24], which improves over
the GOLF algorithm under the presence of misspecification.
51

G
Proofs and Additional Information for Section 4.1: Hindsight RL
This appendix contains additional information and proofs related to algorithmic modularity under
hindsight observations (Section 4.1), and is organized as follows:
• Appendix G.1 contains the pseudocode and proofs related to the online representation learning
oracle EXPWEIGHTS.DR (Lemma 4.1).
• Appendix G.2 contains the proof for our risk bound of the O2L algorithm under hindsight observ-
ability (Theorem 4.1).
G.1
Pseudocode and Proofs for EXPWEIGHTS.DR (Lemma 4.1)
Algorithm 3 Derandomized Exponential Weights (EXPWEIGHTS.DR)
input: Decoder set Φ
for t = 1, 2, · · · , T do
Get dataset

x
(i)
h , ϕ⋆(x
(i)
h )
	
i∈[t−1],h∈[H]
for h = 1, . . . , H do
For ϕ ∈Φ, compute
q
(t)
h (ϕh) ∝exp
 
−
t−1
X
i=1
I

ϕh(x
(i)
h ) ̸= ϕ⋆
h(x
(i)
h )

!
,
and set
¯ϕ
(t)
h (x) = arg max
s∈S
Pϕh∼q(t)
h (ϕh(x) = s).
(52)
end for
Return ¯ϕ(t) = {¯ϕ
(t)
h }H
h=1.
end for
The main result for this estimator is the following.
Lemma 4.1 (Online classification via EXPWEIGHTS.DR). Under decoder realizability (ϕ⋆∈Φ),
EXPWEIGHTS.DR (Algorithm 3) satisfies Assumption 4.2 with23
Estclass(T) = e
O(H log|Φ|).
Proof of Lemma 4.1. For each h ∈[H], consider the realizable online classification problem where
x
(t)
h ∼dπ(t)
h
, for π(t) chosen adversarially, and y
(t)
h = ϕ⋆
h(x
(t)
h ). Consider the exponential weights
estimator
q
(t)
h (ϕ) ∝exp
 
−
t−1
X
i=1
I

ϕ(x
(i)
h ) ̸= ϕ⋆
h(x
(i)
h )

!
.
For every sequence (x
(t)
h )T
t=1, these distributions satisfy the deterministic regret bound
T
X
t=1
Ebϕ(t)
h ∼q(t)
h
h
I
h
bϕ
(t)
h (x
(t)
h ) ̸= ϕ⋆
h(x
(t)
h )
ii
≤2 log |Φ|,
by Corollary 2.3 of Cesa-Bianchi et al. [CBL06]. Taking conditional expectations over x
(t)
h ∼dπ(t)
h
and using Lemma C.3 gives that with probability at least 1 −δ:
T
X
t=1
Ebϕ(t)
h ∼q(t)
h Eπ(t)h
I
h
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
ii
≤4 log |Φ| + 8 log
 2δ−1
.
Taking a union bound over h ∈[H] and summing over h ∈[H] we obtain that with probability at
least 1 −δ:
T
X
t=1
H
X
h=1
Ebϕ(t)
h ∼q(t)
h Eπ(t)h
I
h
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
ii
≤4H log |Φ| + 8H log
 2Hδ−1
.
23In this section, the notations e
O, ≈, and ≲ignore only constants and logarithmic factors of H.
52

Now, recall that at each time t, we define the improper decoder ¯ϕ
(t)
h via:
¯ϕ
(t)
h (x) = arg max
s∈S
Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = s)
(53)
Let ℓh(xh, q
(t)
h ) = Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)). Note that ℓsatisfies
T
X
t=1
H
X
h=1
Eϕ(t)
h ∼q(t)
h Eπ(t)
I

ϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)

=
T
X
t=1
H
X
h=1
Eπ(t) Eϕ(t)
h ∼q(t)
h

I

ϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)

(54)
=
T
X
t=1
H
X
h=1
Eπ(t)[ℓh(xh, q
(t)
h )].
(55)
By abuse of notation we also denote ℓh(xh, ¯ϕh) = I
¯ϕh(x) ̸= ϕ⋆(x)

. We will show that
∀x, t, h : ℓh(xh, ¯ϕ
(t)
h ) ≤2ℓh(xh, q
(t)
h ),
(56)
from which we will obtain that with probability at least 1 −δ:
Regclass(T) =
T
X
t=1
H
X
h=1
Eπ(t)
I
¯ϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)

≤8H log |Φ| + 16H log
 2Hδ−1
.
Integrating the high-probability regret bound gives
E[Regclass(T)] = O(H log(H|Φ|)),
as desired. Towards establishing Eq. (56), let us fix x and let smax denote the argmax in Eq. (53).
There are two cases:
• Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = smax) ≥1
2:
→If smax = ϕ⋆(x), ℓ(x, ¯ϕ
(t)
h ) = 0 so we are done.
→Otherwise, smax ̸= ϕ⋆(x) and we have ℓ(x, ¯ϕ
(t)
h ) = 1. However, since ϕ⋆(x) ̸= smax we have
ϕ
(t)
h (x) = smax =⇒ϕ
(t)
h (x) ̸= ϕ⋆
h(x) and so
Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) ̸= ϕ⋆
h(x)) ≥Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = smax) ≥1
2 = 1
2ℓ(x, ¯ϕ
(t)
h ).
• Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = smax) < 1
2:
→If smax = ϕ⋆
h(x), ℓ(x, ¯ϕ
(t)
h ) = 0 so we are done.
→Otherwise, smax ̸= ϕ⋆(x) and we have ℓ(x, ¯ϕ
(t)
h ) = 1. However, by definition of smax as the
mode we also have
Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = ϕ⋆
h(x)) ≤Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) = smax) < 1
2,
so in particular we have
ℓ(x, q
(t)
h ) = Pϕ(t)
h ∼q(t)
h (ϕ
(t)
h (x) ̸= ϕ⋆
h(x)) > 1
2 = 1
2ℓ(x, ¯ϕ
(t)
h ).
G.2
Proofs for O2L Under Hindsight Observability (Theorem 4.1)
Theorem 4.1 (Risk bound for O2L under hindsight observability). Let ALGlat be a base algorithm
with base risk Risk⋆(K), and REPclass a representation learning oracle satisfying Assumption 4.2.
Then Algorithm 1, with inputs T, K, Φ, REPclass, and ALGlat, has expected risk
E[Riskobs(TK)] ≤Risk⋆(K) + 2K
T Estclass(T).
53

Proof of Theorem 4.1. Let (bϕ(t))t∈[T ] denote the decoders chosen by REPclass, and let ρ(t) denote
the distribution over decoders induced at time t from the interaction of REPclass, ALGlat, and M ⋆
obs.
Let π
(t,k)
obs := π
(t,k)
lat ◦bϕ(t) and p
(t,k)
obs denote the distribution over (observation-space) policies played
at epoch t and episode k, induced by the interaction of REPclass, ALGlat, and M ⋆
obs. We adopt
the notation π
(t,K+1)
lat
:= bπ
(t)
lat ∼p
(t,K+1)
lat
for the final policy output by ALGlat in epoch t and
(x
(t,K+1)
h
, a
(t,K+1)
h
, r
(t,K+1)
h
) for the trajectory collected from that (observation-level) policy bπ
(t)
lat ◦bϕ(t).
We firstly note that by assumption, we have the guarantee
E
" T
X
t=1
K+1
X
k=1
H
X
h=1
Eπ(t,k)
obs
∼p(t,k)
obs
Eπ(t,k)
obs
h
I
h
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
ii#
≤(K + 1)Estclass(T)
≤2KEstclass(T).
(57)
which follows by applying Assumption 4.2 to the distributions ¯p
(t)
obs =
1
(K+1)
PK+1
k=1 p
(t,k)
obs and noting
that
T
X
t=1
H
X
h=1
1
K + 1
K+1
X
k=1
Eπ(t,k)
obs
∼p(t,k)
obs
Eπ(t,k)
obs
h
I
h
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
ii
=
T
X
t=1
H
X
h=1
E¯π(t)
obs ∼¯p(t)
obs E¯π(t)
obs
h
I
h
bϕ
(t)
h (xh) ̸= ϕ⋆
h(xh)
ii
≤Estclass(T).
Let Risk(K, ALGlat, ϕ, M ⋆
obs) = JM ⋆
obs(π⋆
M ⋆
obs) −JM ⋆
obs(bπlat ◦ϕ) be the random variable denot-
ing the risk of the final policy output by ALGlat after K rounds of interaction with M ⋆
obs when
given feature ϕ in any epoch t. For any ϕ : X →S, let Eϕ denote the law over trajectories
(x
(k)
h , a
(k)
h , r
(k)
h )k∈[K+1],h∈[H] and policies (π
(k)
lat ◦ϕ)k∈[K+1] generated after K rounds of interaction
when ALGlat is given feature ϕ in any epoch. (Recall that, for all of the above definitions, a new
instance of ALGlat is initialized at every epoch, so we do not have to specify which epoch it is, only
the current feature ϕ). Finally, let Gt be the “good” event
Gt =
n
∀k ∈[K + 1], ∀h ∈[H] : bϕ
(t)
h (x
(t,k)
h
) = ϕ⋆
h(x
(t,k)
h
)
o
.
Recall that, in any round t, ALGlat only observes the latent (“compressed”) trajectories
(bϕ
(t)
h (x
(t,k)
h
), a
(t,k)
h
, r
(t,k)
h
) as history for choosing policies. We can therefore conclude that, when
bϕ(t)(x
(t,k)
h
) = ϕ⋆(x
(t,k)
h
) for all k ∈[K + 1], h ∈[H], the distribution over final policies bπ
(t)
lat chosen
by ALGlat will be identical as if we had chosen ϕ⋆as our decoder. In particular, this implies
Ebϕ(t)
h
I{Gt}Risk(K, ALGlat, bϕ
(t), M ⋆
obs)
i
= Eϕ⋆[I{Gt}Risk(K, ALGlat, ϕ⋆, M ⋆
obs)]
≤Risk⋆(K),
(58)
where the second line simply follows by removing the indicator function, recalling that Risk⋆(K) =
E[Risk(K, ALGlat, M ⋆
lat)], and using that Risk(K, ALGlat, ϕ⋆, M ⋆
obs) = Risk(K, ALGlat, M ⋆
lat).
Then, we have:
E[Riskobs(TK)] = 1
T
T
X
t=1
Ebϕ(t)∼ρ(t)
h
Ebϕ(t)
h
Risk(K, ALGlat, bϕ
(t), M ⋆
obs)
ii
≤1
T
T
X
t=1
Ebϕ(t)∼ρ(t)
h
Ebϕ(t)
h
I{Gt}Risk(K, ALGlat, bϕ
(t), M ⋆
obs)
ii
+ 1
T
T
X
t=1
Ebϕ(t)∼ρ(t)
h
Ebϕ(t)[I{¬Gt}]
i
≤1
T
T
X
t=1
Risk⋆(K) + 1
T
T
X
t=1
P(¬Gt)
= Risk⋆(K) + 1
T
T
X
t=1
P(¬Gt),
54

where the first equality applies the tower rule for conditional expectation, the second equality applies
linearity of conditional expectations and the upper bound Risk(K, ALGlat, bϕ(t), M ⋆
obs) ≤1, and the
third lines applies the upper bound Eq. (58). It remains to bound the last term. Here, note that by a
union bound,
P(¬Gt) ≤E
"K+1
X
k=1
H
X
h=1
Eπ(t,k)∼p(t,k) Eπ(t,k) I
n
bϕ
(t)(x
(t,k)
h
) ̸= ϕ⋆(x
(t,k)
h
)
o#
,
where we have used that trajectory k in round t is sampled from policy π(t,k), which is in turn sampled
from p(t,k). Summing over t and using the bound in Eq. (57) concludes the proof.
55

H
Proofs for Appendix A: Self-Predictive Estimation
This appendix contains additional information and proofs related to algorithmic modularity under
self-predictive estimation (Appendix A), and is organized as follows:
• Appendix H.1 contains the pseudocode and proofs related to the online representation learning
oracle SELFPREDICT.OPT (Lemma A.1).
• Appendix H.2 contains the proof for our risk bound of the O2L algorithm under self-predictive
estimation (Theorem A.1).
H.1
Pseudocode and Proofs for SELFPREDICT.OPT (Lemma A.1)
The pseudocode for our self-predictive estimation procedure is given in Algorithm 4.
Algorithm 4 Optimistic Self-Predictive Latent Model Estimation (SELFPREDICT.OPT)
1: input: Decoder set Φ, Latent model class Mlat, Mismatch-complete class Llat, Optimism
parameter γ
2: Set β := 1
2
p
CcovH log(T )/T
3: for t = 1, 2, · · · , T do
4:
Get dataset D(t) = {x
(i)
h , a
(i)
h , r
(i)
h , x
(i)
h+1}i∈[t−1],h∈[H]
5:
Compute
(c
M
(t), bϕ
(t)) =
arg max
(M,ϕ)∈(Mlat,Φ)
(
(γβ)−1J
M(πM) +
H
X
h=1
n
X
i=1
log
 Mh(r
(i)
h , ϕh+1(x
(i)
h+1) | ϕh(x
(i)
h ), a
(i)
h )

(59)
−
max
(M ′,ϕ′)∈(Llat,Φ)
n
X
i=1
log
 M ′
h(r
(i)
h , ϕh+1(x
(i)
h+1) | ϕ′
h(x
(i)
h ), a
(i)
h )

)
.
(60)
6:
Return bϕ(t) =
n
bϕ
(t)
h
o
h∈[H].
7: end for
Our main result concerning the SELFPREDICT.OPT estimator for online optimistic self-predictive
estimation is the following. We recall our notation for the instantaneous self-prediction error
[∆h(Mlat, ϕ)](xh, ah) := D2
H
 Mlat,h(ϕh(xh), ah),

ϕh+1♯M ⋆
obs,h

(xh, ah)

.
Lemma A.1 (Optimistic self-predictive estimation via SELFPREDICT.OPT). Let Πlat denote the
set of policies played by ALGlat, and Ccov,st = Ccov,st(M ⋆
lat, ΓΦ ◦Πlat) be the state coverability
parameter on M ⋆
lat over the set of stochastic policies ΓΦ ◦Πlat (Eq. (9)). Then, for any γ > 0,
under decoder realizability (ϕ⋆∈Φ), base model realizability (M ⋆
lat ∈Mlat), and mismatch
function completeness with class Llat (Assumption A.2), the estimator in Algorithm 4 with inputs
Φ, Mlat, Llat, and γ satisfies Assumption A.1 with24
Estself;opt(T, γ) = e
O
q
HCcov,st|A|T log(|Mlat||Llat||Φ|)

.
Proof of Lemma A.1. We will firstly establish that the algorithm obtains low offline estimation error.
Lemma H.1 (SELFPREDICT.OPT attains low offline estimation error). For any γ > 0, under decoder
realizability (ϕ⋆∈Φ), model realizability (M ⋆
lat ∈Mlat), and mismatch function completeness with
class Llat (Assumption A.2), the estimator in Algorithm 4 with inputs Φ, Mlat, Llat, and γ satisfies
24In this section, the notations e
O and ≲ignores constants and logarithmic factors of: H, Ccov,st, |A|, T, and
log(|Mlat||Llat||Φ|).
56

that for all t ∈[T], with probability at least 1 −δ,
H
X
h=0
t−1
X
i=1
Eπ(i)∼p(i) Eπ(i)h
[∆h(c
M
(t), bϕ
(t))](xh, ah)
i
+ γ−1
JM ⋆
lat(πM ⋆
lat) −J
c
M (t)(πc
M (t))

≤O
 log
 |Mlat||Llat||Φ|HTδ−1
.
(61)
Given this result, we can appeal to offline-to-online conversions to establish the final result. Let
Ccov := Ccov(M ⋆
obs, Πlat ◦Φ) denote the (state-action) coverability coefficient in M ⋆
obs over the
set of policies Πlat ◦Φ. Note that by Lemma D.1 we have Ccov,st(M ⋆
obs, Πlat ◦Φ) = Ccov,st and
therefore by Lemma D.4 we have Ccov(M ⋆
obs, Πlat ◦Φ) ≤Ccov,st|A|. Let η > 0 be a parameter
to be chosen later, and βoff = O
 log
 |Mlat||Llat||Φ|HTδ−1
be the offline estimation error
guaranteed by Lemma H.1. We abbreviate α :=
p
CcovH log(T), Ep(t)[·] := Eπ(t)∼p(t) Eπ(t)[·], and
Eep(t) := Pt−1
i=1 Eπ(i)∼p(i) Eπ(i)[·]. Then, we have:
T
X
t=1
H
X
h=1
Ep(t)h
[∆h(c
M
(t), bϕ
(t))](xh, ah)
i
+ γ−1
JM ⋆
lat(πM ⋆
lat) −J
c
M (t)(πc
M (t))

≤α
v
u
u
t
T
X
t=1
H
X
h=1
Eep(t)h
[∆h(c
M (t), bϕ(t))](xh, ah)
i
+ γ−1
T
X
t=1

JM ⋆
lat(πM ⋆
lat) −J
c
M (t)(πc
M (t))

+ O(HCcov)
≤α
 
η
2
T
X
t=1
H
X
h=1
Eep(t)h
[∆h(c
M
(t), bϕ
(t))](xh, ah)
i
+ 1
2η
!
+ γ−1
T
X
t=1

JM ⋆
lat(πM ⋆
lat) −J
c
M (t)(πc
M (t))

+ O(HCcov)
where in the first inequality we have used Lemma C.7 with g
(t)
h = ∆h(c
M (t), bϕ(t)) and in the second
inequality we have used the AM-GM inequality with parameter η. Collecting terms, we proceed via:
= αη
2
T
X
t=1
 H
X
h=1
Eep(t)h
∆h(c
M
(t), bϕ
(t))(xh, ah)
i
+ (γηα
2 )−1
JM ⋆
lat(πM ⋆
lat) −J
c
M (t)(πc
M (t))
!
+ α
2η + O(HCcov)
≤αη
2 Tβoff + α
2η + O(HCcov)
≤O
q
Ccov,st|A|H log(T)Tβoff + HCcov,st|A|

≤O
q
HCcov,st|A|T log(T) log
 |Mlat||Llat||Φ|HTδ−1
,
where in the first inequality we have used Lemma H.1 and the definition of γ in Algorithm 4 (cf. Eq.
(59)) and in the second inequality we have chosen η = 1/
√
T to balance the terms and used the bound
Ccov ≤Ccov,st|A|. We convert to an expected regret bound by picking δ appropriately, which gives
the final result. It remains to show Lemma H.1.
Proof of Lemma H.1.
Fix an iteration t ∈[T], and abbreviate c
M := c
M (t) and bϕ := bϕ(t). We
follow the analysis of maximum likelihood estimation from Geer; Zhang; Agarwal et al. [Gee00;
Zha06; AKKS20]. In particular, we quote Lemma 24 of [AKKS20], which in an abstract conditional
estimation framework with density class F states the following.
Lemma H.2 (Lemma 24 of Agarwal et al. [AKKS20]). Let D = {(xi, yi)} be a dataset collected with
xi ∼p(i)(x1:i−1, y1:i−1) and yi ∼f ⋆(· | xi), L(f, D) = Pn
i=1 ℓ(f, (xi, yi)) be any loss function
that decomposes additively, bf : D →F be an estimator, D′ be a tangent sequence D′ = {(exi, eyi)}
57

sampled independently via exi ∼p(i)(x1:i−1, y1:i−1) and eyi ∼f ⋆(· | exi). Then, with probability at
least 1 −δ, we have
−log ED′ exp

L( bf(D), D′)

≤−L( bf(D), D) + log
 |F|δ−1
,
(62)
For our purposes, we have that F = Mlat ◦Φ, the data distribution is collected adaptively (for each
h ∈[H]) via π(i) ∼p(i), x
(i)
h , a
(i)
h ∼dM ⋆
obs,π(i)
h
, and r
(i)
h , x
(i)
h+1 ∼M ⋆
obs(· | x
(i)
h , a
(i)
h ). For the loss
function L, we take
L((M, ϕ), D) = −
H
X
h=0
t
X
i=1
log
 
M ⋆
obs(r
(i)
h+1, ϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
[Mh ◦ϕh](r
(i)
h , ϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
!
−γ−1
2 (JM ⋆
lat(πM ⋆
lat) −JM(πM)).
We begin by upper bounding the quantity −L((c
M, bϕ)(D), D) appearing on the right-hand side of
Eq. (62), or equivalently lower bounding L((c
M, bϕ)(D), D). Let us abbreviate bV = J c
M(πc
M) and
V ⋆= JM ⋆
lat(πM ⋆
lat). Towards this, note that
L((c
M, bϕ)(D), D) =
H
X
h=0
t
X
i=1
log
h
c
Mh ◦bϕh
i
(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

−
H
X
h=0
t
X
i=1
log

M ⋆
obs(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

+ γ−1
2 (bV −V ⋆)
≥
H
X
h=0
t
X
i=1
log
h
c
Mh ◦bϕh
i
(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

−
H
X
h=0
max
[M ′◦ϕ′]∈Llat◦Φ
t
X
i=1
log

[M ′
h ◦ϕ′
h](r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

+ γ−1
2 (bV −V ⋆)
≥
H
X
h=0
t
X
i=1
log
 
M ⋆
lat,h ◦ϕ⋆
h

(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

−
H
X
h=0
max
[M ′◦ϕ′]∈Llat◦Φ
t
X
i=1
log
 [M ′
h ◦ϕ′
h](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

+ γ−1
2 (V ⋆−V ⋆)
=
H
X
h=0
t
X
i=1
log
 
M ⋆
lat,h ◦ϕ⋆
h

(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

−
H
X
h=0
max
[M ′◦ϕ′]∈Llat◦Φ
t
X
i=1
log
 [M ′
h ◦ϕ′
h](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )

,
where in the second line we have used Lemma D.8 with Assumption A.2 and in the third line we
have used the ERM property of c
M ◦bϕ together with decoder and model realizability. We claim that
this implies
L((c
M, bϕ)(D), D) ≥−log
 |Llat ◦Φ|Hδ−1
(63)
by concentration. Indeed, for each h ∈[H], i ∈[t], and [M ′ ◦ϕ′] ∈Llat ◦Φ, let
Z[M ′◦ϕ′]
i,h
= −1
2 log
 
M ⋆
obs(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
[M ′ ◦ϕ′](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
!
58

Applying Lemma C.1, we have that
t
X
i=1
log
 
M ⋆
obs(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
[M ′ ◦ϕ′](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
!
≥
t
X
i=1
−2 log
 
Eπ(i)∼p(i) Eπ(i)
"
exp
 
−1
2 log
 
M ⋆
obs(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
[M ′ ◦ϕ′](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
!!#!
−log
 δ−1
,
(64)
with probability at least 1 −δ, where we have recalled that data is gathered adaptively according to
π(i) ∼p(i).We now quote the following lemma from Zhang; Agarwal et al. [Zha06; AKKS20].
Lemma H.3 (Lemma 25 of Agarwal et al. [AKKS20]). For any D ∈∆(X) and p, q ∈[X →∆(Y)],
we have
−2 log Ex∼D,y∼q(·|x) exp

−1
2 log(q(y|x)/p(y|x))

≥Ex∼D

D2
H(q(· | x), p(· | x))

Proof of Lemma H.3. We include the proof for completeness. The result follows via the following
steps.
−2 log Ex∼D,y∼q(·|x) exp

−1
2 log(q(y|x)/p(y|x))

= −2 log Ex∼D,y∼q(·|x)
p
p(y|x)/q(y|x)
≥2

1 −Ex∼D,y∼q(·|x)
p
p(y|x)/q(y|x)

(∀x : log(x) ≤x −1)
= Ex∼D
h
2

1 −Ey∼q(·|x)
p
p(y|x)/q(y|x)
i
= Ex∼D

D2
H(p(· | x), q(· | x))

By Lemma H.3, we have that the right-hand-side of Eq. (64) is further lower bounded by
t
X
i=1
log
 
M ⋆
obs(r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
[M ′ ◦ϕ′](r
(i)
h , ϕ⋆
h+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
!
≥
t
X
i=1
Eπ(i)∼p(i) Eπ(i)
D2
H
 ϕ⋆
h+1♯M ⋆
obs(· | x
(i)
h , a
(i)
h ), [M ′ ◦ϕ′](· | x
(i)
h , a
(i)
h )

−log
 δ−1
≥−log
 δ−1
,
where the last line follows from the non-negativity of squared Hellinger. Taking a union bound over
M ′ ◦ϕ′ ∈Llat ◦Φ and h ∈[H] gives the desired lower bound in Eq. (63).
To conclude the proof, it remains to lower bound the left-hand side in Eq. (62). Here, note that:
−log ED′ exp

L((c
M, ˆϕ)(D), D′)

+ γ−1
2 (V ⋆−bV )
= −log ED′

exp

−1
2
H
X
h=1
t
X
i=1
log


M ⋆
obs(er
(i)
h , bϕh+1(ex
(i)
h+1) | x
(i)
h , a
(i)
h )
h
c
Mh ◦bϕh
i
(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )






= −
H
X
h=1
t
X
i=1
log Eπ(i)∼p(i) Eπ(i)

exp

−1
2 log


M ⋆
obs(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
h
c
Mh ◦bϕh
i
(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )





,
(65)
59

where we have used that in the “tangent sequence” D′ the current sample (er
(i)
h , ex
(i)
h+1) is independent
of (r
(i)
h , x
(i)
h+1). To bound this term, we again appeal to Lemma H.3, concluding that
−
H
X
h=1
t
X
i=1
log Eπ(i)∼p(i) Eπ(i)

exp

−1
2 log


M ⋆
obs(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )
h
c
Mh ◦bϕh
i
(r
(i)
h , bϕh+1(x
(i)
h+1) | x
(i)
h , a
(i)
h )






≥1
2
H
X
h=1
t
X
i=1
Eπ(i)∼p(i) Eπ(i)h
D2
H

[c
Mh ◦bϕh](xh, ah), bϕh+1♯M ⋆
obs(xh, ah)
i
Combining everything, we have:
1
2
 H
X
h=1
t
X
i=1
Eπ(i)∼p(i) Eπ(i)h
D2
H
h
c
Mh ◦bϕh
i
(xh, ah), bϕh+1♯M ⋆
obs(xh, ah)
i
+ γ−1(V ⋆−bV )
!
≤log
 |Llat||Φ|Hδ−1
+ log
 |Mlat||Φ|δ−1
Taking an additional union bound over t ∈[T], we have that with probability at least 1 −δ:
H
X
h=1
t
X
i=1
Eπ(i)∼p(i)
h Eπ(i)h
D2
H
h
c
Mh ◦bϕh
i
(xh, ah), bϕh+1♯M ⋆
obs(xh, ah)
i
+ γ−1
JM ⋆
lat(πM ⋆
lat) −JM (t)(πM (t))

≤O
 log
 |Mlat||Llat||Φ|HTδ−1
,
for all t ∈[T], as desired.
Corollary A.1 (Algorithmic modularity via SELFPREDICT.OPT). Under the same conditions as in
Lemma A.1, and for any base algorithm ALGlat, O2L with inputs T, K, Φ, SELFPREDICT.OPT, and
ALGlat achieves
E[Riskobs(TK)] ≲c1·Riskbase(K)+c2γ· K
√
T
q
HCcov,st|A| log(|Mlat||Llat||Φ|)+c3γ−1·KH,
for absolute constants c1, c2, c3. Consequently, for any ALGlat with base risk Riskbase(K), setting
γ and T appropriately gives
E[Riskobs(TK)] ≲Riskbase(K),
with a number of trajectories TK = e
O(K5H3Ccov,st|A| log2(|Mlat||Llat||Φ|)/(Riskbase(K))4).
Proof of Corollary A.1.
The first inequality simply follows by plugging the bound
of Estself;opt from Lemma A.1 into Theorem A.1.
For the second inequality, let ∆
=
c2
p
HCcov,st|A| log(|Mlat||Llat||Φ|).
The result follows by setting γ s.t.
c3γ−1HK
=
Riskbase(K) i.e. γ = c3
KH
Riskbase(K), and T such that γK∆
√
T
= Riskbase(K) i.e. T =
K4∆2γ2
Riskbase(K)2 =
K4∆2H2
(Riskbase(K))4 . Then the result follows by direct substitution and by noting that K
T
≤1 since
Riskbase(K) ≤1.
H.2
Proofs for Main Risk Bound (Theorem A.1)
Our main risk bound (Theorem A.1) follows as a special case of a more general theorem (Theo-
rem H.1), which holds for algorithm that satisfies a property we refer to as CorruptionRobust-ness
(Definition I.2). We now state the more general theorem, postponing its proof (and a formal definition
of corruption robustness) until Appendix I.
60

Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).
Assume REPself;opt satisfies Assumption A.1 with parameter γ > 0 and that Mlat is realizable (i.e.
M ⋆
lat ∈Mlat). Furthermore, let ALGlat be CorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ, ALGlat, and REPself;opt has expected risk
E[Riskobs(TK)] ≤c1 · Riskbase(K) + c2γ · K
T Estself;opt(T, γ) + c3γ−1 ·
 α2 + H

(66)
for absolute constants c1, c2, c3 > 0.
Our main risk bound (Theorem A.1) follows from the following lemma, which establishes that
any ALGlat is CorruptionRobust in the sense of Definition I.2 for a sufficiently large cor-
ruption robustness parameter. Below, for any POMDP f
M over state-action space S × A, we
write f
M(s1:h, a1:h) for the conditional probability over reward rh and sh+1 given s1:h, a1:h, i.e.
f
Mh(s1:h, a1:h) = f
Mh(rh, sh+1 = · | s1:h, a1:h).
Lemma H.4. Let M ⋆be any reference MDP and f
M be any POMDP with the same state and action
space. Then for any algorithm ALGlat, we have
E
f
M,ALGlat[RiskM ⋆(K)] ≤c1 EM ⋆,ALGlat[RiskM ⋆(K)]
+ c2 E
f
M,ALGlat
" K
X
k=1
H
X
h=1
E
f
M,π(k)h
D2
H

M ⋆
h(sh, ah), f
Mh(s1:h, a1:h)
i#
,
where c1, c2 > 0 are absolute constants. In particular, ALGlat is CorruptionRobust (Definition I.2)
with α = c2
√
KH.
Proof of Lemma H.4. Let us abbreviate ALG := ALGlat. For i ∈[K], let τ (i) denote the trajectory
(s
(i)
1 , a
(i)
1 , r
(i)
1 , . . . , s
(i)
H , a
(i)
H , r
(i)
H ). Let P := PM ⋆,ALG denote the law of {(π(i), τ (i))}i∈[K] under ALG
in the true MDP M ⋆, and Q := Pf
M,ALG denote the law of {(π(i), τ (i))}i∈[K] under ALG under the
POMDP f
M. Let us write M ⋆(π) and f
M(π) for the laws of trajectory τ sampled from policy π in
M ⋆or f
M respectively. Let bπ denote the policy output by the algorithm after K rounds of interaction
with the environment. By Lemma C.5 we have
E
f
M,ALGh
JM ⋆(πM ⋆) −JM ⋆(bπ)
i
≤3 EM ⋆,ALGh
JM ⋆(πM ⋆) −JM ⋆(bπ)
i
+4D2
H

PM ⋆,ALG, P
f
M,ALG
.
By the subadditivity property for squared Hellinger distance (Lemma C.4) applied to the sequence
π(1), τ (1), . . . , π(K), τ (K), we have
D2
H

PM ⋆,ALG, P
f
M,ALG
≤7 E
f
M,ALG
" K
X
k=1
D2
H(P(π
(k) | π
(1:k−1), τ
(1:k−1)), Q(π
(k) | π
(1:k−1), τ
(1:k−1)))+
D2
H(P(τ
(k) | π
(1:k), τ
(1:k−1)), Q(τ
(k) | π
(1:k), τ
(1:k−1)))
#
= 7 E
f
M,ALG
" K
X
k=1
D2
H(P(τ
(k) | π
(1:k), τ
(1:k−1)), Q(τ
(k) | π
(1:k), τ
(1:k−1)))
#
= 7 E
f
M,ALG
" K
X
k=1
D2
H

M ⋆(π
(k)), f
M(π
(k))
#
≤49 E
f
M,ALG
" K
X
k=1
H
X
h=1
E
f
M,π(k)h
D2
H

M ⋆
h(sh, ah), f
Mh(s1:h, a1:h)
i#
where in the second step we have used that P(π(k) | π(1:k), τ (1:k−1)) = Q(π(k) | π(1:k), τ (1:k−1))
since the histories are equivalent, in the third step we have used that the trajectories are gener-
ated by the MDP/PODMP M ⋆and f
M, respectively, in the fourth step we have again applied
the subadditivity property of the squared Hellinger distance (Lemma C.4) to the sequence
(s1, a1, r1, . . . , sH, aH, rH).
61

Theorem A.1 (Risk bound for O2L under self-predictive estimation). Suppose REPself;opt satisfies
Assumption A.1 with parameter γ > 0. Then Algorithm 1, with inputs T, K, Φ, REPself;opt, and
ALGlat has expected risk
E[Riskobs(TK)] ≤c1 · Riskbase(K) + c2γ · K
T Estself;opt(T, γ) + c3γ−1 · KH,
for absolute constants c1, c2, c3 > 0.
Proof of Theorem A.1.
This follows from Theorem H.1 as well as Lemma H.4, by taking
α = c2
√
KH and simplifying.
62

I
Additional Results for Appendix A: Self-Predictive Estimation
This section contains a more general result for algorithmic modularity under self-predictive estimation
(Theorem H.1), from which our main result is derived as a special case, along with associated
background, applications, and proofs. This section is organized as follows.
• Appendix I.1 presents: definitions for the ϕ-compressed POMDP and CorruptionRobust algo-
rithms (Appendix I.1.1), statements for properties of the ϕ-compressed dynamics (Appendix I.1.2).
The risk bound for O2L under self-predictive estimation and CorruptionRobustness (Theo-
rem H.1) is given in Appendix I.1.3, and a statement that the GOLF algorithm is CorruptionRobust
(Appendix I.1.4).
• Appendix I.2 presents for the proofs for the properties of the ϕ-compressed POMDPs.
• Appendix I.3 presents a proof for the risk bound of O2L under self-predictive estimation and
CorruptionRobustness.
• Appendix I.4 presents a proof that the GOLF algorithm is CorruptionRobust.
I.1
O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms
I.1.1
Definitions: ϕ-compressed POMDP and CorruptionRobustness
Consider iteration k ∈[K] of epoch t ∈[T] within O2L. Suppose that REPLEARN has
chosen decoder ϕ = ϕ(t) : X
→S.
Then, the latent algorithm has observed the data
D(t,k) = {ϕ(x
(t,k)
h
), a
(t,k)
h
, r
(t,k)
h
, ϕ(x
(t,k)
h+1)} collected from the preceding policies in the epoch:
π
(t,1)
lat ◦ϕ(t), . . . , π
(t,k−1)
lat
◦ϕ(t) (Line 8). Due to possible inaccuracies in the decoder ϕ, the dataset
D(t,k) may not be generated from a Markovian process and must instead be viewed as being generated
from a PODMP, formally defined as follows.
Definition I.1 (ϕ-compressed POMDP). The ϕ-compressed POMDP f
M ⋆
ϕ induced by M ⋆
obs and ϕ
is defined by:
1. Latent state space X
2. Action space A
3. Observation state space S
4. Latent reward functions R⋆
obs,h : X × A →[0, 1]
5. Latent dynamics P ⋆
obs,h : X × A →∆(X)
6. (Deterministic) observation function Oh : X →S defined by Oh(x) = ϕh(x),
7. Horizon H
8. Initial latent distribution P ⋆
obs(x0 | ∅)
Note that the latent space for the POMDP is the observation space of the latent-dynamics MDP M ⋆
obs,
and vice-versa; we adopt this terminology because—from the perspective of the base algorithm, the
observations xh can be viewed as a Markovian (yet partially observed process) that generates the
learned states ϕ(xh) on which the algorithm acts. We write eP πlat
ϕ
:= P
f
M ⋆
ϕ,πlat for the probability
distribution over trajectories (xh, sh, ah, rh)h∈[H] in the ϕ-compressed POMDP when playing policy
πlat : S×[H] →∆(A), where xh ∈X are the POMDP’s latent states, sh ∈S are the observed states,
and ah ∈A are the actions. We let eE
πlat
ϕ
:= E
f
M ⋆
ϕ,πlat denote the corresponding expectation. We write
ePϕ,h(sh+1 | s1:h, a1:h) = eP πlat
ϕ
(sh+1 | s1:h, a1:h) and ˜rϕ,h(rh | s1:h, a1:h) = eP πlat
ϕ
(rh | s1:h, a1:h)
for the conditional distributions of next states and rewards given the first h state-action pairs, which
are policy-independent. We also write f
M ⋆
ϕ(rh, sh+1 | s1:h, a1:h) = ˜rϕ,h(rh | s1:h, a1:h) ePϕ,h(sh+1 |
s1:h, a1:h) for the joint one-step probability. We will abbreviate f
M ⋆
ϕ(s1:h, a1:h) := f
M ⋆
ϕ(rh, sh+1 =
· | s1:h, a1:h).
63

Note that for any πlat, eP πlat
ϕ,h (sh+1 | sh, ah) is a well-defined (Markovian, policy-dependent) proba-
bility kernel, which is equivalent to
eP πlat
ϕ,h (sh+1 | sh, ah) =
X
s1:h−1,a1:h−1
eP πlat
ϕ,h (s1:h−1, a1:h−1 | sh, ah) ePϕ,h(sh+1 | s1:h, a1:h)
(67)
= eE
πlat
ϕ
h
ePϕ,h(sh+1 | s1:h, a1:h) | sh, ah
i
(68)
Similarly, ˜rπlat
ϕ,h (rh | sh, ah) is a Markovian and policy-dependent reward distribution which is
equivalent to
˜rπlat
ϕ,h (rh | sh, ah) =
X
s1:h−1,a1:h−1
eP πlat
ϕ,h (s1:h−1, a1:h−1 | sh, ah)˜rϕ,h(rh | s1:h, a1:h)
(69)
= eE
πlat
ϕ
[˜rϕ,h(rh | s1:h, a1:h) | sh, ah].
(70)
Finally, we let
f
M πlat,⋆
ϕ,h
(rh, sh+1 | sh, ah) = eE
πlat
ϕ
h
f
M ⋆
ϕ(rh, sh+1 | s1:h, a1:h) | sh, ah
i
(71)
denote the associated one-step model over joint rewards and transitions.
Our CorruptionRobustness condition asserts that the agent—when observing data from the ϕ(t)-
compressed dynamics f
M ⋆
ϕ—attains a risk bound for Mlat which is proportional to its risk when
observing data from Mlat itself, plus a term that captures the degree of misspecification between f
M ⋆
ϕ
and Mlat.
Definition I.2 (CorruptionRobust algorithm). We say that ALGlat is CorruptionRobust with
parameters α and Riskbase if there exists a constant c1 such that, for any (ϕ, Mlat) ∈Φ × Mlat, we
have
E
f
M ⋆
ϕ,ALGlat[Risk(K, ALGlat, Mlat)] ≤c1 · Riskbase(K)
+ α E
f
M ⋆
ϕ,ALGlat


v
u
u
t
K
X
k=1
H
X
h=1
Eπ(k)
lat ∼p(k) eE
π(k)
lat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆
ϕ,h(s1:h, a1:h)
i

,
where we recall the definition of the random variable Risk(K, ALGlat, Mlat) from Eq. (1), the
expectation E
f
M ⋆
ϕ,ALGlat denotes the interaction protocol of ALGlat in the ϕ-compressed dynamics
f
M ⋆
ϕ, and p(k) denotes the randomization distribution over latent policies that ALGlat plays.
I.1.2
Basic properties of the ϕ-compressed dynamics (Definition I.1)
We establish a number of basic properties for the ϕ-compressed POMDP and their relation to the
self-prediction guarantee obtained by REPself;opt. These properties are proved in Appendix I.2.
Firstly, we have the following change-of-measure lemma:
Lemma I.1 (Change of measure lemma). For any ϕ ∈Φ, f ∈[S × A →[0, 1]], h ∈[H], and
πlat ∈[S × [H] →∆(A)], we have:
eE
πlat
ϕ
[f(sh, ah)] = Eπlat◦ϕ[[f ◦ϕ](xh, ah)].
(72)
The next lemma states that the kernels of the ϕ-compressed POMDP are well-approximated by the
(Markovian) latent model fit by REPself;opt. We recall the instantaneous self-prediction error
[∆h(Mlat, ϕ)](xh, ah) := D2
H
 Mlat,h(ϕh(xh), ah),

ϕh+1♯M ⋆
obs,h

(xh, ah)

.
Lemma I.2 (Near-markovianity of the ϕ-compressed dynamics). For any decoder ϕ, base model
Mlat, and policy πlat : S × [H] →∆(A), we have:
H
X
h=0
eE
πlat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆
ϕ,h(s1:h, a1:h)
i
≤
H
X
h=0
Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].
(73)
64

Furthermore, we also have
H
X
h=0
eE
πlat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆,πlat
ϕ,h
(sh, ah)
i
≤
H
X
h=0
Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].
(74)
A corollary is the following lemma establishing errors between expectations under Mlat, the model
estimated by REPself;opt, and those under the ϕ-compressed POMDP f
M ⋆
ϕ.
Lemma I.3 (Simulation lemma). For any latent model Mlat with Markovian transition kernel
{Plat,h}h∈[H], latent policy πlat : S × [H] →∆(A), and decoder ϕ ∈Φ, we have that for all
f : S × A →[0, 1]:
|EMlat,πlat[f(sh, ah)] −eE
πlat
ϕ
[f(sh, ah)]|
≤
X
h′<h
Eπlat◦ϕh[Plat ◦ϕ]h(xh′, ah′) −ϕh+1♯P ⋆
obs,h(xh′, ah′)

tv
i
,
(75)
and thus for any sequence of policies π
(t)
lat, latent models M
(t)
lat, and decoders ϕ(t), we have:
T
X
t=1
H
X
h=0
|EM (t)
lat ,π(t)
lat [f(sh, ah)] −eE
π(t)
lat
ϕ(t)[f(sh, ah)]|
(76)
≤H
√
TH
v
u
u
t
T
X
t=1
H
X
h=0
Eπ(t)
lat ◦ϕ(t)
[∆h(M
(t)
lat, ϕ(t))](xh, ah)

.
(77)
I.1.3
Risk bound for O2L under CorruptionRobustness
We state the main risk bound for O2L under self-predictive estimation and the above definition of
corruption robustness.
Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).
Assume REPself;opt satisfies Assumption A.1 with parameter γ > 0 and that Mlat is realizable (i.e.
M ⋆
lat ∈Mlat). Furthermore, let ALGlat be CorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ, ALGlat, and REPself;opt has expected risk
E[Riskobs(TK)] ≤c1 · Riskbase(K) + c2γ · K
T Estself;opt(T, γ) + c3γ−1 ·
 α2 + H

(66)
for absolute constants c1, c2, c3 > 0.
I.1.4
Examples of CorruptionRobust algorithms
In this section, we establish that the GOLF algorithm satisfies the CorruptionRobust definition
(Definition I.2) with a parameter α ≈K−1/2. This improves upon the rate that would be obtained
by invoking the generic guarantee in Lemma H.4. We expect that several other algorithms can
be analyzed in a similar way, thereby leading to tight rates in the same fashion. We restate the
pseudocode in Algorithm 5 for convenience.
Let Mlat = (rlat, Plat) be given, and we let Q⋆
lat := QMlat,⋆, and Tlat,hf(s, a) := rlat,h(s, a) +
Es′∼Plat,h(s,a)[Vf(s′)]. We assume that the algorithm has a latent function class Falg which realizes
Q⋆
lat, as well as a helper class Galg which is Tlat-complete for Falg.
Assumption I.1 (Tlat-completeness). We have:
Q⋆
lat ∈Falg,
and
TlatFalg ⊆Galg.
For our analysis of GOLF, it is most natural to quantify the corruption levels in the following way.
Assumption I.2 (Corruption levels of Mlat and f
M ⋆
ϕ). Let ε2
rep be such that, for any sequence of
policies π
(k)
lat played by the algorithm when interacting with the ϕ-compressed POMDP, we have
K
X
k=1
H
X
h=1
Eπ(k)
lat ∼p(k)
lat
eE
π(k)
lat
ϕ

(rlat,h(sh, ah) −˜rπ(k)
ϕ,h (sh, ah))2 +
Plat,h(sh, ah) −eP π(k)
ϕ,h (sh, ah)

2
tv

≤ε2
rep.
(78)
65

Algorithm 5 GOLF [JLM21]
input: Function classes F and G, confidence width β > 0.
initialize: F (0) ←F, D
(0)
h ←∅∀h ∈[H].
1: for episode t = 1, 2, . . . , T do
2:
Select policy π(t) ←πf (t), where f (t) := arg maxf∈F(t−1) f(x1, πf,1(x1)).
3:
Execute π(t) for one episode and obtain trajectory (x
(t)
1 , a
(t)
1 , r
(t)
1 ), . . . , (x
(t)
H , a
(t)
H , r
(t)
H ).
4:
Update dataset: D
(t)
h ←D
(t−1)
h
∪
 x
(t)
h , a
(t)
h , x
(t)
h+1
	
∀h ∈[H].
5:
Compute confidence set:
F
(t) ←

f ∈F : L
(t)
h (fh, fh+1) −min
gh∈Gh L
(t)
h (gh, fh+1) ≤β ∀h ∈[H]

,
where
L
(t)
h (f, f ′) :=
X
(x,a,r,x′)∈D(t)
h

f(x, a) −r −max
a′∈A f ′(x′, a′)
2
, ∀f, f ′ ∈F.
6: end for
7: Output bπ = Unif(π(1:T )).
We note that
ε2
rep ≲
K
X
k=1
H
X
h=1
eE
π(k)
lat
ϕ

D2
H

Mlat,h(sh, ah), f
M
⋆,π(k)
lat
ϕ,h
(sh, ah)

≤
K
X
k=1
H
X
h=1
eE
π(k)
lat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆
ϕ,h(s1:h, a1:h)
i
by the data-processing inequality (cf. Eq. (90) and Eq. (80)) and the inequality ∥p−q∥2
tv ≤D2
H(p, q),
and thus a CorruptionRobustness bound in terms of εrep implies a CorruptionRobustness bound
in the sense of Definition I.2.
Theorem I.1 (Latent GOLF is CorruptionRobust). Under Assumption I.1 and Assumption I.2,
Algorithm 5 with β = c
 log
 |F||G|KHδ−1
+ εrep

, has regret
K
X
k=1
JMlat(πMlat) −JMlat(π
(k)) ≤O

H
p
CcovK log(K) log(|F||G|HK/δ)

+ O

H3/2q
KCcov log(K)ε2rep

,
and consequently is CorruptionRobust (Definition I.2) with parameters
α = H3/2
√
K
p
Ccov log(K) and Riskbase(K) = O
 H
√
K
p
Ccov log(K) log(|F||G|HK)

.
Corollary I.1 (GOLF applied in O2L).
Let us suppose that the appropriate assump-
tions for the estimator in Algorithm 4 to have regret bounded by Estself(T, γ)
=
O
 √HCcovT log(Ccov|Mlat||Llat||Φ|HT)

(Lemma A.1) hold. Then, we can take γ ≈K−1/2
and T ≈K4, and the bound Theorem H.1 gives an expected risk of ε with a number of trajecto-
ries TK = poly(Ccov, H, log|Mlat|, log|Φ|, log|Llat|) · 1/ε10, improving over the 1/ε14 rate of the
universal result (Corollary A.1).
I.2
Proofs for Appendix I.1.2: Properties of ϕ-compressed POMDPs
Lemma I.1 (Change of measure lemma). For any ϕ ∈Φ, f ∈[S × A →[0, 1]], h ∈[H], and
πlat ∈[S × [H] →∆(A)], we have:
eE
πlat
ϕ
[f(sh, ah)] = Eπlat◦ϕ[[f ◦ϕ](xh, ah)].
(72)
Proof of Lemma I.1. Recall that eP πlat
ϕ
denotes the law of (xh, sh, ah)h∈[H] in the ϕ-compressed
POMDP when playing policy πlat. For clarity, and to differentiate a random variable from its realiza-
tion, in the proofs below we will use upper-case notation such as {Sh = sh, Ah = ah, Xh = xh} to
indicate realizations of random variables in the POMDP.
66

Let ˜dπlat
h
(s, a) = eP πlat
ϕ
(Sh = s, Ah = a) be the marginalized occupancy measure for in the ϕ-
compressed POMDP f
M ⋆
ϕ. We write dπlat◦ϕ
h
:= dM ⋆
obs,πlat◦ϕ
h
. The left-hand side in Eq. (72) is equal
to:
eE
πlat
ϕ
[f(sh, ah)] =
X
s∈S,a∈A
˜dπlat
h
(s, a)f(s, a),
Meanwhile, the right-hand side is equal to:
Eπlat◦ϕ
h
[[f ◦ϕ](xh, ah)] =
X
s∈S,a∈A
f(s, a)
X
x:ϕ(x)=s
dπlat◦ϕ
h
(x, a).
So it only remains to show that, for each s
∈
S and a
∈
A, we have ˜dπlat
h
(s, a)
=
P
x:ϕ(x)=s dπlat◦ϕ
h
(x, a). Firstly, note that it is enough to show that P
xh:ϕ(xh)=sh dπlat◦ϕ
h
(xh) =
˜dπlat
h
(sh), since ˜dπlat
h
(sh, ah) =
˜dπlat
h
(sh)πlat(ah | sh) and P
xh:ϕ(xh)=sh dπlat◦ϕ
h
(xh, ah) =
P
xh:ϕ(xh)=sh dπlat◦ϕ
h
(xh)πlat(ah | ϕ(xh)) = πlat(ah | sh) P
xh:ϕ(xh)=sh dπlat◦ϕ
h
(xh).
Toward
this, we have:
X
xh:ϕ(xh)=sh
dπlat◦ϕ
h
(xh) =
X
xh:ϕ(xh)=sh
X
xh−1,ah−1∈X×A
dπlat◦ϕ
h−1 (xh−1, ah−1)P ⋆
obs,h(xh | xh−1, ah−1)
=
X
xh−1,ah−1∈X×A
dπlat◦ϕ
h−1 (xh−1, ah−1)
X
xh:ϕ(xh)=sh
P ⋆
obs,h(xh | xh−1, ah−1)
=
X
xh−1,ah−1∈X×A
dπlat◦ϕ
h−1 (xh−1, ah−1)P ⋆
obs,h(ϕ(xh) = sh | xh−1, ah−1)
At the same time,
˜dπlat
h
(sh) = eP πlat
ϕ
(Sh = sh)
=
X
ex,ea
eP πlat
ϕ
(Xh−1 = ex, Ah−1 = ea) eP πlat
ϕ
(Sh = sh | Xh−1 = ex, Ah−1 = ea)
=
X
ex,ea
eP πlat
ϕ
(Xh−1 = ex, Ah−1 = ea)P ⋆
obs(ϕ(xh) = sh | xh−1, ah−1),
where in the second equality we have used the definition of the observation function sh = O(xh) =
ϕ(xh).
To conclude, it remains to show that for all h, we have:
dπlat◦ϕ
h
(xh, ah) = eP πlat
ϕ
(Xh = xh, Ah = ah).
We do this by induction. Again, note that it is sufficient to establish dπlat◦ϕ
h
(xh) = eP πlat
ϕ
(Xh = xh).
The case h = 1 is clear. For the general case, we have:
dπlat◦ϕ
h
(xh) =
X
xh−1,ah−1∈X×A
dπlat◦ϕ
h−1 (xh−1, ah−1)P ⋆
obs(xh | xh−1, ah−1)
=
X
xh−1,ah−1∈X×A
eP πlat
ϕ
(Xh = xh−1, Ah−1 = ah−1)P ⋆
obs(xh | xh−1, ah−1)
=
X
xh−1,ah−1∈X×A
eP πlat
ϕ
(Xh = xh−1, Ah−1 = ah−1)
× eP πlat
ϕ
(Xh = xh | Xh−1 = xh−1, Ah−1 = ah−1)
= eP πlat
ϕ
(Xh = xh).
67

Lemma I.2 (Near-markovianity of the ϕ-compressed dynamics). For any decoder ϕ, base model
Mlat, and policy πlat : S × [H] →∆(A), we have:
H
X
h=0
eE
πlat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆
ϕ,h(s1:h, a1:h)
i
≤
H
X
h=0
Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].
(73)
Furthermore, we also have
H
X
h=0
eE
πlat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆,πlat
ϕ,h
(sh, ah)
i
≤
H
X
h=0
Eπlat◦ϕ[[∆h(Mlat, ϕ)](xh, ah)].
(74)
Proof of Lemma I.2. We begin with the first event. Note that, for any πlat, the PODMP kernel
f
M ⋆
ϕ,h(rh, sh+1 = · | s1:h, a1:h) can be written as:
f
M ⋆
ϕ,h(rh, sh+1 = · | s1:h, a1:h) =
X
xh,ah∈X×A
eP πlat
ϕ
(rh, sh+1 = · | xh, ah, s1:h, a1:h)
× eP πlat
ϕ
(xh, ah | s1:h, a1:h)
=
X
xh,ah∈X×A
eP πlat
ϕ
(rh, sh+1 = · | xh, ah) eP πlat
ϕ
(xh, ah | s1:h, a1:h),
where we have used f
M(rh, sh+1 = · | s1:h, a1:h) = eP πlat
ϕ
(rh, sh+1 = · | s1:h, a1:h), the law of total
probability, and that xh, ah is a sufficient statistic for rh and sh+1. We further note that
eP πlat
ϕ
(rh, sh+1 = · | xh, ah) = M ⋆
obs,h(rh, ϕh+1(xh+1) = · | xh, ah),
(79)
since sh+1 = Oh+1(xh+1) = ϕh+1(xh+1) is a deterministic function of xh+1 and rh, xh+1 ∼
M ⋆
obs,h(xh, ah). Thus, for a fixed h and t, and omitting the h indices on the decoder ϕ for cleanliness,
the expectation in equation Eq. (73) becomes:
eE
πlat
ϕ
h
D2
H

Mlat,h(sh, ah), f
M ⋆
ϕ,h(rh, sh+1 = · | s1:h, a1:h)
i
≤
X
s1:h,a1:h∈(S×A)h
eP πlat
ϕ
(s1:h, a1:h)
X
xh,ah
eP πlat
ϕ
(xh, ah | s1:h, a1:h)
× D2
H

Mlat,h(sh, ah), eP πlat
ϕ
(rh, sh+1 = · | xh, ah)

(Jensen)
=
X
s1:h,a1:h∈(S×A)h
xh,ah∈X×A
eP πlat
ϕ
(s1:h, a1:h) eP πlat
ϕ
(xh, ah | s1:h, a1:h)
× D2
H

Mlat,h(ϕ(xh), ah), eP πlat
ϕ
(rh, ϕ(xh+1) = · | xh, ah)

=
X
xh,ah∈X×A
eP πlat
ϕ
(xh, ah)D2
H
 Mlat(ϕ(xh), ah), M ⋆
obs,h(rh, ϕ(xh+1) = · | xh, ah)

(Simplifying & Eq. (79))
= Eπlat◦ϕ
D2
H
 Mlat(ϕ(xh), ah), M ⋆
obs,h(rh, ϕ(xh+1) = · | xh, ah)

(Change of measure (Lemma I.1))
= Eπlat◦ϕ
D2
H
 Mlat(ϕ(xh), ah), ϕ♯M ⋆
obs,h(xh, ah)

,
(By definition of ϕ♯M ⋆
obs)
as desired.
Summing over h ∈[H] we obtain the desired bound. The bound Eq. (74) is a
consequence of Eq. (73) and the data-processing inequality. Namely, using the definition of f
M ⋆,πlat
ϕ,h
from Eq. (71) and the joint convexity of the squared Hellinger distance we have:
D2
H

Mlat,h(· | sh, ah), f
M ⋆,πlat
ϕ,h
(· | sh, ah)

≤eE
πlat
ϕ
h
D2
H

Mlat,h(· | sh, ah), f
M ⋆
ϕ,h(· | s1:h, a1:h)

| sh, ah
i
.
(80)
68

Thus, we have
Eπlat
ϕ
h
D2
H

Mlat,h(· | sh, ah), f
M ⋆,πlat
ϕ,h
(· | sh, ah)
i
≤Eπlat
ϕ
h
Eπlat
ϕ
h
D2
H

Mlat,h(· | sh, ah), f
M ⋆
ϕ,h(· | s1:h, a1:h)

| sh, ah
ii
= Eπlat
ϕ
h
D2
H

Mlat,h(· | sh, ah), f
M ⋆
ϕ,h(· | s1:h, a1:h)
i
,
as desired.
Lemma I.3 (Simulation lemma). For any latent model Mlat with Markovian transition kernel
{Plat,h}h∈[H], latent policy πlat : S × [H] →∆(A), and decoder ϕ ∈Φ, we have that for all
f : S × A →[0, 1]:
|EMlat,πlat[f(sh, ah)] −eE
πlat
ϕ
[f(sh, ah)]|
≤
X
h′<h
Eπlat◦ϕh[Plat ◦ϕ]h(xh′, ah′) −ϕh+1♯P ⋆
obs,h(xh′, ah′)

tv
i
,
(75)
and thus for any sequence of policies π
(t)
lat, latent models M
(t)
lat, and decoders ϕ(t), we have:
T
X
t=1
H
X
h=0
|EM (t)
lat ,π(t)
lat [f(sh, ah)] −eE
π(t)
lat
ϕ(t)[f(sh, ah)]|
(76)
≤H
√
TH
v
u
u
t
T
X
t=1
H
X
h=0
Eπ(t)
lat ◦ϕ(t)
[∆h(M
(t)
lat, ϕ(t))](xh, ah)

.
(77)
Proof of Lemma I.3. Firstly note that, from Lemma I.1, the left-hand-side of Eq. (75) is equivalent
to
|EMlat,πlat[f(sh, ah)] −eE
πlat
ϕ
[f(sh, ah)]| = |EMlat,πlat[f(sh, ah)] −EM ⋆
obs,πlat◦ϕ[[f ◦ϕ](xh, ah)]|
(81)
For any πlat : S × [H] →∆(A), let dπlat
lat,h = dMlat,πlat
h
denote the occupancy in Mlat, and similarly
for any πobs : X × [H] →∆(A) let dπobs
obs,h(xh, ah) = dM ⋆
obs,πobs
h
(xh, ah) denote the occupancy in
M ⋆
obs. We overload notation by letting dπlat◦ϕ
obs,h (s, a) := P
x:ϕ(x)=s dπ◦ϕ
obs,h(x, a). We will establish the
stronger result that
dπlat
lat,h(·) −dπlat◦ϕ
obs,h (·)

tv ≤
X
h′<h
Eπlat◦ϕ
∥[Plat ◦ϕ](xh′, ah′) −ϕ♯P ⋆
obs(xh′, ah′)∥tv

,
(82)
where the tv norm on the left-hand-side is over S × A. Note that this implies the desired bound on
Eq. (81) by Holder’s inequality. We prove this by induction over h. For the base case (h = 0), we
have:
X
s1,a1
dπlat
lat,1(s1, a1) −dπlat◦ϕ
obs
(s1, a1)

=
X
s1,a1
Plat,0(s1 | ∅)πlat(a1 | s1) −
X
x1=ϕ(x1)=s1
dπlat◦ϕ
obs
(x1, a1)

=
X
s1,a1
Plat,0(s1 | ∅)πlat(a1 | s1) −
X
x1=ϕ(x1)=s1
P ⋆
obs,0(x1 | ∅)πlat(a1 | ϕ(x1))

=
X
s1
Plat,0(s1 | ∅) −ϕ1♯P ⋆
obs,0(s1 | ∅)

X
a1
πlat(a1 | s1)
=
Plat,0(∅) −ϕ1♯P ⋆
obs,0(∅)

tv.
69

For the general case, let us further overload notation by letting dπ◦ϕ
obs,h(sh) = P
ah dπ◦ϕ
obs,h(sh, ah)
and P ⋆
obs(sh | xh−1, ah−1) = ϕ♯P ⋆
obs(sh | xh−1, ah−1) = P
xh:ϕ(xh)=sh P ⋆
obs(xh | xh−1, ah−1).
Let us also abbreviate π := πlat.
Firstly note that it is sufficient to establish the result for
P
sh∈S
dπ
lat,h(sh) −dπ◦ϕ
obs,h(sh)
, since
X
sh,ah∈S×A
dπ
lat,h(sh, ah) −dπ◦ϕ
obs,h(sh, ah)
 =
X
sh,ah∈S×A
dπ
lat,h(sh) −dπ◦ϕ
obs,h(sh)
π(ah | sh)
=
X
sh∈S
dπ
lat,h(sh) −dπ◦ϕ
obs,h(sh)
.
Below, all summations over sh (resp. xh) with domain unspecified are over S (resp. X), and likewise
for summations over sh, ah or xh, ah. We have:
X
sh
dπ
lat,h(sh) −dπ◦ϕ
obs,h(sh)

=
X
sh
dπ
lat,h(sh) −
X
xh:ϕ(xh)=sh
dπ◦ϕ
obs,h(xh)

=
X
sh

X
sh−1,ah−1
dπ
lat,h(sh−1, ah−1)Plat,h(sh | sh−1, ah−1)
−
X
xh:ϕ(xh)=sh
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)P ⋆
obs,h(xh | xh−1, ah−1)

=
X
sh

X
sh−1,ah−1
dπ
lat,h(sh−1, ah−1)Plat,h(sh | sh−1, ah−1)
−
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)P ⋆
obs,h(sh | xh−1, ah−1)

=
X
sh

X
sh−1,ah−1
dπ
lat,h(sh−1, ah−1)Plat,h(sh | sh−1, ah−1)
−
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)Plat,h(sh | ϕ(xh−1), ah−1)
+
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)Plat,h(sh | ϕ(xh−1), ah−1)
−
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)P ⋆
obs,h(sh | xh−1, ah−1)

≤
X
sh−1,ah−1

dπ
lat,h(sh−1, ah−1) −
X
xh−1:ϕ(xh−1)=sh−1
dπ◦ϕ
obs,h(xh−1, ah−1)

X
sh
Plat,h(sh | sh−1, ah−1)
+
X
sh

X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)
 (Plat,h ◦ϕ)(sh | xh−1, ah−1) −P ⋆
obs,h(sh | xh−1, ah−1)


≤
dπ
lat,h−1(·) −dπ◦ϕ
obs,h−1(ϕ−1(·))

tv
+
X
xh−1,ah−1
dπ◦ϕ
obs,h(xh−1, ah−1)
X
sh
(Plat,h ◦ϕ)(sh | xh−1, ah−1) −P ⋆
obs,h(sh | xh−1, ah−1)

≤
dπ
lat,h−1(·) −dπ◦ϕ
obs,h−1(ϕ−1(·))

tv + Eπ◦ϕh[Plat,h ◦ϕ](xh−1, ah−1) −ϕ♯P ⋆
obs,h(xh−1, ah−1)

tv
i
.
70

From which it follows that, for each h, we have:
dπ
lat,h(·) −dπ◦ϕ
obs,h(ϕ−1(·))

tv ≤
X
h′<h
Eπ◦ϕh[Plat ◦ϕ]h′(xh′, ah′) −ϕh′+1♯P ⋆
obs,h′(xh′, ah′)

tv
i
≤
X
h′∈[H]
Eπ◦ϕh[Plat ◦ϕ]h′(xh′, ah′) −ϕh′+1♯P ⋆
obs,h′(xh′, ah′)

tv
i
.
I.3
Proofs for Appendix I.1.3: Risk Bound Under CorruptionRobustness (Theorem H.1)
Theorem H.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness).
Assume REPself;opt satisfies Assumption A.1 with parameter γ > 0 and that Mlat is realizable (i.e.
M ⋆
lat ∈Mlat). Furthermore, let ALGlat be CorruptionRobust (Definition I.2) with parameter α.
Then, O2L (Algorithm 1) with inputs T, K, Φ, ALGlat, and REPself;opt has expected risk
E[Riskobs(TK)] ≤c1 · Riskbase(K) + c2γ · K
T Estself;opt(T, γ) + c3γ−1 ·
 α2 + H

(66)
for absolute constants c1, c2, c3 > 0.
Proof of Theorem H.1. Let us write π
(t,K+1)
lat
= bπ
(t)
lat and, for any t, k ∈[T] × [K + 1], π
(t,k)
obs :=
π
(t,k)
lat ◦ϕ(t). Let p
(t,k)
obs denote the distributions of played policies π
(t,k)
obs induced by the interaction
of ALGlat and REPself;opt inside the O2L algorithm. Let us write the online sum of self-prediction
errors as
ε2
rep :=
T
X
t=1
K+1
X
k=1
H
X
h=0
Eπ(t,k)
obs
∼p(t,k) Eπ(t,k)
obs

D2
H
 [M
(t)
lat ◦ϕ
(t)]h(xh, ah), ϕ
(t)
h+1♯M ⋆
obs,h(xh, ah)

(83)
Since the final output policy of O2L satisfies bπlat = Unif(bπ
(1)
lat, . . . , bπ
(T )
lat) (Line 12), we have
E[Riskobs(TK)] = 1
T
T
X
t=1
E
h
JM ⋆
obs(π⋆
obs) −JM ⋆
obs(bπ
(t)
obs)
i
.
We take the following decomposition on the risk
JM ⋆
obs(π⋆
obs) −JM ⋆
obs(bπ
(t)
obs) = JM ⋆
lat(πM ⋆
lat) −JM (t)
lat (πM (t)
lat ) + JM (t)
lat (πM (t)
lat ) −JM (t)
lat (bπ
(t)
lat)
|
{z
}
At
+ JM (t)
lat (bπ
(t)
lat) −JM ⋆
obs(bπ
(t)
obs)
|
{z
}
Bt
.
(84)
We will show that E
hPT
t=1 At
i
≲TRegbase(K) + α
√
T E[εrep] and that E
hPT
t=1 Bt
i
≲
√
TH E[εrep], then return to the first term JM ⋆
lat(πM ⋆
lat) −JM (t)
lat (πM (t)
lat ) at the end of the proof.
To bound E
hPT
t=1 At
i
, we note that
T
X
t=1
E[At] ≤c1TRiskbase(K)+
α
T
X
t=1
E


v
u
u
t
K
X
k=1
H
X
h=1
Eπ(t,k)
lat
∼p(t,k)
lat
eE
π(t,k)
lat
ϕ(t)
h
D2
H

M
(t)
lat,h(sh, ah), f
M ⋆
ϕ(t),h(s1:h, a1:h)
i


≤c1TRiskbase(K) + α
T
X
t=1
E


v
u
u
t
K
X
k=1
H
X
h=1
Eπ(t,k)
lat
∼p(t,k)
lat
Eπ(t,k)
lat
◦ϕ(t)
[∆h(M
(t)
lat, ϕ(t))](xh, ah)



≤c1TRiskbase(K) + α
√
T E


v
u
u
t
T
X
t=1
K
X
k=1
H
X
h=1
Eπ(t,k)
obs
∼p(t,k)
obs
Eπ(t,k)
obs

[∆h(M
(t)
lat, ϕ(t))](xh, ah)



≤c1TRiskbase(K) + α
√
T E[εrep].
71

where the first line follows from the CorruptionRobust definition (Definition I.2), the second line
follows from Lemma I.2, the third line follows by Cauchy-Schwartz, and the last line recalls the
definition of εrep from Eq. (83).
For the term PT
t=1 Bt, for any πlat : S × [H] →∆(A) we let Qπlat
lat(t),h = T
M (t)
lat
h
Qπlat
lat(t),h+1 be the
Qπlat function of the latent MDP M
(t)
lat. Note that
T
X
t=1
n
JM (t)
lat (bπ
(t)
lat) −Ebπ(t)
lat ◦ϕ(t)h
[Qbπ(t)
lat(t) ◦ϕ
(t)]1(x1, a1)
io
(85)
=
T
X
t=1
EM (t)
lat ,bπ(t)
lat
h
Qbπ(t)
lat(t),1(s1, a1)
i
−Ebπ(t)
lat ◦ϕ(t)h
[Qbπ(t)
lat(t) ◦ϕ
(t)]1(x1, a1)
i
≤
T
X
t=1
Ebπ(t)
lat ◦ϕ(t)[P
(t)
lat ◦ϕ
(t)]0(∅) −ϕ
(t)
1 ♯P ⋆
obs,0(∅)

tv

(by Lemma I.3)
≤
T
X
t=1
H
X
h=0
Ebπ(t)
lat ◦ϕ(t)[P
(t)
lat ◦ϕ
(t)]h(xh, ah) −ϕ
(t)
h+1♯P ⋆
obs,h(xh, ah)

tv

≤
√
THεrep,
(by Cauchy-Schwartz)
so it is enough to bound
T
X
t=1

Ebπ(t)
lat ◦ϕ(t)
[Q
bπ(t)
lat
lat(t) ◦ϕ
(t)]1(x1, a1)

−JM ⋆
obs(bπ
(t)
obs)

.
Fix t and h, whose indexing we omit below for cleanliness. Note that, for any πlat : S×[H] →∆(A),
we have:
Eπlat◦ϕ

[Qπlat
lat ◦ϕ]h(xh, ah) −T M ⋆
obs,πlat◦ϕ
h
[Qπlat
lat ◦ϕ]h+1(xh, ah)
2
(86)
≤2 Eπlat◦ϕh [rlat ◦ϕ]h −r⋆
obs,h
2(xh, ah)
i
(87)
+ 2 Eπlat◦ϕ

EPlat,h(ϕ(xh),ah)
h
Qπlat
lat,h+1(·, πlat)
i
−EP ⋆
obs,h(xh,ah)

[Qπlat
lat ◦ϕ]h+1(·, πlat)
2
(88)
≤2 Eπlat◦ϕh [rlat ◦ϕ]h −r⋆
obs,h
2(xh, ah) +
Plat,h(ϕ(xh), ah) −ϕh+1♯P ⋆
obs,h(xh, ah)
2
tv
i
(89)
≤4 Eπlat◦ϕ
D2
H
 Mlat,h(ϕh(xh), ah), ϕh+1♯M ⋆
obs,h(xh, ah)

,
(90)
where the final line follows from two applications of the data-processing inequality (since
Mlat,h(rh, sh+1
| ϕh(xh), ah) = Rlat,h(rh
| ϕh(xh), ah)Plat,h(sh+1
| ϕh(xh), ah) and
ϕh+1♯M ⋆
obs,h(rh, sh+1 | xh, ah) = R⋆
obs,h(rh | xh, ah)ϕh+1♯P ⋆
obs,h(sh+1 | xh, ah)) as well as
the bound ∥p −q∥2
tv ≤D2
H(p, q). Summing this over t, h and using a standard decomposition for
72

regret (Lemma C.6) gives:
T
X
t=1

Ebπ(t)
lat ◦ϕ(t)
[Q
bπ(t)
lat
lat(t) ◦ϕ
(t)]1(x1, a1)

−JM ⋆
obs(bπ
(t)
obs)

=
T
X
t=1
H
X
h=1
Ebπ(t)
lat ◦ϕ(t)
[Q
bπ(t)
lat
lat(t) ◦ϕ
(t)]h(xh, ah) −T
M ⋆
obs,bπ(t)
lat ◦ϕ(t)
h
[Q
bπ(t)
lat
lat(t) ◦ϕ
(t)]h+1(xh, ah)

(Lemma C.6)
≤
√
TH
v
u
u
t
T
X
t=1
H
X
h=1
Ebπ(t)
lat ◦ϕ(t)
"
[Q
bπ(t)
lat
lat(t) ◦ϕ(t)]h(xh, ah) −T
M ⋆
obs,bπ(t)
lat ◦ϕ(t)
h
[Q
bπ(t)
lat
lat(t) ◦ϕ(t)]h+1(xh, ah)
2#
≤
√
4TH
v
u
u
t
T
X
t=1
H
X
h=1
Ebπ(t)
lat ◦ϕ(t)h
D2
H
h
M
(t)
lat,h ◦ϕ
(t)
h
i
(xh, ah), ϕ
(t)
h+1♯M ⋆
obs,h(xh, ah)
i
(By Eq. (90))
≤
√
4THεrep.
Returning to the decomposition of Eq. (84) and combining everything gives:
E[Riskobs] ≤1
T
( T
X
t=1
E
h
JM ⋆
lat(πM ⋆
lat) −JM (t)
lat (πM (t)
lat )
i)
+ 1
T

α
√
T + 4
√
TH

E[εrep]
+ c1 · Riskbase(K)
≤1
T
( T
X
t=1
E
h
J(π⋆) −JM (t)
lat (πM (t)
lat ) + γε2
rep
i)
+ γ−1
T

α
√
T + 4
√
TH
2
+ c1 · Riskbase(K)
≤γ 2K
T Estself;opt(T, γ) + 2γ−1 α2 + 16H

+ c1 · Riskbase(K),
where the second inequality follows by AM-GM applied to the middle term and the third
inequality follows from: i) Jensen’s inequality, ii) Assumption A.1 applied to the distributions
¯p
(t)
obs = 1
K
PK
k=1 p
(t,k)
obs , iii) the bound K +1 ≤2K, and iv) the inequality (x+y)2 ≤2(x2 +y2).
I.4
Proofs for Appendix I.1.4: Examples of CorruptionRobust Algorithms
Theorem I.1 (Latent GOLF is CorruptionRobust). Under Assumption I.1 and Assumption I.2,
Algorithm 5 with β = c
 log
 |F||G|KHδ−1
+ εrep

, has regret
K
X
k=1
JMlat(πMlat) −JMlat(π
(k)) ≤O

H
p
CcovK log(K) log(|F||G|HK/δ)

+ O

H3/2q
KCcov log(K)ε2rep

,
and consequently is CorruptionRobust (Definition I.2) with parameters
α = H3/2
√
K
p
Ccov log(K) and Riskbase(K) = O
 H
√
K
p
Ccov log(K) log(|F||G|HK)

.
Proof of Theorem I.1. Recall that the agent is observing data from the ϕ-compressed POMDP
f
M ⋆
ϕ, and thus the datasets are of the form D
(k)
h
= D
(k)
ϕ,h = {ϕ(x
(i)
h ), a
(i)
h , r
(i)
h , ϕ(x
(i)
h+1)}k−1
i=1 . For any
πlat ∈Πlat, we define
eT πlat
ϕ,h f(sh, ah) = ˜rπlat
ϕ,h (sh, ah) + Es′∼e
P
πlat
ϕ,h (sh,ah)[f(s′)],
where ˜rπlat
ϕ,h and eP πlat
ϕ,h are the policy-dependent Markov operators defined in Eq. (67) and Eq. (69).
As a consequence, we observe the following misspecification guarantee for Tlat.
73

Lemma I.4 (Misspecification guarantee for Tlat).
∀f : S × A →[0, 1] :
K
X
k=1
H
X
h=1
eE
π(k)
ϕ

Tlat,hf(sh, ah) −eT π(k)
ϕ,h f(sh, ah)
2
≤O(ε2
rep).
Proof of Lemma I.4. Follows from Assumption I.2 and the definitions of eT π(k)
ϕ,h
and Tlat,h.
We begin with the following lemmas, which will be proved in the sequel.
Lemma I.5 (Optimism). For the choice of β in Theorem I.1, with probability at least 1 −δ, we have
that for all k ∈[K]:
Q⋆
lat ∈F
(k).
Lemma I.6 (Small in-sample squared Bellman errors). With probability at least 1 −δ, we have that
for all k ∈[K], h ∈[H], and f ∈F (k):
k−1
X
i=1
eE
π(i)
ϕ

f(sh, ah) −eT π(i)
ϕ,h f(sh, ah)
2
≤O(β).
Let us write π
(k)
obs := π(k) ◦ϕ. Let us introduce the shorthand ˜d
(k)
obs,h := Pk−1
i=1 d
π(k)
obs
obs,h, where dπ
obs is
the occupancy for M ⋆
obs, and also the burn-in time
κh(x, a) := min
(
k :
k−1
X
i=1
dπ(k)
obs,h(x, a) ≥Ccovµ⋆
h(x, a)
)
.
Let us recall, from the analysis of [XFBJK23], that for any h ∈[H] and f : S × A →[0, 1] we have
K
X
k=1
Eπ(k)[f(sh, ah)I{k < κh(sh, ah)}] ≤2Ccov,
(91)
as well as
H
X
h=1
K
X
k=1
X
s,a
(d
π(k)
obs
h
(x, a)I{k ≥κh(x, a)})2
˜d
(k)
h (x, a)
≤O(HCcov log(K)).
(92)
74

X
k
JMlat(πMlat) −JMlat(π
(k)) ≤
K
X
k=1
H
X
h=1
EMlat,π(k)[f
(k)(sh, ah) −Tlatf
(k)(sh, ah)]
(Optimism (Lemma I.5))
≤
K
X
k=1
H
X
h=1
eE
π(k)
ϕ
[f
(k)(sh, ah) −Tlatf
(k)(sh, ah)] + H3/2q
Kε2rep
(Simulation Lemma Lemma I.3)
=
K
X
k=1
H
X
h=1
Eπ(k)◦ϕ[[(f
(k) −Tlatf
(k)) ◦ϕ](xh, ah)] + H3/2q
Kε2rep
(Change of measure Lemma I.1)
≤
K
X
k=1
H
X
h=1
Eπ(k)◦ϕ[[(f
(k) −Tlatf
(k)) ◦ϕ](xh, ah)I{k ≥κh(xh, ah)}]
+ 2HCcov + H3/2q
Kε2rep
(Burn-in time Eq. (91))
≤
K
X
k=1
H
X
h=1
Eπ(k)◦ϕhh
(f
(k) −eT π(k)
ϕ,h f
(k)) ◦ϕ
i
(xh, ah)I{k ≥κh(xh, ah)}
i
|
{z
}
(I)
+
K
X
k=1
H
X
h=1
Eπ(k)◦ϕhh
(eT π(k)
ϕ,h f
(k) −Tlat,hf
(k)) ◦ϕ
i
(xh, ah)
i
|
{z
}
(II)
+ 2HCcov + H3/2q
Kε2rep
Note that, by change of measure (Lemma I.1) and the misspecification guarantee (Lemma I.4), the
second term is bounded by:
(II) =
K
X
k=1
H
X
h=1
eE
π(k)
ϕ
h
(eT π(k)
ϕ,h f
(k) −Tlat,hf
(k))(sh, ah)
i
≤
q
KHε2rep.
Turning to the first term, we have:
H
X
h=1
K
X
k=1
Eπ(k)
obs
hh
(f
(k) −eT π(k)
ϕ,h f
(k)) ◦ϕ
i
(xh, ah)I{k ≥κh(xh, ah)}
i
(93)
≤
v
u
u
t
H
X
h=1
K
X
k=1
X
x,a
(d
π(k)
obs
h
(x, a)I{k ≥κh(x, a)})2
˜d
(k)
h (x, a)
v
u
u
t
H
X
h=1
K
X
k=1
E ˜d(k)
obs

(f (k) −eT π(k)
ϕ,h f (k)) ◦ϕ
2
(xh, ah)

(94)
≤
p
HCcov log(K)
v
u
u
t
H
X
h=1
K
X
k=1
E ˜d(k)
obs

(f (k) −eT π(k)
ϕ,h f (k)) ◦ϕ
2
(xh, ah)

(coverability potential Eq. (92))
=
p
HCcov log(K)
v
u
u
t
H
X
h=1
K
X
k=1
k−1
X
i=1
eE
π(i)
ϕ

f (k)(sh, ah) −eT π(k)
ϕ,h f (k)(sh, ah)
2
(change of measure, Lemma I.1)
≤O

H
p
CcovK log(K)β

,
(95)
75

where we have used that, from Lemma I.6, we have:
H
X
h=1
K
X
k=1
k−1
X
i=1
eE
π(i)
ϕ

f
(k)(sh, ah) −eT π(i)
ϕ
f
(k)(sh, ah)
2
≤O(βHK).
This gives an upper bound on the regret of
K
X
k=1
JMlat(πMlat) −JMlat(π
(k)) ≤O

H
p
CcovK log(K)β + H3/2q
Kε2rep

.
Using that β = O

log

|F||G|HK
δ

+ εrep

and simplifying gives
K
X
k=1
JMlat(πMlat)−JMlat(π
(k)) ≤O

H
p
CcovK log(K) log(|F||G|HK/δ)

+O

H3/2q
KCcov log(K)ε2rep

,
as desired. It only remains to establish the concentrations results.
Concentration analysis.
We establish the concentration results of Lemma I.5 and Lemma I.6.
Proof of Lemma I.6. Let
Xk(h, f) =
 fh(s
(k)
h , a
(k)
h ) −r
(k)
h −fh+1(s
(k)
h+1)
2 −

eT π(k)
ϕ
fh(s
(k)
h , a
(k)
h ) −r
(k)
h −fh+1(s
(k)
h+1)
2
.
Let Fk,h = {s
(i)
1 , a
(i)
1 , r
(i)
1 , . . . , s
(i)
H , a
(i)
H , r
(i)
H }k
i=1. Note that
E

r
(k)
h + fh+1(s
(k)
h+1) | Fk,h

= E

r
(k)
h + fh+1(s
(k)
h+1) | π
(k)
= E

E

r
(k)
h + fh+1(s
(k)
h+1) | s
(k)
h , a
(k)
h , π
(k)
| π
(k)
= E
h
eT π(k)
ϕ
f(s
(k)
h , a
(k)
h ) | π
(k)i
= eE
π(k)
ϕ
h
eT π(k)
ϕ
f(sh, ah)
i
,
and thus that
E[Xk(h, f) | Fk,h] = eE
π(k)
ϕ

fh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
.
Next, note that
Var[Xk(h, f) | Fk,h] ≤E
h
(Xk(h, f))2 | Fk,h
i
≤16 E

fh(s
(k)
h , a
(k)
h ) −eT π(k)
ϕ
fh(s
(k)
h , a
(k)
h )
2
| Fk,h

= 16 E[Xk(h, f) | Fk,h].
By Freedman’s inequality (Lemma C.2, Lemma C.3), we have that with probability at least 1 −δ:

X
t<k
Xt(h, f) −
X
t<k
E[Xt(h, f) | Ft,h]
 ≤O


s
log(1/δ)
X
t<k
E[Xt(h, f) | Ft,h] + log(1/δ)


Taking a union bound over [K] × [H] × F, we have that for all k, h, f, with probability at least 1 −δ:

X
t<k
Xt(h, f) −
X
t<k
eE
π(k)
ϕ

fh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
(96)
≤O
 s
ι
X
t<k
eE
π(k)
ϕ

fh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
+ ι
!
,
(97)
where ι = log(|F|HK/δ). We now show that
X
t<k
Xt(h, f
(k)) ≤β + O(εrep + ι) = O(β),
(98)
76

which will imply, from Eq. (96), that
X
t<k
eE
π(k)
ϕ

fh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
≤O(ι + β) = O(β),
as desired. To see Eq. (98), let
∆k =
X
t<k
 Tlatf
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2−

eT π(t)
ϕ
f
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2
and then note that:
X
t<k
Xt(h, f
(k)) =
X
t<k
 f
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2 −

eT π(t)
ϕ
f
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2
=
X
t<k
 f
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2
−
X
t<k
 Tlatf
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2 + ∆k
≤
X
t<k
 f
(k)
h (s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2
−inf
gh∈Gh
X
t<k
 g(s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2 + ∆k
≤β + ∆k.
where the second-to-last line follows from TlatF ⊆G and the last line follows from the definition of
the confidence set. It remains to show that ∆k ≤O(εrep +ι), which we do via a similar concentration
argument. Namely, let
Yt(h, f) =
 Tlatfh(s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2−

eT π(t)
ϕ
fh(s
(t)
h , a
(t)
h ) −r
(t)
h −f
(k)
h+1(s
(t)
h+1)
2
,
and note that, as before,
E[Yt(h, f) | Ft,h] = eE
π(t)
ϕ

Tlatfh(sh, ah) −eT π(t)
ϕ
fh(sh, ah)
2
,
and
Var[Yt(h, f) | Ft,h] ≤16 E[Yt(h, f) | Ft,h],
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that,
with probability at least 1 −δ,

X
t<k
Yt(h, f) −
X
t<k
eE
π(k)
ϕ

Tlatfh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
(99)
≤O
 s
ι
X
t<k
eE
π(k)
ϕ

Tlatfh(sh, ah) −eT π(k)
ϕ
fh(sh, ah)
2
+ ι
!
,
(100)
where ι = log(|F|HK/δ). Recalling the misspecification assumption Lemma I.4, this implies that
X
t<k
Yt(h, f) ≤O(εrep + ι),
for all h, f, k, with high probability. Applying this to f = f (k) concludes the result.
Proof of Lemma I.5. We use similar arguments to the preceding lemma. Let Q⋆
lat,h := Q⋆
Mlat,h. The
aim is to show that, for all h ∈[H], k ∈[K], g ∈G, we have:
X
t<k
 g(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h+1(s
(t)
h+1)
2 −
 Q⋆
lat,h(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h(s
(t)
h+1)
2 ≥−β,
77

from which the conclusion will follow. We show that
X
t<k
 g(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h+1(s
(t)
h+1)
2 −

eT π(t)
ϕ
Q⋆
lat,h(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h(s
(t)
h+1)
2
|
{z
}
:=Wt(h,g)
≥−β/2,
(101)
and also that
X
t<k

eT π(t)
ϕ
Q⋆
lat,h(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h(s
(t)
h+1)
2
−
 Q⋆
lat,h(s
(t)
h , a
(t)
h ) −r
(t)
h −Q⋆
lat,h(s
(t)
h+1)
2
|
{z
}
:=Vt(h)
≥−β/2.
(102)
For Eq. (101), note that
E[Wt(h, g) | Ft,h] = eE
π(t)
ϕ

gh(sh, ah) −eT π(t)
ϕ,h Q⋆
lat,h(sh, ah)
2
,
(103)
and that Var[Wt(h, g) | Ft,h] ≤16 E[Wt(h, g) | Ft,h]. By Freedman, this gives

X
t<k
Wt(h, g) −
X
t<k
E[Wt(h, g) | Ft,h]
 ≤O


s
ι
X
t<k
E[Wt(h, g) | Ft,h] + ι


≤1
2 E[Wt(h, g) | Ft,h] + O(ι),
or in other words
X
t<k
Wt(h, g) ≥1
2
X
t<k
E[Wt(h, g) | Ft,h] −O(ι) ≥−O(ι),
using the non-negativity of Eq. (103). For Eq. (102), note that
E[Vt(h) | Ft,h] = −eE
π(t)
ϕ

Tlat,hQ⋆
lat,h −eT π(t)
ϕ,h Q⋆
lat,h
2
≥−εrep,
(104)
and that Var[Vt(h) | Ft,h] ≤16eE
π(t)
ϕ

Tlat,hQ⋆
lat,h −eT π(t)
ϕ,h Q⋆
lat,h
2
. By Freedman, this gives

X
t<k
Vt(h) −
X
t<k
E[Vt(h) | Ft,h]

(105)
≤O
 s
ι
X
t<k
eE
π(t)
ϕ

TlatQ⋆
lat,h+1(sh, ah) −eT π(t)
ϕ,h Q⋆
lat,h+1(sh, ah)
2
+ ι
!
(106)
= O(εrep + ι),
(107)
or in other words
X
t<k
Vt(h) ≥
X
t<k
E[Vt(h) | Ft,h] −O(εrep + ι) ≥−O(εrep + ι),
where we have used Eq. (104).
78

NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope?
Answer: [Yes]
Justification: All results in this paper are of a theoretical nature, and the stated contributions in the
abstract and introduction are given in a precise, formal language.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims made in the
paper.
• The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: All results in this paper are of a theoretical nature – we precisely state the conditions
under which our results hold.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
aren’t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
79

3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
Justification: Each theoretical result is stated with all necessary assumptions and is accompanied
by complete (and correct) proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experi-
mental results of the paper to the extent that it affects the main claims and/or conclusions of the
paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might
suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary
to either make it possible for others to replicate the model with the same dataset, or provide
access to the model. In general. releasing code and data is often one good way to accomplish
this, but reproducibility can also be provided via detailed instructions for how to replicate the
results, access to a hosted model (e.g., in the case of a large language model), releasing of a
model checkpoint, or other means that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
80

(d) We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be possible,
so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
• The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
• Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
81

• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
• The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of
errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
ments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didn’t make it
into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research conducted in the paper conforms with the NeurIPS Code of Etichs.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10. Broader Impacts
82

Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [NA]
Justification: This is a primarily theoretical work.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied to par-
ticular applications, let alone deployments. However, if there is a direct path to any negative
applications, the authors should point it out. For example, it is legitimate to point out that
an improvement in the quality of generative models could be used to generate deepfakes for
disinformation. On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for
monitoring misuse, mechanisms to monitor how a system learns from feedback over time,
improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justification: This is a purely theoretical work, and as such poses no such risks.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the
paper, properly credited and are the license and terms of use explicitly mentioned and properly
respected?
Answer: [NA]
Justification: The paper does not use existing assets.
83

Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to the asset’s
creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
• The paper should discuss whether and how consent was obtained from people whose asset is
used.
• At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsouring nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Including this information in the supplemental material is fine, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-
jects
84

Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
• For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
85

