Random Cuts are Optimal for Explainable k-Medians
Konstantin Makarychev∗
Northwestern University
Liren Shan∗
TTIC
Abstract
We show that the RANDOMCOORDINATECUT algorithm gives the optimal compet-
itive ratio for explainable k-medians in ℓ1. The problem of explainable k-medians
was introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian in 2020. Several
groups of authors independently proposed a simple polynomial-time randomized
algorithm for the problem and showed that this algorithm is O(log k log log k)
competitive. We provide a tight analysis of the algorithm and prove that its compet-
itive ratio is upper bounded by 2 ln k + 2. This bound matches the Ω(log k) lower
bound by Dasgupta et al (2020).
1
Introduction
Machine learning is being increasingly used to make decisions for critical applications, such as
healthcare, finance, and public policy. Considering the profound impact of algorithmic decisions on
individuals and society, it is essential to understand the underlying logic behind these decisions. In
this paper, we explore an explainable k-medians clustering algorithm (called RANDOMCOORDINATE-
CUT). The algorithm’s aim is to cluster data sets and present results in a manner easily understood
and visualized by humans.
Clustering is a fundamental task in unsupervised learning. Among many clustering methods, k-means,
k-medians, and k-medoids are particularly popular. These are centroid-based methods that choose k
centers and assign each data point to the center nearest to it. As a result, each cluster is a Voronoi
cell in the Voronoi partition of the space. Since these cells may have a complicated boundary (see
Figure 1 for an example of k-medians), it is not always easy for humans to comprehend and visualize
such clustering.
To address this problem, Dasgupta, Frost, Moshkovitz, and Rashtchian [2020] introduced explainable
k-means and k-medians clustering. They argued that decision trees are easy to understand and
interpret. Therefore, in order to make clustering more explainable, we need to use threshold decision
trees to define clusters. A threshold decision tree is a binary space partitioning tree with k leaves.
Each internal node of the threshold decision tree splits the data into two groups using a threshold cut
(j, θ): on the one side of the cut, we have points x with xj ≤θ and, on the other side, points x with
xj > θ. Thus, every node of the tree corresponds to a rectangular region of the space. A decision tree
with k leaves partitions data set X into k clusters, P1, . . . , Pk. See Figure 1 for an example. Dasgupta
et al. [2020] suggested that we use the standard k-medians and k-means objectives to measure the
cost of the threshold decision tree. For k-medians, the cost of a threshold decision tree T equals
cost(X, T ) =
k
X
i=1
X
x∈Pi
∥x −ˆci∥1,
where P1, . . . , Pk is the partitioning of X produced by T ; and ˆc1, . . . , ˆck are the medians of clusters
P1, . . . , Pk. We denote the ℓ1-norm by ∥· ∥1. Note that each Pi is a rectangular region of the space.
Thus, generally speaking, every x is not assigned to the closest center ˆc1, . . . , ˆck like in unconstrained
k-medians or k-means.
∗Equal contribution.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

y ≤8.6
1
x ≤−1.9
2
3
Figure 1: The unconstrained k-medians clustering and explainable k-medians clustering. The left
diagram shows the Voronoi partition of the plane w.r.t. three centers in ℓ1 distance. The Voronoi
cell for each center consists of all points that are closer (in ℓ1 distance) to this center than to any
other center (the boundaries between cells are not straight lines because we use the ℓ1 distance). The
middle diagram shows an explainable partition. The right diagram shows the corresponding decision
tree for explainable clustering.
Dasgupta, Frost, Moshkovitz, and Rashtchian [2020] defined the price of explainability as the ratio of
the k-medians cost of explainable clustering to the optimal cost of unconstrained k-medians clustering.
They showed that the cost of explainability for k-means and k-medians (somewhat surprisingly)
does not depend on the number of points in the data set X and only depends on k. Specifically,
they provided a greedy algorithm that given k reference centers c1, c2, · · · , ck of any unconstrained
k-medians as input, outputs a threshold decision tree of cost at most O(k) times the cost of original
unconstrained k-medians with centers c1, c2, · · · , ck. We call such an algorithm O(k) competitive.
To get an explainable k-medians clustering, we first obtain reference centers c1, c2, · · · , ck using
an off-the-shelf approximation algorithm for k-medians and then run an α-competitive algorithm
for explainable k-medians with centers c1, c2, · · · , ck given as input. This algorithm produces the
desired threshold decision tree. Dasgupta et al. [2020] also gave an O(k2) competitive algorithm for
k-means and showed Ω(log k) lower bounds on the price of explainability for both k-medians and
k-means.
The notion of explainable clustering immediately got a lot of attention in the field (Laber and Murtinho
[2021], Makarychev and Shan [2021], Gamlath et al. [2021], Charikar and Hu [2022], Esfandiari et al.
[2022]). Particularly, Makarychev and Shan [2021], Esfandiari, Mirrokni, and Narayanan [2022]
provided almost optimal algorithms for explainable k-medians, and Makarychev and Shan [2021],
Esfandiari, Mirrokni, and Narayanan [2022], Gamlath, Jia, Polak, and Svensson [2021] provided
almost optimal algorithms for k-means. The competitive ratios of these algorithms are ˜O(log k) for
k-medians and ˜O(k) for k-means.
The algorithms for explainable k-medians by Makarychev and Shan [2021], Esfandiari, Mirrokni,
and Narayanan [2022], Gamlath, Jia, Polak, and Svensson [2021] are variants of the same simple
algorithm, which we call RANDOMCOORDINATECUT. This algorithm receives a set of k reference
centers c1, . . . , ck as input and then builds a threshold decision tree with k leaves. It works as follows.
It recursively partitions d-dimensional space until every cell contains exactly one reference center
ci. The algorithm starts with a tree consisting of one node, the root. Initially, all k reference centers
are assigned to that root. At every step, the algorithm picks a random threshold cut (j, θ) and splits
centers in every cell using this cut. If this cut does not separate any centers in a cell u (i.e., all
centers in u are located on one side of the cut), then the algorithm does not split u into two regions at
this step. Finally, for every leaf u of the constructed tree, the unique center that belongs to the cell
corresponding to u is assigned to u. We provide pseudo-code for this algorithm in Figure 2.
Makarychev and Shan [2021], Esfandiari et al. [2022] showed that the competitive ratio of RAN-
DOMCOORDINATECUT is at most O(log k log log k). That is, for every data set X and set of centers
c1, . . . , ck,
E[cost(X, T )] ≤O(log k log log k) ·
X
x∈X
min
c∈{c1,...,ck} ∥x −c∥1.
Note that the running time of this algorithm is ˜O(kd). Gamlath, Jia, Polak, and Svensson [2021]
provided a slightly worse bound of O(log2 k) on the competitive ratio of this algorithm. They also
2

Input: a data set X ⊂Rd and set of centers C = {c1, c2, . . . , ck} ⊂Rd
Output: a threshold tree T
Create tree T0 containing a root node r. Assign Cr = {c1, c2, · · · , ck} to the root. Let t = 0.
Let M = maxij |ci
j|.
while Tn contains a leaf with at least two distinct centers do
Pick a coordinate j and threshold θ ∈(−M, M) uniformly at random. Let ωn = (j, θ).
For every leaf node u in Tn, split the set Cu into two sets:
Left = {c ∈Cu : cj ≤θ}
and
Right = {c ∈Cu : cj > θ}.
If both sets are not empty, then create two children of u in tree Tt. The left child corresponds
to the subregion of u with xj ≤θ, and the right child corresponds to the subregion of u
with xj > θ. Assign sets Left and Right to the left and right child, respectively.
Denote the updated tree by Tt+1.
Update t = t + 1.
end while
Figure 2: RANDOMCOORDINATECUT algorithm
conjectured that this algorithm is optimal and its competitive ratio is O(log k), more specifically,
Hk−1 + 1, where Hk is the k-th harmonic number. They provided some justification for their
conjecture by proving this bound for a very special set of centers and data points (corresponding to
the case of completely disjoint sets in our Set Elimination Game).
Our Results. In this work, we show that indeed the competitive ratio of RANDOMCOORDINATECUT
is at most 2 ln k + 2, and, therefore, this algorithm has the optimal competitive ratio which matches
the lower bound of Dasgupta, Frost, Moshkovitz, and Rashtchian [2020]. Our analysis is not only
tight but also fairly simple. To get our result we define a game, the Set Elimination Game, which
was also implicitly analyzed in previous works on this topic. We show that the cost of this game is at
most 2 ln k + 2.
Related Work. The unconstrained k-medians clustering has been extensively studied. Charikar,
Guha, Tardos, and Shmoys [1999] gave the first constant factor approximation algorithm for the
problem in general metric spaces. Li and Svensson [2013] provided a 1 +
√
3 + ε approximation
algorithm. Byrka, Pensyl, Rybicki, Srinivasan, and Trinh [2017] improved the approximation factor
to 2.675 + ε. Cohen-Addad, Esfandiari, Mirrokni, and Narayanan [2022] recently improved the
approximation factor to 2.406 for Euclidean k-medians. Megiddo and Supowit [1984] showed that
the k-medians in ℓ1 problem is NP-hard. Cohen-Addad and Lee [2022] showed that it is also NP-hard
to approximate k-medians in ℓ1 within a factor of 1.06.
As we discuss above, Gamlath, Jia, Polak, and Svensson [2021], Esfandiari, Mirrokni, and Narayanan
[2022], Makarychev and Shan [2021], independently proposed the RANDOMCOORDINATECUT
algorithm. They also gave an ˜O(k) algorithm for explainable k-means and showed a lower bound of
˜Ω(k) for the problem. Charikar and Hu [2022] provided an O(k1−2/d · poly(d, log k)) competitive
algorithm for explainable k-means, whose competitive ratio depends on the dimension d of the
instance. For small d ≪log k/ log log k, their bound is better than O(k). They showed an almost
matching Ω(k1−2/d/ploy log k) lower bound for explainable k-means. Esfandiari et al. [2022]
gave an upper bound of O(d log2 d) on the competitive ratio of RANDOMCOORDINATECUT for
explainable k-medians. This bound is better than O(log k) for small d ≪log k/ log log k. Laber
and Murtinho [2021] gave O(d log k) and O(dk log k) competitive algorithms for explainable k-
medians and k-means, respectively. Frost, Moshkovitz, and Rashtchian [2020] provided some
empirical evidence that bi-criteria algorithms for explainable k-means (that partition the data set
into (1 + δ)k clusters) can give a much better competitive ratio than O(k). Then, Makarychev
and Shan [2022] gave a ˜O( 1
δ log2 k) competitive bi-criteria algorithm for explainable k-means.
Bandyapadhyay, Fomin, Golovach, Lochet, Purohit, and Simonov [2022] provided an algorithm
3

that computes the optimal explainable k-medians and k-means clustering in time n2d+O(1) and
(4nd)k+O(1), respectively. Laber, Murtinho, and Oliveira [2023] proposed to use shallow decision
trees for explainable clustering.
Independently and concurrently with our work, Gupta, Pittu, Svensson, and Yuan [2023] proved a
O(log k) bound on the price of explainability for k-medians. They showed that the competitive ratio
of RANDOMCOORDINATECUT is 1 + Hk−1, where Hk is the k-th harmonic number. Their work
answers the open question raised by Gamlath, Jia, Polak, and Svensson [2021]. They also proved a
hardness of approximation result for explainable k-medians clustering and improved the competitive
ratio for explainable k-means from O(k log k) to O(k log log k).
2
Set Elimination Game
In this section, we define the set elimination game. Consider a discrete finite measure space (Ω, µ).
In this space, each element ω ∈Ωhas a measure of µ(ω), and the measure of every set S ⊆Ω
equals µ(S) = P
ω∈S µ(ω). Let S1, S2, . . . , Sk ⊂Ωbe k distinct sets which may overlap with each
other. The set elimination game proceeds in a series of rounds. Initially, all sets S1, . . . , Sk enter the
competition. Formally, they belong to the set of remaining sets R0 = {S1, . . . , Sk}. At every round
n, the host picks a random ωn ∈Ωwith probability Pr(ωn = ω) = µ(ω)/µ(Ω). Then, all sets Si
that contain ωn are eliminated from the game unless all remaining sets contain ωn, in which case, no
set gets eliminated. That is, for n ≥1,
Rn =
Rn−1 \ {Si ∈Rn−1 : ωn ∈Si},
if for some Si ∈Rn−1, ωn /∈Si;
Rn−1,
otherwise.
(1)
The last remaining set is declared the winner. We denote that winner by winner. We say that the cost
of the game is the measure of the winning set, µ(winner).
We remark that Rn cannot get empty (in which case, the winner would not be defined) because of the
“otherwise” clause in the definition (1). We shall always assume that all sets S1, . . . , Sk are not only
distinct and non-empty but also (a) for every i, µ(Si) > 0, and (b) for all i and j, µ(Si△Sj) > 0
(here, Si△Sj denotes the symmetric difference of sets Si and Sj). Then, in every game, there is a
unique winner with probability 1.
We similarly define the set elimination game for arbitrary finite measure spaces: For an arbitrary finite
measure space (Ω, µ), element ωn is chosen with probability function Pr(ωn ∈S) = µ(S)/µ(Ω).
Our main result is the following theorem, which, as we discuss later in Section 2.1, implies that the
competitive ratio of the explainable clustering algorithm is 2 ln k + 2.
Theorem 2.1. Consider a set elimination game with the finite measure space (Ω, µ) and k distinct
sets S1, S2, . . . , Sk (as above). The expected cost of the game is at most
E

µ(winner)

≤(2 ln k + 2) · min
i∈[k] µ(Si).
To simplify the exposition, we will prove this theorem for discrete finite measure sets. If Ωis not a
discrete measure space, we first replace it with a quotient space: We say that ω′ ∈Ωand ω′′ ∈Ω
are equivalent (ω′ ∼ω′′) if they are contained in exactly the same set of sets S1, . . . , Sk. This
equivalence relation partitions Ωinto at most 2k different equivalence classes. We replace Ωwith the
quotient space Ω/∼whose elements are equivalence classes. In other words, we merge all equivalent
ω’s. The measure of a new element ˜ω equals to the measure of the corresponding equivalence class.
Organization. In Section 2.1, we discuss the connection between explainable k-medians and set
elimination games. We define a set elimination game in a set system I ⊂{S1, . . . , Sk} in Section 2.2.
Then, we define the hitting and elimination time in Section 2.3. In Section 3, we first illustrate our
proof strategy by showing Theorem 2.1 for the case when the smallest set S1 does not overlap with
S2, . . . , Sk. An important ingredient of our proof is the notion of surprise sets, which we discuss in
Section 3.1. Finally, we complete the proof of Theorem 2.1 in Section 3.2.
2.1
Explainable k-Medians via Set Elimination Game
In this section, we show how to use Theorem 2.1 to obtain a bound of 2 ln k + 2 on the competitive
ratio of the RANDOMCOORDINATECUT algorithm.
4

Theorem 2.2. The competitive ratio of the RANDOMCOORDINATECUT algorithm for Explainable
k-Medians is at most 2 ln k + 2. That is, for every set of centers C = {c1, . . . , ck} and data set X,
the algorithm finds a random decision tree T such that
E[cost(X, T )] ≤(2 ln k + 2) ·
X
x∈X
min
c∈{c1,...,ck} ∥x −c∥1.
The pseudo-code for the RANDOMCOORDINATECUT algorithm is provided in Figure 2.
Theorem 2.2 shows that given any k centers C = {c1, . . . , ck}, RANDOMCOORDINATECUT finds a
decision tree T with cost at most 2 ln k + 2 times the cost of unconstrained k-medians with centers
C = {c1, . . . , ck}. By using k centers given by any constant approximation algorithm for k-medians,
RANDOMCOORDINATECUT finds a decision tree with cost at most O(log k) times the optimal
unconstrained k-medians cost. This implies an O(log k) upper bound on the price of explainability.
Proof of Theorem 2.2. Consider an arbitrary data set X ⊂Rd and set of k centers C ⊂Rd. We
assume that all points in X and all centers in C are in the cube [−M, M]d. The threshold decision
tree obtained by the RANDOMCOORDINATECUT algorithm partitions the space into k cells. Each cell
contains a single reference center ci. The center ci is not necessarily optimal for cluster Pi (cluster Pi
is the intersection of the data set X and i-th cell). However, we will use it as a proxy for the optimal
center. In other words, we will upper bound the cost of the threshold decision tree as follows:
cost(X, T ) ≡
min
ˆc1,...,ˆck
k
X
i=1
X
x∈Pi
∥x −ˆci∥1 ≤
k
X
i=1
X
x∈Pi
∥x −ci∥1.
Let Ωbe the set of all coordinate cuts: Ω= {(j, θ) : j ∈[d], θ ∈[−M, M]}. We define a measure µ
on Ωas follows. For every subset S ⊂Ω, we set
µ(S) =
d
X
j=1
µL({θ : (j, θ) ∈S}),
where µL is the Lebesgue measure on R. Thus, we have µ(Ω) = 2dM, which implies (Ω, µ) is a
finite measure space.
Consider any data point x ∈X. Define k sets S1, S2, . . . , Sk for the set elimination game. For every
i ∈{1, . . . , k}, let Si be the set of all threshold cuts that separate x and center ci, i.e.,
Si = {(j, θ) ∈Ω: sign(xj −θ) ̸= sign(ci
j −θ)}.
Note that the ℓ1 distance from x to center ci equals the measure of Si: ∥x −ci∥1 = µ(Si). We now
examine the set elimination game with sets S1, . . . , Sk, measure space (Ω, µ), and random sequence
of draws ω1, ω2, . . . (each ωn ∈Ωis the threshold cut chosen by the RANDOMCOORDINATECUT
algorithm at step n). We claim that Si belongs to Rn if and only if center ci lies in the same cell as
point x after step n of the algorithm. This is the case for n = 0, since R0 contains all sets S1, . . . , Sk
and the root of the threshold tree contains all centers c1, . . . , ck. Then, whenever we pick cut ωn, all
centers separated from x by ωn are removed from the cell of x. The only exception from this rule
occurs when all centers in that cell lie on the same side of the cut ωn. That is exactly the same rule
as we have for the set elimination game (note that center ci is separated from x by ωn if and only if
ωn ∈Si). Therefore, the same sets Si remain in the game as center ci in the cell of x (namely, sets
Si and centers ci have the same indices).
The RANDOMCOORDINATECUT algorithm stops when all leaves of the decision tree contain exactly
one center. At this step, the set elimination game contains one set, Si. This set corresponds to the
center ci assigned to point x. The cost of the game µ(Si) equals the distance from x to ci. By
Theorem 2.1, we have
E[cost(x, T )] = E[µ(winner)] ≤(2 ln k + 2) · min
i
µ(Si) = (2 ln k + 2) · min
i
∥x −ci∥1.
We sum this bound over all data points x in X and get the desired result.
5

2.2
Local Competitions
We now revisit the definition of the set elimination game and define competitions in subsets of
{S1, . . . , Sk}. For the rest of the proof, we assume (Ω, µ) is a discrete finite measure space. We
remind the reader that every set elimination game is determined by an infinite sequence of i.i.d.
random variables ω1, ω2, . . . . In each round n, we sample an element ωn from Ωwith probability
Pr(ωn = ω) = µ(ω)/µ(Ω).
Definition 2.3. Consider a finite measure space (Ω, µ). Let I be a set of subsets of Ω. We say that I
is a valid set system if (a) for every S ∈I, µ(S) > 0, and (b) for every S′, S′′ ∈I, µ(S′△S′′) > 0.
The reader may assume that µ(ω) > 0 for all ω in Ω. Then, the definition above says that in a valid
set system I, all sets are non-empty and distinct.
Definition 2.4. Consider a finite measure space (Ω, µ). Let ω1, ω2, . . . be i.i.d. random variables
as described above and I be a valid set system. We define a set elimination game in I. Initially,
R0(I) = I. Then, for every n ≥1,
Rn(I) =
Rn−1(I) \ {S ∈Rn−1(I) : ωn ∈S},
if for some S′ ∈Rn−1(I), ωn /∈S′;
Rn−1(I),
otherwise.
(2)
The winner of the game in I, denoted by winner(I), is the only element remaining, or, formally, the
unique element in ∩n≥0Rn(I). If ∩n≥0Rn(I) contains more than one element, then the winner is
not defined. The cost of the game is the measure of the winner, µ(winner(I)).
We remark that ∩n≥0Rn(I) contains exactly one element with probability 1. Thus, the winner and
cost of the game are defined with probability 1.
Consider sets S1, . . . , Sk from Theorem 2.1. Denote K = {S1, . . . , Sk}. The definition of the
competition among sets S1, . . . , Sk (given in the beginning of Section 2) is exactly the same as
the definition of competition in K. Our goal is to show that E[µ(winner(K))] ≤2(ln k + 1) ·
minSi∈K µ(Si). In the proof of Theorem 2.1, we will consider competitions in different set systems
I ⊆K. We show the following key lemma. We defer the proof of Lemma 2.5 to Appendix A.
Lemma 2.5. Consider a partitioning of the set system K = {S1, . . . , Sk} into m sets I1, . . . , Im.
Then, winner(K) ∈

winner(I1), . . . , winner(Im)
	
.
2.3
Set Elimination with Exponential Clock
Consider a set elimination game on sets S1, . . . , Sk. It is determined by the sequence of random
i.i.d. draws ω1, ω2, . . . . Random variable ωn is chosen in round n. We assign every round a random
time τn. Let the time between two consecutive rounds be an exponential random variable with
parameter µ(Ω). Specifically, let ∆τ1, ∆τ2, . . . be a sequence of i.i.d. exponential random variables
with parameter µ(Ω) and each τn = τn−1 + ∆τn = ∆τ1 + · · · + ∆τn. Note that all ∆τn are positive
and τ1, τ2, . . . is an increasing sequence with probability 1. The number of draws that occur by time
t (i.e., Nt(Ω) = |{n : τn ≤t}|) is a Poisson process with parameter µ(Ω). We now can think of the
set elimination game as follows: The host of the game observes a Poisson process with parameter
µ(Ω). Whenever the process jumps (at time τn), the host picks an element ωn in Ωwith probability
Pr(ωn = ω) = µ(ω)/µ(Ω) and eliminates some sets according to the rules of the game discussed
above. Note that by assigning every round some time τn, we do not change the game, the winner, and
the cost of the game (because the sequence of random draws ω1, ω2, . . . remains the same as before).
This interpretation of the game allows us to introduce a hitting time h(S) of every subset S ⊂Ωwith
the following properties: (a) each h(S) is an exponential random variable with rate µ(S); (b) hitting
times of disjoint sets are mutually independent random variables.
Definition 2.6. For every subset X ⊂Ω, the hitting time h(X) is the time τn when the first ωn is
drawn from X: h(X) = min{τn : ωn ∈X}. When the set contains one element ω, we will write
h(ω) instead of h({ω}).
We also define the elimination time of each set Si.
Definition 2.7. Consider any set elimination game with the measure space (Ω, µ) and k sets
S1, S2, . . . , Sk in Ω. The elimination time e(Si) of set Si is the time when set Si is eliminated
from the game, i.e., e(Si) = min{τn : Si /∈Rn(K)}. If Si is the winner, then we let e(Si) = ∞
(because the winner is never eliminated).
6

Note that e(Si) ≥h(Si). Sometimes, e(Si) may be equal to h(Si), but e(Si) and h(Si) are not
always the same. We now prove that hitting times for disjoint sets are independent. To this end, we
split the Poisson process Nt(Ω) = |{n : τn ≤t}|. Let Nt(ω) = |{n : τn ≤t and ωn = ω}|. It
is easy to see that Nt(Ω) = P
ω∈ΩNt(ω) for every t. It is also true that each Nt(ω) is a Poisson
process with parameter µ(ω) and all Nt(ω) (for ω ∈Ω) are mutually independent. This fact follows
from the Coloring Theorem (see e.g., Kingman [1992], Coloring Theorem, page 53).
Theorem 2.8 (Coloring Theorem). Let Πt be a Poisson process on the real line with rate λ. We
color each event of the Poisson process randomly with one of M colors: The probability that a point
receives the i-th color is pi. The colors of different points are independent. Let Πt(i) be the number
of events of color i in the interval (0, t]. Then, Πt(1), . . . , Πt(M) are independent Poisson processes.
The rate of process Πt(i) is λpi.
Lemma 2.9. For every ω ∈Ω, h(ω) is an exponential random variable with parameter µ(ω), and
all random variables h(ω) (for ω ∈Ω) are mutually independent.
Proof. Observe that h(ω) = min{t : Nt(ω) ≥1}. Thus, h(ω) is an exponential random variable
(the time of the first jump of a Poisson process) with rate µ(ω). Also, since all Nt(ω) (for ω ∈Ω) are
mutually independent, all h(ω) are also mutually independent.
Note that the set elimination game depends only on the hitting times for elements ω in Ω. This is
the case because it matters only when every ω is drawn the first time. At that time – the hitting
time of ω – all sets that contain ω are eliminated unless all remaining sets contain this ω. When the
same ω is drawn again, it does not eliminate any new sets. Also, note that for any set S ⊂Ω, the
hitting time h(S) = minω∈S h(ω). Thus, h(S) is an exponential random variable with parameter
µ(S) = P
ω∈S µ(ω).
3
Proof of Main Result
We now present the proof of our main result, Theorem 2.1. We assume without loss of generality that
S1 is the smallest set i.e., µ(S1) ≤µ(Si) for all i. Then, the expected cost of the game is at most:
µ(S1) +
k
X
i=2
Pr
 Si = winner(K)

µ(Si).
(3)
We first provide some intuition for the proof by considering the case when S1 does not intersect
with sets S2, . . . , Sk, i.e. sets S1 and Si are disjoint for all i = 2, 3, . . . , k. We split all sets into two
groups S1 and the rest of the sets S2, . . . , Sk. We know from Lemma 2.5 that the winner among
all sets S1, . . . , Sk is either S1 or winner
 {S2, . . . , Sk}

. Denote I−= {S2, . . . , Sk}. Each set Si
is eliminated at time e(Si). The set S1 is eliminated at its hitting time h(S1) unless it is the only
remaining set at time h(S1) (because we are considering the case when S1 does not overlap with
other sets). Thus,
winner(K) =
S1,
if h(S1) > e(winner(I−));
winner(I−),
if e(winner(I−)) > h(S1).
(4)
When the winner among S1, . . . , Sk is not S1, we consider two cases of the winner Si: (1) Si is a
surprise set; (2) Si is a non-surprise set.
Definition 3.1. We say that Si is a surprise set if e(Si) ≥h(S1) ≥L/µ(Si), where L = ln k.
We call Si a surprise set because the probability of the event e(Si) ≥h(S1) ≥L/µ(Si) is small. We
give a bound on the probability of e(Si) ≥h(S1) ≥L/µ(Si) in Lemma 3.3. Here, we provide some
intuition. By Lemma 2.9, the hitting time h(Si) is an exponential random variable with parameter
µ(Si). Thus, the expected hitting time for Si is 1/µ(Si). Consider a set Si with a small measure
(µ(Si) is close to µ(S1)). If the hitting time h(S1) ≥L/µ(Si), then h(S1) is much larger than its
expected value 1/µ(S1), which happens with a small probability. Consider a set Si with a large
measure µ(Si) ≫µ(S1). Then, the expected hitting time for Si is 1/µ(Si), which is much smaller
than the expected hitting time of S1. Thus, the event e(Si) ≥h(S1) occurs with a small probability.
7

Let us examine bound (3). Let Surprise be the set of all surprise sets. Note that Surprise is a
random set. Then,
k
X
i=2
Pr
 Si = winner(K)

µ(Si) ≤
k
X
i=2
Pr
 Si = winner(K), Si /∈Surprise

· µ(Si)
(5)
+
k
X
i=2
Pr
 Si ∈Surprise

· µ(Si).
We show in the next section (Lemma 3.3) that the second sum is upper bounded by µ(S1). We
now bound the first sum. For every winner Si which is not a surprise set, we have e(Si) ≥h(S1)
(because Si is the winner) and h(S1) ≤L/µ(Si) (because Si is not a surprise set). We also have
Si = winner(I−), thus
Pr
 Si = winner(K), Si /∈Surprise

≤Pr
 h(S1) ≤L/µ(Si) and Si = winner(I−)

.
By Lemma 2.9, all hitting times h(Si) = minω∈Si h(ω) for i ≥2 are independent from h(S1). Thus,
winner(I−) is also independent of h(S1) (winner(I−) depends only on the hitting times for sets
Si ∈I−). Therefore,
Pr
 Si = winner(K), Si /∈Surprise

≤Pr
 h(S1) ≤L/µ(Si)

· Pr
 Si = winner(I−)

=

1 −e−Lµ(S1)/µ(Si)
|
{z
}
≤Lµ(S1)/µ(Si)
· Pr
 Si = winner(I−)

≤Pr
 Si = winner(I−)

· L · µ(S1)/µ(Si).
We combine all bounds on terms of (5) and get the following bound on the expected cost of the game:
µ(S1) +
k
X
i=2
Pr
 Si = winner(I−)

· L · µ(S1) + µ(S1) = (L + 2) · µ(S1) = (ln k + 2) · µ(S1).
This concludes the proof of the theorem for the case when S1 does not overlap with S2, . . . , Sk. We
now analyze surprise sets.
3.1
Surprise Sets
In this section, we prove a bound on the probability that a set Si is a surprise set. We no longer
assume that S1 does not intersect with other sets Si. We first show a lemma about exponential random
variables.
Lemma 3.2. Let X and Y be two independent exponential random variables with positive parameters
λX and λY , respectively. Then, for every T ≥0, we have
Pr
 Y ≥X ≥T

=
λX
λX + λY
· e−(λX+λY )T .
(6)
Proof. The desired probability can be easily found by computing
R ∞
T (FX(t) −FX(T))fY (t)dt,
where FX(t) = 1 −e−λXt is the cumulative distribution function of X, and fY (t) = λY · e−λY t is
the probability density function of Y . Here, we give an alternative proof. Write,
Pr
 Y ≥X ≥T

= Pr
 Y ≥X & min(X, Y ) ≥T

= Pr
 X ≤Y | min(X, Y ) ≥T) · Pr
 min(X, Y ) ≥T

.
We have Pr
 min(X, Y ) ≥T

= e−(λX+λY )T , because the minimum of two independent exponen-
tial random variables with parameters λX and λY is an exponential random variable with parameter
λX +λY . Then, Pr
 X ≤Y | min(X, Y ) ≥T) = Pr
 X ≤Y ) because the exponential distribution
is memoryless; and Pr
 X ≤Y ) = λX/(λX + λY ).
Lemma 3.3. For every set Si, we have
Pr(Si is surprise set) ≤1
k · µ(S1)
µ(Si) .
8

Proof. First, we show that min(e(Si), h(S1)) ≤h(Si \ S1).
Claim 3.4. We always have min(e(Si), h(S1)) ≤h(Si \ S1).
Proof. Consider an arbitrary realization of the game and the time t = h(Si \ S1) when Si \ S1 is
hit. If by this time, S1 has already been hit then h(S1) < t. Similarly, if by this time, Si has already
been eliminated then e(Si) < t. Otherwise, both S1 and Si are still remaining in the game at time t.
Therefore, when we pick ω ∈Si \ S1 at time t, set Si gets eliminated (since ω ∈Si; ω /∈S1; both
S1 and Si are remaining in the game). Thus, in this case, e(Si) = t. This concludes the proof.
If Si is a surprise set, then min(e(Si), h(S1)) = h(S1) ≥L/µ(Si). By Claim 3.4, we have
h(Si \ S1) ≥min
 e(Si), h(S1)

= h(S1) ≥L/µ(Si).
Thus, Pr(Si is surprise set) ≤Pr

h(Si \ S1) ≥h(S1) ≥L/µ(Si)

. By Lemma 3.2 applied to the
independent exponential random variables h(S1), h(Si \ S1), and time T = L/µ(Si), we have
Pr(Si is surprise set) ≤
µ(S1)
µ(Si \ S1) + µ(S1) · e−L(µ(Si\S1)+µ(S1))
µ(Si)
≤1
k · µ(S1)
µ(Si) .
3.2
General Case
Proof of Theorem 2.1. We upper bound the expected cost of the game for arbitrary sets S1, . . . , Sk.
As before, we assume that S1 is the smallest set. We remind the reader that each hitting time h(Si) is
an exponential random variable with parameter µ(Si). In the proof, we will use the definitions of
surprise sets (see Definitions 3.1). We also set L = ln k. We define all sets Si for i ̸= 1 that are not a
surprise set to be non-surprise sets.
We separately upper bound the cost of the winner depending on whether the winner is (a) set S1, (b)
surprise set, (c) non-surprise set. Write
E

µ(winner(K))

= E

µ(winner(K)) · 1{winner(K) = S1}

(a)
+ E

µ(winner(K)) · 1{winner is surprise set}

(b)
+ E

µ(winner(K)) · 1{winner is non-surprise set}

.
(c)
Term (a) is upper bounded by µ(S1). We bound term (b) using Lemma 3.3: The probability that a set
is a surprise set is at most 1/k · µ(S1)/µ(Si). Thus, the expected total measure of all sets (not only
the surprise winner) is upper bounded by 1
k
Pk
i=2
µ(S1)
µ(Si) µ(Si) < µ(S1).
We now bound term (c). Define a new random variable: Let cost(ω) be the cost of the winner (i.e.,
µ(Si), where Si is the winner) if (1) the winner is a non-surprise set, and (2) ω is the first element
that was chosen in S1. We let cost(ω) = 0, otherwise. If ω is the first element that was chosen in S1,
then h(S1) = h(ω). So, the definition of cost(ω) can be written as follows:
cost(ω) = µ(winner(K)) · 1{h(S1) = h(ω)} · 1{winner(K) ̸∈Surprise}.
Since the hitting time h(S1) is finite with probability 1, the term (c) equals
(c) =
X
ω∈S1
E[cost(ω)].
Lemma 3.5, which we prove below, gives a bound of 2Lµ(S1) on the expression above. Combining
upper bounds on terms (a), (b), and (c), we get
E

µ(winner(K))

≤(1 + 2L + 1)µ(S1) = (2 ln k + 2) · µ(S1).
Lemma 3.5. For every ω ∈S1, we have E[cost(ω)] ≤2Lµ(ω).
9

Proof. We have
E[cost(ω)] = E
h
µ(winner(K)) · 1{h(S1) = h(ω)} · 1{winner(K) ̸∈Surprise}
i
.
(7)
If Si is a non-surprise set, then h(S1) < L/µ(Si) or e(Si) < h(S1). If Si is the winner, then
e(Si) ≥h(S1). Thus, if Si is a non-surprise winner, then h(S1) < L/µ(Si). This observations gives
us the following upper bound on (7):
E

cost(ω)

≤
k
X
i=2
µ(Si) · Pr

Si = winner(K) and h(ω) = h(S1) < L/µ(Si)

.
(8)
Define two set systems I−
ω and I+
ω of sets Si containing and not containing ω:
I−
ω = {Si : ω /∈Si and i ≥2};
I+
ω = {Si : ω ∈Si and i ≥2}.
Note that K ≡{S1, . . . , Sk} = {S1} ∪I−
ω ∪I+
ω . By Lemma 2.5,
winner(K) ∈

S1, winner(I−
ω ), winner(I+
ω )
	
.
Observe that if Si with i ≥2 is the winner, then Si = winner(I−
ω ) or Si = winner(I+
ω ). We replace
the condition Si = winner(K) with Si ∈{winner(I−
ω ), winner(I+
ω )} in (8) and get bound:
E

cost(ω)

≤
k
X
i=2
µ(Si) · Pr

Si ∈{winner(I−
ω ), winner(I+
ω )} and h(ω) <
L
µ(Si)

.
The key observation now is that sets winner(I−
ω ) and winner(I+
ω ) are independent of h(ω). This is
the case, because sets remaining in the competitions Rn(I−
ω ) and Rn(I+
ω ) do not change when we
select ω. The set Rn(I−
ω ) does not change in the round n when ω is chosen because all sets Si in
Rn(I−
ω ) ⊂I−
ω do not contain ω. The set Rn(I+
ω ) does not change in this round because all sets Si
in Rn(I+
ω ) ⊂I+
ω contain ω and consequently when ω is chosen, none of these sets is removed from
Rn(I+
ω ) (otherwise, Rn(I+
ω ) would become empty). Thus,
E

cost(ω)

≤
k
X
i=2
µ(Si) · Pr
 Si ∈{winner(I−
ω ), winner(I+
ω )}

· Pr

h(ω) <
L
µ(Si)

.
Using that h(ω) is an exponential random variable with parameter µ(ω), we get (for every i)
µ(Si) · Pr

h(ω) ≤
L
µ(Si)

= µ(Si) ·

1 −e−L µ(ω)
µ(Si)

≤µ(Si) · L µ(ω)
µ(Si) = µ(ω)L.
Hence,
E

cost(ω)

≤µ(ω)L ·
k
X
i=2
Pr
 Si ∈{winner(I−
ω ), winner(I+
ω )}

.
The sum on the right hand side is at most 2. Thus, E[cost(ω)] ≤2Lµ(ω).
Acknowledgments and Disclosure of Funding
The authors are supported by NSF Awards CCF-1955351, CCF-1934931, EECS-29 2216970.
References
Sayan Bandyapadhyay, Fedor Fomin, Petr A Golovach, William Lochet, Nidhi Purohit, and Kirill
Simonov. How to find a good explanation for clustering? In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 36, pages 3904–3912, 2022.
Jarosław Byrka, Thomas Pensyl, Bartosz Rybicki, Aravind Srinivasan, and Khoa Trinh. An improved
approximation for k-median and positive correlation in budgeted optimization. ACM Transactions
on Algorithms (TALG), 13(2):1–31, 2017.
10

Moses Charikar and Lunjia Hu. Near-optimal explainable k-means for all dimensions. In Proceedings
of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 2580–2606.
SIAM, 2022.
Moses Charikar, Sudipto Guha, Éva Tardos, and David B Shmoys. A constant-factor approximation
algorithm for the k-median problem. In Proceedings of the thirty-first annual ACM symposium on
Theory of computing, pages 1–10, 1999.
Vincent Cohen-Addad and Euiwoong Lee. Johnson coverage hypothesis: Inapproximability of
k-means and k-median in lp-metrics. In Proceedings of the 2022 Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pages 1493–1530. SIAM, 2022.
Vincent Cohen-Addad, Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Improved
approximations for euclidean k-means and k-median, via nested quasi-independent sets. In
Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1621–
1628, 2022.
Sanjoy Dasgupta, Nave Frost, Michal Moshkovitz, and Cyrus Rashtchian. Explainable k-means and
k-medians clustering. In Proceedings of the 37th International Conference on Machine Learning,
pages 7055–7065, 2020.
Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Almost tight approximation algorithms
for explainable clustering. In Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA), pages 2641–2663. SIAM, 2022.
Nave Frost, Michal Moshkovitz, and Cyrus Rashtchian. Exkmc: Expanding explainable k-means
clustering. arXiv preprint arXiv:2006.02399, 2020.
Buddhima Gamlath, Xinrui Jia, Adam Polak, and Ola Svensson. Nearly-tight and oblivious algorithms
for explainable clustering. Advances in Neural Information Processing Systems, 34:28929–28939,
2021.
Anupam Gupta, Madhusudhan Reddy Pittu, Ola Svensson, and Rachel Yuan. The price of explain-
ability for clustering. arXiv preprint arXiv:2304.09743, 2023.
John Frank Charles Kingman. Poisson processes, volume 3. Clarendon Press, 1992.
Eduardo Laber, Lucas Murtinho, and Felipe Oliveira. Shallow decision trees for explainable k-means
clustering. Pattern Recognition, 137:109239, 2023.
Eduardo S Laber and Lucas Murtinho. On the price of explainability for some clustering problems.
In International Conference on Machine Learning, pages 5915–5925. PMLR, 2021.
Shi Li and Ola Svensson. Approximating k-median via pseudo-approximation. In proceedings of the
forty-fifth annual ACM symposium on theory of computing, pages 901–910, 2013.
Konstantin Makarychev and Liren Shan. Near-optimal algorithms for explainable k-medians and
k-means. In International Conference on Machine Learning, pages 7358–7367. PMLR, 2021.
Konstantin Makarychev and Liren Shan. Explainable k-means: don’t be greedy, plant bigger trees!
In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages
1629–1642, 2022.
Nimrod Megiddo and Kenneth J Supowit. On the complexity of some common geometric location
problems. SIAM journal on computing, 13(1):182–196, 1984.
11

A
Proof of Lemma 2.5
Lemma 2.5. Consider a partitioning of the set system K = {S1, . . . , Sk} into m sets I1, . . . , Im.
Then,
winner(K) ∈

winner(I1), . . . , winner(Im)
	
.
The proof of Lemma 2.5 relies on the following observarion.
Lemma A.1. Let X and Y be two subsets of K. If X ⊂Y , then for every n, we always have
Rn(Y ) ∩X = Rn(X)
or
Rn(Y ) ∩X = ∅.
(9)
Proof. We prove that (9) holds by induction on n. Initially, when n = 0, we have R0(X) = X and
R0(Y ) = Y . Therefore, R0(Y ) ∩X = X ∩Y = X = R0(X). Suppose (9) holds for n, we prove
that (9) also holds for n′ = n + 1. If Rn(Y ) ∩X = ∅, then Rn(Y ) ∩X remains empty for all
n′ ≥n. Therefore, (9) holds for n + 1. So, let us assume that Rn(Y ) ∩X = Rn(X). Consider
three cases:
• If ωn+1 belongs to all sets in Rn(Y ), then it also belongs to all sets in Rn(X) = Rn(Y )∩X.
Thus, in this case, no set is eliminated in X or Y . That is, Rn+1(X) = Rn(X) and
Rn+1(Y ) = Rn(Y ).
• If ωn+1 belongs to all sets in Rn(X), but not all sets in Rn(Y ), then, at step n + 1,
we remove all sets that contain ωn+1 and, particularly, all sets in Rn(X), from Rn(Y ).
Consequently, Rn+1(Y ) ∩X = ∅.
• If not all sets in Rn(X) and not all sets in Rn(Y ) contain ωn+1, then we remove exactly
the same sets from both Rn(X) and Rn(Y ) ∩X. Namely, we remove sets Si ∈Rn(Y )
that contain ωn+1.
We conclude that (9) holds for n′ = n + 1.
Proof of Lemma 2.5. Consider an arbitrary realization of the game ω1, ω2, . . . . Let n be the round
when all sets but the winner are eliminated from the competition i.e., Rn contains only one set, the
winner. Since K is the union of I1, . . . , Ik, the winner must belong to some Ij. Now, by Lemma A.1
for X = Ij and Y = K, we have Rn(K) ∩Ij = Rn(Ij) or Rn(K) ∩Ij = ∅. We know that
Rn(K) = {winner(K)} and winner(K) ∈Ij. Thus, Rn(K) ∩Ij = {winner(K)} ̸= ∅, and
Rn(Ij) = Rn(K) ∩Ij = {winner(K)}.
We conclude that at round n, Rn(Ij) contains only one set – the winner in K. Consequently, it is
also the winner in Ij i.e., winner(Ij) = winner(K). This finishes the proof.
12

