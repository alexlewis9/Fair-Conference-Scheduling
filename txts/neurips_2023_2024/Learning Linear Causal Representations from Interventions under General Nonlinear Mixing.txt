Learning Linear Causal Representations from
Interventions under General Nonlinear Mixing
Simon Buchholz1∗
Goutham Rajendran2∗
Elan Rosenfeld2
Bryon Aragam3
Bernhard Schölkopf1
Pradeep Ravikumar2
1Max Planck Institute for Intelligent Systems, Tübingen, Germany
2Carnegie Mellon University, Pittsburgh, USA
3University of Chicago, Chicago, USA
Abstract
We study the problem of learning causal representations from unknown, latent
interventions in a general setting, where the latent distribution is Gaussian but the
mixing function is completely general. We prove strong identifiability results given
unknown single-node interventions, i.e., without having access to the intervention
targets. This generalizes prior works which have focused on weaker classes, such
as linear maps or paired counterfactual data. This is also the first instance of
identifiability from non-paired interventions for deep neural network embeddings
and general causal structures. Our proof relies on carefully uncovering the high-
dimensional geometric structure present in the data distribution after a non-linear
density transformation, which we capture by analyzing quadratic forms of precision
matrices of the latent distributions. Finally, we propose a contrastive algorithm
to identify the latent variables in practice and evaluate its performance on various
tasks.
1
Introduction
Modern generative models such as GPT-4 [59] or DDPMs [26] achieve tremendous performance for a
wide variety of tasks [7]. They do this by effectively learning high-level representations which map to
raw data through complicated non-linear maps, such as transformers [89] or diffusion processes [79].
However, we are unable to reason about the specific representations they learn, in particular they are
not necessarily related to the true underlying data generating process. Besides their susceptibility to
bias [20], they often fail to generalize to out-of-distribution settings [5], rendering them problematic
in safety-critical domains. In order to gain a deeper understanding of what representations deep
generative models learn, one line of work has pursued the goal of causal representation learning
(CRL) [75]. CRL aims to learn high-level representations of data while simultaneously recovering
the rich causal structure embedded in them, allowing us to reason about them and use them for
higher-level cognitive tasks such as systematic generalization and planning. This has been used
effectively in many application domains including genomics [52] and robotics [43, 94].
A crucial primitive in representation learning is the fundamental notion of identifiability [37, 70], i.e.,
the question whether a unique (up to tolerable ambiguities) model can explain the observed data. It is
well known that because of non-identifiability, CRL is impossible in general settings in the absence of
inductive biases or additional information [29, 50]. In this work, we consider additional information
∗Equal Contribution
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

in the form of interventional data [75]. It is common to have access to such data in many application
domains such as genomics and robotics stated above (e.g. [17, 58, 57]). Moreover, there is a pressing
need in such safety-critical domains to build reliable and trustworthy systems, making identifiable
CRL particularly important. Therefore, it’s important to study whether we can and also how to
perform identifiable CRL from raw observational and interventional data. Here, identifiability opens
the possibility to provably recover the true representations with formal guarantees. Meaningfully
learning such representations with causal structure enables better interpretability, allows us to reason
about fairness, and helps with performing high-level tasks such as planning and reasoning.
In this work, we study precisely this problem of causal representation learning in the presence
of interventions. While prior work has studied simpler settings of linear or polynomial mixing
[84, 88, 2, 71, 11], we allow for general non-linear mixing, which means our identifiability results
apply to complex real-world systems and datasets which are used in practice. With our results, we
make progress on fundamental questions on interventional learning raised by [75].
Concretely, we consider a general model with latent variables Z and observed data X generated as
X = f(Z) where f : Rd →Rd′ is an arbitrary non-linear mixing function. We assume Z satisfies
a Gaussian structural equation model (SEM) which is unknown and unobserved. A Gaussian prior
is commonly used in practice (which implies a linear SEM over Z) and further, having a simple
model for Z allows us to learn useful representations, generate data efficiently, and explore the latent
causal relationships, while the non-linearity of f ensures universal approximability of the model. We
additionally assume access to interventional data X(i) = f(Z(i)) for i ∈I where Z(i) is the latent
under an intervention on a single node Zti. Notably, we allow for various kinds of interventions,
we don’t require knowledge of the targets ti, and we don’t require paired data, (i.e., we don’t need
counterfactual samples from the joint distribution (X, X(i)), but only their marginals). Having targets
or counterfactual data is unrealistic in many practical settings but many prior identifiability results
require them, so eliminating this dependence is a crucial step towards CRL in the real world.
Contributions
Our main contribution is a general solution to the identifiability problem that
captures practical settings with non-linear mixing functions (e.g. deep neural networks) and unknown
interventions (since the targets are latent). We study both perfect and imperfect interventions and
additionally allow for shift interventions, where perfect interventions remove the dependence of
the target variable from its parents, while imperfect interventions (also known as soft interventions)
modify the dependencies. Below, we summarize our main contributions:
1. We show identifiability of the full latent variable model, given non-paired interventional
data with unknown intervention targets. In particular, we learn the mixing, the targets and
the latent causal structure.
2. Compared to prior works that have focused on linear/polynomial f, we allow non-linear
f which encompasses representations learned by e.g. deep neural networks and captures
complex real-world datasets. Moreover, we study both imperfect (also called soft) and
perfect interventions, and always allow shift interventions.
3. We construct illustrative counterexamples to probe tightness of our assumptions which
suggest directions for future work.
4. We propose a novel algorithm based on contrastive learning to learn causal representations
from interventional data, and we run experiments to validate our theory. Our experiments
suggest that a contrastive approach, which so far has been unexplored in interventional CRL,
is a promising technique going forward.
2
Related work
Causal representation learning [75, 74] has seen much recent progress and applications since it
generalizes and connects the fields of Independent Component Analysis [13, 28, 30], causal inference
[81, 61, 63, 64, 82] and latent variable modeling [40, 3, 78, 96, 32]. Fundamental to this approach
is the notion of identifiability [36, 15, 93]. Due to non-identifiability in general settings without
inductive biases [29, 50], prior works have approached this problem from various angles — using
additional auxiliary labels [36, 27, 6, 76]; by imposing sparsity [75, 42, 54, 103]; or by restricting
the function classes [41, 9, 104, 22]. See also the survey [32]. Moreover, a long line of works have
proposed practical methods for CRL (which includes causal disentanglement as a special case),
2

(a) No interventions
(b) An imperfect intervention
(c) A perfect intervention
Figure 1: Illustration of an example latent variable model under interventions. (a) No interventions.
(b) An imperfect intervention on node ti = 3. Dashed edges indicate the weights could have
potentially been modified. (c) A perfect intervention on node ti = 3.
[19, 95, 16, 97, 44, 14] to name a few. It’s worth noting that most of these approaches are essentially
variants of the Variational Autoencoder framework [39, 68].
Of particular relevance to this work is the setting when interventional data is available. We first
remark that the much simpler case of fully observed variables (i.e., no latent variables), has been
studied in e.g. [23, 62, 83, 34, 18] (see also the survey [82]). In this work, we consider the more
difficult setting of structure learning over latent variables, which have been explored in [46, 42, 6,
104, 1, 84, 88, 2, 71, 11, 72]. Among these, [46] assumes that the intervention targets are known, [42]
specifically consider instance-level pre- and post- interventions for time-series data and [6, 104, 1]
assumes access to paired counterfactual data. In contrast, we assume unknown targets and work
in general settings with non-paired interventional data, which is important in various real-world
applications [85, 99]. The work [48] require several graphical restrictions on the causal graph and
also require 2d interventions, while we make no graphical restrictions and only require d interventions
(which we also show cannot be improved under our assumptions). [71, 11, 84, 88] consider linear
mixing functions f whereas we study general non-linear f. Finally, [2] consider polynomial mixing
in the more restricted class of deterministic do-interventions. Several concurrent works study related
settings [100, 35, 65, 45, 92]. For a more detailed comparison to the related works, we refer to
Appendix C.
Our proposed algorithm is based on contrastive learning. Contrastive learning has been used in
other contexts in this domain [31, 27, 55, 104, 91] (either in the setting of time-series data or paired
counterfactual data), however the application to non-paired interventional settings is new to the best
of our knowledge.
Notation
In this work, we will almost always work with vectors and matrices in d dimensions where
d is the latent dimension, and we will disambiguate when necessary. For a vector v, we denote by
vi its i-th entry. Let Id denote the d × d identity matrix with columns as the standard basis vectors
e1, . . . , ed. We denote by N(µ, Σ) the multivariate normal distribution with mean µ and covariance
Σ. For a set C, let U(C) denote the uniform distribution on C. For two random variables X, X′, we
write X
D= X′ if their distributions are the same. We denote the set {1, . . . , d} by [d]. For permutation
matrices we use the convention that (Pω)ij = 1j=ω(i) for ω ∈Sd. We use standard directed graph
terminology, e.g. edges, parents. When we use the term “non-linear mixing” in this work, we also
include linear mixing as a special case.
3
Setting: Interventional Causal Representation Learning
We now formally introduce our main settings of interest and the required assumptions. We assume
that there is a latent variable distribution Z on Rd and an observational distribution X on Rd′ given by
X = f(Z) where d ≤d′ and f is a non-linear mixing. This encapsulates the most general definition
of a latent variable model. As the terminology suggests, we observe X in real-life, e.g. images of
cats and Z encodes high-level latent information we wish to learn and model, e.g. Z could indicate
orientation and size.
Assumption 1 (Nonlinear f). The non-linear mixing f is injective, differentiable and embeds into
Rd′.
3

Such an assumption is standard in the literature. Injectivity is needed in order to identify Z from X
because otherwise we may have learning ambiguity if multiple Zs map to the same X. Differentiabil-
ity is needed to transfer densities. Note that Assumption 1 allows for a large class of complicated
nonlinearities and we discuss in Appendix E why proving results for such a large class of mixing
functions is important.
Next, we assume that the latent variables Z encode causal structure which can be expressed through a
Structural Causal Model (SCM) on a Directed Acyclic Graph (DAG). We focus on linear SCMs with
an underlying causal graph G on vertex set [d], encoded by a matrix A through its non-zero entries,
i.e., there is an edge i →j in G iff Aji ̸= 0.
Assumption 2 (Linear Gaussian SCM on Latent Variables). The latent variables Z follow a linear
SCM with Gaussian noise, so
Z = AZ + D1/2ϵ
(1)
where D is a diagonal matrix with positive entries, A encodes a DAG G and ϵ ∼N(0, Id).
For an illustration of the model, see Fig. 1a. It is convenient to encode the coefficients of linear SCMs
through the matrix B = D−1/2(Id −A). Then Z = B−1ϵ and both D and A can be recovered
from B, indeed the diagonal entries of B agree with the entries of D−1/2. Note that we can and will
always assume that the entries of D are positive since ϵ
D= −ϵ.
Assuming that the latent variables follow a linear Gaussian SCM is certainly restrictive but never-
theless a reasonable approximation of the underlying data generating process in many settings. The
Gaussian prior assumption is also standard in latent variable modeling and enables efficient inference
for downstream tasks, among other advantages. Importantly, under the above assumptions, our model
has infinite modeling capacity [53, 86, 33], so they’re able to model complex datasets such as images
and there’s no loss in representational power.
The goal of representation learning is to learn the non-linear mixing f and the high-level latent
distribution Z from observational data X. In causal representation learning, we wish to go a step
further and model Z as well by learning the parameters B which then lets us easily recover A, D
and the causal graph G. For general non-linear mixing, this model is not identifiable and hence we
cannot hope to recover Z, but in this work we use additional interventional data, similar to the setups
in recent works [2, 84, 88].
Assumption 3 (Single node interventions). We consider interventional distributions Z(i) for i ∈I
which are single node interventions of the latent distribution Z, i.e., for each intervention i there is a
target node ti and we change the assigning equation of Z(i)
ti to
Z(i)
ti = (A(i)Z(i))ti + (D(i))1/2
ti,ti(ϵti + η(i))
(2)
while leaving all other equations unchanged. We assume that A(i)
ti,k ̸= 0 only if k →ti in G, i.e., no
parents are added and η(i) denotes a shift of the mean.
For an illustration, see Fig. 1b. An intervention on node ti has no effect on the non-descendants of ti,
but will have a downstream effect on the descendants of ti. In particular, A is modified so that the
weight of the edge k →ti could potentially be changed if it exists already but no new incoming edges
to ti may be added. Also, the noise variable ϵti is also allowed to be modified via a scale intervention
as well as a shift intervention. Note that [84, 2] do not allow shift interventions, whereas we do.
Again, this can be written concisely as Z(i) = (B(i))−1(ϵ + η(i)eti) where B(i) = (D(i))−1/2(Id −
A(i)). Let r(i) denote the row ti of B(i), then we can write B(i) = B −eti(B⊤eti −r(i))⊤. We
assume that interventions are non-trivial, i.e., Z(i) D
̸= Z. In our model, we observe interventional
distributions X(i) = f(Z(i)) for various interventions i ∈I.
Definition 1 (Intervention Types). We call an intervention perfect (or stochastic hard) if A(i)
ti· = 0,
i.e., we remove all connections to former parents and potentially change variance and mean of the
noise variable. Note that for nodes without parents, either D(i)
titi ̸= Dtiti or η(i) ̸= 0 so that the
intervention is non-trivial. We call an intervention a pure noise intervention if A(i)
ti· = Ati· and
Ati· ̸= 0 (i.e., node ti has at least one parent). In other words, a pure noise intervention targets a
4

node with parents by changing only the noise distribution through a change of variance (encoded in
D) or a mean shift (encoded in η).
We call a single-node intervention imperfect if it is not perfect (some prior works call it a soft
intervention). Note that perfect interventions are never of pure noise type because they necessarily
change the relation to the parents. For an illustration of perfect and imperfect interventions, see
Fig. 1c, Fig. 1b respectively. Finally, we require an exhaustive set of interventions.
Assumption 4 ((Coverage of Interventions)). All nodes are intervened upon by at least one interven-
tion, i.e., {ti : i ∈I} = [d]
This assumption was also made in the prior works [84, 2, 88]. When the interventions don’t cover all
the nodes, we have non-identifiability as described in Section 4 and Appendix D. We also extensively
discuss that none of our other assumptions can simply be dropped in these sections.
Remark 1.
• Our theory also readily extends to noisy observations, i.e., X = f(Z) + ν
where ν is independent noise. In this case, we first denoise via a standard deconvolution
argument [36, 42] and then apply our theory.
• Similar to the results in [84] we can assume completely unknown intervention targets, i.e.,
there might be multiple interventions targeting the same node, and we do not need to know
the partition. We only require coverage of all nodes. In contrast to their work we assume
that we know which dataset corresponds to the observational distribution, but we expect
that this restriction can be removed.
To simplify the notation, it is convenient to use B(0) = B, η(0) = 0 for the observational distribution.
We also define ¯I = I ∪{0}. Then, all information about the latent variable distributions Z(i) and
observed distributions X(i) are contained in ((B(i), η(i), ti)i∈¯I, f).
4
Main Results
We can now state our main results for the setting introduced above.
Theorem 1. Suppose we are given distributions X(i) generated using a model ((B(i), η(i), ti)i∈¯I, f)
such that Assumptions 1-4 hold and such that all interventions i are perfect. Then the model is
identifiable up to permutation and scaling, i.e., for any model (( eB(i), eη(i), eti)i∈¯I, ef) that generates
the same data, the latent dimension d agrees and there is a permutation ω ∈Sd (and associated
permutation matrix Pω) and an invertible pointwise scaling matrix Λ ∈Diag(d) such that
eti = ω(ti),
eB(i) = P ⊤
ω B(i)Λ−1Pω,
ef = f ◦Λ−1Pω,
eη(i) = η(i).
(3)
This in particular implies that
eZ(i) D= P ⊤
ω ΛZ(i)
(4)
and we can identify the causal graph G up to permutation of the labels.
This result says that for the interventional model as described in the previous section, we can identify
the non-linear map f, the intervention targets ti, the parameter matrices B, D, A up to permutations
Pω and diagonal scaling Λ. Moreover, we can recover the shifts η(i) exactly and also the underlying
causal graph G up to permutations.
Remark 2. Identifiability and recovery up to permutation and scaling is the best possible for our
setting. This is because the latent variables Z are not actually observed, which means we cannot
(and in fact don’t need to) resolve permutation and scaling ambiguity without further information
about Z. See [84, Proposition 1] for the short proof.
When we drop the assumption that the interventions are perfect, we can still obtain a weaker
identifiability result. Define ≺G to be the minimal partial order on [d] such that i ≺G j if (i, j) is an
edge in G, i.e., i ≺G j if and only if i is an ancestor of j in G. Note that any topological ordering
of G is compatible with the partial order ≺G. Then, our next result shows that under imperfect
interventions, we can still recover the partial order ≺G.
5

Theorem 2.
Suppose we are given the distributions X(i)
generated using a model
((B(i), η(i), ti)i∈¯I, f) with causal graph G such that the Assumptions 1-4 hold and none of the
interventions is a pure noise intervention. Then for any other model (( eB(i), eη(i), eti)i∈¯I, ef) with
causal graph eG generating the same observations the latent dimension d agrees and there is a
permutation ω ∈Sd such that eti = ω(ti) and i ≺e
G j iff ω(i) ≺G ω(j), i.e., ≺G can be identified up
to a permutation of the labels.
Remark 3. We emphasize that neither the full graph nor the coefficients B(i) or the latent variables
Zi are identifiable in this setting, even for linear mixing functions.
Theorem 1 and Theorem 2 generalize the main results of [84] which assume linear f while we
allow for non-linear f. The key new ingredient of our work is the following theorem, which shows
identifiability of f up to linear maps.
Theorem 3. Assume that X(i) is generated according to a model ((B(i), η(i), ti)i∈¯I, f) such
that the Assumptions 1-4 hold. Then we have identifiability up to linear transformations, i.e.,
if (( eB(i), eη(i), eti), ef) generates the same observed distributions ef( eZ(i))
D= X(i), then their latent
dimensions d agree and there is an invertible linear map T such that
ef = f ◦T −1,
eZ(i) = TZ(i).
(5)
The proof of this theorem is deferred to Appendix A. In Appendix B we then review the results of
[84] and show how their results can be extended to obtain our main results, Theorem 1 and Theorem 2.
Now we provide some intuition for the proofs of the main theorems.
Proof intuition The recent work [84] studies the special case when f is linear, and the proof is linear
algebraic. In particular, they consider row spans of the precision matrices of X(i), project them to
certain linear subspaces and use those subspaces to construct a generalized RQ decomposition of the
pseudoinverse of the linear mixing matrix f. However, once we are in the setting of non-linear f, such
an approach is not feasible because we can no longer reason about row spans of the precision matrices
of X, which have been transformed non-linearly thereby losing all linear algebraic structure. Instead,
we take a statistical approach and look at the log densities of the X(i). By choosing a Gaussian prior,
the log-odds ln p(i)
X (x) −ln p(0)
X (x) of X(i) with respect to X(0) can be written as a quadratic form
of difference of precision matrices, evaluated at non-linear functions of x. For simplicity of this
exposition, ignore terms arising from shift interventions and determinants of covariance matrices.
Then, the log-odds looks like θ(x) = f −1(x)⊤(Θ(i) −Θ(0))f −1(x), where Θ(i) is the precision
matrix of Z(i).
At this stage, we again shift our viewpoint to geometric and observe that Θ(i) −Θ(0) has a certain
structure. In particular, for single-node interventions, it has rank at most 2 and for source node
targets, it has rank 1. This implies that the level set manifolds of the quadratic forms θ(x) also
have a certain geometric structure in them, i.e., the DAG leaves a geometric signature on the data
likelihood. We exploit this carefully and proceed by induction on the topological ordering until we
end up showing that f can be identified up to a linear transformation, which is our main Theorem 3.
Here, the presence of shift interventions adds additional complexities, and we have to generalize all
of our intermediate lemmas to handle these. Once we identify f up to a linear transformation, we can
apply the results of [84] to conclude Theorems 1 and 2.
On the optimality and limitations of the assumptions
We make a few brief remarks on our
assumptions, deferring to Appendix D a full discussion and the technical construction of several
illustrative counterexamples.
1. Number of interventions: For our main results, Theorems 1 and 2, we assume that there are
at least d interventions (Assumption 4). This cannot be weakened even for linear mixing
functions [84]. In addition, we also show in Fact 1 that for the linear identifiability proved in
Theorem 3, d−2 interventions are not sufficient. Thus, the required number of interventions
is tight in Theorems 1, 2 and is tight up to at most one intervention in Theorem 3.
2. Intervention type: In the setting of imperfect interventions, the weaker identifiability guaran-
tees in Theorem 2 cannot be improved even when the mixing is linear [84, Appendix C].
We also show in Fact 2 that if we drop the condition that interventions are not pure noise
interventions, we have non-identifiability. Concretely, we show that when all interventions
6

are of pure noise type, any causal graph is compatible with the observations. Finally, in
contrast to the special case of linear mixing, we show in Lemma 8 that we need to exclude
non-stochastic hard interventions (i.e., do(Zi = zi)) for identifiability up to linear maps.
3. Distributional assumptions: We assume Gaussian latent variables, while allowing for very
flexible mixing f. This model has universal approximation guarantees and is moreover used
ubiquitously in practice. While the result can potentially be extended to more general latent
distributions (e.g., exponential families), we additionally show in Lemma 9 that the result is
not true when making no assumption on the distribution of ϵ.
5
Experimental methodology
In this section, we explain our experimental methodology and the theoretical underpinning of our
approach. Our main experiments for interventional causal representation learning focus on a method
based on contrastive learning. We train a deep neural network to learn to distinguish observational
samples x ∼X(0) from interventional samples x ∼X(i). Additionally, we design the last layer of
the model to model the log-likelihood of a linear Gaussian SCM. Due to representational flexibility
of deep neural networks, we will in principle learn the Bayes optimal classifier after optimal training,
which we show is related to the underlying causal model parameters. Accordingly, with careful
design of the last layer parametric form, we indirectly learn the parameters of the underlying causal
model. Similar methods have been used for time-series data or multimodal data [27, 32] but to the
best of our knowledge, the contrastive learning approach to interventional learning is novel.
Denote the probability density of x ∼X(i) (resp. z ∼Z(i)) by p(i)
X (x) (resp. p(i)
Z (z)). The next
lemma describes the log-odds of a sample x coming from the interventional distribution X(i) as
opposed to the observational distribution X(0). We focus on the identifiable case (see Theorem 1) and
therefore only consider perfect interventions. As per the notation in Section 3 and Appendix A.1, let
s(i) denote the row ti of B(0) and let η(i), λi denote the magnitude of the shift and scale intervention
respectively.
Lemma 1. When we have perfect interventions, the log-odds of a sample x coming from X(i) over
coming from X(0) is given by
ln p(i)
X (x) −ln p(0)
X (x) = ci −1
2λ2
i (((f −1(x))ti)2 + η(i)λi · (f −1(x))ti) + 1
2⟨f −1(x), s(i)⟩2 (6)
for a constant ci independent of x.
The proof is deferred to Appendix F. The form of the log-odds suggests considering the following
functions
gi(x, αi, βi, γi, w(i), θ) = αi −βih2
ti(x, θ) + γihti(x, θ) + ⟨h(x, θ), w(i)⟩2
(7)
where h(·, θ) denotes a neural net parametrized by θ, parameters w(i) are the rows of a matrix W and
αi, βi, γi are learnable parameters. Note that the ground truth parameters minimize the following
cross entropy loss
L(i)
CE = Ej∼U({0,i})Ex∼X(j)CE(1j=i, gi(x)) = −Ej∼U({0,i})Ex∼X(j) ln
 e1j=igi(x)
egi(x) + 1

.
(8)
Note that (compare (6) and (7)) W should learn B = D−1/2(Id −A) thus its off-diagonal en-
tries should form a DAG. To enforce this we add the NOTEARS regularizer [102] given by
RNOT EARS(W) = tr exp(W0◦W0)−d (see Appendix F.3) where W0 equals W with the main diag-
onal zeroed out. We also promote sparsity by adding the l1 regularization term RREG(W) = ∥W0∥1.
Thus, the total loss is given by
L(α, β, γ, W, θ) =
X
i∈I
L(i)
CE + τ1RNOT EARS(W) + τ2RREG(W)
(9)
for hyperparameters τ1 and τ2. Our identifiability result Theorem 1 implies that when we assume that
the neural network has infinite capacity, the loss in (9) is minimized, τ1 is large and τ2 small, and we
learn Gaussian latent variables h(X(i), θ), then we recover the ground truth latent variables up to the
7

10
20
30
d
5
10
SHD
10
20
30
d
0.995
1.000
AUROC
10
20
30
d
0.925
0.950
0.975
MCC
10
20
30
d
0.90
0.95
R2
Figure 2: Dependence of performance metrics for ER(d, 2) graphs with d′ = 100 and nonlinear
mixing f on the dimension d.
tolerable ambiguities of labeling and scale, i.e., h recovers f −1 and W recovers B up to permutation
and scale. Thus, we estimate the latent variables using ˆZ = h(X, θ) and estimate the DAG using W0.
Full details of the practical implementation of our approach are given in Appendix H.
Our experimental setup is similar to [84, 2], we consider d interventions with different targets (our
theory holds in full generality) and therefore, we can arbitrarily assign ti = i based on the intervention
index i which removes the permutation ambiguity. We focus on non-zero shifts because the cross
entropy loss together with the quadratic expression for the log-odds results in a non-convex output
layer which makes it hard to find the global loss minimizer, as we will describe in more detail in
Appendix G.1. We emphasize that this is no contradiction to the theoretical results stated above.
Even when there is no shift intervention the latent variables are identifiable, but our algorithm often
fails to find the global minimizer of the loss (9) due to the non-convex loss landscape. For the
sake of exposition and to set the stage for future work, we also briefly describe an approach via
Variational Autoencoders (VAE). VAEs have been widely used in causal representation learning and
while feasible in interventional settings, they are accompanied by certain difficulties, which we detail
and suggest how to overcome in Appendix F.4.
6
Experiments
In this section, we implement our approach on synthetic data and image data. Complete details
(architectures, hyperparameters) are deferred to Appendix H. Additional experiments investigating
the effect of varsortability [66] and the noise distribution can be found in Appendix G.
Data generation
For all our experiments we use Erdös-Rényi graphs, i.e., we add each undirected
edge with equal probability p to the graph and then finally orient them according to some random order
of the nodes. We write ER(d, k) for the Erdös-Rényi graph distribution on d nodes with kd expected
edges. For a given graph G we then sample edge weights from U(±[0.25, 1.0]) and a scale matrix D.
For simplicity we assume that we have n samples from each environment i ∈¯I. We only consider
the setting where each node is intervened upon once and thus the latent dimension is also known. We
consider three types of mixing functions. First, we consider linear mixing functions where we sample
all matrix entries i.i.d. from a Gaussian distribution. Then, we consider non-linear mixing functions
that are parametrized by MLPs with three hidden layers which are randomly initialized, and have
Leaky ReLU activations. Finally, we consider image data as described in [2]. Pairs of latent variables
(z2i+1, z2i+2) describe the coordinates of balls in an image and the non-linearity f is the rendering of
the image. The image generation is based on pygame [77]. A sample image can be found in Figure 3.
Figure 3:
Sample
image with 3 balls.
Evaluation Metrics
We evaluate how well we can recover the ground truth
latent variables and the underlying DAG. For the recovery of the ground
truth latents we use the Mean Correlation Coefficient (MCC) [37, Appendix
A.2]. For the evaluation of the learned graph structure we use the Structural
Hamming Distance (SHD) where we follow the convention that we count the
number of edge differences of the directed graphs (i.e., an edge with wrong
orientation counts as two errors). Since the scale of the variables is not fixed the
selection of the edge selection threshold is slightly delicate. Thus, we use the
heuristic where we match the number of selected edges to the expected number
8

of edges. As a metric that is independent of this thresholding procedure, we
also report the Area Under the Receiver Operating Curve (AUROC) for the edge selection.
Methods
We implement our contrastive algorithm as explained in Section F where we use an MLP
(with Leaky ReLU nonlinearities) for the function h for the linear and nonlinear mixing functions
and a very small convolutional network for the image dataset, which are termed “Contrastive”. For
linear mixing function we also consider a version of the contrastive algorithm where h is a linear
function, termed “Contrastive Linear”. As baselines, we consider a variational autoencoder with
latent dimension d and also the algorithm for linear disentanglement introduced in [84]. Since the
variational autoencoder does not output a causal graph structure we report the result for the empty
graph here which serves as a baseline.
Results for linear f
First, for the sake of comparison, we replicate exactly the setting from
[84], i.e., we consider initial noise variances sampled uniformly from [2, 4] and we consider perfect
interventions where the new variance is sampled uniformly from [6, 8]. We set d = 5, d′ = 10,
k = 3/2, and n = 50000. Results can be found in Table 1. The linear contrastive method identifies
the ground truth latent variables up to scaling. The failure of the nonlinear method to recover the
ground truth latents will be explained and further analyzed in Appendix G.1. Note that the nonlinear
contrastive and the linear contrastive method recover the underlying graph better than the baseline.
Table 1: Results for linear f with d = 5, d′ = 10, k = 3/2, n = 50000.
Method
SHD ↓
AUROC ↑
MCC ↑
R2 ↑
Contrastive
4.6 ± 0.5
0.84 ± 0.02
0.05 ± 0.02
0.02 ± 0.00
Contrastive Linear
5.4 ± 1.6
0.80 ± 0.07
0.90 ± 0.03
1.00 ± 0.00
Linear baseline
7.0 ± 0.5
0.64 ± 0.05
0.83 ± 0.04
1.00 ± 0.00
Results for nonlinear f
We sample all variances from U([1, 2]) (initial variance and resampling
for the perfect interventions) and the shift parameters η(i) of the interventions from U([1, 2]). Results
can be found in Table 2. We find that our contrastive method can recover the ground truth latents and
the causal structure almost perfectly, while the baseline for linear disentanglement cannot recover
the graph or the latent variables (which is not surprising as the mixing is highly nonlinear). Also,
training a vanilla VAE does not recover the latent variables up to a linear map as indicated by the R2
scores. In Figure 2 we illustrate the dependence on the dimension d of our algorithm in this setting.
Table 2: Results for nonlinear synthetic data with n = 10000.
Setting
Method
SHD ↓
AUROC ↑
MCC ↑
R2 ↑
ER(5, 2), d′ = 20
Contrastive
1.8 ± 0.5
0.97 ± 0.01
0.97 ± 0.00
0.96 ± 0.00
VAE
10.0 ± 0.0
0.50 ± 0.00
0.48 ± 0.03
0.57 ± 0.07
Linear baseline
10.6 ± 1.9
0.48 ± 0.11
0.32 ± 0.03
0.34 ± 0.06
ER(5, 2), d′ = 100
Contrastive
1.0 ± 0.0
1.00 ± 0.00
0.99 ± 0.00
0.98 ± 0.00
VAE
10.0 ± 0.0
0.50 ± 0.00
0.59 ± 0.02
0.68 ± 0.04
Linear baseline
3.4 ± 1.2
0.85 ± 0.07
0.18 ± 0.04
0.11 ± 0.04
ER(10, 2), d′ = 20
Contrastive
3.6 ± 1.3
0.98 ± 0.01
0.93 ± 0.00
0.87 ± 0.01
VAE
18.6 ± 0.9
0.50 ± 0.00
0.59 ± 0.02
0.72 ± 0.02
Linear baseline
29.6 ± 2.5
0.49 ± 0.02
0.44 ± 0.02
0.51 ± 0.02
ER(10, 2), d′ = 100
Contrastive
1.6 ± 0.5
1.00 ± 0.00
0.98 ± 0.00
0.97 ± 0.00
VAE
18.6 ± 0.9
0.50 ± 0.00
0.62 ± 0.02
0.78 ± 0.01
Linear baseline
28.4 ± 2.1
0.51 ± 0.04
0.17 ± 0.03
0.13 ± 0.03
Results for image data
Finally, we report the results for image data. Here, we generate the graph as
before and consider variances sampled from σ2 ∼U([0.01, 0.02]) and shifts η(i) from U([0.1, 0.2])
(i.e., the shifts are still of order σ). We exclude samples where one of the balls is not contained in the
image which generates a slight model misspecification compared to our theory. Again we find that
we recover the latent graph and the latent variables as detailed in Table 3.
9

Table 3: Results for image data with ER(d, 2) graphs with d = 2 · #balls and nint = 25000 (per
environment), nobs = nint · d.
# Balls
Method
SHD ↓
AUROC ↑
MCC ↑
R2 ↑
2
Contrastive Learning
1.4 ± 0.4
0.95 ± 0.03
0.87 ± 0.03
0.84 ± 0.03
VAE
6.0 ± 0.0
0.50 ± 0.00
0.19 ± 0.06
0.16 ± 0.08
5
Contrastive Learning
2.0 ± 0.3
1.00 ± 0.00
0.94 ± 0.01
0.91 ± 0.01
VAE
18.6 ± 0.9
0.50 ± 0.00
0.31 ± 0.02
0.36 ± 0.03
10
Contrastive Learning
11.0 ± 3.3
0.98 ± 0.02
0.89 ± 0.01
0.83 ± 0.01
VAE
37.2 ± 3.1
0.50 ± 0.00
0.22 ± 0.01
0.33 ± 0.02
7
Conclusion
In this work, we extend several prior works and show identifiability for a widely used class of
linear latent variable models with non-linear mixing, from interventional data with unknown targets.
Counterexamples show that our assumptions are tight in this setting and could only potentially be
relaxed under other additional assumptions. We leave to future work to extend our results for other
classes of priors, such as non-parametric distribution families, and also to study sample complexity
and robustness of our results. We also proposed a contrastive approach to learn such models in
practice and showed that it can recover the latent structure in various settings. Finally, we highlight
that the results of our experiments are very promising and it would be interesting to scale up our
algorithms to large-scale datasets such as [49].
Acknowledgments
We thank anonymous reviewers for useful comments and suggestions. We acknowledge the support of
AFRL and DARPA via FA8750-23-2-1015, ONR via N00014-23-1-2368, and NSF via IIS-1909816,
IIS-1955532. We also acknowledge the support of JPMorgan Chase & Co. AI Research. This work
was also supported by the Tübingen AI Center.
References
[1] K. Ahuja, J. S. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse
perturbations. Advances in Neural Information Processing Systems, 35:15516–15528, 2022.
[2] K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. Interventional causal representation learn-
ing. In Proceedings of the 40th International Conference on Machine Learning, ICML’23.
JMLR.org, 2023.
[3] S. Arora, R. Ge, Y. Halpern, D. M. Mimno, A. Moitra, D. A. Sontag, Y. Wu, and M. Zhu. A
practical algorithm for topic modeling with provable guarantees. In Proceedings of the 30th
International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June
2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 280–288. JMLR.org,
2013. URL http://proceedings.mlr.press/v28/arora13.html.
[4] K. Bello, B. Aragam, and P. K. Ravikumar. DAGMA: Learning DAGs via m-matrices and
a log-determinant acyclicity characterization. In A. H. Oh, A. Agarwal, D. Belgrave, and
K. Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https:
//openreview.net/forum?id=8rZYMpFUgK.
[5] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.
arXiv preprint arXiv:2108.07258, 2021.
[6] J. Brehmer, P. de Haan, P. Lippe, and T. S. Cohen. Weakly supervised causal representation
learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
10

Advances in Neural Information Processing Systems, volume 35, pages 38319–38331. Curran
Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/
2022/file/fa567e2b2c870f8f09a87b6e73370869-Paper-Conference.pdf.
[7] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee,
Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with
GPT-4. arXiv preprint arXiv:2303.12712, 2023.
[8] S. Buchholz. Some remarks on identifiability of independent component analysis in restricted
function classes. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL
https://openreview.net/forum?id=REtKapdkyI.
[9] S. Buchholz, M. Besserve, and B. Schölkopf. Function classes for identifiable nonlinear
independent component analysis. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors,
Advances in Neural Information Processing Systems, 2022. URL https://openreview.
net/forum?id=DpKaP-PY8bK.
[10] F. Castelletti and S. Peluso. Network structure learning under uncertain interventions. Journal
of the American Statistical Association, pages 1–12, 2022.
[11] Y. Chen, E. Rosenfeld, M. Sellke, T. Ma, and A. Risteski.
Iterative feature matching:
Toward provable domain generalization with logarithmic environments.
In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
Information Processing Systems, volume 35, pages 1725–1736. Curran Associates, Inc.,
2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
0b5eb45a22ff33956c043dd271f244ea-Paper-Conference.pdf.
[12] D. M. Chickering. Optimal structure identification with greedy search. J. Mach. Learn. Res.,
3:507–554, 2002. URL http://jmlr.org/papers/v3/chickering02b.html.
[13] P. Comon. Independent component analysis, a new concept?
Signal processing, 36(3):
287–314, 1994.
[14] J. Cui, W. Huang, Y. Wang, and Y. Wang. Aggnce: Asymptotically identifiable contrastive
learning.
[15] A. D’Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi, A. Beutel, C. Chen, J. Deaton,
J. Eisenstein, M. D. Hoffman, et al. Underspecification presents challenges for credibility in
modern machine learning. The Journal of Machine Learning Research, 23(1):10237–10297,
2022.
[16] N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and
M. Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders.
arXiv preprint arXiv:1611.02648, 2016.
[17] A. Dixit, O. Parnas, B. Li, J. Chen, C. P. Fulco, L. Jerby-Arnon, N. D. Marjanovic, D. Dionne,
T. Burks, R. Raychowdhury, B. Adamson, T. M. Norman, E. S. Lander, J. S. Weissman,
N. Friedman, and A. Regev. Perturb-seq: Dissecting molecular circuits with scalable single-cell
rna profiling of pooled genetic screens. Cell, 167(7):1853–1866, Dec 2016. ISSN 1097-4172
(Electronic); 0092-8674 (Print); 0092-8674 (Linking). doi: 10.1016/j.cell.2016.11.038.
[18] F. Eberhardt, C. Glymour, and R. Scheines.
On the number of experiments suf-
ficient and in the worst case necessary to identify all causal relations among N
variables.
In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Ar-
tificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, pages 178–184. AUAI
Press, 2005. URL https://dslpitt.org/uai/displayArticleDetails.jsp?mmnu=
1&smnu=2&article_id=1235&proceeding_id=21.
[19] F. Falck, H. Zhang, M. Willetts, G. Nicholson, C. Yau, and C. C. Holmes. Multi-facet clustering
variational autoencoders. Advances in Neural Information Processing Systems, 34, 2021.
[20] E. Ferrara. Should ChatGPT be biased? challenges and risks of bias in large language models.
arXiv preprint arXiv:2304.03738, 2023.
11

[21] J. L. Gamella, A. Taeb, C. Heinze-Deml, and P. Bühlmann. Characterization and greedy
learning of gaussian structural causal models under unknown interventions, 2022.
[22] L. Gresele, J. Von Kügelgen, V. Stimper, B. Schölkopf, and M. Besserve. Independent
mechanism analysis, a new concept? Advances in Neural Information Processing Systems, 34,
2021.
[23] A. Hauser and P. Bühlmann. Characterization and greedy learning of interventional markov
equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research, 13
(1):2409–2464, 2012.
[24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–
778, 2016.
[25] C. Heinze-Deml, J. Peters, and N. Meinshausen. Invariant causal prediction for nonlinear
models. Journal of Causal Inference, 6(2), 2018.
[26] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems, 33:6840–6851, 2020.
[27] A. Hyvärinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ICA. In D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Gar-
nett, editors, Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain,
pages 3765–3773, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
d305281faf947ca7acade9ad5c8c818c-Abstract.html.
[28] A. Hyvärinen and E. Oja. Independent component analysis: algorithms and applications.
Neural networks, 13(4-5):411–430, 2000.
[29] A. Hyvärinen and P. Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural networks, 12(3):429–439, 1999.
[30] A. Hyvarinen, J. Karhunen, and E. Oja. Independent component analysis. Studies in informatics
and control, 11(2):205–207, 2002.
[31] A. Hyvarinen, H. Sasaki, and R. Turner. Nonlinear ica using auxiliary variables and generalized
contrastive learning. In The 22nd International Conference on Artificial Intelligence and
Statistics, pages 859–868. PMLR, 2019.
[32] A. Hyvärinen, I. Khemakhem, and R. Monti. Identifiability of latent-variable and structural-
equation models: from linear to nonlinear. arXiv preprint arXiv:2302.02672, 2023.
[33] I. Ishikawa, T. Teshima, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama. Universal approxima-
tion property of invertible neural networks. arXiv preprint arXiv:2204.07415, 2022.
[34] A. Jaber, M. Kocaoglu, K. Shanmugam, and E. Bareinboim. Causal discovery from soft
interventions with unknown targets: Characterization and learning. Advances in neural
information processing systems, 33:9551–9561, 2020.
[35] Y. Jiang and B. Aragam. Learning latent causal graphs with unknown interventions. In
Advances in Neural Information Processing Systems, 2023.
[36] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and
nonlinear ica: A unifying framework. In International Conference on Artificial Intelligence
and Statistics, pages 2207–2217. PMLR, 2020.
[37] I. Khemakhem, R. P. Monti, D. P. Kingma, and A. Hyvärinen. Ice-beem: Identifiable con-
ditional energy-based deep models based on nonlinear ICA. In H. Larochelle, M. Ranzato,
R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/
2020/hash/962e56a8a0b0420d87272a682bfd1e53-Abstract.html.
12

[38] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and
Y. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
[39] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Y. Bengio and Y. LeCun,
editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6114.
[40] B. Kivva, G. Rajendran, P. Ravikumar, and B. Aragam. Learning latent causal graphs via
mixture oracles. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang, and J. W. Vaughan,
editors, Advances in Neural Information Processing Systems 34: Annual Conference on
Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, vir-
tual, pages 18087–18101, 2021. URL https://proceedings.neurips.cc/paper/2021/
hash/966aad8981dcc75b5b8ab04427a833b2-Abstract.html.
[41] B.
Kivva,
G.
Rajendran,
P.
Ravikumar,
and
B.
Aragam.
Identifiabil-
ity
of
deep
generative
models
without
auxiliary
information.
In
NeurIPS,
2022.
URL
http://papers.nips.cc/paper_files/paper/2022/hash/
649f080d8891ab4d4b262cb9cd52e69a-Abstract-Conference.html.
[42] S. Lachapelle, P. Rodríguez, Y. Sharma, K. Everett, R. L. Priol, A. Lacoste, and S. Lacoste-
Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear
ICA. In B. Schölkopf, C. Uhler, and K. Zhang, editors, 1st Conference on Causal Learning
and Reasoning, CLeaR 2022, Sequoia Conference Center, Eureka, CA, USA, 11-13 April,
2022, volume 177 of Proceedings of Machine Learning Research, pages 428–484. PMLR,
2022. URL https://proceedings.mlr.press/v177/lachapelle22a.html.
[43] T. E. Lee, J. A. Zhao, A. S. Sawhney, S. Girdhar, and O. Kroemer. Causal reasoning in
simulation for structure and transfer learning of robot manipulation policies. In 2021 IEEE
International Conference on Robotics and Automation (ICRA), pages 4776–4782. IEEE, 2021.
[44] S. Li, B. Hooi, and G. H. Lee. Identifying through flows for recovering latent representations. In
8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=
SklOUpEYvB.
[45] W. Liang, A. Keki´c, J. von Kügelgen, S. Buchholz, M. Besserve, L. Gresele, and B. Schölkopf.
Causal component analysis. In Advances in Neural Information Processing Systems, 2023.
[46] P. Lippe, S. Magliacane, S. Löwe, Y. M. Asano, T. Cohen, and S. Gavves. Citris: Causal
identifiability from temporal intervened sequences. In International Conference on Machine
Learning, pages 13557–13603. PMLR, 2022.
[47] C. Liu, L. Zhu, and M. Belkin. Loss landscapes and optimization in over-parameterized non-
linear systems and neural networks. Applied and Computational Harmonic Analysis, 59:85–
116, 2022. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2021.12.009. URL https://
www.sciencedirect.com/science/article/pii/S106352032100110X. Special Issue
on Harmonic Analysis and Machine Learning.
[48] Y. Liu, Z. Zhang, D. Gong, M. Gong, B. Huang, A. v. d. Hengel, K. Zhang, and J. Q. Shi.
Identifying weight-variant latent causal models. arXiv preprint arXiv:2208.14153, 2022.
[49] Y. Liu, A. Alahi, C. Russell, M. Horn, D. Zietlow, B. Schölkopf, and F. Locatello. Causal
triplet: An open challenge for intervention-centric causal representation learning. In 2nd
Conference on Causal Learning and Reasoning (CLeaR), 2023. arXiv:2301.05169.
[50] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Schölkopf, and O. Bachem. Chal-
lenging common assumptions in the unsupervised learning of disentangled representations. In
international conference on machine learning, pages 4114–4124. PMLR, 2019.
13

[51] I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In
5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:
//openreview.net/forum?id=Skq89Scxx.
[52] M. Lotfollahi, A. K. Susmelj, C. De Donno, Y. Ji, I. L. Ibarra, F. A. Wolf, N. Yakubova, F. J.
Theis, and D. Lopez-Paz. Compositional perturbation autoencoder for single-cell response
modeling. BioRxiv, 2021.
[53] Y. Lu and J. Lu.
A universal approximation theorem of deep neural networks for ex-
pressing probability distributions.
In H. Larochelle, M. Ranzato, R. Hadsell, M. Bal-
can, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
2000f6325dfc4fc3201fc45ed01c7a5d-Abstract.html.
[54] G. E. Moran, D. Sridhar, Y. Wang, and D. Blei. Identifiable deep generative models via sparse
decoding. Transactions on Machine Learning Research, 2022.
[55] H. Morioka and A. Hyvarinen. Connectivity-contrastive learning: Combining causal discovery
and representation learning for multimodal data. In International Conference on Artificial
Intelligence and Statistics, pages 3399–3426. PMLR, 2023.
[56] J. Moser. On the volume elements on a manifold. Transactions of the American Mathematical
Society, 120(2):286–294, 1965. ISSN 00029947. URL http://www.jstor.org/stable/
1994022.
[57] A. Nejatbakhsh, F. Fumarola, S. Esteki, T. Toyoizumi, R. Kiani, and L. Mazzucato. Predicting
perturbation effects from resting activity using functional causal flow. bioRxiv, pages 2020–11,
2020.
[58] T. M. Norman, M. A. Horlbeck, J. M. Replogle, A. Y. Ge, A. Xu, M. Jost, L. A. Gilbert, and
J. S. Weissman. Exploring genetic interaction manifolds constructed from rich single-cell
phenotypes. Science, 365(6455):786–793, 2019.
[59] OpenAI. GPT-4 technical report, 2023.
[60] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Z. Yang, Z. DeVito, M. Raison,
A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An im-
perative style, high-performance deep learning library. In H. M. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alché-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/
bdbca288fee7f92f2bfa9f7012727740-Abstract.html.
[61] J. Pearl. Causality. Cambridge university press, 2009.
[62] J. Peters, P. Bühlmann, and N. Meinshausen. Causal inference by using invariant prediction:
identification and confidence intervals. Journal of the Royal Statistical Society. Series B
(Statistical Methodology), 78(5):947–1012, 2016. ISSN 13697412, 14679868. URL http:
//www.jstor.org/stable/44682904.
[63] J. Peters, D. Janzing, and B. Schölkopf. Elements of causal inference: foundations and
learning algorithms. The MIT Press, 2017.
[64] G. Rajendran, B. Kivva, M. Gao, and B. Aragam. Structure learning in polynomial time:
Greedy algorithms, bregman information, and exponential families. Advances in Neural
Information Processing Systems, 34:18660–18672, 2021.
[65] G. Rajendran, P. Reizinger, W. Brendel, and P. K. Ravikumar. An interventional perspective
on identifiability in gaussian lti systems with independent component analysis. In UAI 2023
Workshop on Causal inference for time series data, 2023.
14

[66] A. G. Reisach, C. Seiler, and S. Weichwald. Beware of the simulated dag! causal discovery
benchmarks may be easy to game. In M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P. Liang,
and J. W. Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-
14, 2021, virtual, pages 27772–27784, 2021. URL https://proceedings.neurips.cc/
paper/2021/hash/e987eff4a7c7b7e580d659feb6f60c1a-Abstract.html.
[67] A. G. Reisach, M. Tami, C. Seiler, A. Chambaz, and S. Weichwald. Simple sorting criteria help
find the causal order in additive noise models. In Advances in Neural Information Processing
Systems, 2023.
[68] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In International conference on machine learning, pages
1278–1286. PMLR, 2014.
[69] J. Rissanen. Modeling by shortest data description. Autom., 14(5):465–471, 1978. doi: 10.1016/
0005-1098(78)90005-5. URL https://doi.org/10.1016/0005-1098(78)90005-5.
[70] G. Roeder, L. Metz, and D. Kingma. On linear identifiability of learned representations. In
International Conference on Machine Learning, pages 9030–9039. PMLR, 2021.
[71] E. Rosenfeld, P. K. Ravikumar, and A. Risteski. The risks of invariant risk minimization. In
International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=BbNIbVPJ-42.
[72] E. Rosenfeld, P. Ravikumar, and A. Risteski. Domain-adjusted regression or: Erm may already
learn features sufficient for out-of-distribution generalization. arXiv preprint arXiv:2202.06856,
2022.
[73] D. Rothenhäusler, N. Meinshausen, P. Bühlmann, and J. Peters. Anchor regression: Hetero-
geneous data meet causality. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 83(2):215–246, 2021.
[74] B. Schölkopf and J. von Kügelgen.
From statistical to causal learning.
arXiv preprint
arXiv:2204.00607, 2022.
[75] B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio.
Toward causal representation learning. Proceedings of the IEEE, 109(5):612–634, 2021.
arXiv:2102.11107.
[76] X. Shen, F. Liu, H. Dong, Q. Lian, Z. Chen, and T. Zhang. Weakly supervised disentangled
generative causal representation learning. Journal of Machine Learning Research, 23:1–55,
2022.
[77] P. Shinners. Pygame. http://pygame.org/, 2011.
[78] R. Silva, R. Scheines, C. Glymour, P. Spirtes, and D. M. Chickering. Learning the structure of
linear latent variable models. Journal of Machine Learning Research, 7(2), 2006.
[79] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning
using nonequilibrium thermodynamics. In International Conference on Machine Learning,
pages 2256–2265. PMLR, 2015.
[80] P. Spirtes and C. Glymour. An algorithm for fast recovery of sparse causal graphs. Social
science computer review, 9(1):62–72, 1991.
[81] P. Spirtes, C. N. Glymour, and R. Scheines. Causation, prediction, and search. MIT press,
2000.
[82] C. Squires and C. Uhler. Causal structure learning: a combinatorial perspective. Foundations
of Computational Mathematics, pages 1–35, 2022.
[83] C. Squires, Y. Wang, and C. Uhler. Permutation-based causal structure learning with unknown
intervention targets. In Conference on Uncertainty in Artificial Intelligence, pages 1039–1048.
PMLR, 2020.
15

[84] C. Squires, A. Seigal, S. S. Bhate, and C. Uhler. Linear causal disentanglement via interven-
tions. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors,
International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 32540–32560.
PMLR, 2023. URL https://proceedings.mlr.press/v202/squires23a.html.
[85] S. G. Stark, J. Ficek, F. Locatello, X. Bonilla, S. Chevrier, F. Singer, G. Rätsch, and K.-V.
Lehmann. Scim: universal single-cell matching with unpaired feature sets. Bioinformatics, 36
(Supplement_2):i919–i927, 2020.
[86] T. Teshima, I. Ishikawa, K. Tojo, K. Oono, M. Ikeda, and M. Sugiyama. Coupling-based
invertible neural networks are universal diffeomorphism approximators. Advances in Neural
Information Processing Systems, 33:3362–3373, 2020.
[87] B. Varici, K. Shanmugam, P. Sattigeri, and A. Tajer. Scalable intervention target estimation in
linear models. Advances in Neural Information Processing Systems, 34:1494–1505, 2021.
[88] B. Varici, E. Acarturk, K. Shanmugam, A. Kumar, and A. Tajer. Score-based causal represen-
tation learning with interventions. arXiv preprint arXiv:2301.08230, 2023.
[89] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.
[90] C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.
Springer Berlin Heidelberg, 2008. ISBN 9783540710509. URL https://books.google.
de/books?id=hV8o5R7_5tkC.
[91] J. Von Kügelgen, Y. Sharma, L. Gresele, W. Brendel, B. Schölkopf, M. Besserve, and F. Lo-
catello. Self-supervised learning with data augmentations provably isolates content from style.
Advances in Neural Information Processing Systems, 34, 2021.
[92] J. von Kügelgen, M. Besserve, W. Liang, L. Gresele, A. Keki´c, E. Bareinboim, D. M. Blei,
and B. Schölkopf. Nonparametric identifiability of causal representations from unknown
interventions. In Advances in Neural Information Processing Systems, 2023.
[93] Y. Wang, D. Blei, and J. P. Cunningham.
Posterior collapse and latent variable non-
identifiability. Advances in Neural Information Processing Systems, 34:5443–5455, 2021.
[94] S. Weichwald, S. W. Mogensen, T. E. Lee, D. Baumann, O. Kroemer, I. Guyon, S. Trimpe,
J. Peters, and N. Pfister. Learning by doing: Controlling a dynamical system using causality,
control, and reinforcement learning. In NeurIPS 2021 Competitions and Demonstrations Track,
pages 246–258. PMLR, 2022.
[95] M. Willetts and B. Paige. I don’t need u: Identifiable non-linear ica without side information.
arXiv preprint arXiv:2106.05238, 2021.
[96] F. Xie, R. Cai, B. Huang, C. Glymour, Z. Hao, and K. Zhang. Generalized independent
noise condition for estimating latent variable causal graphs. Advances in neural information
processing systems, 33:14891–14902, 2020.
[97] M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang. Causalvae: Disentangled representa-
tion learning via neural structural causal models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages 9593–9602, June 2021.
[98] Y. Yu, J. Chen, T. Gao, and M. Yu. Dag-gnn: Dag structure learning with graph neural
networks. In International Conference on Machine Learning, pages 7154–7163. PMLR, 2019.
[99] J. Zhang, C. Squires, and C. Uhler. Matching a desired causal state via shift interventions.
Advances in Neural Information Processing Systems, 34:19923–19934, 2021.
[100] J. Zhang, C. Squires, K. Greenewald, A. Srivastava, K. Shanmugam, and C. Uhler. Iden-
tifiability guarantees for causal disentanglement from soft interventions. arXiv preprint
arXiv:2307.06250, 2023.
16

[101] S. Zhao, J. Song, and S. Ermon. Infovae: Balancing learning and inference in variational
autoencoders. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019,
The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019,
The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019,
Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 5885–5892. AAAI Press, 2019.
doi: 10.1609/AAAI.V33I01.33015885. URL https://doi.org/10.1609/aaai.v33i01.
33015885.
[102] X. Zheng, B. Aragam, P. K. Ravikumar, and E. P. Xing. Dags with no tears: Continuous
optimization for structure learning. Advances in neural information processing systems, 31,
2018.
[103] Y. Zheng, I. Ng, and K. Zhang. On the identifiability of nonlinear ICA: sparsity and beyond.
In NeurIPS, 2022.
URL http://papers.nips.cc/paper_files/paper/2022/hash/
6801fa3fd290229efc490ee0cf1c5687-Abstract-Conference.html.
[104] R. S. Zimmermann, Y. Sharma, S. Schneider, M. Bethge, and W. Brendel. Contrastive learning
inverts the data generating process. In International Conference on Machine Learning, pages
12979–12990. PMLR, 2021.
17

A
Proof of Identifiability up to Linear Maps
In this section we give a full proof of Theorem 3. We try to make this essential self-contained. To
make this easy to read, we first introduce and recall the required notation in Section A.1, then we
prove two important key relations required for the proof in Section A.2. Finally, we prove first a
simpler version of Theorem 3 with more assumptions in Section A.3 and then give the full proof of
Theorem 3 in Section A.4.
A.1
General Notation
Let us introduce some notation for our identifiability proofs. For a full list of assumptions we refer
to the main text, here we just recall the relevant notation concisely. We will assume that there are
two representations that generate the same observations, i.e., we assume that there are two sets of
Gaussian SCMs
Z(i) = A(i)Z(i) + (D(i))
1
2 (ϵ + η(i)eti)
(10)
eZ(i) = eA(i) eZ(i) + ( eD(i))
1
2 (eϵ + eη(i)eeti)
(11)
where ϵ,eϵ ∼N(0, Id) and i = 0 corresponds to the observational distribution while i > 0 correspond
to interventional settings where we intervene on a single node ti (or eti) by adding a shift and changing
the relation to its parents (without adding new parents). Moreover, there are two functions f, ef such
that X(i) D= f(Z(i)) and X(i) D= ef( eZ(i)). It is convenient to define
B(i) = (D(i))−1
2 (Id −A(i))
(12)
µ(i) = η(i)(B(i))−1eti
(13)
so that Z(i) = (B(i))−1ϵ + µ(i) with ϵ ∼N(0, Id). Note that all model information can be collected
in ((B(i), η(i), ti), f).
As in the main text, it is convenient to use the shorthand B = B(0), A = A(0).
We denote the row ti of B(i) by r(i), i.e.,
r(i) = (B(i))⊤eti.
(14)
Note that for perfect interventions where we cut the relation to all parents of node ti we have
r(i) = λieti for some λi ̸= 0. Similarly, for the observational distribution we refer to the row ti of
B(0) by s(i), i.e.,
s(i) = (B(0))⊤eti.
(15)
We thus find
B(0) −B(i) = eti(e⊤
tiB(0) −e⊤
tiB(i)) = eti(s(i) −r(i))⊤.
(16)
We also consider the covariance matrix Σ(i) and the precision matrix Θ(i) of Z(i) which are given by
Θ(i) = (B(i))⊤B(i),
Σ(i) = (Θ(i))−1
(17)
Indeed,
Σ(i) = E[(B(i))−1ϵϵ⊤(B(i))−⊤] = (B(i))−1E[ϵϵ⊤](B(i))−⊤= (B(i))−1(B(i))−⊤
Finally, the mean µ(i) of Z(i) given by
µ(i) = EZ(i) = η(i)(B(i))−1eti.
(18)
We also define similar quantities for eZ. To simplify the notation, we will frequently use the shorthand
v ⊗w = v · w⊤∈Rd×d for v, w ∈Rd.
18

A.2
Two auxiliary lemmas
The key to our identifiability results is the relation shown in the following lemma.
Lemma 2. Assume that there are two latent variable models ((B(i), η(i), ti)i∈I, f) and
(( eB(i), eη(i), eti)i∈I, ef) as defined above such that f(Z(i))
D= ef( eZ(i)). Then their latent dimen-
sion d agree and h = ef −1 ◦f exists and is a diffeomorphism, i.e., bijective and differentiable with
differentiable inverse. Moreover, it satisfies the relation
1
2z⊤(Θ(i) −Θ(0))z −η(i)(r(i))⊤z
= 1
2h(z)⊤(eΘ(i) −eΘ(0))h(z) −eη(i)(er(i))⊤h(z) + c(i)
(19)
for all z and i and some constant c(i). If η(i) = eη(i) = 0 this simplifies to
1
2z⊤(Θ(i) −Θ(0))z = 1
2h(z)⊤(eΘ(i) −eΘ(0))h(z) + c(i).
(20)
Proof. Note first that X(0)
D= f(Z(0) implies since f is an embedding by assumption that the
dimension of the support of X(0) is the same as the latent dimension and this dimension is identifiable
from the observations, e.g., by considering the tangent space. As Z(0) and eZ(0) have full support
and f and ˜f are by assumption embeddings, the equality f(Z(0))
D= ef( eZ(0)) implies that the
two image manifolds agree and h = ef −1 ◦f is a differentiable map. Moreover, we find that
h∗Z(i) = eZ(i) where h∗denotes the pushforward map. The variables Z(i) are Gaussian with
distribution Z(i) ∼N(µ(i), Σ(i)). This implies that its density p(i) can be written as
ln(p(i)(z)) = −n
2 ln(2π) −1
2 ln |Σ(i)| −1
2(z −µ(i))⊤Θ(i)(z −µ(i)).
(21)
A similar relation holds for ep(i)(z), the density of eZ(i). Finally, we have the relations
p(i)(z) = ep(i)(h(z))| det Jh(z)|
(22)
which is a consequence of h∗Z(i) = eZ(i). Here Jh denotes the Jacobian matrix of partial derivatives.
Thus we conclude that
−n
2 ln(2π) −1
2 ln |Σ(i)| −1
2(z −µ(i))⊤Θ(i)(z −µ(i))
= −n
2 ln(2π) −1
2 ln |eΣ(i)| −1
2(h(z) −eµ(i))⊤eΘ(i)(h(z) −eµ(i)) + ln | det Jh(z)|.
(23)
Calling this relation Ei, we consider the difference E0 −Ei. Then the determinant term cancels, and
we obtain for some constant c(i)
1
2(z −µ(i))⊤Θ(i)(z −µ(i)) −1
2z⊤Θ(0)z
= 1
2(h(z) −eµ(i))⊤eΘ(i)(h(z) −eµ(i)) −1
2h(z)⊤eΘ(0)h(z) + c′(i).
(24)
Expanding the quadratic expression on the left-hand side we obtain
1
2(z −µ(i))⊤Θ(i)(z −µ(i)) −1
2z⊤Θ(0)z = 1
2z⊤(Θ(i) −Θ)z −z⊤Θ(i)µ(i) + 1
2(µ(i))⊤Θ(i)µ(i).
(25)
Finally we simplify the last term using (18)
Θ(i)µ(i) = η(i)(B(i))⊤B(i)(B(i))−1eti = η(i)(B(i))⊤eti = η(i)r(i)
(26)
Apply the same simplification on the right hand side. Finally, we collect the constant terms indepen-
dent of z in c(i), to obtain
1
2z⊤(Θ(i) −Θ(0))z −η(i)(r(i))⊤z
= 1
2h(z)⊤(eΘ(i) −eΘ(0))h(z) −eη(i)(er(i))⊤h(z) + c(i)
(27)
which is (19) as desired.
19

A second key ingredient for our identifiability results is that the specific structure of the interventions
that each target a single node which implies that the difference of precision matrices Θ(i) −Θ(0) has
rank at most 2. This is also the key ingredient used (in a very different manner) in the proof of the
main result in [84]. Here we highlight this simple result because we make use of it frequently.
Lemma 3. The precision matrices satisfy the relation
Θ(i) −Θ(0) = r(i) ⊗r(i) −s(i) ⊗s(i).
(28)
Proof. Using (17) we find
Θ(i) −Θ(0) = (B(i))⊤B(i) −B⊤B
= (B(i))⊤
 d
X
k=1
eke⊤
k
!
B(i) −B⊤
 d
X
k=1
eke⊤
k
!
B
=
d
X
k=1
(B(i))⊤ek((B(i))⊤ek)⊤−
d
X
k=1
B⊤ek(B⊤ek)⊤
= ((B(i))⊤eti) ⊗((B(i))⊤eti) −(B⊤eti) ⊗(B⊤eti)
= r(i) ⊗r(i) −s(i) ⊗s(i).
(29)
Here we used the fact that all rows except row ti of B(i) and B(0) agree as the intervention targets a
single node, i.e., (B(i))⊤ek = (B(0))⊤ek for k ̸= ti.
A.3
Warm-up
To provide some intuition how identifiability arises, we first provide the proof for a simpler result
with a much stronger assumption on the set of intervention targets and the type of interventions. But
note importantly that we don’t relax the assumption of non-linearity of f.
Theorem 4. Let f(Z(i))
D=
ef( eZ(i)) for two sets of parameters ((B(i), η(i), ti)i∈I, f) and
(( eB(i), eη(i), eti)i∈I, ef) as in the above model. We assume that there are 2n perfect pairwise dif-
ferent interventions (i.e., Z(i) D
̸= Z(i′) for i ̸= i′), without shifts (i.e., η(i) = eη(i) = 0). Finally, we
assume that there are 2 interventions for each node, and we know the intervention targets of the
interventions, i.e., we assume that ti = eti for all i. Then, Z(i) and ˜Z(i) agree up to scaling, i.e.,
Z(i) = D ˜Z(i) for a diagonal matrix D ∈Diag(n).
Remark 4. Actually this result can be generalized to a setting where the latent SCM is non-parametric
[92].
Proof. We consider a node k and two interventions i1 and i2 which target node k = ti1 = ti2 (by
assumption those interventions exist and i1 and i2 agree for both representations). Since we assumed
that the interventions are perfect, we have in our notation that
r(i1) = λi1ek,
r(i2) = λi2ek
(30)
for two constants λi1 ̸= λi2 > 0 and similarly for er(ij). Indeed, assuming positivity is not a restriction
since ϵ follows a symmetric distribution (this is also contained in the parametrisation (D(i))
1
2 ) and
λi1 ̸= λi2 is necessary to ensure that the interventional distributions differ. Combining (20) from
Lemma 2 with Lemma 3 we obtain the relation
z⊤
r(ij) ⊗r(ij) −s(ij) ⊗s(ij)
z = z⊤(Θ(ij) −Θ(0))z
= h(z)⊤(eΘ(ij) −eΘ(0))h(z) + 2c(ij)
= h(z)⊤
er(ij) ⊗er(ij) −es(ij) ⊗es(ij)
h(z) + 2c(ij).
(31)
for j = 1, 2.
20

Note that since k = ti1 = ti2, we have s(i1) = s(i2) = B⊤ek by definition. Thus, we subtract the
relation in the last display for i1 from the relation for i2 and find using (30),
(λ2
i1 −λ2
i2)z2
k = (z⊤r(i1))2 −(z⊤r(i2))2
= (h(z)⊤er(i1))2 −(h(z)⊤er(i2))2 + 2(c(i1) −c(i2))
= (eλ2
i1 −eλ2
i2)hk(z)2 + 2(c(i1) −c(i2)).
(32)
for all z. Since eλi1 ̸= eλi2 > 0 we can divide and obtain
λ2
i1 −λ2
i2
eλ2
i1 −eλ2
i2
z2
k = hk(z)2 + 2c(i1) −c(i2)
eλ2
i1 −eλ2
i2
.
(33)
Since h is surjective, we have that hk is also surjective which implies that the range of the right hand
side is [2(c(i1) −c(i2))/(eλ2
i1 −eλ2
i2), ∞). The range of the left hand side is depending on the sign
of the prefactor and is either (−∞, 0] or [0, ∞). Since the ranges have to agree we conclude that
c(i1) = c(i2) and
hk(z) = ±
v
u
u
tλ2
i1 −λ2
i2
eλ2
i1 −eλ2
i2
zk.
(34)
Since k was arbitrary, this ends the proof.
A.4
Proof of identifiability up to linear maps
The proof of our main result relies on two essentially geometric lemmas and a slight extension
of Lemma 2 which we discuss now. The first lemma alone is sufficient to handle the case where
interventions change the scaling matrix D, while the second is required to handle interventions that
change B and the third allows us to consider pure shift interventions.
Lemma 4. Let g : Rd →R be a continuous and surjective function, Q a quadratic form on Rd and
α ∈R a constant such that
g(z)2 = z · Qz + α.
(35)
Then Q has rank 1 and g is linear, i.e., g(z) = v · z for some v ∈Rd.
Proof. Assume that Q has the eigendecomposition Q = Pr
i=1 λiviv⊤
i where r is the rank of Q and
λi ̸= 0 and vi has unit norm. Clearly g surjective implies r > 0. Since g is surjective we find that
the range of the left hand side is [0, ∞) and therefore z · Qz ≥−α. This implies that λi ≥0 for
all i and therefore, the range of the right hand side is [α, ∞) which implies α = 0. Now we fix
any c > 0 and consider Ec = {z ∈Rd| z · Qz = c}. It is easy to see that Ec is the product of an
ellipsoid and Rd−r and thus is connected if r ≥2. We find by (35) that g(z) = ±√c for z ∈Ec
and if g(z) ∈{−√c, √c} then z ∈Ec. By continuity of g and since Ec is connected this implies
that g(z) = √c for all z ∈Ec or g(z) = −√c for all z ∈Ec contradicting the surjectivity of g. We
conclude that r = 1, i.e., Q = λ1v1v⊤
1 = ˜v1˜v⊤
1 with ˜v1 = √λ1v1. Thus we find that
g(z)2 = |˜v1 · z|2.
(36)
Since g is continuous, this implies that g(z) = ±˜v1 · z or g(z) = ±|˜v1 · z|. As only the first functions
are surjective we conclude that g(z) = ±˜v1 · z and the claim follows with w = ±˜v1.
To handle shift interventions we need a slightly stronger version of the lemma above which contains
an additional linear term.
Corollary 1. Let g : Rd →R be a continuous and surjective function, Q a quadratic form on Rd,
w ∈Rd a vector, and α ∈R a constant such that
g(z)2 = z · Qz + w · z + α.
(37)
Then Q has rank 1 and g is affine, i.e., g(z) = v · z + c for some v ∈Rd and c ∈R.
21

Proof. Assume as before that Q has the eigendecomposition Q = Pr
i=1 λiviv⊤
i where r is the rank
of Q and λi ̸= 0 and |vi| = 1. First, we show that w ∈⟨v1, . . . , vr⟩where ⟨.⟩denotes the span.
Indeed, suppose that this is not the case. Then we could find z0 such that z0 · w < 0 and z0vi = 0
for 1 ≤i ≤r, and therefore z0 · Qz0 = 0. Plugging z = λz0 for λ →∞in (37) the right-hand
side tends to −∞while the left-hand side is non-negative. This is a contradiction and therefore
w ∈⟨v1, . . . , vr⟩holds. This implies that there is w′ such that Qw′ = w/2 and thus
z · Qz + w · z + α = (z + w′) · Q(z + w′) + α −w′ · Qw′.
(38)
Then we consider eg(ez) = g(ez −w′), eα = α −w′ · Qw′ which satisfy
eg2(ez) = g2(ez −w′)
(39)
= (ez −w′ + w′) · Q(ez −w′ + w′) + α −w′ · Qw′
(40)
= ez · Qez + eα.
(41)
Using Lemma 4 we conclude that Q has rank 1 and eg(ez) = v · z for some v. But then g(z) =
eg(z + w′) = z · v + w′ · v which concludes the proof.
The second lemma is similar, but slightly simpler.
Lemma 5. Assume that there is a quadratic form Q, a vector 0 ̸= w ∈Rd, w′ ∈Rd, α ∈R and a
continuous function g : Rd →R such that
g(z)(w · z) = z · Qz + w′ · z + α.
(42)
Then g(z) = v · z + c for some v ∈Rd, c ∈R, i.e., g is an affine function.
Proof. By rescaling we can assume that w has unit norm. Plugging in z = 0 we find that α = 0.
Denote by Πw the orthogonal projection on w, i.e., Πwv = (w · v)w and by Π⊥
w the projection on the
orthogonal complement of w, i.e., z = Πwz + Π⊥
wz for all z ∈Rd. Then we find for all z
0 = g(Π⊥
wz)w · Π⊥
wz = (Π⊥
wz) · QΠ⊥
wz + (Π⊥
wz) · w′.
(43)
By scaling z we find that both terms on the right hand side vanish and thus for all z
(Π⊥
wz) · QΠ⊥
wz = 0,
(44)
(Π⊥
wz) · w′ = 0.
(45)
This implies (using also the symmetry of Q)
g(z)w · Πwz = g(z)w · z
= z · Qz + w′ · z
= (Πwz + Π⊥
wz) · Q(Πwz + Π⊥
wz) + w′ · (Πwz + Π⊥
wz)
= (Πwz) · QΠwz + 2(Πwz)QΠ⊥
wz + (Π⊥
wz) · QΠ⊥
wz + Πwz · w′
= (Πwz) · (Q(Πwz + 2Π⊥
wz) + w′).
(46)
Now we use Πwz = (w · Πwz)w and find
g(z)w · Πwz = (w · Πwz)w · (Q(Πwz + 2Π⊥
wz) + w′).
(47)
This implies
g(z) = w · Q(Πwz + 2Π⊥
wz) + w · w′
(48)
for all z such that Πwz ̸= 0. By continuity we conclude that this holds for all z, i.e., g is affine.
Again we need a slight generalization of this lemma.
Corollary 2. Assume that there is a quadratic form Q, a vector 0 ̸= w ∈Rd, w′ ∈Rd, α, β ∈R
and a continuous function g : Rd →R such that
g(z)(w · z + β) = z · Qz + w′ · z + α.
(49)
Then g(z) = v · z + c for some v ∈Rd, c ∈R, i.e., g is an affine function.
22

Proof. Define eg(ez) = g(ez −βw|w|−2). Then
eg(ez)(w · ez) = g(ez −βw|w|−2)(w · (ez −βw|w|−2) + β)
= (ez −βw|w|−2) · Q(ez −βw|w|−2) + w′ · (ez −βw|w|−2) + α
= ez · Qez + ew · ez + eα
(50)
for some ew and eα. So, we conclude from Lemma 5 that eg is affine and thus the same is true for g.
We now discuss a small extension of Lemma 2 by showing that we can complete the squares in (19)
which then allows us to obtain a relation that is very similar to (20).
Lemma 6. Assume the general setup as introduced above and also Θ(i) ̸= Θ. Then there is a vector
µ′(i) and a constant c such that
(z −µ(i))⊤Θ(i)(z −µ(i)) −z⊤Θ(0)z = (z −µ′(i))⊤(Θ(i) −Θ(0))(z −µ′(i)) + c.
(51)
Proof. Using the symmetry of the precision matrices and expanding all the terms, we can express the
difference of the left-hand side and the right-hand side of (51) as
(z −µ(i))⊤Θ(i)(z −µ(i)) −z⊤Θ(0)z −(z −µ′(i))⊤(Θ(i) −Θ(0))(z −µ′(i)) −c
= −2z⊤Θ(i)µ(i) + 2z⊤(Θ(i) −Θ(0))µ′(i) + (µ(i))⊤Θ(i)µ(i) −(µ′(i))⊤(Θ(i) −Θ(0))µ′(i) −c.
(52)
Hence, we can set c at the end such the constant terms cancel and all we have to show is that there
exists µ′(i) such that
(Θ(i) −Θ)µ′(i) = Θ(i)µ(i).
(53)
This is clearly not true for arbitrary µ(i) because Θ(i) −Θ has only rank 2. However, such a µ′(i)
does exist for µ(i) = η(i)(B(i))−1eti (see equation (18)). Indeed, we can simplify using (17) and
Lemma 3
(Θ(i) −Θ)µ′(i) = r(i)((r(i))⊤µ′(i)) −s(i)((s(i))⊤µ′(i))
(54)
Θ(i)µ(i) = η(i)(B(i))⊤B(i)(B(i))−1eti = η(i)(B(i))⊤eti = η(i)r(i).
(55)
Now there are two cases. When r(i) and s(i) are collinear (but not identical, by assumption) we can
choose µ′(i)) = λr(i) for a suitable λ. Otherwise, we can pick any vector µ′(i) such that µ′(i)s(i) = 0,
µ′(i)r(i) = η(i). This is always possible for non-collinear vectors (project r(i) on the orthogonal
complement of s(i)).
Let us now first discuss a rough sketch of the proof of our main result. This allows us to present the
main ideas without discussing all the technical details required for the full proof of the result. We
restate the theorem for convenience.
Theorem 3. Assume that X(i) is generated according to a model ((B(i), η(i), ti)i∈¯I, f) such
that the Assumptions 1-4 hold. Then we have identifiability up to linear transformations, i.e.,
if (( eB(i), eη(i), eti), ef) generates the same observed distributions ef( eZ(i))
D= X(i), then their latent
dimensions d agree and there is an invertible linear map T such that
ef = f ◦T −1,
eZ(i) = TZ(i).
(5)
Proof Sketch. For simplicity of the sketch we ignore shift interventions here. We assume that we
have two sets of parameters ((B(i), ti)i∈¯I, f), ( eB(i), eti)i∈¯I), ef) generating the same observations.
We can relabel the interventions i and the variables eZk such that eti = i and such that 1, 2, . . . is a
valid causal order of the graph. The general idea is to show by induction that (h1(z), . . . , hk(z)) is a
linear function of z. Let us sketch how this is achieved. The key ingredients here are Lemma 2 and
Lemma 3. Applying (20) from Lemma 2 in combination with Lemma 3 we obtain
1
2z⊤(Θ(k+1) −Θ(0))z = 1
2h(z)⊤(er(k+1) ⊗er(k+1) −es(k+1) ⊗es(k+1))h(z) + c(k+1).
(56)
23

By our assumption that the variables eZ(i) follow the causal order of the underlying graph, only the
entries er(k+1)
r
for r ≤k + 1 are non-zero and similarly for es(k+1). Therefore, the right-hand side
is a quadratic form of (h1(z), . . . , hk+1(z)). Now we apply our induction hypothesis which states
that (h1(z), . . . , hk(z)) is a linear function of z. Then we find expanding the quadratic form on
the right-hand side that there is a matrix Q, a vector v, a constant δ (which we here assume to be
non-zero) and another quadratic form Q′ such that
1
2z⊤Q′z = 1
2z⊤Qz + (v · z)hk+1(z) + δh2
k+1(z) + c(k+1)
= 1
2z⊤Qz + δ

hk+1(z) + (v · z)
2δ
2
−(v · z)2
4δ
+ c(k+1).
(57)
Now we can apply Corollary 1 to conclude that hk+1(z) is an affine function of z. Adding the shifts
and considering also the case δ = 0 adds several technical difficulties, which we handle in the full
proof.
After all those preparations and presenting the proof sketch, we will finally prove our main theorem
in full generality.
Full proof of Theorem 3. As the proof has to consider different cases and is a bit technical we
structure it in a series of steps.
Labeling assumptions
We can assume without loss of generality (by relabeling variables and
interventions) that the variables eZi are ordered such that their natural order i = 1, 2, . . . is a valid
topological order of the underlying DAG and that intervention 1 ≤i ≤n targets node Zi, i.e., eti = i.
We make no assumption on the ordering of the Zi or targets of ti which are not necessarily assumed
to be pairwise different. Recall that we defined (and showed existence of) h = ef −1f in Lemma 2
(including the fact that their latent dimensions agree).
Induction claim
We now prove by induction that hk(z) is linear for k ≥0. The base case k = 0 is
trivially true. For the induction step, assume that this is the case for j ≤k, i.e., hj(z) = mj · z for
some mj ∈Rd. We define Mk = (m1, . . . , mk)⊤so that we can write concisely
(h1(z), . . . , hk(z))⊤= Mkz.
(58)
Note that since h is surjective Mk has full rank. To prove the induction step, we need to show that
there exists mk+1 ∈Rd such that hk+1(z) = mk+1 · z.
Precision matrix decomposition
For the DAG eG underlying eZi, let PAi denote the indices of
the set of parents of Zi and let PAi = {i} ∪PAi. Recall the notation es(i) = eB⊤ei (using eti = i)
and er(i) = ( eB(i))⊤ei. Now es(i)
j
̸= 0 only holds if j ∈PAi and we ordered the variables such that
PAi ⊂[i]. In particular, es(i)
j
= 0 and er(i)
j
= 0 for j > i (interventions do not create new parents).
By Lemma 3 we have
(eΘ(i) −eΘ(0)) = er(i) ⊗er(i) −es(i) ⊗es(i)
(59)
We find that
(eΘ(k+1) −eΘ(0))rs = er(k+1)
r
er(k+1)
s
−es(k+1)
r
es(k+1)
s
= 0
(60)
if r > k + 1 or s > k + 1. In particular, there exists a symmetric A ∈R(k+1)×(k+1) such that
h(z)⊤(eΘ(i) −eΘ(0))h(z) =
k+1
X
r,s=1
hr(z)Arshs(z).
(61)
Now we decompose A as
A =
A′
b
b⊤
δ

(62)
24

for some A′ ∈Rk×k, b ∈Rk and δ ∈R. Using the induction hypothesis we find
h(z)⊤(˜Θ(k+1) −˜Θ(0))h(z) =
k+1
X
r,s=1
hr(z)Arshs(z)
= Mkz · A′Mkz + 2hk+1(z)b⊤Mkz + δhk+1(z)2.
(63)
We use the notation v:r ∈Rr for the vector (v1, . . . , vr)⊤. Then we can write
η(k+1)er(k+1) · h(z) = η(k+1)er(k+1)
:k
· Mkz + η(k+1)er(k+1)
k+1 hk+1(z)
(64)
Now we have to consider different cases.
Case δ ̸= 0
We assume first that δ ̸= 0. In this case we find using the last two displays
h(z)⊤(˜Θ(k+1) −˜Θ(0))h(z) −2η(k+1)er(k+1)h(z) + c(k+1)
= z · M ⊤
k A′Mkz + δ
 
hk+1(z) + b⊤Mkz −η(k+1)er(k+1)
k+1
δ
!2
−1
δ z · M ⊤
k bb⊤Mkz
+ 2
δ η(k+1)er(k+1)
k+1 b⊤Mkz −1
δ (η(k+1)er(k+1)
k+1 )2 −2η(k+1)er(k+1)
:k
· Mkz + c(k+1).
(65)
Next we set
g(z) = hk+1(z) + δ−1b · Mkz −δ−1η(k+1)er(k+1)
k+1 .
(66)
Looking carefully at (65) we find that there is a symmetric matrix eQ, a vector ew, and a constant ec
such that
h(z)⊤(˜Θ(k+1) −˜Θ(0))h(z) −2η(k+1)er(k+1)h(z) = δg(z)2 + z · eQz + ewz + ec.
(67)
We claim that g is continuous and surjective. Continuity follows directly from continuity of h.
To show that g is surjective we can ignore the constant shift, and we note that δ−1v⊤Mkz =
δ−1 Pk
r=1 vrhr(z) and thus surjectivity of h implies that g is surjective (for every c pick z such that
hk+1(z) = c and hr(z) = 0 for r ≤k).
Using (19) from Lemma 2 we conclude that
δg(z)2 = z⊤(Θ(k+1) −Θ(0))z −2η(k+1)r(k+1)z −z · eQz −ewz −ec
(68)
= z · Qz + w · z + α
(69)
for all z and some symmetric Q ∈Rd×d, w ∈Rd. Corollary 1 then implies that
hk+1(z) + δ−1vMkz + δ−1η(k+1)er(k+1)
k+1
= g(z) = v · z + c
(70)
for some v ∈Rd, c ∈R. Thus hk+1 is an affine function, i.e., hk+1(z) = mk+1z + α. But
0 = E eZk+1 = EZ · mk+1 + α = α
(71)
and therefore hk+1 is linear.
Case δ = 0, b ̸= 0
Now we consider the case that 0 = δ. Using (63), (64), and (19) we find
similarly to above
Mkz · AMkz + 2hk+1(z)b⊤Mkz −2η(k+1)er(k+1)
:k
· Mkz −2η(k+1)er(k+1)
k+1 hk+1(z) + ec(k+1)
= z⊤(Θ(k+1) −Θ(0))z −2η(k+1)r(k+1)z
(72)
Collecting all terms we find that hk+1 satisfies a relation of the type
hk+1(z)(w · z + β) = z · Qz + w′ · z + α
(73)
for suitable Q, β, w, w′, and α. Note in particular that w⊤= 2b⊤Mk ̸= 0 because Mk has full rank.
If w ̸= 0 then Corollary 2 implies that hk(z) is affine and, as argued above, we conclude that hk is
linear.
25

Case δ = 0, b = 0
It remains to address the case b = 0 and δ = 0. Going back to the definition of
b and δ (see (61) and (62)) we find
(b⊤, δ)⊤= er(k+1)
:(k+1)er(k+1)
k+1
−es(k+1)
:(k+1)es(k+1)
k+1
(74)
Now δ = 0 implies es(k+1)
k+1
= er(k+1)
k+1
(because they are both positive). But then we conclude
er(k+1) = es(k+1) from b = 0 and er(k+1)
r
= es(k+1)
r
= 0 for r > k + 1. This implies
eΘ(k+1) −eΘ(0) = er(k+1) ⊗er(k+1) −es(k+1) ⊗es(k+1) = 0
(75)
That is, this implies that intervention k + 1 is a pure shift intervention, i.e., B = B(k+1) and
η(k+1) ̸= 0 (otherwise we do not actually intervene). In this case, we conclude from Lemma 2 and
Lemma 6 (if Θ(k+1) ̸= Θ)
η(k+1)er(k+1)h(z) + c(k+1) = (z −µ′(k+1))⊤(Θ(k+1) −Θ)(z −µ′(k+1)) + c.
(76)
But then η(k+1)er(k+1)∇h(µ′(k+1)) = 0, this implies that ∇h is not invertible which is a contradiction
to h being a diffeomorphism. Thus, we conclude that also Θ(k+1) = Θ, i.e., intervention k + 1 is
also a pure shift intervention for the Z variables. But then we find
er(k+1)
k+1 hk+1(z) + er(k+1)
:k
Mkz = (eη(k+1))−1(η(k+1)r(k+1)z + c(k+1)).
(77)
So we finally conclude that also in this case hk+1 is a linear function, this ends the proof.
B
From Linear Identifiability to Full Identifiability
In this section, we provide the proof of Theorem 1 which is a combination of our linear identifiability
result Theorem 3 proved in the previous section and the main result of [84] which shows identifiability
for linear mixing f. However, we need to carefully address their normalization choices and in addition
consider shifts.
We emphasize that the full identifiability results could also be derived from our proof of Theorem 3
by carefully considering ranks of quadratic forms and showing that we can inductively identify source
nodes. We do not add these arguments as our reasoning is already quite complex and because for
linear mixing functions this is not substantially different from the original proof in [84].
For the convenience of the reader, we will now restate the main results of [84]. For our work it is
convenient to slightly deviate from their conventions, i.e., we define the causal order of a graph by
i ≺j iff i ∈an(j), in particular i →j implies i ≺j.
We rephrase their results using our notation. Let S(G) denote the set of permutations of the vertices
of G that preserve the causal ordering, i.e., all permutations ρ such that ρ(i) < ρ(j) if i ≺j. They
consider linear mixing functions as specified next.
Assumption 5. Assume f(z) = Lz for some matrix L ∈Rd′×d with trivial kernel and pseudoinverse
H = L+ such that the largest absolute value of each row of H is one and the leftmost is positive.
Then the following result holds.
Theorem 5 (Linear setting, Theorems 1 and 2 in [84]). Suppose latent variables Z(i) are generated
following Assumptions 2 and 3 where η(i) = 0 (i.e., no shift) with DAG G and all A(i) are lower
triangular. Suppose in addition that Assumption 4 holds and the mixing function satisfies Assumption 5.
Then we can identify intervention targets and the partial order ≺G of the underlying DAG G up to
permutation of the node labels. If we in addition assume that all interventions are perfect the problem
is identifiable in the following sense. For every representation ( eB(i), eH) such that all eB(i) are lower
triangular and eL( eZ(i))
D= X(i) where eL = eH+ there is a permutation σ ∈S(G) such that
eH = P ⊤
σ H,
eB(i) = P ⊤
σ B(i)Pσ.
(78)
Remark 5. A few remarks are in order.
1. Compared to their statement in Theorem 2 we replaced Pσ by P ⊤
σ because we used the
transposed convention for the permutation matrices.
26

2. Note that their result per se doesn’t mention identifiability up to scaling but that’s because
part of their assumptions involve normalizing f. Since we don’t normalize the linear map f,
we add scaling in the theorem statement.
3. For the case of imperfect interventions, their result talks about identifiability of the transitive
closure G of the graph G, but this is the same as identifiability of the partial order ≺G that
we state here.
4. We assume that interventions are not only acting on the noise but do change the corre-
sponding row of A. Note that this is equivalent to their genericity assumption which states
that
r(i) = (B(i))⊤eti = (Id −A(i))ti·(D(i))
−1
2
titi
(79)
s(i) = (B(0))⊤eti = (Id −A(0))ti·(D(0))
−1
2
titi
(80)
are linearly independent. Indeed, this holds iff A(0)
ti· ̸= A(i)
ti· (by acyclicity A(i)
titi = 0).
Let us provide a brief intuition why this result holds. The key ingredient is the same as in our proof
of linear identifiability above, namely, Lemma 3 which reads
Θ(i) −Θ(0) = r(i) ⊗r(i) −s(i) ⊗s(i).
(81)
This expression has rank 2 if ti is not a source node in the causal graph and the intervention is not
a pure noise intervention because s(i) and r(i) are linearly independent as explained in the remark
above. On the other hand, this expression has rank one for source nodes because s(i) ∝eti in this
case and the same is true for r(i). This allows us to identify interventions on source nodes. Moreover,
Θ(i) −Θ(k) has rank 2 if intervention i and k target different source nodes and rank 1 if they target
the same source node. Thus we can also identify the set of interventions targeting the same source
node. One can complete the proof by an induction argument based on removing source nodes from
the graph and studying the effect on the precision matrix of the latent variables.
We are now ready to prove our main theorems.
Theorem 1. Suppose we are given distributions X(i) generated using a model ((B(i), η(i), ti)i∈¯I, f)
such that Assumptions 1-4 hold and such that all interventions i are perfect. Then the model is
identifiable up to permutation and scaling, i.e., for any model (( eB(i), eη(i), eti)i∈¯I, ef) that generates
the same data, the latent dimension d agrees and there is a permutation ω ∈Sd (and associated
permutation matrix Pω) and an invertible pointwise scaling matrix Λ ∈Diag(d) such that
eti = ω(ti),
eB(i) = P ⊤
ω B(i)Λ−1Pω,
ef = f ◦Λ−1Pω,
eη(i) = η(i).
(3)
This in particular implies that
eZ(i) D= P ⊤
ω ΛZ(i)
(4)
and we can identify the causal graph G up to permutation of the labels.
Proof of Theorem 1. Suppose
there
are
two
representations
((B(i), η(i), ti)i∈I, f)
and
(( eB(i), eη(i), eti)i∈I, ef) of the distributions X(i), i ≥0.
As explained in the proof of Theo-
rem 3 we can assume by relabeling the nodes that the matrices eB(i) are lower triangular and
the corresponding DAG eG has the property that for every edge i →j we have i < j. We now
apply Theorem 3 and find that there is an invertible linear map T such that ef = f ◦T and
T −1Z(i) D= eZ(i). One minor difference to the setting in [84] is that we in addition consider shift
interventions. They are simpler to analyze than a change of variance, but we can avoid additional
arguments with the following trick. If Σ(i) = Σ(0) for some i we replace Z(i) = N(µ(i), Σ(i))
by Z′(i) = N(0, Σ(i) + µ(i)(µ(i))⊤), otherwise we set Z′(i) = N(0, Σ(i)) and similarly for eZ.
Then T −1(Z(i))
D= eZ(i) implies T −1(Z′(i))
D= eZ′(i). Using (13) and (17) it can be shown that this
distribution corresponds to a change of variance of the noise, i.e., a change of D(i)
titi. This can
also be seen directly from the relation Z′(i) D= (B(i))−1(ϵ + η(i)ϵ′et(i)), where ϵ′ ∼N(0, 1) and
27

independent of ϵ. We can now apply Theorem 5 to the primed distributions. We first find a unique
invertible Λ ∈Diag(d) such that ΛT has largest absolute value entry 1 in every row (and in case of
ties the leftmost entry is 1). We can also find a permutation ρ ∈Sd such that ¯B(i) = PρB(i)Λ−1P ⊤
ρ
is lower triangular for all i (note the distinction between ¯B(i) and eB(i)). Indeed, this is possible since
the underlying DAG G is acyclic by assumption, interventions do not add edges, and Λ is diagonal.
Then we find
(T −1Λ−1P ⊤
ρ )( ¯B(i))−1ϵ
D= (T −1Λ−1P ⊤
ρ )( ¯B(i))−1Pρϵ
(82)
= T −1(B(i))−1ϵ
(83)
D= ( eB(i))−1ϵ.
(84)
Here the last step follows from the fact that T −1Z(i)
D= eZ(i) which implies that the centered
distributions agree.
Now we can apply Theorem 5 (with B replaced by eB and identity mixing) and find that for the
alternative representation in terms of ¯B
PρΛT = P ⊤
σ Id,
¯B(i) = P ⊤
σ eB(i)Pσ
(85)
for some ρ ∈S( eG) because by assumption ¯B(i) and eB are lower triangular. This implies that
T = Λ−1P ⊤
ρ P ⊤
σ ,
(86)
B(i) = P ⊤
ρ ¯B(i)PρΛ = P ⊤
ρ P ⊤
σ eB(i)PσPρΛ.
(87)
To summarize, there is a permutation ω = (ρ ◦σ)−1 (note that PσPρ = Pρ◦σ) and a diagonal matrix
Λ such that
B(i) = Pω eB(i)P ⊤
ω Λ,
T = Λ−1Pω,
Z(i) = T eZ(i) = Λ−1Pω eZ(i),
(88)
We also find the relation ω(ti) = eti from here because Pωei = eω−1(i). Equating the means of
T −1Z(i) and eZ(i) we find
eη(i)( eB(i))−1eeti = η(i)T −1(B(i))−1eti.
(89)
Multiplying by eB(i) we find
eη(i)eeti = η(i) eB(i)T −1(B(i))−1eti = η(i)P ⊤
ω eti
(90)
and we conclude that eη(i) = η(i).
Theorem 2.
Suppose we are given the distributions X(i)
generated using a model
((B(i), η(i), ti)i∈¯I, f) with causal graph G such that the Assumptions 1-4 hold and none of the
interventions is a pure noise intervention. Then for any other model (( eB(i), eη(i), eti)i∈¯I, ef) with
causal graph eG generating the same observations the latent dimension d agrees and there is a
permutation ω ∈Sd such that eti = ω(ti) and i ≺e
G j iff ω(i) ≺G ω(j), i.e., ≺G can be identified up
to a permutation of the labels.
Proof of Theorem 2. Suppose
there
are
two
representations
((B(i), η(i), ti)i∈I, f)
and
(( eB(i), eη(i), eti)i∈I, ef) of the observational distribution.
As shown in Theorem 3 there is a
linear map such that T −1Z(i) = eZ(i). Then the same is true for the transformed variables as
explained above. Viewing eZ(i) as new observations obtained through a linear T mixing we conclude
from Theorem 5 that ≺G and the intervention targets are identifiable up to relabeling.
C
A comparison to related works
In this section, we will discuss the relation of our results to the most relevant prior works and put
them into context.
28

[84]
This work considers linear mixing functions f, and we directly generalize their work to non-
linear f. In particular, their techniques are linear-algebraic and do not generalize to non-linear f (see
more technical details in proof intuition in Section 4); we handle this using a mix of statistical and
geometric techniques. Notably, their finding that the linearly mixed latent variables are recoverable
after observing one intervention per latent node is closely related to an earlier result in domain
generalization, which showed that a number of environments equal to the number of “non-causal”
latent dimensions can ensure recovery of the remaining latents [71], and later work showing that this
requirement can be reduced further under linear mixing with additional modeling structure [11, 72].
This connection is not coincidental—that analysis was for invariant prediction with latent variables,
which was originally presented as a method of causal inference using distributions resulting from
unique interventions [62]. Later this approach was applied for the purpose of robust prediction in
nonlinear settings [25], but the identifiability of the latent causal structure under general nonlinear
mixing via interventional distributions remained an open question until this work.
[88]
This work also considers linear mixing f, but allow for non-linearity in the latent variables.
They use score functions to learn the model and they raise as an open question the setting of non-linear
mixing, which we study in this work. While the models are not directly comparable, our model is
more akin to real-world settings since it’s not likely that high-level latent variables are linearly related
to the observational distribution, e.g. pixels in an image are not linearly related to high-level concepts.
Moreover, Gaussian priors and deep neural networks are extensively used in practice, and our theory
as well as experimental methodology apply to them.
[48]
While they do not discuss interventions, this work considers the setting of multi-environment
CRL with Gaussian priors and their results can be applied to interventional learning. When applied
to interventional data as in our setting (as also shown in [84, Appendix D]), they require additional
restrictive assumptions such as d = d′, a bijective mixing f and e ≤d where e is the number of
edges in the underlying causal graph whereas we make no such restrictions on d, f and e (therefore,
the maximum value of e can be as large as ≈d2/2 in our setting). And when these assumptions are
satisfied, they require 2d interventions whereas we show that d interventions are sufficient as well as
necessary.
[2]
This work also considers the setup of interventional causal representation learning. However,
their setting is different in various ways and we now outline the key differences.
1. Their main results assume that the mixing f is an injective polynomial of finite degree p
(see Assumption 2 in their paper). The injectivity requires their coefficient matrix to be
full rank, which in turn implies d′ ≥Pp
r=0
 r+d−1
d−1

=
 p+d
d

(see the discussion below
Assumption 2 in their work). This means that we cannot set the degree of the decoder
polynomial to be arbitrarily large for fixed output dimension d′ (which is required for
universal approximation). In contrast, we only require d′ ≥d. For a general discussion of
the relation between approximability and identifiability we refer to Appendix E.
2. All their results for general nonlinear mixing functions rely on deterministic do-interventions
which is a type of hard intervention that assigns a constant value to the target. However, we
focus on randomized soft interventions (and also allow shift interventions which have found
applications [73, 58]), which as also pointed out by [84, 87] is less restricted. Moreover,
they require multiple interventions for each latent variable (as they use an ϵ-net argument)
to show approximate identifiability, while the structural assumptions on the causal model
allow us to show full identifiability with just one intervention for each latent variable. Thus,
their setting is not directly comparable to ours and neither is strictly more general than the
other. In particular, their proof techniques and experimental methodology don’t translate to
our setting, and we use completely different ideas for our proofs and experiments.
D
Counterexamples and Discussions
In this section, we first present several counterexamples to relaxed versions of our main results in
Section D.1 and then provide the missing proofs in Section D.2.
29

D.1
Main counterexamples
In this section, we will discuss the optimality and limitations of various assumptions of our main
results. In particular, we consider the number of interventions, the distribution of the latent variables,
and the types of interventions separately.
Number of interventions
Our main results all require d interventions, i.e., one intervention per
node. This is necessary even in the simpler setting of linear f as was shown in [84, Proposition 4],
directly implying it is also necessary for the more general class of non-linear f that we handle here.
Therefore, the number of interventions in our main theorems 1, 2 is both necessary and sufficient.
Going a step further, it is natural to ask whether Theorem 3 remains true for less than d interventions.
However, as we show next, d −2 interventions are not sufficient.
Fact 1. Suppose we are given distributions X(i) generated by ((B(i), η(i), ti)i∈¯I, f) satisfying
Assumption 1-3. If the number of interventions satisfies |I| ≤d −2 it is not possible to identify f up
to linear maps. Consider, e.g., d = 2 and Z = N(0, Id) and no interventions. Then h(Z)
D= Z for
(nonlinear) radius dependent rotations as defined in [29, 9].
Let us next consider the case with a total of d environments which corresponds to d −1 interventional
distributions plus one observational distribution. We can provide a non-identifiability result for a
general class of heterogeneous latent variable models that satisfy an algebraic property.
Lemma 7. Assume that d = 2 and Z(0) ∼N(0, Σ(0)), Z(1) ∼N(0, Σ(1)) and Σ(1) ≻Σ(0) or
Σ(1) ≺Σ(0) in Löewner order (i.e., the difference is positive definite). Then there is a non-linear map
h such that h(Z(i))
D= Z(i) for i = 0, 1.
We provide a proof of this lemma in Appendix D.2. Note that this non-identifiability lemma requires
the assumption on Σ(i). In Lemma 3 in Appendix A we have seen that for the interventions considered
in this work the relation Σ(i) > Σ(0) or Σ(i) < Σ(0) generally does not hold. This is some weak
indication that for Theorem 3, d −1 interventions might be sufficient. We leave this for future work.
As an additional note, in the special case when f is a permutation matrix, we are in the setting of
traditional causal discovery and here, d −1 interventions are necessary and sufficient [10, 18, 84].
Type of intervention
Even when we restrict ourselves to only linear mixing functions as considered
as in [84], we need perfect interventions in Theorem 1. Otherwise, only the weaker identifiability
guarantees in Theorem 2 can be obtained. This is shown in [84]for linear f. Therefore, in the more
general setting of nonlinear f, we cannot hope to obtain Theorem 1 with imperfect interventions,
showing that this assumption is needed.
Next, we clarify that for the identifiability result in Theorem 2 , the condition that the interventions
are not pure noise interventions is also necessary.
Fact 2. Recall that we defined B(i) = (D(i))−1
2 (Id −A(i)). Suppose X(i) is generated by
((B(i), η(i), ti)i∈¯I, f) satisfying Assumptions 1-4 where all interventions are pure noise interventions.
By definition, this implies A(i) = A for all i. Consider any matrix eA such that the corresponding
DAG is acyclic. Then (( eB(i), η(i), ti)i∈¯I, ef) with eB(i) = (D(i))−1
2 (Id −eA) for all i and
ef = f ◦(B−1 eB) = f ◦(Id −A)−1(D(0))−1
2 (D(0))
1
2 (Id −eA) = f ◦(Id −A)−1(Id −eA) (91)
generates the same distributions X(i). Indeed,
ef( eZ(i)) = f ◦(B−1 eB)( eB(i))−1(ϵ + η(i)eti)
= f((Id −A)−1(Id −eA)(Id −eA)−1(D(i))
1
2 (ϵ + η(i)eti))
= f((Id −A)−1(D(i))
1
2 (ϵ + η(i)eti))
= f((B(i))−1(ϵ + η(i)eti))
= f(Z(i)).
(92)
This implies that any underlying causal graph could generate the observations. In other words, for
pure noise interventions we can identify the scale of the noise variables, but we obtain no intervention
30

about the causal variables and structure (exactly because the interventions do not target the actual
causal variables but the noise variables).
Finally, we remark that in contrast to the case with linear mixing functions, we need to exclude
non-stochastic hard interventions for linear identifiability.
Lemma 8. Consider d = 2 and Z(0) = N(0, Id) and the non-stochastic hard interventions do(Z1 =
0) and do(Z2 = 0) resulting in the degenerate Gaussian distributions Z(1) = N(0, e2 ⊗e2) and
Z(2) = N(0, e1 ⊗e1). Then there is a nonlinear h such that h(Z(i))
D= Z(i) for all i.
Intuitively, to show this, we choose h to be identity close to the supports {Z1 = 0} and {Z2 = 0}
but some nonlinear measure preserving deformation in the four quadrants. The full argument can
be found in Appendix D.2. A similar counterexample was discussed in [2]. However, here we in
addition constrain the distribution of the latent variables, which adds a bit of complexity.
Distribution of the Noise Variables
Our key additional assumption compared to the setting
considered in [84, 2, 88] is that we assume that the latent variables are Gaussian which allows us to
put only very mild assumptions on the mixing function f. However, we do this in a manner so that
we still preserve universal representation guarantees.
[88] consider arbitrary noise distributions but restrict to linear mixing functions. We believe that
our result can be generalized to more general distributions of the noise variables, e.g., exponential
families, but those generalizations will require different proof strategies, which we leave for future
work. However, the result will not be true for arbitrary noise distributions, as the following lemma
shows.
Lemma 9. Consider Z(0) = (ϵ1, ϵ2)⊤, Z(1) = (ϵ′
1, ϵ2), and Z(2) = (ϵ1, ϵ′
2) where ϵ1, ϵ2 ∼
U([−1, 1]) and ϵ′
1, ϵ′
2 ∼U([−3, 3]). Also define eZ(0) = (ϵ1, ϵ1 + ϵ2), eZ(1) = (ϵ′
1, ϵ′
1 + ϵ2), and
eZ(2) = (ϵ1, ϵ′
2). Then there is h such that
h(Z(i))
D= eZ(i).
(93)
A proof of this lemma can be found in Appendix D.2. The result shows that even with d perfect
interventions and linear SCMs it is not possible to identify the underlying DAG (empty graph for
Z and eZ1 →eZ2) and we also cannot identify f up to linear maps. While our example relies on
distributions with bounded support, we conjecture that full support is not sufficient for identifiability.
Structural Assumptions
A final assumption used in our results is the restriction to linear SCMs.
This is used for most works on causal representation learning (see Section 2), even when they restrict
to linear mixing functions. Although this may potentially be a nontrivial restriction, the advantage
is that the simplicity of the latent space will enable us to meaningfully probe the latent variables,
intervene and reason about them. Nevertheless, it’s an interesting direction to study to what generality
this can be relaxed. Indeed, even for the arguably simpler problem of causal structure learning (i.e.,
when Z(i) is observed) there are many open questions when moving beyond additive noise models.
We leave this for future work.
D.2
Technical constructions
In this section, we provide the proofs for the counterexamples in Section D.1. For the first two
counterexamples we construct functions h such that h(Z(i))
D= Z(i) by setting h = Φt where Φt is
defined as the flow of a suitable vectorfield X, i.e.,
∂tΦt(z) = X(Φt(z)).
(94)
This is a common technique in differential geometry and was introduced to construct counterexamples
to identifiability ICA with volume preserving functions in [9, 8]. To find a suitable vectorfield X we
rely on the fact that the density pt of Pt = (Φt)∗P satisfies the continuity equation
∂tpt(z) + Div(pt(z)X(z)) = 0.
(95)
In particular Pt = P holds iff
Div(pt(z)X(z)) = 0.
(96)
31

Therefore it is sufficient to find a vectorfield X such that Div(p(i)X) = 0 for all environments i to
construct a suitable h = Φ1.
Lemma 7. Assume that d = 2 and Z(0) ∼N(0, Σ(0)), Z(1) ∼N(0, Σ(1)) and Σ(1) ≻Σ(0) or
Σ(1) ≺Σ(0) in Löewner order (i.e., the difference is positive definite). Then there is a non-linear map
h such that h(Z(i))
D= Z(i) for i = 0, 1.
Proof of Lemma 7. Let us first discuss the high-level idea of the proof. The general strategy is to
construct a vectorfield X whose flow preserves Z(i) for i = 0, 1. This holds if and only if the
densities p0, p1 of Z(0), Z(1) satisfy
Div(p0X) = Div(p1X) = 0
(97)
Using Div(fX) = ∇f · X + f Div X we find
X · ∇(ln p0(z) −ln p1(z)) = X · ∇p0(z)
p0(z)
−X · ∇p1(z)
p1(z)
−p0(z) Div X
p0(z)
+ p1(z) Div X
p1(z)
= 0.
(98)
We conclude that any X satisfying (97) must be orthogonal to ∇(ln p0(z)−ln p1(z)) and thus parallel
to the level lines of ln p0(z) −ln p1(z). This already fixes the direction of X and the magnitude can
be inferred from (97). To simplify the calculations we can first linearly transform the data so that the
directions of X, i.e., equivalently the level lines of ln p0(z)−ln p1(z) have a simple form. We assume
that Σ0 ≺Σ1. Clearly, it is sufficient to show the result for any linear transformation of the Gaussian
distributions Z(i). Note that generally for G ∼N(µ, Σ) we have AG ∼N(Aµ, AΣA⊤) Applying
Σ
−1
2
0
we can reduce the problem to Id ≺Σ′
1 = Σ
−1
2
0
Σ1Σ
−1
2
0
. Diagonalizing Σ′
1 by U ∈SO(d) it is
sufficient to consider Id ≺Λ = UΣ′
1U ⊤where Λ = Diag(λ1, λ2). Finally, we can rescale z1 and
z2 such that the resulting covariance matrices Λ0 = Diag(λ0
1, λ0
2), Λ1 = Diag(λ1
1, λ1
2) satisfy
1
λ1
1
−1
λ0
1
= 1
λ1
2
−1
λ0
2
= 1.
(99)
Thus, it is sufficient to show the claim for such covariances Λ0, Λ1 and the result for arbitrary
covariances follows by applying suitable linear transformation.
For such covariances we find for some constant c
ln(p0(z)) −ln(p1(z)) = −z2
1
2λ0
1
−z2
2
2λ0
2
+ z2
1
2λ1
1
+ z2
2
2λ1
2
+ c = z2
1 + z2
2 + c.
(100)
The level lines are circles. Thus, we consider the vector field
X0(z) =

−z2
z1

.
(101)
We see directly that Div X0 = 0. We consider the Ansatz X(z) = f(|z|)X0(z)/p0(z). We now
fix the radial function f : R+ →R such that it has compact support away from 0. We find using
Div X0 = 0
Div(X(z)p0(z)) = Div(X(z)p0(z))
(102)
= X0(z) · (∇|z|)f ′(z)
(103)
=

−z2
z1

·
z
|z|2 f ′(z)
(104)
= 0.
(105)
Note, that close to 0 where |z| is not differentiable the vector field vanishes by our construction of f.
We find similarly using (100)
Div(X(z)p1(z)) = X0(z) · ∇

f(|z|)p1(z)
p0(z)

(106)
= X0(z) · ∇

f(|z|)e−|z|2−c
(107)
32

Z(1)
Z(0)
Z(2)
R2
R1
eZ(0)
eZ(1)
eZ(2)
Q2
Q1
h
Figure 4: Sketch of the setting in Lemma 9. The map h agrees with (z1, z2) →(z1, z1 + z2) on the
red rectangle and maps Ri to Qi such that the uniform measure is preserved.
= X0(z) · ∇˜f(|z|)
(108)
= 0.
(109)
Finally we remark that the flow Φt of the vectorfield X generates a family of functions h as desired,
i.e., for all t we have (Φt)∗Z(i) = Z(i) and Φt is clearly nonlinear as it is the identity close to 0 but
not globally.
The proof of the next lemma is similar but simpler.
Lemma 8. Consider d = 2 and Z(0) = N(0, Id) and the non-stochastic hard interventions do(Z1 =
0) and do(Z2 = 0) resulting in the degenerate Gaussian distributions Z(1) = N(0, e2 ⊗e2) and
Z(2) = N(0, e1 ⊗e1). Then there is a nonlinear h such that h(Z(i))
D= Z(i) for all i.
Proof of Lemma 8. Pick any smooth divergence free vectorfield X0 with compact support in [ϵ, ∞)2
for some ϵ > 0. Then the vector field X = X0/p0 satisfies
Div Xp0 = Div X0 = 0.
(110)
Thus the flow Φt preserves the distribution of Z(0). But it also preserved the interventional dis-
tributions Z(i) for i = 1, 2 because X(z) = 0 and thus Φt(z) = z for z close to the supports of
Z(i).
Finally we prove Lemma 9 that shows that we cannot completely drop the assumption on the
distribution of the noise variables ϵ.
Lemma 9. Consider Z(0) = (ϵ1, ϵ2)⊤, Z(1) = (ϵ′
1, ϵ2), and Z(2) = (ϵ1, ϵ′
2) where ϵ1, ϵ2 ∼
U([−1, 1]) and ϵ′
1, ϵ′
2 ∼U([−3, 3]). Also define eZ(0) = (ϵ1, ϵ1 + ϵ2), eZ(1) = (ϵ′
1, ϵ′
1 + ϵ2), and
eZ(2) = (ϵ1, ϵ′
2). Then there is h such that
h(Z(i))
D= eZ(i).
(93)
Proof of Lemma 9. A sketch of the setting can be found in Figure 4. We define h0(z) = (z1, z1 +z2).
If h(z) = h0(z) for −1 ≤z2 ≤1 we find h(Z(i))
D= eZ(i) for i = 0 and i = 1. It remains to modify
h0 such that h preserves the uniform measure on [−1, 1] × [−3, 3]. We do this in two steps, first we
33

construct a modification eh of the map h0 such that [−1, 1] × [−3, 3] is mapped bijectively to itself
but eh = h0 for |z2| ≤1 (so that eh(Z(i))
D= eZ(i) for i = 0 and i = 1 holds) and then we apply a
general result to make the map measure preserving. We start with the first step. Let ψ : R →[0, 1]
be differentiable such that ψ(t) = 1 for −1 ≤z ≤1 and ψ(t) = 0 for −5/2 ≤z ≤5/2 with
|ψ′(t)| < 1. Then
˜h(z) = ψ(z2)z + (1 −ψ(z2))h0(z) =

z1
z2 + ψ(z2)z1

(111)
is injective on [−1, 1] × [−3, 3] (note that the second coordinate is increasing in z2) and maps
[−1, 1] × [−3, 3] bijectively to itself and agrees with h0 for |z2| ≤1. However, it is not necessarily
volume preserving. But applying Moser’s theorem (see [56]) to the image of the rectangles R1 =
[−1, 1]×[−3, −1] and R2 = [−1, 1]×[1, 3] which are the quadrilaterals Q1 = ˜h(R1) with endpoints
(−1, −3), (1, −3), (1, 0), (−1, −2) and Q2 = ˜h(R2) with endpoints (1, 3), (−1, 3), (−1, 0), (1, 2)
with the same size as R1 and R2 we infer that there is φ supported in Q1 ∪Q2 such that h = φ ◦˜h
satisfies h(Z(2))
D= eZ(2) D= U([−1, 1] × [−3, 3]).
E
Identifiability and Approximation
In this section, we explain that approximation properties of function classes cannot be leveraged to
obtain stronger identifiability results. The general setup is that we have two function classes G ⊂F
and we assume that G is dense in F (in the topology of uniform convergence on Ω), i.e., for f ∈F
and any ϵ > 0 there is g ∈G such that
sup
z∈Ω
|f(z) −g(z)| ≤ϵ.
(112)
We now investigate the relationship of identifiability results for mixings in either G or in F.
While this point is completely general, we will discuss this for concreteness in the context of
nonlinear ICA and considering polynomial mixing functions for G. We also assume that F are
all diffeomorphisms (onto their image). Suppose we have latents Z ∼U([−1, 1]d) and observe a
mixture X = h(Z). We use the shorthand Ω= [−1, 1]d from now on. Let us suppose that h ∈G.
Then the following result holds.
Lemma 10. Suppose eh(Z)
D= X
D= h(Z) where Z ∼U(Ω), h,eh ∈G and h satisfies an injectivity
condition as defined in Assumption 2 in [2]. Then h and eh agree up to permutation of the variables.
Proof. This is a consequence of Theorem 4 in [2]. Essentially, they show in Theorem 1 that h and ˜h
agree up to a linear map and then use independence of supports to conclude (in our simpler setting
here, one could also resort to the identifiability of linear ICA).
The injectivity condition is a technical condition that ensures that the embedding h is sufficiently
diverse, but this is not essential for the discussion here. Let us nevertheless mention here that it is not
clear whether this condition is sufficiently loose to ensure identifiability in a dense subspace of F
because it requires an output dimension depending on the degree of the polynomials.
We now investigate the implications for F. It is well known that ICA in general nonlinear function
is not identifiable, i.e., for any f ∈F there is a ef ∈F such that f(Z)
D= ef(Z). To find such an
ef one use, e.g., the Darmois construction, radius dependent rotations or, more generally, measure
preserving transformations m such that m(Z)
D= Z. Then we have the following trivial lemma.
Lemma 11. For every ϵ > 0 there are functions g, eg ∈G such that
sup
Ω
|g −f| < ϵ,
sup
Ω
|eg −ef| < ϵ
(113)
and then the Wasserstein distance satisfies W2(f(Z), g(Z)) < ϵ, W2( ef(Z), eg(Z)) < ϵ.
Remark 6. For the definition and a discussion of the Wasserstein metric we refer to [90]. Alternatively
we could add a small amount of noise, i.e. X = f(Z) + ν and then consider the total variation
distance.
34

Proof. The first part follows directly from the Stone-Weierstrass Theorem which shows that polyno-
mials are dense in the continuous function on compact domains. The second part follows from the
coupling Z →(f(Z), g(Z)) and (113).
The main message of this Lemma is that whenever there are spurious solutions in the larger function
class F we can equally well approximate the ground truth mixing f and the spurious solution ef by
functions in G. In this sense, the identifiability of ICA in G does not provide any guidance to resolve
the ambiguity of ICA in F. So identifiability results in G are not sufficient when there is no reason to
believe that the ground truth mixing is exactly in G.
Actually, one could view the approximation capacity of G also as a slight sign of warning as we will
explain now. Suppose the ground truth mixing satisfies g ∈G. Since ICA in F is not identifiable we
can find ef ∈F which can be arbitrarily different from g but such that g(Z) = ef(Z). Then we can
approximate ef by eg with arbitrarily small error. Thus, we find almost spurious solutions, i.e., eg ∈G
such that W2(eg(Z), g(Z)) < ϵ for an arbitrary ϵ > 0 but eg and g correspond to very different data
representations, in this sense the identifiability result is not robust.
When only considering the smaller space G this problem can be resolved by considering norms or
seminorms on G, for polynomials a natural choice is the degree of the polynomial. Indeed, suppose
that we observe data generated using a mixing g ∈G. Then there might be spurious solutions ef ∈F
which can be approximated by eg ∈G but this approximation will generically have a larger norm (or
degree for polynomials) than g. Therefore, the minimum description length principle [69] favors g
over eg.
This reasoning can only be extended to the larger class F if the ground truth mixing f (for the
relevant applications) can be better approximated (i.e., with smaller norm) by functions in G than
all spurious solutions ef. This is hard to verify in practice and difficult to formalize theoretically.
Nevertheless, this motivates to look for identifiability results for function classes that are known to
be useful representation learners and used in practice, in particular neural nets. We emphasize that
alternatively one can also directly consider norms on the larger space F penalizing, e.g., derivative
norms, to perform model selection.
F
Additional details on experimental methodology
In this appendix, we give more details about our experimental methodology. First, we derive the
log-odds and prove Lemma 1 in Appendix F.1. Then, we use it to design our model and theoretically
justify our contrastive approach in Appendix F.2, by connecting it to the Bayes optimal classifier. Then,
we describe the NOTEARS regularizer in Appendix F.3, describe the limitations of the contrastive
approach in Appendix G.1 and finally, in Appendix F.4, we describe the ingredients needed for an
approach via Variational Autoencoders, the difficulties involved and how to potentially bypass them.
F.1
Proof of Lemma 1
Lemma 1. When we have perfect interventions, the log-odds of a sample x coming from X(i) over
coming from X(0) is given by
ln p(i)
X (x) −ln p(0)
X (x) = ci −1
2λ2
i (((f −1(x))ti)2 + η(i)λi · (f −1(x))ti) + 1
2⟨f −1(x), s(i)⟩2 (6)
for a constant ci independent of x.
Proof. We will write the log likelihood of X(i) = f(Z(i)) using standard change of variables. As
we elaborate in Appendix A.1, we have
Z(i) = (B(i))−1ϵ + µ(i) with B(i) = (D(i))−1/2(I −A(i)), µ(i) = η(i)(B(i))−1eti
(114)
Also, denote by Θ(i) = (B(i))⊤B(i) the precision matrix of Z(i) (see Appendix A.1 for full deriva-
tion). By change of variables,
ln p(i)
X (x) = ln |Jf −1| + ln p(i)
Z (f −1(x))
35

= ln |Jf −1| −n
2 ln(2π) −1
2 ln |Σ(i)| −1
2(f −1(x) −µ(i))T Θ(i)(f −1(x) −µ(i))
where Jf −1 denotes the Jacobian matrix of partial derivatives. Let s(1), s(2), . . . , s(d) denote the
rows of B(0). We then compute the log-odds with respect to X(0), the base distribution without
interventions (and where µ(0) = 0) to get
ln p(i)
X (x) −ln p(0)
X (x)
=

−1
2 ln |Σ(i)|
|Σ(0)| −1
2(µ(i))⊤Θ(i)µ(i)

−1
2(f −1(x))⊤(Θ(i) −Θ(0))(f −1(x)) + (f −1(x))⊤Θ(i)µ(i)
= ci −1
2(f −1(x))⊤(r(i) ⊗r(i) −s(i) ⊗s(i))(f −1(x)) + η(i) · (f −1(x))⊤(B(i))⊤eti
= ci −1
2(f −1(x))⊤(λ2
i etie⊤
ti −(s(i))(s(i))⊤)(f −1(x)) + η(i)λi · (f −1(x))⊤eti
= ci −1
2(f −1(x))⊤(λ2
i etie⊤
ti −(s(i))(s(i))⊤)(f −1(x)) + η(i)λi · (f −1(x))ti
= ci −1
2λ2
i (((f −1(x))ti)2 + η(i)λi · (f −1(x))ti) + 1
2⟨f −1(x), s(i)⟩2
for a constant ci independent of z. For the second equality, we used Lemma 3.
F.2
A theoretical justification for the log-odds model
In this section, we provide more details on our precise modeling choices for the contrastive approach,
and show theoretically why it encourages the model to learn the true underlying parameters.
Recall that, motivated by Lemma 1, we model the log-odds as
gi(x, αi, βi, γi, w(i), θ) = αi −βih2
ti(x, θ) + γihti(x, θ) + ⟨h(x, θ), w(i)⟩2
(115)
where h(·, θ) denotes a neural net parametrized by θ, parameters w(i) are the rows of a matrix W and
αi, βi, γi are learnable parameters. Moreover, we have g0(x) = 0 as we compute the log-odds with
respect to X(0). Therefore, the cross entropy loss given by
L(i)
CE = −Ej∼U({0,i})Ex∼X(j) ln
 e1j=igi(x)
egi(x) + 1

.
(116)
Let C(x) ∈{0, 1} denote the label of the datapoint x indicating whether it was an observational
datapoint or an interventional datapoint respectively. By using the cross entropy loss, we are treating
the model outputs as logits, therefore we can write down the posterior probability distribution using a
logistic regression model as
Pr[C(x) = 1|x] =
exp(gi(x, αi, βi, γi, w(i), θ))
1 + exp(gi(x, αi, βi, γi, w(i), θ))
(117)
Moreover, by using a sufficiently wide or deep neural network with universal approximation capacity
and sufficiently many samples, our model will learn the Bayes optimal classifier, as given by Lemma 1.
Therefore, we can equate the probabilities to arrive at
exp(gi(x, αi, βi, γi, w(i), θ))
1 + exp(gi(x, αi, βi, γi, w(i), θ)) =
exp(ln p(i)
X (x) −ln p(0)
X (x))
1 + exp(ln p(i)
X (x) −ln p(0)
X (x))
(118)
implying
αi −βih2
ti(x, θ) + γihti(x, θ) + ⟨h(x, θ), w(i)⟩2
(119)
= ci −1
2λ2
i (((f −1(x))ti)2 + η(i)λi · (f −1(x))ti) + 1
2⟨f −1(x), s(i)⟩2
(120)
This suggests that in optimal settings, our model will learn (up to scaling)
αi = ci,
βi = 1
2λ2
i ,
γi = η(i)λi,
w(i) = s(i),
h(x, θ) = f −1(x)
(121)
thereby inverting the nonlinearity and learning the underlying parameters.
36

F.3
NOTEARS [102]
In our algorithm, using the parameters w(i) as rows, we learn the matrix W, which we saw in optimal
settings will be B = D−1/2(Id −A). Note that the off-diagonal entries of B form a DAG. Therefore,
the graph encoded by W0, which is defined to be W with the main diagonal zeroed out, must also be
a DAG.
However, in our algorithm, we don’t explicitly enforce this acyclicity. There are two ways to get
around this. One way is to assume a default causal ordering on the Zis, which is feasible as we
don’t directly observe Z. Then, we can simply enforce W0 to be triangular and train via standard
gradient descent and W0 is guaranteed to be a DAG. However, this approach doesn’t work because
the interventional datasets are given in arbitrary order and so we are not able to match the datasets to
the vertices in the correct order. So this merely defers the issue.
Instead, the other approach that we take in this work is to regularize the learnt W0 to model a directed
acyclic graph. Learning an underlying causal graph from data is a decades old problem that has been
widely studied in the causal inference literature, see e.g. [12, 102, 64, 80, 4] and references therein.
In particular, the work [102] proposed an analytic expression to measure the DAGness of the causal
graph, thereby making the problem continuous and more efficient to optimize over.
Lemma 12 ([102]). A matrix W ∈Rd×d is a DAG if and only if
R := tr exp(W ◦W) −d = 0
(122)
where ◦is the matrix Hadamard product and exp is the matrix exponential. Moreover, R has gradient
∇R = exp(W ◦W)⊤◦2W
(123)
Therefore, we add RNOT EARS(W) = tr exp(W0 ◦W0) −d as a regularization to our loss function.
As in prior works, we could also consider the augmented Lagrangian formulation however it leads
to additional training complexity. There have been several follow-up works to NOTEARS such as
[98] (see [4] for an overview) that suggest different regularization schemes. We leave exploring such
alternatives to future work.
F.4
A Variational Autoencoder approach
In this section, we describe the technical details involved with adapting Variational Autoencoders
(VAEs) [39, 68] for our setting. We highlight some difficulties that we will encounter and also suggest
possible ways to work around them.
Indeed, most earlier approaches for causal representation learning have relied on maximum likelihood
estimation via variational inference. In particular, they have relied on autoencoders or variational
autoencoders [36, 2]. In our setting, VAEs are a viable approach. More concretely, we can encode
the distribution X(0) to ϵ, then apply the B(i) parameter matrix before finally decoding to X(i). To
handle the fact that we don’t have paired interventional data as in [6], we could potentially modify
the Evidence Lower Bound (ELBO) to include some divergence measure between the predicted and
measured interventional data. Finally, this can be trained end-to-end as in traditional VAEs. We now
describe this in more detail.
We will use VAEs to encode the observational distribution X(0) to ϵ, so the encoder will formally
model B(0) ◦(f −1(.) −µ(i)) where we use the notation from Appendix A.1. Note here that we
cannot directly design the encoder to map X to Z because we do not know the prior distribution on
Z. Indeed, the objective is to learn it.
Let I denote the set of intervention targets. Assume the targets are chosen uniformly at random,
which can be done in practice by subsampling each interventional distribution to have the same
size. Suppose we had paired counterfactual data D = ∪i∈IDi of paired interventional datasets,
i.e. Di consists of pairs (X(0), X(i)) with corresponding probability density denoted p(i)(x(0), x(i)).
Following [39, 101], define an amortized inference distribution as q(ϵ|x(0)). Then, we can bound the
expected log-likelihood as
ED[ln p(i)(x(0), x(i))] = Ei∼U(I)EDi[ln p(i)(x(0), x(i))]
(124)
= Ei∼U(I)EDi ln
Z
ϵ
p(i)(x(0), x(i), ϵ) dϵ
(125)
37

= Ei∼U(I)EDi ln
Z
ϵ
p(i)(x(0), x(i), ϵ)q(ϵ|x(0))
q(ϵ|x(0))
dϵ
(126)
= Ei∼U(I)EDi ln Eϵ∼q(ϵ|x(0))
p(i)(x(0), x(i), ϵ)
q(ϵ|x(0))
(127)
≥Ei∼U(I)EDiEϵ∼q(x(0)) ln p(i)(x(0), x(i), ϵ)
q(ϵ|x(0))
(Jensen’s inequality)
(128)
= Ei∼U(I)EDiEϵ∼q(x(0)) ln p(i)(x(0), x(i)|ϵ)p(ϵ)
q(ϵ|x(0))
(129)
= Ei∼U(I)EDi

Eϵ∼q(x(0)) ln p(i)(x(0), x(i)|ϵ) −Eϵ∼q(x(0)) ln q(ϵ|x(0))
p(ϵ)

(130)
= Ei∼U(I)E(x(0),x(i))∼Di

Eϵ∼q(ϵ|x(0))[ln p(i)(x(0), x(i)|ϵ)]
(131)
−KL(q(ϵ|x(0))||N(0, I))

(132)
where the last term is the standard Evidence Lower Bound (ELBO). This can then be trained end-to-
end via the reparameterization trick. A similar approach was taken in [6] who had access to paired
counterfactual data.
However, we do not have access to such paired interventional data, but instead we only observe the
marginal distributions X(i). To this end, conditioned on ϵ, we can split the term ln p(i)(x(0), x(i)|ϵ) =
ln ep(0)(x(0)|ϵ) + ln ep(i)(x(i)|ϵ) where ep(i) denotes the density of the intervened marginal of p(i),
which corresponds to using the interventional decoder f ◦((B(i))−1(.) + µ(i)). Nevertheless,
Eϵ∼q(x(0))[ln ep(i)(x(i)|ϵ)] is still not tractable in unpaired settings. As a heuristic, we could consider
a modified ELBO by modifying this term to measure some sort of divergence metric between the true
interventional data X(i) and the generated interventional data ep(i)(x(i)|ϵ), similar to [101]. We leave
it for future work to explore this direction.
G
Additional experimental results
In this section, we provide additional experimental results and discussions to explore the effect of
shift strength of the interventions, data scale, and noise distribution.
G.1
Limitations of the contrastive approach
0.0
0.5
1.0
10
20
SHD
0.0
0.5
1.0
0.8
0.9
1.0
AUROC
0.0
0.5
1.0
0.5
1.0
MCC
0.0
0.5
1.0
0.0
0.5
1.0
R2
Figure 5: Dependence of the performance metrics on the shift η. Other settings are as for nonlinear
mixing functions (see Table 7) with ER(10, 2) graphs and d′ = 100.
As mentioned in the main part of the paper, our contrastive algorithm struggles to recover the ground
truth latent variables when considering interventions without shifts. We illustrate the dependence on
the shift strength in Figure 5. In this section, we provide some evidence why this is the case. Apart
38

2
0
2
z
0.6
0.8
1.0
CE
2
0
2
z
2
4
Figure 6: Cross entropy loss for a = 1, c = 0, z0 = 1 and b = 0 (left), b = 2 (right) as a function of
the estimated latent variable ˆz.
from setting the stage for future works, this also enables practitioners to be aware of potential pitfalls
when applying our techniques.
5
0
5
Z1
0.0
0.1
0.2
0.3
0.4
Ground truth Z1
Recovered Z1
5.0
2.5
0.0
2.5
Z1
0.0
0.1
0.2
0.3
0.4
Ground truth Z1
Recovered Z1
2
0
2
Z1
2
1
0
1
2
3
4
Z1
2
0
2
Z1
2
1
0
1
2
3
Z1
Figure 7: Density of standardized ground truth latents and recovered latents for η = 0 (top left) and
η = 1 (top right), scatter plot of the ground truth latent variables Z1 and recovered latent variables ˆZ1
for η = 0 (bottom left) and η = 1 (bottom right). Results shown for ER(10, 2) graphs and d′ = 100,
all further parameters as in Table 7.
We suspect that the main reason for this behavior is the non-convexity of the parametric output layer.
Note that this is very different from the well known non-convexity of (overparametrized) neural
networks. While neural networks parametrize non-convex functions their final layer is typically a
convex optimization problem, e.g., a least squares regression or a logistic regression on the features.
39

Moreover, it is well understood that convergence to a global minimizer using gradient descent is
possible under suitable conditions, see, e.g., [47].
This is different for our quadratic log-odds expression where gradient descent is not sufficient to find
the global minimizer. To illustrate this we consider the case of a single latent, i.e., d = 1, then the
ground truth parametric form of the log-odds in 6 can be expressed as
ln p(1)
X −ln p(0)
X (x) = g(x) = h(f −1(x)) = a(f −1(x) −b)2 + c
(133)
for some constants a, b, c and b = 0 if η(1) = 0, i.e., there is no shift. We moreover assume, for the
sake of argument, that the final layer implementing h(z) = a(z −b)2 + c is fixed to the ground truth
parametric form of the log-odds (then there is also no scaling ambiguity left). Now, let us fix a point
x0 and define z0 = f −1(x0) and
p = P(i = 1|X = x0) =
p(1)
X (x0)
p(1)
X (x0) + p(0)
X (x0)
=
eg(x0)
1 + eg(x0) .
(134)
Then the (sample conditional) cross entropy loss as a function of ˆz = ˆf −1(x0) for our learned
function ˆf is given by
LCE = −p ln

eh(ˆz)
1 + eh(ˆz)

−(1 −p) ln

1
1 + eh(ˆz)

= −ph(ˆz) + ln(1 + eh(ˆz))
(135)
We here drop the intervention index, since we focus on a one dimensional illustration. We plot two
examples of such loss functions for b = 0 and b ̸= 0 in Figure 6.
To verify that this is indeed the reason for our failure to recover the ground truth latent variables,
we investigate the relation between estimated latent variables and ground truth latent variables. The
result can be found in Figure 7 and, as we see, when η = 0, the distribution of the recovered latents is
skewed. Moreover, we find that we essentially recover the latent variable up to a sign, i.e., ˆZ = |Z|
(there is a small offset because we enforce ˆZ to be centered). Note that this explains the tiny MCC
scores in Table 1, since E(Z|Z|) = 0 for symmetric distributions. This observation might also be a
potential starting point to improve our algorithm. For non-zero shifts we recover the latent variables
almost perfectly.
G.2
Varsortability and data scale
An important observation in causal discovery was that many benchmark settings exhibit specific
correlation structures that can be exploited to infer causal directions. The most prominent example
is varsortability [66] which measures how well the causal order agrees with the order induced by
the magnitude of the variance. This is a measure between 0 and 1 and for values close to 0 and 1
the variances contain a lot of information about the causal order while close to 0.5 this is not the
case. It was shown in [66] that typical parameter choices exhibit high varsortability ≈1 which can
be exploited by trivial algorithms. A related notion is R2-varsortability [67] which is a similar notion
where the variance of the variable is replaced by the residual variance after standardizing the variables
and then regressing out all other variables, i.e., the unexplained variance.
In our setting the varsortability of the latent variables Z is quite high (see [66] for the full definition),
e.g., for d = 10 and k = 2 average varsortability is 0.92 ± 0.02. Note that it is not obvious how
this can be exploited algorithmically. Indeed, we only observe a mixture X = f(Z) and the scale
of Y is not identifiable. Nevertheless, to rule out that we implicitly use the scale of the variables
we ran the same experiments except that we standardized the latent variables Z of the observable
distribution (and apply the same scaling to the interventional distributions). The results can be found
in Table 4 and are roughly similar to the result in Table 2 for the same settings without standardization.
Note that R2-sortability is not substantially different from .5 (roughly 0.4) in our settings, thus the
R2-sortability cannot be exploited to infer the causal structure, even when observing Z.
G.3
Effect of noise distribution
Our theoretical results only cover the setting of Gaussian SCMs and we even show that identifiability
does not necessarily hold for uniformly distributed noise (see Appendix D.2). Nevertheless, we can
40

Table 4: Results for nonlinear synthetic data with n = 10000 and standardized latent variables Z (up
to standardization the setting is the same as in Table 2 in the paper).
Setting
Method
SHD ↓
AUROC ↑
MCC ↑
R2 ↑
ER(5, 2), d′ = 20
Contrastive Learning
2.6 ± 0.4
0.92 ± 0.02
0.96 ± 0.01
0.93 ± 0.01
VAE
10.0 ± 0.0
0.50 ± 0.00
0.11 ± 0.01
0.04 ± 0.01
ER(5, 2), d′ = 100
Contrastive Learning
1.4 ± 0.4
0.98 ± 0.02
0.98 ± 0.00
0.97 ± 0.01
VAE
10.0 ± 0.0
0.50 ± 0.00
0.14 ± 0.03
0.10 ± 0.04
ER(10, 2), d′ = 20
Contrastive Learning
9.6 ± 2.1
0.90 ± 0.03
0.90 ± 0.01
0.84 ± 0.01
VAE
18.6 ± 0.9
0.50 ± 0.00
0.27 ± 0.04
0.29 ± 0.04
ER(10, 2), d′ = 100
Contrastive Learning
2.8 ± 1.4
0.99 ± 0.01
0.98 ± 0.00
0.96 ± 0.00
VAE
18.6 ± 0.9
0.50 ± 0.00
0.14 ± 0.03
0.11 ± 0.04
Table 5: Noise dependence of contrastive algorithm for ER(5, 2), d′ = 20, n = 10000 and nonlinear
mixing (same setting as first row of Table 2 in the paper).
Noise distribution
SHD ↓
AUROC ↑
MCC ↑
R2 ↑
Gaussian
1.8 ± 0.5
0.97 ± 0.01
0.97 ± 0.00
0.96 ± 0.00
Laplace
3.0 ± 0.6
0.89 ± 0.03
0.93 ± 0.01
0.89 ± 0.01
Gumbel
3.0 ± 0.9
0.92 ± 0.02
0.94 ± 0.01
0.91 ± 0.01
Uniform
3.4 ± 0.4
0.87 ± 0.03
0.96 ± 0.01
0.93 ± 0.01
Exponential
3.8 ± 0.5
0.86 ± 0.02
0.89 ± 0.02
0.86 ± 0.02
evaluate the performance of our algorithm for linear SCMs with non-Gaussian noise distribution.
We rerun one of the settings in Table 2 for different noise distributions. The results can be found in
Table 5, and they show a slightly worse but still reasonable performance than for Gaussian data. This
is in line with the observation that using a Gaussian likelihood for non-Gaussian data gives good
results in, e.g., causal discovery.
H
Hyperparameters, architectures and further additional details on
experiments
In this section, we give additional details on the experiments. We use PyTorch [60] for all our
experiments.
Data generation
To generate the latent DAG and sample the latent variables we use the sempler
package [21]. We always sample ER(d, k) graphs and the non-zero weights are sampled from the
distribution wij ∼U(±[0.25, 1.0]). To generate linear synthetic data, we sample all entries of the
mixing matrix i.i.d. from a standard Gaussian distribution. For non-linear synthetic data f is given
by the architecture in Table 9 with weights sampled from U([−
1
√
infeat ,
1
√
infeat ]). For image data,
similar to [2], the latents Z are the coordinates of balls in an image (but we sample them differently
as per our setting) and a 64 × 64 × 3 RGB image is rendered using Pygame [77] which encompasses
the nonlinearity. Other parameter choices such as dimensions, DAGs, variance parameters, and shifts
are already outlined in Section 6 but for clarity we also outline them in Tables 6, 7, and 8.
Models
For synthetic data, the model architecture for h(x, θ) is a one hidden-layer MLP with 512
hidden units and LeakyReLU activation functions. For the parametric final layer, we decompose
the matrix W as W = D(Id −A) where D is a diagonal matrix and A is a matrix with a masked
diagonal and they are both learned. This factorization shall improve stability. We initialize A = 0 and
D = Id. Also, the sparsity penalty and the DAGness penalty are only applied to A. For the baseline
VAE we use the same architecture for both encoder and decoder. The code for the linear baseline is
from [84].
41

Table 6: Parameters used for linear synthetic data.
d
5
d′
10
k
3/2
n
{2500, 5000, 10000, 25000, 50000}
σ2
obs
U([2, 4])
σ2
int
U([6, 8])
η(i)
0
runs
5
mixing
linear
Table 7: Parameters used for nonlinear syn-
thetic data.
d
{5, 10}
d′
{20, 100}
k
{1, 2}
n
10000
σ2
obs
U([1, 2])
σ2
int
U([1, 2])
η(i)
U(±[1, 2])
runs
5
mixing
3 layer mlp
Table 8: Parameters used for image data.
d
{4, 6}
d′
3 · 642
k
{1, 2}
n
25000
σ2
obs
U([0.01, 0.01])
σ2
int
U([0.01, 0.02])
η(i)
U(±[0.1, 0.2])
runs
5
mixing
image rendering
Table 9: Synthetic data generation
Architecture
Layer Sequence
MLP Embedding
Input: z ∈Rd
FC 512, LeakyReLU(0.2)
FC 512, LeakyReLU(0.2)
FC 512, LeakyReLU(0.2)
FC d′ ←−Output x
For image data, the model architectures for h(x, θ) are small convolutional networks. For the
contrastive approach the architecture can be found in Table 10. For the VAE model, we use the
encoder architecture as in Table 11 and the decoder architecture as in Table 12 respectively.
Table 10: Contrastive model architecture for image data.
Architecture
Layer Sequence
Conv Embedding
Input: x ∈R64×64×3
Conv (3, 10, kernel size = 5, stride = 3), ReLU()
MaxPool (kernel size = 2, stride = 2)
FC 64, LeakyReLU()
FC d ←−Output ˆz
Training
For training we use the hyperparameters outlined in Table 13. We use a 80–20 split for
train and validation/test set respectively. After subsampling each dataset to the same size, we copy the
observation samples for each interventional dataset in order to have an equal number of observational
and interventional samples so they can be naturally paired during the contrastive learning. We select
the model with the smallest validation loss on the validation set, where we only use the cross entropy
loss for validation. For the VAE baseline we use the standard VAE validation loss for model selection.
42

Table 11: VAE Encoder architecture for image data
Architecture
Layer Sequence
Conv Encoder
Input: x ∈R64×64×3
Conv(3, 16, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
Conv(16, 32, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
Conv(32, 32, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
Conv(32, 64, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
FC 2d ←−Output (mean, logvar)
Table 12: VAE Decoder architecture for image data
Architecture
Layer Sequence
Deconv Decoder
Input: ˆz ∈Rd
FC (3 × 3) × d
DeConv (d, 32, kernel size = 4, stride = 2, padding = 0), LeakyReLU()
DeConv (32, 16, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
DeConv (16, 8, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
DeConv (8, 3, kernel size = 4, stride = 2, padding = 1), LeakyReLU()
Sigmoid() ←−Output ˆx
Table 13: Hyperparameters used for training.
τ1 (see Eq. (9))
10−5
τ2 (see Eq. (9))
10−4
learning rate
5 · 10−4
batch size
512
epochs
200 (synthetic data), 100 (image data)
optimizer
Adam [38]
learning rate scheduler
CosineAnnealing [51]
Compute and runtime
The experiments using synthetic data were run on 2 CPUs with 16Gb
RAM on a compute cluster. For image data we added a GPU to increase the speed. The runtime per
epoch for the synthetic data and 10000 samples was roughly 10s. The total run-time of the reported
experiments was about 100h (i.e. 200 CPU hours) and 20h on a GPU, but hyperparameter selection
and preliminary experiments required about 10 times more CPU compute.
Mean Correlation Coefficient (MCC)
Mean Correlation Coefficient (MCC) is a metric that has
been utilized in prior works [37, 41] to quantify identifiability. MCC measures linear correlations
up to permutation of the components. To compute the best permutation, a linear sum assignment
problem is solved and finally, the correlation coefficients are computed and averaged over. A high
MCC indicates that the true latents have been recovered. In this work, we compute the transformation
using half of the samples and then report the MCC using the other half, i.e. the out-of-distribution
samples.
Further experimental attempts and dead Ends
We will briefly summarize a few additional
settings we tried in our experiments.
• We fixed D = Id in the parametrization of W. This fixes the scaling of the latent variables
in some non-trivial way. This resulted in very similar results.
• We tried to combine the contrastive algorithm with a VAE approach. Since the ground
truth latent variables Z are highly correlated, they are not suitable for an autoencoder that
typically assumes a factorizing prior. Therefore, we map the estimated latent variables for
the observational distribution to the noise distribution ϵ which factorizes. Note that for the
ground truth latent variables the relation ϵ = B(0)Z(0) holds so we consider ˆϵ = W ˆZ. We
43

use ˆϵ as the latent space of an autoencoder on which we then stack a decoder. We train using
the observational samples and the ELBO for the VAE part and with the contrastive loss (and
interventional and observational samples) for the ˆZ embedding. This resulted in a slightly
worse recovery of the latent variables (as measured by the MCC score) but much worse
recovery of the graph. We suspect that this originates from the fact that the matrix W is
used both in the parametrization of the log-odds and to map ˆZ to ϵ which might make the
learning more error-prone. Note that this is different from the suggestion in Section F.4.
• Choosing a larger learning rate for the parametric part than for the non-parametric part
seemed to help initially, but using the same sufficiently small learning rate for the entire
model was more robust.
• We tried larger architectures for both synthetic and image data. They turned out to be more
difficult to train without offering any benefit.
• For image data we often observed overfitting when using smaller sample sizes. The overfit-
ting persisted even when we used a pre-trained ResNet [24] with frozen layers. We suspect
that for image data the performance (in particular the sample complexity) can be further
improved by carefully tuning regularization and model size.
44

