Statistical Efﬁciency of Distributional Temporal
Difference Learning
Yang Peng∗
Liangyu Zhang†
Zhihua Zhang‡
Abstract
Distributional reinforcement learning (DRL) has achieved empirical success in
various domains. One core task in the ﬁeld of DRL is distributional policy evalua-
tion, which involves estimating the return distribution ηπ for a given policy π. The
distributional temporal difference learning has been accordingly proposed, which
is an extension of the temporal difference learning (TD) in the classic RL area.
In the tabular case, Rowland et al. [2018] and Rowland et al. [2024a] proved the
asymptotic convergence of two instances of distributional TD, namely categori-
cal temporal difference learning (CTD) and quantile temporal difference learning
(QTD), respectively. In this paper, we go a step further and analyze the ﬁnite-
sample performance of distributional TD. To facilitate theoretical analysis, we
propose non-parametric distributional temporal difference learning (NTD). For a
γ-discounted inﬁnite-horizon tabular Markov decision process, we show that for
NTD we need eO

1
ε2p(1−γ)2p+1

iterations to achieve an ε-optimal estimator with
high probability, when the estimation error is measured by the p-Wasserstein dis-
tance. This sample complexity bound is minimax optimal up to logarithmic fac-
tors in the case of the 1-Wasserstein distance. To achieve this, we establish a novel
Freedman’s inequality in Hilbert spaces, which would be of independent interest.
In addition, we revisit CTD, showing that the same non-asymptotic convergence
bounds hold for CTD in the case of the p-Wasserstein distance for p ≥1.
1
Introduction
In high-stake applications of reinforcement learning (RL), such as healthcare [Lavori and Dawson,
2004, Böck et al., 2022] and ﬁnance[Ghysels et al., 2005], only considering the mean of returns is
insufﬁcient. It is necessary to take risk and uncertainties into consideration. Distributional reinforce-
ment learning (DRL) Morimura et al. [2010], Bellemare et al. [2017, 2023] addresses such issues by
modeling the complete distribution of returns instead of their expectations.
In the ﬁeld of DRL, one of the most fundamental tasks is to estimate the return distribution ηπ for
a given policy π, which is referred to as distributional policy evaluation. Distributional temporal
difference learning (TD) is probably the most widely-used approach for solving the distributional
policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how
to represent the return distribution, an inﬁnite-dimensional object, via a computationally feasible
ﬁnite-dimensional parametrization. This has led to the development of two special instances of dis-
tributional TD: categorical temporal difference learning (CTD) [Bellemare et al., 2017] and quantile
temporal difference learning (QTD) [Dabney et al., 2018]. These algorithms provide computation-
ally tractable parametrizations and updating schemes of the return distribution.
∗School of Mathematical Sciences, Peking University; email: pengyang@pku.edu.cn.
†School of Statistics and Management, Shanghai University of Finance and Economics;
email:
zhangliangyu@sufe.edu.cn.
‡School of Mathematical Sciences, Peking University; email: zhzhang@math.pku.edu.cn.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).

Previous theoretical works have primarily focused on the asymptotic behaviors of distributional TD.
In particular, Rowland et al. [2018] and Rowland et al. [2024a] showed the asymptotic convergences
of CTD and QTD in the tabular case, respectively. A natural question arises: can we depict the sta-
tistical efﬁciency of distributional TD by non-asymptotic results similar to the classic TD algorithm
[Li et al., 2024]?
1.1
Contributions
In this paper, we manage to answer the above question afﬁrmatively in the synchronous setting
[Kakade, 2003, Kearns et al., 2002]. Firstly, we introduce non-parametric distributional temporal
difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding.
We show that eO

1
ε2p(1−γ)2p+1

4 iterations are sufﬁcient to yield an estimator ˆηπ, such that the
p-Wasserstein metric between ˆηπ and ηπ is less than ε with high probability (Theorem 4.1). This
bound is minimax optimal (Theorem B.1) in the 1-Wasserstein metric case, if we neglect all loga-
rithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the p-Wasserstein
metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is
worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman’s in-
equality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the
current work.
1.2
Related Work
Non-asymptotic results of DRL.
Recently, there has been an emergence of work focusing on
ﬁnite-sample/iteration results of the distributional policy evaluations.
Wu et al. [2023] studied the ofﬂine distributional policy evaluation problem. They solved the prob-
lem via ﬁtted likelihood estimation (FLE) inspired by the classic ofﬂine policy evaluation algorithm
ﬁtted Q evaluation (FQE), and provided a generalization bound in the p-Wasserstein metric case.
Zhang et al. [2023] proposed to solve distributional policy evaluation by the model-based ap-
proach and derived corresponding sample complexity bounds, namely eO

1
ε2p(1−γ)2p+2

in the
p-Wasserstein metric case, and eO

1
ε2(1−γ)4

in both the Kolmogorov-Smirnov metric and total
variation metric under different conditions. Rowland et al. [2024b] proposed direct categorical ﬁxed-
point computation (DCFP), a model-based version of CTD, in which they constructed the estimator
by solving a linear system directly instead of performing an iterative algorithm. They showed that
the sample complexity of DCFP is eO

1
ε2(1−γ)3

in the 1-Wasserstein metric case by introducing
the novel stochastic categorical CDF Bellman operator and equation. Their result matches the mini-
max lower bound (up to logarithmic factors) eΩ

1
ε2(1−γ)3

proposed in [Zhang et al., 2023], which
implies that learning the full return distribution can be as sample-efﬁcient as learning just its expecta-
tion. It’s worth noting that the algorithms analyzed in both [Zhang et al., 2023] and [Rowland et al.,
2024b] are model-based, hence they are less similar to practical algorithms. While distributional
TD analyzed in this paper, as a model-free method, is more practical, and also involves a more
complicated theoretical analysis.
Böck and Heitzinger [2022] also considered model-free method. They proposed speedy categorical
policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They
showed that the sample complexity of SCPE is eO

1
ε2(1−γ)4

in the 1-Wasserstein metric case.
Compared to [Böck and Heitzinger, 2022], our work shows that even if we do not introduce any
acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax
optimal sample complexity bounds. Thus, we give sharper bounds based on a simpler algorithm.
Table 1 gives more detailed comparisons of sample complexity with the previous work in the 1-
Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional
4Throughout this paper, the notation f(·) = ˜O (g(·)) (f(·) = ˜Ω(g(·))) means that f(·) is order-wise no
larger (smaller) than g(·), ignoring logarithmic factors poly(log |S| , log |A| , log(
1
1−γ ), log( 1
ε), log( 1
δ )), as
|S| , |A| ,
1
1−γ , 1
ε, 1
δ →∞.
2

Sample Complexity
Algorithms
Task
[Gheshlaghi Azar et al., 2013]
eO

1
ε2(1−γ)3

Model-based
PE
[Li et al., 2024]
eO

1
ε2(1−γ)3

TD (Model-free)
PE
[Rowland et al., 2018]
Asymptotic
CTD (Model-free)
DPE
[Rowland et al., 2024a]
Asymptotic
QTD (Model-free)
DPE
[Rowland et al., 2024b]
eO

1
ε2(1−γ)3

DCFP (Model-based)
DPE
[Böck and Heitzinger, 2022]
eO

1
ε2(1−γ)4

SCPE (Model-free)
DPE
Our Work
eO

1
ε2(1−γ)3

CTD (Model-free)
DPE
Table 1. Sample complexity of algorithms for solving policy evaluation (PE) in the ℓ∞norm, and
distributional policy evaluation (DPE) in the supreme 1-Wasserstein metric.
policy evaluation task by taking expectation of the return distribution estimator. And the supreme
1-Wasserstein metric error of the return distribution estimator is not smaller than the ℓ∞error of the
induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed
the sample complexity of the policy evaluation task in Table 1 for comparison.
Freedman’s inequality.
Freedman’s inequality was originally proposed in [Freedman, 1975]. It
can be viewed as a Bernstein’s inequality for martingales, which is crucial for analyzing stochas-
tic approximation algorithms. Tropp [2011] generalized Freedman’s inequality to matrix martin-
gales. And Talebi et al. [2022] established Freedman inequalities for martingales in the setting of
noncommutative probability spaces. The closest literature to ours is [Tarres and Yao, 2014] and
[Martinez-Taboada and Ramdas, 2024], where they provided a special case of our Theorem A.2
with H = 0 independently. When H = 0, we can only utilize the deterministic upper bound on
the quadratic variation rather than the high-probability upper bound. In certain problems, such as
the distributional TD learning we aim to investigate, it is impossible to achieve the optimal upper
bound using the H = 0 version. [Martinez-Taboada and Ramdas, 2024] also proposed an empirical
Freedman’s inequality in (2, D)-smooth Banach space, which can be used to construct conﬁdence
sets or perform hypothesis testing. To the best of our knowledge, we are the ﬁrst to present this
version (Theorem A.2) of Freedman’s inequality in Hilbert spaces5.
The remainder of this paper is organized as follows. In Section 2, we introduce some background
of DRL and state Freedman’s inequality in Hilbert spaces. In Section 3, we revisit distributional
TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic
convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results,
and Section 6 concludes our work. We put the detailed results with Freedman’s inequality in Hilbert
spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in
Appendix B.
2
Background
An inﬁnite-horizon tabular Markov decision process (MDP) is deﬁned by a 5-tuple M
=
⟨S, A, PR, P, γ⟩, where S represents a ﬁnite state space, A a ﬁnite action space, PR the distri-
bution of rewards, P the transition dynamics, i.e., PR(·|s, a) ∈∆([0, 1]), P(·|s, a) ∈∆(S) for any
state action pair (s, a) ∈S × A, and γ ∈(0, 1) a discount factor. Here we use ∆(·) to represent
the set of all probability distributions over some set. Given a policy π: S →∆(A) and an initial
state s0 = s ∈S, a random trajectory {(st, at, tt)∞
t=0} can be sampled from M: at | st ∼π(· | st),
rt | (st, at) ∼PR(· | st, at), st+1 | (st, at) ∼P(· | st, at) for any t ∈N. Given a trajectory, we
deﬁne the return by Gπ(s) := P∞
t=0 γtrt ∈
h
0,
1
1−γ
i
. We denote return distribution ηπ(s) as the
probability distribution of Gπ(s), and ηπ := (ηπ(s))s∈S. The expected return V π(s) = EGπ(s) is
the value function in the traditional RL setting.
5In this paper, we assume that all the Hilbert spaces we encounter are separable, which can avoid measura-
bility issues, ensure that the expectation can be deﬁned, and guarantee tightness of any distribution. See Pisier
[2016] for more details about probability in Hilbert space
3

2.1
Distributional Bellman Equation and Operator
Recall that the classic policy evaluation aims at computing the value functions V π. It is known that
V π = (V π(s))s∈S satisfy the Bellman equation. That is, for any s ∈S,
V π(s) = [T π(V π)] (s) = Ea∼π(·|s),r∼PR(·|s,a),s′∼P (·|s,a) [r + γV π(s′)] .
(1)
The operator T π : RS →RS is called the Bellman operator, and V π is a ﬁxed point of T π.
The task of distribution policy evaluation is ﬁnding ηπ given some ﬁxed policy π. ηπ satisﬁes a
distributional version of the Bellman equation (1). That is, for any s ∈S,
ηπ(s) = (T πηπ) (s) = Ea∼π(·|s),r∼PR(·|s,a),s′∼P (·|s,a)
h
(br,γ)# ηπ(s′)
i
,
(2)
where br,γ : R →R is an afﬁne function deﬁned by br,γ(x) = r + γx. And f#µ is the push forward
measure of µ through any function f : R →R, so that f#µ(A) = µ(f −1(A)) for any Borel set A,
where f −1(A) := {x: f(x) ∈A}. The operator T π : ∆
h
0,
1
1−γ
iS
→∆
h
0,
1
1−γ
iS
is known
as the distributional Bellman operator, and ηπ is a ﬁxed point of T π. For notational simplicity, we
denote ∆
h
0,
1
1−γ
i
as P from now on.
2.2
T π as Contraction in P
A key property of the Bellman operator T π is that it is a γ-contraction w.r.t. the supreme norm (i.e.
ℓ∞norm). However, before we can properly discuss the contraction properties of T π, we need to
specify a metric d on P. And for any metric d on P, we denote ¯d as the corresponding supreme
metric on PS, i.e., ¯d (η, η′) := maxs∈S d (η(s), η′(s)) for any η, η′ ∈PS.
Suppose
µ
and
ν
are
two
probability
distributions
on
R
with
ﬁnite
p-moments
for
p
∈
[1, ∞].
The p-Wasserstein metric between µ and ν is deﬁned as Wp(µ, ν)
:=
 infκ∈Γ(µ,ν)
R
R2 |x −y|p κ(dx, dy)
1/p. Each element κ ∈Γ(µ, ν) is a coupling of µ and ν, i.e.,
a joint distribution on R2 with prescribed marginals µ and ν on each “axis.” When p = 1 we have
W1(µ, ν) =
R
R |Fµ(x) −Fν(x)|dx, where Fµ and Fν are the cumulative distribution function of
µ and ν, respectively. It can be shown that T π is a γ-contraction w.r.t. the supreme p-Wasserstein
metric ¯Wp.
Proposition 2.1. [Bellemare et al., 2023, Propositions 4.15] The distributional Bellman operator
is a γ-contraction on PS w.r.t. the supreme p-Wasserstein metric for p ∈[1, ∞]. That is, for any
η, η′ ∈PS, we have ¯Wp (T πη, T πη′) ≤γ ¯Wp(η, η′).
The ℓp metric between µ and ν is deﬁned as ℓp(µ, ν) =
 R
R |Fµ(x) −Fν(x)|p dx
 1
p for p ∈[1, ∞),
and T π is γ
1
p -contraction w.r.t. the supreme ℓp metric ¯ℓp.
Proposition 2.2. [Bellemare et al., 2023, Propositions 4.20] The distributional Bellman operator is
a γ
1
p -contraction on PS w.r.t. the supreme ℓp metric for p ∈[1, ∞). That is, for any η, η′ ∈PS,
we have ¯ℓp (T πη, T πη′) ≤γ
1
p ¯ℓp(η, η′).
Note that the ℓ1 metric coincides with the 1-Wasserstein metric. And the ℓ2 metric is also called the
Cramér metric, which plays an important role in subsequent analysis because the zero-mass signed
measure space equipped with this metric
 M, ∥·∥ℓ2

(deﬁned in Section 5.1) is a Hilbert space6.
Thereby, we can apply Freedman’s inequality in Hilbert spaces.
2.3
Freedman’s Inequality in Hilbert Spaces
Just as Freedman’s inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]),
a Hilbert space version of Freedman’s inequality is indispensable for deriving the minimax non-
asymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space ver-
sion of the original Freedman’s inequality (Theorem 1.6 in [Freedman, 1975]), and more detailed
results can be found in Appendix A.
6In fact, the space

M, ∥·∥ℓ2

is not complete. However, the completeness property does not affect the
non-asymptotic analysis, see Section 5.1 for more details.
4

Let X be a Hilbert space, {Xi}n
i=1 be an X-valued martingale difference sequence adapted to the
ﬁltration {Fi}n
i=1, Yi := Pi
j=1 Xj be the corresponding martingale, and Wi := Pi
j=1 σ2
j be the
corresponding quadratic variation process. Here σ2
j := Ej−1 ∥Xj∥2, and Ei [·] := E [·|Fi] denotes
the conditional expectation.
Theorem 2.1 (Freedman’s inequality in Hilbert spaces). Suppose maxi∈[n] ∥Xi∥≤b for some
constant b > 0. Then, for any ε and σ > 0, the following inequality holds
P
 ∃k ∈[n], s.t. ∥Yk∥≥ε and Wk ≤σ2
≤2 exp

−
ε2/2
σ2 + bε/3

.
3
Distributional Temporal Difference Learning
If the MDP M = ⟨S, A, PR, P, γ⟩is known, and because V π is the ﬁxed point of the contraction
T π, V π can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for
any initialization V (0) ∈RS, if we deﬁne the iteration sequence V (k+1) = T π(V (k)) for k ∈N,
we have limk→∞
V (k) −V π
∞= 0 by the contraction mapping theorem (Proposition 4.7 in
[Bellemare et al., 2023]).
Similarly, the distributional dynamic programming algorithm deﬁnes the iteration sequence as
η(k+1) = T πη(k) for any initialization η(0). In the same way, we have limk→∞¯Wp(η(k), ηπ) = 0
for p ∈[1, ∞] and limk→∞¯ℓp(η(k), ηπ) = 0 for p ∈[1, ∞).
In most application scenarios, the transition dynamic P and reward distribution PR are unknown,
and instead we can only get samples of P and PR in a streaming manner. In this paper, we as-
sume a generative model [Kakade, 2003, Kearns et al., 2002] is accessible, which generates in-
dependent samples for all states in each iteration, i.e., in the t-th iteration, we collect sample
at(s) ∼π(·|s), st(s) ∼P(·|s, at(s)), rt(s) ∼PR(·|s, at(s)) for each s ∈S. Similar to TD
[Sutton, 1988] in classic RL, distributional TD also employs the stochastic approximation (SA)
[Robbins and Monro, 1951] technique to address the aforementioned problem and can be viewed as
an approximate version of distributional DP.
Non-parametric Distributional TD
We ﬁrst introduce non-parametric distributional temporal dif-
ference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the
setting of NTD, we assume the return distributions can be precisely updated without any parametriza-
tion. For any initialization ηπ
0 ∈PS, the updating scheme is given by
ηπ
t = (1 −αt)ηπ
t−1 + αtT π
t ηπ
t−1
for any t ≥1. Here αt is the step size. The empirical Bellman operator at the t-th iteration T π
t is
deﬁned as
(T π
t η) (s) = (brt(s),γ)#(η(st+1)),
which is an unbiased estimator of (T πη) (s). It is evident that NTD is a SA modiﬁcation of distribu-
tional DP. Consequently, we can analyze NTD using the techniques from the SA area.
Categorical Distributional TD
Now, we revisit the more practical CTD. In this case, the updates
in CTD is computationally tractable, due to the following categorical parametrization of probability
distributions:
PK :=
( K
X
k=0
pkδxk : p0, . . . , pK ≥0 ,
K
X
k=0
pk = 1
)
,
where K ∈N, and 0 ≤x0 < · · · < xK ≤
1
1−γ are ﬁxed points of the support. For simplicity,
we assume {xk}K
k=0 are equally-spaced, i.e., xk =
k
K(1−γ). We denote the gap between two points
by ιK =
1
K(1−γ). When updating the return distributions, we need to evaluate the ℓ2-projection
of PK, ΠK : P →PK, ΠKµ := argminˆµ∈PK ℓ2(µ, ˆµ). It can be shown (Proposition 5.14 in
[Bellemare et al., 2023]) that the projection is uniquely given by
ΠKµ =
K
X
k=0
pk(µ)δxk, where
pk(µ) = EX∼µ
"
1 −

X −xk
ιK


+
#
,
5

(x)+ := max {x, 0} for any x ∈R. It is known that ΠK is non-expansive w.r.t. the Cramér metric
(Lemma 5.23 in [Bellemare et al., 2023]), i.e., ℓ2(ΠKµ, ΠKν) ≤ℓ2(µ, ν) for any µ, ν ∈P. For
any η ∈PS, s ∈S, we slightly abuse the notation and deﬁne (ΠKη) (s) := ΠKη(s). ΠK is still
non-expansive w.r.t. ¯ℓ2. Hence T π,K := ΠKT π is a √γ-contraction w.r.t. ¯ℓ2, we denote its unique
ﬁxed point as ηπ,K ∈PS
K. The approximation error induced by categorical parametrization is given
by (Proposition 3 in Rowland et al. [2018])
¯ℓ2(ηπ, ηπ,K) ≤
1
√
K(1 −γ)
,
¯W1(ηπ, ηπ,K) ≤
1
√1 −γ
¯ℓ2(ηπ, ηπ,K) ≤
1
√
K(1 −γ)3/2 .
(3)
Now, we are ready to give the updating scheme of CTD, given any initialization ηπ
0 ∈PS
K,
ηπ
t = (1 −αt)ηπ
t−1 + αtΠKT π
t ηπ
t−1
for any t ≥1. We can ﬁnd that the only difference between CTD and NTD lies in the additional
application of the projection operator ΠK at each iteration in CTD.
4
Statistical Analysis
In this section, we state our main results. For both NTD and CTD, we give the non-asymptotic
convergence rates of ¯
Wp(ηπ
T , ηπ) and ¯ℓ2(ηπ
T , ηπ), respectively.
4.1
Non-asymptotic Analysis of NTD
We ﬁrst provide a non-asymptotic convergence rate of ¯W1(ηπ
T , ηπ) for NTD, which is minimax
optimal (Theorem B.1) up to logarithmic factors.
Theorem 4.1 (Sample complexity of NTD in the 1-Wasserstein metric). Given any δ ∈(0, 1) and
ε ∈(0, 1), let the initialization be ηπ
0 ∈PS, the total update number T satisfy
T ≥C1 log3 T
ε2(1 −γ)3 log |S| T
δ
for some large universal constant C1 > 1, i.e., T = eO

1
ε2(1−γ)3

, and the step size αt satisfy
1
1 + c2(1−√γ)t
log t
≤αt ≤
1
1 + c3(1−√γ)t
log t
for some small universal constants c2 > c3 > 0. Then, with probability at least 1−δ, the last iterate
estimator satisﬁes ¯W1 (ηπ
T , ηπ) ≤ε.
Because ¯W1 (ηπ
T , ηπ) ≤
1
1−γ always holds, we can translate the high probability bound to a mean
error bound, that is,
E
 ¯W1 (ηπ
T , ηπ)

≤ε(1 −δ) +
δ
1 −γ ≤2ε
if we take δ ≤ε(1 −γ). In the subsequent discussion, we will not state the mean error bound
conclusions for the sake of brevity.
The key idea of our proof is to ﬁrst expand the error term ¯W1 (ηπ
T , ηπ) over the time steps. Then it
can be decomposed into an initial error term and a martingale term. The initial error term becomes
smaller as the iteration goes due to the contraction properties of T π. To control the martingale
term, we ﬁrst use the basic inequality (Lemma E.1) W1 (µ, ν) ≤
1
√1−γ ℓ2 (µ, ν), which allows us to
analyze this error term in the Hilbert space (M, ∥·∥ℓ2) deﬁned in Section 5.1. Consequently, we can
bound it using Freedman’s inequality in the Hilbert space (Theorem A.2). A more detailed outline
of proof can be found in Section 5.2.
Combining Theorem 4.1 with the basic inequality ¯Wp(η, η′) ≤
1
(1−γ)
1−1
p
¯W
1
p
1 (η, η′) for any η, η′ ∈
PS (Lemma E.1), we can derive that T = eO

1
ε2p(1−γ)2p+1

iterations are sufﬁcient to ensure
6

¯Wp(ηπ
T , ηπ) ≤ε. As pointed out in the example after Corollary 3.1 in [Zhang et al., 2023], when
p > 1, the slow rate in terms of ε is inevitable without additional regularity conditions.
Although the 1-Wasserstein metric cannot bound the Cramér metric properly, by making slight mod-
iﬁcations to the proof we have the following non-asymptotic convergence rate of ¯ℓ2(ηπ
T , ηπ). See
Appendix C.5 for our proof.
Corollary 4.1 (Sample complexity of NTD in the Cramér metric). Given any δ ∈(0, 1) and ε ∈
(0, 1), let the initial value ηπ
0 ∈PS, the total update number T satisfy
T ≥
C1 log3 T
ε2(1 −γ)5/2 log |S| T
δ
for some large universal constant C1 > 1, i.e., T = eO

1
ε2(1−γ)5/2

, and the step size αt satisfy
1
1 + c2(1−√γ)t
log t
≤αt ≤
1
1 + c3(1−√γ)t
log t
for some small universal constants c2 > c3 > 0. Then, with probability at least 1−δ, the last iterate
estimator satisﬁes ¯ℓ2 (ηπ
T , ηπ) ≤ε.
4.2
Non-asymptotic Analysis of CTD
We ﬁrst state a parallel result to Theorem 4.1.
Theorem 4.2 (Sample complexity of CTD in the 1-Wasserstein metric). Given any δ ∈(0, 1) and
ε ∈(0, 1), suppose K >
4
1−γ , the initial value ηπ
0 ∈PS
K, the total update number T satisﬁes
T ≥C1 log3 T
ε2(1 −γ)3 log |S| T
δ
for some large universal constant C1 > 1, i.e., T = eO

1
ε2(1−γ)3

, and the step size αt satisﬁes
1
1 + c2(1−√γ)t
log t
≤αt ≤
1
1 + c3(1−√γ)t
log t
for some small universal constants c2 > c3 > 0. Then, with probability at least 1 −δ, the last
iterate estimator satisﬁes ¯W1
 ηπ
T , ηπ,K
≤ε
2. Furthermore, according to the upper bound (3) of
the approximation error ¯W1
 ηπ,K, ηπ
, if we take K >
4
ε2(1−γ)3 , we have ¯W1 (ηπ
T , ηπ) ≤ε.
Note that the order (modulo logarithmic factors) of sample complexity of CTD is better than the
previous results of SCPE [Böck and Heitzinger, 2022], and we do not need the additional term
introduced in the updating scheme of SCPE.
The proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Sec-
tion 5.2. The ¯W1 metric result can be translated into sample complexity bound eO

1
ε2p(1−γ)2p+1

in
the ¯Wp metric. We comment that this theoretical result matches the sample complexity bound in the
model-based setting [Rowland et al., 2024b].
As in the NTD setting, we have the following non-asymptotic convergence rate of ¯ℓ2(ηπ
T , ηπ) as a
corollary of Theorem 4.2. See Appendix C.5 for the proof.
Corollary 4.2 (Sample complexity of CTD in the Cramér metric). For any given δ ∈(0, 1) and
ε ∈(0, 1), suppose K >
4
1−γ , the initialization is ηπ
0 ∈PS
K, the total update number T satisﬁes
T ≥
C1 log3 T
ε2(1 −γ)5/2 log |S| T
δ
for some large universal constant C1 > 1, i.e., T = eO

1
ε2(1−γ)5/2

, and the step size αt satisﬁes
1
1 + c2(1−√γ)t
log t
≤αt ≤
1
1 + c3(1−√γ)t
log t
for some small universal constants c2 > c3 > 0. Then, with probability at least 1 −δ, the last
iterate estimator satisﬁes ¯ℓ2
 ηπ
T , ηπ,K
≤ε
2. Furthermore, according to the upper bound (3) of the
approximation error ¯ℓ2
 ηπ,K, ηπ
, if we take K >
4
ε2(1−γ)2 , we have ¯ℓ2 (ηπ
T , ηπ) ≤ε.
7

5
Proof Outlines
In this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1,
Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we ﬁrst deﬁne some
notation.
5.1
Zero-mass Signed Measure Space
To analyze the distance between the estimator and the ground-truth ηπ, we will work with the zero-
mass signed measure space M deﬁned as follows
M :=

µ: µ is a signed measure with |µ| (R) < ∞, µ(R) = 0, supp(µ) ⊆[0,
1
1 −γ ]

,
where |µ| is the total variation measure of µ, and supp(µ) is the support of µ. See [Bogachev, 2007]
for more details about signed measures.
For any µ ∈M, we deﬁne its cumulative function as Fµ(x) := µ[0, x). We can check that Fµ is
linear w.r.t. µ, that is, Fαµ+βν = αFµ + βFν for any α, β ∈R, µ, ν ∈M.
To analyze the Cramér metric case, we deﬁne the following Cramér inner product on M:
⟨µ, ν⟩ℓ2 :=
Z
1
1−γ
0
Fµ(x)Fν(x)dx.
It is easy to verify that ⟨·, ·⟩ℓ2 is indeed an inner product on M. The corresponding norm, called the
Cramér norm, is given by ∥µ∥ℓ2 =
q
⟨µ, µ⟩ℓ2 =
qR
1
1−γ
0
(Fµ(x))2 dx. We have ν1 −ν2 ∈M and
∥ν1 −ν2∥ℓ2 = ℓ2 (ν1, ν2) for any ν1, ν2 ∈P.
The W1 norm on M is deﬁned as ∥µ∥W1 :=
R
1
1−γ
0
|Fµ(x)| dx. We have ∥ν1 −ν2∥W1 = W1 (ν1, ν2)
for any ν1, ν2 ∈P.
We can extend the distributional Bellman operator T π and the Cramér projection operator ΠK nat-
urally to MS. Here, the product space MS is also a Banach space, and we use the supreme norm:
∥η∥¯ℓ2 := maxs∈S ∥η(s)∥ℓ2, and ∥η∥¯
W1 := maxs∈S ∥η(s)∥W1 for any η ∈MS. We denote by I
the identity operator in MS.
When the norm ∥·∥is applied to A
∈
L(X), where X is any Banach space, and L(X)
is the space of all bounded linear operators in X, we refer ∥A∥to the operator norm
of A, which is deﬁned as ∥A∥
:=
supη∈X,∥η∥=1 ∥Aη∥.
With this notation, L(X)
=
{A: A is a linear operator mapping from X to X, and ∥A∥< ∞}.
Proposition 5.1. T π and ΠK are linear operators in MS.
Furthermore, ∥T π∥¯ℓ2
≤√γ,
∥T π∥¯
W1 ≤γ, ∥ΠK∥¯ℓ2 = 1, and ∥ΠK∥¯
W1 ≤1.
The proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results
are trivial. We omit the proofs for brevity.
Moreover, we have the following matrix (of operators) representations of T π and ΠK: T π ∈
L(M)S×S for any η ∈MS,
(T πη) (s) =
X
a∈A,s′∈S
π(a | s)P(s′ | s, a)
Z 1
0
(br,γ)# η(s′)PR(dr | s, a) =
X
s′∈S
T π(s, s′)η(s′),
where T π(s, s′) ∈L(M) for any ν ∈M,
T π(s, s′)ν =
X
a∈A
π(a | s)P(s′ | s, a)
Z 1
0
(br,γ)# νPR(dr | s, a).
It can be veriﬁed that ∥T (s, s′)∥ℓ2 ≤√γ P
a∈A π(a | s)P(s′ | s, a) =: √γP π(s′|s). Similarly,
∥T (s, s′)∥W1 ≤γP π(s′|s), and ΠK = diag
 ΠK

M

s∈S ∈L(M)S×S. With these represen-
tations, ΠKT π ∈L(M)S×S can be interpreted as matrix multiplication, where the scalar mul-
tiplication is replaced by the composition of operators. It can be veriﬁed that (ΠKT π) (s, s′) =
ΠKT π(s, s′), and ∥(ΠKT π) (s, s′)∥ℓ2 ≤√γP π(s′|s).
8

Remark 1: In Lemma E.2, we show that both
 M, ∥·∥ℓ2

and
 M, ∥·∥W1

are separable. And in
Lemma E.3, we show that
 M, ∥·∥W1

is not complete. To resolve this problem, we will use their
completions to replace them without loss of generality, because the completeness property does not
affect the separability. For simplicity, we still use M to denote the completion space. According to
the BLT theorem [Theorem 5.19 Hunter and Nachtergaele, 2001], any bounded linear operator can
be extended to the completion space, and still preserves its operator norm.
5.2
Analysis of Theorems 4.1 and 4.2
For simplicity, we abbreviate both ∥·∥¯ℓ2 and ∥·∥ℓ2 as ∥·∥in this part.
For all t ∈[T] :=
{1, 2, · · · , T}, we denote Tt := T π
t , T := T π, η := ηπ for NTD; Tt := ΠKT π
t , T := ΠKT π,
η := ηπ,K for CTD; and ηt := ηπ
t , ∆t := ηt −η ∈MS for both NTD and CTD. According to
Lemma E.4, ηt ∈PS for NTD and ηt ∈PS
K for CTD. Our goal is to bound the ¯W1 norm of the
error term ∥∆T ∥¯
W1. This can be achieved by bounding ∥∆T ∥, as ∥∆T ∥¯
W1 ≤
1
√1−γ ∥∆T ∥.
According to the updating rule, we have the error decomposition
∆t = ηt −η
= (1 −αt)ηt−1 + αtTtηt−1 −η
= (1 −αt)∆t−1 + αt (Ttηt−1 −T η)
= (1 −αt)∆t−1 + αt (Tt −T ) ηt−1 + αtT (ηt−1 −η)
= [(1 −αt)I + αtT ] ∆t−1 + αt (Tt −T ) ηt−1.
Applying it recursively, we can further decompose the error into two terms
∆T =
T
Y
t=1
[(1 −αt)I + αtT ] ∆0
|
{z
}
(I)
+
T
X
t=1
αt
T
Y
i=t+1
[(1 −αi)I + αiT ] (Tt −T ) ηt−1
|
{z
}
(II)
,
where Qt
k=1 Ak is deﬁned as AtAt−1 · · · A1 for any operators or matrices {Ak}t
k=1 throughout
the paper. Term (I) is an initial error term that becomes negligible when T is large because T is a
contraction. Term (II) can be bounded via Freedman’s inequality in the Hilbert space (Theorem A.2).
Combining the two upper bound, we can establish a recurrence relation. Solving this relation will
lead to the conclusion.
We ﬁrst establish the conclusion for step sizes that depend on T. Speciﬁcally, we consider
T ≥C4 log3 T
ε2(1 −γ)3 log |S| T
δ
,
1
1 + c5(1−√γ)T
log2 T
≤αt ≤
1
1 + c6(1−√γ)t
log2 T
,
where c5 > c6 > 0 are small constants satisfying c5c6 ≤
1
8, and C4 > 1 is a large constant
depending only on c5 and c6. As shown in Appendix C.1, once we have established the conclusion
in this setting, we can recover the original conclusion stated in the theorem.
Now, we introduce the following useful quantities involving step sizes and γ
β(t)
k
:=



Qt
i=1
 1 −αi(1 −√γ)

,
if k = 0,
αk
Qt
i=k+1
 1 −αi(1 −√γ)

,
if 0 < k < t,
αT ,
if k = t.
The following lemma provides useful bounds for β(t)
k .
Lemma 5.1. Suppose c5c6 ≤1
8. Then, for all t ≥
T
c6 log T , we have that
β(t)
k
≤1
T 2 , for 0 ≤k ≤t
2;
β(t)
k
≤
2 log3 T
(1 −√γ)T , for t
2 < k ≤t.
9

The proof can be found in Appendix C.2. From now on, we only consider t ≥
T
c6 log T .
The upper bound of term (I) is given by
(I) ≤
tY
k=1
∥(1−αk)I+αkT ∥∥∆0∥≤
tY
k=1
((1−αk)+αk
√γ)
1
√1−γ =
β(t)
0
√1−γ ≤
1
√1−γT 2 ,
where ∥∆0∥≤
qR
1
1−γ
0
dx =
1
√1−γ .
As for term (II), we have the following upper bound with high probability by utilizing Freedman’s
inequality (Theorem A.2).
Lemma 5.2. For any δ ∈(0, 1), with probability at least 1 −δ, we have for all t ≥
T
c6 log T , in the
NTD case,

t
X
k=1
αk
tY
i=k+1
[(1 −αi)I + αiT ] (Tk −T ) ηk−1

≤34
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)2T

1 +
max
k: t/2<k≤t ∥∆k−1∥¯
W1

.
The conclusion still holds for the CTD case if we take K ≥
4
ε2(1−γ)2 + 1.
The proof can be found in Appendix C.3. Combining the two results, we ﬁnd the following recur-
rence relation in terms of the ¯W1 norm holds given the choice of T, with probability at least 1 −δ,
for all t ≥
T
c6 log T
∥∆t∥¯
W1 ≤
1
√1 −γ ∥∆t∥≤35
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T

1 +
max
k: t/2<k≤t ∥∆k−1∥¯
W1

.
In Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator:
∥∆T ∥¯
W1 ≤C7



v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T
+
 log3 T
 
log |S|T
δ

(1 −γ)3T


,
where C7 > 1 is a large universal constant depending on c6. Now, we can obtain the conclusion if
taking C4 ≥2C2
7 and T ≥C4 log3 T
ε2(1−γ)3 log |S|T
δ .
6
Conclusions
In this paper we have studied the statistical performance of the distributional temporal difference
learning (TD) from a non-asymptotic perspective. Speciﬁcally, we have considered two instances
of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distri-
butional TD (CTD). For both NTD and CTD, we have shown that eO

1
ε2p(1−γ)2p+1

iterations are
sufﬁcient to achieve a p-Wasserstein ε-optimal estimator, which is minimax optimal (up to logarith-
mic factors). We have established a novel Freedman’s inequality in Hilbert spaces to prove these
theoretical results, which has independent theoretical value beyond the current work. We leave the
details to Appendix A.
Acknowledgments and Disclosure of Funding
This work has been supported by the National Key Research and Development Project of China
(No. 2022YFA1004002), the National Natural Science Foundation of China (No. 12271011 and
No. 12350001), and the MOE Project of Key Research Institute of Humanities and Social Sciences
(No.22JJD110001).
10

References
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
In International conference on machine learning, pages 449–458. PMLR, 2017.
M. G. Bellemare, W. Dabney, and M. Rowland. Distributional Reinforcement Learning. MIT Press,
2023. http://www.distributional-rl.org.
M. Böck and C. Heitzinger. Speedy categorical distributional reinforcement learning and complexity
analysis. SIAM Journal on Mathematics of Data Science, 4(2):675–693, 2022. doi: 10.1137/
20M1364436. URL https://doi.org/10.1137/20M1364436.
M. Böck, J. Malle, D. Pasterk, H. Kukina, R. Hasani, and C. Heitzinger. Superhuman performance
on sepsis mimic-iii data by distributional reinforcement learning. PLoS One, 17(11):e0275358,
2022.
V. I. Bogachev. Measure theory, volume 1. Springer, 2007.
W. Dabney, M. Rowland, M. Bellemare, and R. Munos. Distributional reinforcement learning with
quantile regression. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2018.
V. H. de la Pena. A general class of exponential inequalities for martingales and ratios. The Annals
of Probability, 27(1):537–564, 1999.
R. Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.
D. A. Freedman. On tail probabilities for martingales. The Annals of Probability, pages 100–118,
1975.
M. Gheshlaghi Azar, R. Munos, and H. J. Kappen. Minimax pac bounds on the sample complexity
of reinforcement learning with a generative model. Machine learning, 91:325–349, 2013.
E. Ghysels, P. Santa-Clara, and R. Valkanov. There is a risk-return trade-off after all. Journal of
ﬁnancial economics, 76(3):509–548, 2005.
J. K. Hunter and B. Nachtergaele. Applied analysis. World Scientiﬁc Publishing Company, 2001.
S. M. Kakade.
On the Sample Complexity of Reinforcement Learning.
PhD thesis, University
College London, 2003.
M. Kearns, Y. Mansour, and A. Y. Ng. A sparse sampling algorithm for near-optimal planning in
large markov decision processes. Machine learning, 49:193–208, 2002.
P. W. Lavori and R. Dawson. Dynamic treatment regimes: practical design considerations. Clinical
trials, 1(1):9–20, 2004.
G. Li, C. Cai, Y. Chen, Y. Wei, and Y. Chi. Is q-learning minimax optimal? a tight sample complexity
analysis. Operations Research, 72(1):222–236, 2024.
D. Martinez-Taboada and A. Ramdas. Empirical bernstein in smooth banach spaces. arXiv preprint
arXiv:2409.06060, 2024.
T. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, and T. Tanaka. Nonparametric return dis-
tribution approximation for reinforcement learning.
In Proceedings of the 27th International
Conference on Machine Learning (ICML-10), pages 799–806, 2010.
A. Pananjady and M. J. Wainwright. Instance-dependent ℓ∞-bounds for policy evaluation in tabular
reinforcement learning. IEEE Transactions on Information Theory, 67(1):566–585, 2020.
I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of
Probability, pages 1679–1706, 1994.
G. Pisier. Martingales in Banach Spaces. Cambridge Studies in Advanced Mathematics. Cambridge
University Press, 2016. doi: 10.1017/CBO9781316480588.
11

H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statis-
tics, pages 400–407, 1951.
M. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh. An analysis of categorical
distributional reinforcement learning. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 29–37. PMLR, 2018.
M. Rowland, R. Munos, M. G. Azar, Y. Tang, G. Ostrovski, A. Harutyunyan, K. Tuyls,
M.
G.
Bellemare,
and
W.
Dabney.
An
analysis
of
quantile
temporal-difference
learning.
Journal
of
Machine
Learning
Research,
25(163):1–47,
2024a.
URL
http://jmlr.org/papers/v25/23-0154.html.
M. Rowland, L. K. Wenliang, R. Munos, C. Lyle, Y. Tang, and W. Dabney. Near-minimax-optimal
distributional reinforcement learning with a generative model. arXiv preprint arXiv:2402.07598,
2024b.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9–44,
1988.
A. Talebi, G. Sadeghi, and M. Moslehian.
Freedman inequality in noncommutative probability
spaces. Complex Analysis and Operator Theory, 16(2):22, 2022.
P. Tarres and Y. Yao. Online learning as stochastic approximation of regularization paths: Optimality
and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716–5735, 2014.
J. Tropp.
Freedman’s inequality for matrix martingales.
Electronic Communications
in
Probability,
16(none):262
–
270,
2011.
doi:
10.1214/ECP.v16-1624.
URL
https://doi.org/10.1214/ECP.v16-1624.
C. Villani et al. Optimal transport: old and new, volume 338. Springer, 2009.
R. Wu, M. Uehara, and W. Sun. Distributional ofﬂine policy evaluation with predictive error guar-
antees. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors,
Proceedings of the 40th International Conference on Machine Learning, volume 202 of Pro-
ceedings of Machine Learning Research, pages 37685–37712. PMLR, 23–29 Jul 2023. URL
https://proceedings.mlr.press/v202/wu23s.html.
L. Zhang, Y. Peng, J. Liang, W. Yang, and Z. Zhang. Estimation and inference in distributional
reinforcement learning. arXiv preprint arXiv:2309.17262, 2023.
12

A
The Key Lemma: Freedman’s Inequality in Hilbert Spaces
Freedman’s inequality, proposed in [Freedman, 1975], can be viewed as a Bernstein’s inequality
for martingales, which is crucial for analyzing stochastic approximation algorithms. Compared
to the Azuma-Hoeffding inequality which only utilizes the boundedness of martingale difference
sequences, Freedman’s inequality incorporates second-order information, namely the quadratic vari-
ation (cumulative conditional variance) of martingales. This may leads to a sharper concentration
result. It has various generalizations, such as matrix Freedman’s inequality [Tropp, 2011]. However,
to the best of our knowledge, a Freedman’s inequality in Hilbert spaces has not been established yet.
Just as Freedman’s inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), it is
indispensable for deriving the minimax non-asymptotic convergence bound for distributional TD.
In this section, we will present a Freedman’s inequalities in Hilbert spaces. Firstly, we will state
a Hilbert space version of the original Freedman’s inequality (Theorem 1.6 in [Freedman, 1975]).
After that, we state a generalization of a more powerful version (Theorem 6 in [Li et al., 2024])
to Hilbert spaces. We will provide self-contained proofs in Appendix A.1, primarily inspired by
Theorem 3.2 in [Pinelis, 1994]. The necessary knowledge of martingale theory for the proofs can be
found in any standard textbook, such as [Durrett, 2019].
Let X be a Hilbert space, {Xi}n
i=1 be an X-valued martingale difference sequence adapted to the
ﬁltration {Fi}n
i=1, Yi := Pi
j=1 Xj be the corresponding martingale, Wi := Pi
j=1 σ2
j be the cor-
responding quadratic variation process. Here σ2
j := Ej−1 ∥Xj∥2, and Ei [·] := E [·|Fi] is the
conditional expectation.
Theorem A.1 (Freedman’s inequality in Hilbert spaces). Suppose maxi∈[n] ∥Xi∥≤b for some
constant b > 0. Then, for any ε, σ > 0, the following inequality holds
P
 ∃k ∈[n], s.t. ∥Yk∥≥ε and Wk ≤σ2
≤2 exp

−
ε2/2
σ2 + bε/3

.
(4)
Now, we are ready to state the generalization of Theorem 6 in [Li et al., 2024] to Hilbert spaces,
which is used in our non-asymptotic analysis.
Theorem A.2 (Freedman’s inequality in Hilbert spaces with bounded quadratic variation). Suppose
maxi∈[n] ∥Xi∥≤b and Wn ≤σ2 for some constants b, σ > 0 almost surely. Then, for any
δ ∈(0, 1), and any positive integer H ≥1, with probability at least 1 −δ, for all k ∈[n], the
following inequality holds
∥Yk∥≤
r
8 max
n
Wk, σ2
2H
o
log 2H
δ
+ 4
3b log 2H
δ .
(5)
The proof can be found in Appendix A.2.
Remark 2: Theorem 2.1 can be straightforwardly extended to the case where (∥Xi∥)n
i=1 satisﬁes
the Bernstein condition (Theorem 1.2A in [de la Pena, 1999]), thereby relaxing the boundedness
assumption on ∥Xi∥. Namely, Ei−1 ∥Xi∥k ≤
1
2k!σ2
i bk−2 for some b > 0, and for all i ∈[n],
k ∈{2, 3, · · ·}. In this case, Freedman’s inequality still holds, albeit with a worse constant.
P
 ∃k ∈[n], s.t. ∥Yk∥≥ε and Wk ≤σ2
≤2 exp

−ε2/2
σ2 + bε

.
(6)
The proof only requires making appropriate modiﬁcations after the ﬁfth line of Equation (12). Note
that Bernstein condition holds if maxi∈[n] ∥Xi∥≤b.
A.1
Proof of Theorem 2.1
Proof. For any λ > 0, t ∈[0, 1] and j ∈[n], let φ(t) = φj,λ(t) := Ej−1 cosh (λ ∥Yj−1 + tXj∥) =
Ej−1 cosh (λu(t)), where u(t) := ∥Yj−1 + tXj∥. We aim to use the Newton-Leibniz formula to es-
tablish the relationship between φ(1) = Ej−1 cosh (λ ∥Yj∥) and φ(0) = cosh (λ ∥Yj−1∥). This will
allow us to construct a positive supermartingale (Bi)n
i=0. By utilizing the positive supermartingale
and optional stopping theorem, we can derive the desired concentration inequality.
13

Firstly, we calculate the derivative of φ.
u′(t) = ⟨Yj−1 + tXj, Xj⟩
u(t)
,
(7)
φ′(t) = λEj−1 [sinh (λu(t)) u′(t)]
= λEj−1

sinh (λu(t)) ⟨Yj−1 + tXj, Xj⟩
u(t)

,
(8)
φ′(0) = λEj−1

sinh (λu(0)) ⟨Yj−1, Xj⟩
u(0)

= λ sinh (λ ∥Yj−1∥) ⟨Yj−1, Ej−1 [Xj]⟩
∥Yj−1∥
= 0.
(9)
By utilizing Newton-Leibniz formula, we have
φ(1) = φ(0) +
Z 1
0
φ′(s)ds
= φ(0) +
Z 1
0
Z s
0
φ′′(t)dtds
= φ(0) +
Z 1
0
(1 −t)φ′′(t)dt.
(10)
Now, we calculate the second order derivate of φ.
φ′′(t) = λEj−1
 d
dt [sinh (λu(t)) u′(t)]

= λEj−1
h
λ (u′(t))2 cosh (λu(t)) + u′′(t) sinh (λu(t))
i
≤λ2Ej−1
h
(u′(t))2 + u′′(t)u(t)

cosh (λu(t))
i
= λ2
2 Ej−1
h u2′′ (t) cosh (λu(t))
i
= λ2Ej−1
h
∥Xj∥2 cosh (λ ∥Yj−1 + tXj∥)
i
≤λ2 cosh (λ ∥Yj−1∥) Ej−1
h
∥Xj∥2 exp (λt ∥Xj∥)
i
,
(11)
where in the third line, we used u′′(t) =
∥Xj∥2u(t)−⟨Yj−1+tXj ,Xj⟩
u(t)
2
u2(t)
≥0 by Cauchy-Schwarz
inequality, and h(x) = x cosh(x) −sinh(x) ≥0 for any x ≥0, the inequality holds be-
cause h(0) = 0 and h′(x) = x sinh(x) ≥0 for any x ≥0.
In the fourth line, we used
 u2′′ (t) = 2

(u′(t))2 + u′′(t)u(t)

. In the ﬁfth line, we used
 u2′′ (t) = d2
dt2 ∥Yj−1 + tXj∥2 = d
dt (2 ⟨Yj−1 + tXj, Xj⟩) = 2 ∥Xj∥2 .
And in the last line, we used
cosh (λ ∥Yj−1 + tXj∥) ≤cosh (λ ∥Yj−1∥) exp (λt ∥Xj∥) ,
this holds since
exp (λ ∥Yj−1 + tXj∥) ≤exp {λ (∥Yj−1∥+ t ∥Xj∥)} = exp (λ ∥Yj−1∥) exp (λt ∥Xj∥) ,
exp (−λ ∥Yj−1 + tXj∥) ≤exp {−λ (∥Yj−1∥−t ∥−Xj∥)} = exp (−λ ∥Yj−1∥) exp (λt ∥Xj∥) .
14

Hence, we can derive the following inequality for all j ∈[n]
Ej−1 [cosh (λ ∥Yj∥)] = φ(1) = φ(0) +
Z 1
0
(1 −t)φ′′(t)dt
≤cosh (λ ∥Yj−1∥) + λ2 cosh (λ ∥Yj−1∥) Ej−1

∥Xj∥2
Z 1
0
(1 −t) exp (λt ∥Xj∥) dt

= cosh (λ ∥Yj−1∥) + λ2 cosh (λ ∥Yj−1∥) Ej−1
"
∥Xj∥2 exp (λ ∥Xj∥) −λ ∥Xj∥−1
λ2 ∥Xj∥2
#
=Ej−1 [exp (λ ∥Xj∥) −λ ∥Xj∥] cosh (λ ∥Yj−1∥)
=Ej−1
"
1 +
∞
X
k=0
1
(k + 2)! (λ ∥Xj∥)k+2
#
cosh (λ ∥Yj−1∥)
≤Ej−1
"
1 + λ2 ∥Xj∥2
2
∞
X
k=0
λb
3
k#
cosh (λ ∥Yj−1∥)
=
 
1 +
λ2σ2
j
2(1 −λb/3)
!
cosh (λ ∥Yj−1∥)
≤exp
(
λ2σ2
j
2(1 −λb/3)
)
cosh (λ ∥Yj−1∥) ,
(12)
which holds for any λ ∈(0, 3
b). In the ﬁfth line, we used Taylor expansion ex = P∞
k=0
xk
k! . In the
sixth line, we used (k + 2)! ≥2(3k) and ∥Xj∥≤b. In the seventh line, we used Taylor expansion
1
1−x = P∞
k=0 xk for x ∈(−1, 1).
Let B0 := 1, Bi := exp
n
−
λ2Wi
2(1−λb/3)
o
cosh (λ ∥Yi∥), then
Ei−1 [Bi] = exp

−
λ2Wi−1
2(1 −λb/3)

exp

−
λ2σ2
i
2(1 −λb/3)

Ei−1 [cosh (λ ∥Yi∥)]
≤exp

−
λ2Wi−1
2(1 −λb/3)

cosh (λ ∥Yi−1∥)
= Bi−1,
(13)
i.e., (Bi)n
i=0 is positive supermartingale. By optional stopping theorem (Theorem 4.8.4 in [Durrett,
2019]), for any stopping time τ, we have E [Bτ] ≤E [B0] = 1.
Let τ := inf {k ∈[n] : ∥Yk∥≥ε} be a stopping time, and inf ∅:= ∞. Deﬁne an event
A :=

∃k ∈[n], s.t. ∥Yk∥≥ε and Wk ≤σ2	
,
(14)
15

then on A, we have τ < ∞, ∥Yτ∥≥ε and Wτ ≤σ2, noting that Wk is non-decreasing with k. Our
goal is to provide an upper bound for P(A).
P(A) = E
p
Bτ
1
√Bτ
1(A)

≤
s
E [Bτ] E
 1
Bτ
1(A)

≤
v
u
u
u
tE


exp
n
λ2Wτ
2(1−λb/3)
o
cosh (λ ∥Yτ∥) 1(A)


≤
v
u
u
u
tE


exp
n
λ2σ2
2(1−λb/3)
o
cosh (λε)
1(A)


≤
s
2 exp

−λε +
λ2σ2
2(1 −λb/3)

P(A),
(15)
where in the second line, we used Cauchy-Schwarz inequality. In the third line, we used E [Bτ] ≤1.
In the fourth line, we used ∥Yτ∥≥ε and Wτ ≤σ2 on A, and cosh(x) is increasing when x ≥0. In
the last line, we used cosh(x) ≥1
2ex.
Hence for any λ ∈(0, 3
b)
P(A) ≤2 exp

−λε +
λ2σ2
2 (1 −λb/3)

,
(16)
we can choose λ⋆=
ε
σ2+εb/3 ∈(0, 3
b), then
P(A) ≤2 exp
(
−λ⋆ε +
(λ⋆)2 σ2
2 (1 −λ⋆b/3)
)
= 2 exp


−
ε2
σ2 + εb/3 +
σ2
2

1 −
εb/3
σ2+εb/3

ε2
(σ2 + εb/3)2



= 2 exp

−
ε2/2
σ2 + εb/3

,
(17)
which is the desired conclusion.
A.2
Proof of Theorem A.2
Proof. According to Theorem 2.1, for any ε, ˜σ > 0, we have
P
 ∃k ∈[n], ∥Yk∥≥ε and Wk ≤˜σ2
≤2 exp

−
ε2/2
˜σ2+bε/3

.
(18)
We can check that when ε =
q
4˜σ2 log 2
δ + 4
3b log 2
δ , the upper bound on RHS is less than δ. Hence,
P
 
∃k ∈[n], ∥Yk∥≥
r
4˜σ2 log 2
δ + 4
3b log 2
δ and Wk ≤˜σ2
!
≤δ.
(19)
16

For each k ∈[n], deﬁne the events
H(k)
H :=
(
∥Yk∥≥
r
8 max
n
Wk, σ2
2H
o
log 2H
δ
+ 4
3b log 2H
δ
)
,
B(k)
H,H :=
(
∥Yk∥≥
r
4 σ2
2H−1 log 2H
δ
+ 4
3b log 2H
δ
and Wk ≤
σ2
2H−1
)
,
B(k)
h,H :=
(
∥Yk∥≥
r
4 σ2
2h−1 log 2H
δ
+ 4
3b log 2H
δ
and σ2
2h ≤Wk ≤
σ2
2h−1
)
,
1 ≤h ≤H −1.
(20)
By the deﬁnition, we only need to show P
S
k∈[n] H(k)
H

≤δ. Since Wk ≤Wn ≤σ2 almost
surely, we can ﬁnd that H(k)
H
⊆S
h∈[H] B(k)
h,H (we will justify this later). Then S
k∈[n] H(k)
H
⊆
S
h∈[H]
S
k∈[n] B(k)
h,H.
By the inequality (19) with ˜σ2 =
σ2
2h−1 and setting δ as
δ
H , we have
P
S
k∈[n] B(k)
h,H

≤δ
H for all h ∈[H]. By the union bound, we can arrive at the conclusion:
P

[
k∈[n]
H(k)
H

≤
H
X
h=1
P

[
k∈[n]
B(k)
h,H

≤δ.
(21)
To justify H(k)
H ⊆S
h∈[H] B(k)
h,H, we can consider the decomposition
H(k)
H =
[
h∈[H]

H(k)
H ∩C(k)
h,H

,
(22)
where
C(k)
H,H :=

Wk ≤
σ2
2H−1

,
C(k)
h,H :=
σ2
2h ≤Wk ≤
σ2
2h−1

,
1 ≤h ≤H −1.
(23)
The decomposition holds because Wk ≤Wn ≤σ2 almost surely. We only need to show that for
each h ∈[H],
H(k)
H ∩C(k)
h,H ⊆B(k)
h,H.
(24)
On the event H(k)
H ∩C(k)
h,H, we have
∥Yk∥≥
r
8 max
n
Wk, σ2
2H
o
log 2H
δ
+ 4
3b log 2H
δ
≥
r
4 σ2
2h−1 log 2H
δ
+ 4
3b log 2H
δ ,
(25)
hence H(k)
H ∩C(k)
h,H ⊆B(k)
h,H.
B
Minimax Lower Bound of Distributional Policy Evaluation
In this section, we still consider inﬁnite-horizon tabular MDP deﬁned in Section 2, and assume a
generative model is accessible. For any positive integer D, we deﬁne M (D) as the set of all MDPs
with state space size |S| = D. For any MDP M and policy π, we denote V π
M as the corresponding
value function, and ηπ
M as the corresponding return distribution.
Now, we can state the minimax lower bound of the distributional policy evaluation task in the 1-
Wasserstein metric.
Theorem B.1 (Minimax lower bound of distributional policy evaluation in the 1-Wasserstein metric).
For any positive integer D ≥3, and sample size T ≥
C
1−γ log D
2 , the following result holds
inf
ˆη
sup
M∈M(D)
sup
π E
 ¯W1 (ˆη, ηπ
M)

≥
c
(1 −γ)3/2
s
log D
2
T
.
17

Here, c, C > 0 are universal constants, and the inﬁmum ˆη ∈PD ranges over all measurable
functions of T samples from the generative model.
The theorem states that for any algorithm, there exist corresponding MDP M and policy π, such that
to ensure E
 ¯W1 (ˆη, ηπ
M)

≤ε for some ε > 0, at least ˜Ω

1
ε2(1−γ)3

samples are required.
Proof of Theorem B.1. For any η ∈PD, we deﬁne V(η) ∈RD as the entry-wise expectation of η.
It is easy to check that V(ηπ
M) = V π
M. And recall the dual representation of 1-Wasserstein metric
(Corollary 5.16 in [Villani et al., 2009])
W1(µ, ν) =
sup
f : f is 1-Lipschitz
|EX∼µ [f(X)] −EY ∼ν [f(Y )]| ,
∀µ, ν ∈P,
(26)
we have ¯W1 (ˆη, ηπ
M) ≥∥V(ˆη) −V π
M∥∞. Hence
inf
ˆη
sup
M∈M(D)
sup
π E
 ¯W1 (ˆη, ηπ
M)

≥inf
ˆη
sup
M∈M(D)
sup
π E [∥V(ˆη) −V π
M∥∞]
≥inf
ˆV
sup
M∈M(D)
sup
π E
h ˆV −V π
M

∞
i
≥
c
(1 −γ)3/2
s
log D
2
T
,
(27)
where the second inequality holds because V (ˆη) ∈RD is also a measurable function of T sam-
ples from the generative model, and the inﬁmum ˆV ∈RD ranges over all measurable functions
of T samples from the generative model.
And the last inequality is due to Theorem 2(b) in
[Pananjady and Wainwright, 2020].
C
Omitted Proofs in Section 5
C.1
Remove the Dependence on T for Step Sizes
We have shown that the conclusion holds for
T ≥C4 log3 T
ε2(1 −γ)3 log |S| T
δ
,
(28)
1
1 + c5(1−√γ)T
log2 T
≤αt ≤
1
1 + c6(1−√γ)t
log2 T
,
(29)
where c5c6 ≤1
8, c5 > c6 > 0 and C4 > 0.
Then for some c2 > c3 > 0 to be determined, now we assume
1
1 + c2(1−√γ)t
log2 t
≤αt ≤
1
1 + c3(1−√γ)t
log2 t
.
(30)
Next, we will show that if we consider the result of the T
2 -th iteration with this step size scheme
as the initialization of a new iteration process, then the step sizes in the subsequent T
2 iterations lie
in the previously established range. If this is done, the conclusion still holds if we choose T ≥
2C4 log3 T
ε2(1−γ)3 log |S|T
δ , since the initialization ηπ
T/2 ∈PS (or PS
K in the case of CTD) is independent
of the samples obtained for T
2 < t ≤T.
For any T
2 < t ≤T, we denote τ := t −T
2 , we can see that there exist c2 > c3 > 0, such that the
last inequality in both of the following lines hold simultaneously, which is desired.
˜ατ := αt ≤
1
1 + c3(1−√γ)(τ+T/2)
log2(τ+T/2)
≤
1
1 + c3(1−√γ)τ
log2 T
≤
1
1 + c6(1−√γ)τ
log2(T/2)
,
(31)
and
˜ατ = αt ≥
1
1 + c2(1−√γ)(τ+T/2)
log2(τ+T/2)
≥
1
1 + 2c2(1−√γ)T/2
log2(T/2)
≥
1
1 + c5(1−√γ)T/2
log2(T/2)
.
(32)
18

C.2
Range of Step Size
Proof of Lemma 5.1.
(1 −√γ)αt ≥
1 −√γ
1 + c5(1−√γ)T
log2 T
≥
1 −√γ
2c5(1−√γ)T
log2 T
= log2 T
2c5T .
(33)
For any 0 ≤k ≤t
2,
β(t)
k
≤

1 −αt/2(1 −√γ)
t/2
≤

1 −log2 T
2c5T
t/2
≤

1 −log2 T
2c5T

T
2c6 log T
=




1 −log2 T
2c5T
 2c5T
log2 T



log T
4c5c6
≤1
T 2 ,
(34)
where in the last inequality, we used c5c6 ≤1
8.
And for any t
2 < k ≤t,
β(t)
k
≤αk ≤
1
c6(1−√γ)k
log2 T
≤
2 log3 T
(1 −√γ)T .
(35)
C.3
Concentration of the Martingale Term
Proof of Lemma 5.2. We will show that the inequality holds for each t ≥
T
c6 log T and then apply the
union bound. For any s ∈S, we denote
ζk(s) := ζ(t)
k (s) = αk
(
tY
i=k+1
[(1 −αi)I + αiT ] (Tk −T ) ηk−1
)
(s),
(36)
where we omit the superscript (t) for brevity, then LHS in the lemma equals

Pt
k=1 ζk
 for each
t. Let Fk denote the σ-ﬁeld that contains all information up to time step k, then {ζk(s)}t
k=1 is a
{Fk}t
k=1-martingale difference sequence:
Ek−1 [ζk(s)] = αk
(
tY
i=k+1
[(1 −αi)I + αiT ] Ek−1 [(Tk −T ) ηk−1]
)
(s) = 0.
(37)
the ﬁrst equality holds because a Bochner integral can be exchanged with a bounded linear operator
(see Pisier [2016] for more details about Bochner integral), and the second equality holds due to the
deﬁnition of the empirical distributional Bellman operator.
We hope to use Freedman’s inequality (Theorem A.2) to bound this martingale. To this end, we need
to give a deterministic upper bound of the martingale difference sequence, and an upper bound of
its quadratic variation.
19

Deterministic upper bound of maxk∈[t] ∥ζk(s)∥.
The norm of the martingale difference ∥ζk(s)∥
can be bounded as follow
∥ζk(s)∥≤∥ζk∥
≤αk

tY
i=k+1
[(1 −αi)I + αiT ]
 ∥(Tk −T ) ηk−1∥
≤αk
tY
i=k+1
((1 −αi) + αi
√γ)
1
√1 −γ
=
β(t)
k
√1 −γ .
(38)
Hence, maxk∈[t] ∥ζk(s)∥≤
maxk∈[t] β(t)
k
√1−γ
≤
1
√1−γ max
n
1
T 2 ,
2 log3 T
(1−√γ)T
o
≤
4 log3 T
(1−γ)3/2T =: b.
Upper bound of quadratic variation.
Now, let’s calculate the quadratic variation.
We ﬁrst introduce some notations. For any k ∈N, we denote Var(ξ) :=

E
h
∥ξ(s)∥2i
s∈S ∈RS,
Vark(ξ) :=

Ek
h
∥ξ(s)∥2i
s∈S ∈RS for any random element ξ in MS.
For any ξ ∈MS, we deﬁne its one-step update Cramér variation as σ(ξ) := Var

(bT −T )ξ

∈
RS, where bT is a random operator and has the same distribution as T1.
For any x, y ∈RS, we say x ≤y if x(s) ≤y(s) for all s ∈S.
In this part, ∥x∥:=
∥x∥∞= maxs∈S |x(s)|, √x :=
p
x(s)

s∈S. And for any U ∈RS×S, ∥U∥:= ∥U∥∞=
supx∈RS,∥x∥=1 ∥Ux∥= maxs∈S
P
s′∈S |U(s, s′)|.
For any {xk}n
k=1 ⊂RS, we denote maxk∈[n] xk as
 maxk∈[n] xk(s)

s∈S.
We denote I ∈RS×S as the identity matrix, 1 ∈RS as the all-ones vector, and P := P π ∈RS×S,
i.e., P (s, s′) := P π(s′|s) = P
a∈A π(a|s)P(s′|s, a).
With these notations, the quadratic variation is Wt := Pt
k=1 Vark−1 (ζk). To bound the quadratic
variation Wt, we need to bound Vark−1 (ζk).
Lemma C.1.
Vark−1 (ζk) ≤αkβ(t)
k
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1).
20

Hence, the quadratic variation Wt can be bounded as follow
Wt =
t
X
k=1
Vart−1 (ζk)
≤
t
X
k=1
αkβ(t)
k
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1)
≤
t/2
X
k=1
αkβ(t)
k

tY
i=k+1
[(1 −αi)I + αi
√γP ]
 ∥σ(ηk−1)∥1 +
t
X
k=t/2+1
αkβ(t)
k
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1)
≤
t/2
X
k=1

β(t)
k
2
1
1 −γ 1 +

max
k: t/2<k≤t β(t)
k

t
X
k=t/2+1
αk
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1)
≤
1
2(1 −γ)T 3 1 +
2 log3 T
(1 −√γ)T



t
X
k=t/2+1
αk
tY
i=k+1
[(1 −αi)I + αi
√γP ]



max
k: t/2<k≤t σ(ηk−1)
≤
1
2(1 −γ)T 3 1 + 4 log3 T
(1 −γ)T (I −√γP )−1
max
k: t/2<k≤t σ(ηk−1),
(39)
where in the fourth line, we used
αk

tY
i=k+1
[(1 −αi)I + αi
√γP ]
 ≤αk
tY
i=k+1
[(1 −αi) + αi
√γ] = β(t)
k ,
and
∥σ(ηk−1)∥≤
Z
1
1−γ
0
dx =
1
1 −γ .
In the last line, we used the fact that maxk: t/2≤k<t σ(ηk−1) ≥0 and the following lemma:
Lemma C.2. For any t ∈N, (αi)i∈[t] ∈[0, 1]t, the following inequality holds entry-wise:
t
X
k=t/2+1
αk
tY
i=k+1
[I −αi (I −√γP )] ≤(I −√γP )−1.
(40)
According to (39), we have the following deterministic upper bound for ∥Wt∥= maxs∈S Wt(s),
∥Wt∥≤
1
2(1 −γ)T 3 + 4 log3 T
(1 −γ)T
(I −√γP )−1
max
k: t/2<k<≤t ∥σ(ηk−1)∥
≤
1
2(1 −γ)T 3 +
8 log3 T
(1 −γ)3T
≤
9 log3 T
(1 −γ)3T
=: σ2.
(41)
Let H =

2 log2
1
1−γ

, we have
σ2
2H ≤9 log3 T
(1 −γ)T .
(42)
21

By applying Freedman’s inequality (Theorem A.2) and utilizing the union bound over s ∈S, we
obtain with probability at least 1 −δ, for all t ∈[T] and s ∈S
 
t
X
k=1
ζk(s)

!
s∈S
≤
s
8

Wt + σ2
2H 1

log
8|S|T log
1
1−γ
δ
+ 4
3b log
8|S|T log
1
1−γ
δ
1
≤
s
16

Wt + 9 log3 T
(1 −γ)T 1

log |S|T
δ
+ 3b log |S|T
δ
1
≤8
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)T

(I −√γP )−1
max
k: t/2<k≤t σ(ηk−1) + 3 · 1

+
12
 log3 T
 
log |S|T
δ

(1 −γ)3/2T
1,
(43)
where we used log
8|S|T log
1
1−γ
δ
≤2 log |S|T
δ
in the second line, which holds due to the choice of T.
The following lemmas are required for deriving the upper bound, which hold for both cases of NTD
and CTD.
Lemma C.3. For any t ∈[T],
σ(ηt) −σ(η) ≤4 ∥∆t∥¯
W1 1.
Lemma C.4.
(I −√γP )−1σ(η) ≤
4
1 −γ 1.
Combining the upper bound with the two lemmas, we get the desired conclusion
 
t
X
k=1
ζk(s)

!
s∈S
≤8
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)T

4
max
k: t/2<k≤t ∥∆k−1∥¯
W1 (I −√γP )−11 +
8
1 −γ 1

+
12
 log3 T
 
log |S|T
δ

(1 −γ)3/2T
1
≤22
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)2T

1 +
max
k: t/2<k≤t ∥∆k−1∥¯
W1

1 +
12
 log3 T
 
log |S|T
δ

(1 −γ)3/2T
1
≤34
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)2T

1 +
max
k: t/2<k≤t ∥∆k−1∥¯
W1

1,
(44)
where in the last line, we used that, excluding the constant term, the ﬁrst term is larger than the
second term, given the choice of T ≥C4 log3 T
ε2(1−γ)3 log |S|T
δ .
C.4
Solve the Recurrence Relation
Theorem C.1. Suppose for all t ≥
T
c6 log T ,
∥∆t∥¯
W1 ≤35
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T

1 +
max
k: t/2<k≤t ∥∆k−1∥¯
W1

.
Then there exists some large universal constant C7 > 0, such that
∥∆T ∥¯
W1 ≤C7



v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T
+
 log3 T
 
log |S|T
δ

(1 −γ)3T


.
22

Proof. For any k ≥0, we denote
uk := max

∥∆t∥¯
W1
 2k
T
c6 log T ≤t ≤T

,
(45)
for 0 ≤k ≤log2 (c6 log T). We can see that ∥∆T ∥¯
W1 ≤uk for any valid k. Hence, it sufﬁces to
show the upper bound holds for uk for any valid k. It can be veriﬁed that u0 ≤
1
1−γ , and for k ≥0
uk+1 ≤35
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T
(1 + uk).
(46)
We ﬁrst show that once uk ≤1, the subsequent values of uk+l will also remain upper bounded by 1.
Namely, if uk ≤1 for some k ≥1, then
uk+1 ≤35
v
u
u
t2
 log3 T
 
log |S|T
δ

(1 −γ)3T
≤1,
(47)
if T ≥2450 log3 T log |S|T
δ
(1−γ)3
.
Let τ := inf {k : uk ≤1}, then for any k > τ, we have
uk ≤35
v
u
u
t2
 log3 T
 
log |S|T
δ

(1 −γ)3T
=: a.
(48)
For k ≤τ, we have uk ≥1 and thereby
uk+1 ≤35
v
u
u
t2
 log3 T
 
log |S|T
δ

(1 −γ)3T
uk = a√uk,
(49)
i.e.,
log uk+1 −2 log a ≤1
2 (log uk −2 log a) .
(50)
Apply it recursively, we have
log uk+1 ≤2 log a +
1
2
k+1
(log u0 −2 log a) ,
(51)
i.e.,
uk+1 ≤a2 u0
a2
1/2k
= a2(1−1/2k)u1/2k
0
≤a2(1−1/2k)
1
(1 −γ)1/2k .
(52)
To sum up, for any k ≥0, uk+1 is always less than the sum of the upper bounds in cases of k > τ
and k ≤τ,
uk+1 ≤a + a2(1−1/2k)
1
(1 −γ)1/2k
(53)
Note that, a2(1−1/2k) ≤max {a, √a}, and if we take k ≥c8 log log
1
1−γ for any constant c8, we
have
1
(1−γ)1/2k = O(1). We can take the constant c8 small enough such that c8 log log
1
1−γ <
log2 (c6 log T) (this can be done and c8 is universal since
1
1−γ = o(T)), and thereby we can ﬁnd a
valid k⋆≥c8 log log
1
1−γ + 1. Then
∥∆T ∥¯
W1 ≤uk⋆≤C7



v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)3T
+
 log3 T
 
log |S|T
δ

(1 −γ)3T


,
(54)
which is the desired conclusion, and C7 is some large universal constant related to c8.
23

C.5
Analysis of Corollaries 4.1 and 4.2
The difference in the proof compared to Section 5.2 arises in Lemma 5.2 when we control term (II).
Now we further bound the result in Lemma C.3 by the Cramér norm of the error term,
σ(ηt) −σ(η) ≤4 ∥∆t∥¯
W1 1 ≤
1
√1 −γ ∥∆t∥1.
(55)
In the same way, we can derive the following recurrence relation: with probability at least 1 −δ, for
all t ≥
T
c6 log T
∥∆t∥≤35
v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)5/2T

1 +
max
k: t/2<k≤t ∥∆k−1∥

.
(56)
By repeating the reasoning of Theorem C.1, we can obtain the desired conclusion,
∥∆T ∥≤C7



v
u
u
t
 log3 T
 
log |S|T
δ

(1 −γ)5/2T
+
 log3 T
 
log |S|T
δ

(1 −γ)5/2T


,
(57)
which is less than ε if we take C4 ≥2C2
7 and T ≥
C4 log3 T
ε2(1−γ)5/2 log |S|T
δ . Here, C7 > 1 is a large
universal constant depending on c6.
C.6
Proof of Lemma C.1
Proof. We ﬁrst introduce some notations. For any matrix of operators U ∈L (M)S×S, we denote
U(s) = (U(s, s′))s′∈S ∈L (M)S as the s-row of U. And for any ξ ∈MS, we deﬁne the vector
inner product operation U(s)ξ := P
s′∈S U(s, s′)ξ(s′) ∈M.
We need the following lemma, which holds for both cases of NTD and CTD.
Lemma C.5. For any ν ∈M, n ∈N, (αi)i∈[n] ∈[0, 1]n, let Un = Qn
i=1 [(1 −αi)I + αiT ],
Un = Qn
i=1

(1 −αi)I + αi√γP

, un = Qn
i=1

(1 −αi) + αi√γ

then for any s, s′ ∈S, we
have
∥Un(s, s′)ν∥2 ≤unUn(s, s′) ∥ν∥2 .
Utilizing this lemma, we get the following result. Recall that bT is a random operator and has the
same distribution as T1. Then, for any non-random ξ ∈MS,
E
Un(s)(bT −T )ξ

2
=E



X
s′∈S
Un(s, s′)
h
(bT −T )ξ
i
(s′)

2

=E



X
s′∈S
Un(s, s′)
h
bT (s′)ξ −T (s′)ξ
i
2

=
X
s′∈S
E
Un(s, s′)
h
bT (s′)ξ −T (s′)ξ
i
2
≤un
X
s′∈S
Un(s, s′)E
 bT (s′)ξ −T (s′)ξ

2
=un
X
s′∈S
Un(s, s′)σ(ξ)(s′)
=unUn(s)σ(ξ),
(58)
24

where we used different rows of bT are independent, and bT (s′)ξ is an unbiased estimator of T (s′)ξ ∈
M. Hence, Var

Un(bT −T )ξ

≤unUnσ(ξ).
Now, we are ready to bound Vark−1 (ζk)
Vark−1 (ζk) = α2
kVark−1
 
tY
i=k+1
[(1 −αi)I + αiT ] (Tk −T ) ηk−1
!
≤α2
k
tY
i=k+1
[(1 −αi) + αi
√γ]
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1)
= αkβ(t)
k
tY
i=k+1
[(1 −αi)I + αi
√γP ] σ(ηk−1).
(59)
C.7
Proof of Lemma C.2
Proof.
t
X
k=t/2+1
αk
tY
i=k+1
[(1 −αi)I + αi
√γP ]
=
t
X
k=t/2+1
tY
i=k+1
[(1 −αi)I + αi
√γP ] αk(I −√γP )(I −√γP )−1
=
t
X
k=t/2+1
(
tY
i=k+1
[(1 −αi)I + αi
√γP ] −
tY
i=k
[(1 −αi)I + αi
√γP ]
)
(I −√γP )−1
=


I −
tY
i=t/2+1
[(1 −αi)I + αi
√γP ]


(I −√γP )−1
≤(I −√γP )−1,
(60)
where the inequality holds entry-wise since we can verify that all entries of (I −√γP )−1 =
P∞
k=0
 √γP
k and (1 −αi)I + αi√γP are non-negative.
C.8
Proof of Lemma C.3
Proof. For any s ∈S,
σ(ηt)(s) −σ(η)(s)
=
Z
1
1−γ
0

E

F 2
( b
T ηt)(s)(x)

−F 2
(T ηt)(s)(x) −E

F 2
( b
T η)(s)(x)

+ F 2
(T η)(s)(x)

dx
=
Z
1
1−γ
0

E

F 2
( b
T ηt)(s)(x) −F 2
( b
T η)(s)(x)

+ F 2
(T η)(s)(x) −F 2
(T ηt)(s)(x)

dx
=
Z
1
1−γ
0
n
E
h
F( b
T ηt)(s)(x) −F( b
T η)(s)(x)
 
F( b
T ηt)(s)(x) + F( b
T η)(s)(x)
i
+
 F(T η)(s)(x) −F(T ηt)(s)(x)
  F(T η)(s)(x) + F(T ηt)(s)(x)
 o
dx
≤2
Z
1
1−γ
0
n
E
hF( b
T ηt)(s)(x) −F( b
T η)(s)(x)

i
+
F(T η)(s)(x) −F(T ηt)(s)(x)

o
dx
=2

E
bT (ηt −η) (s)

W1

+ ∥T (ηt −η) (s)∥W1

.
(61)
25

In the case of NTD, T and bT are γ-contraction w.r.t. the supreme 1-Wasserstein metric, hence
σ(ηt)(s) −σ(η)(s) ≤2

E
bT (ηt −η) (s)

W1

+ ∥T (ηt −η) (s)∥W1

≤4γ ∥ηt −η∥¯
W1
≤4 ∥∆t∥¯
W1 .
(62)
In the case of CTD, if we can show ΠK is non-expansive w.r.t. 1-Wasserstein metric, the conclusion
still holds. For any x, y ∈
h
0,
1
1−γ
i
such that x < y, we denote x ∈[xk, xk+1) and y ∈[xl, xl+1),
then k ≤l, by the deﬁnition of ΠK, we have
ΠK(δx) = xk+1 −y
ιK
δxk + y −xk
ιK
δxk+1,
(63)
ΠK(δy) = xl+1 −y
ιK
δxl + y −xl
ιK
δxl+1.
(64)
If k = l, we can check that W1 (ΠKδx, ΠKδy) = ιK
y−x
ιK
= y −x.
If k < l, we have
W1 (ΠKδx, ΠKδy) ≤W1 (ΠKδx, xk+1) + W1 (xk+1, xl) + W1 (xl, ΠKδy) = (xk+1 −x) + (xl −
xk+1) + (y −xxl) = y −x. Hence, for any ν1, ν2 ∈P and for any transport plan κ ∈Γ(ν1, ν2),
the previous results tell us the cost of the transport plan ΠKκ ∈Γ (ΠKν1, ΠKν2) induced by ΠK
is no greater than the cost of κ. Consequently, W1 (ΠKν1, ΠKν2) ≤W1(ν1, ν2), i.e., ΠK is non-
expansive w.r.t. 1-Wasserstein metric, which is desired.
C.9
Proof of Lemma C.4
Proof. Firstly, we show that for any v ≥0, we have
(I −√γP )−1v
 ≤2
(I −γP )−1v

(I −√γP )−1v
 =
(I −√γP )−1(I −γP )(I −γP )−1v

=
(I −√γP )−1 [(1 −√γ)I + √γ(I −√γP )] (I −γP )−1v

=

(1 −√γ)(I −√γP )−1 + √γI

(I −γP )−1v

≤(1 −√γ)
(I −√γP )−1(I −γP )−1v
 + √γ
(I −γP )−1v

≤
1 −√γ
1 −√γ + √γ
 (I −γP )−1v

≤2
(I −γP )−1v
 .
(65)
In the case of NTD, by Corollary D.1, we have
(I −γP )−1σ (η)
 ≤
1
1 −γ ,
(66)
In the case of CTD, by Corollary 5.12 in [Rowland et al., 2024b], we have
(I −γP )−1σ (η)
 ≤
2
1 −γ ,
(67)
given K >
4
1−γ .
C.10
Proof of Lemma C.5
Proof. We proof this result by induction. For n = 0, we have U0 = I, U0 = I, u0 = 1, thereby the
inequality holds trivially. Suppose the inequality holds true for n −1. To prove that the inequality
holds for n, it is sufﬁcient to show that, for any µ ∈M,
∥[(1 −αn)δs,s′ + αnT (s, s′)] µ∥2 ≤[(1 −αn) + αn
√γ] [(1 −αn)δs,s′ + αn
√γP (s, s′)] ∥µ∥2 ,
where δs,s′ = 1 if s = s′, and 0 otherwise.
26

LHS can be bounded as follow
∥[(1 −αn)δs,s′ + αnT (s, s′)] µ∥2
=(1 −αn)2δs,s′ ∥µ∥2 + 2(1 −αn)αnδs,s′ ⟨µ, T (s, s′)µ⟩+ α2
n ∥T (s, s′)µ∥2
≤(1 −αn)2δs,s′ ∥µ∥2 + 2(1 −αn)αnδs,s′ ∥µ∥∥T (s, s′)µ∥+ α2
n ∥T (s, s′)µ∥2 ,
(68)
where we used Cauchy-Schwarz inequality. We need to give an upper bound for ∥T (s, s′)µ∥2.
Note that (ΠKT π) (s, s′) = ΠK (T π(s, s′)) and ∥ΠK∥= 1, we only need to consider the case of
NTD, by the deﬁnition of T (s, s′), we have
∥T (s, s′)µ∥2 =
Z
1
1−γ
0
"X
a∈A
π(a|s)P(s′|s, a)
Z 1
0
Fµ
x −r
γ

PR(dr|s, a)
#2
dx
= P (s, s′)2
Z
1
1−γ
0
"X
a∈A
π(a|s)P(s′|s, a)
P (s, s′)
Z 1
0
Fµ
x −r
γ

PR(dr|s, a)
#2
dx
= P (s, s′)2
Z
1
1−γ
0

Ea∼π(·|s),r∼PR(·|s,a)

Fµ
x −r
γ
 s′
2
dx
≤P (s, s′)2Ea∼π(·|s),r∼PR(·|s,a)
(Z
1
1−γ
0

Fµ
x −r
γ
2
dx
s′
)
= γP (s, s′)2 ∥µ∥2 ,
(69)
where we used Jensen’s inequality and Fubini’s theorem. Substitute it back to the upper bound,
∥[(1 −αn)δs,s′ + αnT (s, s′)] µ∥2
≤(1 −αn)2δs,s′ ∥µ∥2 + 2(1 −αn)αnδs,s′ ∥µ∥∥T (s, s′)µ∥+ α2
n ∥T (s, s′)µ∥2
≤

(1 −αn)2δs,s′ + 2(1 −αn)αnδs,s′√γP (s, s′) + α2
nγP (s, s′)2
∥µ∥2
=

(1 −αn)2δs,s′ + αn
√γP (s, s′)
2 ∥µ∥2
≤[(1 −αn) + αn
√γ] [(1 −αn)δs,s′ + αn
√γP (s, s′)] ∥µ∥2 ,
(70)
which is desired.
D
Stochastic Distributional Bellman Equation and Operator
In this section, we use the same notations as in Appendix C and only consider the NTD setting.
Inspired by stochastic categorical CDF Bellman operator introduced in [Rowland et al., 2024b], we
introduce stochastic distributional Bellman operator T : ∆
 PS
→∆
 PS
to derive an upper
bound for
(I −γP )−1σ(η)
 in the case of NTD. For any φ ∈∆
 PS
, we denote ηφ be the
random element in PS with law φ.
T φ := Law

bT ηφ

,
(71)
where (bT ηφ)(ω) := (bT )(ω)(ηφ)(ω) ∈PS for any ω ∈Ω, Ωis the corresponding probability
space, and bT is independent of ηφ. In this part, bT does not consist of ΠK since we only consider
the NTD setting.
We consider the 1-Wasserstein metric W1 on ∆
 PS
, the space of all probability measures on the
space
 PS, ¯ℓ2

. Since
 PS, ¯ℓ2

is Polish (complete and separable), the space
 ∆
 PS
, W1

is
also Polish (Theorem 6.18 in [Villani et al., 2009]).
Proposition D.1. The stochastic distributional Bellman operator T
is a √γ-contraction on
∆
 PS
, i.e., for any φ, φ′ ∈∆
 PS
, we have
W1 (T φ, T φ′) ≤√γW1 (φ, φ′) .
27

Proof. Let κ⋆∈Γ (φ, φ′) be the optimal coupling between φ and φ′. The existence of κ⋆is
guaranteed by Theorem 4.1 in [Villani et al., 2009]. And let the random element ξ = (ξ1, ξ2) in
 PS2 has the law κ⋆, where ξ1 and ξ2 are both random elements in PS. We denote T κ⋆:=
Law
h
bT ξ1, bT ξ2
i
∈Γ (T φ, T φ′).
W1 (T φ, T φ′) =
inf
κ∈Γ(T φ,T φ′)
Z
(PS)2
¯ℓ2 (ξ, ξ′) κ (dξ, dξ′)
≤
Z
(PS)2
¯ℓ2 (ξ, ξ′) T κ⋆(dξ, dξ′)
= E
h
¯ℓ2

bT ξ1, bT ξ2
i
≤√γE
¯ℓ2 (ξ1, ξ2)

= √γ
Z
(PS)2
¯ℓ2 (ξ, ξ′) κ⋆(dξ, dξ′)
= √γ
inf
κ∈Γ(φ,φ′)
Z
(PS)2
¯ℓ2 (ξ, ξ′) κ (dξ, dξ′)
= √γW1 (φ, φ′) .
(72)
By the proposition and contraction mapping theorem, there exists a unique ﬁxed point of T , we
denote ψ ∈∆
 PS
as the ﬁxed point. Hence, the stochastic distributional Bellman equation reads
ψ = T ψ.
(73)
We denote ηψ as the random element in P with law ψ, then bT ηψ and ηψ have the same law. As
shown in the following proposition, ηψ can be regarded as a noisy version of η.
Proposition D.2.
E [ηψ] = η,
where the expectation is regarded as the Bochner integral in the space of all ﬁnite measures on PS,
which is a normed linear space equipped with Cramér metric as its norm.
Proof.
E [ηψ] = E
h
bT ηψ
i
= E
n
E
h
bT ηψ
ηψ
io
= E [T ηψ]
= T E [ηψ] ,
(74)
where we used bT is independent of ηψ. Since E [ηψ] is the ﬁxed point of T , we have E [ηψ] =
η.
Based on the concepts of T and ψ, we can obtain the following second order distributional
Bellman equation, which is similar to the classic second-order Bellman equation (Lemma 7 in
[Gheshlaghi Azar et al., 2013]).
Recall the one-step Cramér variation σ(η) =

E


bT η

(s) −η(s)

2
s∈S
∈RS used in the
NTD setting. We denote σ := σ(η) for simplicity, and Σ :=

E
h
∥ηψ(s) −η(s)∥2i
s∈S ∈RS.
Proposition D.3 (Second order distributional Bellman equation).
Σ = σ + γP Σ.
28

Proof. For any s ∈S,
Σ(s) = E
h
∥ηψ(s) −η(s)∥2i
= E


bT ηψ

(s) −η(s)

2
= E


bT ηψ

(s) −

bT η

(s) +

bT η

(s) −η(s)

2
= E


bT η

(s) −η(s)

2
+ E


bT ηψ

(s) −

bT η

(s)

2
,
(75)
where the last equality holds since the cross term is zero as below
E
hD
bT η

(s) −η(s),

bT ηψ

(s) −

bT η

(s)
Ei
=E
n
E
hD
bT η

(s) −η(s),

bT ηψ

(s) −

bT η

(s)
E bT
io
=E
nD
bT η

(s) −η(s), E
h
bT ηψ

(s)
 bT
i
−

bT η

(s)
Eo
=E
hD
bT η

(s) −η(s), 0
Ei
=0.
(76)
The ﬁrst term in (75) is σ(s), we need to deal with the second term.
E


bT ηψ

(s) −

bT η

(s)

2
=E

E


bT ηψ

(s) −

bT η

(s)

2 ηψ

=E
(
Ea(s)∼π(·|s),s′(s)∼P (·|s,a(s)),r(s)∼PR(·|s,a(s))
"Z
1
1−γ
0

F(ηψ)(s′(s))
x −r
γ

−Fη(s′(s))
x −r
γ
2
dx
ηψ
#)
=γ
X
s′∈S
E
h
∥ηψ(s′) −η(s′)∥2i X
a∈A
π(a|s)P(s′|s, a)
=γ
X
s′∈S
P (s, s′)Σ(s′).
(77)
Put these together, and we can arrive at the conclusion.
Now, we can derive a tighter upper bound for
(I −γP )−1σ(η)
.
Corollary D.1.
(I −γP )−1σ(η)
 ≤
(I −γP )−1σ
 = ∥Σ∥≤
1
1 −γ .
Proof. Note that all entries of (I−γP )−1 = P∞
k=0 (γP )k are positive, thereby (I−γP )−1σ(η) ≤
(I −γP )−1σ = Σ, and Σ(s) = E
h
∥ηψ(s) −η(s)∥2i
≤
R
1
1−γ
0
dx =
1
1−γ for any s ∈S.
E
Other Technical Lemmas
Lemma E.1 (Basic Inequalities for Metrics on the Space of Probability Measures). For any
ν1, ν2
∈P, we have (ℓ2(ν1, ν2))2
≤W1(ν1, ν2) ≤
1
√1−γ ℓ2(ν1, ν2) and Wp(ν1, ν2) ≤
1
(1−γ)
1−1
p W
1
p
1 (ν1, ν2).
29

Proof. By Cauchy-Schwarz inequality,
W1(ν1, ν2)
=
Z
1
1−γ
0
|Fν1(x) −Fν2(x)|dx
≤
sZ
1
1−γ
0
12dx
sZ
1
1−γ
0
|Fν1(x) −Fν2(x)|2dx
=
1
√1 −γ ℓ2(ν1, ν2).
And
ℓ2(ν1, ν2)
=
sZ
1
1−γ
0
|Fν1(x) −Fν2(x)|2dx
≤
sZ
1
1−γ
0
|Fν1(x) −Fν2(x)|dx
=
p
W1(ν1, ν2).
And
Wp(ν1, ν2)
=
 
inf
κ∈Γ(ν1,ν2)
Z
[0,
1
1−γ ]
2 |x −y|p κ(dx, dy)
!1/p
≤
1
(1 −γ)1−1
p
 
inf
κ∈Γ(ν1,ν2)
Z
[0,
1
1−γ ]
2 |x −y| κ(dx, dy)
!1/p
=
1
(1 −γ)1−1
p W
1
p
1 (ν1, ν2).
Lemma E.2.
 M, ∥·∥ℓ2

and
 M, ∥·∥W1

are separable.
Proof. Recall that (P, W1) is separable [Theorem 6.18 Villani et al., 2009], and by Lemma E.1, the
Cramér distance ℓ2 can be bounded by 1-Wasserstein distance W1, hence (P, ℓ2) is also separable.
Let d be either W1 or ℓ2, and A be the countable dense subset of (P, d). For any ǫ > 0, µ ∈M,
we denote ˜µ :=
2µ
|µ|(R). Then we can ﬁnd a q ∈Q, ¯µ+, ¯µ−∈A, s.t.
q −1
2 |µ| (R)
 ≤
ǫ|µ|(R)
6∥µ∥d ,
d(¯µ+, ˜µ+) ≤
ǫ
3q, d(¯µ−, ˜µ−) ≤
ǫ
3q. Note that the set of all possible ¯µ = ¯µ+ −¯µ−is countable. Let
ˆµ := q¯µ, then we have
∥µ −ˆµ∥d
≤∥µ −q˜µ∥d + ∥q˜µ −ˆµ∥d
=
(1
2 |µ| (R) −q)˜µ

d
+ q ∥(˜µ+ −¯µ+) −(˜µ−−¯µ−)∥d
≤
q −1
2 |µ| (R)

2 ∥µ∥d
|µ| (R) + q ∥˜µ+ −¯µ+∥d + q ∥˜µ−−¯µ−∥d
≤ǫ |µ| (R)
6 ∥µ∥d
2 ∥µ∥d
|µ| (R) + q ǫ
3q + q ǫ
3q
=ǫ.
Hence we have found a countable dense subset for (M, ∥·∥d) for d = W1 or ℓ2.
Therefore,
 M, ∥·∥ℓ2

and
 M, ∥·∥W1

are separable.
30

Lemma E.3.
 M, ∥·∥W1

is not complete.
Proof. Consider the Cauchy sequence in
 M, ∥·∥W1

µn = Pn
i=1

δ 1
i + 1
2i −δ 1
i −1
2i

, which satis-
ﬁes ∥µn −µn+k∥W1 ≤Pk
i=1
1
2n+i−1 ≤
1
2n−1 →0, but its limit P∞
i=1

δ 1
i + 1
2i −δ 1
i −1
2i

/∈M
because its total variation is inﬁnity. Hence, this is a Cauchy sequence without a limit, implying the
space is not complete.
Lemma E.4 (Range of ηπ
t ). Suppose that αt ∈[0, 1] for all t ≥0. Assume that ηπ
0 ∈PS, then we
have, for all t ≥0, ηπ
t ∈PS in the case of NTD. Similarly, assume that ηπ
0 ∈PS
K, then we have,
for all t ≥0, ηπ
t ∈PS
K in the case of CTD.
Proof. We will only prove the case of NTD, and the proof for CTD is similar by utilizing the property
of the projection operator ΠK : PS →PS
K. We prove the result by induction. It is trivial that
ηπ
t ∈PS for t = 0. Suppose that ηπ
t−1 ∈PS, recall the updating scheme of NTD
ηπ
t = (1 −αt)ηπ
t−1 + αtT π
t ηπ
t−1.
(78)
It is evident that PS is a convex set, considering that PS is a subset of the product signed measure
space, which is a linear space. Therefore, we only need to show that T π
t ηπ
t−1 ∈PS, which trivially
holds since T π
t is a random operator mapping from PS to PS, and ηπ
t−1 ∈PS. By applying the
induction argument, we can arrive at the conclusion.
31

NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reﬂect the
paper’s contributions and scope?
Answer: [Yes]
Justiﬁcation: The main claims reﬂect the paper’s contributions and scope.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reﬂect how
much the results can be expected to generalize to other settings.
• It is ﬁne to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justiﬁcation: The theoretical assumptions used in the paper are widely accepted and sup-
ported by a rich body of references in the literature, and the references are cited in our
paper.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate "Limitations" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-speciﬁcation, asymptotic approximations only holding locally). The au-
thors should reﬂect on how these assumptions might be violated in practice and what
the implications would be.
• The authors should reﬂect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reﬂect on the factors that inﬂuence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efﬁciency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be speciﬁcally instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
32

Answer: [Yes]
Justiﬁcation: Our paper provide the full set of assumptions and a self-contained proof.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justiﬁcation: Given that our paper focuses on the theoretical analysis of existing popular
algorithms, it does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or veriﬁable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might sufﬁce, or if the contribution is a speciﬁc model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
33

Question: Does the paper provide open access to the data and code, with sufﬁcient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justiﬁcation: Given that our paper focuses on the theoretical analysis of existing popular
algorithms, it does not include experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please
see
the
NeurIPS
code
and
data
submission
guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy)
for
more
de-
tails.
• While we encourage the release of code and data, we understand that this might not
be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run
to reproduce the results.
See the NeurIPS code and data submission guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justiﬁcation: Given that our paper focuses on the theoretical analysis of existing popular
algorithms, it does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Signiﬁcance
Question: Does the paper report error bars suitably and correctly deﬁned or other appropri-
ate information about the statistical signiﬁcance of the experiments?
Answer: [NA]
Justiﬁcation: Given that our paper focuses on the theoretical analysis of existing popular
algorithms, it does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer "Yes" if the results are accompanied by error bars, conﬁ-
dence intervals, or statistical signiﬁcance tests, at least for the experiments that support
the main claims of the paper.
34

• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not veriﬁed.
• For asymmetric distributions, the authors should be careful not to show in tables or
ﬁgures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding ﬁgures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufﬁcient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justiﬁcation: Given that our paper focuses on the theoretical analysis of existing popular
algorithms, it does not include experiments.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justiﬁcation: The research conform with NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justiﬁcation: Our paper only focuses on theory, hence there is no negative societal impact.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
35

• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake proﬁles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
ciﬁc groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efﬁciency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justiﬁcation: Our paper only focuses on theory.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety ﬁlters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justiﬁcation: The paper does not use existing assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
36

• If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [NA]
Justiﬁcation: The paper does not release new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip ﬁle.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Including this information in the supplemental material is ﬁne, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justiﬁcation: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
• Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
37

• We recognize that the procedures for this may vary signiﬁcantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
38

