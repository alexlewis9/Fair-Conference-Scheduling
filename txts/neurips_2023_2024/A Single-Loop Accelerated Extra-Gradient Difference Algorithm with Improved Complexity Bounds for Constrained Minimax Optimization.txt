A Single-Loop Accelerated Extra-Gradient Difference
Algorithm with Improved Complexity Bounds for
Constrained Minimax Optimization
Yuanyuan Liu1, Fanhua Shang2∗, Weixin An1, Junhao Liu1, Hongying Liu3,6∗, Zhouchen Lin4,5,6
1Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education,
School of Artiﬁcial Intelligence, Xidian University, China
2School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University
3Medical College, Tianjin University, China
4National Key Lab of General AI, School of Intelligence Science and Technology, Peking University
5Institute for Artiﬁcial Intelligence, Peking University
6Peng Cheng Laboratory
yyliu@xidian.edu.cn, fhshang@tju.edu.cn, hyliu2009@tju.edu.cn
zlin@pku.edu.cn
Abstract
In this paper, we propose a novel extra-gradient difference acceleration algorithm
for solving constrained nonconvex-nonconcave (NC-NC) minimax problems. In
particular, we design a new extra-gradient difference step to obtain an important
quasi-cocoercivity property, which plays a key role to signiﬁcantly improve the
convergence rate in the constrained NC-NC setting without additional structural
assumption. Then momentum acceleration is also introduced into our dual ac-
celerating update step. Moreover, we prove that, to ﬁnd an ϵ-stationary point
of the function f, the proposed algorithm attains the complexity O(ϵ−2) in the
constrained NC-NC setting, while the best-known complexity bound is e
O(ϵ−4),
where e
O(·) hides logarithmic factors compared to O(·). As the special cases of the
constrained NC-NC setting, the proposed algorithm can also obtain the same com-
plexity O(ϵ−2) for both the nonconvex-concave (NC-C) and convex-nonconcave
(C-NC) cases, while the best-known complexity bounds are e
O(ϵ−2.5) for the NC-C
case and e
O(ϵ−4) for the C-NC case. For fair comparison with existing algorithms,
we also analyze the complexity bound to ﬁnd ϵ-stationary point of the primal
function φ for the constrained NC-C problem, which shows that our algorithm can
improve the complexity bound from e
O(ϵ−3) to O(ϵ−2). To the best of our knowl-
edge, this is the ﬁrst time that the proposed algorithm improves the best-known
complexity bounds from e
O(ϵ−4) and e
O(ϵ−3) to O(ϵ−2) in both the NC-NC and
NC-C settings.
1
Introduction
This paper considers the following smooth minimax optimization problem,
min
x∈X max
y∈Y f(x, y),
(1)
∗Corresponding authors
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

where X ⊆Rm and Y ⊆Rn are nonempty closed and convex feasible sets, and f : Rm ×
Rn →R is a smooth function. In recent years, this problem has drawn considerable interest from
machine learning and other engineering communities such as generative adversarial networks [14],
adversarial machine learning [15, 28], game theory [3], reinforcement learning [12, 41], empirical
risk minimization [54, 39], and robust optimization [36, 13, 5]. While there is an extensive body
of literature on minimax optimization, most prior works such as [54, 39, 42, 33, 50] focus on the
convex-concave setting, where f(x, y) is convex in x and concave in y. However, in many nonconvex
minimax machine learning problems as in [35, 6], f(x, y) is nonconvex in x and (strongly) concave
in y, or f(x, y) is (strongly) convex in x and nonconcave in y. For nonconvex-strongly-concave
(NC-SC) minimax problems, a number of efﬁcient algorithms such as [34, 25, 27] were proposed,
and their complexity can be e
O(κ2
yϵ−2) for achieving an ϵ-stationary point ˆx (e.g., ∥∇φ(ˆx)∥≤ϵ) of
the primal function φ(·) := maxy∈Y f(·, y), where κy is the condition number for f(x, ·), and e
O
hides logarithmic factors. More recently, [24] proposed an accelerated algorithm, which improves the
gradient complexity to e
O(√κyϵ−2), which exactly matches the lower complexity bound in [52, 23].
Therefore, we mainly consider the problem (1) in nonconvex-nonconcave (NC-NC), nonconvex-
concave (NC-C) and convex-nonconcave (C-NC) settings.
1.1
Algorithms in NC-C and C-NC Settings
For nonconvex-concave (NC-C, but not strongly concave) and convex-nonconcave (C-NC) problems,
there are two types of algorithms (i.e., multi-loop (including double-loop and triple-loop) and
single-loop algorithms), and most of them are multi-loop algorithms such as [18, 31, 40, 24]. [31]
proposed a multi-step framework that ﬁnds an ϵ-ﬁrst order Nash equilibrium of f(x, y) with the
complexity e
O(ϵ−3.5). [40] designed a proximal dual implicit accelerated algorithm and proved that
their algorithm ﬁnds an ϵ-stationary point of φ with the complexity e
O(ϵ−3). More recently, [24]
proposed an accelerated algorithm, which achieves the complexity e
O(ϵ−2.5) to ﬁnd an ϵ-stationary
point of f. These multi-loop algorithms require at least O(ϵ−2) outer iterations, and thus their
complexities are more than O(ϵ−2). Even though single-loop methods are more popular in practice
due to their simplicity, few single-loop algorithms have been proposed for NC-C setting. The most
natural approach is the gradient descent-ascent (GDA) method, which performs a gradient descent
step on x and a gradient ascent step on y at each iteration. However, GDA fails to converge even
for simple bilinear zero-sum games [22]. Subsequently, several improved GDA algorithms such as
[8, 25, 27, 44] were proposed. For instance, [25] proved that the complexity of their two-time-scale
GDA to ﬁnd an ϵ-stationary point of φ is O(ϵ−6) for NC-C problems. Moreover, [27] presented an
efﬁcient algorithm, which obtains an ϵ-stationary point of f with the complexity O(ϵ−4). In fact, the
complexity only counts the number of times the maximization subproblem is solved, and does not
consider the complexity of solving this subproblem. [44] proposed a uniﬁed algorithm, and proved
that the complexity of their algorithm to ﬁnd an ϵ-stationary point of f is O(ϵ−4) for both NC-C
and C-NC problems. More recently, [51] presented a smoothed-GDA algorithm and proved that
the complexity can be improved to O(ϵ−2) for optimizing a special case of NC-C problems (i.e.,
minimizing the pointwise maximum of a ﬁnite collection of nonconvex functions). However, its
complexity is still O(ϵ−4) for both NC-C and C-NC problems. One natural question is: can we
design a single-loop accelerated algorithm with the optimal complexity bound O(ϵ−2) for both NC-C
and C-NC problems?
1.2
Algorithms in NC-NC Settings
This paper mainly considers constrained NC-NC minimax problems, i.e., f(x, y) is nonconvex
in x and nonconcave in y in the constrained setting (called constrained NC-NC). In recent years,
some works such as [7, 31, 46, 38, 20] focus on structured NC-NC problems. That is, the saddle
gradient operator of such minimax problems or their objectives must satisfy one of the structured
assumptions: the minty variational inequality (MVI) condition, the weak MVI condition, or negative
comonotone condition and the Polyak-Łojasiewicz condition. However, structured NC-NC problems
have limited practical applications because they are required to satisfy strong structural assumptions.
For more practical constrained NC-NC problems or more general NC-NC problems, the convergence
guarantee of the algorithms is still a challenge. In this paper, we also focus on convergence analysis
2

Table 1: Comparison of complexities of the minimax algorithms to ﬁnd an ϵ-stationary point of f(·, ·)
in the NC-C, C-NC and NC-NC settings. Note that Smoothed-GDA [51] can ﬁnd an ϵ-stationary
point for a special problem of (1) with O(ϵ−2), and e
O hides logarithmic factors compared to O(·).
Optimality Criteria
References
NC-C
C-NC
NC-NC
Simplicity
Lu et al. [27]
e
O(ϵ−4)
-
-
Single-Loop
Stationarity of f
Nouiehed et al. [31]
e
O(ϵ−3.5)
-
-
Multi-Loop
with smoothness
Lin et al. [24]
e
O(ϵ−2.5)
-
-
Multi-Loop
and compact sets
Zhang et al. [51]
O(ϵ−4)
-
-
Single-Loop
assumptions
Xu et al. [44]
O(ϵ−4)
O(ϵ−4)
-
Single-Loop
This work (Theorem 1)
O(ϵ−2)
O(ϵ−2)
O(ϵ−2)
Single-Loop
Table 2: Comparison of complexities of existing minimax algorithms and the proposed algorithm to
ﬁnd an φ(·) := maxy∈Y f(·, y) in the nonconvex-concave (NC-C) setting. This table only highlights
the dependence of ϵ, and compared with O(·), e
O(·) hides logarithmic factors.
NC-C Settings
References
Compact set
Complexity
Simplicity
Raﬁque et al. [34], Jin et al. [17]
X, Y
e
O(ϵ−6)
Multi-Loop
NC-C
Lin et al. [25]
X, Y
e
O(ϵ−6)
Single-Loop
(Stationarity of φ)
Thekumprampil et al. [40]
X, Y
e
O(ϵ−3)
Multi-Loop
Zhao [55], Lin et al. [24]
X, Y
e
O(ϵ−3)
Multi-Loop
This work (Theorem 2)
Y
O(ϵ−2)
Single-Loop
for solving more practical constrained NC-NC problems. Another natural question is: can we design
a single-loop accelerated algorithm to further improve the bound in the constrained NC-NC setting?
1.3
Motivations and Our Contributions
Motivations: For NC-C minimax problems, can we design a single-loop directly accelerated algorith-
m with the gradient complexity lower than the best-known result e
O(ϵ−2.5)? Though Smoothed-GDA
[51] can obtain the complexity O(ϵ−2) for a special case of Problem (1), it only attains the complex-
ity O(ϵ−4) for NC-C minimax problems. For C-NC minimax problems, for any given x, to solve
the nonconcave maximization subproblem with respect to y is NP-hard. As a result, all existing
multi-loop algorithms will lose their theoretical guarantees as discussed in [44]. Can we propose
a single-loop directly accelerated algorithm with the complexity lower than the best-known result
O(ϵ−4) for C-NC and NC-NC minimax problems?
Our Contributions: This paper proposes a novel single-loop extra-gradient difference acceleration
algorithm to push towards optimal gradient complexities for constrained NC-NC minimax problems
(1), and answer the above-mentioned problems.
We summarize the major contributions of this paper.
• We design a new single-loop accelerating algorithm for solving constrained NC-NC problems. In
the proposed algorithm, we design a new extra-gradient difference scheme, and combine the gradient
ascent and momentum acceleration steps for the dual variable update. In our algorithm, we present
an important quasi-cocoercivity property. By leveraging the quasi-cocoercivity property, we can
improve the complexity bound in our theoretical analysis.
• We analyze the convergence properties of the proposed algorithm for constrained NC-NC
problems. Theorem 1 shows that to ﬁnd an ϵ-stationary point of f, the proposed algorithm can obtain
the gradient complexity O(ϵ−2), which is the ﬁrst time to attains the complexity bound in constrained
NC-NC setting. The constrained NC-C and C-NC problems can be viewed as two special cases of
the constrained NC-NC problem, and the proposed algorithm is also applicable to these two special
problems. And its complexity is still O(ϵ−2) for both NC-C and C-NC problems, which signiﬁcantly
improves the gradient complexity from O(ϵ−4) of existing single-loop algorithms or e
O(ϵ−2.5) of
existing multi-loop algorithms to O(ϵ−2). The complexities of some recently proposed algorithms
are listed in Table 1.
3

• In order to make a comprehensive comparison with existing algorithms, we also provide the
theoretical analysis of our algorithm in terms of another convergence criteria (i.e., an ϵ-stationary
point of φ) for constrained NC-C problems. The result shows that our algorithm improves the
best-known result as in [40, 24, 55] from e
O(ϵ−3) to O(ϵ−2), as shown in Table 2.
2
Preliminaries and Related Works
Notation: Throughout this paper, we use lower-case letters to denote vectors such as x, y, and
calligraphic upper-case letters to denote sets such as X, Y. For a differentiable function f, ∇f(x) is
the gradient of f at x. For a function f(·, ·) of two variables, ∇xf(x, y) (or ∇yf(x, y)) is the partial
gradient of f with respect to the ﬁrst variable (or the second variable) at (x, y). For a vector x, ∥x∥
denotes its ℓ2-norm. We use PX and PY to denote projections onto the sets X and Y.
Assumption 1 (Smoothness). f(·, ·) is continuously differentiable, and there exists a positive constant
L such that
∥∇xf(x1, y1)−∇xf(x2, y2)∥≤L∥x1−x2∥, ∥∇yf(x1, y1)−∇yf(x2, y2)∥≤L∥y1−y2∥
holds for all x1, x2 ∈Rm, y1, y2 ∈Rn.
Deﬁnitions of the monotone operators: A operator F(·) : Rn →Rn is monotone, if [F(s) −
F(t)]T (s −t) ≥0, ∀s, t ∈Rn. If [F(s) −F(t)]T (s −t) ≤0, F is negative monotone.
A mapping F(·) is co-coercive if there is a positive constant α, such that
[F(s) −F(t)]T (s −t) ≥α∥F(s) −F(t)∥2, ∀s, t ∈Rn.
Nonconvex-Concave Minimax Optimization: Due to the nonconvex nature of these minimax
problems, ﬁnding the global solution is NP-hard in general. The recently proposed algorithms aim to
ﬁnd stationary solutions to such problems. For instance, the ﬁrst-order Nash equilibrium condition
(called game stationary) is used as an optimality condition in [31]. Besides game stationary, there are
two main optimality criteria (i.e., an ϵ-stationary point of f(·, ·) or φ(·) := maxy∈Y f(·, y)) for the
convergence analysis of the algorithms such as [4, 44, 24].
For solving NC-C minimax problems, there exist a number of efﬁcient multi-loop and single-loop
algorithms such as [31, 25, 4, 44, 24]. Most of them are multi-loop algorithms, which either employ
an accelerated update rule of x by adding regularization terms to its subproblem, or use multiple
gradient ascent steps for the update of y to solve the subproblem exactly or inexactly. Compared
with multi-loop algorithms, single-loop algorithms are easier to implement. One of the most popular
single-loop algorithms is GDA. However, GDA with a constant stepsize can fail to converge even for
a simple bilinear minimax problem [29].
To address this issue, only a few single-loop algorithms such as [25, 27, 44] were proposed, and
most of them employed a smoothing or proximal point technique. For instance, Smoothed-GDA [51]
introduces a smooth function ϕ(x, y, z)=f(x, y) + a
2∥x−z∥2 for the update of the primal variable
x, where z is an auxiliary variable and a is a constant, and its main update steps are
xt+1 =PX (xt−ηx∇xϕ(xt, zt, yt)) , yt+1 =PY(yt+ηy∇yf(xt+1, yt)) , zt+1 =zt+β(xt+1−zt),
where ηx, ηy >0 are two stepsizes, and 0<β ≤1. Smoothed-GDA can obtain the gradient complexity,
O(ϵ−4), for nonconvex-concave minimax problems.
Extra-Gradient Methods: There are some extra-gradient methods such as [48] for solving minimax
problems. Let η0 > 0 and ut be the t-th iterate, the extra-gradient method [53] has the following
projection-type prediction-correction step:
Prediction : ut+1/2 = PΩ(ut −ηtF(ut)) , Correction : ut+1 = PΩ
 ut −ηtF(ut+1/2)

.
In this paper, we call ut+1/2 as the prediction point at the t-iteration.
3
Single-Loop Extra-Gradient Difference Acceleration Algorithm
In this section, we propose a single-loop Extra-Gradient Difference Acceleration algorithm (EGDA)
for solving constrained NC-NC minimax problems.
4

Algorithm 1 EGDA for NC-NC minimax problems
1: Initialize: x0, y0, u0, u−1/2,τ > 0.5, ηx, ηt
y, β.
2: for t = 0, 1, . . . , T −1 do
3:
xt+1 = PX [xt−ηx∇xf(xt, yt)];
ut+1/2 =yt+β[∇yf(xt, ut−1/2)−∇yf(xt, yt−1)];
4:
ut+1 =PY

yt+ηt
y∇yf(xt, ut−1/2)

;
yt+1 = τyt + (1−τ)ut+1;
5: end for
6: Output: (xT , yT ).
3.1
Extra-Gradient Difference Acceleration
In recent years, many algorithms such as [27, 31, 32, 24, 51, 44] have been proposed for solving
NC-C minimax problems. In essence, most of them are “indirect” acceleration algorithms, which
are used to optimize the surrogate functions with a smoothing or proximal point term instead of the
original function. However, this may hurt the performance of these algorithms both in theory and in
practice [2, 1]. To address this issue, we propose a single-loop directly accelerating algorithm to ﬁnd
an ϵ-stationary points of f and φ with a signiﬁcantly improved complexity O(ϵ−2). The main update
steps of our EGDA algorithm are designed as follows.
• Gradient descent:
xt+1 = arg min
x∈X {⟨∇xf(xt, yt), x⟩+ ∥x−xt∥2/ηx}.
(2)
• Extra-gradient difference prediction:
ut+1/2 = yt + β[∇yf(xt,ut−1/2)−∇yf(xt,yt−1)].
(3)
• Gradient ascent correction:
ut+1 =arg max
u∈Y {⟨∇yf(xt, ut−1/2), u⟩−∥u−yt∥2/ηt
y}.
(4)
• Momentum acceleration:
yt+1 = τyt + (1 −τ)ut+1.
(5)
Here, 0 < β < 1
L, ηx, ηt
y > 0 are two stepsizes, ut+1/2, ut+1 are auxiliary variables, ut+1/2 is a
prediction point, and 1/2 < τ ≤1 is a momentum parameter. Our EGDA algorithm is formally
presented in Algorithm 1. Our EGDA algorithm ﬁrst performs one proximal gradient descent step on
the primal variable x, and then we design a new dual-accelerating scheme in (3), (4) and (5) for the
dual variable y.
3.2
Advantages of Our Algorithm and Comparison to Related Work
We ﬁrst design a new prediction point ut+1/2 in Eq. (3). Compared with extra-gradient-type methods
such as [7, 16, 20], one of the main differences is that the proposed prediction point ut+1/2 in (3)
is updated by the gradient difference (i.e., ∇yf(xt, ut−1/2) −∇yf(xt, yt−1)), while the prediction
point in other extra-gradient-type algorithms is updated only by using the gradient information at
the correction point ut. Then the gradient at the new prediction point ut+1/2 is used in the gradient
ascent step (4). And the dual variable y is update by the momentum acceleration step in (5).
• Prediction Point: The monotonicity and co-coercivity properties of gradient operators play a
crucial role for convergence analysis. However, these important properties do not always hold for
nonconvex problems. Some researchers have made great progress in some special nonconvex settings,
such as structured nonconvex and weakly convex settings, which require a weaker condition such as
weakly monotone [26], pseudo-monotone [16], and MVI [7]. However, such conditions seriously
limit the application scope of Problem (1).
To address this challenge, we design a new prediction point scheme in (3), which can help us
obtain a useful quasi-cocoercivity property. As a result, it does not require any monotone or
structural assumption. Speciﬁcally, we ﬁnd that we only require a weaker property in our theoretical
analysis, that is, the co-coercivity is required at some special points {ut+1/2, yt} (⟨∇yf(xt, ut+1/2)−
∇yf(xt, yt), ut+1/2−yt⟩≥β∥∇yf(xt, ut+1/2)−∇yf(xt, yt)∥2 with β > 0). Thus, we develop a
5

decoupling idea to construct the prediction point ut+1/2. That is, we use the gradients with respect
to y at ut−1/2, yt−1 instead of those at the points ut+1/2, yt. We can obtain a property (called the
quasi-co-coercivity) in Section 4.2, which plays a key role in our theoretical analysis.
• Gradient Difference: We also brieﬂy discuss the underlying intuition of the proposed gradient
difference in (3). We ﬁnd that the proposed update in (3) is similar to the forms in [43] (see Eqs.
(12) and (14) in [43]). [43] proposed a ﬁrst-order procedure (i.e., difference of gradients) to achieve
the negative curvature of a Hessian matrix. Therefore, our algorithm has a similar procedure, which
contains second-order information. Moreover, we use the difference of gradients in the gradient
ascent procedure for the dual update. But our EGDA algorithm only requires the Lipschitz gradient
assumption for minimax problems to ﬁnd ﬁrst-order stationary points, while [43] requires both the
Lipschitz gradient and Lipschitz Hessian assumptions for solving second-order stationary points of
nonconvex minimization problems.
• Momentum Acceleration: We design a dual-accelerating update rule in (4) for the dual variable
y in our EGDA algorithm, which is different from standard momentum acceleration schemes as in
[31, 40, 24]. That is, the accelerated rules of existing algorithms are for the primal variable x, while
our accelerated rule is designed for the dual variable y.
Therefore, the proposed new dual-accelerating step (including the gradient different prediction step
in (3), the gradient ascent correction in (4) and momentum acceleration in (5)) is a key accelerated
technique for our EGDA algorithm, and can help to improve the complexity bound from O(ϵ−4)
to O(ϵ−2). In particular, our EGDA algorithm performs both gradient descent and ascent steps to
the original function f. In contrast, many existing algorithms such as [51, 47, 44, 18] optimize
their surrogate functions with smoothing terms instead of the original function. In particular, their
smoothing parameters need to tune by repeatedly executing the algorithms, which may make them
impractical [1]. As in our theoretical guarantees below, the proposed single-loop algorithm is able to
signiﬁcantly improve the best-known gradient complexity, O(ϵ−4), of existing single-loop algorithms
such as [27, 51, 44] to O(ϵ−2).
4
Convergence Guarantees
In this section, we provide the convergence guarantees of our EGDA algorithm (i.e., Algorithm 1) for
solving constrained NC-NC and NC-C problems. We ﬁrst present the deﬁnitions of the two optimality
criteria (i.e., an ϵ-stationary point of f or φ). All the proofs of the lemmas, properties and theorems
below are included in the Supplementary Material.
4.1
Optimality Criteria and Key Property
Since ﬁnding a global minimum of a nonconvex optimization problem is NP-hard, ﬁnding a global
saddle point (or Nash equilibrium) of a NC-NC function f is intractable [30]. As in the literature
in the NC-NC setting, we introduce the local surrogates (i.e., the stationary point of f) and in
the NC-C setting, we introduce the local surrogates (i.e., the stationary point of f or φ), whose
gradient mappings are equal to zero. Below we deﬁne the following two optimality measures (i.e., an
ϵ-stationary points of f or φ) for our theoretical analysis.
Deﬁnition 1 (An ϵ-stationary point of f [27]). A point (x, y) ∈X × Y is an ϵ-stationary point of
f(·, ·) if ∥π(x, y)∥≤ϵ, where ηx and ηy are two constants, and
π(x, y) :=
 (1/ηx)(x −PX (x −ηx∇xf(x, y)))
(1/ηy)(y −PY(y + ηy∇yf(x, y)))

.
(6)
If ϵ = 0, then (x, y) is a stationary point of f.
For the NC-C setting, we also present another convergence criterion used in [40, 24]. Let φ(x) :=
maxy∈Y f(x, y), ˆx is called an ϵ-stationary point of a smooth primal function φ : Rm →R, if
∥∇φ(ˆx)∥≤ϵ. However, the function φ is not necessarily differentiable for minimax problems.
Following [9, 40, 24], we introduce the Moreau envelope of φ for the optimality criterion, especially
when φ is a weakly convex function, i.e., φ is L-weakly convex if the function φ(·) + L
2 ∥· ∥2 is
convex. We refer readers to [9, 24] for the comparison of these two criteria.
6

Deﬁnition 2 (An ϵ-stationary point of L-weakly convex function φ). ˆx is an ϵ-stationary point of an
L-weakly convex function φ : Rm →R, if ∥∇φ1/(2L)(ˆx)∥≤ϵ, where φ1/(2L) is the Moreau envelope
of φ and is deﬁned as: φρ(x):=minz φ(z)+(1/2ρ)∥z−x∥2. If ϵ = 0, then ˆx is a stationary point.
Then we give the following important property for the analysis of the proposed algorithm. By
leveraging the property, we can obtain the complexity bound of the proposed algorithm.
Property 1 (Quasi-Cocoercivity). Let ut+1/2 be updated in Eq. (3), then

∇yf(xt, ut−1/2) −∇yf(xt, yt−1), ut+1/2 −yt

= β∥∇yf(xt, ut−1/2) −∇yf(xt, yt−1)∥2.
4.2
Core Lemma
Our theoretical results mainly rely on Lemma 1 below, which plays a key role in the proofs of
Theorems 1 and 2. Let {(xt, yt, ut, ut−1/2)} be a sequence generated by Algorithm 1, and we deﬁne
the potential function
Gt := f(xt, yt)+9L∥ut−yt−1∥2+8Lβ2∥∇yf(xt−1, ut−3/2)−∇yf(xt−1, yt−2)∥2.
Next, we need to prove that our deﬁned potential function can make sufﬁcient decrease at each
iteration, i.e., Gt −Gt+1 >0 as in Lemma 1 below. To prove Lemma 1, we will provide and prove
the following upper bounds.
Proposition 1 (Upper bound of primal-dual updates). Suppose Assumption 1 holds. Let {(xt, yt, ut)}
be a sequence generated by Algorithm 1 with ηt
y=min{
β∥∇yf(xt,ut−1/2)−∇yf(xt,yt−1)∥2
2∥∇yf(xt,ut−1/2)∥2
,
1
28L, ηx}.
f(xt+1, ut+1/2)−f(xt, yt)
≤−
 1
ηx
−L
2

∥xt+1−xt∥2+L∥ut+1/2 −yt∥2 + L∥yt −yt−1∥2+

∇yf(xt, yt−1), ut+1/2−yt

|
{z
}
A1
.
Proposition 2 (Upper bound of dual updates). Suppose Assumption 1 holds. Let {(xt, yt, ut)} be a
sequence generated by Algorithm 1, then
f(xt+1, yt+1)−f(xt+1, ut+1/2)
≤τ⟨∇yf(xt, ut−1/2),yt−ut+1⟩
|
{z
}
A2
+⟨∇yf(xt,ut−1/2),ut+1−ut+1/2⟩
|
{z
}
A3
+at,
where at := 2L∥xt+1−xt∥2+6Lβ2∥∇yf(xt, ut−1/2)−∇yf(xt, yt−1)∥2 +8L(1−τ)2∥ut−yt−1∥2 +
8Lβ2∥∇yf(xt−1, ut−3/2)−∇yf(xt−1, yt−2)∥2 + 3L∥ut+1−yt∥2.
Using the optimal condition of Problem (4) and our quasi-cocoercivity in Property 1, we can further
bound A1 + A2 + A3. The proof sketch of Lemma 1 is listed as follows:
Gt+1−Gt :=f(xt+1,yt+1)−f(xt+1,ut+1/2)
|
{z
}
Proposition 1
+f(xt+1,ut+1/2)−f(xt,yt)
|
{z
}
Proposition 2
+other terms
=
A1 + A2 + A3
|
{z
}
Quasi-Cocoercivity in Property 1
+
other terms.
Combining the above results and the deﬁnition of Gt, we can get the descent estimate of the potential
function in Lemma 1, which is a main lemma for Theorems 1 and 2 below.
Lemma 1 (Descent estimate of G). Suppose Assumption 1 holds. Let {(xt, yt, ut)} be a sequence
generated by Algorithm 1, then
G0 −GT
=
T −1
X
t=0
(Gt −Gt+1)
≥
T −1
X
t=0
 1
4ηty
∥ut+1 −yt∥2+ β−30Lβ2
2
∥∇yf(xt, ut−1/2)−∇yf(xt, yt−1)∥2+ 1
2ηx
∥xt+1−xt∥2

.
7

4.3
Convergence Results
In this subsection, by using the optimality measure in Deﬁnitions 1 and 2, we present the following
theoretical results in Theorem 1 and Theorem 2 for the constrained NC-NC setting and NC-C setting,
respectively.
Assumption 2. Y is a closed, convex and compact set with a diameter DY > 0, and X is a closed
and convex set.
Furthermore, using Lemma 1 and the optimality measure in Deﬁnition 1, we will study the relation
between π(xt, ut) and the difference (i.e., Gt−Gt+1) to obtain the gradient complexity in Theorem 1
by computing the number of iterations to achieve an ϵ-stationary point of f.
Theorem 1 (Stationarity of f in constrained NC-NC settings). Suppose Assumptions 1 and 2 hold.
Let the two stepsizes ηx ≤
1
4L, ηt
y = min{
β∥∇yf(xt,ut−1/2)−∇yf(xt,yt−1)∥2
2∥∇yf(xt,ut−1/2)∥2
,
1
28L, ηx}, β ≤
1
60L and
τ ≥1/2, then the complexity of Algorithm 1 to ﬁnd an ϵ-stationary point of f is bounded by
O
G0 −G + 2LD2
Y
ϵ2

,
where G0 := G(x0, y0), and G := minx∈X φ(x).
Remark 1. For constrained NC-NC problems, the gradient complexity of our EGDA algorithm to
ﬁnd an ϵ-stationary point of f is O(ϵ−2). That is, our EGDA algorithm is ﬁrst to obtain the gradient
complexity in constrained NC-NC setting. In addition, our method can achieve the same complexity,
O(ϵ−2), as the algorithm with an additional structured assumption for NC-NC problems. However,
different from existing algorithms, our algorithm is more practical. That is, it only requires that
Y is a compact set, while existing algorithms need some stronger structured assumptions in the
structured NC-NC setting. The detailed comparison is shown in Table 1. As two special cases of the
constrained NC-NC problem, this theoretical result in Theorem 1 can be extended to the constrained
NC-C and C-NC settings. For NC-C problems, the gradient complexity of our EGDA algorithm to ﬁnd
an ϵ-stationary point of f is O(ϵ−2), while the best-known result of single-loop algorithms such as
[27, 51, 44] is O(ϵ−4), and the best-known result of multi-loop algorithms such as [24] is e
O(ϵ−2.5).
That is, our EGDA algorithm improves the best-known gradient complexity from e
O(ϵ−2.5) to O(ϵ−2).
Smoothed-GDA [51] can also obtain the complexity O(ϵ−2) for a special case of Problem (1) and
O(ϵ−4) for general NC-C minimax problems, while our algorithm attains the optimal result O(ϵ−2)
for all NC-C minimax problems. Existing algorithms such as HiBSA [27] and AGP [44] require
the compactness of the domain X in the NC-C setting, while our EGDA algorithm does not, which
signiﬁcantly extends its applicability. For C-NC minimax problems, the proposed algorithm can
obtain the complexity, O(ϵ−2), while the best-known complexity as in [44] is O(ϵ−4). In other words,
the proposed algorithm can improve the best-known result from O(ϵ−4) to O(ϵ−2).
By using the criterion in Deﬁnition 2 as the optimality measure, we use the deﬁnition of φ1/2L and
introduce the property of ∇φ1/2L as in [24]. With a similar setting for varying stepsizes as in [24],
we study the relation between ∇φ1/2L and the basic descent estimation of the potential function
in Lemma 1 by using the property of ∇φ1/2L, and compute the number of iterations to achieve an
ϵ-stationary point of φ. We provide the following theoretical result and its detailed proof in the
Supplementary Material.
Theorem 2 (Stationarity of φ for constrained NC-C settings). Using the same notation as in Theo-
rem 1, and f is concave with respect to y. Let {(xt, yt, ut)} be a sequence generated by Algorithm 1
with the stepsizes ηt
y =min{ βa2
t
2b2
t ,
1
28L, βa2
t
2bt , βc2
t
2dt }, where at :=∥∇yf(xt,ut−1/2)−∇yf(xt,yt−1)∥,bt :=
∥∇yf(xt, ut−1/2)∥,ct :=∥∇yf(xt, ut−1/2)∥, dt :=∥∇yf(xt, ut)−∇yf(xt, ut−1/2)∥. Then the gra-
dient complexity of Algorithm 1 to ﬁnd an ϵ-stationary point of φ is bounded by
O
DY(G0 −G + 2LD2
Y)
ϵ2

.
Remark 2. From Theorem 2, it is clear that the gradient complexity of the proposed algorithm is
O(ϵ−2). That is, Algorithm 1 can improve the best-known gradient complexity from e
O(ϵ−3) as in
[24] to O(ϵ−2). Therefore, Algorithm 1 is the ﬁrst algorithm, which attains the gradient complexity
O(ϵ−2) to ﬁnd an ϵ-stationary point of φ for NC-C minimax problems.
8

0
100
200
300
400
500
Number of iterations
10-30
10-20
10-10
100
GDA
EG
FEG
EGDA
0
100
200
300
400
500
Number of iterations
10-30
10-20
10-10
100
GDA
EG
FEG
EGDA
Figure 1: Comparison of all the methods for solving the NC-NC problem, f(x, y) = x2 +
3 sin2x sin2y −4y2 −10 sin2y. Left: Convergence in terms of ∥xt −x∗∥2 + ∥yt −y∗∥2, where
(x∗, y∗) is the global saddle point; Right: Convergence in terms of ∥∇xf∥2 + ∥∇yf∥2.
5
Numerical Results
We conduct many experiments to illustrate the performance of the proposed algorithm for solving
NC-NC, NC-C and convex-concave problems.
NC-NC Problems. We conduct some experiments to illustrate the performance of the proposed
algorithm, EGDA, for solving NC-NC problems. Moreover, we compare it against existing methods
such as GDA [46], EG [10], EAG [49] and FEG [20]. Detailed setup is provided in the Supplementary
Material.
We compare EGDA with GDA, EG and FEG for solving a simple NC-NC minimax problem (i.e.,
f(x, y)=x2+3 sin2x sin2y−4y2−10 sin2y), which satisﬁes the Polyak-Łojasiewicz condition, as
shown in Fig. 1. All the results show that EGDA indeed converges to the global saddle point and is
signiﬁcantly faster than other methods including GDA and FEG in terms of both ∥xt −x∗∥2 + ∥yt −
y∗∥2 and ∥∇xf∥2 + ∥∇yf∥2. Although FEG has a fast theoretical rate, O(1/t2), with a negative
monotone assumption, it converges much slower than EGDA in practice.
0
100
200
300
400
500
Number of iterations
10-2
100
GDA
EAG
EGDA
Figure 2: Comparison of all the methods for solving the convex-concave problem, f(x, y) =
log(1+ex) + 3xy −log(1+ey). Left: Trajectories of the three algorithms; Right: Convergence in
terms of ∥xt −x∗∥2 + ∥yt −y∗∥2.
Convex-Concave Problems. Fig. 2 shows the convergence results of the methods including GDA,
EAG [49] and EGDA for solving the convex-concave problem, f(x, y) = log(1 + ex) + 3xy −
log(1 + ey). It is clear that EGDA converges much faster than other methods such as EAG. We
also observe empirically when the same step-size is used, even if small, GDA may not converge to
stationary points, and it is proven to always have bounded iterates.
NC-C Problems. We also apply our EGDA algorithm to train robust neural networks against
adversarial attacks on Fashion MNIST and MNIST, and verify our theoretical results. In [15, 28, 31],
9

Figure 3: Convergence speed of all the algorithms on Fashion MNIST (left) and MNIST (right).
the robust training procedure can be formulated into a NC-C minimax optimization problem. It is
clear that the minimax problem is nonconvex in x, but concave in y.
FGSM [15] and PGD [19] are two popular methods for generating adversarial examples. To obtain
the targeted attack ˆxij, we use the same procedure as in [31, 51] as follows. The perturbation level ε
is chosen from {0.0, 0.1, 0.2, 0.3, 0.4}, and the stepsize is 0.01. Note that the number of iterations
is set to 40 for FGSM and 10 for PGD, respectively. The details of the network are included in
the Supplementary Material. We compare our EGDA method with GDA [25], MGDA [31] and
Smoothed-GDA [51], and illustrate the convergence of all the algorithms on the loss function in Fig. 3.
The results show that Smoothed-GDA and EGDA converge signiﬁcantly faster than other algorithms.
This veriﬁes that they have a faster convergence rate, O(ϵ−2). Moreover, EGDA converges much
faster than Smoothed-GDA.
6
Conclusions and Future Work
In this paper, we proposed a new single-loop accelerated algorithm for solving various constrained
NC-NC minimax problems. In the proposed algorithm, we designed a novel extra-gradient difference
scheme for dual updates. Moreover, we provided the convergence guarantees for the proposed
algorithm, and the theoretical results show that to ﬁnd an ϵ-stationary point of f, the proposed
algorithm obtains the complexity bound O(ϵ−2) for constrained NC-NC problems. That is, this
is the ﬁrst time that the proposed algorithm attains the complexity bound O(ϵ−2) in constrained
NC-NC settings (including constrained NC-C and C-NC). For NC-C problems, we provided the
theoretical guarantees of the proposed algorithm under the stationarity of φ, which show that our
algorithm improves the complexity bound from e
O(ϵ−3) to O(ϵ−2). Experimental results also veriﬁed
the theoretical results of the proposed algorithm, which have the factors of ϵ−1 and ϵ−2 faster
than existing algorithms, respectively. For further work, we will extend our directly accelerating
algorithm to stochastic, non-smooth, nonconvex-nonconcave and federated learning settings as in
[47, 11, 4, 55, 45, 21, 37].
Acknowledgments
We want to thank the anonymous reviewers for their valuable suggestions and comments. This work
was supported by the National Key R&D Program of China (No. 2022ZD0160302), the National
Natural Science Foundation of China (Nos. 6227071567, 61976164, 62276004 and 61836009), the
National Science Basic Research Plan in Shaanxi Province of China (No. 2022GY-061), the major
key project of PCL, China (No. PCL2021A12), and Qualcomm.
References
[1] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. In STOC, 2017.
[2] Z. Allen-Zhu and Y. Yuan. Improved SVRG for non-strongly-convex or sum-of-non-convex objectives. In
ICML, pages 1080–1089, 2016.
10

[3] J. P. Bailey, G. Gidel, and G. Piliouras. Finite regret and cycles with ﬁxed step-size via alternating gradient
descent-ascent. In The Annual Conference on Learning Theory (COLT), pages 391–407, 2020.
[4] R. I. Bot and A. Bohm. Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax
problems. SIAM J. Optim., 2023.
[5] Y. Cai and W. Zheng. Accelerated single-call methods for constrained min-max optimization. In Interna-
tional Conference on Learning Representations (ICLR), 2023.
[6] Z. Chen, Z. Hu, Q. Li, Z. Wang, and Y. Zhou. A cubic regularization approach for ﬁnding local minimax
points in nonconvex minimax optimization. Transactions on Machine Learning Research, pages 1–43,
2023.
[7] C. D. Dang and G. Lan. On the convergence properties of non-euclidean extragradient methods for
variational inequalities with generalized monotone operators. Comput. Optim. Appl., 60(2):277–310, 2015.
[8] C. Daskalakis and I. Panageas. The limit points of (optimistic) gradient descent in min-max optimization.
In The Annual Conference on Neural Information Processing Systems (NeurIPS), pages 9256–9266, 2018.
[9] D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM J.
Optim., 29(1):207–239, 2019.
[10] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efﬁcient methods for structured nonconvex-nonconcave
min-max optimization. In AISTATS, pages 2746–2754, 2021.
[11] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efﬁcient methods for structured nonconvex-nonconcave
min-max optimization. In The International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pages 2746–2754, 2021.
[12] S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochastic variance reduction methods for policy evaluation.
In International Conference on Machine Learning (ICML), pages 1049–1058, 2017.
[13] J. C. Duchi and H. Namkoong. Variance-based regularization with convex objectives. J. Mach. Learn. Res.,
20:1–55, 2019.
[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In The Annual Conference on Neural Information Processing Systems (NIPS),
pages 2672–2680, 2014.
[15] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International
Conference on Learning Representations (ICLR), 2015.
[16] A. N. Iusem, A. Jofr´e, R. I. Oliveira, and P. Thompson. Extragradient method with variance reduction for
stochastic variational inequalities. SIAM J. Optim., 27(2):686–724, 2017.
[17] C. Jin, P. Netrapalli, and M. I. Jordan. What is local optimality in nonconvex-nonconcave minimax
optimization? In International Conference on Machine Learning (ICML), pages 4880–4889, 2020.
[18] W. Kong and R. D. Monteiro. An accelerated inexact proximal point method for solving nonconvex-concave
min-max problems. In arXiv preprint arXiv:1905.13433v3, 2021.
[19] A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In International
Conference on Learning Representations (ICLR), 2017.
[20] S. Lee and D. Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax
problems. In NeurIPS, 2021.
[21] Y. Lei, Z. Yang, T. Yang, and Y. Ying. Stability and generalization of stochastic gradient methods for
minimax problems. In International Conference on Machine Learning (ICML), pages 6175–6186, 2021.
[22] A. Letcher, D. Balduzzi, S. Racaniere, J. Martens, J. N. Foerster, K. Tuyls, and T. Graepel. Differentiable
game mechanics. J. Mach. Learn. Res., 20:1–40, 2019.
[23] H. Li, Y. Tian, J. Zhang, and A. Jadbabaie. Complexity lower bounds for nonconvex-strongly-concave
min-max optimization. In arXiv preprint arXiv:2104.08708v1, 2021.
[24] T. Lin, C. Jin, and M. I. Jordan. Near-optimal algorithms for minimax optimization. In The Annual
Conference on Learning Theory (COLT), pages 2738–2779, 2020.
11

[25] T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In
International Conference on Machine Learning (ICML), pages 6083–6093, 2020.
[26] M. Liu, H. Raﬁque, Q. Lin, and T. Yang. First-order convergence theory for weakly-convex-weakly-concave
min-max problems. J. Mach. Learn. Res., 22:169:1–169:34, 2021.
[27] S. Lu, I. Tsaknakis, M. Hong, and Y. Chen. Hybrid block successive approximation for one-sided non-
convex min-max problems: algorithms and applications. IEEE Trans. Signal Process., 68:3676–3691,
2020.
[28] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.
[29] A. Mokhtari, A. Ozdaglar, and S. Pattathil. A uniﬁed analysis of extra-gradient and optimistic gradient
methods for saddle point problems: Proximal point approach. In The International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pages 1497–1507, 2020.
[30] K. G. Murty and S. N. Kabadi. Some NP-complete problems in quadratic and nonlinear programming.
Mathematical Programming, 39(2):117–129, 1987.
[31] M. Nouiehed, M. Sanjabi, T. Huang, J. D. Lee, and M. Razaviyayn. Solving a class of non-convex min-max
games using iterative ﬁrst order methods. In NeurIPS, pages 14905–14916, 2019.
[32] D. M. Ostrovskii, A. Lowy, and M. Razaviyayn. Efﬁcient search of ﬁrst-order Nash equilibria in nonconvex-
concave smooth min-max problems. SIAM J. Optim., 2021.
[33] Y. Ouyang and Y. Xu. Lower complexity bounds of ﬁrst-order methods for convex-concave bilinear
saddle-point problems. Mathematical Programming, 185:1–35, 2021.
[34] H. Raﬁque, M. Liu, Q. Lin, and T. Yang. Weakly-convex concave min-max optimization: Provable
algorithms and applications in machine learning. Optim. Method Softw., 2022.
[35] M. Razaviyayn, T. Huang, S. Lu, M. Nouiehed, M. Sanjabi, and M. Hong. Nonconvex min-max optimiza-
tion: Applications, challenges, and recent theoretical advances. IEEE Signal Process. Mag., 37(5):55–66,
2020.
[36] S. Shaﬁeezadeh-Abadeh, P. M. Esfahani, and D. Kuhn. Distributionally robust logistic regression. In The
Annual Conference on Neural Information Processing Systems (NIPS), pages 1576–1584, 2015.
[37] P. Sharma, R. Panda, G. Joshi, and P. K. Varshney. Federated minimax optimization: Improved convergence
analyses and algorithms. In International Conference on Machine Learning (ICML), pages 19683–19730,
2022.
[38] C. Song, Z. Zhou, Y. Zhou, Y. Jiang, and Y. Ma. Optimistic dual extrapolation for coherent non-monotone
variational inequalities. In NeurIPS, 2020.
[39] C. Tan, T. Zhang, S. Ma, and J. Liu. Stochastic primal-dual method for empirical risk minimization
with o(1) per-iteration complexity. In The Annual Conference on Neural Information Processing Systems
(NeurIPS), pages 8376–8385, 2018.
[40] K. K. Thekumprampil, P. Jain, P. Netrapalli, and S. Oh.
Efﬁcient algorithms for smooth minimax
optimization. In The Annual Conference on Neural Information Processing Systems (NeurIPS), pages
12659–12670, 2019.
[41] H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double averaging
primal-dual optimization. In The Annual Conference on Neural Information Processing Systems (NeurIPS),
pages 9672–9683, 2018.
[42] G. Xie, L. Luo, Y. Lian, and Z. Zhang. Lower complexity bounds for ﬁnite-sum convex-concave minimax
optimization problems. In International Conference on Machine Learning (ICML), pages 10504–10513,
2020.
[43] Y. Xu, R. Jin, and T. Yang. First-order stochastic algorithms for escaping from saddle points in almost
linear time. In NeurIPS, pages 5535–5545, 2018.
[44] Z. Xu, H. Zhang, Y. Xu, and G. Lan. A uniﬁed single-loop alternating gradient projection algorithm for
nonconvex-concave and convex-nonconcave minimax problems. Mathematical Programming, 2023.
12

[45] J. Yang, N. Kiyavash, and N. He. Global convergence and variance-reduced optimization for a class of
nonconvex-nonconcave minimax problems. In The Annual Conference on Neural Information Processing
Systems (NeurIPS), 2020.
[46] J. Yang, N. Kiyavash, and N. He. Global convergence and variance reduction for a class of nonconvex-
nonconcave minimax problems. In NeurIPS, 2020.
[47] J. Yang, S. Zhang, N. Kiyavash, and N. He. A catalyst framework for minimax optimization. In The
Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.
[48] T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with
o(1/kˆ2) rate on squared gradient norm. In M. Meila and T. Zhang, editors, ICML, pages 12098–12109,
2021.
[49] T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with
o(1/k2) rate on squared gradient norm. In ICML, pages 12098–12109, 2021.
[50] J. Zhang, M. Hong, and S. Zhang. On lower iteration complexity bounds for the saddle point problems.
Mathematical Programming, 194:901–935, 2022.
[51] J. Zhang, P. Xiao, R. Sun, and Z. Luo. A single-loop smoothed gradient descent-ascent algorithm for
nonconvex-concave min-max problems. In The Annual Conference on Neural Information Processing
Systems (NeurIPS), pages 7377–7389, 2020.
[52] S. Zhang, J. Yang, C. Guzman, N. Kiyavash, and N. He. The complexity of nonconvex-strongly-concave
minimax optimization. In The Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2021.
[53] W. Zhang and D. Han. A new alternating direction method for co-coercive variational inequality problems.
Comput. Math. Appl., 57(7):1168–1178, 2009.
[54] Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization.
J. Mach. Learn. Res., 18:1–42, 2017.
[55] R. Zhao. A primal dual smoothing framework for max-structured nonconvex optimization. Mathematics of
Operations Research, 2023.
13

