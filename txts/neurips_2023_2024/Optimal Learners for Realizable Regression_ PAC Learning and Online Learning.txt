Optimal Learners for Realizable Regression:
PAC Learning and Online Learning
Idan Attias
Ben-Gurion University of the Negev
idanatti@post.bgu.ac.il
Steve Hanneke
Purdue University
steve.hanneke@gmail.com
Alkis Kalavasis
Yale University
alvertos.kalavasis@yale.edu
Amin Karbasi
Yale University, Google Research
amin.karbasi@yale.edu
Grigoris Velegkas
Yale University
grigoris.velegkas@yale.edu
Abstract
In this work, we aim to characterize the statistical complexity of realizable regres-
sion both in the PAC learning setting and the online learning setting.
Previous work had established the sufficiency of finiteness of the fat shattering di-
mension for PAC learnability and the necessity of finiteness of the scaled Natarajan
dimension, but little progress had been made towards a more complete character-
ization since the work of Simon (SICOMP ’97). To this end, we first introduce
a minimax instance optimal learner for realizable regression and propose a novel
dimension that both qualitatively and quantitatively characterizes which classes of
real-valued predictors are learnable. We then identify a combinatorial dimension
related to the Graph dimension that characterizes ERM learnability in the realizable
setting. Finally, we establish a necessary condition for learnability based on a
combinatorial dimension related to the DS dimension, and conjecture that it may
also be sufficient in this context.
Additionally, in the context of online learning we provide a dimension that charac-
terizes the minimax instance optimal cumulative loss up to a constant factor and
design an optimal online learner for realizable regression, thus resolving an open
question raised by Daskalakis and Golowich in STOC ’22.
1
Introduction
Real-valued regression is one of the most fundamental and well-studied problems in statistics and
data science [Vap99, GBC16, Bac21], with numerous applications in domains such as economics
and medicine [DG17]. However, despite its significance and applicability, theoretical understanding
of the statistical complexity of real-valued regression is still lacking.
Perhaps surprisingly, in the fundamental realizable Probably Approximately Correct (PAC) setting
[Val84] and the realizable online setting [Lit88, DG22], we do not know of any characterizing
dimension or optimal learners for the regression task. This comes in sharp contrast with binary and
multiclass classification, both in the offline and the online settings, where the situation is much more
clear [Lit88, HLW94, BEHW89, Han16, DSS14, BCD+22, Nat89, DSBDSS15, RST23]. Our goal
in this work is to make progress regarding the following important question:
37th Conference on Neural Information Processing Systems (NeurIPS 2023).

Which dimensions characterize PAC and online learnability for realizable real-valued regression?
Consider an instance space 𝒳, label space 𝒴= [0, 1] and hypothesis class ℋ⊆[0, 1]𝒳. In this work,
we focus on the case of the regression framework with respect to the absolute loss ℓ(𝑥, 𝑦) ≜|𝑥−𝑦|
for any 𝑥, 𝑦∈[0, 1], following previous works [BLW94, Sim97, ABDCBH97, BL98, DG22]. Our
qualitative results hold for more general losses, namely any approximate pseudo-metric loss, which
includes common losses such as ℓ𝑝for any 𝑝≥1. See the formal statements in Appendix H.
PAC/Offline Realizable Regression.
Let us first recall the definition of realizable real-valued
regression in the PAC setting. Informally, the learner is given i.i.d. labeled examples drawn from an
unknown distribution 𝒟with the promise that there exists a target hypothesis ℎ⋆∈ℋthat perfectly
labels the data. The goal is to use this training set to design a predictor with small error on future
examples from the same distribution. Note that given a learner 𝐴and sample 𝑆∼𝒟𝑛, we let 𝐴(𝑆; 𝑥)
be its prediction on 𝑥∈𝒳when the training set is 𝑆.
Definition 1 (PAC Realizable Regression). Let ℓ: [0, 1]2 →R≥0 be the absolute loss function.
Consider a class ℋ⊆[0, 1]𝒳for some domain 𝒳. Let ℎ⋆∈ℋbe an unknown target function and
let 𝒟𝒳be an unknown distribution on 𝒳. A random sample 𝑆of size 𝑛consists of points 𝑥1, . . . , 𝑥𝑛
drawn i.i.d. from 𝒟𝒳and the corresponding values ℎ⋆(𝑥1), . . . , ℎ⋆(𝑥𝑛) of the target function.
• An algorithm 𝐴: (𝒳× [0, 1])𝑛→[0, 1]𝒳is an 𝑛-sample PAC learner for ℋwith respect
to ℓif, for all 0 < 𝜀, 𝛿< 1, there exists 𝑛= 𝑛(𝜀, 𝛿) ∈N such that for any ℎ⋆∈ℋand any
domain distribution 𝒟𝒳, it holds that E𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥))] ≤𝜀, with probability at
least 1 −𝛿over 𝑆∼𝒟𝑛
𝒳.
• An algorithm 𝐴: (𝒳× [0, 1])𝑛→[0, 1]𝒳is an 𝑛-sample cut-off PAC learner for ℋwith
respect to ℓif, for all 0 < 𝜀, 𝛿, 𝛾< 1, there exists 𝑛= 𝑛(𝜀, 𝛿, 𝛾) ∈N such that for any
ℎ⋆∈ℋand any domain distribution 𝒟𝒳, it holds that Pr𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥)) > 𝛾] ≤
𝜀, with probability at least 1 −𝛿over 𝑆∼𝒟𝑛
𝒳.
We remark that these two PAC learning definitions are qualitatively equivalent (cf. Lemma 1). We
note that, throughout the paper, we implicitly assume, as e.g., in [HKS19], that all hypothesis classes
are admissible in the sense that they satisfy mild measure-theoretic conditions, such as those specified
in [DKLD84] (Section 10.3.1) or [Pol12] (Appendix C).
The question we would like to understand in this setting (and was raised by [Sim97]) follows:
Question 1. Can we characterize learnability and design minimax optimal PAC learners for realiz-
able regression?
Traditionally, in the context of statistical learning a minimax optimal learner 𝐴⋆is one
that, for every class ℋ, given an error parameter 𝜀and a confidence parameter 𝛿requires
min𝐴max𝒟ℳ𝐴(ℋ; 𝜀, 𝛿, 𝒟) samples to achieve it, where ℳ𝐴(ℋ; 𝜀, 𝛿, 𝒟) is the number of samples
that some learner 𝐴requires to achieve error 𝜀with confidence 𝛿when the data-generating distribution
is 𝒟. There seems to be some inconsistency in the literature about realizable regression, where some
works present minimax optimal learners when the maximum is also taken over the hypothesis class
ℋ, i.e., min𝐴max𝒟,ℋℳ𝐴(ℋ, 𝜀, 𝛿, 𝒟). This is a weaker result compared to ours, since it shows that
there exists some hypothesis class for which these learners are optimal.
Our main results in this setting, together with the uniform convergence results from prior work, give
rise to an interesting landscape of realizable PAC learning that is depicted in Figure 1.
Online Realizable Regression.
We next shift our attention to the classical setting of online learning
where the learner interacts with the adversary over a sequence of 𝑇rounds: in every round the
adversary presents an example 𝑥𝑡∈𝒳, the learner predicts a label ̂︀𝑦𝑡and then the adversary reveals
the correct label 𝑦⋆
𝑡. In this context, realizability means that there always exists some function
ℎ𝑡∈ℋthat perfectly explains the examples and the labels that the adversary has chosen. In the
agnostic setting, the goal of the learner is to compete with the performance of the best function
in ℋ, i.e., achieve small regret. This setting was introduced by [Lit88] in the context of binary
classification, where they also characterized learnability in the realizable setting and provided an
optimal algorithm. Later, [BDPSS09] provided an almost optimal algorithm in the agnostic setting,
which suffered from an additional log 𝑇factor in its regret bound. This extra factor was later shaved
2

Figure 1: Landscape of Realizable PAC Regression: the “deleted” arrows mean that the implication
is not true. The equivalence between finite fat-shattering dimension and the uniform convergence
property is known even in the realizable case (see [SSSSS10]) and the fact that PAC learnability
requires finite scaled Natarajan dimension is proved in [Sim97]. The properties of the other three
dimensions (scaled Graph dimension, scaled One-Inclusion-Graph (OIG) dimension, and scaled
Daniely-Shalev Shwartz (DS) dimension) are shown in this work. We further conjecture that finite
scaled Natajaran dimension is not sufficient for PAC learning, while finite scaled DS does suffice.
Interestingly, we observe that the notions of uniform convergence, learnability by any ERM and PAC
learnability are separated in realizable regression.
by [ABED+21]. The study of multiclass online classification was initiated by [DSBDSS15], who
provided an optimal algorithm for the realizable setting and an algorithm that is suboptimal by
a factor of log 𝑘· log 𝑇in the agnostic setting, where 𝑘is the total number of labels. Recently,
[RST23] shaved off the log 𝑘factor. The problem of online regression differs significantly from that
of online classification, since the loss function is not binary. The agnostic online regression setting
has received a lot of attention and there is a series of works that provides optimal minimax guarantees
[RST10, RS14, RST15b, RST15a, BDR21].
To the best of our knowledge, the realizable setting has received much less attention. A notable excep-
tion is the work of [DG22] that focuses on realizable online regression using proper1 learners. They
provide an optimal regret bound with respect to the sequential fat-shattering dimension. However, as
they mention in their work (cf. Examples 1 and 2), this dimension does not characterize the optimal
cumulative loss bound.
Interestingly, Daskalakis and Golowich [DG22] leave the question of providing a dimension that
characterizes online realizable regression open. In our work, we resolve this question by providing
upper and lower bounds for the cumulative loss of the learner that are tight up to a constant factor of
2 using a novel combinatorial dimension that is related to (scaled) Littlestone trees (cf. Definition 13).
Formally, the setting of realizable online regression is defined as follows:
Definition 2 (Online Realizable Regression). Let ℓ: [0, 1]2 →R≥0 be a loss function. Consider
a class ℋ⊆[0, 1]𝒳for some domain 𝒳. The realizable online regression setting over 𝑇rounds
consists of the following interaction between the learner and the adversary:
• The adversary presents 𝑥𝑡∈𝒳.
• The learner predicts ̂︀𝑦𝑡∈[0, 1], possibly using randomization.
• The adversary reveals the true label 𝑦⋆
𝑡∈[0, 1] with the constraint that ∃ℎ⋆
𝑡∈ℋ, ∀𝜏≤
𝑡, ℎ(𝑥𝜏) = 𝑦⋆
𝜏.
• The learner suffers loss ℓ(̂︀𝑦𝑡, 𝑦⋆
𝑡).
The goal of the learner is to minimize its expected cumulative loss C𝑇= E
[︁∑︀
𝑡∈[𝑇] ℓ(̂︀𝑦𝑡, 𝑦⋆
𝑡)
]︁
.
We remark that in the definition of the cumulative loss C𝑇, the expectation is over the randomness of
the algorithm (which is the only stochastic aspect of the online setting). As we explained before, the
main question we study in this setting is the following:
1A learner is proper when the predictions ̂︀𝑦𝑡can be realized by some function ℎ𝑡∈ℋ.
3

Question 2. Can we characterize the optimal cumulative loss and design optimal online learners for
realizable regression?
Our main result in this setting provides a dimension that characterizes the optimal cumulative loss up
to a factor of 2 and provides an algorithm that achieves this bound.
1.1
Related Work
Our work makes progress towards the characterization of realizable regression both in the offline
and online settings. Similar results are known for the more studied settings of binary and mul-
ticlass (PAC/online) classification [Lit88, HLW94, BEHW89, Han16, DSS14, BCD+22, Nat89,
DSBDSS15, RBR09, BLM20, RST23, ABED+21, HKLM22, KKMV23, BGH+23]. The funda-
mental works of [BLW94, Sim97, ABDCBH97, BL98] study the question of PAC learnability for the
regression task. However, none of them characterizes learnability in the realizable setting. [Sim97]
showed that finiteness of scaled Natarajan dimension is necessary for realizable PAC regression.
[BL98] employed the one-inclusion graph (OIG) algorithm to get a real-valued predictor whose
expected error is upper bounded by the 𝑉𝛾-dimension, whose finiteness is sufficient but not necessary
for realizable learnability in this setting. We refer to [KS23] for details about this dimension which
was introduced in [ABDCBH97]. [ABDCBH97] showed that finiteness of the fat shattering dimen-
sion at all scales is equivalent to PAC learnability in the agnostic setting. Nevertheless, this does
not hold in the realizable case as Example 1 demonstrates. Similarly, the work of [BLW94] shows
that fat shattering dimension characterizes the regression task when the labels are corrupted by noise.
Recently, the concurrent and independent work of [AACSZ23] provides high-probability bounds for
the one-inclusion graph algorithm in the realizable PAC regression setting using the 𝑉𝛾-dimension.
We underline that this dimension does not characterize realizable regression. For more general losses,
see [Men02, BBM05].
In the area of online regression, the work of [DG22] studies the realizable setting of online regression
with the absolute loss (as we do) and presents a randomized proper learning algorithm which achieves
a near-optimal cumulative loss in terms of the sequential fat-shattering dimension of the hypothesis
class. We emphasize that this dimension does not tightly capture this setting and, hence, does
not address the question that we study. In particular, the lower bound they provide is related to
the unimprovability of a bound concerning the sequential fat-shattering dimension and does not
tightly capture the complexity of the problem. In the more general agnostic case, regret bounds
have been obtained in the work of [RST15a], using the sequential fat-shattering dimension and
sequential covering numbers. These quantities are also not tight in the realizable setting. See
also [RS14, RST15b, RST17]. Moreover, regression oracles have been used in contextual bandits
problems, see [FR20, SLX22] for further details.
Our characterization (cf. Theorem 2) for the offline setting via the OIG algorithm [HLW94] is further
motivated by the work of [MHS22], where they propose a dimension of similar flavor for adversarially
robust PAC learnability. We mention that recently the OIG algorithm has received a lot of attention
from the statistical learning theory community [AACSZ22, BHM+21, KVK22, SMB22, AHHM22,
CP22]. Finally, for a small sample of works that deal with offline and online regression problems, see
[BDGR22, Gol21, AH22, HKS19, She17, MKFI22] and the references therein.
2
PAC Learnability for Realizable Regression
In this section, we present various combinatorial dimensions that provide necessary or sufficient
conditions for learnability of real-valued functions. All the definitions that we consider have a similar
flavor. We first define what it means for a class ℋto “shatter” a set of 𝑛points and then we define the
dimension to be equal to the cardinality of the largest set that ℋcan shatter. Moreover, since we are
considering real-valued learning, these dimensions are parameterized by a scaling factor 𝛾∈(0, 1)
which should be interpreted as the distance that we can get to the optimal function. We start with the
standard notion of projection of a class to a set of unlabeled examples.
Definition 3 (Projection of ℋto 𝑆). Given 𝑆= {𝑥1, . . . , 𝑥𝑛} ∈𝒳𝑛, the projection of ℋ⊆𝒴𝒳to
𝑆is ℋ|𝑆= {(ℎ(𝑥1), . . . , ℎ(𝑥𝑛)) : ℎ∈ℋ}.
Furthermore, we say that a labeled sample 𝑆∈(𝒳× [0, 1])𝑛is realizable with respect to ℋif there
exists ℎ∈ℋsuch that ℎ(𝑥𝑖) = 𝑦𝑖, ∀𝑖∈[𝑛].
4

2.1
𝛾-Fat Shattering Dimension
Perhaps the most well-known dimension in the real-valued learning setting is the fat shattering
dimension that was introduced in [KS94]. Its definition is inspired by the pseudo-dimension [Pol90]
and it is, essentially, a scaled version of it.
Definition 4 (𝛾-Fat Shattering Dimension [KS94]). Let ℋ⊆[0, 1]𝒳. We say that a sample 𝑆∈𝒳𝑛
is 𝛾-fat shattered by ℋif there exist 𝑠1, . . . , 𝑠𝑛∈[0, 1]𝑛such that for all 𝑏∈{0, 1}𝑛there exists
ℎ𝑏∈ℋsuch that:
• ℎ𝑏(𝑥𝑖) ≥𝑠𝑖+ 𝛾, ∀𝑖∈[𝑛] such that 𝑏𝑖= 1.
• ℎ𝑏(𝑥𝑖) ≤𝑠𝑖−𝛾, ∀𝑖∈[𝑛] such that 𝑏𝑖= 0.
The 𝛾-fat shattering dimension Dfat
𝛾
is defined to be the maximum size of a 𝛾-fat shattered set.
In the realizable setting, finiteness of the fat-shattering dimension (at all scales) is sufficient for
learnability and it is equivalent to uniform convergence2 [BLW94]. However, the next example shows
that it is not a necessary condition for learnability. This comes in contrast to the agnostic case for
real-valued functions, in which learnability and uniform convergence are equivalent for all ℋ.
Example 1 (Realizable Learnability ⇏Finite Fat-Shattering Dimension, see Section 6 in [BLW94]).
Consider a class ℋ⊆[0, 1]𝒳where each hypothesis is uniquely identifiable by a single example, i.e.,
for any 𝑥∈𝒳and any 𝑓, 𝑔∈ℋwe have that 𝑓(𝑥) ̸= 𝑔(𝑥), unless 𝑓≡𝑔. Concretely, for every 𝑑∈
N, let {𝑆𝑗}0≤𝑗≤𝑑−1 be a partition of 𝒳and define ℋ𝑑=
{︀
ℎ𝑏0,...,𝑏𝑑−1 : 𝑏𝑖∈{0, 1}, 0 ≤𝑖≤𝑑−1
}︀
,
where
ℎ𝑏0,...,𝑏𝑑−1(𝑥) = 3
4
𝑑−1
∑︁
𝑗=0
1𝑆𝑗(𝑥)𝑏𝑗+ 1
8
𝑑−1
∑︁
𝑘=0
𝑏𝑘2−𝑘.
For any 𝛾≤1/4, Dfat
𝛾(ℋ𝑑) = 𝑑, since for a set of points 𝑥0, . . . , 𝑥𝑑−1 ∈𝒳such that each 𝑥𝑗belongs
to 𝑆𝑗, 0 ≤𝑗≤𝑑−1, ℋ𝑑contains all possible patterns of values above 3/4 and below 1/4. Indeed,
it is not hard to verify that for any 𝑗∈{0, . . . , 𝑑−1} if we consider any ℎ′ := ℎ𝑏0,...,𝑏𝑑−1 ∈ℋ𝑑
with 𝑏𝑗= 1 we have ℎ′(𝑥𝑗) ≥3/4. Similarly, if 𝑏𝑗= 0 then it holds that ℎ′(𝑥𝑗) ≤1/4. Hence,
Dfat
𝛾(∪𝑑∈Nℋ𝑑) = ∞. Nevertheless, by just observing one example (𝑥, ℎ⋆(𝑥)) any ERM learner finds
the exact labeling function ℎ⋆.
We remark that this example already shows that the PAC learnability landscape of regression is quite
different from that of multiclass classification, where agnostic learning and realizable learning are
characterized by the same dimension [BCD+22].
To summarize this subsection, the fat-shattering dimension is a natural way to quantify how well the
function class can interpolate (with gap 𝛾) some fixed function. Crucially, this interpolation contains
only inequalities (see Definition 4) and hence (at least intuitively) cannot be tight for the realizable
setting, where there exists some function that exactly labels the features. Example 1 gives a natural
example of a class with infinite fat-shattering dimension that can, nevertheless, be learned with a
single sample in the realizable setting.
2.2
𝛾-Natarajan Dimension
The 𝛾-Natarajan dimension was introduced by [Sim97] and is inspired by the Natarajan dimension
[Nat89], which has been used to derive bounds in the multiclass classification setting. Before
explaining the 𝛾-Natarajan dimension, let us begin by reminding to the reader the standard Natarajan
dimension. We say that a set 𝑆= {𝑥1, ..., 𝑥𝑛} of size 𝑛is Natarajan-shattered by a concept class
ℋ⊆𝒴𝒳if there exist two functions 𝑓, 𝑔: 𝑆→𝒴so that 𝑓(𝑥𝑖) ̸= 𝑔(𝑥𝑖) for all 𝑖∈[𝑛], and for all
𝑏∈{0, 1}𝑛there exists ℎ∈ℋsuch that ℎ(𝑥𝑖) = 𝑓(𝑥𝑖) if 𝑏𝑖= 1 and ℎ(𝑥𝑖) = 𝑔(𝑥𝑖) if 𝑏𝑖= 0. Note
that here we have equalities instead of inequalities (recall the fat-shattering case Definition 4).
From a geometric perspective (see [BCD+22]), this means that the space ℋprojected on the set
𝑆contains the set {𝑓(𝑥1), 𝑔(𝑥1)} × ... × {𝑓(𝑥𝑛), 𝑔(𝑥𝑛)}. This set is "isomorphic" to the Boolean
2Informally, uniform convergence means that for all distributions 𝒟, with high probability over the sample,
the error of all ℎ∈ℋon the sample is close to their true population error.
5

hypercube of size 𝑛by mapping 𝑓(𝑥𝑖) to 1 and 𝑔(𝑥𝑖) to 0 for all 𝑖∈[𝑛]. This means that the
Natarajan dimension is essentially the size of the largest Boolean cube contained in ℋ.
[Sim97] defines the scaled analogue of the above dimension as follows.
Definition 5 (𝛾-Natarajan Dimension [Sim97]). Let ℋ⊆[0, 1]𝒳. We say that a set 𝑆∈𝒳𝑛is
𝛾-Natarajan-shattered by ℋif there exist 𝑓, 𝑔: [𝑛] →[0, 1] such that for every 𝑖∈[𝑛] we have
ℓ(𝑓(𝑖), 𝑔(𝑖)) ≥2𝛾, and
ℋ|𝑆⊇{𝑓(1), 𝑔(1)} × . . . × {𝑓(𝑛), 𝑔(𝑛)} .
The 𝛾-Natarajan dimension DNat
𝛾
is defined to be the maximum size of a 𝛾-Natarajan-shattered set.
Intuitively, one should think of the 𝛾-Natarajan dimension as indicating the size of the largest Boolean
cube that is contained in ℋ. Essentially, every coordinate 𝑖∈[𝑛] gets its own translation of the 0, 1
labels of the Boolean cube, with the requirement that these two labels are at least 2𝛾far from each
other. [Sim97] showed the following result, which states that finiteness of the Natarajan dimension at
all scales is a necessary condition for realizable PAC regression:
Informal Theorem 1 (Theorem 3.1 in [Sim97]). ℋ⊆[0, 1]𝒳is PAC learnable in the realizable
regression setting only if DNat
𝛾
(ℋ) < ∞for all 𝛾∈(0, 1).
Concluding these two subsections, we have explained the main known general results about realizable
offline regression: (i) finiteness of fat-shattering is sufficient but not necessary for PAC learning and
(ii) finiteness of scaled Natarajan is necessary for PAC learning.
2.3
𝛾-Graph Dimension
We are now ready to introduce the 𝛾-graph dimension, which is a relaxation of the definition of the
𝛾-Natarajan dimension. To the best of our knowledge, it has not appeared in the literature before. Its
definition is inspired by its non-scaled analogue in multiclass classification [Nat89, DSS14].
Definition 6 (𝛾-Graph Dimension). Let ℋ⊆[0, 1]𝒳, ℓ: R2 →[0, 1]. We say that a sample 𝑆∈𝒳𝑛
is 𝛾-graph shattered by ℋif there exists 𝑓: [𝑛] :→[0, 1] such that for all 𝑏∈{0, 1}𝑛there exists
ℎ𝑏∈ℋsuch that:
• ℎ𝑏(𝑥𝑖) = 𝑓(𝑖), ∀𝑖∈[𝑛] such that 𝑏𝑖= 0.
• ℓ(ℎ𝑏(𝑥𝑖), 𝑓(𝑖)) > 𝛾, ∀𝑖∈[𝑛] such that 𝑏𝑖= 1.
The 𝛾-graph dimension DG
𝛾is defined to be the maximum size of a 𝛾-graph shattered set.
We mention that the asymmetry in the above definition is crucial. In particular, replacing the equality
ℎ𝑏(𝑥𝑖) = 𝑓(𝑖) with ℓ(ℎ𝑏(𝑥𝑖), 𝑓(𝑖)) ≤𝛾fails to capture the properties of the graph dimension.
Intuitively, the equality in the definition reflects the assumption of realizability, i.e., the guarantee
that there exists a hypothesis ℎ⋆that exactly fits the labels. Before stating our main result, we can
collect some useful observations about this new combinatorial measure. In particular, we provide
examples inspired by [DSS14, DSBDSS15] which show (i) that there exist gaps between different
ERM learners (see Example 3) and (ii) that any learning algorithm with a close to optimal sample
complexity must be improper (see Example 4).
Our first main result relates the scaled graph dimension with the learnability of any class ℋ⊆[0, 1]𝒳
using a (worst case) ERM learner. This result is the scaled analogue of known multiclass results
[DSS14, DSBDSS15] but its proof for the upper bound follows a different path. For the formal
statement of our result and its full proof, we refer the reader to Appendix B.
Informal Theorem 2 (Informal, see Theorem 1). Any ℋ⊆[0, 1]𝒳is PAC learnable in the realizable
regression setting by a worst-case ERM learner if and only if DG
𝛾(ℋ) < ∞for all 𝛾∈(0, 1).
Proof Sketch. The proof of the lower bound follows in a similar way as the lower bound regarding
learnability of binary hypothesis classes that have infinite VC dimension [VC71, BEHW89]. If ℋ
has infinite 𝛾-graph dimension for some 𝛾∈(0, 1), then for any 𝑛∈N we can find a sequence of
𝑛points 𝑥1, . . . , 𝑥𝑛that are 𝛾-graph shattered by ℋ. Then, we can define a distribution 𝒟that puts
most of its mass on 𝑥1, so if the learner observes 𝑛samples then with high probability it will not
observe at least half of the shattered points. By the definition of the 𝛾-graph dimension this shows
6

that, with high probability, there exists at least one ERM learner that is 𝛾-far on at least half of these
points. The result follows by taking 𝑛→∞.
The most technically challenging part of our result is the upper bound. There are three main steps in
our approach. First, we argue that by introducing a “ghost” sample of size 𝑛on top of the training
sample, which is also of size 𝑛, we can bound the true error of any ERM learner on the distribution
by twice its error on the ghost sample. This requires a slightly more subtle treatment compared to the
argument of [BEHW89] for binary classification. The second step is to use a “random swaps” type of
argument in order to bound the performance of the ERM learner on the ghost sample as follows: we
“merge” these two samples by considering any realizable sequence 𝑆of 2𝑛points, we randomly swap
elements whose indices differ by 𝑛, and then we consider an ERM learner who gets trained on the
first 𝑛points. We can show that if such a learner does not make many mistakes on the unseen part of
the sequence, in expectation over the random swaps, then it has a small error on the true distribution.
The main advantage of this argument is that it allows us to bound the number of mistakes of such a
learner on the unseen part of the sequence without using any information about 𝒟. The last step of the
proof is where we diverge from the argument of [BEHW89]. In order to show that this expectation is
small, we first map ℋto a partial concept class3 ℋ[AHHM22], then we project ℋto the sequence
𝑆, and finally we map ℋback to a total concept class by considering a disambiguation of it. Through
these steps, we can show that there will be at most (2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛)) many different functions in
this “projected” class. Then, we can argue that, with high probability, any ERM learner who sees the
first half of the sequence makes, in expectation over the random swaps, a small number of mistakes on
the second half of the sequence. Finally, we can take a union bound over all the (2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛))
possible such learners to conclude the proof.
2.4
𝛾-One-Inclusion Graph Dimension
In this section, we provide a minimax optimal learner for realizable PAC regression. We first review
a fundamental construction which is a crucial ingredient in the design of our learner, namely the
one-inclusion (hyper)graph (OIG) algorithm AOIG for a class ℋ⊆𝒴𝒳[HLW94, RBR09, DSS14,
BCD+22]. This algorithm gets as input a training set (𝑥1, 𝑦1), ..., (𝑥𝑛, 𝑦𝑛) realizable by ℋand an
additional example 𝑥. The goal is to predict the label of 𝑥. Let 𝑆= {𝑥1, ..., 𝑥𝑛, 𝑥}. The idea is to
construct the one-inclusion graph 𝐺OIG
ℋ|𝑆induced by the pair (𝑆, ℋ). The node set 𝑉of this graph
corresponds to the set ℋ|𝑆(projection of ℋto 𝑆) and, so, 𝑉⊆𝒴[𝑛+1]. For the binary classification
case, two vertices are connected with an edge if they differ in exactly one element 𝑥of the 𝑛+ 1
points in 𝑆. For the case where 𝒴is discrete and |𝒴| > 2, the hyperedge is generalized accordingly.
Definition 7 (One-Inclusion Hypergraph [HLW94, RBR09, BCD+22]). Consider the set [𝑛] and
a hypothesis class ℋ⊆𝒴[𝑛]. We define a graph 𝐺OIG
ℋ
= (𝑉, 𝐸) such that 𝑉= ℋ. Consider a
direction 𝑖∈[𝑛] and a mapping 𝑓: [𝑛] ∖{𝑖} →𝒴. We introduce the hyperedge 𝑒𝑖,𝑓= {ℎ∈𝑉:
ℎ(𝑗) = 𝑓(𝑗), ∀𝑗∈[𝑛] ∖{𝑖}}. We define the edge set of 𝐺OIG
ℋ
to be the collection
𝐸= {𝑒𝑖,𝑓: 𝑖∈[𝑛], 𝑓: [𝑛] ∖{𝑖} →𝒴, 𝑒𝑖,𝑓̸= ∅} .
In the regression setting, having created the one-inclusion graph with 𝒴= [0, 1], the goal is to orient
the edges; the crucial property is that “good” orientations of this graph yield learning algorithms with
low error. An orientation is good if the maximum out-degree of the graph is small (cf. Definition 8).
Informally, if the maximum out-degree of any node is 𝑀, then we can create a predictor whose
expected error rate is at most 𝑀/(𝑛+ 1). Note that in the above discussion we have not addressed
the issue that we deal with regression tasks and not classification and, hence, some notion of scale is
required in the definition of the out-degree.
Intuitively, the set 𝑆that induces the vertices ℋ|𝑆of the OIG consists of the features of the training
examples {𝑥1, ..., 𝑥𝑛} and the test point 𝑥. Hence, each edge of the OIG should be thought of as
the set of all potential labels of the test point that are realizable by ℋ, so it corresponds to the set
of all possible meaningful predictions for 𝑥. An orientation 𝜎maps every edge 𝑒to a vertex 𝑣∈𝑒
and, hence, it is equivalent to the prediction of a learning algorithm. We can now formally define the
notion of an orientation and the scaled out-degree.
3For an introduction to partial concept classes and their disambiguations we refer the reader to Appendix B.1.
7

Definition 8 (Orientation and Scaled Out-Degree). Let 𝛾∈[0, 1], 𝑛∈N, ℋ⊆[0, 1][𝑛]. An
orientation of the one-inclusion graph 𝐺OIG
ℋ
= (𝑉, 𝐸) is a mapping 𝜎: 𝐸→𝑉so that 𝜎(𝑒) ∈𝑒for
any 𝑒∈𝐸. Let 𝜎𝑖(𝑒) ∈[0, 1] denote the 𝑖-th entry of the orientation.
For a vertex 𝑣∈𝑉, corresponding to some hypothesis ℎ∈ℋ(see Definition 7), let 𝑣𝑖be the 𝑖-th entry
of 𝑣, which corresponds to ℎ(𝑖). The (scaled) out-degree of a vertex 𝑣under 𝜎is outdeg(𝑣; 𝜎, 𝛾) =
|{𝑖∈[𝑛] : ℓ(𝜎𝑖(𝑒𝑖,𝑣), 𝑣𝑖) > 𝛾}|. The maximum (scaled) out-degree of 𝜎is outdeg(𝜎, 𝛾) =
max𝑣∈𝑉outdeg(𝑣; 𝜎, 𝛾).
Finally, we introduce the following novel dimension in the context of real-valued regression. An
analogous dimension was proposed in the context of learning under adversarial robustness [MHS22].
Definition 9 (𝛾-OIG Dimension). Consider a class ℋ⊆[0, 1]𝒳and let 𝛾∈[0, 1]. We define the
𝛾-one-inclusion graph dimension DOIG
𝛾
of ℋas follows:
DOIG
𝛾
(ℋ) = sup{𝑛∈N : ∃𝑆∈𝒳𝑛such that ∃finite subgraph 𝐺= (𝑉, 𝐸) of 𝐺OIG
ℋ|𝑆= (𝑉𝑛, 𝐸𝑛)
such that ∀orientations 𝜎, ∃𝑣∈𝑉, where outdeg(𝑣; 𝜎, 𝛾) > 𝑛/3} .
We define the dimension to be infinite if the supremum is not attained by a finite 𝑛.
In words, it is the largest 𝑛∈N (potentially ∞) such that there exists an (unlabeled) sequence 𝑆of
length 𝑛with the property that no matter how one orients some finite subgraph of the one-inclusion
graph, there is always some vertex for which at least 1/3 of its coordinates are 𝛾-different from the
labels of the edges that are attached to this vertex. We remark that the hypothesis class in Example 1
has 𝛾-OIG dimension equal to 𝑂(1). Moreover, a finite fat-shattering dimension of hypothesis class
implies a finite OIG dimension of roughly the same size (see Appendix C). We also mention that the
above dimension satisfies the “finite character” property and the remaining criteria that dimensions
should satisfy according to [BDHM+19] (see Appendix F). As our main result in this section, we
show that any class ℋis learnable if and only if this dimension is finite and we design an (almost)
optimal learner for it.
Informal Theorem 3 (Informal, see Theorem 2). Any ℋ⊆[0, 1]𝒳is PAC learnable in the realizable
regression setting if and only if DOIG
𝛾
(ℋ) < ∞for all 𝛾∈(0, 1).
The formal statement and its full proof are postponed to Appendix C.
Proof Sketch. We start with the lower bound. As we explained before, orientations of the one-
inclusion graph are, in some sense, equivalent to learning algorithms. Therefore, if this dimension is
infinite for some 𝛾> 0, then for any 𝑛∈N, there are no orientations with small maximum out-degree.
Thus, for any learner we can construct some distribution 𝒟under which it makes a prediction that is
𝛾-far from the correct one with constant probability, which means that ℋis not PAC learnable.
Let us now describe the proof of the converse direction, which consists of several steps. First,
notice that the finiteness of this dimension provides good orientations for finite subgraphs of the one-
inclusion graph. Using the compactness theorem of first-order logic, we can extend them to a good
orientation of the whole, potentially infinite, one-inclusion graph. This step gives us a weak learner
with the following property: for any given 𝛾there is some 𝑛0 ∈N so that, with high probability
over the training set, when it is given 𝑛0 examples as its training set it makes mistakes that are of
order at least 𝛾on a randomly drawn point from 𝒟with probability at most 1/3. The next step is
to boost the performance of this weak learner. This is done using the “median-boosting” technique
[Kég03] (cf. Algorithm 2) which guarantees that after a small number of iterations we can create an
ensemble of weak learners such that, a prediction rule according to their (weighted) median will not
make any 𝛾-mistakes on the training set. However, this is not sufficient to prove that the ensemble of
these learners has small loss on the distribution 𝒟. This is done by establishing sample compression
schemes that have small length. Essentially, such schemes consist of a compression function 𝜅which
takes as input a training set and outputs a subset of it, and a reconstruction function 𝜌which takes as
input the output of 𝜅and returns a predictor whose error on every point of the training set 𝑆is at most
𝛾. Extending the arguments of [LW86] from the binary setting to the real-valued setting we show
that the existence of such a scheme whose compression function returns a set of “small” cardinality
implies generalization properties of the underlying learning rule. Finally, we show that our weak
learner combined with the boosting procedure admit such a sample compression scheme.
Before proceeding to the next section, one could naturally ask whether there is a natural property of
the concept class that implies finiteness of the scaled OIG dimension. The work of [Men02] provides
8

a sufficient and natural condition that implies finiteness of our complexity measure. In particular,
Mendelson shows that classes that contain functions with bounded oscillation (as defined in [Men02])
have finite fat-shattering dimension. This implies that the class is learnable in the agnostic setting
and hence is also learnable in the realizable setting. As a result, the OIG-based dimension is also
finite. So, bounded oscillations are a general property that guarantees that the finiteness of OIG-based
dimension and fat-shattering dimension coincide. We also mention that deriving bounds for the
OIG-dimension for interesting families of functions is an important yet non-trivial question.
2.5
𝛾-DS Dimension
So far we have identified a dimension (cf. Definition 9) that characterizes the PAC learnability of
realizable regression. However, it might not be easy to calculate it in some settings. Our goal in
this section is to introduce a relaxation of this definition which we conjecture that also characterizes
learnability in this context. This new dimension is inspired by the DS dimension, a combinatorial
dimension defined by Daniely and Shalev-Shwartz in [DSS14]. In a recent breakthrough result,
[BCD+22] showed that the DS dimension characterizes multiclass learnability (with a possibly
unbounded number of labels and the 0-1 loss). We introduce a scaled version of the DS dimension.
To this end, we first define the notion of a scaled pseudo-cube.
Definition 10 (Scaled Pseudo-Cube). Let 𝛾∈[0, 1]. A class ℋ⊆[0, 1]𝑑is called a 𝛾-pseudo-cube
of dimension 𝑑if it is non-empty, finite and, for any 𝑓∈ℋand direction 𝑖∈[𝑑], the hyper-edge
𝑒𝑖,𝑓= {𝑔∈ℋ: 𝑔(𝑗) = 𝑓(𝑗) ∀𝑗∈[𝑑], 𝑖̸= 𝑗} satisfies |𝑒𝑖,𝑓| > 1 and ℓ(𝑔1(𝑖), 𝑔2(𝑖)) > 𝛾for any
𝑔1, 𝑔2 ∈𝑒𝑖,𝑓, 𝑔1 ̸= 𝑔2.
Pseudo-cubes can be seen as a relaxation of the notion of a Boolean cube (which should be intuitively
related with the Natarajan dimension) and were a crucial tool in the proof of [BCD+22]. In our
setting, scaled pseudo-cubes will give us the following combinatorial dimension.
Definition 11 (𝛾-DS Dimension). Let ℋ⊆[0, 1]𝒳. A set 𝑆∈𝒳𝑛is 𝛾-DS shattered if ℋ|𝑆contains
an 𝑛-dimensional 𝛾-pseudo-cube. The 𝛾-DS dimension DDS
𝛾
is the maximum size of a 𝛾-DS-shattered
set.
Extending the ideas from the multiclass classification setting, we show that the scaled-DS dimension
is necessary for realizable PAC regression. Simon (Section 6, [Sim97]) left as an open direction to
“obtain supplementary lower bounds [for realizable regression] (perhaps completely unrelated to the
combinatorial or Natarajan dimension)”. Our next result is a novel contribution to this direction.
Informal Theorem 4 (Informal, see Theorem 3). Any ℋ⊆[0, 1]𝒳is PAC learnable in the realizable
regression setting only if DDS
𝛾(ℋ) < ∞for all 𝛾∈(0, 1).
The proof is postponed to Appendix D. We believe that finiteness of DDS
𝛾(ℋ) is also a sufficient
condition for realizable regression. However, the approach of [BCD+22] that establishes a similar
result in the setting of multiclass classification does not extend trivially to the regression setting.
Conjecture 1 (Finite 𝛾-DS is Sufficient). Let ℋ⊆[0, 1]𝒳. If DDS
𝛾(ℋ) < ∞for all 𝛾∈(0, 1), then
ℋis PAC learnable in the realizable regression setting.
3
Online Learnability for Realizable Regression
In this section we will provide our main result regarding realizable online regression. Littlestone
trees have been the workhorse of online classification problems [Lit88]. First, we provide a definition
for scaled Littlestone trees.
Definition 12 (Scaled Littlestone Tree). A scaled Littlestone tree of depth 𝑑≤∞is a complete
binary tree of depth 𝑑defined as a collection of nodes
⋃︁
0≤ℓ<𝑑
{︀
𝑥𝑢∈𝒳: 𝑢∈{0, 1}ℓ}︀
= {𝑥∅} ∪{𝑥0, 𝑥1} ∪{𝑥00, 𝑥01, 𝑥10, 𝑥11} ∪...
and real-valued gaps
⋃︁
0≤ℓ<𝑑
{︀
𝛾𝑢∈[0, 1] : 𝑢∈{0, 1}ℓ}︀
= {𝛾∅} ∪{𝛾0, 𝛾1} ∪{𝛾00, 𝛾01, 𝛾10, 𝛾11} ∪...
9

such that for every path 𝑦∈{0, 1}𝑑and finite 𝑛< 𝑑, there exists ℎ∈ℋso that ℎ(𝑥𝑦≤ℓ) = 𝑠𝑦≤ℓ+1
for 0 ≤ℓ≤𝑛, where 𝑠𝑦≤ℓ+1 is the label of the edge connecting the nodes 𝑥𝑦≤ℓand 𝑥𝑦≤ℓ+1 and
ℓ(𝑠𝑦≤ℓ,0, 𝑠𝑦≤ℓ,1) = 𝛾𝑦≤ℓ.
In words, scaled Littlestone trees are complete binary trees whose nodes are points of 𝒳and the
two edges attached to every node are its potential classifications. An important quantity is the gap
between the two values of the edges. We define the online dimension Donl(ℋ) as follows.
Definition 13 (Online Dimension). Let ℋ⊆[0, 1]𝒳.
Let 𝒯𝑑be the space of all scaled Lit-
tlestone trees of depth 𝑑(cf.
Definition 12) and 𝒯
= ⋃︀∞
𝑑=0 𝒯𝑑.
For any scaled tree 𝑇=
⋃︀
0≤ℓ≤dep(𝑇)
{︀
(𝑥𝑢, 𝛾𝑢) ∈(𝒳, [0, 1]) : 𝑢∈{0, 1}ℓ}︀
, let 𝒫(𝑇) = {𝑦= (𝑦0, ..., 𝑦dep(𝑇)) : 𝑦𝑖∈
{0, 1}𝑖)} be the set of all paths in 𝑇. The dimension Donl(ℋ) is
Donl(ℋ) = sup
𝑇∈𝒯
inf
𝑦∈𝒫(𝑇)
dep(𝑇)
∑︁
𝑖=0
𝛾𝑦𝑖.
(1)
In words, this dimension considers the tree that has the maximum sum of label gaps over its path with
the smallest such sum, among all the trees of arbitrary depth. Note that we are taking the supremum
(infimum) in case there is no tree (path) that achieves the optimal value. Providing a characterization
and a learner with optimal cumulative loss C𝑇for realizable online regression was left as an open
problem by [DG22]. We resolve this question (up to a factor of 2) by showing the following result.
Informal Theorem 5 (Informal, see Theorem 4). For any ℋ⊆[0, 1]𝒳and 𝜀> 0, there exists a
deterministic learner with C∞≤Donl(ℋ) + 𝜀and any, potentially randomized, learner satisfies
C∞≥Donl(ℋ)/2 −𝜀.
The formal statement and the full proof of our results can be found in Appendix E. First, we underline
that this result holds when there is no bound on the number of rounds that the learner and the
adversary interact. This follows the same spirit as the results in the realizable binary and multiclass
classification settings [Lit88, DSBDSS15]. The dimension that characterizes the minimax optimal
cumulative loss for any given 𝑇follows by taking the supremum in Definition 13 over trees whose
depth is at most 𝑇and the proof follows in an identical way (note that even with finite fixed depth 𝑇
the supremum is over infinitely many trees). Let us now give a sketch of our proofs.
Proof Sketch. The lower bound follows using similar arguments as in the classification setting: for
any 𝜀> 0, the adversary can create a scaled Littlestone tree 𝑇that achieves the sup inf bound, up to
an additive 𝜀. In the first round, the adversary presents the root of the tree 𝑥∅. Then, no matter what the
learner picks the adversary can force error at least 𝛾∅/2. The game is repeated on the new subtree. The
proof of the upper bound presents the main technical challenge to establish Theorem 4. The strategy
of the learner can be found in Algorithm 3. The key insight in the proof is that, due to realizability,
we can show that in every round 𝑡there is some ̂︀𝑦𝑡∈[0, 1] the learner can predict so that, no matter
what the adversary picks as the true label 𝑦⋆
𝑡, the online dimension of the class under the extra
restriction that ℎ(𝑥𝑡) = 𝑦⋆
𝑡, i.e, the updated version space 𝑉= {ℎ∈ℋ: ℎ(𝑥𝜏) = 𝑦⋆
𝜏, 1 ≤𝜏≤𝑡},
will decrease by ℓ(𝑦⋆
𝑡, ̂︀𝑦𝑡). Thus, under this strategy of the learner, the adversary can only distribute
up to Donl(ℋ) across all the rounds of the interaction. We explain how the learner can find such a ̂︀𝑦𝑡
and we handle technical issues that arise due to the fact that we are dealing with sup inf instead of
max min in the formal proof (cf. Appendix E).
4
Conclusion
In this work, we developed optimal learners for realizable regression in PAC learning and online
learning. Moreover, we identified combinatorial dimensions that characterize learnability in these
settings. We hope that our work can lead to simplified characterizations for these problems. We believe
that the main limitation of our work is that the OIG-based dimension we propose is more complicated
than the dimensions that have been proposed in the past, like the fat-shattering dimension (which,
as we explain, does not characterize learnability in the realizable regression setting). Nevertheless,
despite its complexity, this is the first dimension that characterizes learnability in the realizable
regression setting. More to that, our work leaves as an important next step to prove (or disprove) the
conjecture that the (combinatorial and simpler) 𝛾-DS dimension is qualitatively equivalent to the
𝛾-OIG dimension. Another future direction, that is not directly related to this conjecture, is to better
understand the gap between the fat-shattering dimension and the OIG-based dimension.
10

Acknowledgements
Amin Karbasi acknowledges funding in direct support of this work from NSF (IIS-1845032), ONR
(N00014- 19-1-2406), and the AI Institute for Learning-Enabled Optimization at Scale (TILOS).
Grigoris Velegkas is supported by TILOS, the Onassis Foundation, and the Bodossaki Foundation.
This work was done in part while some of the authors were visiting Archimedes AI Research Center.
References
[AACSZ22] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhivo-
tovskiy. The one-inclusion graph algorithm is not always optimal. arXiv preprint
arXiv:2212.09270, 2022. 4
[AACSZ23] Ishaq Aden-Ali, Yeshwanth Cherapanamjeri, Abhishek Shetty, and Nikita Zhiv-
otovskiy.
Optimal pac bounds without uniform convergence.
arXiv preprint
arXiv:2304.09167, 2023. 4
[AB99] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical founda-
tions, volume 9. Cambridge university press Cambridge, 1999. 18, 21
[ABDCBH97] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-
sensitive dimensions, uniform convergence, and learnability. Journal of the ACM
(JACM), 44(4):615–631, 1997. 2, 4
[ABDH+20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas
Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust
learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM),
67(6):1–42, 2020. 25
[ABED+21] Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, and Eylon
Yogev. Adversarial laws of large numbers and optimal regret in online classification.
In Proceedings of the 53rd annual ACM SIGACT symposium on theory of computing,
pages 447–455, 2021. 3, 4
[AH22] Idan Attias and Steve Hanneke. Adversarially robust learning of real-valued functions.
arXiv preprint arXiv:2206.12977, 2022. 4, 25
[AHHM22] Noga Alon, Steve Hanneke, Ron Holzman, and Shay Moran. A theory of pac
learnability of partial concept classes. In 2021 IEEE 62nd Annual Symposium on
Foundations of Computer Science (FOCS), pages 658–671. IEEE, 2022. 4, 7, 17, 19,
25
[AHM22] Idan Attias, Steve Hanneke, and Yishay Mansour.
A characterization of semi-
supervised adversarially-robust pac learnability. arXiv preprint arXiv:2202.05420,
2022. 25
[Bac21] Francis Bach. Learning theory from first principles. Online version, 2021. 1
[BBM05] Peter Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complex-
ities. Annals of Statistics, 33(4):1497–1537, 2005. 4
[BCD+22] Nataly Brukhim, Daniel Carmon, Irit Dinur, Shay Moran, and Amir Yehudayoff. A
characterization of multiclass learnability. In 2022 IEEE 63rd Annual Symposium on
Foundations of Computer Science (FOCS), pages 943–955. IEEE, 2022. 1, 4, 5, 7, 9,
25, 28
[BDGR22] Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed
online learning is as easy as statistical learning. In Conference on Learning Theory,
pages 1716–1786. PMLR, 2022. 4
[BDHM+19] Shai Ben-David, Pavel Hrubeš, Shay Moran, Amir Shpilka, and Amir Yehudayoff.
Learnability can be undecidable. Nature Machine Intelligence, 1(1):44–48, 2019. 8,
30
11

[BDPSS09] Shai Ben-David, Dávid Pál, and Shai Shalev-Shwartz. Agnostic online learning. In
COLT, volume 3, page 1, 2009. 2
[BDR21] Adam Block, Yuval Dagan, and Alexander Rakhlin. Majorizing measures, sequential
complexities, and online learning. In Conference on Learning Theory, pages 587–590.
PMLR, 2021. 3
[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM),
36(4):929–965, 1989. 1, 4, 6, 7
[BGH+23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann
Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections
between replicability, privacy, and adaptive generalization. In Proceedings of the
55th Annual ACM Symposium on Theory of Computing, pages 520–527, 2023. 4
[BHM+21] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon Van Handel, and Amir
Yehudayoff. A theory of universal learning. In Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing, pages 532–541, 2021. 4
[BHMZ20] Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy. Proper
learning, helly number, and an optimal svm bound. In Conference on Learning
Theory, pages 582–609. PMLR, 2020. 25
[BL98] Peter L Bartlett and Philip M Long. Prediction, learning, uniform convergence, and
scale-sensitive dimensions. Journal of Computer and System Sciences, 56(2):174–
190, 1998. 2, 4
[BLM20] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification
and online prediction. In 2020 IEEE 61st Annual Symposium on Foundations of
Computer Science (FOCS), pages 389–402. IEEE, 2020. 4
[BLW94] Peter L Bartlett, Philip M Long, and Robert C Williamson. Fat-shattering and the
learnability of real-valued functions. In Proceedings of the seventh annual conference
on Computational learning theory, pages 299–310, 1994. 2, 4, 5
[CKW08] Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple
sources. Journal of Machine Learning Research, 9(8), 2008. 31
[CP22] Moses Charikar and Chirag Pabbaraju. A characterization of list learnability. arXiv
preprint arXiv:2211.04956, 2022. 4
[DG17] Dheeru Dua and Casey Graff. UCI machine learning repository. 2017. 1
[DG22] Constantinos Daskalakis and Noah Golowich. Fast rates for nonparametric online
learning: from realizability to learning in games. In Proceedings of the 54th Annual
ACM SIGACT Symposium on Theory of Computing, pages 846–859, 2022. 1, 2, 3, 4,
10, 28
[DKLD84] Richard M Dudley, H Kunita, F Ledrappier, and RM Dudley. A course on empirical
processes. In Ecole d’été de probabilités de Saint-Flour XII-1982, pages 1–142.
Springer, 1984. 2
[DMY16] Ofir David, Shay Moran, and Amir Yehudayoff. Supervised learning through the lens
of compression. Advances in Neural Information Processing Systems, 29, 2016. 25,
26
[DSBDSS15] Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass
learnability and the erm principle. J. Mach. Learn. Res., 16(1):2377–2404, 2015. 1,
3, 4, 6, 10, 25, 30
[DSS14] Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems.
In Conference on Learning Theory, pages 287–316. PMLR, 2014. 1, 4, 6, 7, 9, 17,
25, 30
12

[FHMM23] Yuval Filmus, Steve Hanneke, Idan Mehalel, and Shay Moran. Optimal predic-
tion using expert advice and randomized littlestone dimension.
arXiv preprint
arXiv:2302.13849, 2023. 30
[FR20] Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual
bandits with regression oracles. In International Conference on Machine Learning,
pages 3199–3210. PMLR, 2020. 4
[FW95] Sally Floyd and Manfred Warmuth. Sample compression, learnability, and the
vapnik-chervonenkis dimension. Machine learning, 21(3):269–304, 1995. 25
[GBC16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press,
2016. 1
[GHST05] Thore Graepel, Ralf Herbrich, and John Shawe-Taylor. Pac-bayesian compression
bounds on the prediction error of learning algorithms for classification. Machine
Learning, 59:55–76, 2005. 25
[GKN14] Lee-Ad Gottlieb, Aryeh Kontorovich, and Pinhas Nisnevitch. Near-optimal sample
compression for nearest neighbors. Advances in Neural Information Processing
Systems, 27, 2014. 25
[Gol21] Noah Golowich. Differentially private nonparametric regression under a growth
condition. In Conference on Learning Theory, pages 2149–2192. PMLR, 2021. 4
[Han16] Steve Hanneke. The optimal sample complexity of pac learning. The Journal of
Machine Learning Research, 17(1):1319–1333, 2016. 1, 4
[HKLM22] Max Hopkins, Daniel M Kane, Shachar Lovett, and Gaurav Mahajan. Realizable
learning is all you need. In Conference on Learning Theory, pages 3015–3069.
PMLR, 2022. 4, 31
[HKS18] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Agnostic sample
compression for linear regression. arXiv preprint arXiv:1810.01864, 2018. 25
[HKS19] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample com-
pression for real-valued learners. In Algorithmic Learning Theory, pages 466–488.
PMLR, 2019. 2, 4, 25, 26
[HLW94] David Haussler, Nick Littlestone, and Manfred K Warmuth. Predicting {0, 1}-
functions on randomly drawn points. Information and Computation, 115(2):248–292,
1994. 1, 4, 7
[J´S03] Tadeusz Januszkiewicz and Jacek ´Swi ˛atkowski. Hyperbolic coxeter groups of large
dimension. Commentarii Mathematici Helvetici, 78(3):555–583, 2003. 28
[Kég03] Balázs Kégl. Robust regression by boosting the median. In Learning Theory and
Kernel Machines, pages 258–272. Springer, 2003. 8, 25
[KKMV23] Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. Statistical
indistinguishability of learning algorithms. arXiv preprint arXiv:2305.14311, 2023.
4
[KS94] Michael J Kearns and Robert E Schapire. Efficient distribution-free learning of
probabilistic concepts. Journal of Computer and System Sciences, 48(3):464–497,
1994. 5
[KS23] Pieter Kleer and Hans Simon. Primal and dual combinatorial dimensions. Discrete
Applied Mathematics, 327:185–196, 2023. 4
[KSW17] Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Nearest-neighbor sample compres-
sion: Efficiency, consistency, infinite dimensions. Advances in Neural Information
Processing Systems, 30, 2017. 25
13

[KVK22] Alkis Kalavasis, Grigoris Velegkas, and Amin Karbasi. Multiclass learnability beyond
the pac framework: Universal rates and partial concept classes. arXiv preprint
arXiv:2210.02297, 2022. 4
[Lit88] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-
threshold algorithm. Machine learning, 2:285–318, 1988. 1, 2, 4, 9, 10
[LW86] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability.
1986. 8, 25, 26
[Men02] Shahar Mendelson. Improving the sample complexity using global data. IEEE
transactions on Information Theory, 48(7):1977–1991, 2002. 4, 8, 9
[MHS19] Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially
robustly learnable, but only improperly. In Conference on Learning Theory, pages
2512–2530. PMLR, 2019. 25
[MHS20] Omar Montasser, Steve Hanneke, and Nati Srebro. Reducing adversarially robust
learning to non-robust pac learning. Advances in Neural Information Processing
Systems, 33:14626–14637, 2020. 25
[MHS21] Omar Montasser, Steve Hanneke, and Nathan Srebro. Adversarially robust learning
with unknown perturbation sets. In Conference on Learning Theory, pages 3452–
3482. PMLR, 2021. 25
[MHS22] Omar Montasser, Steve Hanneke, and Nati Srebro. Adversarially robust learning: A
generic minimax optimal learner and characterization. Advances in Neural Informa-
tion Processing Systems, 35:37458–37470, 2022. 4, 8, 25
[MKFI22] Jason Milionis, Alkis Kalavasis, Dimitris Fotakis, and Stratis Ioannidis. Differen-
tially private regression with unbounded covariates. In International Conference on
Artificial Intelligence and Statistics, pages 3242–3273. PMLR, 2022. 4
[MY16] Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes.
Journal of the ACM (JACM), 63(3):1–10, 2016. 25, 27
[Nat89] Balas K Natarajan. On learning sets and functions. Machine Learning, 4(1):67–97,
1989. 1, 4, 5, 6
[Osa13] Damian L Osajda. A construction of hyperbolic coxeter groups. Commentarii
Mathematici Helvetici, 88(2):353–367, 2013. 28
[Pol90] David Pollard. Empirical processes: theory and applications. Ims, 1990. 5
[Pol12] David Pollard. Convergence of stochastic processes. Springer Science & Business
Media, 2012. 2
[RBR09] Benjamin IP Rubinstein, Peter L Bartlett, and J Hyam Rubinstein. Shifting: One-
inclusion mistake bounds and sample compression. Journal of Computer and System
Sciences, 75(1):37–59, 2009. 4, 7
[RS14] Alexander Rakhlin and Karthik Sridharan. Online non-parametric regression. In
Conference on Learning Theory, pages 1232–1264. PMLR, 2014. 3, 4
[RST10] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random
averages, combinatorial parameters, and learnability. Advances in Neural Information
Processing Systems, 23, 2010. 3
[RST15a] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning via
sequential complexities. J. Mach. Learn. Res., 16(1):155–186, 2015. 3, 4, 30
[RST15b] Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities
and uniform martingale laws of large numbers. Probability theory and related fields,
161:111–153, 2015. 3, 4, 30
14

[RST17] Alexander Rakhlin, Karthik Sridharan, and Alexandre B Tsybakov. Empirical entropy,
minimax regret and minimax risk. 2017. 4
[RST23] Vinod Raman, Unique Subedi, and Ambuj Tewari. A characterization of online
multiclass learnability. arXiv preprint arXiv:2303.17716, 2023. 1, 3, 4
[She17] Or Sheffet. Differentially private ordinary least squares. In International Conference
on Machine Learning, pages 3105–3114. PMLR, 2017. 4
[Sim97] Hans Ulrich Simon. Bounds on the number of examples needed for learning functions.
SIAM Journal on Computing, 26(3):751–763, 1997. 2, 3, 4, 5, 6, 9
[SLX22] David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler op-
timal algorithm for contextual bandits under realizability. Mathematics of Operations
Research, 47(3):1904–1931, 2022. 4
[SMB22] Han Shao, Omar Montasser, and Avrim Blum. A theory of pac learnability under
transformation invariances. arXiv preprint arXiv:2202.07552, 2022. 4
[SSSSS10] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnabil-
ity, stability and uniform convergence. The Journal of Machine Learning Research,
11:2635–2670, 2010. 3
[Val84] Leslie G Valiant.
A theory of the learnable.
Communications of the ACM,
27(11):1134–1142, 1984. 1
[Vap99] Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on
neural networks, 10(5):988–999, 1999. 1
[VC71] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability and its Applications, 16(2):264–
280, 1971. 6
[WHEY15] Yair Wiener, Steve Hanneke, and Ran El-Yaniv.
A compression technique for
analyzing disagreement-based active learning. J. Mach. Learn. Res., 16:713–745,
2015. 25
15

A
Notation and Definitions
We first overview standard definitions about sample complexity in PAC learning.
PAC Sample Complexity.
The realizable PAC sample complexity ℳ(ℋ; 𝜀, 𝛿) of ℋis defined as
ℳ(ℋ; 𝜀, 𝛿) = inf
𝐴∈𝒜ℳ𝐴(ℋ; 𝜀, 𝛿) ,
(1)
where the infimum is over all possible learning algorithms and ℳ𝐴(ℋ; 𝜀, 𝛿) is the minimal integer
such that for any 𝑚≥ℳ𝐴(ℋ; 𝜀, 𝛿), every distribution 𝒟𝒳on 𝒳, and, true target ℎ⋆∈ℋ, the
expected loss E𝑥∼𝒟𝒳[ℓ(𝐴(𝑇)(𝑥), ℎ⋆(𝑥))] of 𝐴is at most 𝜀with probability 1 −𝛿over the training
set 𝑇= {(𝑥, ℎ⋆(𝑥)) : 𝑥∈𝑆}, 𝑆∼𝒟𝑚
𝒳.
PAC Cut-Off Sample Complexity.
We slightly overload the notation of the sample complexity
and we define
ℳ(ℋ; 𝜀, 𝛿, 𝛾) = inf
𝐴∈𝒜ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾) ,
(2)
where the infimum is over all possible learning algorithms and ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾) is the minimal integer
such that for any 𝑚≥ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾), every distribution 𝒟𝒳on 𝒳, and, true target ℎ⋆∈ℋ, the
expected cut-off loss Pr𝑥∼𝒟𝒳[ℓ(𝐴(𝑇)(𝑥), ℎ⋆(𝑥)) > 𝛾] of 𝐴is at most 𝜀with probability 1 −𝛿over
the training set 𝑇= {(𝑥, ℎ⋆(𝑥)) : 𝑥∈𝑆}, 𝑆∼𝒟𝑚
𝒳.
Lemma 1 (Equivalence Between Sample Complexities). For every 𝜀, 𝛿∈(0, 1)2 and every ℋ⊆
[0, 1]𝒳, where 𝒳is the input domain, it holds that
ℳ(ℋ; √𝜀, 𝛿, √𝜀) ≤ℳ(ℋ; 𝜀, 𝛿) ≤ℳ(ℋ; 𝜀/2, 𝛿, 𝜀/2)
Proof. Let 𝐴be a learning algorithm. We will prove the statement for each fixed 𝐴and for each data-
generating distribution 𝒟, so the result follows by taking the infimum over the learning algorithms.
Assume that the cut-off sample complexity of 𝐴is ℳ𝐴(ℋ; 𝜀/2, 𝛿, 𝜀/2). Then, with probability 1 −𝛿
over the training sample 𝑆∼𝒟, for its expected loss it holds that
E
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥))] ≤𝜀
2 +
(︁
1 −𝜀
2
)︁
· 𝜀
2 ≤𝜀,
thus, ℳ𝐴(ℋ; 𝜀, 𝛿) ≤ℳ𝐴(ℋ; 𝜀/2, 𝛿, 𝜀/2).
The other direction follows by using Markov’s inequality. In particular, if we have that with probability
at least 1 −𝛿over 𝑆∼𝒟it holds that
E
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥))] ≤𝜀,
then Markov’s inequality gives us that
Pr
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥)) ≥√𝜀] ≤√𝜀,
which shows that ℳ𝐴(ℋ; 𝜀, 𝛿) ≥ℳ𝐴(ℋ; √𝜀, 𝛿, √𝜀).
ERM Sample Complexity.
In the special case where 𝒜is the class ERM of all possible ERM
algorithms, i.e., algorithms that return a hypothesis whose sample error is exactly 0, we define the
ERM sample complexity as the number of samples required by the worst-case ERM algorithm, i.e.,
ℳERM(ℋ; 𝜀, 𝛿) =
sup
𝐴∈ERM
ℳ𝐴(ℋ; 𝜀, 𝛿) ,
(3)
and its cut-off analogue as
ℳERM(ℋ; 𝜀, 𝛿, 𝛾) =
sup
𝐴∈ERM
ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾) ,
(4)
B
𝛾-Graph Dimension and ERM Learnability
In this section, we show that 𝛾-graph dimension determines the learnability of ℋ⊆[0, 1]𝒳using any
ERM learner. We first revisit the notion of partial concept classes which will be useful for deriving
our algorithms.
16

B.1
Partial Concept Classes and A Naive Approach that Fails
[AHHM22] proposed an extension of the binary PAC model to handle partial concept classes, where
ℋ⊆{0, 1, ⋆}𝒳, for some input domain 𝒳, where ℎ(𝑥) = ⋆should be thought of as ℎnot knowing
the label of 𝑥∈𝒳. The main motivation behind their work is that partial classes allow one to
conveniently express data-dependent assumptions. As an intuitive example, a halfspace with margin
is a partial function that is undefined inside the forbidden margin and is a well-defined halfspace
outside the margin boundaries. Instead of dealing with concept classes ℋ⊆𝒴𝒳where each concept
ℎ∈ℋis a total function ℎ: 𝒳→𝒴, we study partial concept classes ℋ⊆(𝒴∪{⋆})𝒳, where
each concept ℎis now a partial function and ℎ(𝑥) = ⋆means that the function ℎis undefined at 𝑥.
We define the support of ℎas the set supp(ℎ) = {𝑥∈𝒳: ℎ(𝑥) ̸= ⋆}. Similarly as in the case of
total classes, we say that a finite sequence 𝑆= (𝑥1, 𝑦1, . . . , 𝑥𝑛, 𝑦𝑛) is realizable with respect to ℋif
there exists some ℎ* ∈ℋsuch that ℎ*(𝑥𝑖) = 𝑦𝑖, ∀𝑖∈[𝑛].
An important notion related to partial concept classes is that of disambiguation.
Definition 14 (Disambiguation of Partial Concept Class [AHHM22]). Let 𝒳be an input domain.
A total concept class ℋ⊆{0, 1}𝒳is a special type of a partial concept. Given some partial
concept class ℋ⊆{0, 1, ⋆}𝒳we say that 𝐻is a disambiguation of ℋif for any finite sequence
𝑆∈(𝒳× {0, 1})* if 𝑆is realizable with respect to ℋ, then 𝑆is realizable with respect to ℋ.
Intuitively, by disambiguating a partial concept class we convert it to a total concept class without
reducing its “expressivity”.
Let us first describe an approach to prove the upper bound, i.e., that if the scaled-graph dimension
is finite for all scales then the class is ERM learnable, that does not work. We could perform the
following transformation, inspired by the multiclass setting [DSS14]: for any ℎ∈ℋ⊆[0, 1]𝒳, let us
consider the function ̃︀ℎ: 𝒳× [0, 1] →{0, 1} with ̃︀ℎ(𝑥, 𝑦) = 1 if and only if ℎ(𝑥) = 𝑦, ̃︀ℎ(𝑥, 𝑦) = 0
if and only if ℓ(ℎ(𝑥), 𝑦) > 𝜀and ̃︀ℎ(𝑥, 𝑦) = ⋆otherwise. This induces a new binary partial hypothesis
class ̃︀
ℋ= {̃︀ℎ: ℎ∈ℋ} ⊆{0, 1, ⋆}𝒳. We note that DG
𝜀(ℋ) = VC( ̃︀
ℋ). However, we cannot use
ERM for the partial concept class since in general this approach fails. In particular, a sufficient
condition for applying ERM is that VC({supp(̃︀ℎ) : ̃︀ℎ∈̃︀
ℋ)} < ∞.
Remark 1. Predicting ⋆in [AHHM22] implies a mistake for the setting of partial concept classes.
However, in our regression setting, ⋆is interpreted differently and corresponds to loss at most 𝛾
which is desirable. In particular, the hard instance for proper learners in the partial concepts paper
(see Proposition 4 in [AHHM22]) is good in settings where predicting ⋆does not count as a mistake,
as in our regression case.
B.2
Main Result
We are now ready to state the main result of this section. We will prove the next statement.
Theorem 1. Let ℓbe the absolute loss function. For every class ℋ⊆[0, 1]𝒳and for any 𝜀, 𝛿, 𝛾∈
(0, 1)3, the sample complexity bound for realizable PAC regression by any ERM satisfies
Ω
(︃
DG
𝛾(ℋ) + log(1/𝛿)
𝜀
)︃
≤ℳERM(ℋ; 𝜀, 𝛿, 𝛾) ≤𝑂
(︃
DG
𝛾(ℋ) log(1/𝜀) + log(1/𝛿)
𝜀
)︃
.
In particular, any ERM algorithm A achieves
E
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥)] ≤
inf
𝛾∈[0,1] 𝛾+ ̃︀Θ
(︃
DG
𝛾(ℋ) + log(1/𝛿)
𝑛
)︃
,
with probability at least 1 −𝛿over 𝑆of size 𝑛.
Proof. We prove the upper bound and the lower bound of the statement separately.
Upper Bound for the ERM learner.
We deal with the cut-off loss problem with parameters
(𝜀, 𝛿, 𝛾) ∈(0, 1)3. Our proof is based on a technique that uses a “ghost” sample to establish
generalization guarantees of the algorithm. Let us denote
er𝒟,𝛾(ℎ) ≜
Pr
𝑥∼𝒟𝒳[ℓ(ℎ(𝑥), ℎ⋆(𝑥)) > 𝛾] ,
(1)
17

and for a dataset 𝑧∈(𝒳× [0, 1])𝑛,
̂︀er𝑧,𝛾(ℎ) ≜1
|𝑧|
∑︁
(𝑥,𝑦)∈𝑧
I{ℓ(ℎ(𝑥), 𝑦) > 𝛾} .
(2)
We will start by showing the next symmetrization lemma in our setting. Essentially, it bounds the
probability that there exists a bad ERM learner by the probability that there exists an ERM learner on
a sample 𝑟whose performance on a hidden sample 𝑠is bad. For a similar result, see Lemma 4.4 in
[AB99].
Lemma 2 (Symmetrization). Let 𝜀, 𝛾∈(0, 1)2, 𝑛> 0. Fix 𝑍= 𝒳× [0, 1]. Let
𝑄𝜀,𝛾= {𝑧∈𝑍𝑛: ∃ℎ∈ℋ: ̂︀er𝑧,𝛾(ℎ) = 0, er𝒟,𝛾(ℎ) > 𝜀}
(3)
and
𝑅𝜀,𝛾= {(𝑟, 𝑠) ∈𝑍𝑛× 𝑍𝑛: ∃ℎ∈ℋ: er𝒟,𝛾(ℎ) > 𝜀, ̂︀er𝑟,𝛾(ℎ) = 0, ̂︀er𝑠,𝛾(ℎ) ≥𝜀/2} .
(4)
Then, for 𝑛≥𝑐/𝜀, where 𝑐is some absolute constant, we have that
𝒟𝑛(𝑄𝜀,𝛾) ≤2𝒟2𝑛(𝑅𝜀,𝛾) .
Proof. We will show that 𝒟2𝑛(𝑅𝜀,𝛾) ≥𝒟𝑛(𝑄𝜀,𝛾)
2
. By the definition of 𝑅𝜀,𝛾we can write
𝒟2𝑛(𝑅𝜀,𝛾) =
∫︁
𝑄𝜀,𝛾
𝒟𝑛(𝑠: ∃ℎ∈ℋ, er𝒟,𝛾(ℎ) > 𝜀, ̂︀er𝑟,𝛾(ℎ) = 0, ̂︀er𝑠,𝛾(ℎ) ≥𝜀/2)𝑑𝒟𝑛(𝑟) .
For 𝑟∈𝑄𝜀,𝛾, fix ℎ𝑟∈ℋthat satisfies ̂︀er𝑟,𝛾(ℎ𝑟) = 0, er𝒟,𝛾(ℎ) > 𝜀. It suffices to show that for ℎ𝑟
𝒟𝑛(𝑠: ̂︀er𝑠,𝛾(ℎ𝑟) ≥𝜀/2) ≥1/2 .
Then, the proof of the lemma follows immediately. Since er𝒟,𝛾(ℎ𝑟) > 𝜀, we know that 𝑛· ̂︀er𝑠,𝛾(ℎ)
follows a binomial distribution with probability of success on every try at least 𝜀and 𝑛number of
tries. Thus, the multiplicative version of Chernoff’s bound gives us
𝒟𝑛(𝑠: ̂︀er𝑠,𝛾(ℎ𝑟) < 𝜀/2) ≤𝑒−𝑛·𝜀
8 .
Thus, if 𝑛= 𝑐/𝜀, for some appropriate absolute constant 𝑐we see that
𝒟𝑛(𝑠: ̂︀er𝑠,𝛾(ℎ𝑟) < 𝜀/2) < 1/2 ,
which concludes the proof.
Next, we can use a random swap argument to upper bound 𝒟2𝑛(𝑅𝜀,𝛾) with a quantity that involves a
set of permutations over the sample of length 2𝑛. The main idea behind the proof is to try to leverage
the fact that each of the labeled examples is as likely to occur among the first 𝑛examples or the last
𝑛examples.
Following [AB99], we denote by Γ𝑛the set of all permutations on {1, . . . , 2𝑛} that swap 𝑖and
𝑛+ 𝑖, for all 𝑖that belongs to {1, . . . , 𝑛} . In other words, for all 𝜎∈Γ𝑛and 𝑖∈{1, . . . , 𝑛} either
𝜎(𝑖) = 𝑖, 𝜎(𝑛+ 𝑖) = 𝑛+ 𝑖or 𝜎(𝑖) = 𝑛+ 𝑖, 𝜎(𝑛+ 𝑖) = 𝑖. Thus, we can think of 𝜎as acting
on coordinates where it (potentially) swaps one element from the first half of the sample with the
corresponding element on the second half of the sample. For some 𝑧∈𝑍2𝑛we overload the notation
and denote 𝜎(𝑧) the effect of applying 𝜎to the sample 𝑧.
We are now ready to state the bound. Importantly, it shows that by (uniformly) randomly choosing
a permutation 𝜎∈Γ𝑛we can bound the probability that a sample falls into the bad set 𝑅𝜀,𝛾by a
quantity that does not depend on the distribution 𝒟.
Lemma 3 (Random Swaps; Adaptation of Lemma 4.5 in [AB99]). Fix 𝑍= 𝒳× [0, 1]. Let 𝑅𝜀,𝛾be
any subset of 𝑍2𝑛and 𝒟any probability distribution on 𝑍. Then
𝒟2𝑛(𝑅𝜀,𝛾) =
E
𝑧∼𝒟2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ≤max
𝑧∈𝑍2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ,
where U(Γ𝑛) is the uniform distribution over the set of swapping permutations Γ𝑛.
18

Proof. First, notice that the bound
E
𝑧∼𝒟2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ≤max
𝑧∈𝑍2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ,
follows trivially and the maximum exists since Pr𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] takes finitely many values
for any finite 𝑛and all 𝑧∈𝑍2𝑛. Thus, the bulk of the proof is to show that
𝒟2𝑛(𝑅𝜀,𝛾) =
E
𝑧∼𝒟2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] .
First, notice that since example is drawn i.i.d., for any swapping permutation 𝜎∈Γ𝑛we have that
𝒟2𝑛(𝑅𝜀,𝛾) = 𝒟2𝑛(︀{︀
𝑧∈𝑍2𝑛: 𝜎(𝑧) ∈𝑅𝜀,𝛾
}︀)︀
.
(5)
Thus, the following holds
𝒟2𝑛(𝑅𝜀,𝛾) =
∫︁
𝑍2𝑛I{𝑧∈𝑅𝜀,𝛾} 𝑑𝒟2𝑛(𝑧)
=
1
|Γ𝑛|
∑︁
𝜎∈Γ𝑛
∫︁
𝑍2𝑛I{𝜎(𝑧) ∈𝑅𝜀,𝛾} 𝑑𝒟2𝑛(𝑧)
=
∫︁
𝑍2𝑛
(︃
1
|Γ𝑛|
∑︁
𝜎∈Γ𝑛
I{𝜎(𝑧) ∈𝑅𝜀,𝛾}
)︃
𝑑𝒟2𝑛(𝑧)
=
E
𝑧∼𝒟2𝑛
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ,
where the first equation follows by definition, the second by Equation (5), the third because the
number of terms in the summation is finite, and the last one by definition.
As a last step we can bound the above RHS by using all possible patterns when ℋis (roughly
speaking) projected in the sample 𝑟𝑠∈𝑍2𝑛.
Lemma 4 (Bounding the Bad Event). Fix 𝑍= 𝒳× [0, 1]. Let 𝑅𝜀,𝛾⊆𝑍2𝑛be the set
𝑅𝜀,𝛾= {(𝑟, 𝑠) ∈𝑍𝑛× 𝑍𝑛: ∃ℎ∈ℋ: ̂︀er𝑟,𝛾(ℎ) = 0, ̂︀er𝑠,𝛾(ℎ) ≥𝜀/2} .
Then
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ≤(2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛))2−𝑛𝜀/2 .
Proof. Throughout the proof we fix 𝑧= (𝑧1, ..., 𝑧2𝑛) ∈𝑍2𝑛, where 𝑧𝑖= (𝑥𝑖, 𝑦𝑖) = (𝑥𝑖, ℎ⋆(𝑥𝑖))
and let 𝑆= {𝑥1, ..., 𝑥2𝑛}. Consider the projection set ℋ|𝑆. We define a partial binary concept class
ℋ′ ⊆{0, 1, ⋆}2𝑛as follows:
ℋ′ :=
⎧
⎨
⎩ℎ′ ∈{0, 1, ⋆}2𝑛: ∃ℎ∈ℋ|𝑆: ∀𝑖∈[2𝑛]
⎧
⎨
⎩
ℎ(𝑥𝑖) = 𝑦𝑖,
ℎ′(𝑖) = 0
ℓ(ℎ(𝑥𝑖), 𝑦𝑖) > 𝛾,
ℎ′(𝑖) = 1
0 < ℓ(ℎ(𝑥𝑖), 𝑦𝑖) ≤𝛾,
ℎ′(𝑖) = ⋆
⎫
⎬
⎭
⎫
⎬
⎭.
Importantly, we note that, by definition, VC(ℋ′) ≤DG
𝛾(ℋ).
Currently, we have a partial binary concept class ℋ′. As a next step, we would like to replace the
⋆symbols and essentially reduce the problem to a total concept class. This procedure is called
disambiguation (cf. Definition 14). The next key lemma shows that there exists a compact (in terms
of cardinality) disambiguation of a VC partial concept class for finite instance domains.
Lemma 5 (Compact Disambiguations, see [AHHM22]). Let ℋbe a partial concept class on a
finite instance domain 𝒳with VC(ℋ) = 𝑑. Then there exists a disambiguation ℋof ℋwith size
|ℋ| = |𝒳|𝑂(𝑑log |𝒳|).
This means that there exists a disambiguation ℋ′ of ℋ′ of size at most
(2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛)) .
19

Since this (total) binary concept class is finite, we can apply the following union bound argument.
We have that 𝜎(𝑧) ∈𝑅if and only if some ℎ∈ℋsatisfies
∑︀𝑛
𝑖=1 I{ℓ(ℎ(𝑥𝜎(𝑖)), 𝑦𝜎(𝑖)) > 𝛾}
𝑛
= 0,
∑︀𝑛
𝑖=1 I{ℓ(ℎ(𝑥𝜎(𝑛+𝑖)), 𝑦𝜎(𝑛+𝑖)) > 𝛾}
𝑛
≥𝜀/2 .
We can relate this event with an event about the disambiguated partial concept class ℋ′ since the
number of 1’s can only increase. In particular, for any swapping permutation 𝜎of the 2𝑛points, if
there exists a function in ℋthat is correct on the first 𝑛points and is off by at least 𝛾on at least 𝜀𝑛/2
of the remaining 𝑛points, then there is a function in the disambiguation ℋ′ that is 0 on the first 𝑛
points and is 1 on those same 𝜖𝑛/2 of the remaining points.
If we fix some 𝜎∈Γ𝑛, and some ℎ′ ∈ℋ′ then ℎ′ is a witness that 𝜎(𝑧) ∈𝑅𝜀,𝛾only if ∀𝑖∈[𝑛] we do
not have that ℎ′(𝑖) = 1, ℎ′(𝑖+ 𝑛) = 1. Thus at least one of ℎ′(𝑖), ℎ′(𝑛+ 𝑖) must be zero. Moreover,
at least 𝑛𝜀/2 entries must be non-zero. Thus, when we draw random swapping permutation the
probability that all the non-zero entries land on the second half of the sample sample is at most
2−𝑛𝜀/2.
Crucially since the number of possible functions is at most |ℋ′| ≤(2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛)) a union
bound gives us that
Pr
𝜎∼U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ≤(2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛)) · 2−𝑛𝜀/2 .
Thus, since 𝑧∈𝑍2𝑛was arbitrary we have that
max
𝑧∈𝑍2𝑛
Pr
𝜎U(Γ𝑛)[𝜎(𝑧) ∈𝑅𝜀,𝛾] ≤(2𝑛)𝑂(DG
𝛾(ℋ) log(2𝑛)) · 2−𝑛𝜀/2 .
This concludes the proof.
Lower Bound for the ERM learner.
Our next goal is to show that
ℳERM(ℋ; 𝜀, 𝛿, 𝛾) ≥𝐶0 · DG
𝛾(ℋ) + log(1/𝛿)
𝜀
.
To this end, we will show that there exists an ERM learner satisfying this lower bound. This will
establish that the finiteness of 𝛾-graph dimension for any 𝛾∈(0, 1), is necessary for PAC learnability
using a worst-case ERM. It suffices to show that there exists a bad ERM algorithm that requires
at least 𝐶0
DG
𝛾(ℋ)+log(1/𝛿)
𝜀
samples to cut-off PAC learn ℋ. First let us consider the case where
𝑑= DG
𝛾(ℋ) < ∞and let 𝑆= {𝑥1, ...., 𝑥𝑑} be a 𝛾-graph-shattered set by ℋwith witness 𝑓0.
Consider the ERM learner 𝒜that works as follows: Upon seeing a sample 𝑇⊆𝑆consistent with 𝑓0,
𝒜returns a function 𝒜(𝑇) that is equal to 𝑓0 on elements of 𝑇and 𝛾-far from 𝑓0 on 𝑆∖𝑇. Such a
function exists since 𝑆is 𝛾-graph-shattered with witness 𝑓0. Let us take 𝛿< 1/100 and 𝜀< 1/12.
Define a distribution over 𝑆⊆𝒳such that
Pr[𝑥1] = 1 −2𝜀,
Pr[𝑥𝑖] = 2𝜀/(𝑑−1), ∀𝑖∈{2, ..., 𝑑} .
Let us set ℎ⋆= 𝑓0 and consider 𝑚samples {(𝑧𝑖, 𝑓0(𝑧𝑖)}𝑖∈[𝑚]. Since we work in the scaled PAC
model, 𝒜will make a 𝛾-error on all examples from 𝑆which are not in the sample (since in that
case the output will be 𝛾-far from the true label). Let us take 𝑚≤𝑑−1
6𝜀. Then, the sample will
include at most (𝑑−1)/2 examples which are not 𝑥1 with probability 1/100, using Chernoff’s bound.
Conditioned on that event, this implies that the ERM learner will make a 𝛾-error with probability at
least
2𝜀
𝑑−1·(𝑑−1−𝑑−1
2 ) = 𝜀, over the random draw of the test point. Thus, ℳ𝒜(ℋ; 𝜀, 𝛿, 𝛾) = Ω( 𝑑−1
𝜀).
Moreover, the probability that the sample will only contain 𝑥1 is (1−2𝜀)𝑚≥𝑒−4𝜀𝑚which is greater
that 𝛿whenever 𝑚≤log(1/𝛿)/(4𝜀). This implies that the 𝛾-cut-off ERM sample complexity is
lower bounded by
max
{︂𝑑−1
6𝜀, log(1/𝛿)
2𝜀
}︂
= 𝐶0 · DG
𝛾(ℋ) + log(1/𝛿)
𝜀
.
Thus ℳERM(ℋ; 𝜀, 𝛿, 𝛾), satisfies the desired bound when the dimension is finite. Finally, it remains
to claim about the case where DG
𝛾(ℋ) = ∞for the given 𝛾. We consider a sequence of 𝛾-graph-
shattered sets 𝑆𝑛with |𝑆𝑛| = 𝑛and repeat the claim for the finite case. This will yield that for any 𝑛
the cut-off ERM sample complexity is lower bounded by Ω((𝑛+ log(1/𝛿))/𝜀) and this yields that
ℳERM(ℋ; 𝜀, 𝛿, 𝛾) = ∞.
20

However, as Example 4 shows, the optimal learner cannot be proper and as a result, this dimension
does not characterize PAC learnability for real-valued regression (there exist classes whose 𝛾-graph
dimension is infinite but are PAC learnable in the realizable regression setting).
C
𝛾-OIG Dimension and Learnability
In this section we identify a dimension characterizing qualitatively and quantitatively what classes of
predictors ℋ⊆[0, 1]𝒳are PAC learnable and we provide PAC learners that achieve (almost) optimal
sample complexity. In particular, we show the following result.
Theorem 2. Let ℓbe the absolute loss function. For every class ℋ⊆[0, 1]𝒳and for any 𝜀, 𝛿, 𝛾∈
(0, 1)3, the sample complexity bound for realizable PAC regression satisfies
Ω
(︃
DOIG
2𝛾(ℋ)
𝜀
)︃
≤ℳ(ℋ; 𝜀, 𝛿, 𝛾) ≤𝑂
(︃
DOIG
𝛾
(ℋ)
𝜀
log2 DOIG
𝛾
(ℋ)
𝜀
+ 1
𝜀log 1
𝛿
)︃
.
In particular, there exists an algorithm 𝐴such that
E
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥)] ≤
inf
𝛾∈[0,1] 𝛾+ ̃︀Θ
(︃
DOIG
𝛾
(ℋ) + log(1/𝛿)
𝑛
)︃
,
with probability at least 1 −𝛿over 𝑆∼𝒟𝑛.
A finite fat-shattering dimension implies a finite OIG dimension.
Let ℱ⊆[0, 1]𝒳be a function
class with finite 𝛾-fat shattering dimension for any 𝛾> 0. We show that DOIG
𝛾
(ℱ) is upper bounded
(up to constants and log factors) by Dfat
𝑐𝛾(ℱ), for some 𝑐> 0, where the OIG dimension is defined
with respect to the ℓ1 loss. Note that the opposite direction does not hold. Example 1 exhibits a
function class with an infinite fat-shattering dimension that can be learned with a single example,
and as a result, the OIG dimension has to be finite. On the one hand, we have an upper bound on the
sample complexity of 𝑂
(︀1
𝜖
(︀
Dfat
˜𝑐𝜀(ℱ) log2 1
𝜖+ log 1
𝛿
)︀)︀
, for any 𝜀, 𝛿∈(0, 1). See sections 19.6 and
20.4 about the restricted model in [AB99]. On the other hand, we prove in Lemma 6 a lower bound
on the sample complexity of Ω
(︁
DOIG
2𝜀
(ℱ)
𝜖
)︁
, for any 𝜀∈(0, 1), and so DOIG
𝛾
(ℱ) is upper bounded by
Dfat
𝑐𝛾(ℱ) up to constants and log factors.
C.1
Proof of the Lower Bound
Lemma 6. [Lower Bound of PAC Regression] Let 𝐴be any learning algorithm and 𝜀, 𝛿, 𝛾∈(0, 1)3
such that 𝛿< 𝜀. Then,
ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾) ≥Ω
(︃
DOIG
2𝛾(ℋ)
𝜀
)︃
.
Proof. Let 𝑛0 = DOIG
2𝛾(ℋ). Let 𝑛∈N, 1 < 𝑛≤𝑛0. We know that for each such 𝑛there exists
some 𝑆∈𝒳𝑛such that the one-inclusion graph of ℋ|𝑆has the property that: there exists a finite
subgraph 𝐺= (𝑉, 𝐸) of 𝐺OIG
ℋ|𝑆such that for any orientation 𝜎: 𝐸→𝑉of the subgraph, there exists
a vertex 𝑣∈𝑉with outdeg(𝑣; 𝜎, 2𝛾) > DOIG
2𝛾(ℋ)/3.
Given the learning algorithm 𝐴: (𝒳×[0, 1])⋆×𝒳→[0, 1], we can describe an orientation 𝜎𝐴of the
edges in 𝐸. For any vertex 𝑣= (𝑣1, . . . , 𝑣𝑛) ∈𝑉let 𝑃𝑣be the distribution over (𝑥1, 𝑣1), ...., (𝑥𝑛, 𝑣𝑛)
defined as
𝑃𝑣((𝑥1, 𝑣1)) = 1 −𝜀, 𝑃𝑣((𝑥𝑡, 𝑣𝑡)) =
𝜀
𝑛−1, 𝑡∈{2, . . . , 𝑛} .
Let 𝑚= 𝑛/(2𝜀). For each vertex 𝑣∈𝑉and direction 𝑡∈[𝑛], consider the hyperedge 𝑒𝑡,𝑣. For each
𝑢∈𝑒𝑡,𝑣we define
𝑝𝑡(𝑢) =
Pr
𝑆∼𝑃𝑚
𝑢
[ℓ(𝐴(𝑆; 𝑥𝑡), 𝑢𝑡) > 𝛾|(𝑥𝑡, 𝑢𝑡) /∈𝑆] ,
and let 𝐶𝑒𝑡,𝑣= {𝑢∈𝑒𝑡,𝑣: 𝑝𝑡(𝑢) < 1/2}. If 𝐶𝑒𝑡,𝑣= ∅, we orient the edge 𝑒𝑡,𝑣arbitrarily. Since for
all 𝑢, 𝑣∈𝑒𝑡,𝑣the distributions 𝑃𝑚
𝑢, 𝑃𝑚
𝑣conditioned on the event that (𝑥𝑡, 𝑢𝑡), (𝑥𝑡, 𝑣𝑡) respectively
21

are not in 𝑆are the same, we can see that ∀𝑢, 𝑣∈𝐶𝑒𝑡,𝑣it holds that ℓ(𝑢𝑡, 𝑣𝑡) ≤2𝛾. We orient the
edge 𝑒𝑡,𝑣using an arbitrary element of 𝐶𝑒𝑡,𝑣.
Because of the previous discussion, we can bound from above the out-degree of all vertices 𝑣∈𝑉
with respect to the orientation 𝜎𝐴as follows:
outdeg(𝑣; 𝜎𝐴, 2𝛾) ≤
∑︁
𝑡
I{𝑝𝑡(𝑣) ≥1/2} ≤1 + 2
𝑛
∑︁
𝑡=2
Pr
𝑆∼𝑃𝑚
𝑣
[ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾|(𝑥𝑡, 𝑦𝑡) /∈𝑆] .
Notice that
Pr
𝑆∼𝑃𝑚
𝑣
[ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾|(𝑥𝑡, 𝑦𝑡) /∈𝑆] = Pr𝑆∼𝑃𝑚
𝑣[{ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾} ∧{(𝑥𝑡, 𝑦𝑡) /∈𝑆}]
Pr𝑆∼𝑃𝑚
𝑣[(𝑥𝑡, 𝑦𝑡) /∈𝑆]
,
and by the definition of 𝑃𝑣, we have that
Pr
𝑆∼𝑃𝑚
𝑣
[(𝑥𝑡, 𝑦𝑡) /∈𝑆] =
(︂
1 −
𝜀
𝑛−1
)︂𝑚
≥1 −
𝑛
2(𝑛−1) ,
since 𝑚= 𝑛/(2𝜀). Combining the above, we get that
outdeg(𝑣; 𝜎𝐴, 2𝛾) ≤1 + 2
(︂
1 −
𝑛
2(𝑛−1)
)︂
𝑛
∑︁
𝑡=2
E
𝑆∼𝑃𝑚
𝑣
[I{ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾} · I{(𝑥𝑡, 𝑦𝑡) /∈𝑆}] ,
and so
outdeg(𝑣; 𝜎𝐴, 2𝛾) ≤1 + 2
(︂
1 −
𝑛
2(𝑛−1)
)︂
E
𝑆∼𝑃𝑚
𝑣
[︃𝑛
∑︁
𝑡=2
I{ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾}
]︃
= 1 + 2
(︂
1 −
𝑛
2(𝑛−1)
)︂𝑛−1
𝜀
E
𝑆∼𝑃𝑚
𝑣
[︃
𝜀
𝑛−1
𝑛
∑︁
𝑡=2
I{ℓ(𝐴(𝑆, 𝑥𝑡), 𝑦𝑡) > 𝛾}
]︃
≤1 + 2
(︂
1 −
𝑛
2(𝑛−1)
)︂𝑛−1
𝜀
E
𝑆∼𝑃𝑚
𝑣
[︂
E
(𝑥,𝑦)∼𝑃𝑣[I{ℓ(𝐴(𝑆; 𝑥), 𝑦) > 𝛾}]
]︂
= 1 + 2
(︂
1 −
𝑛
2(𝑛−1)
)︂𝑛−1
𝜀
E
𝑆∼𝑃𝑚
𝑣
[︂
Pr
(𝑥,𝑦)∼𝑃𝑣[ℓ(𝐴(𝑆; 𝑥), 𝑦) > 𝛾]
]︂
≤1 + 𝑛−2
𝜀
E
𝑆∼𝑃𝑚
𝑣
[︂
Pr
(𝑥,𝑦)∼𝑃𝑣[ℓ(𝐴(𝑆; 𝑥), 𝑦) > 𝛾]
]︂
By picking “hard” distribution 𝒟= 𝑃𝑣⋆, where 𝑣⋆∈arg max𝑣′∈𝑉outdeg(𝑣′; 𝜎𝐴, 2𝛾) we get that
that
E
𝑆∼𝑃𝑚
𝑣⋆
[︂
Pr
(𝑥,𝑦)∼𝑃𝑣⋆[ℓ(𝐴(𝑆; 𝑥), 𝑦) > 𝛾]
]︂
≥(outdeg(𝑣⋆; 𝜎𝐴, 2𝛾) −1) ·
𝜀
𝑛−2
≥𝜀
6 ,
since outdeg(𝑣⋆; 𝜎𝐴, 2𝛾) > 𝑛/3. By picking 𝑛= 𝑛0 we see that when the learner uses 𝑚= 𝑛0/𝜀
samples then its expected error is at least 𝜀/6. Notice that when the learner uses 𝑚′ = ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾)
samples we have that
E
𝑆∼𝑃𝑚′
𝑣⋆
[︂
Pr
(𝑥,𝑦)∼𝑃𝑣⋆[ℓ(𝐴(𝑆; 𝑥), 𝑦) > 𝛾]
]︂
≤𝛿+ (1 −𝛿)𝜀≤𝛿+ 𝜀≤2𝜀.
Thus, we see that for any algorithm 𝐴
ℳ𝐴(ℋ; 𝜀, 𝛿, 𝛾) ≥Ω
(︃
DOIG
2𝛾(ℋ)
𝜀
)︃
,
hence
ℳ(ℋ; 𝜀, 𝛿, 𝛾) ≥Ω
(︃
DOIG
2𝛾(ℋ)
𝜀
)︃
.
22

C.2
Proof of the Upper Bound
Let us present the upper bound. For this proof, we need three tools: we will provide a weak learner
based on the scaled one-inclusion graph, a boosting algorithm for real-valued functions, and consistent
sample compression schemes for real-valued functions.
To this end, we introduce the one-inclusion graph (OIG) algorithm AOIG
𝛾
for realizable regression at
scale 𝛾.
C.2.1
Scaled One-Inclusion Graph Algorithm and Weak Learning
First, we show that every scaled orientation 𝜎of the one-inclusion graph gives rise to a learner A𝜎
whose expected absolute loss is upper bounded by the maximum out-degree induced by 𝜎.
Lemma 7 (From Orientations to Learners). Let 𝒟𝒳be a distribution over 𝒳and ℎ⋆∈ℋ⊆[0, 1]𝒳,
let 𝑛∈N and 𝛾∈(0, 1). Then, for any orientation 𝜎: 𝐸𝑛→𝑉𝑛of the scaled-one-inclusion graph
𝐺OIG
ℋ
= (𝑉𝑛, 𝐸𝑛), there exists a learner A𝜎: (𝒳× [0, 1])𝑛−1 →[0, 1]𝒳, such that
E
𝑆∼𝒟𝑛−1
𝒳
[︂
Pr
𝑥∼𝒟𝒳[ℓ(A𝜎(𝑥), ℎ⋆(𝑥)) > 𝛾]
]︂
≤max𝑣∈𝑉𝑛outdeg(𝑣; 𝜎, 𝛾)
𝑛
,
where A𝜎is trained using a sample 𝑆of size 𝑛−1 realized by ℎ⋆.
Algorithm 1 From orientation 𝜎to learner A𝜎
Input: An ℋ-realizable sample {(𝑥𝑖, 𝑦𝑖)}𝑛−1
𝑖=1 and a test point 𝑥∈𝒳, 𝛾∈(0, 1).
Output: A prediction A𝜎(𝑥).
1. Create the one-inclusion graph 𝐺OIG
ℋ|(𝑥1,...,𝑥𝑛−1,𝑥).
2. Consider the edge in direction 𝑛defined by the realizable sample {(𝑥𝑖, 𝑦𝑖)}𝑛−1
𝑖=1 ; let
𝑒= {ℎ∈ℋ|(𝑥1,...,𝑥𝑛−1,𝑥) : ∀𝑖∈[𝑛−1] ℎ(𝑖) = 𝑦𝑖} .
3. Return A𝜎(𝑥) = 𝜎(𝑒)(𝑛).
Proof. By the classical leave-one-out argument, we have that
E
𝑆∼𝒟𝑛−1
𝒳
[︂
Pr
𝑥∼𝒟𝒳[ℓ(A𝜎(𝑥), ℎ⋆(𝑥)) > 𝛾]
]︂
=
E
(𝑆,(𝑥,𝑦))∼𝒟𝑛[I{ℓ(ℎ𝑆(𝑥), 𝑦) > 𝛾}] =
E
𝑆′∼𝒟𝑛,𝐼∼U([𝑛])[I{ℓ(ℎ𝑆′
−𝐼(𝑥′
𝐼), 𝑦′
𝐼) > 𝛾}] ,
where ℎ𝑆is the predictor A𝜎using the examples 𝑆, and U([𝑛]) is the uniform distribution on
{1, . . . , 𝑛}. Now for every fixed 𝑆′ we have that
E
𝐼∼U([𝑛)][I{ℓ(ℎ𝑆′
−𝐼(𝑥′
𝐼), 𝑦′
𝐼) > 𝛾}] = 1
𝑛
∑︁
𝑖∈[𝑛]
I{ℓ(𝜎(𝑒𝑖)(𝑖), 𝑦′
𝑖) > 𝛾} = outdeg(𝑦′; 𝜎, 𝛾)
𝑛
,
where 𝑦′ is the node of the scaled OIG that corresponds to the true labeling of 𝑆′. By taking
expectation over 𝑆′ ∼𝒟𝑛we get that
E
𝑆′∼𝒟𝑛,𝐼∼U([𝑛])[I{ℓ(ℎ𝑆′
−𝐼(𝑥′
𝐼), 𝑦′
𝐼) > 𝛾}] ≤
E
𝑆′∼𝒟𝑛
[︂outdeg(𝑦′; 𝜎, 𝛾)
𝑛
]︂
≤max𝑣∈𝑉𝑛outdeg(𝑣; 𝜎, 𝛾)
𝑛
.
Equipped with the previous result, we are now ready to show that when the learner gets at least
DOIG
𝛾
(ℋ) samples as its training set, then its expected 𝛾-cutoff loss is bounded away from 1/2.
Lemma 8 (Scaled OIG Guarantee (Weak Learner)). Let 𝒟𝒳be a distribution over 𝒳and ℎ⋆∈ℋ⊆
[0, 1]𝒳, and 𝛾∈(0, 1). Then, for all 𝑛> DOIG
𝛾
(ℋ) there exists an orientation 𝜎⋆such that for the
prediction error of the one-inclusion graph algorithm AOIG
𝜎⋆
: (𝒳× [0, 1])𝑛−1 × 𝒳→[0, 1] , it holds
that
E
𝑆∼𝒟𝑛−1
𝒳
[︂
Pr
𝑥∼𝒟𝒳
[︀
ℓ(AOIG
𝜎⋆(𝑥), ℎ⋆(𝑥)) > 𝛾
]︀]︂
≤1/3 .
23

Proof. Fix 𝛾∈(0, 1). Assume that 𝑛> DOIG
𝛾
(ℋ) and let 𝐺OIG
ℋ|(𝑆,𝑥) = (𝑉𝑛, 𝐸𝑛) be the possibly
infinite scaled one-inclusion graph. By the definition of the 𝛾-OIG dimension (see Definition 9), for
every finite subgraph 𝐺= (𝑉, 𝐸) of 𝐺OIG
ℋ|(𝑆,𝑥) there exists an orientation 𝜎: 𝐸→𝑉such that for
every vertex in 𝐺the out-degree is at most 𝑛/3, i.e.,
∀𝑆∈𝒳𝑛, ∀finite 𝐺= (𝑉, 𝐸) of 𝐺OIG
ℋ|𝑆, ∃orientation 𝜎𝐸s.t. ∀𝑣∈𝑉, it holds outdeg(𝑣; 𝜎𝐸, 𝛾) ≤𝑛/3 .
First, we need to create an orientation of the whole (potentially infinite) one-inclusion graph.
We will create this orientation using the compactness theorem of first-order logic which states that a
set of formulas Φ is satisfiable if and only if it is finitely satisfiable, i.e., every finite subset Φ′ ⊆Φ
is satisfiable. Let 𝐺OIG
ℋ|𝑆= (𝑉𝑛, 𝐸𝑛) be the (potentially infinite) one-inclusion graph of ℋ|𝑆. Let 𝒵
be the set of pairs 𝑧= (𝑣, 𝑒) ∈𝑉𝑛× 𝐸𝑛so that 𝑣∈𝑒. Our goal is to assign binary values to each
𝑧∈𝒵. We define the following sets of formulas:
• For each 𝑒∈𝐸𝑛we let Φ𝑒:= ∃exactly one 𝑣∈𝑒: 𝑧(𝑣, 𝑒) = 1.
• For each 𝑣∈𝑉𝑛we let Φ𝑣:= ∃at most 𝑛/3 different 𝑒𝑖,𝑓
∈𝐸𝑛: 𝑣∈𝑒𝑖,𝑓∧
(∃𝑣′ ∈𝑒𝑖,𝑓: (𝑧(𝑣′, 𝑒) = 1 ∧ℓ(𝑣′
𝑖, 𝑣𝑖) > 𝛾))
It is not hard to see that each Φ𝑒, Φ𝑣can be expressed in first-order logic. Then, we define
Φ :=
(︃⋂︁
𝑒∈𝐸𝑛
Φ𝑒
)︃
∩
(︃⋂︁
𝑣∈𝑉𝑛
Φ𝑣
)︃
.
Notice that an orientation of the edges of 𝐺OIG
ℋ|𝑆is equivalent to picking an assignment of the elements
of 𝒵that satisfies all the Φ𝑒. Moreover, notice that for such an assignment, if all the Φ𝑣are satisfied
then then maximum 𝛾-scaled out-degree of 𝐺OIG
ℋ|𝑆is at most 𝑛/3.
We will now show that Φ is finitely satisfiable. Let Φ′ be a finite subset of Φ and let 𝐸′ ⊆𝐸𝑛, 𝑉′ ⊆
𝑉𝑛, be the set of edges, vertices that appear in Φ′, respectively. If 𝑉′ = ∅, then we can orient the
edges in 𝐸′ arbitrarily and satisfy Φ′. Similarly, if 𝐸′ = ∅we can let all the 𝑧(𝑒, 𝑣) = 0 and satisfy
all the Φ𝑣, 𝑣∈𝑉′. Thus, assume that both sets are non-empty. Consider the finite subgraph of 𝐺OIG
ℋ|𝑆
that is induced by 𝑉′ and let 𝐸′′ be the set of edges of this subgraph. For every edge 𝑒∈𝐸′ ∖𝐸′′4,
pick an arbitrary orientation, i.e, for exactly one 𝑣∈𝑒set 𝑧(𝑒, 𝑣) = 1 and for the remaining 𝑣′ ∈𝑒set
𝑧(𝑒, 𝑣′) = 0. By the definition of DOIG
𝛾
(ℋ) there is an orientation 𝜎𝐸′′ of the edges in 𝐸′′ such that
∀𝑣∈𝑉′outdeg(𝑣; 𝜎𝐸′′, 𝛾) ≤𝑛/3. For every 𝑒∈𝐸′′ pick the assignment of all the 𝑧(𝑣, 𝑒), 𝑣∈𝑒,
according to the orientation 𝜎𝐸′′. Thus, because of the maximum out-degree property of 𝜎𝐸′′ we
described before, we can also see that all the Φ𝑣, 𝑣∈𝑉′, are satisfied. Hence, we have shown that Φ
is finitely satisfiable, so it is satisfiable. This assignment on 𝑧(𝑣, 𝑒) induces an orientation 𝜎⋆under
which all the vertices of the one-inclusion graph have out-degree at most 𝑛/3.
We will next use the orientation 𝜎⋆of 𝐺OIG
ℋ|𝑆= (𝑉𝑛, 𝐸𝑛) to design a learner AOIG
𝜎⋆
: (𝒳×[0, 1])𝑛−1×
𝒳→[0, 1], invoking Lemma 7. In particular, we get that, from Lemma 7 with the chosen orientation,
E
𝑆∼𝒟𝑛−1
𝒳
[︂
Pr
𝑥∼𝒟𝒳
[︀
ℓ(AOIG
𝜎⋆(𝑥), ℎ⋆(𝑥)) > 𝛾
]︀]︂
≤max𝑣∈𝑉𝑛outdeg(𝑣; 𝜎⋆, 𝛾)
𝑛
≤1/3 ,
which concludes the proof.
C.2.2
Boosting Real-Valued Functions
Definition 15 (Weak Real-Valued Learner). Let ℓbe a loss function. Let 𝛾∈[0, 1], 𝛽∈(0, 1
2),
and ℋ⊆[0, 1]𝒳. For a distribution 𝒟𝒳over 𝒳and true target function ℎ⋆∈ℋ, we say that
𝑓: 𝒳→[0, 1] is (𝛾, 𝛽)-weak learner with respect to 𝒟𝒳and ℎ⋆, if
Pr
𝑥∼𝒟𝒳
[︁
ℓ(𝑓(𝑥), ℎ⋆(𝑥)) > 𝛾
]︁
< 1
2 −𝛽.
4Since the edges in 𝐸′′ are of finite length, we first need to map them to the appropriate edges in 𝐸′.
24

Following [HKS19], we define the weighted median as
Median(𝑦1, . . . , 𝑦𝑇; 𝛼1, . . . , 𝛼𝑇) = min
{︃
𝑦𝑗:
∑︀𝑇
𝑡=1 𝛼𝑡I[𝑦𝑗< 𝑦𝑡]
∑︀𝑇
𝑡=1 𝛼𝑡
< 1
2
}︃
,
and the weighted quantiles, for 𝜃∈[0, 1/2], as
𝑄+
𝜃(𝑦1, . . . , 𝑦𝑇; 𝛼1, . . . , 𝛼𝑇) = min
{︃
𝑦𝑗:
∑︀𝑇
𝑡=1 𝛼𝑡I[𝑦𝑗< 𝑦𝑡]
∑︀𝑇
𝑡=1 𝛼𝑡
< 1
2 −𝜃
}︃
𝑄−
𝜃(𝑦1, . . . , 𝑦𝑇; 𝛼1, . . . , 𝛼𝑇) = max
{︃
𝑦𝑗:
∑︀𝑇
𝑡=1 𝛼𝑡I[𝑦𝑗> 𝑦𝑡]
∑︀𝑇
𝑡=1 𝛼𝑡
< 1
2 −𝜃
}︃
,
and
we
let
𝑄+
𝜃(𝑥)
=
𝑄+
𝜃(ℎ1(𝑥), . . . , ℎ𝑇(𝑥); 𝛼1, . . . , 𝛼𝑇), 𝑄−
𝜃(𝑥)
=
𝑄−
𝜃(ℎ1(𝑥), . . . , ℎ𝑇(𝑥); 𝛼1, . . . , 𝛼𝑇),
where ℎ1, . . . , ℎ𝑇, 𝛼1, . . . , 𝛼𝑇
are the values returned
by Algorithm 2. The following guarantee holds for this procedure.
Lemma 9 (MedBoost guarantee [Kég03]). Let ℓbe the absolute loss and 𝑆= {(𝑥𝑖, 𝑦𝑖)}𝑚
𝑖=1,
𝑇= 𝑂
(︀1
𝜃2 log(𝑚)
)︀
. Let ℎ1, . . . , ℎ𝑇and 𝛼1, . . . , 𝛼𝑇be the functions and coefficients returned from
MedBoost. For any 𝑖∈{1, . . . , 𝑚} it holds that
max
{︁
ℓ
(︁
𝑄+
𝜃/2(𝑥𝑖), 𝑦𝑖
)︁
, ℓ
(︁
𝑄−
𝜃/2(𝑥𝑖), 𝑦𝑖
)︁}︁
≤𝛾.
Algorithm 2 MedBoost [Kég03]
Input: 𝑆= {(𝑥𝑖, 𝑦𝑖)}𝑚
𝑖=1.
Parameters: 𝛾, 𝛽, 𝑇.
Initialize 𝒫1 = Uniform(𝑆).
For 𝑡= 1, . . . , 𝑇:
1. Find a (𝛾, 𝛽)-weak learner ℎ𝑡with respect to (𝑥𝑖, 𝑦𝑖) ∼𝒫𝑡, using a subset 𝑆𝑡⊆𝑆.
2. For 𝑖= 1, . . . , 𝑚:
(a) Set 𝑤(𝑡)
𝑖
= 1 −2I
[︀
ℓ(ℎ𝑡(𝑥𝑖), 𝑦𝑖) > 𝛾
]︀
.
(b) Set 𝛼𝑡= 1
2 log
(︂
(1−𝛾) ∑︀𝑛
𝑖=1 𝒫𝑡(𝑥𝑖,𝑦𝑖)I
[︁
𝑤(𝑡)
𝑖
=1
]︁
(1+𝛾) ∑︀𝑛
𝑖=1 𝒫𝑡(𝑥𝑖,𝑦𝑖)I
[︁
𝑤(𝑡)
𝑖
=−1
]︁
)︂
.
(c)
• If 𝛼𝑡= ∞: return 𝑇copies of ℎ𝑡, (𝛼1 = 1, . . . , 𝛼𝑇= 1), and 𝑆𝑡.
• Else:
𝑃𝑡+1(𝑥𝑖, 𝑦𝑖)
=
𝑃𝑡(𝑥𝑖, 𝑦𝑖) exp(−𝛼𝑡𝑤𝑡
𝑖) /𝑍𝑡,
where
𝑍𝑡
=
∑︀𝑛
𝑗=1 𝒫𝑡(𝑥𝑗, 𝑦𝑗) exp
(︀
−𝛼𝑡𝑤𝑡
𝑗
)︀
.
Output: Functions ℎ1, . . . , ℎ𝑇, coefficients 𝛼1, . . . , 𝛼𝑇and sets 𝑆1, . . . , 𝑆𝑇.
C.2.3
Generalization via Sample Compression Schemes
Sample compression scheme is a classic technique for proving generalization bounds, introduced
by [LW86, FW95]. These bounds proved to be useful in numerous learning settings, such as
binary classification [GHST05, MY16, BHMZ20], multiclass classification [DSBDSS15, DSS14,
DMY16, BCD+22], regression [HKS18, HKS19], active learning [WHEY15], density estimation
[ABDH+20], adversarially robust learning [MHS19, MHS20, MHS21, MHS22, AHM22, AH22],
learning with partial concepts [AHHM22], and showing Bayes-consistency for nearest-neighbor
methods [GKN14, KSW17]. As a matter of fact, compressibility and learnability are known to be
equivalent for general learning problems [DMY16]. Another remarkable result by [MY16] showed
that VC classes enjoy a sample compression that is independent of the sample size.
We start with a formal definition of a sample compression scheme.
Definition 16 (Sample compression scheme). A pair of functions (𝜅, 𝜌) is a sample compression
scheme of size ℓfor class ℋif for any 𝑛∈N, ℎ∈ℋand sample 𝑆= {(𝑥𝑖, ℎ(𝑥𝑖))}𝑛
𝑖=1, it holds
for the compression function that 𝜅(𝑆) ⊆𝑆and |𝜅(𝑆) | ≤ℓ, and the reconstruction function
𝜌(𝜅(𝑆)) = ˆℎsatisfies ˆℎ(𝑥𝑖) = ℎ(𝑥𝑖) for any 𝑖∈[𝑛].
25

We show a generalization bound that scales with the sample compression size. The proof follows
from [LW86].
Lemma 10 (Sample compression scheme generalization bound). Fix a margin 𝛾∈[0, 1]. For any
𝑘∈N and fixed function 𝜑: (𝒳× [0, 1])𝑘→[0, 1]𝒳, for any distribution 𝒟over 𝒳× [0, 1] and
any 𝑚∈N, for 𝑆= {(𝑥𝑖, 𝑦𝑖)}𝑖∈[𝑚] i.i.d. 𝒟-distributed random variables, if there exist indices
𝑖1, ..., 𝑖𝑘∈[𝑚] such that ∑︀
(𝑥,𝑦)∈𝑆I{ℓ(𝜑((𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘))(𝑥), 𝑦) > 𝛾} = 0, then
E
(𝑥,𝑦)∼𝒟[I{ℓ(𝜑((𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘))(𝑥), 𝑦) > 𝛾}] ≤
1
𝑚−𝑘(𝑘log 𝑚+ log(1/𝛿)) .
with probability at least 1 −𝛿over 𝑆.
Proof. Let us define ̂︀ℓ𝛾(ℎ; 𝑆)
=
1
|𝑆|
∑︀
(𝑥,𝑦)∈𝑆I{ℓ(ℎ(𝑥), 𝑦)
>
𝛾} and ℓ𝛾(ℎ; 𝒟)
=
E(𝑥,𝑦)∼𝒟[I{ℓ(ℎ(𝑥), 𝑦) > 𝛾}]. For any indices 𝑖1, ..., 𝑖𝑘∈[𝑚], the probability of the bad event
Pr
𝑆∼𝒟𝑚[̂︀ℓ𝛾(𝜑((𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘)); 𝑆) = 0 ∧ℓ𝛾(𝜑((𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘)); 𝒟) > 𝜀]
is at most
E
[︁
I{ℓ𝛾(𝜑({(𝑥𝑖𝑗, 𝑦𝑖𝑗)}𝑗∈[𝑘]); 𝒟) > 𝜀} Pr[̂︀ℓ𝛾(𝜑({(𝑥𝑖𝑗, 𝑦𝑖𝑗)}𝑗∈[𝑘]); 𝑆∖{(𝑥𝑖𝑗, 𝑦𝑖𝑗)}𝑗∈[𝑘]) = 0|{(𝑥𝑖𝑗, 𝑦𝑖𝑗)}𝑗∈[𝑘]]
]︁
< (1 −𝜀)𝑚−𝑘
where the expectation is over (𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘) and the inner probability is over 𝑆∖
(𝑥𝑖1, 𝑦𝑖1), ..., (𝑥𝑖𝑘, 𝑦𝑖𝑘). Taking a union bound over all 𝑚𝑘possible choices for the 𝑘indices, we get
that the bad event occurs with probability at most
𝑚𝑘exp(−𝜀(𝑚−𝑘)) ≤𝛿⇒𝜀=
1
𝑚−𝑘(𝑘log 𝑚+ log(1/𝛿)) .
C.3
Putting it Together
We now have all the necessary ingredients in place to prove the upper bound of Theorem 2. First,
we use Lemma 8 on a sample of size 𝑛0 = DOIG
𝛾
(ℋ) to obtain a learner which makes 𝛾-errors with
probability at most 1/35. Then, we use the boosting algorithm we described (see Algorithm 2) to
obtain a learner that does not make any 𝛾-mistakes on the training set. Notice that the boosting
algorithm on its own does not provide any guarantees about the generalization error of the procedure.
This is obtained through the sample compression result we described in Appendix C.2.3. Since we
run the boosting algorithm for a few rounds on a sample whose size is small, we can provide a sample
compression scheme following the approach of [DMY16, HKS19].
Lemma 11 (Upper Bound of PAC Regression). Let ℋ⊆[0, 1]𝒳and 𝜀, 𝛿, 𝛾∈(0, 1)3. Then,
ℳ(ℋ; 𝜀, 𝛿, 𝛾) ≤𝑂
(︃
DOIG
𝛾
(ℋ)
𝜀
log2 DOIG
𝛾
(ℋ)
𝜀
+ 1
𝜀log 1
𝛿
)︃
.
Proof. Let 𝑛be the number of samples 𝑆= ((𝑥1, 𝑦1), . . . , (𝑥𝑛, 𝑦𝑛)) that are available to the learner,
𝑛0 = DOIG
𝛾
(ℋ) and let 𝐴be the algorithm obtained from Lemma 8. We have that
E
𝑆∼𝒟𝑛0−1
𝒳
[︂
Pr
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆; 𝑥), ℎ⋆(𝑥)) > 𝛾]
]︂
≤1/3 .
This means that, for any distribution 𝒟𝒳and any labeling function ℎ⋆∈ℋwe can draw a sample
𝑆⋆= ((𝑥1, 𝑦1), . . . , (𝑥𝑛0−1, 𝑦𝑛0−1)) with non-zero probability such that
Pr
𝑥∼𝒟𝒳[ℓ(𝐴(𝑆⋆; 𝑥), ℎ⋆(𝑥)) > 𝛾] ≤1
3 .
5In expectation over the training set.
26

Notice that such a classifier is a (𝛾, 1/6)-weak learner (see Definition 15). Thus, by executing the
MedBoost algorithm (see Algorithm 2) for 𝑇= 𝑂(log 𝑛) rounds we obtain a classifier ˆℎ: 𝒳→R
such that, ℓ(ˆℎ(𝑥𝑖), 𝑦𝑖) ≤𝛾, ∀𝑖∈[𝑛]. We underline that the subset 𝑆𝑡that is used in line 1 of
Algorithm 2 has size at most 𝑛0, for all rounds 𝑡∈[𝑇]. Thus, the total number of samples that is used
by MedBoost is at most 𝑂(𝑛0 log 𝑛). Hence, following the approach of [MY16] we can encode the
classifiers produced by MedBoost as a compression set that consists of 𝑘= 𝑂(𝑛0 log 𝑛) samples
that were used to train the classifiers along with 𝑘log 𝑘extra bits that indicate their order. Thus, using
generalization based on sample compression scheme as in Lemma 10, we have that with probability
at least 1 −𝛿over 𝑆∼𝒟𝑛,
E
(𝑥,𝑦)∼𝒟
[︁
I
{︁
ℓ(ˆℎ(𝑥), 𝑦) > 𝛾
}︁]︁
≤
𝐶
𝑛−𝑛0 log(𝑛)
(︀
𝑛0 log2 𝑛+ log(1/𝛿)
)︀
,
which means that for large enough 𝑛,
E
(𝑥,𝑦)∼𝒟
[︁
I
{︁
ℓ(ˆℎ(𝑥), 𝑦) > 𝛾
}︁]︁
≤𝑂
(︂𝑛0 log2 𝑛
𝑛
+ log(1/𝛿)
𝑛
)︂
.
Thus,
Pr
(𝑥,𝑦)∼𝒟
[︁
ℓ(ˆℎ(𝑥), 𝑦) > 𝛾
]︁
≤𝑂
(︂𝑛0 log2 𝑛
𝑛
+ log(1/𝛿)
𝑛
)︂
.
Hence, we can see that
ℳ(ℋ; 𝜀, 𝛿, 𝛾) ≤𝑂
(︃
DOIG
𝛾
(ℋ)
𝜀
log2 DOIG
𝛾
(ℋ)
𝜀
+ 1
𝜀log 1
𝛿
)︃
.
D
𝛾-DS Dimension and Learnability
In this section, we will show that finiteness of 𝛾-DS dimension is necessary for PAC learning in the
realizable case.
Theorem 3. Let ℋ⊆[0, 1]𝒳, 𝜀, 𝛿, 𝛾∈(0, 1)3. Then,
ℳ(ℋ; 𝜀, 𝛿, 𝛾) ≥Ω
(︃
DDS
2𝛾(ℋ) + log(1/𝛿)
𝜀
)︃
.
Proof. Let 𝑑= DOIG
2𝛾(ℋ). Then, there exists some 𝑆= (𝑥1, . . . , 𝑥𝑑) ∈𝒳𝑑such that ℋ|𝑆contains
a 2𝛾-pseudo-cube, which we call ℋ′. By the definition of the scaled pseudo-cube, ∀ℎ∈ℋ′, 𝑖∈[𝑑],
there is exactly one ℎ′ ∈ℋ′ such that ℎ(𝑥𝑗) = ℎ′(𝑥𝑗), 𝑗̸= 𝑖, and ℓ(ℎ(𝑥𝑖), ℎ′(𝑥𝑖)) > 2𝛾. We pick
the target function ℎ⋆uniformly at random among the hypotheses of ℋ′ and we set the marginal
distribution 𝒟𝒳of 𝒟as follows
Pr[𝑥1] = 1 −2𝜀,
Pr[𝑥𝑖] = 2𝜀/(𝑑−1), ∀𝑖∈{2, ..., 𝑑} .
Consider 𝑚samples {(𝑧𝑖, ℎ⋆(𝑧𝑖)}𝑖∈[𝑚] drawn i.i.d. from 𝒟. Let us take 𝑚≤𝑑−1
6𝜀. Then, the sample
will include at most (𝑑−1)/2 examples which are not 𝑥1 with probability 1/100, using Chernoff’s
bound. Let us call this event 𝐸. Conditioned on 𝐸, the posterior distribution of the unobserved
points is uniform among the vertices of the 𝑑/2-dimensional 2𝛾-pseudo-cube. Thus, if the test
point 𝑥falls among the unobserved points, the learner will make a 𝛾-mistake with probability at
least 1/2. To see that, let ̂︀𝑦be the prediction of the learner on 𝑥. Since every hyperedge has size at
least 2 and all the vertices that are on the hyperedge differ by at least 2𝛾in the direction of 𝑥, no
matter what ̂︀𝑦is the correct label 𝑦⋆is at least 𝛾-far from it. Since Pr[𝐸] ≥1/100, we can see that
ℳ𝒜(ℋ; 𝜀, 𝛿, 𝛾) = Ω( 𝑑
𝜀). Moreover, by the law of total probability there must exist a deterministic
choice of the target function ℎ⋆, that could depend on 𝒜, which satisfies the lower bound. For
the other part of the lower bound, notices the probability that the sample will only contain 𝑥1 is
27

(1 −2𝜀)𝑚≥𝑒−4𝜀𝑚which is greater that 𝛿whenever 𝑚≤log(1/𝛿)/(4𝜀). This implies that the
𝛾-cut-off sample complexity is lower bounded by
max
{︂
𝐶1 · 𝑑
𝜀, 𝐶2 · log(1/𝛿)
𝜀
}︂
= 𝐶0 · DDS
2𝛾(ℋ) + log(1/𝛿)
𝜀
.
Thus ℳ𝒜(ℋ; 𝜀, 𝛿, 𝛾), satisfies the desired bound when the dimension is finite. Finally, it remains
to claim about the case where DDS
2𝛾(ℋ) = ∞for the given 𝛾. We consider a sequence of 2𝛾-DS
shattered sets 𝑆𝑛with |𝑆𝑛| = 𝑛and repeat the claim for the finite case. This will yield that for any
𝑛the 𝛾-cut-off sample complexity is lower bounded by Ω((𝑛+ log(1/𝛿))/𝜀) and this yields that
ℳ(ℋ; 𝜀, 𝛿, 𝛾) = ∞.
We further conjecture that this dimension is also sufficient for PAC learning.
Conjecture 2. A class ℋ⊆(0, 1)𝒳is PAC learnable in the realizable regression setting with respect
to the absolute loss function if and only if DDS
𝛾(ℋ) < ∞for any 𝛾∈(0, 1).
We believe that there must exist a modification of the approach of [BCD+22] that will be helpful in
settling the above conjecture.
Conjecture 3. There exists ℋ⊆(0, 1)𝒳for which DNat
𝛾
(ℋ) = 1 for all 𝛾∈(0, 1) but DDS
𝛾(ℋ) < ∞
for some 𝛾∈(0, 1).
In particular, we believe that one can extend the construction of [BCD+22] (which uses various
tools from algebraic topology as a black-box) and obtain a hypothesis class ℋ⊆[0, 1]𝒳that has
𝛾-Natarajan dimension 1 but is not PAC learnable (it will have infinite 𝛾-DS dimension). This
construction though is not immediate and requires new ideas related to the works of [J´S03, Osa13]
E
Online Realizable Regression
In this section, we present our results regarding online realizable regression. The next result resolves
an open question of [DG22]. It provides an online learner with optimal (off by a factor of 2)
cumulative loss in realizable regression.
Theorem 4 (Optimal Cumulative Loss). Let ℋ⊆[0, 1]𝒳and 𝜀> 0. Then, there exists a deterministic
algorithm (Algorithm 3) whose cumulative loss in the realizable setting is bounded by Donl(ℋ) + 𝜀.
Conversely, for any 𝜀> 0, every deterministic algorithm in the realizable setting incurs loss at least
Donl(ℋ)/2 −𝜀.
Algorithm 3 Scaled SOA
Parameters: {𝜀𝑡}𝑡∈N.
Initialize 𝑉(1) = ℋ.
For 𝑡= 1, . . .:
1. Receive 𝑥𝑡∈𝒳.
2. For every 𝑦∈[0, 1], let 𝑉(𝑡)
(𝑥𝑡,𝑦) =
{︀
ℎ∈𝑉(𝑡) : ℎ(𝑥𝑡) = 𝑦
}︀
.
3. Let ̂︀𝑦𝑡be an arbitrary label such that
Donl (︁
𝑉(𝑡)
(𝑥𝑡,̂︀𝑦𝑡)
)︁
≥sup
𝑦′ Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦′)
)︁
−𝜀𝑡.
4. Predict ̂︀𝑦𝑡.
5. Receive the true label 𝑦⋆
𝑡and incur loss ℓ(̂︀𝑦𝑡, 𝑦⋆
𝑡).
6. Update 𝑉(𝑡+1) =
{︀
ℎ∈𝑉(𝑡) : ℎ(𝑥𝑡) = 𝑦⋆
𝑡
}︀
.
Proof. Let us begin with the upper bound. Assume that Donl(ℋ) < ∞. Suppose we are pre-
dicting on the 𝑡-th point in the sequence and let 𝑉(𝑡) be the version space so far, i.e., 𝑉(𝑡) =
28

{ℎ∈ℋ: ∀𝜏∈[𝑡−1], ℎ(𝑥𝜏) = 𝑦𝜏}. Let 𝑥𝑡be the next point to predict on. For each label 𝑦∈R,
let 𝑉(𝑡)
(𝑥𝑡,𝑦) = {ℎ∈𝑉(𝑡) : ℎ(𝑥𝑡) = 𝑦}. From the definition of the dimension Donl, we know that for
all 𝑦, 𝑦′ ∈R such that 𝑉(𝑡)
(𝑥𝑡,𝑦), 𝑉(𝑡)
(𝑥𝑡,𝑦′) ̸= ∅,
Donl(𝑉𝑡) ≥ℓ(𝑦, 𝑦′) + min
{︁
Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦)
)︁
, Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦′)
)︁}︁
.
Let ̂︀𝑦𝑡be an arbitrary label with Donl (︁
𝑉(𝑡)
(𝑥𝑡,̂︀𝑦𝑡)
)︁
≥sup𝑦′ Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦′)
)︁
−𝜀𝑡, where 𝜀𝑡is some
sequence shrinking arbitrarily quickly in the number of rounds 𝑡. The learner predicts ̂︀𝑦𝑡. Assume
that the adversary picks 𝑦⋆
𝑡as the true label and, so, the learner incurs loss ℓ(̂︀𝑦𝑡, 𝑦⋆
𝑡) at round 𝑡. Then,
the updated version space 𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡) has
Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁
≤sup
𝑦′ Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦′)
)︁
≤Donl (︁
𝑉(𝑡)
(𝑥𝑡,̂︀𝑦𝑡)
)︁
+ 𝜀𝑡,
which implies
min
{︁
Donl (︁
𝑉(𝑡)
(𝑥𝑡,̂︀𝑦𝑡)
)︁
, Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁}︁
≥Donl (︁
𝑉(𝑡)
(𝑥,𝑦⋆
𝑡)
)︁
−𝜀𝑡.
This gives that
Donl(𝑉(𝑡)) ≥ℓ(̂︀𝑦𝑡, 𝑦*
𝑡) + min
{︁
Donl (︁
𝑉(𝑡)
(𝑥𝑡,̂︀𝑦𝑡)
)︁
, Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁}︁
≥ℓ(̂︀𝑦𝑡, 𝑦*
𝑡) + Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁
−𝜀𝑡,
and, by re-arranging,
Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁
≤Donl(𝑉(𝑡)) −ℓ(̂︀𝑦𝑡, 𝑦*
𝑡) + 𝜀𝑡.
(1)
So every round reduces the dimension by at least the magnitude of the loss (minus 𝜀𝑡). Notice that
Donl(𝑉(𝑡+1)) = Donl (︁
𝑉(𝑡)
(𝑥𝑡,𝑦⋆
𝑡)
)︁
. Thus, by choosing the {𝜀𝑡}𝑡∈N sequence such that
∑︁
𝑡
𝜀𝑡≤𝜀′ ,
and summing up Equation (1) over all 𝑡∈N, we get a cumulative loss bound
∑︁
𝑡
ℓ(̂︀𝑦𝑡, 𝑦⋆
𝑡) ≤Donl(ℋ) + 𝜀′ .
Hence, we see that by taking the limit as 𝜀′ goes to 0 shows that the cumulative loss is upper bounded
by Donl(ℋ). This analysis shows that Algorithm 3 achieves the cumulative loss bound Donl(ℋ) + 𝜀′,
for arbitrarily small 𝜀′ > 0.
Let us continue with the lower bound. For any 𝜀> 0, we are going to prove that any deterministic
learner must incur cumulative loss at least Donl(ℋ)/2 −𝜀. By the definition of Donl(ℋ), for any
𝜀> 0, there exists a tree 𝑇𝜀such that, for every path 𝑦,
∞
∑︁
𝑖=1
𝛾𝑦≤𝑖≥Donl(ℋ) −2𝜀,
i.e., the sum of the gaps across the path is at least Donl(ℋ) −2𝜀. The strategy of the adversary is
the following: in the first round, she presents the learner with the instance 𝑥1 = 𝑥∅. Then, no matter
what label ˆ𝑦1 the learner picks, the adversary can choose the label 𝑦⋆
1 so that |ˆ𝑦1 −𝑦⋆
1| ≥𝛾∅/2. The
adversary can keep picking the instances 𝑥𝑡based on the induced path of the choices of the true
labels {𝑦⋆
𝜏}𝜏<𝑡and the loss of the learner in every round 𝑡is at least 𝛾𝑦≤𝑡/2. Thus, summing up over
all the rounds as 𝑇→∞, we see that the total loss of the learner is at least
Donl(ℋ)
2
−𝜀.
29

Remark 2 (Randomized Online Learners). We highlight that, unlike the setting of realizable online
classification, in the case of realizable online regression randomization does not seem to help
the learner (see also [FHMM23]). In particular, the lower bound of Donl(ℋ)
2
−𝜀holds even for
randomized learners. To see it, notice that for all distributions over 𝒟over [0, 1] it holds that
max
𝑐1,𝑐2
{︁
E
𝑋∼𝒟[ℓ(𝑋, 𝑐1)], E
𝑋∼𝒟[ℓ(𝑋, 𝑐2)]
}︁
≥ℓ(𝑐1, 𝑐2)/2 .
Example 2 (Sequential Complexity Measures). Sequential fat-shattering dimension and sequen-
tial covering numbers are two standard combinatorial measures for regression in online settings
[RST15b, RST15a]. Note that Example 1 can be learned with 1 sample even in the online realizable
setting. Hence, Example 1 shows that sequential fat-shattering dimension fails to characterize online
realizable regression (since this dimension is at least as large as fat-shattering dimension which
is infinite in this example). Moreover, we know that sequential covering numbers and sequential
fat-shattering dimension are of the same order of magnitude and so they are also infinite in the case
of Example 1.
F
Dimension and Finite Character Property
[BDHM+19] gave a formal definition of the notion of “dimension” or “complexity measure”, that all
previously proposed dimensions in statistical learning theory comply with. In addition to characteriz-
ing learnability, a dimension should satisfy the finite character property:
Definition 17 (Finite Character [BDHM+19]). A dimension characterizing learnability can be
abstracted as a function 𝐹that maps a class ℋto N ∪{∞} and satisfies the finite character property:
for every 𝑑∈N and ℋ, the statement “𝐹(ℋ) ≥𝑑” can be demonstrated by a finite set 𝑋⊆𝒳
of domain points, and a finite set of hypotheses 𝐻⊆ℋ. That is, “𝐹(ℋ) ≥𝑑” is equivalent to
the existence of a bounded first order formula 𝜑(𝒳, ℋ) in which all the quantifiers are of the form:
∃𝑥∈𝒳, ∀𝑥∈𝒳or ∃ℎ∈ℋ, ∀ℎ∈ℋ.
Claim 1. The scaled one-inclusion graph dimension DOIG
𝛾
(ℋ) satisfies the finite character property.
Proof. To demonstrate that DOIG(ℋ) ≥𝑑, it suffices to find a set 𝑆of 𝑛domain points and present
a finite subgraph 𝐺= (𝑉, 𝐸) of the one-inclusion hypergraph induced by 𝑆where every orientation
𝜎: 𝐸→𝑉has out-degree at least 𝑛/3. Note that 𝑉is, by definition, a finite collection of datasets
realizable by ℋand so this means that we can demonstrate that DOIG(ℋ) ≥𝑑with a finite set of
domain points and a finite set of hypotheses.
G
Examples for Scaled Graph Dimension
These examples are adaptations from [DSS14, DSBDSS15].
Example 3 (Large Gap Between ERM Learners). For every 𝑑∈N, consider a domain 𝒳𝑑such that
|𝒳𝑑| = 𝑑and 𝒳𝑑, 𝒳𝑑′ are disjoint for 𝑑̸= 𝑑′. For all 𝑑∈N, let 𝑃(𝒳𝑑) denote the collection of all
finite and co-finite6 subsets of 𝒳𝑑. Let us fix 𝛾∈(0, 1). Consider a mapping 𝑓: ∪𝑑∈N𝑃(𝒳𝑑) →
[0, 1] such that 𝑓(𝐴𝑑) ∈(𝛾, 1) for all 𝑑∈N, 𝐴𝑑∈𝑃(𝒳𝑑), and 𝑓(𝐴𝑑) ̸= 𝑓(𝐴′
𝑑′) for all 𝐴𝑑̸=
𝐴′
𝑑′, 𝐴𝑑∈𝑃(𝒳𝑑), 𝐴𝑑′ ∈𝑃(𝒳𝑑′). Such a mapping exists due to the density of the reals. For any
𝑑∈N, 𝐴𝑑⊆𝒳𝑑, let ℎ𝐴𝑑(𝑥) = 𝑓(𝐴𝑑) · 1{𝑥∈𝐴𝑑} and consider the scaled first Cantor class
ℋ𝒳𝑑,𝛾= {ℎ𝐴𝑑: 𝐴𝑑∈𝑃(𝒳𝑑)}. We claim that DNat
𝛾
(ℋ𝒳𝑑,𝛾) = 1 and that DG
𝛾(ℋ𝒳𝑑,𝛾) = |𝒳𝑑| = 𝑑
since one can use 𝑓∅for the 𝛾-graph shattering. Consider the following two ERM learners for the
scaled first Cantor class ℋ𝒳𝑑,𝛾:
1. Whenever a sample of the form 𝑆= {(𝑥𝑖, 0)}𝑖∈[𝑛] is observed, the first algorithm outputs
ℎ∪{𝑥𝑖}𝑐
𝑖∈[𝑛] which minimizes the empirical error. If the sample contains a non-zero element,
the ERM learner identifies the correct hypothesis. The sample complexity of PAC learning is
Ω(𝑑).
2. The second algorithm either returns the all-zero function or identifies the correct hypothesis
if the sample contains a non-zero label. This is a good ERM learner 𝒜ERM
good with sample
complexity 𝑚(𝜀, 𝛿) = 1
𝜀log
(︀1
𝛿
)︀
.
6A set 𝑆⊆𝒳𝑑is co-finite if its complement 𝑆𝑐is finite.
30

The construction that illustrates the poor performance of the first learner is exactly the same as in
the proof of the lower bound of Theorem 1. The second part of the example is formally shown in
Claim 2, which follows.
Claim 2 (Good ERM Learner). Let 𝜀, 𝛿∈(0, 1)2. Then, the good ERM learner of Example 3 has
sample complexity ℳ(𝜀, 𝛿) = 1
𝜀log
(︀1
𝛿
)︀
.
Proof. Let 𝑑∈N, 𝒟𝒳𝑑be a distribution over 𝒳𝑑and ℎ𝐴⋆
𝑑be the labeling function. Consider a sample
𝑆of length 𝑚. If the learner observes a value that is different from 0 among the labels in 𝑆, then
it will be able to infer ℎ𝐴⋆
𝑑and incur 0 error. On the other hand, if the learner returns the all zero
function its error can be bounded as
E
𝑥∼𝒟𝒳𝑑
[ℓ(ℎ∅(𝑥), ℎ𝐴⋆
𝑑(𝑥))] ≤
Pr
𝑥∼𝒟𝒳𝑑
[𝑥∈𝐴⋆
𝑑] .
Since in all the 𝑚= 1
𝜀log
(︀1
𝛿
)︀
draws of the training set 𝑆there were no elements from 𝐴⋆
𝑑we can
see that, with probability at least 1 −𝛿over the draws of 𝑆it holds that
Pr
𝑥∼𝒟𝒳𝑑
[𝑥∈𝐴⋆
𝑑] ≤𝜀.
Thus, the algorithm satisfies the desired guarantees.
The next example shows that no proper algorithm can be optimal in the realizable regression setting.
Example 4 (No Optimal PAC Learner Can be Proper). Let 𝒳𝑑contain 𝑑elements and let 𝛾∈(0, 1).
Consider the subclass of the scaled first Cantor class (see Example 3) with ℋ′
𝑑,𝛾= {ℎ𝐴: 𝐴∈
𝑃(𝒳𝑑), |𝐴| = ⌊𝑑/2⌋}. First, since this class is contained in the scaled first Cantor class, we can
employ the good ERM and learn it. However, this learner is improper since ℎ∅/∈ℋ′
𝑑,𝛾. Then, no
proper algorithm is able to PAC learn ℋ′
𝑑,𝛾using 𝑜(𝑑) examples.
Proof. Suppose that an adversary chooses ℎ𝐴∈ℋ′
𝑑,𝛾uniformly at random and consider the distribu-
tion on 𝒳𝑑which is uniform on the complement of 𝐴, where |𝐴| = 𝑂(𝑑). Note that the error of every
hypothesis ℎ𝐵∈ℋ′
𝑑,𝛾is at least 𝛾|𝐵∖𝐴|/𝑑. Therefore, to return a hypothesis with small error, the
algorithm must recover a set that is almost disjoint from 𝐴and so recover 𝐴. However the size of 𝐴
implies that it cannot be done with 𝑜(𝑑) examples.
Formally, fix 𝑥0 ∈𝒳𝑑and 𝜀∈(0, 1). Let 𝐴⊆𝒳𝑑∖{𝑥0} of size 𝑑/2. Let 𝒟𝐴be a distribution with
mass 𝒟𝐴((𝑥0, ℎ𝐴(𝑥0))) = 1 −16𝜀and is uniform on the points {(𝑥, ℎ𝐴(𝑥)) : 𝑥∈𝐴𝑐}, where 𝐴𝑐
is the complement of 𝐴(without 𝑥0).
Consider a proper learning algorithm 𝒜. We will show that there is some algorithm-dependent set 𝐴,
so that when 𝒜is run on 𝒟𝐴with 𝑚= 𝑂(𝑑/𝜀), it outputs a hypothesis with error at least 𝛾with
constant probability.
Pick 𝐴uniformly at random from all sets of size 𝑑/2 of 𝒳𝑑∖{𝑥0}. Let 𝑍be the random variable
that counts the number of samples in the 𝑚draws from 𝒟𝐴that are not (𝑥0, ℎ𝐴(𝑥0)). Standard
concentration bounds imply that with probability at least 1/2, the number of points from (𝒳𝑑∖
{𝑥0}) ∖𝐴is at most 𝑑/4. Conditioning on this event, 𝐴is a uniformly chosen random set of size 𝑑/2
that is chosen uniformly from all subsets of a set 𝒳′ ⊂𝒳𝑑with |𝒳′| ≥3𝑑/4 (these points are not
present in the sample). Now assume that the learner returns a hypothesis ℎ𝐵, where 𝐵is a subset of
size 𝑑/2. Note that E[|𝐵∖𝐴|] ≥𝑑/6. Hence there exists a set 𝐴such that with probability 1/2, it
holds that |𝐵∖𝐴| ≥𝑑/6. This means that 𝒜incurs a loss of at least 𝛾on all points in 𝐵∖𝐴and the
mass of each such point is Ω(𝜀/𝑑). Hence, in total, the learner will incur a loss of order 𝛾· 𝜀.
H
Extension to More General Loss Functions
Our results can be extended to loss functions that satisfy approximate pseudo-metric axioms (see
e.g., [HKLM22, CKW08]). The main difference from metric losses is that we allow an approximate
triangle inequality instead of a strict inequality. Many natural loss functions are captured by this
definition, such as the well-studied ℓ𝑝losses for the regression setting. Abstractly, in this context, the
31

label space7 is an abstract non-empty set 𝒴, equipped with a general loss function ℓ: 𝒴2 →R≥0
satisfying the following property.
Definition 18 (Approximate Pseudo-Metric). For 𝑐≥1, a loss function ℓ: 𝒴2 →R≥0 is 𝑐-
approximate pseudo-metric if (i) ℓ(𝑥, 𝑥) = 0 for any 𝑥∈𝒴, (ii) ℓ(𝑥, 𝑦) = ℓ(𝑦, 𝑥) for any 𝑥, 𝑦∈𝒴,
and, (iii) ℓsatisfies a 𝑐-approximate triangle inequality; for any 𝑦1, 𝑦2, 𝑦3 ∈𝒴, it holds that
ℓ(𝑦1, 𝑦2) ≤𝑐(ℓ(𝑦1, 𝑦3) + ℓ(𝑦2, 𝑦3)).
Furthermore, note that all dimensions for ℋ, DG
𝛾(ℋ), DOIG
𝛾
(ℋ), DDS
𝛾(ℋ), and Donl
𝛾(ℋ) are defined
for loss functions satisfying Definition 18.
Next, we provide extensions of our main results for approximate pseudo-metric losses and provide
proof sketches for the extensions.
ERM Learnability for Approximate Pseudo-Metrics.
For ERM learnability and losses satisfying
Definition 18, we can obtain the next result.
Theorem 5. Let ℓbe a loss function satisfying Definition 18. Then for every class ℋ⊆𝒴𝒳, ℋis
learnable by any ERM in the realizable PAC regression setting under ℓif and only if DG
𝛾(ℋ) < ∞
for all 𝛾∈(0, 1).
The proof of the upper bound and the lower bound follow in the exact same way as with the absolute
loss.
PAC Learnability for Approximate Pseudo-Metrics.
As for PAC learning with approximate
pseudo-metric losses, we can derive the next statement.
Theorem 6. Let ℓbe a loss function satisfying Definition 18. Then every class ℋ⊆𝒴𝒳is PAC
learnable in the realizable PAC regression setting under ℓif and only if DOIG
𝛾
(ℋ) < ∞for any
𝛾∈(0, 1).
Proof Sketch. We can generalize the upper bound in Theorem 2 for the scaled OIG dimension as
follows. One of the ingredients of the proof for the absolute loss is to construct a sample compression
scheme through the median boosting algorithm (cf. Algorithm 2). While the multiplicative update
rule is defined for any loss function, the median aggregation is no longer the right aggregation
for arbitrary (approximate) pseudo-metrics. However, for each such loss function, there exists an
aggregation such that the output value of the ensemble is within some cutoff value from the true label
for each example in the training set, which means that we have a sample compression scheme for
some cutoff loss. In particular, we show that by using weak learners with cutoff parameter 𝛾/(2𝑐),
where 𝑐is the approximation level of the triangle inequality, the aggregation of the base learners can
be expressed as a sample compression scheme for cutoff loss with parameter 𝛾.
Indeed, running the boosting algorithm with (𝛾/(2𝑐), 1/6)-weak learners yields a set ℎ1, . . . , ℎ𝑁
of weak predictors, with the property that for each training example (𝑥, 𝑦), at least 2/3 of the
functions ℎ𝑖(as weighted by coefficients 𝛼𝑖), 1 ≤𝑖≤𝑁, satisfy ℓ(ℎ𝑖(𝑥), 𝑦) ≤𝛾/(2𝑐). For any
𝑥, let ˆℎ(𝑥) be a value in 𝒴such that at least 2/3 of ℎ𝑖(as weighted by 𝛼𝑖), 1 ≤𝑖≤𝑁, satisfy
ℓ(ℎ𝑖(𝑥), ˆℎ(𝑥)) ≤𝛾/(2𝑐), if such a value exists, and otherwise ˆℎ(𝑥) is an arbitrary value in 𝒴. In
particular, note that on the training examples (𝑥, 𝑦), the label 𝑦satisfies this property, and hence ˆℎ(𝑥)
is defined by the first case. Thus, for any training example, there exists ℎ𝑖(indeed, at least 2/3 of
them) such that both ℓ(ℎ𝑖(𝑥), 𝑦) ≤𝛾/(2𝑐) and ℓ(ℎ𝑖(𝑥), ˆℎ(𝑥)) ≤𝛾/(2𝑐) are satisfied, and therefore
we have
ℓ(ˆℎ(𝑥), 𝑦) ≤𝑐(ℓ(ˆℎ(𝑥), ℎ𝑖(𝑥)) + ℓ(ℎ𝑖(𝑥), 𝑦)) ≤𝛾.
This function ˆℎcan be expressed as a sample compression scheme of size 𝑂
(︁
DOIG
𝛾/(2𝑐)(ℋ) log(𝑚)
)︁
for cutoff loss with parameter 𝛾: namely, it is purely defined by the ℎ𝑖functions, where each ℎ𝑖is
7We would like to mention that, in general, we do not require that the label space is bounded. In contrast, we
have to assume that the loss function takes values in a bounded space. This is actually necessary since having
an unbounded loss in the regression task would potentially make the learning task impossible. For instance,
having some fixed accuracy goal, one could construct a learning instance (distribution over labeled examples)
that would make estimation with that level of accuracy trivially impossible.
32

specified by 𝑂
(︁
DOIG
𝛾/(2𝑐)(ℋ)
)︁
training examples, and we have 𝑁= 𝑂(log(𝑚)) such functions, and ˆℎ
satisfies ℓ(ˆℎ(𝑥), 𝑦) ≤𝛾for all 𝑚training examples (𝑥, 𝑦). Thus, by standard generalization bounds
for sample compression, we get an upper bound that scales with ˜𝑂
(︁
DOIG
𝛾/(2𝑐)(ℋ) 1
𝑚
)︁
for the cutoff
loss with parameter 𝛾, and hence by Markov’s inequality, an upper bound
E[ℓ(ˆℎ(𝑥), 𝑦)] = ˜𝑂
(︂
DOIG
𝛾/(2𝑐)(ℋ) 1
𝑚𝛾
)︂
.
We next deal with the lower bound. For the absolute loss, we scale the dimension by 2𝛾instead of 𝛾
since for any two possible labels 𝑦1, 𝑦2 the learner can predict some intermediate point, and we want
to make sure that the prediction will be either 𝛾far from 𝑦1 or 𝑦2. For an approximate pseudo-metric,
we should take instead 2𝑐𝛾in order to ensure that the prediction is 𝛾far, which means that the lower
bounds in Theorem 2 hold with a scale of 2𝑐𝛾.
Online Learnability for Approximate Pseudo-Metrics.
Finally, we present the more general
statement for online learning.
Theorem 7. Let ℓbe a loss function satisfying Definition 18 with parameter 𝑐≥1. Let ℋ⊆𝒴𝒳and
𝜀> 0. Then, there exists a deterministic algorithm whose cumulative loss in the realizable setting is
bounded by Donl(ℋ) + 𝜀. Conversely, for any 𝜀> 0, every deterministic algorithm in the realizable
setting incurs loss at least Donl(ℋ)/(2𝑐) −𝜀.
Proof Sketch. The upper bound of Theorem 4 works for any loss function. Recall the proof idea;
in every round 𝑡there is some ̂︀𝑦𝑡∈𝒴the learner can predict such that no matter what the
adversary picks as the true label 𝑦⋆
𝑡, the online dimension of the version space at round 𝑡, i.e,
𝑉= {ℎ∈ℋ: ℎ(𝑥𝜏) = 𝑦⋆
𝜏, 1 ≤𝜏≤𝑡}, decreases by ℓ(𝑦⋆
𝑡, ̂︀𝑦𝑡), minus some shrinking number 𝜖𝑡
that we can choose as a parameter. Therefore we get that the sum of losses is bounded by the online
dimension and the sum of 𝜖𝑡that we can choose to be arbitrarily small.
The lower bound for online learning in Theorem 4 would be Donl(ℋ)/(2𝑐) −𝜀, for any 𝜖> 0, since
the adversary can force a loss of 𝛾𝑦≤𝑡/(2𝑐) in every round 𝑡, where 𝛾𝑦≤𝑡is the sum of the gaps
across the path 𝑦.
33

