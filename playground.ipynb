{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "\n",
    "def tau_closest_agents(agent, agents, d, tau) -> tuple[list[str], float]:\n",
    "    \"\"\"Return the list of tau-closest agents to agent.\n",
    "    agent: agent's id\n",
    "    agents: list of agents' id\n",
    "    d: distance function\n",
    "    tau: threshold number (usually number of agents in a cluster)\n",
    "\n",
    "    Return:\n",
    "        - list of tau-closest agents' id\n",
    "        - distance to the furthest agent\n",
    "    \"\"\"\n",
    "    return None # TODO: implement\n",
    "\n",
    "\n",
    "def SmallestAgentBall(agents, d, tau) -> list[str]:\n",
    "    \"\"\"Return the set of per_clusterclosest agents to the agent of the smallest ball.\n",
    "    N: list of agents' id\n",
    "    d: distance function\n",
    "    tau: threshold number (usually number of agents in a cluster)\n",
    "    \"\"\"\n",
    "    if len(agents) <= tau:\n",
    "        return agents[:]\n",
    "    lst = [] # (agent, its tau-closest agent, distance) #TODO: find a better name\n",
    "    for agent in agents:\n",
    "        closest_agents = tau_closest_agents(agent, agents, d, tau) # (list, distance to the furthest)\n",
    "        lst.append((agent, *closest_agents))\n",
    "    min_ball = min(lst, key=lambda x: x[2])\n",
    "    return min_ball[1]\n",
    "\n",
    "\n",
    "def GreedyCohesiveClustering(agents, d, k) -> list[list[str]]:\n",
    "    \"\"\" Return the k cohesive clusters of agents by metric d. Each cluster is a list of id.\n",
    "    agents: list of agents' id\n",
    "    d: distance function\n",
    "    k: number of clusters to return\n",
    "    \"\"\"\n",
    "    clusters = [] # each cluster is a list of id\n",
    "    N = agents[:]\n",
    "    j = 1\n",
    "    per_cluster = math.ceil(len(agents)/k)\n",
    "    while N:\n",
    "        C_j = SmallestAgentBall(N, d, per_cluster)\n",
    "        clusters.append(C_j)\n",
    "        for agent in C_j:\n",
    "            N.remove(agent)\n",
    "        j += 1\n",
    "    return clusters\n"
   ],
   "id": "1cd7d8b7cdaddb13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ],
   "id": "f578a4a43c0fd3d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(\"API KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
   "id": "f19b07c79b382cbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10de2eda30f2c507"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:02:27.590680Z",
     "start_time": "2025-05-13T20:02:27.274417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import fitz\n",
    "from tqdm import tqdm"
   ],
   "id": "8d4c2d4bd210f7e2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:11:10.620595Z",
     "start_time": "2025-05-13T20:11:10.617816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {str(e)}\")\n",
    "        return \"\""
   ],
   "id": "e5968c16715c695",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:22:12.008995Z",
     "start_time": "2025-05-13T20:21:42.316339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read metadata\n",
    "df = pd.read_csv('./data/metadata/ICLR_2021_2024.csv')\n",
    "\n",
    "# Create unified text directory if it doesn't exist\n",
    "os.makedirs('./data/unified_text/ICLR', exist_ok=True)\n",
    "\n",
    "# Process each year\n",
    "# years = range(2021, 2025)\n",
    "years = [2024]\n",
    "for year in years:\n",
    "    year_data = []\n",
    "    year_df = df[df['year'] == year]\n",
    "\n",
    "    for _, row in tqdm(year_df.iterrows(), total=len(year_df)):\n",
    "        paper_id = row['id']\n",
    "        title = row['title']\n",
    "        txt_path = f'./data/txts/ICLR_2021_2024/{title}.txt'\n",
    "        pdf_path = f'./data/pdfs/ICLR_2021_2024/{title}.pdf'\n",
    "\n",
    "        paper_dict = row.to_dict()\n",
    "\n",
    "        # Try to get text from txt file first\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                paper_dict['text'] = f.read()\n",
    "        # If txt doesn't exist, extract from PDF\n",
    "        elif os.path.exists(pdf_path):\n",
    "            paper_dict['text'] = extract_text_from_pdf(pdf_path)\n",
    "        else:\n",
    "            paper_dict['text'] = \"\"\n",
    "            print(f\"Neither txt nor pdf found for paper {paper_id}\")\n",
    "\n",
    "        year_data.append(paper_dict)\n",
    "\n",
    "    # Save to JSON\n",
    "    output_path = f'./data/unified_text/ICLR/ICLR_{year}.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(year_data, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "b067757079e3049e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [00:28<00:00,  8.66it/s]\n",
      "100%|██████████| 86/86 [00:00<00:00, 102.87it/s]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:41:25.793020Z",
     "start_time": "2025-05-13T19:41:25.632110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = extract_text_from_pdf('./data/pdfs/ICLR_2021_2024/An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment.pdf')\n",
    "with open(\"./data/unified_text/ICLR/ICLR_2024.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "for entry in data:\n",
    "    if entry['title'] == 'An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment':\n",
    "        entry['text'] = text\n",
    "with open(\"./data/unified_text/ICLR/ICLR_2024.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "4543f961585775c6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:11:16.887011Z",
     "start_time": "2025-05-13T20:11:16.647474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# unify for neurips\n",
    "# Read metadata\n",
    "df = pd.read_csv('./data/metadata/neurips_2023_2024.csv')\n",
    "\n",
    "# Create unified text directory if it doesn't exist\n",
    "os.makedirs('./data/unified_text/NeurIPS', exist_ok=True)\n",
    "\n",
    "# Process each year\n",
    "years = range(2023, 2025)\n",
    "for year in years:\n",
    "    year_data = []\n",
    "    year_df = df[df['year'] == year]\n",
    "\n",
    "    for _, row in tqdm(year_df.iterrows(), total=len(year_df)):\n",
    "        paper_id = row['id']\n",
    "        title = row['title']\n",
    "        txt_path = f'./data/txts/neurips_2023_2024/{title}.txt'\n",
    "        pdf_path = f'./data/pdfs/neurips_23_24/{title}.pdf'\n",
    "\n",
    "        paper_dict = row.to_dict()\n",
    "\n",
    "        # Try to get text from txt file first\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                paper_dict['text'] = f.read()\n",
    "        # If txt doesn't exist, extract from PDF\n",
    "        elif os.path.exists(pdf_path):\n",
    "            paper_dict['text'] = extract_text_from_pdf(pdf_path)\n",
    "        else:\n",
    "            paper_dict['text'] = \"\"\n",
    "            print(pdf_path)\n",
    "            print(f\"Neither txt nor pdf found for paper {paper_id}\")\n",
    "\n",
    "        year_data.append(paper_dict)\n",
    "\n",
    "    # Save to JSON\n",
    "    output_path = f'./data/unified_text/NeurIPS/{year}.json'\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(year_data, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "69a249537e748361",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [00:00<00:00, 2208.22it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 1984.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/pdfs/neurips_23_24/The PRISM Alignment Dataset_ What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models.pdf\n",
      "Neither txt nor pdf found for paper DFr5hteojx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:22:54.091950Z",
     "start_time": "2025-05-13T20:22:52.989533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = extract_text_from_pdf('./data/pdfs/neurips_23_24/The PRISM Alignment Dataset_ What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large L.pdf')\n",
    "with open(\"./data/unified_text/NeurIPS/2024.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "for entry in data:\n",
    "    if entry['title'] == 'The PRISM Alignment Dataset_ What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models':\n",
    "        entry['text'] = text\n",
    "        print('success')\n",
    "with open(\"./data/unified_text/NeurIPS/2024.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "266e259a9f2e91e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T20:25:39.046487Z",
     "start_time": "2025-05-13T20:25:38.623303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/metadata/ICLR_2021_2024.csv')\n",
    "df = df[df['year'] == 2024]\n",
    "print(df.shape)\n",
    "\n",
    "with open(\"./data/unified_text/ICLR/ICLR_2024.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "count = 0\n",
    "miss_content = 0\n",
    "for entry in data:\n",
    "    count += 1\n",
    "    if entry['text'] == '':\n",
    "        miss_content += 1\n",
    "        print(entry['title'])\n",
    "print(count)\n",
    "print(miss_content)"
   ],
   "id": "49a322a0526e8cb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 11)\n",
      "86\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
